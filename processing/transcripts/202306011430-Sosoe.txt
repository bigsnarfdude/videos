Joseph is going to be telling us about real bounds for APZ models he's done for research. Like many of you, I haven't had the pleasure to have a lot of close interactions with Timo, so I certainly don't have a story that can tell Duncan's live at first sight. But I think actually the first time I met Timo was at IMA in 2015, and then I said, I'm Phil, and he said, I know. So I don't know whether to interpret this as something good or bad. But I have been influenced by Timo's work a lot over the last few years. Actually, for a long time, even when I was a postdoc, I was interested in KPZ, but I didn't really have the stomach to deal with all these formulas. And so Timo's work on getting exponents for these stationary models was always an inspiration. But as you probably know, the notation in Timo's paper. Notation in England's paper is often legendary, so it was not always easy to really understand what's going on. Actually, I remember the time he visited Harvard, several of Yao's postdocs and graduate students, we're interested in finally understanding where does the two-thirds, one-third exponent come from. And then after his talk, we had a discussion, and I don't remember who, but one of us said, I think he's cubing something from us. So, I guess what I'm going to do today is just summarize what I've learned from Timo over the years, from his nice papers. And as usual, this is work with John Fan Lein. Two goals are like in this goal when you report and you're reading, or when I understand Timo's work. And as a case study or an example of this, I'll complement one of Timo's breakthrough results, this T213 current fluctuations. This T313 current fluctuations for stationary ASAP balance separate. This is one of the papers that was highlighted a couple of times already from this conference. Okay, so the model is ASAP. Many of you know much more about ASAP than I do. We have particles on Z that perform continuous time random walks, asymmetric, with rate R to the right and rate L to the left, subject to this exclusion condition. There's already a particle of L. Condition. There's already a vertical at the target site, and you type moment. Okay? And the object of interest for us will be the current. The current is the number of particles that cross the interval 1 half x plus 1 half in time t. So you run ASEP for a long time and you look at how many particles have crossed this interval. You can think of it as a current across the edge of zero. Across the edge 0, 1 minus all the particles that at time t, which should be capital T here, are between 0 and x. And this is a picture. You can also think of it in space-time as the number of particles, if t time t equal to 0, 0, and time capital T is over here. There's the number of particles during the evolution across this plane here from one half from this edge here. From this edge here, say 1/2 1/2 on the VR axis. And so the famous result of Boss and Sepolina, this breakthrough result, is they showed that you look at the current in a characteristic direction, so if you choose your X0 exponent here in ASAP with Bernoulli initial data, so stationary ASAP with Bernoulli initial data, like Initial data B, then the variance of the current is of order p to the two-thirds. This is characteristic of the ChipZ universality class. And what I'm going to talk about today is extending this result to tailbounds that are consistent with this team to the winter fluctuation work. In the meantime, so between this result from 10 years or more ago and today I'm off proved that. Moff proved that when you take this quantity here in the stationary case, you remove the expectation, you divide by t to the 1/3, you actually get a limiting distribution, the byte-Grains distribution. And the byte-Reigns distribution looks in its tails like the Tracy Winter distribution. So it has this U to the three halves behavior for its tails. So this is like a moderate deviation version of it in Cape New Zealand. All right. Um there's one more result that I want to highlight that comes from this great paper again, and that's the location or a bound for the location of the second class particle. So in ASAP, I think several people already have talked about second-class particles. What you can do is you can introduce particles of the different colors, right? And these particles try to perform ASEP. And these particles try to perform ASAP, except that they are lower in the hierarchy than normal particles. In particular, if a first-class particle tries to jump to the location of a second-class particle, then the exchange between costs, but a second-class particle cannot do this to a first-class particle. One way that second-class particles appear in ASAP is when you look at the discrepancy between two initial data that are coupled according to basic coupling. You can think of these. Can think of these clocks, the ring, that is being attached to the edges. Actually, that's the proper way to think about it. And then, for a given initial data, you can run the dynamics according to these clocks, but you can use the same jump times for, or try to use the same jump times for another initial data. This way you can couple two initial data, and you can look at the discrepancy between them. So, here you see one initial data, eta. It has variable here, whereas new doesn't. If you look at the difference between them, it's not a good The difference between them, it's not a good choice of color here, or whether it's red or something, that's the second class purple. It evolves in the second class purple. Okay, so the result that, oh, this is, okay, I went back. Okay, so the result that Walash and Sepulan obtained for the location of a second-class particle started at zero in an environment that's otherwise permovic was the following. They could show that. is the following. They could show that all the moments up to three behave consistently with the centered location of Q being of order t to the two thirds. And so we can upgrade or complement, maybe it's better, this result with tails of order u cubed here with the correct exponent u t. And so this is the reason I chose to talk about this. This is the reason I chose to talk about this is because this is a great way to talk about two really highlights of point. Timo's work, this is microscopic concavity coupling that was introduced with Martin Baus and it was used in many papers, including his famous paper on the current. And a newer work, the work with Amrajanji. M. Raj and Jupyter and stuff like that. Sorry about this, where they introduced an exponential version of the method that Timo developed with Martina. And in this case, it will be complemented, put together, by an extra ingredient, which is a degeneration of the stochastic six-virtux model, which as we'll see is the perfect model to apply the ideas that we developed over the years. Okay. So what is the stochastics expertise model? The stochastics expertise model, like all the models or many of the models that we've seen in this conference, is best thought of as taking place in a quadrant. And so let's take a sake of finite sub-rectangle of the quadrant. And it's an assignment of arrows to the vertices inside this quadrant. So every vertex has. Every vertex has incoming and outgoing arrows, and the rule is that the number of incoming arrows at every vertex has to be equal to the number of outgoing arrows. So, just to give you an idea, there are six possible configurations, and they're depicted here. These are the six configurations that are consistent with this rule of having incoming arrows, so local configurations at one vertex. They're consistent with having as many incoming as outgoing arrows. Config as point arrows. And each of these configurations is going to contribute to the total measure of a stochastic stick vertex on configuration according to the weights that we see over here. And one way to think about this, that I'll show you in a moment, to think about these weights and to make sense of them, to remember them, is to think about this model as a model of non-intersecting paths. So once you build So, once you fill in here, I'm not going to do it very well, but once you fill in a six-vertex configuration into this rectangle, then you see that you can interpret the configuration as upright paths that can intersect, but they sort of bounce each other, off each other, so they don't cross. So, for example, here we have a path that starts over here, going up, and here we have another path that meets this path over here, but then bounces off. This path over here, but then it bounces off and it goes up, and so on. In this way, you have these non-intersecting paths. And once you've filled in a configuration like this to decide which weight it has, you look at the number of vertices of the different types according to this chart, and you multiply the weights and exponentially into the number of vertices of that type. So that's the stochastic six-virtive small. Everybody happy? Everybody happy? So, this really sounds like it's going to be a KPZ type model. We're going to be looking at the stationary stochastics expertise model. This is a stationary version of the model. And even without me telling you anything, if you've been listening to this conference, you can feel what, you know, you can probably have a feeling for what stationary is going to mean. So it turns out that if you choose boundary conditions, so far I've talked about what you do inside, but you can choose boundary conditions. You do inside, but you can choose boundary conditions. You can decide on the boundary here to force some arrows to either be there or not. So I mean, horizontal boundary, and you can do the same over here. And you can do this at random. So you can put a Bernoulli distribution of vertical arrows over here, and a Bernoulli distribution of arrows of arrows on the vertical axis. And this distribution we're going to call zero, this one, sorry. This one, sorry. Okay. And if you choose your parameters in this way, I haven't told you what kappa is, but there's a choice of parameter for which kappa depends on delta 1 and delta 2. There's a choice of parameter for which the mall is stationary. And what that means for us, it means that if we look at the mall with this initial data, so B2 here, V1 here, and we tune them depending on delta 1 and delta 2, there's a choice. On delta 1 and delta 2, there's a choice for which, if we look at a downright fact, or more simply, if we look at the top here, this Bernoulli distribution of arrows, which is now no longer specifies it, comes from the dynamics, is also going to be V2, Bruno V2. And the distribution of arrows coming out here, east, is going to be V1. So this feels a lot like many of the models that we saw. For example, stationary LPP that might be. Example, stationary LPP that Martin was talking about, or the law gamma form, and so on. So, this looks like a great candidate to apply. He was like, yes. But how is it related to ASEP? That's not so clear yet. Okay, and that's what the next slide is about. It turns out that ASEP is contained in this model. Let's take delta 1 and delta 2. Remember, delta 1 and delta 2 were these probabilities that a path coming into a vertex goes straight through. Vertex goes straight through if it's coming from the bottom, and delta 2 is a probability that it goes straight through when it's coming from the left. Okay, so if we take these to be of order epsilon, so we think of epsilon as being small, and then L and R will be fixed, and we select the initial data, here's our kappa here, we select the initial data to be of this form. And we think of this model as a model of particles in the following way. We look at some level t. Look at some level t, say this is t, and we take this level t to be a half integer in height. And we look at the trace of all these upgrade paths in the following sense. We draw a particle. We say there's a particle at this location if there's an arrow coming up through this location. So P I T are the particles in my stochastics expertise model at height t. So this is at height t. So this is at high t, where do I have arrows crossing up? And then I look at an ASAP, an ASAP, you know, as I defined it before, with Bernoulli initial data of V2, so V2 is the same as here. And then these paths here have a tendency to go up into the right, so they have a tendency to go to the right. So let's subtract T here, subtract the time. What was observed Observed in a paper in 2016 by Oradine, COVID, and Vadimbir, and then proved in full detail by Amol, and used extensively later in several papers, starting with Amal and Voradine, is that if we look far out, so if we look at these locations, so remember the location of Qi here were equal to the Gi of T. So, say this is. Of t, so say this is, for example, p3 of t minus t, and we accelerate time by inverse of epsilon. And remember that the parameters of the stochastic vertex models, delta 1 and delta 2, depended on epsilon as well. Then finitely many of these particles behave with finitely many particles of ASF. Okay? With a little bit of work, dealing with the particles. With a little bit of work dealing with things that happen at infinity, we can actually prove that the height function, which I'll define in a moment actually, I shouldn't put this later, maybe I'll come back to this slide. So in some sense, let's forget about this for now. In some sense, you can find the D, or not in some sense, in this sense, you can find the ASAP inside the stochastic six-partics model. Now, we're interested in the current, which is not, you know, it's not a point yet that the You know, it's not a point here that that depends on a fixed number of what happens in a fixed set, necessarily. So to emulate the current, we will need to discuss the height function and SSC's function. But before I go on, is everything to use so far? Okay, so the main quantity of interest, the thing that corresponds to the log partition function or the passage time or all the quantities that we've heard about in this conference. About in this conference for this model is called the height function. And what it is, is we look at this, we look at the line between this bottom left corner here of the rectangle and the top right, so this should remind you the line that I had for the current. And we look at how many times paths from the stochastic vertex configuration cross this line, but counting upwards or right to left. Upwards or right to left crosses as plus 1 and left to right crosses minus 1. I'll have a picture in a second. Okay, so here's an example. You look at all these paths and you count how many of them in net cross this line of work over the entire configuration. That makes sense? Okay, so one of the results in a 2016 paper A 2016 paper by Corwin and Corwin. I should mention, I don't have this on any slides, the stochastic 6 vertex model is a special case of the classical 6 vertex model, which has a great history in mathematical physics. But this special choice of width was studied first, I think, by Gua and Chong in the 90s. And they realized that with this choice of widths, we have a certain Markovian property. Markovian property that says that you can evolve the configuration step by step. So, one way that you could do it is you can specify these boundary conditions and then you can choose the weights in a Markovian weight on the antidiagonals step by step. You can actually construct the configuration step by step. Then for mathematical physics, that means you can use transferred matrix methods. And of course, you know, a lot of mathematical physics papers are semi-rigorous, and this Guan School paper has a lot of great ideas, but Paper has a lot of great ideas, but really the rigorous, the full study of the spectral theory of this transformation started with this paper by Gordon, Corinne and Gordon. And one of the things that they showed is that if you take not necessarily this initial data that's Bernoulli, but initial data that's Riemann, so that we have a density of particles on the vertical axis and another density, which is called V2, of particles on the right axis. What I mean by this is that if you look at a long interval, you divide by the length of the interval and you count the number of particles. Life of the vector one, you count the number of particles, and it's approximately equal to v2. For such boundary conditions, they show that they found the hydrodynamic limits. So they found actually explicit functions that shift when you rescale the height function close to this function. And they also showed that, as was expected due to Launch Bohm's calculations, when you rescale H by its hydrodynamic limit and It's a hydrodynamic limit, and you divide by epsilon to the minus one-third, and you get this tracy better movement. Excuse me. A Riemann boundary condition. It's just what I said. It's like you have some density here, U2. The density varies with position. No, the density doesn't vary with position. It's a step boundary. Oh, just one step. One step. And then more general boundary conditions were investigated. General boundary conditions were investigated by Argola. Okay, and so where does our work for, where does this result that I stated for ASEP come from? It comes from these bounds. The scaling here may be a bit strange. So subject to a characteristic direction condition that I don't want to state, basically, as in all these models, you need this rectangle, you need to tune this rectangle, the aspect ratio of this rectangle, in a certain way, depending on the parameter. Certain way depending on the parameters delta 1 and delta 2, so that you see KPZ fluctuations. Otherwise, as is typical in these models, one direction will be preferred and you'll see Gaussian fluctuations instead. So we're in the moderate deviation range, I guess, for the except I didn't specify this, so you can't go all the way to the far tail. But in this range, if you look at the probability that the stationary height function in the stochastics is pretty small is bigger by u. Is bigger by u than its expectation, you get this behavior here. So you should think of, if you think in terms of n, or n in the last passage correlation, you should think of n as being y. So characteristic direction condition means that x is going to be proportional to y. Just sorry, I think I missed your point. So your your paths were going up right. Yep. And you said that you can match this to A such, but That you can map this to ASAP, but ASAP sometimes jumps left. Oh, yeah? How is that? So these paths are centered by T. Oh, I see. Okay. Does that make sense? Okay. Okay, so we have upper and lower bounds, but not, you know, these constants here don't match, but we have the correct behavior in the tail, which is consistent with this Tracy-Wittem distribution, for example. Distribution, for example, the D plus one. Everything okay? All right, so we're going to discuss some ideas in the proof, and this is the moment when I will talk about the general philosophy that I think, or my understanding of the general philosophy that Timo developed, and I call it the Sepilanen machine because it appeared in so many papers and it looks orally similar to me, but I was never able to understand how you could get so many. Get generated, so many models for which it worked. And the basic idea is, it comes from far, it comes from this work called Ballast for Sepulana. And it's basically, it's based on simultaneously estimating two quantities. And you can think of the stochastic X-rays model, but you can also think of LPT, or you can think of the lock-down polymer, which was invented so that this strategy would work. Work. And one of them is, I'll call the height function, but again, passage time, partition function, current, and so on, is also appropriate. And its derivative with respect to one of the parameters. So for example, it's derivative. Yes, sorry. Sorry, just one remark. We originated from a paradigm. Yeah, yeah, so I should say that's that's correct. Sorry. Sorry. What was I saying? Okay, so the two qualities So the two quantities are the quantity you're interested in, and then an auxiliary quantity that turns out to be very interested in solvent, which can be interpreted as the derivative with respect to, say, the parameter P and the dynamics. And this is, say, for polymers, or last passage perfolation is the first jump. It's a second-class particle in particle systems, and it's the time spent on the boundary for the column or call. Okay, and what you need for this tool. What you need for this to work is you need a stationary model. So you need something like this. Preferably, it takes place in a quadrant, but it's not 100% necessary. And I'm writing everything abstractly just to show that it is very general and to explain why, you know, that it is really a machine that you can make work in many situations once you understand what's going on. So let's imagine that you can write the height function as a sum of increments along this path. Increments along this boundary and then this boundary. Say, for example, mass massage population or the height function for the stochastic experience model where you can count the number of incoming arrows here, and subtract the number of outgoing arrows here to find the total flux. And then you rearrange this quantity and you take the variance on both sides. And you see here, if you're stationary, these variances you'll be able to compute. Be able to compute. These will be explicit. I'm not going to write them down, but typically, if you are in a model where the invariant measure is iid and this is m, then this is n, then this is going to be of order m, and this is going to be of order n. And what we want is the variance of h. So to make any progress, we basically assume that the variance of b and the variance of h are the same or comparable. They're already within n to the two-thirds of each other. These are two-thirds of each other. And the next step is to look at this remaining term. This is the term, the covariance between east and south in that T-organization. So what we can do with this is we can just do a trick where we rewrite this B here as just the derivative of an exponential. And you think of this exponential as being a change of measure that modifies the distribution inside H. So it modifies. H. So it modifies the distribution here and changes it. It tilts it by some factor in delta, but then it takes the derivative with respect to delta. In very nice cases, or in many reasonable cases, you can actually think of this change of measure as being constructed through a coupling. There's actually a coupling of these boundary random variables for different values of the parameter, and you can embed this deformation around zero into this code. So they're basically looking at the derivative. Basically looking at the derivative of h with respect to the boundary values. That's what happens in all these models. So you get this equation. The variance of what you're interested in is twice the expectation of the derivative with respect to the parameter, the dynamics. Everything okay? This is an exact version of the KKZ relation to chi is equal to z. Because over here, q will, as I Will, as I said before, Q will have the interpretation of the first jump, of the deviation from the diagonal, of the time spent on the boundary, so it will be transversal. It will be related to transversal fluctuations. Whereas the variance of h here is n to the 2 chi. So this is an exact version of 2 chi is equal to c. So if you want to solve for chi x, we need one more. For kin C, we need one more relation. Everything okay? Okay, so as Nico said, what Timo does, this is usually the part where you have to be a bit crafty, and Timo usually, in all his papers, he finds interesting couplings to basically bound the probability that Q has, you know, that you have a deviation of this derivative by board and u in terms of h. And then you try to close, you have a system of two equations. You have a system of two equations and two unknowns for the variance of age and q. But this is fine, but you know, you have to be teamo for this to work. You would like to have some general method that you know is going to work in general. But basically, the basic idea, and I think if you read his papers, you understand that this is really often what's going on, is that um q is the derivative of a convex function. And really what you're doing is you want to go back to h, right? Because h is what you can compute because sorry. Because H is what you can compute. Sorry. H is stationary. So you can compute it. Unfortunately, it's the derivative in just one of the variables. So it would be great if it were the total derivative. Then it would really be the derivative of a convex function. Then it would be basically done. But unfortunately, you have this deviation here, but thanks to the JS formula, you'll see this is not actually such a big problem. But in the early papers, it was not so obvious how to deal with this. But it turns out that in most cases, basically in all the That in most cases, basically, in all the integrable cases, there is no. What you can do is you can work and work and work and replace this h ladder, replace this partial derivative by a total derivative up to some error. And if you have the total derivative, then you're in business, because then you can replace Q by H centered, H lambda centered, and then the difference between the expectations. This is a stationary quantity, and it behaves because of curvature, like n to the Because of curvature, like n to the lambda once theta. Okay, this error I'm not going to discuss. This error comes from sending this back to theta. Sending this lambda back to theta. So in total, what do we have? We have that the variance was equal to the expectation of q, but that's bounded by 1 over lambda minus theta times the square root of the variance, right? If I do Cauchy-Schwarz on this guy, from Rougelensen on this guy on this guy, and then some error that I'm not going to talk about, but it's there. And then n times lambda minus theta squared. Then if you set lambda Lambda minus theta squared. Then if you set lambda to be n to lambda minus theta to be n to the minus one third, you get chi is bounded by one third. Okay, so that's the sepulchre. So to make it work, we need to understand how to replace in the case of the any model, how to, okay, so this is fairly general. It works. Note that this part of the derivation doesn't even really use anything. It's just totally general. Use anything. It's just totally general. And it's not, you know, Timo often emphasizes in his papers that what he's trying to do is something that has a chance of going beyond a dual model. And really, at almost every step, except for this purple step here, everything is based just on general monotonicities and convexity and the possibility of computing the invariant measure to high precision, to the point where you can basically turn it back. If you can do this, you. But if you can do this, there's a very good chance, and I'll show you some integrable model, model for which we don't know that they're integrable, that where this method actually works. But it is restricted to the stationary case. Often you can compare stationary to non-stationary, so if we do succeed in making this a fully general machine, then maybe the problem of exponents would be reduced to some sort of comparison or Google CD. Okay, so let's go back. So here this, you know, you probably weren't able to follow this magical. Probably weren't able to follow this magical step here, but maybe up to here, it was still okay. So, the problem here is that this is an equilibrium quantity where the initial data matches the dynamics, but this is not an equilibrium quantity. There's a second problem. This is a formula for the variance, and we want these exponential bounds. But Timo and Chris and Elmore have us covered. Sorry, I just can't get it, but this is down. They have us covered, they found. They have discovered, they found a remarkable formula that allows you to compute h, the moment-generating function at h, at a single point, but half-equilibrium. And this is amazing. So basically, their observation is that for any model that's stationary with product and variant measurement, it can put into this framework, if you look at the height function, the log partition function, whatever you're interested in, minus the expectation equilibrium, you get an exponential with a cubic power here. With a cubic power here. And this is where the n to the one-third fluctuations come from. And this suggests a general method that we actually implemented in many cases to solve the problem of fluctuations. You just write h, which you want to bound, as h with one of the parameters shifted because EJS allows you to, formula ratio, here allows you to compute this. Then you use convexity to bound this derivative here by the upper value in this integral, and we get a bound like this. If we subtract. Abound like this. If we subtract the expectation of H theta theta, then multiply by theta minus eta and exponentiate, we get that the quantity we're interested in, the centered fluctuations of h, exponentiated, is bounded by something that we can compute, exponentials we can compute, plus this thing. And basically, we're done provided that we can control this. And this is, again, this is something that is not fully general yet. But if you think about it a little bit, it's looking pretty good. A little bit, it's looking pretty good, right? Because Q itself can be bounded by a ratio of H's. So, okay, I don't want to go too much. You can talk to me if you want to know about the details. But it turns out that in many models, including all the non-integrable models, we know how to make this work. This is a paper with Ben Landon. And I should say that in some work in one of thesis students, some have similar upper moderate deviation balance. Moderate deviation balance than the ones we obtain occur. But one thing we can do that sort of demonstrates the generality of the model that was the method that was originated by Huneboom and Kator and Martin and Timo and many other people, is that we can do this even for some models. And we chose these interacting diffusion models that include the model, but also include many. Model, but but also include many many um many diffusions that don't seem to be integral in the sense that you probably can't write explicit formulas for their own geometric functions. And we show that you get these moderate deviations on scale n to the one-third with u to the three-half power in both directions. So the correct power in the negative direction for negative deviation should be uq, like for the trace-William distribution, but our method just Everything okay? All right. Um but so th th we're pretty far from the uh from the six vertex model here. So how do we deal with the six vertex model? What is the height function in terms of uh you know a sum of two uh yep two minutes have two minutes? Okay, so that's perfect. I have a lot of slides left, but let's skip all of them. I just want to explain to you that H can be written To you, that H can be written as a difference of the boundary values. You can look at the flux of arrows coming in here, and you can subtract the flux of arrows coming out here. That's the representation for H, that we can determine for a second. And then we need something to replace Q, the derivative with respect to the initial data, or the derivative with respect to the dynamics, the derivative with respect to this parameter, these parameters delta i, that tell us how often the paths are allowed to go through, first straight up. And those are. Straight up, and those are second-class particles, second-class paths. We can look at two contributors, just like Martin and Sebolinen, later on with many collaborators, including me and Jeremy, looked at the ASAP and looked at the behavior of second-class particles as a replacement for a derivative of the current. We can do the same thing here. We can consider second-class paths that come from looking at the difference between two couples. Difference between two couple of configurations on the boundary. So, say, for example, I put arrows, horizontal arrows here at all these black, I hit all these black sites, and then there's a second configuration that's dominated by this black configuration, and consisting of red arrows, and you obtain gray paths by taking the difference. And it turns out that you can control the behavior of increments of h in terms of random watts on these. Walks on these great paths using the method that's traveling. I guess this is the end for this talk. What's the next step? As I said, let's start with the second one. This method looks really very generic, and it seems like it should be the case that as long as you can compute the equilibrium quantities and differentiate them in the parameters, there's a very good chance that if you're crafty enough with the coupling. That if you're crafty enough with the couplings, like finding couplings that are convex, it could apply to some invariant measures that are not in product form. This is something we're thinking about. So, one example is integrable models that has-based models. They have some invariant measures that are not in product form. And then another thing that requires some new ideas is lower tail balance. So, we know the lower tail has a different exponent, e to the minus u cubed. Billand and Shashenko Kangui have a general method that works for some. Have a general method that works for some geometric models. They did it for WP and then we extended it to the Colin Euro Polymer. But if you have a model that's not so geometric, for example, these diffusions, it's not so clear how you want to write. And that's the end for today. One question. Yep. So this Yep. So this uh setting is equilibrium? Yeah, this is equilibrium. Can can you do something which is based off equilibrium? Um uh I don't want to say anything. So in some cases you can do comparisons, but it's not clear. So this is new enough that I just say it's just very important. But it's possible to do something. So for the model we understand best, you call your polymer and then these diffusions there we have some comparison principle. Some stuff you can. It's specific to the core ideas are really reliable. So, you're saying non-solvable models might be available? Yeah, you need stationary. You need to know the stationary measure also. So, what I said is you need to know the stationary measure well enough that you can tailor expand in the parameter, for example. So, first of all, what does this mean? You have to have an invariant measure that has a parameter to start with. Invariant measure that has a parameter to start with, and you can claim it and differentiate. So you need to be able to compute with this. So that's why these half-space models are conceivably a direction. A good chance that you can compute, as long as you can compute linear quantities, perhaps exponential quantities to some sufficient precision, especially like how they behave when you change your parameters, those such things. But no, I don't want to claim anything. For non-integrable models, we we've already done it. These diffusions are non-integrable, but they have product integral. Not but they have product just to try to figure out what's necessary. Why can't we do XMAC in the product? Okay, so if you have some small if you have like some jump distributions that's reasonable, then you may be able to approximate with some other vertex model. But if you tell me like an arbitrary exclusion process, then I don't know that you can find some vertex model that is reasonable and that can degenerate. Generate with. Does that make sense? Maybe we should stop here. So we have two minutes break.