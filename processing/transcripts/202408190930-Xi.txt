Okay, so this is my talk, right? It's an inverse spectral problem, and I will tell you more about that. So I really like the format of this kind of workshop because I'm working actually in harmonic analysis, which is not in the title of the workshop. Still, I have to share some of my work. Okay, so here's the table of tense. Let's just go into Let's just go into the setup of this talk. And for people who are not familiar with doing analysis on manifold, it's okay. You don't need to think about just a general manifold. You can think about the simplest case, like a torus. It's really the periodic case of the flat metric. So you can think about the torus. And we allow the manifold to have a boundary. It can be empty. In many of our results, we. In many of our results, we need it to be empty, but it's not essential in any sense. Okay, we'll consider the Laplacian operator associated to the Riemannian metric. Okay, then we have this standard spectral theorem. We have a discrete series because the manifold is compact of eigenvalues and the sequence of corresponding eigenfunctions. We will choose also a normal basis. We call them E1, E2, and others. If there is a boundary, we also need Okay, if there's a boundary, we also need a Dirichlet or Neumann boundary condition imposed on the eigenfunctions. And they satisfy this eigenfunction equation, eigenvalue equation. I want to note one thing is that unlike geometers, okay, so our minus lambda will be a positive operator. So our eigenvalues will be minus lambda squared. And there's a score here, so our eigenvalues are actually the frequency of the eigenfunction of the energy. Okay? function of the energy. Okay? So the simplest case, our torus will be, eigenfunction will look at something like this, right? n dot x. So n are integers. x are just the coordinates on the torus. And this is a dot product. Okay, so it's really plain wave. And your eigenvalue here, you can compute that, right? You can just. Compute that, right? You can just put the abla fashion, which is just the flat line fashion, onto here. You get minus n norm squared. So the absolute value of n, the size of n, is actually the frequency which is expected. Okay. So there are many problems concerning the eigenvalues or eigenfunctions of the Laplacian. And the first ever question posed was this kind of how does this eigenvalue distribute, right? observations from physical experiments and they made conjectures about how the eigenvalues should distribute this was considered a very hard problem at the time by Hilbert but one year later Weil himself which is a still which was a student of Hilbert actually proved the Weill law which is the distribution The distribution of the eigenvalues here. So you can see that just from the eigenvalues, you can read off some geometry information of the manifold. In this case, the simplest case, you can see the highest order term has a coefficient, which is the volume of the manifold. This constant here, C D, is some absolute constant which has nothing to do with the manifold. Okay, so what's the inverse spectral problem? Let's look at the simplest case, right? The simplest case, right? Let's consider a manifold which is just an integral. So there will be a boundary, right? So two points are the boundary. So we impose the directional boundary condition. This is actually a very simple model for the vibration of a string. This is exactly how you solve a wave equation on a string. You use this kind of model. But in this case, we're in 1D. The Laplacian is the simplest. If it's of the simplest form, right? It's this derivative operator twice, right? Operator twice. So you take x-derivative twice. And we can actually solve the eigenvalue problem, solve for eigenfunctions explicitly. It will just be sine functions. And you can see that the frequency of your sine functions, which are the eigenfunctions, are determined by the length of the string, which is A here. Okay. And of course, we can also write down what's the eigenvalues, which are just pi times the integer over the length. times the integer over the length of your string. And we can also write down what's n lambda, which is a distribution of your eigenvalues counting multiplicity. But in this case, there's no multiplicity, it's 1D, everything is simple. Okay, we can also write, we can also just draw the profile of each eigenfunction like here. This will be the vibration modes of your string. And very simple, but also we actually use a lot in our real life inverse spectral problem: is that Inverse spectral problem is that if you can hear someone is playing a string, it can be from a piano, right? It just needs to be a string. Can you tell just from the sounds which string he's playing? Or in the mathematical words, can you tell the geometry of the string? But assuming the same material and the same thickness, the vibration of the string is completely determined, the geometry of the string is completely determined by its length. But you can see that from n lambda, But you can see that from n lambda, which are the frequencies you can hear from the vibration of the sounds of string, okay, it determines A. And vice versa, A also determines a number. So this is actually, even though it's very simple, it's actually a real life thing, right? People who are gifted has this kind of ability, which is called perfect pitch. So they can hear what you are playing on piano, they can write down the score, right? That's exactly what he's doing. That's exactly what he's doing here. It's just the Fourier transform machine in your brain that's doing it, so you don't know how it's functioning. That's exactly what happens. So there's a famous question by Keck in the 60s, which basically generalized this phenomenon, but in just two higher-dimensional case. So instead of of uh uh instrument which is one dimensional, can we do the same thing for two dimensional instrument which vibrates, right? That's a drop. Right, that's a drum, okay. So, basically, next question was: if you have two planar domains, of course, and always the duration boundary condition, because it's a drum, right? We want to fix the boundary. If they have the same wild counting function, that means that they have the same spectrum, okay, must they be the same geometric object, must they be isometric. Okay. All right, so, but the string case is not the only inspiration for CAC to process. Inspiration for Keck to pose this question. There are something more, right? So we want to look into that. We can actually pull out more geometry invariant for a lambda outside from the volume of the manifold. So let's look at this equation here. So basically, we're doing the Laplace transform for the spectral measure. Then by some spectrum theory, you can write this out. This is just some expansion of x. It's just some expansion of value functions. If you write this out, this will be the trace of the heat kernel. Okay, this is just by pure computation, there's nothing fancy here. But for the trace of the heat kernel, we actually have a classical expansion near time equals zero. And the coefficient of the first term is the area, the volume of our manifold, and the next term is the length of the parameter, the circumference of your domain. Your domain. So not only can you hear the area, the volume of your manifold, you can also hear its circumference. So you know the size of your domain, you know the size of the boundary, okay, just from a lambda. You can see that everything here is determined by the spectrum. Okay, then one very interesting thing then happened, right? We can hear the area and the length of your manifold and its boundary. Okay, but there are certain domains which are very, very Domains which are very, very special in planar domains, which is a disk, right? A disk is the maximizer of the isoparametric inequality. So if you are drumming a disk, a disk-shaped drum, you can collect from the frequency of the vibration of the drum to find the area of the drum and the length of its boundary. Then you can decide that you are really drumming a disc. Okay, because that's the only shape that can achieve the maximum and the equality in the SO parametric inequality. Okay, so that's the observation made in Keck's paper. And that's why he proposed this question, because there is a possible result, right? This actually works. But in general, this kind of problem has a very long history and it's kind of mixed. The answer to it really depends on the context. Depends on the context. So there's a very obvious question: why Keck only asked for the planet domains, right? Why didn't Keck just ask for general Romanian manifolds with all results boundary? The reason is that at the time when Keck proposed this, it's well known, actually, it's known to him at least. MÃ¼ller in sixty four actually found a six a pair of sixteen uh dimensional tori which share the same spectrum, but they are not the same geometry object. But they are not the same geometry object. So, in general, this question doesn't hold for general Riemannian manifolds, but it's still expected to hold in the two-dimensional case until 92, right? Golden Warburt actually found this two pair of planar domains, which share the same area and the same circumference, same parameter. But they are not the same geometric object. You can clearly see it from the picture, they have the same spectrum. So, to this So, to this degree of general reality, this problem is actually fully settled. But you can also see that these kind of two drums don't have smooth boundary. So, there are still something we can think about. And more examples of this kind was proposed by many different authors. I'm putting this work here because I'll later refer to it again. So, in other direction, there are also some positive results, right? Some positive results, right? First positive result is disk is okay, disk is factually unique. And then there are many partial results from many different groups of people, which requires further assumptions on the class of domains. So usually they will assume the boundary to be analytic, not only smooth, but analytic. They also need some symmetries from the domain. Okay, but the only non-restrictive relationship. Non-restrictive results other than the disk was done by Hizari and Zaldich two years ago in also a math paper. They proved that if you have an ellipse which is very close to being a disk, it's quantitatively close, you can look at its eccentricity, eccentricity must be very smoo uh small, then they are spectrally rigid. So if you drama ellipse of this kind, you can hear the shape. Okay, among all smooth stone. Okay, among all smooth story. So this is the very first, this kind of result outside the web disk. Okay. Now let's talk about our problem, which is also about drumming a drum, right? So how do we think about this? Let's really look at what do we really hear, right? When we are actually drumming some point on a drum. But this is very classical, right? We are just solving a wave equation. So we are solving a wave equation. So we're solving a wave equation on the manifold by striking the drum at some point x. Okay? So our initial condition will involve something like a delta x there. Whether you put it at the dt initial condition or just u0, it doesn't really matter. But after some argument with my collaborator, we think we should put it here. It's an initial movement to the drum. Okay? But for the argument, it doesn't really matter. Then we can actually solve this. Okay, then we can actually solve this wave equation just by Fourier multipliers. But in this case, the Fourier multipliers will be coming from the eigenfunctions. Okay, you can very easily check that this is solved the wave equation. If you take the t derivative twice, it hits on this sine function. You get it back, you get minus lambda j squared. If you take the Laplacian, it hits this EJ Y here. You still get minus lambda j squared. So it's actually solved the wave equation. So, it's actually solved the wave equation. You just need to check that it has satisfied this initial condition, but I'm not going to do it here. Okay? All right. So, one thing to be careful about here is that our x is the point where we strike the strong. So, x is fixed. Y is the variable of your solution. Then we separate this UTY into different frequencies. So, what we really hear, what we hear is that, so Here, what we hear is that, so this is a composition, superposition of different wave frequencies. What we hear is for each frequency lambda, the energy of this function, right? That's corresponding to the volume of a certain frequency formula of vibration. So, we fix lambda j equals lambda, we call it u lambda, that's the mode which is vibrating at frequency lambda. Then we can calculate its energy. Then we can calculate its energy. It's very classical. We have kinetic energy and potential energy. We put them together. You get Ejx squared, norm squared, at lambj equals to lambda. That's exactly what you get. If you cannot imagine it, you can look at this expression, right? Basically, taking the error, this gives you cosine squared and sine squared, combine them, it gives you one. That's what you get. Which is kind of expected. While dropping at a point x, the energy should be. X, the energy should be depending on X. But this is exactly how it depends on X. Okay, so what we hear is actually a very classical object. What we hear is the point-wise wild counting function. Okay, if you can recall, the n lambda was defined to be counting the eigenvalues up to the number lambda. Okay, so we're putting a one here. But in this case, it's a local. But in this case, it's a local version. Instead of putting 1 there, we put Ejx squared there. And this is also very natural in the sense that Ej as an eigenfunction is an auto-normal basis. So Ej norm square, if you integrate it over the whole manifold, you get one. So this is actually a probability distribution. This is the total probability. So this is really a local version of it. And the study of n x lambda actually goes back to. The study of NX lambda actually goes back further. So, actually, nowadays, when people prove the while law, they prove a while law for NX lambda. Then they integrate over the manifold. So, the study of this, this is not what we define, it's a very classical object. Okay. So, the next question was basically from n lambda, can you ask an inverse spectral problem? So, we think maybe for n x lambda we can still ask something similar. So, Something similar. So, but in this case, there are two unknowns, right? From NX, you don't know the shape of your M. Also, you don't know the location of X. Apparently, if you do know both of them, it's very hard to solve. And actually, we have counterexamples. You have to know at least one of them. We start with the easy one. So, we assume that we know the shape of M. We ask where is X. Okay. So, basically, if we know the geometry of a drum, and we drum it at some point X, which is unknown. Drum is at some point x, which is unknown. Can we determine x by listening to the vibration of the drum? Okay, but apparently, there are something we need to take into consideration before stating the problem. We need to consider the symmetry of the drum. For example, if you have a circular drum, okay, we are drumming on the top of it and on the bottom of it, same distance from the boundary, the vibration will apparently be the same because it's symmetric. So we can only So we can only, if we can hope for, determine the location of x up to symmetry. So the question for us mathematically is that given a manifold m and two points on m, if the n x and y are the same, so we hear the same when we draw met x and y, okay, must there be an isometry from m to m itself taking x to y? So they are essentially the same point. Okay, geometrically. Let me introduce some terminology that I will use later. We see that two points sound the same. We then call them co-spectral, so they share the same spectral information. If two points are the same under an isometry, we say that they are similar. Those are actually terminologies taken from graph theory, which we'll evolve later. Okay, so our question basically becomes: being cospectral, does that imply they are similar as well? Okay, it's a very simple question. Okay, it's a very simple simple question. Okay, let's look at some simple examples. Let me try to convince you this is actually makes sense in some cases. So let's look at the simplest case, which is a string. This is the simplest thing that we can actually calculate. If this doesn't work, the question doesn't really matter in any sense, right? Let's take a string. We know how its eigenvalue behaves. We also know how the next behaves. Because in this case, Because in this case, our index is just the sum of ej x squared norm squared, right? But on the string, the spectrum is simple. So we are really hearing the norm squared of each eigenfunction. That's what we get, right? So we are able to recover each sine squared of pi. This is actually the very first eigenfunction. So the j is equal to 1 is pi x over a. But this will be enough, actually. So this is the normal. So, this is the norm squared of the first eigenfunction. Why is it enough? Because for the string, it has a left-to-right symmetry. So, we only need to determine, distinguish points on one half of your string. But you can see that the first eigenfunction, norm squared, is actually monotone on one half of your string. Of course, it's injective, it tells you where you are. And more interestingly, in this case, we do not even need to know beforehand the geometry of our manifold, which is Of our manifold, which is just the length of the string. Because you can find it by looking at the first eigenvalue, which is the first jump of our nx function. So in this case, we do not need to know what is beforehand. But that's probably the only case we can do that. Okay. Let's look at something a little bit harder. Let's look at a drum, right? Let's look at a rectangle. And we assume that it's strictly a rectangle, which is not square by assuming the silence is this. By assuming the silence is different, I raise the square, the symmetry changes, but it still works. I just don't have time to show you that case. Okay? In this case, you can also solve for the eigenfunctions. It's just a separation of variables. It's really solving two strings, right? You have the multiplication of two sine functions. And the eigenvalues are the square root of the norm squares of the two respective eigenvalues. And in this case, there is already some potential obstruction to our problem. Obstruction to our problem. Because our agromatics can have multiplicity. You can have different pairs of J and K which gives you the same lambda. Okay, that means that you will not be able to hear the norm square of each individual eigenfunctions, but the sum of them. But luckily for us, when a is less than 1, at least the first two eigenvalues, which w is j k equal to one, one and one two, they are both simple. Okay. It's very easy to find actually because It's very easy to find actually because a is now equal to one, right? You increase j okay by one, you look at which one is smaller. That one is your second eigenvalue, and it has to be having no multiplicity. Okay, so they're both simple. That means that the first and the second eigenfunction squared must be audible. Okay, because there's nothing else contributing to them. And you can see that there's a very simple and interesting structure. So you have sine square x, sine square y, sine square x, sine square two. sine square x, sine square 2, 2y. So they have lots of common factors. They are both audible. Their quotient is also audible. If you divide them, you get cosine squared y. And cosine squared y is essentially the same information as sine squared y. That tells you the location of your y coordinate up to this up-down symmetry. Okay, that determines sine square y. But since you know sine square y, you also know the product of sine square x, sine square y. Of sine score x, sine score y, you also get sine score x, which determines x up to this left-to-right change. Okay, so this case is good. Our eye co-location holds here. Okay, but before we go into more examples, I want to talk about this negative example and remark. So, remember this paper I mentioned earlier in constructing negative examples to text question? So, Kanye and collaborators. COME and collaborators actually found this pair of domains which are not isometric to each other. But furthermore, they show that at these two red dots on the domain, if you drum there, okay, if you drum there, you will hear exactly the same sound. So all the active functions excited will be up to the same degree, hear exactly the same. So if we take these two domains, the destroying union of these two domains as one single manifold. Two domains as one single manifold, then our echolocation will fail here. Because you get a pair of points which are not symmetric to each other, but they produce the same sound when you're drumming them. Okay, so this tells us that we need to impose a further requirement, which is for the drum to be connected. If it's disconnected, we have this kind of example. Later on, when I talk about more physical interpretations of this problem, I think you think this connectedness are some I think you think this connectedness assumption is very natural. We need it. Okay, so I think two more examples. Before that, let's introduce a notion, which is actually used this already, right? So we say that some quantity is audible if it's decided by the function index. So any information we can pull from the function index will be audible because index is audible. It's just the name. And one very interesting fact is that. And one very interesting fact is that we can, just like the n lambda function, the spectral measure, we did a Laplace transform for it, remember, right? We can also do Laplace transform for a foot nx instead of n lambda. And the difference between nx and n lambda is just that they are off by an integration over the manifold. So for n lambda, if you do Laplace transform, you get heat trace. For NX, if you do Laplace transform, you get the heat kernel on the diagonal. Get the heat kernel on the diagonal. So if you integrate on both sides in X, you get H again. But for the local expression of the heat kernel, we also get an asymptotics, which is also classical, can be found in the book by Li and Yao, for example. Okay? Then the first term is just the Euclidean term, so it's one. All the other terms will be the curvature correction terms. In particular, the second term will have a coefficient, which is the scale. Term we have a coefficient which is the scalar curvature at the point. If you're not familiar with Riemannian geometry, it's okay. In 2D, in the surface case, this is just twice the Gaussian curvature of your surface. So the Gaussian curvature of a surface is audible. It's determined by mnaps at the point M. Okay, then we can already use this to solve some cases. I'll just talk about the first case. The other one is easier, but this actually requires some blackboard work. I'm not going to do that. In this case, this is a tool. That. In this case, this is a torus, but this is not a flat torus. It's a surface of revolutions. It's the torus we can smoothly embed into R3. Okay? Okay, let's think about this. So of course, this has lots of symmetry. This has this rotational symmetry. So you only need to distinguish point from one singular circle here. That'll be enough. Well, more than that, they also have this top-down symmetry, right? So you only need to distinguish a point from the upper half. Distinguished point from the upper half of your circle. It's a section, right? Okay? But if you look at the Gaussian curvature on the upper half of the circle, it actually can be computed just by staring at the graph. It has two principal curvature, one is along the direction of the circle, which is constant. The other one is perpendicular to this circle, which will have radius, curvature radius smallest here and at the maximum here. So the Gaussian curvature, as long as So, the Gaussian curvature, as long as the second principal curvature, will be a multiple function from here to here. Okay, so then you can distinguish all the points you want. Similarly, we can do it here for the spheroid, which I'm not going to compute. Okay, let's do a quick summary of what we know and we don't know about examples. So, basically, let me explain something, right? This superscript DNN just stands for the duration and Neumann boundary condition. Apparently, And boundary condition. Apparently, for some examples, the calculations are actually quite different. So we separate them. Okay, so we know square and rectangles, we did before. We know disks. I didn't show you, but we know how to do it. We know flat client bottles, which is actually not trivial, but we can do it by computation by hand. Spheroids and torial revolution is from the previous slide. And there are much more that we don't know. And some of them we didn't try, but some of them we tried, it didn't work. It didn't work. So, for example, higher-dimensional rectangular boxes, we can do some special cases. For example, a cube, right? All the silence are the same. Or when all the silence are different, we can do them. But for the intermediate cases, it seems very hard because of the multiplicity, actually. And for triangles, we don't know how to do it. We don't know how to write down eigenfunction basis for a general triangle, right? If I write triangle, maybe it's doable. And for planar ellipses, And for planar ellipses, for traxo ellipsoid, for hyperbolic surfaces, I mean compact hyperbolic surfaces, we don't know how to do them. But we think all of them are very good examples to consider, especially for the last case, because the compact hyperbolic surfaces basically can give you any topology. So it's a very general case, actually. And some cases are trivial. The sphere, projective sphere, and the torus are trivial. Just because every point on them are the same, right? So on the sphere, if you draw them at two different points, you must hear the same. On that two different points, you must hear the same thing because the isometric group acts transitively on them. Okay. Now let's talk about some more physical interpretations. Okay, so let me first say that this problem actually has five equivalent physical interpretations. I will introduce to you three more of them, which I like the most. So the first one is actually what we found initially when we are writing our first paper. Can you hear your location on manifold? That's why we call it. Your location on our manifold. That's why we call this eye colour location. So it's equivalent to this physical problem. So let's think about this room as our manifold. We know exactly the structure, we know the geometry of the room. And at some random point, I'm closing my eye and I'm snapping my finger. And I listen to the echoes and reverberations of the sound made by the snap. Can I tell where I'm standing? That's actually a very real-life problem. Some blind people actually are able to, in some sense, do it in their room. Do it in their room. They are very familiar with. Okay? So let me show you briefly why this is the case mathematically. So just like before, we did the Laplace transform for next. We can also do Fourier transform. In this case, let's just do the cosine transform, which is a real powerhouse of the Fourier transform. And as you expected, this will give you the wave kernel on the diagonal. Just like the Laplace transform form gives you. Like the Laplace Renform gives you the heat kernel on the diagonal. But let's interpret this wave kernel on the diagonal furthermore, right? You are basically solving a wave equation with initial value delta x at time zero. What does this mean? This means that at time zero, you are snapping our finger at point x, and what are we collecting? We're collecting for all time t at the same point x, what do we hear resulting from that snap. And we're asking where it's at. And we're asking where it's at. So that literally translates into echolocation. Okay? All right. So we can do wave equation. Of course, we can also do heat equation and Schrodinger equation. That's why we have so many interpretations. And why do we have five, not six? It's because for Schrodinger and the wave equations, we're using Fourier transform. There's a L2 invariant from we can get interpretations from both sides. But for heat kernel, we can only get But for heat kernel, we can only get one interpretation. Okay, which is actually quite similar. So in this case, we still need to stand on the manifold, just like the echolocation problem. On the drawing problem, we are actually outside the manifold. It's quite different, actually. So in this case, instead of slapping our fingers at a random point x, I just release a Brownian motion particle. And I do it many times, of course. I record its return time to the point x to find the probability distribution. To find the probability distribution near the point X. Okay, why do I do that? That's exactly because if you write down, there's a very nice and classical interpretation of the heat kernel, which is that P T XY just tells you at time zero, you release a Riemannian, a Brownian, I'm sorry, Brownian particle at the point X. What's the probability density of its returning to the point Y at time t? The point y at time t. So if you do it with txx, it's just releasing a particle at time zero at x and observe it will come back to x. But you need to repeat the experiment. It's not a deterministic thing. It's a probabilistic thing. Okay. And this I want to introduce this uh this thing just because it also ex extends naturally to graphs, which I'll talk about later. It actually extends to many settings. The only one we looked is about the finite graphs. One way look is about the finite graphs. Okay, the last one, which also I like a lot. So instead of releasing a Brownian particle, this time we release a quantum particle. So let's imagine that we have a hydrogen atom here, release an electron near the atom core. Okay? Then what will happen? This time we're outside the atom, right? We're looking at it. Okay? Then this electron will immediately become a superposition. Become a superposition of different energy states. And when we make observation, it will actually become collapse into one state. But it's not deterministic. It has a probability to collapse into any energy state. You can actually do this experiment. Then you can actually write down what's the probability distribution function by repeating this experiment. And it turns out that that probability density is determined by NX. Density is determined by NX and vice versa. So you can do this experiment repeatedly to recover NX, and we can still ask the same question: can we recover from that? What is X? Okay. All right, now let's talk about some results. All we did before are examples, so let's look at something which requires proof. This is the very first result that we did when we introduced this question. So if we have a smooth compact manifold, in this case, for some techniques, In this case, for some technical reason, we don't want a boundary, okay, with dimensions greater than or equal to 2. Then, for a large class of remaining metric, it's large innocence of the bare category. Okay, so it's a residual class, if you know the term. Okay, then we can determine x and y by looking at the spectral information of x and y. And it's actually better than what we expected, because in this case, the index function will be injective. That means that they are cospectal if and only if they are the same. Cross-factor if and only if they are the same or similar. Which is also kind of expected after we finish the proof, because the symmetry of a manifold is actually very fragile. If you perturb the metric a little bit, it destroys all symmetry. So this is kind of expected. So this tells us that the proof basically follows from if we have two cospectral points which are not different, we can perturb near x, and use the spectral information coming from the perturbation to. Information coming from the perturbation to tell between x and y. That's exactly the proof, but it's much longer than that. That's the idea. All right. The next result we have is from the other extreme. So the first result is from a very disorganized metric. This one we look at extremely organized metrics. So we ask if we know that on a manifold, in this case a surface, if all points are cospectral, so if you draw them at any Co-spectrum. So if you draw them at any point, you hear the same. Must those be the manifolds that we expect? Basically, a sphere orators. Can other manifolds produce as a sound? The answer is yes, right? And the proof is actually also, this is in fact very simple, so I can actually tell you the proof. So if your index is constant, okay, that means your Gaussian curvature must be constant. Because Gaussian curvature is determined by index. So you actually have a class. So, you actually have a class of surfaces which are which have a constant curvature. And we only have a finite number of them. So, either your sphere, projective sphere, flat tori, flat climb bottle, or compact hyperbolic surfaces. In those, we just need to rule out the flat climb bottle and the compact hyperbolic quotient. Because they are their asymmetry group is not transitive. And we did rule out them and fin to finish this uh theorem. To finish this theorem. Okay. This is a very recent work, right? So we are thinking about this a lot. From our question to Keck's question, is there a bridge that can bridge them? It turns out, yes. So basically the question is that if we know some information about nx, of course, in this case for a lot of x, can we determine the shape of that? And the extreme case is that we know nx for all x. Can we say something about n? We say something about M. And recall that NX corresponds to exactly the acoustic deformation when we are drumming the drum at the point X. It's like when we actually try to pick a watermelon, right? You punch the watermelon at different points. You want to know the geometry, right? It's not a very good metaphor because watermelon is three-dimensional. You can only punch on the boundary. But still. Okay. So basically, the question becomes, can we hear the shape of a drum? Becomes: Can we hear the shape of a drum if we are allowed to knock everywhere on the drum and listen to the sound it produces? So that's the mathematical problem. If we given a compact manifold with an unknown metric, if we x lambda is known to us for all point x, can we determine the metric g up to assumption? It turns out that the answer is actually yes for a very large class of manifolds. And we can prove that. But we need to assume a very strong condition. A very strong condition if the spectrum is simple. But the spectrum being a simple condition is still generic. So for most manifolds, it will actually hold. So what we proved is that if you have a compact smooth manifold, possibly with a smooth boundary of any dimension, and G1 and G2 are metrics on them, if they give you the same nx, the same point-wise wide configuration at each point, then the two metrics must be the same. Okay. Okay. Uh in fact uh this is an existence uh theorem, but in fact we are we are actually putting it constructively. So we can we're actually able to re recover the metric given us the information of NX. So we give a three-step procedure to recover local expression of G given NX at any point X on your manifold. It might seem that this spectrum being simple is too much restrictive of a condition, but it turns out that if we drop this condition, we can find a pair Condition, we can find a pair of non-isometric flat tori with identical analysis. And of course, their spectrum cannot be simple. So, this condition is actually sharp in that sense. But also, very interesting that we actually are borrowing examples from the research of Keck's problem, the flat tori, but this kind of example only appears in four-dimensional or higher. So, it could be the case that we can actually do it without the simplicity. That we can actually do it without the simplicity condition for into the and 3D. Well, we don't know. Okay, last part of my, I'm out of time. Last part of my talk, which is about finite graphs. So like I told you before, there are five equivalent physical interpretations of our problem, which gives us at least actually five equivalent mathematical interpretations of our problem. As long as there's one quantity, you want to look at it, you can interpret it. It, you can interpret it in your setting, you can ask that question. In the graph case, you can look at nx itself, which is now the competent norm squared of your eigenvectors, or you can look at the Brownian motion on a graph, which is actually random walks. Okay. So that's why we define this co-spectral and similarity notion and terminology. We actually borrowed them from the We actually borrowed them from the graph series. If two vertices sound the same, in the graph case, the simplest thing to think about is if you have a graph, you start with a vertex, you release a particle doing random walks, then there will be a returning probability distribution to that point at certain steps. If that distribution is the same for two vertices, they sound the same. So basically, it's a random walk version. Basically, it's the random walk version of the Brownian motion. So, in this case, they are called cospectral. That's the terminology the graph series really use. If there is a graph automorphism maps one maps one vertex to the other, they are called similar. But it turns out that a little more than 50 years ago, the graph series already know that the cross-spectality doesn't imply similarity. And because we are interested in this problem, we actually build a program. This problem, we actually built a program to find the smallest graph which gave us this condition, which is a graph like this. So let me, since I have time, let me briefly explain. So this V1, V7, V8, of course, they are symmetric to each other. That's very obvious. And also the vertices on the outside, they are symmetric to each other. But it turns out that, for example, this V1 and V2, they are actually cospectral. So the random walk with starting point V1. So, the random walks with starting point V1 or V2 have the same returning probability distribution. In that case, we're really only counting walks of a certain length. It's very easy to see that V1 and V2 are not similar, because V1 is adjacent to two pentagons, but V2 is only adjacent to one pentagon. So, apparently, they are not similar. But if you notice, since V1 is adjacent to V2, If you notice this, since VY is adjacent to two pendagons, it actually has four different walks of length five. So alongside each pentagon, clockwise and counterclockwise. But V2 only has one of these walks. So how can they be cospectal? So actually, so V1, you start with V2, you can go along this pending, you can also go to V5 and go around this triangle and come back. That's the lens 5. So it turns out that in the So it turns out that in the finite setting, there are many possible ways to have obstructions to our problem. Okay, but that's just examples, right? We cannot write a paper just with examples. So we really want to prove something. Can we prove something positive? Okay, so we look more into the problem. We want to reproduce what we did on the surfaces. We want to assume that all vertices to be cospectral. So all the points sound the same. Must we didn't have some structure of the graph. Have some structure of the graph. That's the question. And there's also a terminology for that already in the graph theory. The graph will be called work regular. The work will be having the same probability, returning probability distribution starting at the ending point. So it's called walk regular. Okay? If our vertices, then we want to show that our vertices must be similar. And in that case, the graph is called vertex transitive, because the automorphism group acts transitively on Acts transitively on the set of vertices. So our question becomes: if we have a walk-werograph, must it also be vertex-transitive? We were really hopeful for that. But it turns out, answer is still no. We use the same program that we use to find the minimum example of such a graph. So the red dots will, all points will be cospectral. They all sound the same. But the red dots will be on one orbit of your One orbit of your automorphism group, the blue dots will be on the other orbit. So they are not the same. But also, luckily for us, there are some questions. Yeah, yeah, it must be regular. Because you can look at, it's a very good question. You can look at the walks of last two. It tells you the degree. So if you have a walk-regular graph, it must be regular. But we don't know anything further. But we don't know anything further than that at this point. Yeah, that's a very good question. So, this must be a regular graph. What regular is stronger than being white regular? Okay, let's come back here. But we also make a very good, very important observation. Just like many authors working on Kex problem observed that the original counterexample has no smooth boundary. In our case, we actually have a bunch of this kind of We actually have a bunch of this kind of counterexample. We think there are infinitely many of them. We're not sure, but we think there are infinite many of them. And all of them are not planar. They're not planar graphs. Planarness is actually a very natural condition to impose on graphs because we want to mimic the behavior of the same problem on manifolds, right? So you really want it to be at least locally planned. Okay. They're not planar. So we want to impose a further condition, which is the planarness to the graph. Can we say something? To the graph, can we say something about the structure of the graph with the conditional work regular? It turns out that we can. So, all three connected planar finite graph, the three connected is a connectedness assumption which are used, commonly used on planet graphs. Otherwise, you'll get some degenerate case. So, we also impose this condition. We can show that they must all be vertex transitive. Actually, we can classify all of them. There are only beyond the Beyond the, how do I say, the degenerate ones, there are only the nets of Archimedean solids. Okay, so there are a certain number of them. But there are a few more classes which has infinite numbers, but they are trivial ones. So they are exactly the same class of graphs which are vertex transit. Okay. Some final remarks. So there are many questions to consider further for this question, of course. Consider further for this question, of course. And the very first question is that: can we do echolocations on more class of manifolds? The ideal case will be that we can solve it for all manifolds, right? We can classify all manifolds that echolocation holds, the other ones which doesn't hold. Initially, we thought this problem should, this question should hold on all manifolds. That would be our conjecture. But now that we have found this counterexample on the graph, and also the disconnected counterexample from The uh discontucted counterexample from Conway's work. We're not so sure this time. Oh, by the way, I forgot to mention that if you think about the echo location prob problem of the slapping your fingers, listen to the echoes, it's very natural that you want your manifold to be conducted. You don't want to do it in two separate rooms. You're only doing it in one room. It doesn't really make sense. So we want it to be conducted. And one very important class that we want to consider, or maybe you can consider, as the class of compact hyperbolic surfaces. The class of compact hyperbolic surfaces that it has a very rich geometry and topology. But we just don't know how to do it. But we do have simple linemas which are known to experts that on a company hyperbolic surface, your nx, so the sound produced by a point x, tells you the length of all geodesic loops, starting at x and close at x, uh, including multiplicity. So if you have two loops of length three, you get two. Of lens three, you get two. So you know all that thing. But still, we cannot know for sure whether that tells us the location of x or not. But we already know pretty much a lot from the geometry. Okay. And conversely, can we have produced any negative samples which are connected by nickels? We don't know that either. And for the very recent work, hearing the shape of a drum by knocking around. The shape of a drum by knocking around. Can we do it in 2D or 3D without assuming the spectrum is simple? But apparently, there are many obstructions to that as well, which we talked about in our paper. I'm not going to spend time on that here. And the last part, can we say more things about finite graphs? We only deal with the planar graph case. Can we say more positive things about them? Okay, that's the note of my talk. Thank you. 