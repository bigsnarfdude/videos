Thank you for the introduction and also thank you for the invitation. I'm very excited to present at this workshop. I'm gonna talk about a software project called GeomStats, which is a Python package for Riemannian geometry and statistics and machine learning. Now, Geomets is a collaborative project, so you can see the names of So, you can see the names of the main collaborators on the screen. It's also an open source project, and at the bottom of the screen, you can see the link to the GitHub repository of the project if you want to try it out yourself. But so, first, why did we decide to code GMStats? So, the goal of GMStats is to compute with data on manifolds and with it to Manifolds and with it to be able to generalize statistics, machine learning, and deep learning to data and manifolds. So, the question is: what are data and manifolds and what are manifolds? So, in the illustration on the screen, you can see the distinction between data that belong to a vector space and data that belong to a manifold. So, you can see that for data on a vector space, the vector space here is a 2D plane, it's a flat vector space. So, the vector space is. Space. So the vector space is a data space that is flat. And on the manifold, on the other hand, the data space is allowed to be curved. So it's a generalization of a vector space that is allowed to be curved. And we have real data that belong to such manifolds. The easiest example of it is geographic data, such as coordinates of cities or any events that happen on Earth, such as earthquakes. And as an example, And as an example on the right of the figure, you can see real data on the sphere, which are in this case the coordinates of CDs. So on the next slides, I'm going to introduce examples of more complicated manifolds and data on these more complicated manifolds. And I want to point out that this also serves as an illustration of the module data sets from GMStats, because all of the data sets. Because all of the data sets that I'm going to show are readily available because they are open source data sets, so you can access them very easily through the GMStats package. And if you want a full description on how to access them, in the notebook folder of the GMStats repository on GitHub, you have a notebook called Data on Manifold that shows you how to access the data that I'm going to present on the next slides. Next slide. So, more examples of data on manifolds. First, Lie groups of continuous transformations are examples of manifolds. So, any data on Lie groups will be data on manifolds. So, on the screen, you can see an illustration of poses of objects that are elements of the Lie group SO3, so the Lie group of 3D rotations, or SC3, which is the Lie group of 3D. Which is a legoup of 3D rotations and translations. At the bottom of the screen, you can see how you can access this toy dataset from GMSTATS datasets module in one line. And so what this dataset gives you is pairs of images. For example, here the image of a bed to the image of a bed with corresponding label, which is in this case an element of the Lie group SO3, which is a 3D rotation. So three, which is a 3D rotation represented as a frame in 3D. So on this illustration, you have two images, and on the right, the two 3D rotations that correspond to the two images that correspond to the pose of the beds in these two images. So Li groups of contiguous transformations are examples of manifolds. Another example of manifolds are quotients of manifolds or vector spaces by Manifolds or vector spaces by the action of a Lie group. And this is very interesting when we consider shape data. Should it be shape of landmarks, so a set of points, shapes of curves or shapes of surfaces? Because what is a shape? For example, the shape of five landmarks, five points in 3D. It's the coordinates of these five landmarks in 3D quotiented by all the possible rotations, 3D rotations. rotations, 3D rotations, and all the possible 3D translation of these landmarks, because 3D rotations and 3D translation do not change the shapes of the landmarks coordinates. And so when we remove the action of the rotations or the translations, we get a question space. And question spaces are example of manifolds. And so data that are shapes are example of data on manifolds. So on the bottom of the screen, you can see two lines of codes. You can see two lines of codes that show you how to load two data sets of shapes. The first one is a data set of optical nerves, and the second one is a data set of cancer cells. So for the data set of optical nerves, this is the data set represented on the left of this illustration. This is a data set that comes with images of normal eyes and eyes with glaucoma on which On which five landmarks or five points have been placed. And these five points are placed specifically on the optical nerve. So that when you plot the data set, so this is on the bottom left, you can see five landmarks that represent the shape represented as landmarks of the optical nerve. And on the right of the illustration, you see the data set of cancer cells that are now represented by curves in By curves in 2D. And you see on the bottom of the illustration the type of data that you can compute with. More examples of manifolds include hyperbolic spaces. Hyperbolic spaces are very interesting for computation on graphs, for computation of networks, for example, social networks. So on the bottom of the screen, you see how you can load one data set of a social network called the category. Network called the karate graph, also called the karate club network, which is the network of interactions of social interaction between the members of a karate club. And so this is a network or a graph, and it can be conveniently embedded in the hyperbolic space of dimension two, which is on the left of this figure represented by the Poincar√© ball. Another example of manifolds are the manifold. Manifolds are the manifold of symmetric positive definite matrices. On the bottom of the screen, you can see an example of open source data set that belongs to the space of symmetric positive definite matrices. And this is a data set of brain connectomes, which are basically correlation matrices that we model as element of the manifold of symmetric positive definite matrices. And you have on the right of this figure two points on this manifold of symmetric positive definite. On this manifold asymmetric positive definite matrices, one is the brain connectum of a healthy control, and the second one is the brain connectom of a subject with schizophrenia. Last examples of manifolds and data of manifolds coming from the field of information geometry. And I saw that there was a very interesting talk before on information geometry. So maybe it's interesting to the community there. In information geometry, researchers Information geometry researchers model the parameters of gravity distributions as elements of manifolds that are called statistical manifolds. And so you have interesting manifolds that are, for example, the manifolds of normal distributions, beta distribution, and DHL distributions. And GMSAT, you have a special module that allows to compute for data on statistical manifolds, where now one data point is a probability distribution. Okay, so we saw that there exists multiple examples of data on manifolds that we can also access through the datasets module of GMStats. And so this is why we implemented GMSTAT. We implemented GMStats as a Python package to perform computations on this data and also computations that relate to statistics, machine learning, and deep learning, which we call geometry. And deep learning, which we call geometric statistics, geometric learning, or geometric deep learning. GeomStats comes in four modules. The first module is the dataset module, which is the module that we use to show how you can load these dataset examples in the previous slides. Visualization module, which is also a module that I used to show you the illustrations on the previous slides. And then the two main modules are the ones that are going to be the core of this presentation. The core of this presentation, which are the geometry module and the learning module. So, in the geometry module, we're going to see how we can perform computations for data that belong on manifolds. And the learning module, we'll see more specifically how we can implement geometric statistics and geometric learning for these data on manifolds. Importantly, GeomStats come with four backends. So, you can use it with NumPy, Autograd, TensorFlow. Py, Autograd, TensorFlow, and PyTouch. The last three backends are interesting for two reasons. The first one is that they come with automatic differentiation. And this is very interesting for the Riemannian geometry that we are trying to implement, because Riemannian geometry comes with relies on differentiation. And so, if you can do automatic differentiation with these three backends, that's interesting. And the second interest of how. And the second interest of having these three backends is that they allow to put the computations on GPUs, which is interesting to speed up the computations. Excuse me, there is some noise. I'm going to close the window one second. Sorry for this. So, this is the motivation behind this. This is the motivation behind these three additional backends. In terms of the objectives behind GeomStats, we have three objectives. The first one is to provide a software, a Python software, that allows us to teach hands-on geometric statistics and learning. So, for example, I'm teaching classes that uses concept of Riemannian geometry. And while there are a lot of very good textbooks that allow us to build these classes, That allows us to build these classes. Now I can also use GMstat to provide code examples that I can run within my slides using Jupyter notebooks, and I can have the students run as exercises for their homeworks. And they can really use the data sets that I've shown in introduction as an example for hands-on homeworks. So it's very motivating for the students and also it allows them to play with the different concepts and understand them better. Concepts and understand them better. The second objective is to democratize the use of geometric statistics and learning. So, as we're going to see, geometric statistics and learning has, there is a very rich literature on it, but very often it relies on complicated concepts from Riemannian geometry. And so, even though researchers from other fields or engineers in industry would like to use it, the fact that it comes with so much differential geometry and Riemannian geometry. So much differential geometry and Riemannian geometry can be an obstacle. And when we build Geomet, we make sure to kind of hide all the differential geometry core operations in the geometry module, which is here, such that a researcher from another field or an engineer only needs to be able to use the learning module to use all of these operations. And they can easily use the learning module because the learning module uses an API or a Uses an API or a syntax that is very similar to scikit-learn. And therefore, anyone who can use scikit-learn can use the learning module of GMStats without having to look at all the greedy details of differential geometry. So the second objective is geared towards non-expert in Riemannian geometry and differential geometry. And the last objective is to support research in geometric statistics and learning, including differential geometry and Riemannian geometry. Geometry and Riemannian geometry. And so, this last objective now is geared toward experts in Riemannian geometry, differential geometry, geometric learning, geometric statistics. Because now these are the people who are going to implement the core functions in the geometry module. So, all the details of Riemannian geometry, for example, and the core functions behind the learning module. And Geomets supports their work in the sense that once they make their Work in the sense that once they make their call open source and publicly available, then it's way easier for all other researchers and computer scientists to use, and therefore it showcases the work of all of this community and allow the spread of their work. At a very high level, before I go into the details, how to use GMStats, it's easy in the sense that you only have to think about two steps. Think about two steps. The first step is to instantiate the manifold of interest. So we have seen examples of manifolds of interest in introduction. The way you do that is that you look in the geometry module of the geomstats package. Here I import the class hypersphere and I can instantiate a manifold. Here I instantiate the manifold sphere as an object. So it's object-oriented programming. Oriented programming and as an object of the class hypersphere dimension two. And I can instantiate any other manifold of interest from the ones that you have seen in the introduction. And so when this is done, I can apply any estimation of our learning method. So we'll go a bit more details about what learning we have available in geomets. But just as a high-level example, if I want to use any type of statistics or statistical learning or machine learning, here I use the learning module of the G. The learning module of the GMStats package. In this specific case, I want to import an estimator, let's call the Fresh Em. And the way I can use this estimator is that I instantiate it on the geometry of interest. In this case, the geometry of interest is the geometry of the sphere. So it's sphere dot metric. And then I can fit my estimators to the data, where points are points that will have generated or, for example, imported from the data. Or, for example, imported from the datasets module. So, you can see that both the structure of the implementation of the package is split between a geometry module and a learning module, but also the way we use it is also in two steps that split between first, I look at what geometry, what is the geometry to which my data belong, and I import this geometry. And second, given this geometry, I can instantiate. I can instantiate the estimator of the mean, running algorithm, k-means, generalization of PC, etc. Instantiate it given the geometry that my data belong to, and then fit it on the point. So this is going to be the outline of the talk. First, I'm going to talk about computing on manifolds. I'm going to show how you can compute on manifolds using geomets. This is going to be Using geomets. This is going to be a presentation of the metric module of geomets. And in the second part, I'll talk about how we can do geometric learnings on manifold. And this is going to be a presentation of the learning module of GMStats. Then I'll compare Geomet as a software to related softwares or libraries that exist in Python. All right, so how can we compute with data on manifests? With data on manifolds. So we said that manifolds are generalization of vector spaces. And so the idea is to think about which operation we can do on the vector space and generalize this to manifolds. So which operation can we do on the vector space? On the vector space, the elements of inference are mostly points and vectors. So on the top right of the screen, you can see and the vector in Azure. The vector in black. So these are the main elements of interest. And with these elements, we can consider addition and subtraction in the sense that we can add a vector to a point to get another point, or we can subtract two points to get a vector. And then we can put some metric structures on this vector space by considering a Euclidean inner product, an associated notion of dense, and then path of short distance. Short distance. So, the idea is that these are the building blocks of any type of algorithm that you may want to implement on manifolds and specific type of learning algorithm that you may want to implement on manifolds. And so, if you generalize this operation for manifolds and implement that in some stat library, then we're going to implement any possible algorithm on manifolds. So, our goal now is to. Manifolds. So, our goal now is to generalize these operations to a manifold M. I'm going to use the example of here in the slides that follow, just because it's visual, but we'll see just after how they are implemented for all the different manifolds. All right, so how can we generalize the basic elements on spaces? So, we had points and vectors on a vector space. On the manifold, we have points and tangents. We have points and tangent vectors. So, with the sphere on the right, you can see two points: one blue and one orange, and you have, as the black arrow, it's a tangent vector, which is tangent to the manifold at the blue point. Now on the vector space, we could put a notion of metric, which is a notion of inner product at coexistence, etc. And on the vector space, we have notions of additions and subtraction. Additions and subtractions. So, let me first stop the notion of additions and subtractions. So, addition and subtraction are generalized from vector spaces to manifolds by the concept of Riemannian exponential, Riemannian logarithm. So, Riemannian exponential first is the concept that generalized the notion of addition, was the addition of a vector to a point to get another point. And so, on the manifold, if you use this exp that stands for XP. exp that stands for exponential you can also add now a tangent vector to a point and you get a point in order to do this in geom stats you can look at this part of the code so as before i've instantiated the manifold of interest which in that case is peer which is a hypersphere dimension two using the data sets module to load some data on Some data on. I create a point and a time vector, and all of this is to show you how to use the remaining exponent, which is the generalization of the additional spaces that allows to add a tangent vector to a point. And so this is what is illustrated on the right. If you start from the point in blue, you can add the tangent vector to the Riemannian exponential, you get the end point, which is the point in orange. Which is the point in orange. And similarly, the subtraction of two vectors on the vector space that gives the subtraction of two points on the vector space that gives a tangent vector is generalized by what's called the Riemannian logarithm. And that's what's illustrated with this line of code. It's called log. And so you can subtract a point to another point, and that gives you a tangent vector. And on the figure of the right, you can subtract the point to another point. point to another and the original tangent vector. Now on a vector you could put a Euclidean inner product on the whole space and this concept is generalized by the notion of Riemannian metric on the manifold which is basically a collection of inner products at each possible tangent space of the manifolds. And once you have this you can use it to create the notion of geodesic or geodesic create the notion of geodesic or geodesic distance. So the geodesic is the generalization to manifolds to the concept of shortest curve, the concept of shortest curve, and it's a generalization to manifold of the straight line. And now the geodesic distance is the length of the geodesic between two points. So in geomstats, you called the geodesic method between so either between two points or either you parametrize it. Or either you parametrize it as an initial point and an initial tangent vector, which is what's written here. And the geodesic on the illustration on the right is what's represented as the series of black dots. So it generalizes the concept of shortest curve on the concept of linear line between two points on the Euclidean space. And now it's a geosynchronous on the manifold. And then you can use the distance. And then you can use the distance function to compute the jet distance between two points. You have also more complicated operations that are implemented on Riemannian manifolds in GM stats, and one of which is a parallel transport. So parallel transport is an operation that allows to compute, to compare different tangent vectors at different points. So because we are not in a vector space, if we have two tangent vectors that belong to Two tangent vectors that belong to two different tangent spaces at two different points on the manifold, we cannot really compare them. In order to be able to compare them, we need to transport one tangent vector onto the other tangent space. And so in GMSET, you have operations that allow you to do that, which are parallel transport operations. And they are illustrated on the right. So now I had shown all of this operation implemented on the screen. Operation implemented on the sphere because it's an interesting, it's a simple example of manifolds. But I want to show you how it looks like for other manifolds that are implemented in Gemstat. And specifically, GMSTAT implements over 20 manifolds at the moment. So in the next slide, there are going to be a decent amount of code. You don't need to read all of this code. The important information are the part of the code that appears in red and the part of the code that appears in blue. Because basically, In blue, because basically what appears in red in the next slides are going to be the geometric classes. So, here the one that I have is a Riemannian metric that have I highlighted. And once the element of this geometric class is instantiated, we can look at the parts in blue, which are basically the different methods of functions that we can use. So, here the geometric structure of interest is the Geometric structure of interest is the spd metric phi that comes from the geometry module, and this is a Riemannian metric defined on the manifold of symmetric positive matrices. This is a fine invariant. So we can instantiate it. So now we have a Riemannian metric, which is an object of the class SPD metric affine. And as before, I can use this metric to call the Riemannian logarithm, the Riemannian exponential, and the Riemannian geodesic. And the Riemannian geodesic between two points, or between points and tangent vectors of the corresponding manifold, which is a manifold of symmetric positive definite matrices. And so on the bottom of the screen, you see a visualization of a geolysic between two elements of the manifold of symmetric positive definite matrices, which are two brain connectomes, which are in this case correlation matrices. So you see the visualization of a geodesic between Of a geodesic between a brain connectome from a schizophrenic subject to the brain connectome of a healthy subject. It's a geodesic computed with the Riemannian metric, which in this case is the fine invariant metric on the manifold of symmetric positive definite matrices. Another example from information geometry now. Again, we can only look at this code what is in red and blue. Red and blue. So, in red, we import the class that corresponds to the statistical manifold of normal distributions in 1D. So, we instantiate the statistical manifold, which is a manifold of the class of normal distribution. We get the canonical metric that is usually defined on this statistical manifold, which is the Fisher metric. And then the next part of the code shows how you can call the GRZ. Can call the geodesic function of this metric to get a geodesic on the space of on the statistical manifold. And so it's as simple as saying the Riemannian metric dot geodesic. And as part of the visualization model, you can look at this geodesic in the space of normal distributions either directly. So on the left, you see a series of points where each point corresponds to a normal distribution. point correspond to a normal distribution and you see the geodesic or in the space of privity density function so it's the on the right it's the same geodesic but represented as a continuum between two privity density functions which are normal distributions and the last example from the case of shapes actually the first the top of the screen is an example of a GIZ condoley group A GIZ on the lead group, and the bottom of the screen is an example of a GRD-Z on the question space. So, again, you don't need to look at all the code, just look at the pattern in red and blue. For the top of the screen, we see that we import a manifold that's called special Euclidean. It stands for the Lie group of rotation and translation in 3D. And again, we can instantiate this Lie group. We extract its metric. We extract its metric. In this case, it's a left-invariant metric corresponding to the canonical inner product of the identity. And once we have this metric, we can build the geodesic on the group SE3 in that case. And on the right, you see a visualization of these geodesics, where each point of SE3 is a rotation and a translation. The translation is used to place the origin of a 3D frame in 3D. And the 3D rotation. And the 3D rotation is used to orient that 3D frame in 3D. So each of these very little 3D frames correspond, each of them corresponds to an element of SC3. And so we see that we have a geodesic in SC3 on the right. On the bottom of the screen, you see an example of a geodesic in the space of shapes of curves. And so again, we import the manifold of interest, which is in that case in the manifold of discrete curves. The manifold of discrete curves on R2 because it's discrete curves in 2D. We extract the metric that we want, which for in this literature is called the square root velocity metric. And then once we have that metric, we can call the geodesic method in order to compute a geodesic between two curves. And on the right, you see an example of what it is, where it has been applied to the data set of cancer cells that I showed in introduction. Showed in the introduction. So you can see in gray the geodesics. So each point of this geodesic is itself a closed curve. And you can see the geodesic between the cell one to the cell two. And again, here I show example of geodesic because it's very visual, but we can keep in mind that once we have the geodesic, we can also compute the distance. And so all of this gives us a notion of distances between elements, shape. Elements, shapes, points in a statistical manifold, etc. etc. One comment that I want to make is that even though the I've shown are examples of where we have closed form to compute the Riemannian exponential, the Riemannian rhythm, the geodesic and the geodesic distance. We have implemented in GMSTAT a way to do that when we don't have closed form. And the idea is that in differential geometry, That in differential geometry, a lot of the computations rely on differences. And so by using automatic differentiation and integrations, we can access the Geodesic equation, then the Geodesic equation, then the Riemannian exponential, and the logarithm. And so we don't need to go in what this code means, but it just means that the basic equations of differential geometry can be implemented with automatic. Can be implemented with automatic differentiation and integrations. And so, what all of this means is that all the operations I have shown before on the sphere, on the manifold of SPD matrices, etc., can actually be used for any manifolds, where in that case, any manifold means any manifold that can be represented by its immersion in Euclidean space. So, for example, if I define any For example, if I define any graph as a surface, it would be a surface in 3T, and I compute the corresponding immersion, then I can directly get the metric that exists on my manifold by instantiating an object of the class pullback metric, which is basically using the ambient metric to define a Riemannian metric on this surface I have just defined. And all of the operation, the Riemannian exponential, et cetera, are available just by automatic differentiation. Just by automatic differentiation and integration of the structure in differential autometry. As a summary of the part on the geometry implemented in the package, this is a graph that shows the manifolds that are currently implemented. More precisely, this shows the manifolds that are implemented with closed form. But as I just said, we also have the non-closed form manifolds. And you can see that we have different classes of manifolds that are abstract classes. Of manifolds that are abstract classes, such as matrix Lie algebra, matrix D group. And then when you go down the hierarchy here, you get the different types of manifolds. So a general linear group, special autogenal group, special exclusion, etc. So the image credit is from Nicola Gigi, but not only the image is from Nicola Gigi, he also made a significant refactoring of the library. So this is where this graph. Is where this graph comes from. Okay, so I have introduced more the geometry side of Geomets from a user perspective mostly to kind of show you how you can use what is implemented in the library if you want to develop algorithms on Riemannian manifold or just compute on Riemannian manifold. If you're interested in how it's implemented, then the code is open source and we try to keep the document. We try to keep the document detailed and up to date. So, I would invite you to look there. Now, I'm going to show you how you can use GMStats to perform geometric learning on manifold. So, insisting now on the learning module of GMStats. And first of all, I'm trying to motivate a little bit why we need this learning module and why we need to compute with data on manifold as we showed. Compute with data manifold, as we showed in the previous section, and why it would not be sufficient to use addition and subtraction or usual statistics that we know on vector spaces. So let me take a moment to modify why we need this and specifically the learning module. And I'm going to mitigate this by the definition of the mean. So the usual definition of the mean is a linear definition. The mean is defined as a weighted sum of Defined as a weighted sum of the element xi, that gives x bar. And it's a linear definition. On the other hand, we have seen that a manifold is defined as a space that can be curved. For example, the sphere is curved. And so now this is a non-linear definition. And because of this, there is an inherent conflict when trying to use a linear definition of the mean on the non-linear space. And the conflict is expressed by the fact that the linear mean of the data on Linear mean of the data on the manifold will not necessarily belong to the manifold. So, to give an illustration of that, using geomstats, you can instantiate a sphere, which is an object of the class hypersphere, take two points on the sphere, and if we compute the usual mean of these two points, it gives the orange point here, and we see that the orange point does not belong to the surface of the sphere, and so it's not a point on the manifold. And that's a problem because it means that if we were to use Because it means that if we were to use traditional statistics to compute on manifolds, we'll have mean of rotations, that is not a rotation, mean of shapes of curves, that is not a shape, etc., etc. So the learning module of geometry is there to fill that gap by generalizing the mean, the estimation of the mean, I mean, by implementing what the literature has done in the field to generize the mean and the other statistical algorithm. Other statistical algorithm and providing an open source implementation of them. So, in the case of the mean, the linear mean now becomes the fresh mean, which is a definition of the mean that works on Riemannian manifold. And now it's defined basically by using a property of the vector mean and making that property the new definition of manifold. And so, the property that we use is the fact that the mean x-bar is the point that minimizes the sum of point that minimizes the sum of the squared distances to the data point, where the distance here is in terms of the geodesic distance on the manifold. And now if we do this, we see that the Freshy mean belongs to the manifold. So that's what's illustrated on the right here. We see that the Freshymin in orange is now a point on the manifolds. So now maybe one critique that we could make to this is that maybe it would have been enough to use a linear amin, get a point that's not on the manifold. Get a point that's not on the manifold, and then instead of implementing all of the operations I've shown you before, just implementing a projection that projects this linear mean back to the manifold. But if we do that, it means that we need to embed all of our manifolds in a higher dimensional vector space, which is in practice what is done for the sphere, because I can show you that sphere that is a 2D manifold as embedded in the 3D vector space that we know. The problem when we embed our manifold. The problem when we embed our manifolds in this, it's always possible to embed a manifold in a higher-dimensional vector space, which is renewed, which means we could do that. But the problem is that the dimension of the vector space that we need as an embedding space can be up to twice the dimension of the manifold. And so, in terms of efficiency of the computations, it means that we have twice as many variables to deal with. So, the hope is that by generalizing all the steps, Is that by generalizing all the statistics and the operations of the machine learning algorithm directly on the manifolds, that we don't need to double the dimension of the data space? So that's the first argument. And then the second argument is that if the manifold is built like this, and if we know that our data belong to that manifold, maybe operating directly with the intrinsic coordinates of the manifold will provide better intrusion, which we can see with the example of the sphere, because when we have a point of the sphere, it can be more. When we have a point of the sphere, it can be more intuitive to represent it as a latitude and a longitude rather than by its 3D coordinate xyz. So, this is why we choose to have all of this algorithm that are, we have some extrinsic algorithm, but all of these operations that I've shown you are intrinsic, meaning we do not need to use the embedding of the manifold in the higher dimensional vector space. All right, so beyond the estimator on the mean, I'm going to show you two. I'm going to show you two types of algorithms that we can use to generize learning on the vector space to learning on a manifold. And the two algorithms that I'm going to show you are from two class of machine learning. One is supervised learning, going to show you how you can do logistic regression. And the second one is from the class of unsupervised learning. I'm going to show you how you can do principal component analysis using the code from Jim. Using the code from GMStats. So, from logistic regression to tangent regression, this is basically just an example on how you can use any algorithm from scikit-learn, which is a Python package for machine learning, on vector spaces. You can use all the algorithms that are available in scikit-learn once you have brought the data on the manifolds back to a vector space. And so the vector space that should be used in this... Space that should be used in this case is the tension space at the freshening. So basically, any supervised algorithm that is available or unsupervised that is available in scikit-learn can be used through GMStat just by transforming the data first. And this is done via the pre-processing module of GMStats, which has a class called two-tangent space, which is used to project the data to the tangent space at their mean, at their Frischie mean. At the fresh eminence, and so we don't need to look at all these codes. The thing that I want to showcase in this code is how you can use the structure of scikit-learn, where the way we use scikit-learn for our machine learning is that first we instantiate a pipeline, which is the list of operations that we want to do on our data. And then we can fit this pipeline on the training set, and then we can use this pipeline to predict labels on the test sets. On the test sets. And so the pre-processing part of GMSAT would project the data to the tangent space at the Fresh En can be naturally embedded within a pipeline of scikit-learn. Another example of machine learning operations is the machine learning operation that machine learning algorithm that comes from the class of unsupervised learning. And so here I'm showcasing a very simple example. Using a very simple example, which is the example of tangent TCA. So we can see that we can use geomstats the same way we usually use it, which is first we instantiate the geometry and then we instantiate the learning algorithm that we want to do. One comes here and one comes here. So here on the sphere, we have a sphere which is an object of the hypersphere class. We instantiate the We instantiate the object that allows us to compute the Freshie mean, and we also instantiate the object that allows us to do tangent PCA at the Fresh Emin. And we can fit the tangent PCA object on our data. We tell it to do tangent PCA at the base band, which is the estimate of the mean. And we can get the transform data using tangent PCA. So I've shown simple examples of geometric learning on manifold. Of geometric learning on manifold. But the goal of GeomSats is really to provide the geometric statistics and learning algorithm that basically allow to fill this table. So in this table, you have as many, you have rows that represent different type of geometric structures that you can put on manifold. So we've talked a little bit about the structure of Lie group, the structure of quotient spaces, the structure of Riemannian metric, but there are many different types of geometrics. there are many different types of geometric structures that we can equip the manifolds with. And then for each of these geometric structures, we can look at the different columns of this table. These are basically the type of statistical estimation or machine learning algorithm or probability theory processes that we would like to generalize to this geometry. And so the goal for GeomStat is to implement the geometric aspect in the geometry module and the statistics and learning aspect in the learning module module. Statistics and learning aspect in the learning module so that any learning algorithm can be used with any geometry. Great, so I've introduced the geometry part of geomets and the learning part of geomets. I'm going to go fast on the comparison with RAITEL library because I don't have much time left, but let's do it. So, these are the main libraries that do similar computations than GM stats, but they have different. But they have different goals. I'm going to compare them here a little bit. First, you have two libraries called Pyreman and PyQuaternion that perform computations and some statistical algorithm on manifolds, but they are dedicated to a specific manifold. So Pyriman is dedicated to the manifold of SPD matrices, and PyQuater is dedicated to the manifold or the Lie group of 3D rotations. So while they are very powerful for these manifolds, because they are specialized on this manifold, Are very they are specialized on these manifolds, they do not provide the possibility to implement any statistical learning on any manifold. Then we have PyManopt, which is a Python package that was building upon a package called Manopt, and that stands for optimization of manifolds. And so this is a package that specializes for optimization on manifolds, whereas GMSTAD provides the basic building blocks to do basic operational manifolds. To do basic operation on manifolds. So it's very low-level in that regards. And then it germs up is more high-level because we look at the learning algorithm. And PiMANAPT in that respect can be seen as in between because optimization of manifold is what's used by many learning algorithms, but Germstat is both providing the low-level computations and then the higher level learning operations. Then we have TNO geometry that provides Then we have TNO geometry that provides non-linear statistics on manifolds. This one would be maybe the closer to geomets, but it's not maintained anymore and were also implemented in TNO, which is not maintained. And as opposed to this, Geomet implements four different backends. MacTorch and GeoOpts are similar to Pymana, but they focus on more specific optimization problems. Optimization problems. So MacTouch is only iTouch and focuses on optimization on manifolds specifically for deep learning. And GEOOpt focuses on optimization on manifold, but for optimization that are stochastic and adaptive. This is a comparison of the geometric size functions that are implemented in these packages. So in terms of the manifolds that are implemented, this is a list. That are implemented. This is a list of what is implemented in terms of the geometric operations. This is a list of what type of geometric operations are available on the manifolds that are implemented on these packages. And this is a comparison in terms of engineering. So what backends are available in each of these packages? And importantly, also how these packages are made. Also, how these packages are maintained in terms of continuous integration and code coverage. Meaning, do the maintainers of the package run unit tests on their code to make sure that it doesn't bug, basically? And so here you can see that 86% of PyManop is tested. This is as of 2020. 84% for MacTorch. And in GMSats, we give the code coverage for each of the different backends. In conclusion, GeomStat is a collaborative effort and we organize hackathons here and then now and then. So, if you want to join, you're very welcome to join the team. And I thank you very much for your attention. Well, thank you very much for such a wonderful presentation, Jeremy, and thanking Nina for such a beautiful work with lots of ramifications. And I don't know, we should. I don't know. We should first open up the questions to the audience. So, does anyone have a question at the moment? I think there's one in the chat. It says, will JAX be supported in the near future? JAX is an increasingly popular framework. So it is an upgraded version of Autograb with GPU acceleration. So, do you have plans for JAX? So, yes, excellent question. So, yes, excellent question. This has been the focus of a lot of discussions. So, we actually try to port the library to JAX, which is very easy in theory. It is very easy in theory to add a new backend to geomstats because we have abstracted the backends from the library. So, all the library is implemented with what we call GS. So, you import geomstat backend as GS, and then all the libraries. As GS, and then all the libraries implemented with functions that look like GS dot, which mimics np dot for NumPy. And so if you want to add a new backend, you basically you just add a new folder in our backends folder and you make the translation between these operations, a little bit what Igoby is doing, but we've done it in-house. And so, yes, we have added actually a new folder with checks. The problem, though, is that when we run the few benches, Though is that when we run the few benchmarks, it seemed that JAX did not provide a significant speed up compared to Autograd, and so we were wondering why that would be the case. And so the conclusion that we reached at there is that maybe JAX is very useful for a very big data set, such as the one that you have in deep learning, big data set sample sizes. But for us, because we have smaller data set sizes, there is this overhead that makes There is this overhead that makes all of the computation probably slower. That being said, I think we should still add it because even though the main computation that we tried did not show a speed up, maybe for other computations and all the applications, where it will be an improvement in terms of speed. And so, yeah, I think we should add it and maybe we'll add it soon. It's already in a PR somewhere if you want to try it yourself. But yeah, there were some issues, I think. Yeah, there were some issues, I think. Is there any other question by the audience? Maybe I do want to ask. So here you have a parametrization of a lot of manifolds, but one could also present a manifold using an atlas and then use patches, and that allows for other things. So, could you explain maybe how? Could you explain maybe how a presentation using an Atlas could be integrated to germstats so that maybe other things could be integrated as well, like local computations, local contracts? So we do not have the implementation of ATLAS per se, but if you're interested in this, I recently came across another package that's called Sage Manifold. And Sage Manifold, and also the reason why I haven't presented it here, is that. I haven't presented it here is that it belongs to the community of theoretical physics. And so, there they have the implementation of manifolds as atlases. So, if you were interested to do that, I would start there. And for us, if you're interested in bringing that into Germstatt, I will look at there also how they do it. But for now, basically, the main strategy that we have is to project the data at the tangent space of the mean, if that's feasible, and then basically just use. Feasible and then basically just use that vector space as a way to parametrize the manifold. Maybe a similar but different question is: you do cover the hyperbolic plane as one of the manifolds that you have. So I was wondering if you had thought about using gamma invariant regions. And by gamma, I'm thinking of groups that would be fundamental groups of surfaces so that they're Of surfaces so that they're discrete subgroups of the isometries of the Poincar√© plane of the hyperbolic plane in such a way that when you're looking at the gamma invariant parts, it's the same as looking at what would happen in the quotient manifold. So instead of having like a presentation of the curved surface, you're looking just at the fundamental region and then you're doing your computations there. So is that something that you think could be Something that you think could be something that would be interesting for the future, maybe? So, yes, definitely. And we have not started to look in that direction at the moment. But I know that the hyperbolic space has a lot of interest from the deep learning and generally the machine learning community. So, every type of algorithm or methods that can improve how we perform computations, PGLIS, the operations, or just add. Or just adding new computations, I think will be very interesting. But we haven't looked at this yet. So there is place for contributions.