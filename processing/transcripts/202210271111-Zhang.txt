But it's not the same thing. Okay, so this is a joint work with Wen and Kui from Columbia University. Okay. I have the sharing. The sharing is not just exit. Oh, yes. Okay. So what is the inverse problem? Of course, this is a workshop for inverse problems, so everybody knows this. But I personally, I like this picture because this is always remind me the first time I step into this build. Okay, so basically, for the forward problem, we have some forward model, FM, and with some. Model Fm and with some given parameters we can generate the data G. Now for the inverse problem we kind of like okay we have the observed data G and we try to reconstruct the parameter A. Okay and so for the inverse problem there are a lot of applications especially in the geoscience. For example for the carbon dioxide monitoring in the subsurface and contaminant source identification, climate change and hydro hydraulic tomography. Hydraulic tomography. Except the main usage is geophysics, and there are also many other applications for the inverse problem in some other fields, such as the medical imaging and non-destructive testing and neuroscience image deployment. So there is a huge demand to try to develop the efficient and stable methods to solve the inverse problem in real world. real world. Okay, so but however, because of the lack of the explicited formulation for the inverse operator of this forward map here, so typically one classical way to solve this inverse problem is try to do the model-based inversion, which means, okay, we minimize the L2 loss between the observed loss data and the model prediction data here, and also came with some potential. With some potential regularization term, if we have some priority information on the parameter field A. Okay, so however, for this minimization, L2 minimization problem here, this problem is highly nonlinear and it is non-convex. So basically we have a lot of local minimums and maybe if we use the gradient-based methods, we may not find the global one. And the algorithm can be easily checked. And the algorithm can be easily trapped into the local minimum. As Bian mentioned, on Monday, and they try to use the substitute this matrix by the W2 to help improve the landscape of the objective function. And Chenstone also mentioned that, okay, we can kind of do the relaxation of the forward model and try to reduce the nonlinearity of the objective function. Except the non-convexity of the objective function. Of the objective function, this classical one, and also the computational cost for solving this minimization problem could be very high. So, especially we know, okay, if we use an iterative method to update the parameter field, and at each iteration, we have to re-evaluate the forward forward model here. So, if the computational cost for this is very huge, and the total comput computational cost for summing this could be very expensive. This could be very expensive. For recent decades, there are huge interests to use the narrow network, try to substitute this model-based inversion. And here, I kind of like list some recent work on the data driving end-to-end immersion. And if you are interested in, you can take a look at those references. And thanks for the last talk of this more. Thanks for the nice talk of this morning from Aumi and Richard. I don't need to introduce what is the neural network here. Okay, so for this kind of data driving end-to-end immersion, now what we want to do is we want to train a neural network which can represent the inverse of this forward map here we denoted by the F inverse. And this data is neural network parameters, just the weight functions. Weight functions, weight matrix and the virus vectors. And then we try to minimize the L2 nodes between the exact parameter field and the predicted by the neural network. And of course, here for this noise function, you can use more complicated one and also add some regularization term to stabilize this training process. But here for the presentation purpose, we just missed this and L2 misfit here. Okay, so for Okay, so for the data driven to under inversion, there is a benefit, like okay, because the training, you can do this training offline and later if you have the new data, you can just apply this trained neural network to get the reconstruction results. But there are also some intrinsic problems. So, as we know, for the machine learning, it is very, very hard to learn the high frequency. Is very, very hard to learn the high-frequency information from the data, and it tends to capture the low-frequency information. And also, the learning process for the high-frequency is very highly unstable. Okay. Second, the learning results is not transferable. In the sense, the nerd inversion is hardly used to another different, slightly different environment. Different, a slightly different environment, actually. So, for example, if you have the NERD inverse operator from one discreditation, and which can be different from the one from another discreditation. Okay, so now our question is, what should we do for this? Is there a way we can kind of like improve the performance of these two typical methods? Okay, so this is the motivation for this project. So, here, in this project, So, here in this project, we try to combine those two catalog methods to potentially improve the performance for both of them. Here is the general idea. So, again, for the inverse problem here, we have, okay, so here there is a type of, there should be some G data. So, this is the data can contain some noise. Okay, and we try to reconstruct the parameter field from the observed data. Okay, and instead of directly reconstructing the field M from the data, either based on the model-based inversion or the data-driving inversion, we propose to first learn an intermediate state from the observed data, which used the data-driving inversion. And then start from this intermediate state, we try to use the model-based inversion to recover. With the inversion to reconstruct the field, the field which we are interested in. So, if you look at this diagram here, those two states are independent in the sense that, okay, you don't need to solve this simultaneously. So, we first have this one to get some intermediate state, and for this one, it's easy to train, and then we from here, we use a model-based inversion to capture the high-frequency information from the data. Okay. So here, so one important thing here is kind of like, okay, we need to have this intermediate state. So, what is this intermediate state? So, here, after we realize that the learning process is only reliable on the low frequency components of the data. So, here we propose, okay, we only know the dominant feature of the inverse operator. So, eliminate the low frequency part. And this is what we mean the intermediate. And this is what we mean by the intermediate state. And the thing we use start from here, we use the nerd inverse, the approximate inverse, as a preconditioner to solve the model-based inversion problem. So specifically, now we say, okay, our proposed method, we have two stages. One is the learning stage, and this one can be done offline. And the goal of this stage is to try to capture the low frequency information of the inverse operator. information of the inverse operator and then use instead of the exact one. So here we can kind of like save the we can save the training time all of those. And then after we have this approximate inverse and we use it to do the online reconstruction and try to capture the high frequency information from the inverse operator by using the model space match speed. Okay, so specifically this person looks at the learning Specifically, let's first look at the learning stage. So, here again, because as we mentioned now, we focus on learning the dominant feature of the vernacity of the inverse operator. And here, so we first transfer the parameter space from the physical space to the feature space. And here, you can imagine this one. The curl M here is just the forward transform. Okay. And then. And then we try to train the neural network to get the approximate of the inverse operator by solving this weighted LO2 misfit function here. So here, the F2TA here inverse here is the trained inverse operator. And again, the theta just the neural network parameters. So here, notice that this we have a weighted matrix mu here. So this mu has a property. So, this mu has a property which weighs the loss heavily on the feature we are interested in, which means the no frequency part, and while damping the feature that we get hardened to nerve stable, which is the high frequency part. After we train this to get this approximately inverse operator, then on the inversion stage, we use it as a preconditioner to solve this, to solve the model. Solve the model-based inversion problem. Okay, so here, because our training is again training is in the feature space, so here to back into the physical space, we just do an inverse for our transform here to get the F hat. So this F hat is our intermediate state. Okay. Okay, so now we try to solve this. Now we are use the model-based inversion. So here we try to minimize the LO2 misphase between these two quantities. Meet space between these two quantities, and also we can add some regularization term here. Here, the gradient is just used to impose the smoothness of the parameter A. Okay, so one thing to note is that when the nerding is sufficient accuracy, which means okay, you have a very good approximation f hat to the f inverse, and then this one can this objective function is vector close to convex. Later close to convince. So, indeed, you can imagine in the perfect world. And suppose we can train the inverse operator exactly. Now we have this f hat inverse equals f inverse. Now the problem just reduce to the trivial one. We have the linear least square problem here. Right? So, and this one can be solved directly. We can get the exact solution. Which we have the F star equals this. So this is essentially just a smooth version of the neural network prediction here. However, in real world, of course, we cannot trim the inverse operator exactly. So if the trimmed inverse operator here is good enough, in the sense that the norm of the identity operator and the f-had inverse. And the f had inverse part of the way the f is very small. And in this case, now we can introduce an operator which is a difference operator between those two operators. And then we can rewrite the modified inversion problem here to this one. And after we have this, now since the norm of k is very small, so we can solve this by simply use the Neumann series. Use the Neumann series and to generate the exact solution of the M star. So now, if you look at this here, we don't need to compute an gradient. Of course, this one is okay. We have the trinity inversion is very good. It's good enough. Obviously, we don't have, we cannot add the infinity number of the series. So basically, we use a truncated Newman series with some type of G, and G is actually small. Happy to J and J is actually small. Because you can imagine if this one is very small, actually, just several new math, new math terms can give you a very, very good approximation results. Okay. Ask a really quick question between the K will be not any operator, right? The K here. Yeah. Yes, because this one is uh is uh is a difference between this. Right. Yeah. But then what I mean, what are the guarantees that the norm that's here is we're it's the inverse. Yeah. It's the inverse. Okay, so you have this, then you solve this one. Okay, or you can think about this one in some sense, approximately still good. Okay, yeah, maybe we can discuss it more online. Okay. Yeah, this one. Okay. So, okay, so now we look at one application of our scheme. So, here, we use it to do the four-wheel immersion. So, for the So, for the previous talk, we have many talks on those catalytic stuff. And so, here I just briefly do some introduction to some process here. So, the four-wheel immersion is kind of like we want to extract the physical parameters of the wave equation from the recorded data, and it has many applications. So, here I just list one for the ultrasound, medical ultrasound image. And here, okay, so here for this, okay, we have some incident source, and then this way we get. And then this way we direct this one into the underlying object here. This is the skull of the human. And then we generate some wave and those waves passed through this object. And on the other hand, on the other side, we have some receivers which can record the time sequence of the wave which arrived at this part. So here, for this application here, For this, for this application here, for this project, we just assume that we have the simple acoustic wave equation. And here, the M is what we are interested in, which is the speed field of this object, and the U is the pressure field, and H is the source. Okay, so now here, again, we try to solve this problem, and we first simplify it to the two-dimensional space. And so, here, we consider We consider that domain just a slice, so just to retaggle. And here, the red dot is our sources, and then we have the receiver on the other side. So now, what we try to do is we try to use the data recorded at those receivers to reconstruct the speed field M. Okay, so and in this setting here, now RF, the forward operator, is simply just this wave equation. Simply just this wave equation. Okay, so now let's see, because we propose a method, we first try to nerd an intermediate state, which is our approximate inverse, and then we solve the model-based inversion problem. And now we want to see if this one really improves the X-natural versus the netscape of our objective function for that preconditional one. So, here, indeed, we have some small results here for that type of setting. So, here, if we can guarantee our training is between If we can guarantee our training is good enough, and in a sense, this one is small enough, and also we need to require that the free shift derivative of the trained neural network is bounded, and the noise in the observer's data is also sufficiently small, then we can guarantee the difference between the modified versus the pre-conditional problem with the quadratical problem is very small. And moreover, if we have some If we have some, for example, when we solve that augmentation problem, we have some, we use the internet method, we have some initial data. If that one is close enough to the two kids, then we can guarantee this one is almost like a quadratic problem here. And this initial gaze can easily get in our city because we have a pre-twinned inverse operator there, and we can use that one as the initial data. Okay, and here is some. Okay, and here is some democratic examples. So this one here, here we consider the wave field is just one single Gaussian in two-dimensional space. And to visualize this picture, the landscape here, we fix the amplitude as the variance of the Gaussian profile and just to look at the variance with respect to the location X0 and Z0. X0 and Z0. And from here, you can see for the original model-based L2 needs to square need function here, indeed we have many local minimum and it is hard for gradient-based message to find the global minimum here. But now if we do the pre-conditional problem now for this, this is in our inversion stage. So if you look at the netscape here, now it becomes more convex. And essentially, if you have a good initial dis. Initial data, go to initial guess, and you can check your problem to this region. This region. And then this one, the gradient-based method can easily solve this problem. Can I ask a question? Why is this meaning monitor of these two doesn't they don't look the same? The top row, I don't know the top row and the bottom row, they should have the same global meaning I think. See they are here, right? I guess. Yeah, the color is hard to see, so you and maybe I change the angle of this one a little bit and you can see the motion. Yes, yes. Okay. Okay, so now let's look at some reconstruction results. So for the first Toyota problem here, we just try to change them. Okay, for the velocity field, we only have the low frequency information. So you can see we only consider the velocity field contains five four remotes. So that's very, very low. Remote. So that's very, very low frequency part. And so for this test, the training city still contains the low frequency part. And now you can, from this results here, you can see the first one is a ground two velocity field, and the second one is a neural network prediction. And this one is reconstruction from the Copeland scheme. So if you compare the two with the neural network prediction, you can see for this case, okay, the neural network F hat inverse actually is pretty accurate. Inverse actually is pretty accurate if you compare these two pictures. Okay, and in this instance, now we say, okay, now we can use the Nouma series, just use several terms, and we can do this reconstruction here. And indeed, here we only add 20 Neu Man series term which can be solved within like 20 seconds to improve the reconstruction to this level. So this is, okay, of course, this is a very, very simple example. Okay, so now let's look at a slightly different example. Different difficult problem here. So, in this case, now our velocity field we are interested in is out of the training data set. So, for the velocity field here, we have kind of like the piecewise function here. So, you have those color here. And those contents are not of high-frequency information. But our little network, what we only focus on is training the low-frequency part. So, if you look at this prediction result here, At this prediction result here, and it only have the low frequency, kind of like the background here, background information. It did not capture the high frequency information. Okay, and then we use this one, and we also use the copper I here to add the 20, I remember here, is the 20 Neumann series to get this slightly improved results, reconstructing results compared with this. Okay, but this is still not close to. Not close to the ground 2 model. Okay, now start from here. This is what we mentioned from that theory. So now we can use this one to be as our initial data. So that then the misfit function will be trapped into that convex region. And then we use a gradient-based algorithm, but in the traditional way, and then we can reconstruct this result here. Is this example also receive source on the top, receiver on the bottom? Yes. Okay. Yes. Okay. Um okay. Um okay, so uh there are uh there so that is um so we can say okay for our idea actually it can work pretty nice at least for those like artificial problem. We haven't tested this idea in the experimental data yet. So but here I want to make some remarks here. So first as we say okay to for the learning stage we try For the nerding stage, we try to nurture a mapping from the input data to the output velocity here. So, here this is our approximate inverse. So, we need to generate those data. So, what we do here is we have like have here the input data which is generated from the velocity field, which contains more information compared with the output. Compared with the output velocity filter here. And in a sense, here we have the MJ plus MJTilda to generate our training data. So for this MJTilda, how do we pick this one? It has the property satisfy the Fourier transform of this MJTLTA as the frequency which is in the output, in the output vertical field, which is this one, okay, equals zero. If the frequency in the input, The frequency in the input, which is this whole thing, but it's not in the output, and this is not equal to zero. So, in this sense, then when we try to solve that pre-conditional problem, we can grab the information which does not contain the trained inverse operator. Okay, the second one is, so we can kind of like have a sense on the training error. So, here, we can think about the We can think about the simplest case where we have the linear setting. So now if you think about this one, it's just a linear mapping. Maybe you do a Taylor expansion on some fixed points. Just think about this one is linear. Now, if this is linear, if we have the training error, suppose we have training error is order x0 square, we can actually solve this. So this one, the inside of this. This, so this one, the inside of this will be the order you said, of course, we have a square here. So now, then after we have the new input data, g delta, observed one, and we want to apply this trend inverse operator to here, and we can actually solve this problem. It's just a linear problem. From here, you can see now our error, because remember, this mu is the weighted matrix when we're training, and it has a property which weights heavily on the low frequency part. part and while damping damping damping is a high frequency part. In other words, the elements in this matrix mu corresponds to the low frequency information have the large value and corresponding to the high frequency part has a smaller value. So if you have the mu inverse here and then you can see the relative error in the high frequency will be much larger than the low frequency part. Than the low frequency part. So, this is also where we see: okay, we only focus on this setting up here, is we focus on the training for the low frequency part, that's the high frequency one. And also, usually for the neural network, the low frequency part is easily to be captured. Okay, the next thing is for the second example, we say that, okay, if we have the velocity model we are interested in is outside of the training data set. Outside of the training data set, and then we have to do if we want to capture this high-frequency information, we have to do the model-based inversion. And for this step, we still need to solve this by the gradient-based algorithm. So we need to have those derivative gradient information. So here we just use the adjunct method to compute the derivative here. And we're not going to go through the details here because you are the expert on this part. So, but one thing. This part. So, but one thing we want to point out is that: okay, if you look at the adjunct problem here for the pre-conditional problem, and the source here actually involves the derivative of your trend neural network. So, in the sense that, okay, your training neural network, you need to guarantee that one, not only the trend is accurate and the derivative is also good. Or say you have some generalization here. But again, because we only focus on But again, because we only focus on the low frequency pipe, so this one uh this one can be done. Okay. And how are you? Okay, so uh this is just a natural uh natural uh extension. So as we mentioned we introduced, okay, we try to train train an approximation to the inverse operator, but actually this idea can be extended to other problems. For example, if you have the joint immersion, and then you can try to And then you can try to use a neural network first to capture the intermediate stage, which is some kind of cross-meters relation between your market federal, and then use this trained neural network to help the adjoint reconstruction. Okay, so I think I'm out of time. So I will stop here, and I'm happy to take any questions. I have a question.