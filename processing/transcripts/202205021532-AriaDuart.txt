Thank you for the introduction. So welcome. In this presentation, I would like to introduce our new evaluation score or feature attribution methods. So as you may know, the main motivation for these feature attribution methods is to shed light on how the model gets to reach a particular output. So these feature attribution methods So these feature attribution methods are usually applied when the model is already trained. So these attribution methods compute the attribution of each input feature of the input image to represent its contribution to the output. So in practice, if we have this model and this model predict this image as a dog, we would like to ask this feature attribution method why it is a dog. A lot. And ideally, we will get a feature attribution map highlighting with that. Of course, if the model release is well trained. But we can also ask to this feature attribution method why would it be a variable? And we will get a feature attribution map highlighting the variable. So the class the explainability method is asked to explain is what we call the target class. So in the field of image classification, many features image classification many feature many feature attribution methods have been proposed with with this purpose right with the purpose of explaining the the model behavior however there is no well as sara introduced before there is no standard standardized metric to to assess and select these models so the existing evaluation methods can be divided in two board groups so the qualitative and the quantitative method qualitative and quantitative methods so the qualitative Quantitative methods. So the qualitative methods are based on assumptions induced by the human understanding of perception. So these evaluations built on top of these human biases may not be aligned with the model behavior. In contrast, the quantitative methods avoid such human biases by excluding the human from this assessment process. These quantitative methods can be also divided into two groups. Can be also divided into two groups, the ones providing categorical evaluations and the ones providing numerical ones. So, words proposing categorical evaluations serve or define tests or axioms that these explainability methods should satisfy or fulfill. While these categorical evaluation methods serve to validate whether or not these explainability methods fulfill certain properties, they are limited in terms of Are limited in terms of ranking and comparison. In contrast, numerical ones provide comparable scores that can be easily run. A barrier, or as Sara introduced before, that this quantitative method should overcome is the lack of a ground truth defining what defines a correct explanation. So, this barrier can be overcome with Can be overcome with two approaches with an assumption, for example, assuming that the object is the only important part for the prediction. However, as we know, this is not always true. And the other way is by inducing a source of nodes. For example, these explains are in TDC before also, so distarving the image and then quantifying the or distarving the image according to the explanation and then quantifying the change in. And then quantifying the change in its output. However, these disturbed images are images outside the distribution which produce the evaluation approach. So our in-noise approach induces in distribution noise. That is, we alterate the input but preserving the features or The features or the patterns found in the original data distribution. So, in practice, we create a new sample with samples of other classes. We call it the MOSAC image, and I will explain it in more detail later on. So, now let's see how deep focus our metric is coping. So, for this for the focus metric, we need three Three ingredients. The first one is the explainability method to be explained, sorry, to be evaluated, the train classification model to be explained, and then a set of MOSA examples to be able to calculate the focus metric. So we evaluated six explainability methods: the GRADCAM, DLRP, DSMOTRAD, the GRADCAM, integrated gradients align, and to run an And to run an explainability method, one needs a model to explain, right? A model generated from a dataset and from an architecture trained through a dataset. So we evaluate, we use three architectures, AlisNet, VGG and ResNet. And we use four different datasets. So one data set, the Dogs Personal Cat dataset, which is composed of two classes, dogs and cats, the widely known ImageNet dataset, composed of two. Widely known ImageNet dataset composed of 1000 object categories, the also well-known MIT67 dataset with 67 indoor categories, and the Museum Arwork Medium dataset, which is composed of 29 medium categories. So, the last element to compute the focus are the MOSA. So, to formalize MOSAX, let me define. Let me define a dataset D composed by a set of images I and a set of classes C. So let's imagine that we have three classes. So each image of the dataset has assigned one of these classes, right? So from here we build a set of mosaics. So in this case, each mosaic has four samples, four images, and this is characterized by a target class. So the specific class, the explainability method, is expected to explain. To explain. So let's see an example. The target class of this mosaic is the CAD class. So we have two CADs, one dog, and one rabbit. So let's see more examples of mosaics. So here we have a mosaic buildup with the images of the MIT67 data set. So the target class of the mosaic is the A4 TIN side class. So we have two images of of APOT and the other two are randomly chosen. And the other two are randomly chosen among the rest of classes. So in this specific case, one corresponds to the state case class and the other to the greenhouse class. So let's see another example. Here we have an example of a MOSA made up of images from the MAM dataset. So the target class of this dataset is the finance class. So we have two images of the finance class and the other two random each observance. So here one corresponds to the Eulon canvas class and the other to the good engraving class. Class and the other to the put engraving class. So you can notice that the target classes, images, are not always in the same position. So the position or the exact position of the target class within the MOSAR are chosen randomly for every MOSA. So now that we have the three elements needed to compute the focus, let's see how the focus is computed. So in a sort of an A-tracking game, the focus is. An A-tracking game, the focus asked to the explainability method why does a mosaic M belongs to the target class. So in a mosaic which is composed of two samples belonging to the target class and two other not belonging to the target class. So given this question and a good underlying model, a reliable feature attribution method should be able to concentrate most of its relevance in the two correct squares. Correct squares, the two cut squares in this case. So the focus score measures how much of the explanation is located in the two correct squares, in this case in the two plus squares. So let's see an example. If we want to compute the focus metric for this mosaic and for the target class cut, the focus will be the sum of the relevance located on the two plus squares with respect to the sum. Squares with respect to the sum of the relevance of all squares. So, as I said before, the exact position of the images of the target class is chosen for immediate mosaic randomly. But we wanted to test the effect of the position of the target class within the mosaic on the focus. So, here we use the Graptan method applied on a BGG model, pre-trained on image deck and then training the dog species. And then training the docs versus CAD dataset. So we tested six different configurations: so CAD CAD, DOCDOC, DOCDOC, CAD CAD, and so on, and plus a seventh configuration with random positioning. So we can see how, for this specific example, when the configuration of the two target classes are continuously, tend to be better. Tend to be better. And in the case of the left-drive positioning of the target class, we can see on a small gain when the CADs are. Sorry, I forgot to say, I think, that the target class of this motive is the CAD class. Well, these properties are specific of this motion, and we cannot guarantee that they will hold among different data sets, target class, or models. Class or models. So that is why we use random positioning for avoiding this or compensating these differences. Another experiment we wanted to test was to test or to analyze the effect of the model randomization on the focus. So for this experiment we used two models, a VTG model, pre-trained only metadata and then trained for DOX versus CAD data set. TOGOS versus CARD data set and then another VTT totally randomized. So we also use the LATCAM method for this experiment and you can see here the focus distribution as histograms for both experiments. So while with this pre-trained model, train model, we reach 0.94 focus, when we use the totally randomized, the mean focus is 0.49, which is the 49, which is the default or the explainability method is located 50% of the relevance in the room squares. Okay, so before analyzing all the experiments together, I will explain how we illustrate them. So here we have the experiments for the MAM dataset. So each column is the focus distribution obtained for each experiment. Here on the y-axis, Here on the y-axis we have the focus, so as I said before, a focus of 0.5 would be that the relevance is random, so 50% of the relevance is located in the low squares, and a focus score of one would be that all the relevance is in the correct images. So then, as I introduced before, we use three different architectures, AlexNet, AlexNet, BTG, and ResNet, and then we evaluate the different extrain methods. So each colour is one feature distribution method. So BlackCam, LRP, Smooth RAM, LINE, LALTCAM ‚Åá  and integrating CAM. So now that I hope the plot is clear, so let's analyze all the experiments together. So for the DOTS versus CAD dataset, the MIT data set and the MAM data set. Dataset and the MAM dataset, we use 100 MOSAX per class, and for the ImageNet dataset, we use 10 MOSAX per class. For the case of the LIME, since it's computationally expensive, we restricted the experiment with this method to the DOTS versus CAT dataset and to the mark plates. So our findings are the following. Gracam obtains in general the best score, reaching a mean focus above 81 in all experiments. Focus above 81 in all experiments but work and, according to our results, is particularly robust when applied to noisy models. Gradcam Plus scores lower than RATCAM in every experiment, being the third or the fourth in the overall ranking. LRP wins in five experiments and gets the second best score in another five. And according to our And according to our results, it's a very good methodology for explainability when applied to very accurate models. LIME performs remarkably well when applied to very accurate models, however for lower accurate models, it becomes less reliable. Smooth grad obtains a focus around 50% in every experiment. Experiment, well, being like random. And integrity gradients also obtains a focus quasi-random in general. So now that we have seen how the focus is computed and also our results, let's move to the last part. So explainality has been used, as introduced before, as you probably know, to detect bias. You know, to detect biases before. However, this approach typically relies on a human identifying biases and reviewing all the explanations. So with this motivation, we can go beyond automating this identification process while providing a visual explanation to the user. So for this experiment, we train a model to classify dots and cuts, but we introduce a correlation, so by only using So, by only using dogs outdoor and cats indoor. So, we trained the model using 960 images and for devalidation 100 images. And we got an accuracy of 87%. So, even if we got a high accuracy, we wanted to test if the model has left some unwanted correlation. So, we selected the three worst dot dot prediction and the three worst cut prediction. CAP prediction. Starting from the hypothesis that if an image is predicted with low probability, it means or it is likely that it has a pattern or a characteristic pattern of the other class. So from this hypothesis, we create the mosaics combining these images. So for this experiment, we use two by one mosaics and then we compute the explainability. For this experiment, we use blackcom and the target. We use Blackcom and the target class is the dog class. So, without the need of reviewing all the validation images or all the explanation of the validation images and only by reviewing these mosaics, we can realize that the leaves or the trees is a pattern that the model has learned as a characteristic of the dog class. So, let's check if this is indeed an This is indeed an unwanted bias that the model has there. So, if we fit the model with this image of grass, the model predicts it as a dog with an almost 100% of probability. And the same with this image. And if we fit the model with this QTK term, the model predicts and so on. So, well, this methodology is still Well this methodology is still, this bias detection methodology is still in the early stages, so we will need to check whether these unwanted biases can be detected in other models or the data sets. Also, we would like to combine this prediction with the focus score to select this pair of images to construct the MOSEX. And ideally, we would like to detect more than one bias. That's it. Thank you. That's it. Thank you. Do you have any questions or if we have some questions? Yes, it's good. So, thanks for the talk. I have been doing some research on neural networks, tension networks, with RCT induced at 24 networks. I have seen. I have seen that integrated versions are not reliable as you have seen. So, what we did was just bootstrapping the whole process of training and computing the single cell. It's quite expensive. At the end, we saw similar results that fail and they're automatic. So, do you have any Good for me because you have synthesizing. So do you have any insight about why this and the performance of a method that has been built? Well, my opinion is that as humans, we saw what we want to see in these explanations. So I agree with you that I don't know why this, for example, I don't know why this, for example, also saliency maps are really used in this field, and they are providing really, according to our methodology, really bad explanations. However, it's true that there are metrics that obtains different, I don't know if you obtain the same, but there are metrics that obtain different rankings as so. I think the next step is to try to compare all these metrics and Try to compare all these metrics and try to understand why are methods that are better with one metrics and why are methods that are better with other metrics. I was wondering more on the practical side. So, like when you're feeding the Zyka, are you reducing the images to the original? No, I'm keeping the images they are all frame with 224 so we are keeping the original size but the mosaic at 2 times the size of 348 by 225 because we want to keep the same size of the original pattern for here for integrated gradients But study also like on the effect of the mazeline because I feel like it actually doesn't know what to answer, right? Sorry, I'm not sure. To classify. So, like, also the example before you give two examples of one class and two other examples, right? So, then, if what you're asking is trying to understand which class is the most present in the mosaic? No, I asked which the question is if one class is present. So, you want to ask to the class that there are two images. So, if there are two cats, you ask why is the cat or Why is the cut or what do you think that is a why do you think that there is a cut there? So this technique like in the model and the model has to give a prediction It's not a prediction but this feature attribution method you can ask the explanation for a specific class. Yes, but so in the model we predict something right we don't look at the prediction you never okay because You never, okay, because I would expect if it's an unbiased model, if you give one example of one class and one example of the other, the prediction should tell you, I don't know, right? If I were a model and you were asking me, is it a cat or a dog that was like, I would have like, I don't know, it's both, right? Because there are both. But we are looking to the to explain the internal representations of the model, we are not evaluating how well the model is classified. Well, the model is classifying the images. What the model has learned. I I understand the point. Yes, we did the experiment to see if the focus were correlated with the loss or the accuracy, but the Or the accuracy, but the focus were telling us different things at the accuracy. So it's not the same as the accuracy. No, of course, of course it's the same as the other. No. If you show the I'll come screen up when you get the speaker. Do you have any hypothesis on why those two perform well? Why the best performs? Best? No. What's the difference between GradCam and GradCam Plus Plus? Because it's quite that loss. Yes, no. I I read it, but I cannot take it. Voice attribution. You have channels, right? And each channel has a feature map. Right now, you compute the weight per feature map. Right camp plus plus, they kind of generalize even further, this even further, having the weights of the gradients. the weights that the gradients that are computed per pixel. That's the main difference then. Now that you know that grass leaves the box, can you direct the focus? Can you maybe mask If I remove the grass, you may have to. Because we could put the graph the dogs outdoor or the dogs indoor. So when we train the same model with dog outdoor and dog indoor both. Indoor both together, we have difficulties to train the model with new images we have. So the model has difficulties to train cat and dog only with only with 960 images. So the mosaic idea reminded me a lot of the like the idea to crop images. There's an idea like to crop images for smooth labelling. Like you you you take two images, like dog and cat, and you mix them. It should be called mix crop or something like that. And then you have like a mixed level, like oh that's 50 dog, 50 cat, and you take a random pixel for both. So like it sounds like um uh an even harder task than the mosaic. So do like mixing cats and dogs in a single image and looking if so And look if so then the prediction is not cut and dog, but like seventy percent dog and thirty percent cut, and to see if the garment is able to find the pixels of the cat. It's like the dog. So you break all the garage thing because the pixels are merged, but it's should be just an idea. It's called cat mix. Cat mix. Yeah. That's worse than I have to use. Okay, that's fine. I was thinking about the out of distribution as well because your model actually, when you pass the mosaic, your model has never seen a mosaic before. So I was wondering if you check the distribution, how it is in the butt, you could do a UMARP or something, right, to see first your latent space and cats and dogs and then put the light in there and and see where it forms in the very difficult in the distribution or you're not out of distribution and that might be one of the reasons for lying on top of other I don't know if I should I have luck image black image I think there are anything to cook. I think it's the worst thing. I can try another question. Part of the mosaic is part of the solution are there. Yeah, I think it's just might be interesting to check because if your input data is having distribution, then your model prediction might be just very noisy. Out of curiosity to know what is happening. Because you could always retrain on muzzled otherwise. You know, like it's not a limitation, it's just a further step that you could have. I will check if my mosaic is still under distribution. I will see how my model performs on those. So, back to the question that I had before: if you have an equal number of two classes, what is your model? Of two classes, what is your model gonna choose? It's just random choice, right? If you have two dogs and two cats, and you have to choose which is what is done for a cat in general, it's just a random guess. So then you could have a look at all these edge cases and try to retrain your model directly on the mosaic. The next speaker, but less than Anna for?