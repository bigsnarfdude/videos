Excuse me. My name is Jeremias Sulam. I'm an assistant professor at Johns Hopkins University, and I'm thrilled to open this workshop. There are a few of the members that should have been here by now, but there were some issues with the weather and flights, so they will be arriving shortly. But in the meantime, we're going to be entertaining you just fine with the crowd that we have here. So I'm going. So, I'm going to be telling you a little bit about some of the work that myself and some of my students have been thinking about over the last couple of years on trying to understand what the networks do sort of from a bottom-down type of approach. So I don't think I need to convince you that these tools, broadly speaking, based on deep neural networks, are somehow revolutionizing the way that we think about science, the way that we think about games, or even Our games, or even make us rethink the way that we do very simple operations, like even matrix multiplication, or perhaps folding very complex chains of amino acids into protein structures or developing chatbots and a plethora of applications. In fact, there's one application that one of my students pointed out to me. There is a competition that is ongoing for the Mathematical Olympiads. So, if you can develop an AI model. Develop an AI model that wins gold at the Mathematic Olympiads, at the international ones, you get a $10,000 price. So I don't know, but that's a good point. In any case, if you have students who are into math and want to get some funding for your lab, this might be worth the per se. Okay, so everybody here is interested in mathematical principles. Principles. And so I wanted to bring to you this quote from Richard Sutton, who's someone that many of you might be familiar with. So I'm just going to read it for you briefly. So Rich Sutton writes, the biggest lesson that can be learned from 70 years of AI research is that general methods that leverage computation are ultimately the most effective and by a large margin. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but only the thing, the only thing that matters in the long run is The only thing that matters in the long run is leveraging computation. We want AI agents that can discover like we can, not AI agents that contain things that we have already discovered. And he calls, you know, this little paragraph is taken out of a little blog post called The Bitter Lesson, because it is a bit of a bittersweet realization. Because everybody here takes pride and thoroughly enjoys making nice mathematical observations that develops. Nice mathematical observations that develops better models. But what Rich Sutton is saying is that in the long run, that doesn't really matter as much as long as you can crack up power, computations, and data. And from the, you know, on the one hand, it is what it is. It's a pragmatic news, and it might be good news in that as long as we can scale up computation and data, we're going to be fine. But on the other hand, we are left with settings where we have these very complex machinery that has learned. Machinery that has learned something very useful about the world and performs tasks perhaps successfully, but we can't really infer what it is about the world that these things have learned. So we're caught up in this sort of black box problem. And this is not just sort of intellectually annoying. It's also a matter of safety and regulation. This is the fact sheet of the presidential order that was issued last October in the United States, which is basically a States, which is basically a call to develop tools that really make these models more robust, more interpretable, and more reliable. So, I'm trying to understand what these deep black boxes have learned is really, at least to me, an important question. So, what I'm going to do is walk you through some partial answers that we have trying to understand these things. One in the context of inverse problems, which is something that's very close to my heart, and then on image classification as well. Classification as well. Feel free to interrupt me for questions if you want. There are microphones here on the table. And we can also have, I think, about 10 minutes for questions at the end of the presentation. Okay. So what are inverse problems? Inverse problems are essentially a set of scenarios where we are trying to estimate or reconstruct some underlying signal or underlying image that we. Image that we sensed, but the sensing process was noisy and was corrupted, and we only received sort of incomplete measurements. So you can think of trying to restore. See, this works. All right, so trying to restore an old photograph or, for example, performing super resolution on microscopy or even doing virtually all kinds of medical imaging. Virtually, all kinds of medical imaging are instances of inverse problems. Or, you know, maybe you went on vacation with your partner and then you're no longer together and you want to take that partner out of the picture and impaint whatever was in the background. All of these cases are examples of inverse problems. So mathematically, what we can do is model that measurement process. We're going to do it in this linear form, although it need not be linear. We get measurements y from some ground truth vector x up to some corrupted noise v. Corrupted noise V. And if you have this forward model, then a reasonable thing to do would be to try to reconstruct X by means of this problem that you see here, design optimization problem where we are minimizing the basically the likelihood, where's the pointer, there we go, some likelihood term that is forcing that our reconstruction has to be close to the measurements that we were given. And because this problem is ill-posed, there are infinitely many X's that minimize this. Many x's that minimize this to zero in general, we need some regularizer or some function that promotes good solutions. And if you're Bayesian, in some sense, the optimal regularizer to use is the negative log prior or something that scales inversely with the prior distribution of your data. Because if this is the case, then this problem here is nothing else than trying to estimate the map, the maximum posteriority estimate of the A posterior estimate of their known given year measurements. So, this formulation has been used thoroughly for decades now. And then, what prior to use is really a question of where in time are you and what kind of models of the world do you have access to. So, for example, if you go back 70 years, then something reasonable to do would have been to just set that regularizer to be the L2 norm squared of your signal. The L2 norm squared of your signal, because for example, noise contains very high energy as opposed to things that are perhaps smooth. But maybe you want to refine whatever smooth means, and so you're going to require low energy under some operator. Maybe the L2 is not ideal. Maybe you want to recover things that are piecewise constant, and so the TV norm would make sense. Or maybe you want to generalize this operator that you observe here, and then you're going to require sparsity under some wavelet coefficients. Sparsity under some wavelet coefficients. And maybe you can generalize this further and you can forget about wavelets and generalize to sort of data-driven over-complete dictionaries and then search for sparse representations under those dictionaries. And then right around the time when I was doing my PhD, of course, this is by no means an exclusive list of models. This is just a subset of them. There are many, many more. And then around 2010, 2012, deep learning came about. 2012, deep learning came about. And it was great news in some sense because it allowed us to do things that we were trying to do for a very long time much, much, much better and basically overnight. But at the same time, it made us a little bit lazy because we didn't need to think about elegant and analytical and effective models of the world anymore. We could just seemingly learn these implicitly from data, close our eyes, and these. From data, close their eyes, and these things would just perform well. So, what I'm going to do now is show you very briefly a couple of ways in which people have used deep learning in the context of inverse problems. And then we are going to try to close the gap between what these functions can give us versus what we know about all the machinery that makes inverse problems work. So, there are a couple of things you can do if you want to use deep networks to solve inverse. Networks to solve inverse problems. The first thing you might think of is to say, well, if I have access to a bunch of pairs of ground truth XIs, if you have ground truth XIs and observations YI, then what you could envision doing is just training a network. And by the way, for the rest of the talk, whenever you see F sub theta, that's going to denote some parametric function. It could be a new. Some parametric function. It could be a neural network, it could be some other algorithm, some parametric estimator. So I could give it a bunch of paired samples and train a function so that when I input a given y, this function outputs something that looks like the x that generated my y. And of course, you can do this a little bit smarter. For example, if you have access to the forward operator A, then it would be reasonable to sort of project onto the space of the forward operator and then only predict the residual that you. Predict the residual that you have. And people have been doing this for a long time. And this is, you know, sort of the most immediate thing that you might want to do with these networks. And in general, this performs okay. Something better that you can do is to go back to the formulation. I still want to have an optimization problem that I want to solve, but now I want to learn this regularization term from data. I don't want to rely on my expertise of the domain because that might be inaccurate, it might be incomplete. I want to learn this from. Complete. I want to learn this from data for a specific data distribution. And there is, there's a number of ways in which you can do this. So, for example, you could say, well, this regularizer function should tell us what is good under our distribution and what is bad. So, great, I could train a critic or a function that precisely distinguishes these two things. And so, for example, you could search over functions R so that whenever your sample So that whenever your samples come from the true data distribution, the value of R is small. And whenever these samples come from a different data distribution, the value of the regularizer is high. And once you have this function, then you can go back, plug it in here, and then solve this optimization problem. If you like global convergence guarantees, you can restrict these functions to be convex. Otherwise, you can have them be non-convex. But you can do many other things. You can, for example, train generative models for your data and then have these regularizer. And then have this regularizer basically be an indicator function on whether you are in the span of your generator or not. And there are many other things you can do. There's a third alternative, which I call implicit priors. And this is very much related to these notions of plug and play. And the motivation behind these implicit priors is the following. Let's assume for a moment that you had access to your regularizer R. If you did, then If you did, then sort of a popular thing to do, we're not going to assume that R is smooth. And so, a reasonable thing to do would be to minimize this problem via proximal gradient descent. And proximal gradient descent, what it does is it iterates a gradient step with respect to the smooth part of this loss and the proximal operator for R. And in case this is not familiar to you, In case this is not familiar to you, the proximal operator is simply the argmin, the minimizer, of this simpler optimization problem. So, here the only thing we are doing is finding, so if you give me u, I'll give you an x that is close to u, but also minimizes the value of r. And this is nice because on the one hand, you see this is a simpler problem than the one we had before. There's no forward operator. And also for a number of regularizers, well, the hope is that this Is that this problem can be solved in closed form, and you have efficient ways of computing this thing. Now, something to keep in mind is that this proximal operator is itself a maximum a posteriori estimate for your ground truth x, if you're assuming that u is just x plus Gaussian noise. Okay, so because of this, you can interpret this proximal operator. This proximal operator simply as a Gaussian denoiser. If you corrupt your measurements X with Gaussian noise, computing these prox will effectively implement a Gaussian noise denoiser algorithm. And so this connection was made about more than 10 years ago now. And this gave rise to this very large family of methods called plug and play, because the observation was that if this is simply a denoiser, Was that if this is simply a denoiser, I can then just go to GitHub and grab the latest and best denoiser out there for Gaussian noise based on neural networks or based on diffusion models or based on whatever methods you want, and plug them in here. And basically for free, you now have a new solver for your original problem. The only thing you're doing is just replacing the procs with a denoising network or a denoising algorithm, whatever that algorithm might be. And so that has led to really a bunch of really productive research directions. It is very much related to these notions of unrolled optimization and has worked in general really, really well. Yes? Just to understand this why you're doing this. Just for the zoom. Yeah, just so I'm understanding why you're doing this. So the power points. The prior point is that you don't want to assume knowledge of the prior. Correct. So that's. So you don't want to assume knowledge of the prior. Correct. So there are two avenues here. Either you can use the best of your domain knowledge and say, I'm going to assume that there exists another complete basis under which. Complete basis under which, or representation under which my signal is sparse, and I'm going to expose a sparsity. But you know, our ability to make that models about the world are limited. And so, putting my rich Saturn hat, I'm just going to leverage computation and just try to learn this from data automatically. And I'm going to assume that these inoisters have learned something about the world that make them good as a some implicit prior. Does that make sense? All right, great. Can I also ask the question? Yes, of course. So, what is the A typically in this case? Like, how do you find out A? Do you just use existing wavelet bases or what is it? Perfect. Thank you for the question. So, right now, we're assuming that A is given to us. So, for example, if we were simply in a denoising setting, the operator A would just be an identity operator. If you were on a On a super solution problem, then A would basically be a downsampling and blurring matrix. So A is just a forward operator that tells me how my measurements are obtained from the ground truth vector. And I'm going to assume that it's given to me. I see. Thank you. Perfect. Okay. So this is in a nutshell, in one slide, this framework of plug and play. But the question that we really wanted to answer. That we really wanted to answer was: I mean, great, this works well, but surely not every single network that I will grab from GitHub will give me an implementation of our proximal operator. And even if it did, I want to know what is the underlying prior that this thing has learned, if at all. And I want to be able to query it. I want to be able to characterize it because I want to understand still what are my models of the world that these black boxes have learned. So, we're going to ask these three concrete questions. The first one is: when will a given function, and in particular a neural network, compute exactly a proximal operator? The second is, if I can know that indeed my function, my network is computing a proximal operator, I want to know for what regularizer. And a question that's going to be relevant for training is: okay, if you're training this from data. You're training this from data. I don't want to have any regularizer. In particular, as I said before, I would like my regularizer to actually be the negative log, like the negative log prior, right? Because if I do this, then I know exactly that I'm solving this map formulation for my inverse problem. So we are going to answer these questions one at a time. The first one is actually not too complicated, given a very beautiful work from a couple of years ago. From a couple of years ago, this is a paper by Remy Grimumbal and Nicolova. And basically, they show the following. This is a simplified version of their result, and it says the following. Any function f will be a proximal operator for some regularizer R if and only if there exists a convex function phi so that f is a gradient of phi. Is a gradient of phi. So the picture that you should keep in mind is what you see here. So there are three functions involved. All three of them are tightly interconnected. We are interested in f. f is the prox of r and what this result tells us is that f will indeed be the prox of r if and only if there exists a function phi which is convex so that f is its gradient now the nice thing about this is that Now, the nice thing about this is that R actually need not be convex. So, I can obtain in closed form the proximal operator or approximal, excuse me, the approximal of a non-convex function r, as long as I know that this f is a proximal, sorry, is a gradient or is in the sub-differential of my convex p. Yes? Exactly. Exactly. Exactly right. So now it's going to be. So this is going to be a set. The prox of R is going to, in general, are going to be a set. And what I will promise is that my solution is going to be in the set of solutions. Yeah, yeah, correct. Correct. No, no, yes, it is. It's not necessarily strongly. Okay, perfect. So this basically answers the first question because all we need now is to make, all we need basically is just to be able to have networks that parametrize complex functions. So why isn't this working? Okay, this stopped working. So all we are going to do is have a family of convex. Family of convex functions, so rather a family of neural networks that can parametrize convex functions and only convex functions. And all we need is to take our function f to be the gradient of that function. And luckily for us, there's already sort of a growing literature on how to come up with networks that parametrize convex functions. And so, for instance, we are going to use these input convex neural networks from the group of Ziko Culture. This is already a couple of years old paper. already a couple of years old paper that tell us how to parametrize networks what architectures we should use they're pretty general so that these networks can only parametrize convex functions and then we simply take one of these networks and compute its gradient map and this will be our f and this will be by construction approximate operator for some r good okay yeah What does it mean intuitively for f to be the gradient of a context? The function that you're learning is a gradient of a context, but I don't really have the intuition for what that's saying about what you're learning. So, let me put it this way: so, the intuition is not immediate, it's not that clear. My way of thinking. That's clear. My way of thinking about this is that you're saying, look, I'm going to, if you want your f to be a prox, you can learn any function. They must be the gradient of a convex function. What that also tells you is that the R that you will learn, even though it's non-convex, it's not going to be arbitrarily non-convex. And it will have enough structure so that the primitive of F is. The primitive of f is convex. If that helps a little bit. Okay. All right. Question number two. Fine. We have an F. This F computes a proximal operator. I want to now know what is the regularizer that is hidden there for which my function is a proximal of. So on the one hand, there's a nice, well, there is an expression, an analytical expression that links all. Expression that links all three functions that links R with f and psi. And it is what it is. Don't try to parse it. It's kind of complicated. But the only thing that I want you to notice is that you could compute r if you were able to invert your proximal map. And this in general can be pretty complicated. But in our case, this is actually not too hard and it boils. Too hard, and it boils down to just solving a convex program. In fact, this we make this strongly convex by small modifications that I can tell you about later. But it suffices to see that if you just minimize this convex problem, remember this function is convex by construction, just an inner product, the solution of this will be the inverse of f. So we can indeed indeed invert f and in fact we use a conjugate. In fact, we use a conjugate gradient to do this on real networks. So, if you give me your procs, and these procs is given by these learned proximal networks that we are using, then computing R given the procs is doable. And I'm going to show you examples of giving you back R. No, correct. Yeah, yeah a little, but it's different. Right, right, right, right. Yeah, very nice. Okay. So now we've understood. So now we've understood what hypothesis class we should constrain ourselves to. We know exactly what we need in order for our functions to compute exact proximal operators. The question is now, given this function class, what function are we going to pick, right? For our purposes. And in particular, I want an f that computes the map of x given y, right? Which means that I want my r function to be. My R function to be the minus log P of X. And of course, I don't know P of X, but I'm going to assume that I have samples from this. This is what we do in machine learning. And so here's the setup. We are going to take, yeah, it's not going to work. So we're going to take samples x from P of X, and we are going to add Gaussian noise, and we are going to construct measurements Y. And we are going to search for our function. Search for our function as the minimizer over again functions that are approximate operators of this risk. And it's just the expected loss of how well my f of y approximates the ground truth x. Okay, easy enough. The question, though, is what loss function should we use? And you might be tempted, as we were in the beginning, to just use an L2 loss here, for example. And if you do that, And if you do that, you will actually recover the incorrect thing. And by that, I mean your solution will be a proximal operator by construction, but that proximal operator will approximate the conditional mean, because indeed the solution of the L2 is just the MMSC. And this is not the maximum of the posterior. This is the mean of the posterior. That's not the prox that we want. You could go to an L1, and the L1 will sort of converge to this generalized notion of median. This generalized notion of median, but this is still not the map, the maximum of the posterior. So, this is, in a sense, one of the main results in our work. We have this loss function that we call a proximal matching loss. Don't worry too much about parsing it. It's just a Gaussian, it's an inverted Gaussian function. And the width of this Gaussian is controlled by this parameter gamma. Gamma. And what we can show is that if you minimize this risk for this loss function, this proximal matching loss, which depends on gamma, there should be a superscript gamma here. Sorry. In the limit as gamma goes to zero, this function actually recovers for you the correct proximal operator scaled. Scaled by the noise level that you had in the sampling process. In fact, the result is more general than this. You don't need to necessarily use this loss function. There's a family of functions that will do. The intuition behind this, preemptively addressing the question of intuition, is that this is a relaxation of an L0 loss. Right? So in 1D, it is very easy to convince yourself that. Very easy to convince yourself that if you want to retrieve the mode of a distribution, then the reasonable thing to do is to just output the value that appears the most. And that is precisely the mode. And so this is sort of a generalization of that broad intuition. And that's why the Gaussian is a relaxation of this L0. Okay. Can I answer the question? Yes, absolutely. So in the slides, even before. So, in the slides, even before this one, you have a Gaussian example. Is that merely an example, or is that what the theorem is for? You mean in terms of how is Y constructed? Even this? Yeah. Yes. No, this is part of the result. So, when I say that this holds, I mean that the y vector that you see here is indeed a Gaussian. Is indeed a Gaussian noise corrupted version of X. So for this to be true, you need to use Gaussian noise. I see. Thank you. But I'm going to show you. So remember, this is only for us to obtain F. The moment you have F, you can apply it to a variety of inverse problems because all you care is for F to give you the map, but then you can apply it to other settings where the noise is not Gaussian. Perfect. So that is debatable. There are settings where, sure, if you know that this will be your sigma that you're interested in and you care about, yes. But for example, what if I wanted some universal prox that was scale-free in some sense? Then this is limiting. And so we are thinking of ways. And so we are thinking of ways to generalize this so that to get away from the limiting factors. Okay, let me show you a couple of examples of how this works. So here's a toy setting, but to me, at least, this was the example that convinced me that this thing could actually work. So we are going to construct, as before, Gaussian observations y from my signal x, but my signal x now will come from a Laplacian. x now will come from a Laplacian, 0, 1. And we take a Laplacian because you know the log p of x here is just an L1 norm, and we know exactly what the proximal of the L1 norm. So we're going to train our function, just as I mentioned before, restricted to this class of proximal networks. And now we're going to see what this function computes. And what you see here is in the dotted line, you see the ground truth value of the shrinkage function, which is the Of the shrinkage function, which is the correct proximal operator of the L1. In red and blue, that are very close to each other. What you see is the result of learning these proximal networks. If you were to use an L2 or an L1 loss while training the network, and these functions are proximal operators by construction, it's just that they're not the proximal that you want because they converge to different things. And in green, what you see here is the result. You see here is a result of this learned proximal network, and then the smaller gamma becomes, then the tighter this approximates the shrinkage function. And as I promised, you can now sort of invert the prox and compute R. So you can compute the regularizer, which remember is the log P of X, which should give you the L1. And as you see, the green one, which is the one learned with this proximal matching loss, sort of nails the L1. Of nails the L1 almost, and the L2 and the L1 just recover things that are sort of close to a Huber loss and they're sort of not the regularizer that you want. You can do more stuff with this. So for example, here there are examples of learning these learn proximal networks for digits. And what I want to do now is sort of understand what kind of priors has been learned from this distribution of digits. Digits. So, what you see here is a natural-looking number six, and I can query R as I promised, right? So, if I query R, I get a value of 2.2, whatever. This is not normalized. The actual value doesn't mean anything. But then I'm going to query the value of the regularizer as I perturb with more and more Gaussian noise. And you see that the value of the regularizer increases as I perturb with noise, because indeed, these things on the right are not likely under my data distribution. Under my data distribution. And so the regularizer should reflect this and should shoot up. And this is what you see in the graph here on the left. This is perhaps a more, yes, there's a question. Yeah, so you said, so I was just wondering, maybe R do mean something, because if it equals to the negative log of p, is that what r means here, or it's different? So when I said it doesn't mean anything, I meant in absolute terms. mean anything i meant in absolute terms it doesn't mean anything so all these things are scale invariant in a sense and so um the the offset of where this value is the absolute value of the thing doesn't mean much the relative change does basically you can convince of this by by the fact that you know we we you're not having to learn any partition function we are not learning really p of x i see thank you of course that makes sense yeah so So, here is another example that demonstrates that the regularizer that you're obtaining is indeed non-convex. So, here you have a nice looking 4, and on the other end, you have a nice looking 1. And what I'm going to do is take a convex combination of these points. And for the things in the middle, this thing is not a natural-looking number. It's not a number, right? It's a convex combination of two numbers. And indeed, the value of the regular. And indeed, the value of the regularizer goes up, reflecting the fact that this is not a natural digit. And what you see here on the left is what happens over a variety of samples. And this indeed reflects the fact that the R that you found is non-convex. If it were convex, you wouldn't see this behavior. And for us, this is good news because we want regularizers that are indeed adapted to the natural distribution of the data. And if my distribution of the data is non-convex, as it typically is, I want my regularizers. Is, I want my regularizer to reflect this. Here's a bonus question that I don't want to spend much time on because of time constraints, but basically, because our function that we've learned is by constructing a proximal operator, it has a lot of structure that we can benefit from. And so, if you care about solving inverse problems of the kind that you see here, that is sort of the motivation for this, we can show. Motivation for this, we can show convergence now not to a global optimum, but to a fixed point if you use proximal gradient descent or if you use ADMM as well. And this follows basically by coupling sort of our stuff with recent results for non-convex optimizations of these kind of inverse problems. As I said before, nothing stops you from doing these on digits and toy problems like Laplacian distributions. The goal was... You know, Laplacian distributions. The goal was to obtain sort of general-purpose solvers for inverse problems in imaging. And what you're seeing here are results of a number of approaches, all based on this plug-and-play idea, compared with ours for de-blurring and denoising. And by and large, and there are many more examples in the paper, the performance is basically as good as the state of the art, sometimes better. But you get the added benefit that you know that your function. That you know that your function is exactly computer approximate operator, and you can go back and query the regularizer that was learned from your data. We've also used this for sparse view CT and for compress sensing problems. And in general, this performs really well. Okay, I'm going to switch gears now and move to a supervised setting as opposed to an unsupervised setting problem. And we are going to talk about image classification. So, imagine the setting where you want to, you're given a bunch of samples, x, these samples are images, and you need to predict or rather classify each of these from whether they came from patients who are sick or patients who are healthy. And the only thing that you are given is a binary label per image. And so, we can by now develop these functions f that. Develop these functions f that predicts whether an image corresponds to a person who's sick with very high accuracy. But we want to understand a little bit more. So I would like to know, in this case, what parts of the image, loosely speaking, are important or are relevant for making the prediction that this person is sick? Even slightly more formally, and we're going to make this much more formally in a minute, I want to understand if there is a subset of features in my input. Subset of features in my input so that once I condition my sample on those subsets C, my function doesn't really change, whatever this conditioning means. So there is a variety of approaches to studying these questions of interpretability. Some of those approaches rely on notions of information theory, some others rely on notions of perturbations and gradient sensitivity, some other relies on notions of cost. Relies on notions of causal inference. What we are going to do now, and we will generalize this, will have to do with Shapley values. Shapley values are a notion from game theory where we have a game. The game here is given by a set of players, n, the subset of players, n, and a characteristic function f. This characteristic function simply maps the power set of the players to a real number. So it just tells me, given a subset of players, what is the A subset of players, what is the output of the game that these subset of players played? And so, in this context, if I want to understand what is the value of that player for the outcome of the game, and I want certain properties to hold for these values, then the solution is given by the expression that you see here. These are called Shapley coefficients. And what you need to do is simply take the sum. Is simply take the sum over all possible subsets of players, and you basically play the game with that subset SJ with player I, and you compare the value of that game to the value you had obtained that you would have obtained had you not played with player I in that set. And then you do this for every possible subset of n players. And in some sense, so this is nice because it satisfies some specific properties. Some specific properties. So, this is axiomatic in some sense. If you want these properties to hold, then this is the only coefficients that you can compute that satisfy these properties. But of course, I think everybody can appreciate that because you need to look at every possible subset of players, then this scales exponentially with the dimension or the number of players that you have. So, the first question that is relevant here is. That is relevant here is: I mean, this is in the context of game theory. We're talking about a game. What does this have to do with predictors in a statistical learning sense? So to bring that to machine learning, what one can do is, again, we are talking about inputs x in, let's say, Rn or binary responses. We're going to have a predictor F. We are, again, not going to assume anything about F. You should think of F as. About f, you should think of f as being a good predictor for a task. So you could think of f as sort of approximating the expected mean. And this is a main definition that we will use for the remaining 20 minutes of the talk. What we need is a notion of including players or not including players, right? So this is what we are going to do. If you give me a sample x, X and a subset of the features S. I am going to construct a random variable X tilde S, which has the values that I observed in X, in the S subset. And for the remaining ones, the ones in the complement of S, these are going to be random. And these are going to be random drawn from my distribution, conditioned on having observed. Condition on having observed Xs. Okay, so pictorially, let's say that you give me the sample X, and I want to condition on these features Xs. These are the complement. And so what I can do, in principle, is sample these random variables where all these are kept fixed and all of these are sampled at random from the corresponding distribution of the data condition on having observed these. So all of this relies on mean. So, all of this relies on me being able to sample from this conditional distribution. And for the rest of the talk, I'm going to assume that I can do this. You know, 10 years ago, by the way, this would have been a very way too optimistic assumption. Now, it turns out that it's not so optimistic. So, for example, here, imagine these are real input X. And let's say that here I'm setting my set of features S as a part of the face that has a smile. And now I want to construct this. And now I want to construct these random samples. I want to sample from this conditional distribution. And in this case, we train a diffusion model to do this. And all these images that you see here contain exactly the same features in XS and contain other samples from the complement so that all of these samples come from the data distribution. So in practice, we are getting to the point where sampling from these condition distributions, even in these very high-dimensional settings, is approximately correct. Is approximately correct, at least empirically. Okay, so given these definitions, the way to adapt these Shapley coefficients to these predictors is, well, now adding a player boils down to conditioning on SJ plus feature I. And here versus just conditioning on the subset of features SJ. These are random variables now, so I'm going to take an expectation. So this is the expected. Expectation. So, this is the expected marginal contribution of the pixel i. And of course, the same problems remain. This is just as hard as it was before, if not harder, because now I need to sample from this distribution. Yes, yes, yes. So, unless unless you assume something. So, unless you assume something from P of X, yes. In fact, I'm going to show you now something about P of X that's going to resolve this problem. So, precisely, the first question is, when can I get around this computational bottleneck and compute distractively? That's going to be the first question. The second question is, I want to understand what these Shapley coefficients mean, right? Because we're using notions of game theory and we're applying them to. Notions of game theory, and we're applying them to features. I mean, what do this really mean from a statistical feature importance perspective? And yeah, one second. And the third question, though, if time permits, I'll get to is how can we extend these ideas to now not explain predictors with respect to input features, but rather with respect to some abstract semantic notions that we might care about? Yes, there's a question in Zoom. Yeah. In Zoom, yeah. Um, so the notion of Shapley value you used here was based on sampling the complement from the conditionals, right? But there's also a notion where you kind of sample from, well, if you will, like the counterfactual, which is basically I sample any data point and I replace X subscript S as the values I want, which has more of a causal interpretation. Because if you do this, then the Shapley values can get affected by the. Values can get affected by the correlations between your sample features from the conditional. So, I was wondering if you had any comments on which one is better for the theory. You choose the conditional one. Yes, yes, great, great. I don't know who's speaking, but I'm going to guess this is Aditya. Yeah. Perfect. So, yes, you're right. So, this, in some sense, I'm presenting this in a very general formulation. And you're right that you can sample these from some distribution. You know, you can sample these from some distribution, and you don't need to use the data distribution. It could be something else. For example, you could fix it to be something. You could fix that thing to be the conditional mean already. And so you can essentially push the expectation. And now that's a lot easier because I don't need to sample. And like Aditya says, in some cases, if you choose these distributions accordingly, you can say something causal about what these things are computing. I'm sort of refraining myself to talk about this sort of in the most, let's say, Let's say, general form of these coefficients, because I want to show what these things will mean if you were to compute them from a conditional independence criteria that I will show shortly. Does that make sense? Yes. Perfect. It could also mean that, like, smile in your chat work. I don't know. That's a good question. So, at ETA, if you didn't hear, the question is: in the examples of the smile, Is in the examples of the smile, what would those samples from this counterfactual distribution look like? But maybe we can discuss more about this. That's a good question. I like it. Okay, so let me show you an answer to the first question, which is how to get around the computational complexity of these things. In general, there's really nothing you can do, but if you assume something about the data, then there's a lot of benefit to be leveraged. To be leveraged. So, we're going to have this assumption that I don't want to spend too much time parsing. The only thing this says is that whenever your predictor is one, there exists a feature that is basically determining the response to be one. And this might seem strong, but I argue this holds in a number of places. So, for example, in the case where we were predicting sickness or health, this is exactly true. And in fact, here the prediction is one if the patient has malaria, and we know that it has malaria because there exists Know that it has malaria because there exist specific cells that have been infected with malaria and you can detect those. So, condition of those features, the response is one, right? And in general, whenever you're doing a task that involves detecting the presence of an object, this assumption will hold. So, what we're going to do is leverage this assumption, because here, pictorially, imagine that you have two of these features that condition the response to be one. If this is true, I don't need to play a game between eight players. I can just. Between eight players, I can just make a binary split, just play a game between two players. In this case, I will see that both players are important because both contain the important feature. And so I can continue to expand this binary tree. And I will only expand the ones that contain important features and not expand the ones that don't. And I hope I've sort of in a very hand-wavely manner convinced you that this reduces the cost from exponential to log linear. log linear and under the assumption that you see at the above that you see above after you are done after a normalization you will actually retrieve the exact shaple coefficients that you would have obtained had you computed the exponential gain right but we get this by making assumptions about the distribution josh there's a question yes ladies ah that's a good question i don't know but I don't know, but it sounds like it. That's nice. I don't know. I should also say people have done this in other domains. So, for example, if you're testing Shafti coefficients in the context of natural language processing, strings of tokens also have a lot of structure to them. And so you can use analogous structure to also reduce the exponential complexity of computing these things in those cases as well. This works quite nice in practice. Works quite nice in practice. So, what you're seeing here is the examples of detecting six cells. What you're seeing is in green squares is the ground truth locations of the six cells that, of course, we don't see during training. In red are the important features that were found in this case by our approach, which is called hierarchical Shapley. And you see that they overlay quite nicely with a subset of the grand truths. These two columns are methods that. These two columns are methods that are also based on Shapley coefficients. And you don't see anything here because most of the methods that try to compute Shapley coefficients for images do these things by basically Monte Carlo sampling of this exponentially big space. And these are large images. And so if you let these run for a long time, you're still not going to see anything basically. And what you see on the right are things based on sort of propagation of gradients and other stuff. You can actually. You can actually quantify this. So, if you measure F1 score, which is basically a notion of the accuracy that you're detecting these important features, as a function of the time that you're using to compute them, you'll see that this hierarchical Shapley approach basically performs as better than the rest and one or two orders of magnitude faster than all of the other Shapley. Okay, but I want to go back to the second question, which was. But I want to go back to the second question, which was: what do these things mean? Right? So we are detecting Shapley coefficients, and these sort of reflects a notion of importance of players, but features are not players, and they're not really playing a game. So what do these things really mean? And in particular, I want to tie it to this notion of feature importance, which is a lot more common in statistics, which says that basically you pose a null hypothesis, and my null is going to say that. And my null is going to say that my features XS are independent of Y the moment I observe the rest of the features. So, in some sense, this is, you know, the features XS are not important because the moment I observed all the others, XS doesn't give me anything else about my features Y. So, what we're going to use, what we're going to do is adapt this to our notion of local importance. And when I say local, I mean an assemble. And we're going to do the following. If I have a sample from my distribution, I'm going to have the following null hypothesis, which is that the distribution of my predictor, the moment I condition on S and U, remember this is a random variable, so there is a distribution that is induced by F, that distribution doesn't change the moment I condition only on S. So if this is true, I is not important with respect to S. If I manage to reject. Respect to S, if I manage to reject this, I will conclude that feature I is important with respect to S. And I want to stress that the reason I'm using these definitions is because they allow me, if I have a procedure to test for this, then I'm going to be able to reject this at a specific significance level. And I can use the significance level as indeed a test for, or basically, a type 1 error control. And I can do false discovery error control if I want. So this allows us to. One. So this allows us to do interpretability in a very precise statistical manner. Yes? Yeah. Yeah. So at the moment, there's nothing about Shapi here. I'm just presenting to you a definition, and in the next slide, you're going to see how they are related. This is just a demonstration for you sort of to. Just a demonstration for you sort of to intuitively understand this definition. Imagine I have here four players, four quadrants, and in particular, this quadrant, the second quadrant, has the important feature. So if I condition on all four pixels, all four quadrants, the distribution that I have is a single, it's a delta function at one, because the response is one. Now, if I condition on only one, three, and four, and I sample the second quadrant at And I sample the second quadrant at random. Well, sometimes there will be a 6L, sometimes there will not be a 6L. And so the distribution of this predictor will change. And I will be able to reject this null. And I will conclude that indeed this quadrant is important for this prediction because indeed it contains the important feature. I'm not going to present the test that we have for this. We call it an XRT explanation randomization test that calls. Optimization test that basically tests for this and returns for us a p-value for this null. So, going back, why am I talking about all this if I started talking about Shapley coefficients? Here is the definition of the Shapley coefficient that I presented before. Nothing has changed. What I'm going to do is define this expectation as gamma IS. This is the expected marginal contribution of feature I. Marginal contribution of feature I with respect to subset S. And on the other hand, I have this p-value that I obtained from testing for this null. Okay? In fact, this is the expected p-value. So it turns out that this expected p-value is upper bounded by one minus this expected marginal contribution. So, why is this interesting, or what is it saying? Interesting, or what is it saying? This is saying the following. Imagine that you obtain a feature for which its Shapley coefficient is close to one. That will happen when all of these things are close to one. What does that mean from a hypothesis testing perspective? That means that you would be able to reject, because this will be very small, you would be able to reject that local hypothesis test. So, in other words, So, in other words, high Shapley coefficients imply, in this sense, rejecting these nulls for equality in distribution. And now I have the benefit that I can associate type 1 error control to Shapley coefficients. You can generalize this as lambda ways. Like basically, what I'm just showing you here is what you can say about gamma is. You can also end up, so now you're You can also end up so now you're aggregating p-values, and we know a lot about what happens if you aggregate p-values, so you can perform tests for the overall Shape coefficients. So, I'm just showing you a sort of the gist of what you can do by connecting these two ideas. Okay, and in the last five minutes that I have, what I'm going to show you is how to extend these ideas to now notions of semantics. Notions of semantic constructs. So, for example, now maybe if I have this picture of my cat, I don't want necessarily to understand the importance of every single pixel. Maybe I want to understand something more abstractly. So, for example, I would like to know whether the piano that is here is important for this function predicting that there is a cat in the picture. Or maybe, you know, I want to know whether. You know, I want to know whether the fact that I'm already, I already observed that there's a cute thing and that it's fairy, whether observing the piano gives me anything about the label that I don't already have from these quantities. So I want to try to formalize these kind of notions to test for the semantic concepts. And I want to do it in the most general way possible. The most general way that we came up with is this. Is this. We're going to have a function, and the only thing that we are going to assume about this function is that the classifier is the composition of an encoder F that maps X to embeddings H, some features, and a classifier G that takes those features and outputs scores for each of the classes. So we have embeddings, we have the predictions, and what we're gonna do. And what we're going to do is we're going to have some semantic concepts. So I'm going to have a collection of concepts C. These are going to be vectors in the same space of my embeddings H. And I will regard their inner product as the amount of these concepts that are present in the features for each input. So if I want to know, for example, how much piano there is in this image, I compute the embedding and then I I compute the embedding and then I inner product with a vector that corresponds to the concept piano, and that will give me a value that will tell me how much piano there is. Yep. Perfect. No, very good. So there's a number of ways in which you can obtain the embeddings of the concepts. Traditionally, what people did in the past is if you have a bunch of examples, for example, If you have a bunch of examples, for example, with pianos and without pianos, and with fur and without fur, and all these sort of collection of samples, then you can train linear classifiers on the space of H to detect these things. Now, you don't actually need that because you can use things like clip and language, vision language models, which is actually what we use here, where by default, the embedding space for both text and images is the same. And so, literally now. The same. And so literally now you can input the word piano into the text encoder of clip and it will map it to the same space of edge. So now we actually, you don't need to train these concepts. We get them for free in some sense. Of course, I'm assuming the correctness of flip. So what the majority of people do when they try to come up with semantic concepts is to say, ah, okay, now I have a decomposition in terms of Now, I have a decomposition in terms of amounts of concepts. I'm going to train a linear classifier, y hat, y tilde, and then I will tell you how important each of these concepts is to predicting y tilde. And this tells you something, but this is really not what we want, because this is explaining a different linear classifier. It's not explaining the original predictor y-hat, right? It's predicting a surrogate predictor. This is not what I want. Predictor. This is not what I want. So instead, what we're going to do is basically write down this, there's a lot of notation here, but this is just a generalization of the same test that you saw before. The only difference now is that the test is with respect to the variables h, which are the embeddings, and we are conditioning on z, on the amounts of concepts. So what we are asking is, does the distribution of the predictor, this is the response, does the This is the response. Does the distribution of the response change if I observe samples that, for example, have a certain amount of piano and cuteness versus if I only observe a certain amount of piano. And of course, the rest of the concepts, they come from the corresponding conditional distribution in the space of H. So again, I'm not telling you how we do this. We do this actually. How we do this, we do this actually with an online testing scheme. So, I'm just going to show you some quick results. Here, there's an image of a French horn, and indeed the label is French horn. And I have a collection of concepts that I obtained, does not matter how. And I want to test these importance in the sense that I just showed you before. And in particular, I'm ranking them based on their rejection rate, which is how often. Rate, which is how often I'm randomizing this. So, how often do I reject the null hypothesis? And because I'm doing this via an online testing procedure, I actually get another parameter, which is the rejection time. How quickly do I reject the null in this online setting? And this rejection time is nice because it induces an order of importance. The quicker it rejects, the stronger the violation of the null is. Somewhat those speakers. Somewhat loosely speaking. And you see that the concept brass is very important. You see that the concept trumpet is very important. And you get this order. And the nice thing is that now I literally have a test for each of these. So I can tell you that these reject the null, add a significance levels of 5%, and these don't. And you could add, you know, FDR control for multiple hypothesis testing on top of this if you want. The fact that you are conditioning low. The fact that you are conditioning locally on every given image is also very important. So, for example, here, if you look at the top left, there's a picture of a cocker spaniel. But because there are two, the concept of siblings that you see there is actually also very important for this prediction. So, you get to really start to query things that are very constant. First of all, they're abstract notions, but they are dependent on the sample that you're given. Jacopo, who's the student who's been leading this, actually has. This actually has a hugging phase demo for this. So you can go up and put the image that you want, and you can literally ask whatever concepts you want. And it will run these tests in the background, and it will give you a list of importance, and it will show you which ones reject the null and which ones don't. So you can experiment with it as much as you want. We call this, I bet you didn't mean that. Okay, so I'm pretty much on time. So I think I convinced you that there's a number I convinced you that there's a number of interesting problems at the intersection of, on the one hand, using these models in sort of a black fashion, but at the same time, understanding problems with specific guarantees so that we can use them in a safe and sort of sound manner. These are my students and the people that took part in the research that I just showed you. And I'll be happy to take more questions if there are any. To take more questions if there are any. Is there actually a camera here? Is there a camera? Oh, cool. Oh, yes, there. Perfect. All right. If there are no more questions, I believe that there is a half an hour break, and then we will resume at 10.30 local time, whatever that is in your time zones. Aditya, let me know if that ended up answering your question about the distributions. Yeah, that did. Thank you. All right, thank you. 