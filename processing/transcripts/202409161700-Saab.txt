Low-ranked approximation, but not in that order. So, okay, so this is joint work in part with two current PhD students, Hao Yu Zhang and Xi Hao Zhang, and a former PhD student, Jinji Zhang. There is no requirement for what a dots. And thank you to the spot. So if you know. And thank you to the sponsor. So, if you know someone, you know. Okay, they did not, uh, apparently, they're not related. So, um, the question of compressing neural networks is really driven by just the ridiculous size that they are, that they are getting to these days. And I mean, I've truncated, I've stopped updating this plot at 2022, 2023. And the game here is basically this. Here is basically this: state-of-the-art Pareto curve and top one accuracy for image classification. So, as an example, you've got a couple of models from 2022. They're state-of-the-art, 2,100 million parameters. That's a lot. To do an inference, you need something like 2,586. 2586 giga flops. So, you know, you feed it the picture, it does 2,586 Gega calculations, and then it tells you this is a plane or a cat, right? So it's kind of nuts. So then the question is, can you do something better? Is there always going to be some simpler neural network that approximates the existing one better? And the simpler question here isn't a question of, oh, count parameters and make. Of, oh, count parameters and make them smaller. That's one way you can make it simpler. It's also, can I compute with it faster? Because inference time matters. And just constructing this new network, can I do it in a reasonable amount of time? Or do I have to retrain and redo everything from scratch? Can I obtain theoretical error guarantees? So, you know, coming from the background that most of us come from, there are certain techniques that seem reasonable to try to get. To try to get these neural networks to be faster, smaller. Low-rank approximation, sparse approximation, and quantization. So, for different reasons, each one of these reduces the amount of computation that you need to do. Low-rank approximation brings you weight matrices that have fewer parameters. Sparsity allows you to multiply by zeros, which is pretty fast. And quantization allows you to do multiplications with lower precision and hopefully preserve accuracy. Hopefully, preserve accuracy. Okay, so in principle, these work possibly well together, but we won't explore that fully. So I'll start with Low-Rank approximation with a caveat that I will only present like a little sample of results. We have more, but we're still polishing the paper and I'm not ready to talk about all the results just yet. So the game, but this will give you a good flavor of what's going on here. So the game is this. I have some data. On here. So the game is this. I have some data which is organized as rows of a matrix. So it's m by n, n is like the ambient dimension. And then I have my neural network. It's just the usual composition of linearities and nonlinearities. And I would like to output a new set of weights. I'm going to call them W tilde, which are going to be Lohrang, hopefully. And this doesn't have to apply to every single one of them. Have to apply to every single one of the layers, but where appropriate, with the objective that I get the approximation error subsequently, right? First layer, then second layer, then L F layer to be as small as possible. And so the idea is you want L times R. And the reason why this works, if it's not at all obvious, is because why it works from a computational point of view is because you could just not ever form W tilde, but instead form L. U-tilde, but instead form as Rebecca did in the morning, a matrix which is a composite, a product of two linear layers. And then if you count parameters, it's much less. Correct. Correct. Okay, so this theorem is really just a hunting license for us. It won't be the best we can do, but it tells us that there is hope and that we can do. There is hope, and that we can do that we can do things. So, the game is this: I have X and I have X tilde. So, what X tilde is, is a proxy for what the activation functions are at a particular layer, having done some stuff in the layers before. So I feed my original data to my neural network, which is no longer my original network because I've approximated it, and I get x tilde at some layer. And then x is what my original object is. What is my original object? So, this is what these referred to. And I have, and I'm gonna assume that there's a planted solution. So, like, if I'm going to do a low-rank approximation, I have to assume that something like what Becca was doing this morning is true, that there exists a low-rank matrix that's gonna do a good job here. If there isn't a low-ranked matrix, there isn't a low-ranked matrix. Okay, so suppose there is such a low-ranked matrix in Low-ranked matrix in the appropriate dimensions in such a way that if I had the original object, the unperturbed neural network at a particular layer, and I look at the activations, then these are going to be equal to the activations or pre-activations of the new neural network plus some perturbation, which is to be expected because I can't expect that my LoRank approximation doesn't introduce any error. And here in this toy. error and here in this toy proposition i'm assuming that this error is gaussian with some variance and then you can of course go and solve the first thing that comes to mind which is basically find me the low rank matrix that approximates the the the only thing to pay attention to is that i'm not saying approximate w by w tilde i'm saying approximate by w by w tilde in a way that the action of the thing on the data is preserved not just in terms of Is preserved, not just in terms of counting singular values, because I only care about what it does to data. So if there's some covariance structure, it would show up here. Right, so I'm at layer five. I've done some low-rank approximations in layer one through four. I feed my data from layer one through layer four, I get x tilde. I feed my data from layer one through layer four of the original network, I get x. Oh, Z is just because it's a dummy variable that I'm optimizing over. W are the weights of the original neural network. W tilde are going to be, W tilde is my planted solution. Suppose that there exists a W tilde that is low-ranked that approximates the origin. No, I'm just looking at. So X is rho acting on X1, W1, and then there's a W2. And so it's just whatever fed through your neural network, linear, nonlinear, doesn't matter. And it arrived at a particular layer right now. So there's no assumption of linear network. Exactly. The way it's at that layer. Right. Yeah, so there's going to be, before you go to the next layer, there's going to be a nonlinearity acting on xw and a nonlinearity acting on this. But I'm ignoring the nonlinearity in this formulation because, A, it's Lipschitz and B, I have. Yeah. Yeah. Yes. Yes. Yes. Okay, okay. So, so this, as I said, this is like a kind of like a toy model to see if something can be done. So, I'm assuming that the error that is being introduced by this process that has gone on for the first four layers has created a situation where the original pre-activations are equal to pre-activations with some lower. Pre-activations with some low-rank object that I don't know, plus some error that has been introduced by the pro. Can be errors, yeah. Yes, exactly. Because these two networks are now different. They diverge after every layer, right? Okay, so the theorem just tells you that if you were to solve this optimization problem and it has a solution with SVDs, basically, then your editor is. Then your editor is going to behave like the parse student. Oh, that's great. Okay, we'll put this here. Yeah. So we're going to get an approximation that depends on the sparsity. Or the low rank. Okay, so the important thing is basically you have d over d squared, and then you have. D over d squared, and then you have a k here. Okay, so I'll come back at the end, show you some numerical results with both this approach and kind of natural generalizations of this approach. But let's stable low-rankedness for a moment and go to quantization and sparsity. And maybe this will make this a little bit clearer. So I have my train neural network as before. It's L layers. It's L layers. I want to replace the weights in every layer, so the numbers in the matrices, by negative ones, zeros, or ones, or more generally, four-bit numbers, eight-bit numbers, whatever. Typically, they come out as 16-bit floats or 32-bit floats from a training process. And so I'm going to output a Q matrix. And again, my goal is that successively, after the first layer, things are close. After the second layer, things are close, and so on and so forth. Things that are close, and so on, and so forth. And the hope is that by kind of chaining this object and making sure that it doesn't diverge after every layer, I can do an approximation of the full network without bad things happening. So some reductions. You start with a full neural network. You might as well start with the first layer, tree nonlinearity. If you're looking at L2 errors, Frobenius norms are sums, and then you can see And then you can see that if I'm trying to approximate W by Q, the terms don't intersect with each other. So I might as well just focus on a single neuron and try to quantize it. You might say, okay, well, this is easy, except this is an NP-hard problem because I'm working on the corners of the cube. So even in the single neuron, single layered problem, this is still bad. So we're just going to do the second most naive approach. The most naive approach would be. The most naive approach would be to say, if my weight is 0.7, send it round it to one. The second most naive approach is to say, well, what I actually care about is w1x1 plus w2x2 being approximated by q1x1 plus q2x2. So every time I make a mistake, in the next step, I try to correct it. Pretty simple. And so we end up with constructing Q sequentially and in a greedy way, where T is the where T is the current kind of step in my greedy method, and it goes from one to the length of the neuron and not. So you end up with a nice, easy to write algorithm. This is just the rounding function. And instead of just rounding my weight, I'm rounding my weight plus a correction term. The correction term, you can think of it kind of loosely as an inner product or a projection rather of the error at my previous. The error at my previous step on my current data vector. So, because the only thing you can correct is errors in the direction of your current data vector. So, rounding of this is pretty straightforward. And then you update your header vector. So, the new header is the old editor plus the header you committed with your current decision, QT. Okay, so detour into computational complexity. If my data matrix is size m by n, this is O of M operations because it's inner products in M dimensions. This is just updates in M dimensions, and I have N steps. So my computational complexity is O of M and my data matrix is size M and so if I just wanted to read the data, I'm paying constant times times that. All right. All right. Uh okay, sparsity. Easy, easy intuitive sort of modification. Instead of quantizing this object, I'm going to just enlarge the zero bin. So normally, if you were just rounding, you would say anything between negative half and half goes to zero. So if you instead soft threshold the argument, then things like negative. Argument, then things like negative 0.7 to 0.7 get mapped to things like negative 0.5 to 0. And then they go to zero. So by inserting like a soft thresholding function between the quantizer and its argument, you're encouraging sparse solutions. And everything else stays the same, update-wise. And you can write this down in sort of a rigorous way as a solution to an optimization problem. To an optimization problem that involves basically L2 norms of headers and L1 norms of the bits. So this comes out rigorously, but it also has this nice intuitive interpretation. Yes. Absolutely. Absolutely. 100%. The alphabet has to include zero. Yeah. To get any sparsity. Okay. Okay, so that was one layer. Now we need to deal with multiple layers and we need to deal with it rigorously. So the issue that maybe I should have brought up first is the thing that Ren√© was mentioning. So after I deal with one layer, my output activation is white tilde, let's call it, just so that I don't reuse X. And so now I have phi of XQ. My original object is phi of XW. So when I get to a, now this little Q. Get to a now. This little Q is a neuron in the second layer. So it's not a column of these guys. It's something new. And this W is again a neuron of the second layer. I want YW to be as close as possible to Y tilde Q. So now all of a sudden I have a mismatch between the vectors of my data. So that's one issue. And one way we're going to get around it along with being able to prove things in this setting is to introduce a stochastic version. is to introduce a stochastic version of the quantizer in order to avoid an adversarial selection of data. Because some evil person could decide to, knowing exactly how we're going to quantize, could decide to put all the headers orthogonal to us. And then basically we always add. So we defend against this by making random choices of the bits, but we don't make any random choices. We make them in such a way that the expectation of the argument of our Of the argument of our quantizer is the argument. So if my alphabet is negative 1, 0, 1, and I want to quantize 0.7, there's a 70% chance I send it to 1, and a 30% chance I send it to 0, so that on average, I got the right number. Okay, so now the algorithm. I'll present a numerically expensive version of it first, and then a quicker version in a second. In a second. So the trick is this. I'm going to do a data alignment thing to get past this problem of having a y tilde and a y that are unaligned. These were the vectors coming from the previous layers. So what I'm going to do is I'm going to find a fake set of weights. These fake set of weights are going to be such that if I feed them to my quantized, if I feed my quantized data through that fake set of weights, it creates the same. Fake set of weights, it creates the same output as my original network. And once I have that fake set of weights, that's the set of weights that I'm going to use for the quantization rather than the original weights of the network. So this little trick recasts the problem in the same setting as before with the stochastic quantizer with the same set of updates as before. So there is nothing numerically expensive here. They're solving the linear system here. There's solving the linear system here that we don't like. So, let me present a couple of quick theorems about this, and then we'll get rid of that ugly linear step. So, here, top theorem is basically just telling you that the infinity norm of stator, meaning on the activations, is going to be bounded by some quantity that is in some sense out of your control. In some sense, out of your control, times the square root log factor. And this gamma is basically just determining your probability. And then you string together all these things with triangle inequalities and Lipschitz constants. And you end up with a theorem that tells you that for any layer, so anywhere you want to look, in particular at the end, the header between the output of the neural network and the quantized neural network is bounded by something that Bounded by something that looks ugly. Apply this theorem to a reasonable setting, like if the weights were Gaussian. So weights are Gaussian. It's just something to calculate to see if these error bounds are nice. You can also justify it by looking at Daisy training and saying I start somewhere Gaussian and then I end up somewhere close by. So, okay, what do my relative error bounds look like? Well, Well, after running through the calculations, you end up with something that looks like square root m log n over n. So size of theta, size of the neuron. So this is like something that decays with overparametrization. Delta is basically how fine your quantization is. So if your alphabet has more elements, delta will be smaller. If your neuron is bigger, the header will be smaller. So overparametrization is good in this setting. In the settings, yes, yes. Yeah, so the catch, and that's an important one, is that this holds for the training, the data that we use to do the quantization. We can prove analogous results for generalization if you assume either that data points are close by or that you're in some low rank. Or that you're in some low rank setting. In that case, m's get replaced by the rank of the of the data. Okay, so let's get rid of the let's quickly get rid of the linear solve. So what we are maybe working our way backwards here, this looks exactly like the stuff we've seen before, except now I have this index r on the w tildes. X R on the W tildes, and that's just to indicate that this is the output of some algorithm that's been running for R loops. So, what is the algorithm? The algorithm is basically just saying, do exactly the same kind of optimization problem that you're the greedy optimization that you're trying to do to replace W by Q, except forget that you have a quantizer. So, just basically say, So, just basically say what is the real-valued weight that I can replace my current weight by, so that the data that is in the direction of y and the data that is in the direction of y tilde aligns as closely as possible. If you do this once, then you're doing this for r equals one. If you do this, if you loop over it and keep correcting errors, then you have something for whatever the number of loops that you have is. That you have is. And so the reason why this is kind of a nice way to solve the linear problem is because each one of these things is each one of these loops is O of Mn. So if I do it R loops, I'm doing O M N R times. And so I'm still on the order of the size of the data matrix. And the nice thing about it, even though the theorem again looks bad, it looks ugly, is basically that you have Basically, that you have in the error bound a projection term raised to the power r. So, this is gonna be, this is gonna have singular values less than one, you're raising them to the power r, you're sending this to zero. And it shows up when you do the reductions. Okay, so in the last couple of minutes, some toy examples. So, I was waiting for bigger experiments to run, but this is all I was able to. This is all I was able to do for the lower end case. So, this is completely a toy, but it captures what's going on. So, the blue curve, rather, going on the x-axis is kind of a proxy for the rank of the new object, except aggregated over the whole neural network because we're doing it for multiple layers. And then the y-axis is accuracy. And basically, you're comparing singular value decomposition. Comparing singular value decomposition, which is just take the weight matrices and replace them, to the thing that I presented the proposition about, which we're calling one layer linear. And then you could start incorporating the effects of future layers in more sophisticated algorithms, which are successively more computationally intensive and successively better. But they follow the same idea of kind of trying to make the data guide you as to how to do the lowering. How to do the lowering approximation. So here, yes, yeah. So when you look at a point on here, all of these things end up with the same number of parameters. So this is like 6% of the total number of parameters. But I mean, MNIST is easy and it's, so I wouldn't read. And it's so, I wouldn't read too much into the numbers down here, just more that you can get pretty close without having to go full array. Good. Okay, quantization stuff. What we have here is different curves for different size alphabets, four bits, five bits, six bits. Solid and dashed for whether I'm doing the deterministic algorithm or the stochastic algorithm. The stochastic algorithm lets me prove theorems because it avoids adversarial settings. The deterministic one is going to be better in a lot of cases because it's quantizing 0.7 to 1 and not to 0, right? So, but it's not uniform. I mean, there are places where the stochastic wins, and the stochastic can be rerun multiple times to get something better. This is one. To get something better. This is one run. So, the important thing to point out: x-axis is the size of the data set. So, x is m by n. This is how big m is. This is just 2 to the 9, 2 to the 10. So instead of working with all of ImageNet, I'm only working with a thousand images and just using these thousand images to quantize the network. And then we end up with pretty good accuracy. With pretty good accuracy. We don't have the compute to go further. Yeah. No, no. The uncle quantized model is the solid line at the top. It's the flat line at the top. Yeah. This is generalization. Yeah. Yeah. Yeah. Yeah, this is like very surprising to us. And I mean, we have, as I said, we have these generalization results for toy examples where like data points are close to data points or you have low rank data. But this works on like these real ImageNet examples and it generalizes and we're trying to understand what's going on. So this is just to show that it's So this is just to show that it's sparsity is also a good tool in our toolbox. What we have here is the lambda is just whatever is in the soft thresholding. So as it grows bigger, it's sending more things to zero. And then I think this is a five-bit quantization. What we know, and it's on one of these, I mean, to me, big, now no longer that big, VGG, 140 million per. In 40 million parameter models. But basically, the green y-axis on the right is telling you how sparse the solutions are as lambda changes. And then the blue curves are telling you top one and top five accuracy, top one and top five. And so what we can see, for example, is that at a 50% sparsity level, we barely have any loss in accuracy. You basically went from 32 bits to 5 bits. To five bits, and you set half of your weights to zero, and you barely lost any accuracy. So these things actually do work pretty well in practice. We've been trying them on some of the large language models that we can handle with the help of some collaborators in industry, and they seem to do pretty well there, too. So some questions that So, some questions that, as I said, are on our minds are generalization. I kind of hinted that instead of looking just at a layer by layer kind of narrow myopic view of what's going on, maybe if you kind of try to look ahead at what the next layer wants to do, maybe you can do better. And the early numerics seem to suggest that that is a good idea at the cost of computation. At the cost of computation. And then Dustin asked the question earlier: is this the same architecture? We want to know if we can do simpler architectures and still get the same types of results with rigorous guarantees that are kind of unlike the student-teacher type models that are common in computer science. Okay, I'll stop. You mentioned that over-parameterization quantization. Is there a philosophy underneath all this that over-pragmatizes Yes, I think so. I think so. Yes, I think so. I think so. The more I overparameter, the more compute. And then when I just pawn the data I take away the compute. So then, how do I stop this now? So, yeah, that's a great question. I mean, the way I kind of reconcile it in my head is basically that the truth. Is basically that the training is much easier, even though it's computationally intensive when you have this massive overparameterization. And kind of for free, you get that the massive overparametrization allows you to do these model reductions cheaply and easily. So you pay the price at the training side to get your nice high accuracy model with your multi-billion parameters. And then you have a chance to like. And then you have a chance to like bring it down. I am just huge problem. Yeah. Absolutely. Yes, so there are this ties into something that we're also thinking about, which is this notion of fine-tuning models that a lot of people do. So there are results out there, like mainly numerical, where they have some. Numerical, where they have some approach to approximating a matrix with something low-rank. So they start the usual way, then they get their low-rank object that approximates it. It does pretty poorly. And then they take that thing as an initialization for more training. And then they get something that sometimes does even better than the original model. So that kind of plays into also the question that Dustin is asking: that sometimes the low-rank model can be even better. Model can be even better than the original. We just don't know how to find it in the original training. And so we kind of have to post-process somehow. But once we know, then