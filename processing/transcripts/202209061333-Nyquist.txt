This is going to be a lot of talk where there's no mention of rough paths. And the point of it is also not to get to very specific results from us that I'm excited about. It's more about trying to convey a research agenda that we started and we think could be promising and sort of plant that seed and sort of motivate that research agenda. In the end, I hope we'll get to some of the stuff we've done, but it's mostly about sort of giving an introduction to this and trying to tell you why we think this is useful. I hope this is useful. So it's based on work, ongoing work, past work with a lot of people. So a lot of it dates back to work from I know with Marc Pelladier, who's the person in the red sweater, if you don't know him. So he's the one who got me into thinking about gradient flows and stuff related to that. So his name is not on these papers, but it's sort of his influence is sort of in there. And then it's with Colwyn Jew Colwyn Dew, who's at UT Austin now. Henry Cole's the fancy picture, that's the one they took when he was made professor. Picture, that's the one they took when he was made professor. He's at KGH, and then Adam and Victor are both PhD students at KGH right now. So it's based on sort of work with all these people. And I wanted to start by talking about sort of what are large deviations. I'm not assuming that you're very familiar with this, sort of try to motivate their applications. So, what are large deviations about? It's about things like this. So, this is a black gap in Lower Manhattan, or something else that some of you might have experienced on the way here. You might have experienced on the way here, you know, things like this. And jokes aside, so what are these? I mean, you imagine that you have some kind of probabilistic model for these things, and every now and then things go awry and things clash. So if you think about the airport example, okay, that's a queuing system. And you might ask, okay, what's the probability that the queue length exceeds some threshold that it's not supposed to do in a given time? And you want to sort of investigate, okay, what's the probability of that? If I want to counter that, I'll do it. Probability of that. If I want to counter that, I'll do any of these things. Same for the blackout. So, a couple of years back, there was a large trend in the sort of applied probability community of modeling power groups. And one typical example is that you have some kind of diffusion model underlying this, and when it exceeds some threshold, you will have blackouts. Okay, how does that happen? When does it happen? Things like this. Especially if you have network models of that. So, marge deviations is essentially about answering these questions. So, if we do it formally, don't stare at any quality. Formally, don't stare at inequalities. This is just a formal definition. So you have some collection of random objects, this XN sequence, and we say that that sequence satisfies the large division principle with some function i, that's the rate function, that's the important thing here, if it satisfies these inequalities. So ignore those. The gist is this. Take a measurable set in your state space. Then for fixed n, the probability of my xn object rising the set is roughly e to the minus n. I look at the rate function and I infinite. I look at the rate function and I informize it over the set G that I'm interested in. So if you go back and you think of the examples, G would be sort of the set where you have a blackband. So when your process exceeds some threshold, that would be your G. And the point is that we can compute, I mean, we get approximation of probabilities like this. That's not really the point of the theory. The point is, the second is that if I can find this minimizer, that's going to tell me how things happen. And being sweet. And being Swedish, I would be remiss to sort of neglect that this sort of originated Carlo Kramer, who was a Swedish mathematician, probabilist, statistician, early 1900s. So a lot of the lustre theory just stems from his work. This sense of pride from Swedish people. Harold Kramer. Yes. So he was, well, at some point he was the head of Stockholm University as well, but he's got so he was an actuary doing analytical number theory. Actually, doing analytical number theory, then transition to probably alternative statistics. How much of that is the number? Came out would be the sort of speedy version. So I wanted to just give you an example. What do I mean by tells you how things happen? So if you've seen this, that's fine. So let's do Schilder's theorem, sort of simpler example. Take standard Brownian motion, scale it with some epsilon, and I'm going to let that go down to zero. And you could ask this question: well, what's the probability that when epsilon becomes small? Probability that when epsilon becomes small, that I'm going to exit some size minus, say, C to C. I put two thresholds, I start in zero, okay, what's the probability that I exit in time zero to t? Of course, these are Gaussian things, and so we could compute it, but imagine that we couldn't. So we're not interested in that, we're interested to see what we can say from a large deviation perspective. And the more interesting question is that, oh, if it's going to exceed, how is that going to happen? And if you sort of take this Large Deviation picture, the answer is this. Well, I have a Large Deviation rate. Well, I have a large deviation principle here. The rate function is just this. So you take absolutely continuous paths, take the derivative, square that, and you integrate over the time. And then roughly you have this, if I'm going to exit d with some fixed epsilon, it's going to be e to the minus 1 over epsilon times the informization problem, where I have the rate function, and I look at predictors that start in zero, and at some point it reaches the boundary of the set D. So it turns out if I So it turns out if I solve this, I can essentially approximate the 12 relative. And you do this, and you see, okay, it's going to be linear trajectories, plus minus, depending on which one of the boundaries you hit. Okay, so if you look at this, so these are, I did 50,000 samples, and the probability is roughly 10 to the minus 4 to exit. So you pick this as 100 samples of those, and then you pick out the ones that actually exit and plot the linear trajectories, and you see this. And if epsilon becomes smaller or C becomes larger, it's going to just condense to a linear. Logic is going to just condense to linear functions. So that's what I mean when I say that, okay, logic agent tells you how things are going to happen. And why is that important? Well, let's say you want to design an algorithm and you're worried about things going wrong. Then you want to know, okay, how do things go wrong so I can counter that? That's sort of the underlying idea using knowledge tradition in these settings. So first, questions on this. Wait, can you explain the how again? How does it give the how? How does it give the hell? So, when you find the informizer, that sort of tells you that, okay, when the random objects do whatever you want them to do, they're going to be very similar to this deterministic object. So, in this case, it's this, right? That this is the interimizer. And the more I go to epsilon going to zero, if I sort of increase the threshold, that's the same thing here. They're just going to be closer and closer to these things. So, in the limit, they're going to converge to this. That's sort of the Gibbs principle. That if this is going to happen, I go to the limit, epsilon goes to zero. And I go to the limit, epsilon goes to zero, then I'm going to converge to this, the deterministic object. Does that make sense? So it's sort of a large numbers, it's conditional large numbers, essentially. So then why do I care about this in the context of data science? I mean, you could do this here, but what's the data science part? Well, the first thing is this, that if I have the Delhi-deviation rate function for some complex system, you could say, well, if I sort of probe the rate function, I could potentially characterize. Function, I could potentially characterize how different parameters affect performance or convergence in these things. One thing that's sort of motivating this, this because of my background, large deviations have been used very successfully in the design of Monte Column methods. If you do sort of rare men sampling, they're all based on large deviations of ideas. So, I'm not going to sort of say what it is. I'll put a bunch of references there if someone wants to look at it afterwards. But I can explain what it is essentially. If you want to. If you want to do, for example, rare event simulation, so estimate these probabilities of long Q lengths. The probability is hopefully going to be small, otherwise, your system is very ill-defined. And if you want to do that correctly, you say, okay, what's the way I'm going to have these long Q lines? You pick that out by ideal architectians, and then you sort of build that into your algorithms that we want to reassemble this. And that gives you a more efficient way of estimating those probabilities. And you can turn that into essentially solving a Hamilton-Jacobi equation. Solving a Hamilton-Jacobi equation that attaches to the rate function. There's a lot of steps in there, and let's ignore the details, but there are those references. But you turn sort of this sampling problem into essentially looking at PDs. And in the sort of more MCMC type problems, you can say it's the following. Okay, so George, you mentioned yesterday that, okay, looking at mixing times is maybe not the right object, because there's a lot of things. But why are mixing times not sort of directly correlated with convergence? Correlated with convergence, right? It's sort of convergence of operators rather than convergence of the objects you're using. You could say, well, let's look at the, I have a Markov chain. Now I look at the empirical measure of that Markov chain, and I look at the convergence of that explicitly. And you can do that, and you can sort of do large durations for that, look at the rate function, and say, okay, what's it going to be like if I don't have convergence up to some time point? How is that going to happen that you can't do it? So that's how it's used in MCMC. So that's much more recent. That's like the past 10 years it's started. Like the past 10 years, it's started being used in the MCMC community as well. But that's sort of a motivation for trying to use it in a data science context. Like Monte Colum methods, we know that efficient methods are tied to large agents, sort of ideas. And then even more so, so this increased presence of not only Wassage time, but gradient flows in general in the data science, I mean in theoretical data science, in theoretical machine learning. That also sort of, at least to me, tells you, okay, you should really look at the architectures of these systems. At large relations with these systems. So, why is that? So, let's first sort of give some examples of this. So, starting five, six years ago, there's a lot of work where you sort of phrase sort of learning tasks. I'm not going to specify what those are. You can take a bunch of examples. You phrase them in terms of bus line gradient flows, and then you use optimal transport type methods to analyze them. So this paper by Shusa M. Bak from twenty eighteen is one of these really good papers sort of phrasing a lot of different problems in this little setting. Of different problems in this setting. So, other examples like these famous papers from 2018, where you look at neural networks, send the number of parameters to infinity, and you also send the number of steps in your SGD dynamics to infinity. And what happens there is, okay, you look at the parameter, you say, okay, I consider these as particles, because they're interacting because of the structure of our neural networks. Look at those numbers of particles, form an empirical measure, send the number of particles to infinity. And as it does, as you do that, the empirical measure converts. Does after you do that, the empirical measure converts to a solution of a gradient flow. Don't bother with the sort of what these things in the gradient flow equation mean. This is just to tell you, okay, there is some equation going on there, and this V theta, they relate to the dynamics that you put on your SGD. So this is what it's phrased like in Grant and Eric's paper, so Ratzko Fundamine. And you have these three papers appearing essentially a month in between each other. And I mean, some of you, I guess, you don't know Justin from Oxford, so you can talk to him and hear the story about. So, you can talk to them and hear the story about that. I've heard it from Carlstas, it's pretty funny. But you had all these sort of essentially deriving the same kind of result at the same time. Another example is if you go to more like algorithmic game theory, but this is tied to training generative models. So, there you're interested in finding, well, first you say Nash equilibria. You can't really do that, so you say, okay, let's find some mixed Nash equilibria. I'll tell you more about that later. And there's a paper from 2020. There's a new 2020, there's a new paper where you say, well, let's try to approximate this by looking at particle systems. So each strategy is a particle, and you impose dynamics on those, and then you look at the limit there, which is again going to be a gradient flow. So look at something like this. Again, don't worry about the Ds and these things are there for later. But this is sort of the common thread. So you have similar papers. So Grant Rotskoff and co-authors had another paper in 2019 looking at something similar to this. Yang Kim Liu and Yang Feng Liu and Yufeng Liu, they also have a paper from 2018 that's sort of doing this. We have like diffusion dynamics, but also bird depth dynamics, and you go to sort of the gradient flow limits there. This is just to try to convince you, this is something that's happening quite a lot in old data sites, traditional machine learning. And then the question is, how's that tied to large devices? Because that's not apparent. I mean, gradient flow is this whole separate quadrature. So I'll try to convince you a bit about that, but then get to some. You a bit about that, but then get to some new results. So, just to set the terminology, lost time, v2 distance, I think we're all fine. But then we can talk about the metric derivative and local slope. So, if I have a functional on the space of probability measured second moments, and I could talk about absolutely continuous curves in that space, right? And the metric derivative is what you think it is. You take one of these curves, look at the distance for H-length interval. H length interval, and you sort of send that to zero. And the local slope of your function is sort of a similar thing. And then we say that rho is a gradient flow with respect to this functional f, typically called entropy or energy, we talk to, if it satisfies this equation at the bottom. Like, this is how you characterize the gradient flow. And you could think of this as this is a natural, if you think about how you would define like a gradient flow in a Hilbert space, this is sort of a natural thing to do. So think of an OD in a Hilbert space, you can write it on integral form, and then it's sort of similar. You can write it on integral form, and then it's sort of so multiplicated. So, that's what motivates this. And so, how is this? Again, how is this tight to archives? Well, think about the Brownian particles again. So now I don't scale in with square root of epsilon, I scale them with square root of 2. This is just to get rid of the one half in the heat equation. So we know that we have the Fokker-Plac equation for this. And the point is that if I take the empirical measure of my particles, send n to infinity, that's going to converge to the solution of this equation. And then in 1998, so there's this famous JKO paper where they phrase this, I mean, not only this, but evolution equations like this, as fossil-fine gradient flows, saying that, okay, in this case, the energy function of the f is going to be just the entropy. And then this is going to be precisely a Vusasigne gradient flow. Oh, yeah, we can see. Yeah, so that's just what you I mean this is this would be. Yeah, so that's just what you. I mean, this would be the trajectory, so the other project. Yeah, yeah, yeah. And what then happened, so in 2011, so Mark, and I think it's Johannes Simmer and Adams is also on the paper, they made this observation that if you look at this, again, this Bramian particle system, where you know sort of if you want to do Large Techniques, there's going to be a relative entropy term that gives you the rate function. And they looked at this and they sort of say, well, let's look at At this, and they sort of say, Well, let's look at the row is the solution to the Fokker-Planck equation. And for small times, it's going to behave like the thing you see on the right-hand side. And from there, they sort of made a real sort of made these steps of, well, you really phrase the LDP grade function on this form. You can write it on that form, which is now, this is precise on the gradient flow form, where you sort of identify the metric derivative and the local slope for determining what functionals you should take for this slope. So that's sort of the gist of that paper, or sort of the subsequent paper. Of that paper, or sort of the subsequent papers as well, that if you have the rate function, that sort of tells you what the gradient flow structure for your particle system is. You can sort of go back and forth a little bit. And you know, I mean, there's, if I have a particle system, there are a bunch of ways I can formulate a gradient flow structure. And the idea is to say that, well, if I look at large deviations, that tells me, in some sense, a very natural gradient flow structure. Because this doesn't only tell you about the limit, that's the low large numbers, it tells you about fluctuations as well. Fluctuations as well. So the Washington gradient flow in this case incorporates information about the fluctuations, which you don't have if you do some other type of gradient flow. So that's sort of the underlying idea. Sorry, U is the measure called U is a plot. Nu is a plot in measure space now. So the usual one, like the gradient for that I know is the Leon's third that is sitting there on the spire. Yeah, so this will not be Lyon's third necessary. So this is a different gradient for you. Yes. Yeah, you can talk. Yeah, you can talk about sort of these things, how they may. I mean, if you want to formulate like HB equations on this, then that becomes important, sort of how you define the derivatives of these things. But that's the gist that if I have modifications, I can sort of go back and tell you this is the great control that you should use. So Mark has excellent notes on this, and there's a lecture series from Bonn, and I think it's online, but that's really nice if you want to sort of delve into this. But that's sort of okay, so that motivates it. There's a lot of gradients. Physics, there's a lot of gradient flows in machine learning. Launch deviations is sort of, I mean, to me, it's a more natural way of getting to the gradient flow structure. That's how I'm thinking, why don't we look at launch deviations in the data science context as well? So I'm going to give you, I'll spend a few minutes on some results along that route. So first is this looking for mixed equilibrium points in two-player zero-sum games. So this is the problem setup. You have some cost function or loss function, and then how you want it. function or loss function, depending how you want to call it, it's L. So script X, script Y, think of these as just Euclidean spaces now. So this comes from a paper by Domingue Enrich, Grant Rothskoff and others. So there they look at X and Y are some manifolds, but they're essentially Euclidean spaces. You can think of them that way. You can do this for manifolds as well, but it's easier this way. And so if I have this, I look at a zero-sum game, and Nash equilibria is going to be a point that satisfies these inequalities, right? So if I plug in x star, y star, if I change either one, I'll get. Star, if I change either one, I'll get an upper bound or lower bound. So I can't really do better. And the problem is that these typically don't exist unless you have some sort of convex concave function, or convex concave. So an alternative to say, well, let's lift this to the space of probability measures instead. So you would have a probability measure on the potential strategies that you have. And then you can talk about mixed Nash equilibria. So if you define this function L, capital L, which is Function L, capital L, which is just integrating your loss function according to the measures that you pay. And then you have a mix-nash equilibrium if you have sort of the same inequalities, but for this sort of lifted loss function. And again, the point is that these are not guaranteed to exist, but mixed Nash equilibria or they exist in much greater generality. And they're also more amenable to computational methods, so trying to approximate them rather than to find some pure Nash equilibrium. Find some pure Nash equilibria. And so this is a common task that appears in algorithmic game theory. Trying to find an approximation of M and E. Sorry, so a mixed Nash equilibria is when I'm a player, I use randomization for first part of the step. Yeah, so you lift it through space of property of measure first step. You can sort of see how that's kind of a relaxation, right? So you can sort of imagine that that's going to give you more existence. And you can think about sort of how do I quantify if I do enough. Think about sort of how do I quantify if I do an approximation like this. One common thing is used in Nicaragua's order error, which is defined like this, but it doesn't matter. But the point of this paper from New York 2020 was that, okay, let's use the particle dynamics for this. So you specify the particle dynamics like this. So essentially, without this, it's sort of a natural thing you would do. It's just a gradient descent with respect to this L function. And then you add this diffusive term to sort of just get you out of local. To sort of just get you out of local minimums and these things. Okay, so you look at this and they look at things and when n goes to infinity, saying, okay, if you look at the empirical measure, it's going to converge to this gradient flow where you have sort of these v functions, which are now, you lock in one of the measures and you lock in one of the variables, and then you integrate this with the depth. And so this is what shows up in the, it's sort of the natural thing that should show up. And when you see this, and then you some other things. When you see this, and they do some other things as well. They look at the Nikado, the NI error or what happens with that as an incost and so forth. And we looked at this and said, okay, but what about the large divisions? At least to me, if you have a particle system, it's very natural to start thinking about fluctuations. If you have some older McKim-Blassep type results, what would that look like here? So it turns out, so you do the following: you have the particle system, and then you define the empirical measure that takes both components. Rather than keeping them separate, which is Rather than keeping them separate, which is what's typically done, you track both components, and then you can say, sort of, this is not a difficult result to get. If you sort of know the theory and knowledge, it's sort of automatic, that if you have some modular regularity properties of your loss function L, then you're going to have a large deviation principle. The rate function, I have it specified in a slide, it's two slides to define it, so I don't want to try to show it here, but it's it's sort of a control problem that defines it. It's sort of a control problem that defines it. Now, what I said earlier was: well, you want to use the rate function to try to sort of tease out information about your system. Okay, so if I have a very control problem that's sort of difficult to even write down, that's going to be difficult to sort of try to tease out information from. So you want to write it in some other form. So what you can do is the following. So instead, go from looking at these as elements in the space of probability measures from paths, and you say, now I look at them as. And you say, now I look at them as elements, paths, and space of probability automations instead. I mean, this is a very sort of subtle distinction, but if you do that, you can rewrite the rate function on this variation form instead. So this is much closer to if you do like Feiden-Winslow theory. This is the thing you would get. I mean, really, this is you look at a difference in the norm of a difference squared. You just define the norm in a suitable way. And with the generator that's sort of attached to controlled SD underlying it. And the hope is then, so this is where we are, that you can actually use this to try to probe things. So, for example, you could look at what's the impact of the parameter beta here. So, beta is this thing that you set on the diffusion term that's going to tell you how much is the diffusion going to affect my dynamics. And if I send it to zero, okay, then I just have my gradient to send. If I let it be large, okay, then the the diffusion is really moving it around, but you're going to have slower convergence because of that. So you can sort of tease out, sort of kind of balance on how this is going to affect the convergence. On how this is going to affect the convergence. And so you could get these other things that when you have the rate function, you can get that, well, you get it for free, that it converges to this gradient flow solution. That's just automatic because that's the, if I don't put any constraints in the optimization of the rate function, this is what gives you zero. And that's low large numbers you get from that. And then from that, you can sort of do the NI error as well, saying, okay, it's going to converge to the NI error of this. NRI error of this minimizer of the rate function. So you get these results for free rather than having to sort of go through all the hoops of getting them. And as I said, so this is important to emphasize here, once you look at the system in the right way, you can use existing results to get the large deviation. So the point is not sort of that, okay, it's super hard to get these. It's more okay. This is another avenue of trying to understand these systems a bit better. Questions on this? Questions on this? So once you get this rate function, you get convergence, you get the existence of an invariant measure, and you get convergence. Can you get a rate? Yeah, what it tells you is you have this exponential rate. But what's in the exponential is this rate function, right? So if you can't really for specific problems, you would like to compute that. And that might be difficult because that's solving a variational problem. Difficult because that's solving a variational problem or solving a Hamilton-Dakovi equation. But there is sort of, in an abstract sense, you have the rate, yes. Can you just show again what was the definition like a particle? Oh, the particle dynamics, so it looks like this. So the point is really that you sort of collect these into being just, I mean, sort of meta-particle that takes these as both components, and then you sort of say, okay, then you can see once you define the sort of the right mapping. Once you define the right mapping, this mapping B, then you sort of use the things that exist. That's the problem. The problem, the loss function is dissipated. So you can get the existence of an invariant metric for any data strictly possible. Yes, but then sort of you get into sort of issues when you want to say like convergence in time, then sort of beta has to be above some limit. And you can sort of see that here as well, that that unless you have it above something, you're sort of going to kill the convergence. You're sort of gonna kill the convergence because you're just gonna get a rate. Yeah, I mean you're sort of gonna spend too much. I mean, you're spending too much energy on either sort of diffusing or not diffusing, and then you sort of get thresholds and data from that. I finish including questions. Good, that's perfect. So I didn't intend to go through everything anyway. So let me just end them by saying something else which is completely different, but this is stochastic approximation library. So this is again, I'm just trying to convey that large deviations might be useful tools in this. Large dimensions might be useful tools in this. So, jumping into stochastic approximation, what's that? Well, we'll look at systems of this form. So, you have your xk plus 1, initialize at some x0, and then evolve it according to the previous point. You have some step size, epsilon k, so it's diminishing with k, and you have some function that takes xk, and then noise yk plus 1. And we let yk, this sort of be depending on both the noise sequence and. Depending on both the noise sequence and the state variable. And then you can ask, okay, well, to make it simple, you want to analyze the tail behaviors, you sort of start it at n at every point. When n grows, you start it at n. But then what about orchid agents in this setting? Because I mean, this is really the bread and butter for a lot of stuff we're doing, right? So SGD and all these things are really on this board. So this is what's being used in all different types of applications. And you understand, the convergence is in some sense well under. The convergence is in some sense well understood, but again, you try to understand what fluctuations are like. When do things go wrong if you want to improve these things? How do different step sizes, how does this affect things? How do different choices of noise sequences, how do that affect things? And you say, okay, this is a fairly classical system. There has to be results on this. And you go into the literature and you find precisely the names you suspect. You have Freyling, you have Paul Dupuy, you have Harold Kushner, Dupuy Anne Kushner, Isko Nein-Muling. I mean, there's a bunch of papers. I mean, there's a bunch of papers on this. But then you start reading it, I mean, and they're all from like Freylin is from late seventies, Paul and Harold's stuff is from late eighties, early nineties, and then it sort of drops off. And you go into this and say, well, they all sort of rely on abstract is not sort of the right way, so obscure assumptions, let's say that. So, this is, for example, from one of Paul's papers from 1988. So, he looks at this system with constant step size. And then, sort of to get And then, sort of to get the large deviations, you assume all this stuff: that there's some kind of function, convex and upper semi-continuous, some sequence of filtrations, and you have some inequality, limit sequence, some inequality with this function. And if you have that, you get large deviations, and they're phrased in terms of this age. This age is going to be the Fenger-Legender transform of your local rate function. Okay, but this doesn't really help it because I want to understand what the rate is going to be with sort of the data that I have. Going to be with sort of the data that I specify when I specify my system. If you know what to expect, you can sort of see, you can guess what date should be, but that's sort of kind of difficult. So what we've been doing is sort of getting these large deviations, but putting the sort of assumptions on the function g and the noise sequence. How do I specify the next step in the noise sequence? And then you sort of recuperate all these previous results. It's a technical mess, so I don't want to go into it. I think we're at 50. Want to go into it? I think we're at 50 pages right now, just sort of cleaning it up. But the point is that you sort of get this high-predate function that you suspect, and everything is phrased in terms of G and this transition control model. And with that, I'll just leave this here. What's the fun thing to start looking at? Well, it's looking at specific examples of different types of stochastic approximation algorithms. And if you think about the mixed Nash equilibria part, Next Nash equilibria part, we want to look at stochastic mirror descent algorithms because those are what's really being used. And so, those of you at Imperial and Greg Caviotes have been working on those things and looking at convergence of those. So, there's a bunch of stuff that's really out the next step. That's the more interesting part. So, I'll leave it there. Some references, and if you have any further questions. Okay. 