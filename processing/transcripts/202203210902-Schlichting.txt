Okay. Yeah, thank you very much, Martin, for the introduction. And thanks to all of you for letting me part of this workshop. Together with André, we would like to report on a joint work, also together with Martin Boeber from Ellen, Frank Hoffmann from Bonn, and then Daniel Matthews from Munich, which is concerned with a variant of the classical transformation. Of the classical transport problem that you all know, which we termed as covariance-modulated transport problem or Wasserstein geometry. So, these kind of problems came up and take some motivation from inverse problems, where as we will see later. And here, covariance-modulated Basserstein geometry. Covariance modulated Basserstein geometry means that, as opposed to the classical transport problem, the cost of moving mass in certain directions will depend in a certain way on the current covariance structure. Okay, so Andri, I would like to use this snot to put focus on different aspects of this problem. Of this problem. So, I will start and focus more on the kind of geometric aspects of this transport problem. And Andrei later on will put more focus on gradient flow dynamics in this geometry. Okay, so let me start right away and define to you what I mean by this covariance modulated optical transport. So, if we're given two measures Measures mu naught and mu1, probability measures on Id. We consider an optimal transport problem in the dynamic formulation. So let's denote the value of the cost by a curly W with a little cough to remind us that the covariance plays a role. And it's defined to be an infimum over curves of measures mu t interpolating between mu naught. μ t interpolating between μ naught and μ1 driven by some continuity equation with a vector field V. And what we're minimizing is some squared norm of the vector field integrated against the curve of measures mu t and integrated in time. So this should probably be very familiar to you. So this is very close to the classical Benamu. Classical Bernamot-Brunier interpretation of optimal transport. So here are our two measures, mu naught and mu1. Now we seek some interpolation between time zero and one. We kind of imagine that Mars moves along a certain trajectories here. So individual particles, if you like, they travel with. If you like, they travel with velocity vt. And then what we're minimizing here in the classical Benham-O-Brinier picture would be the total kinetic energy needed to move mu naught into mu1. Now, the twist in this problem lies in the way we calculate the norm of the velocity. And here the covariance comes into play. So let me denote by n of mu the mean of the measure mu and by Of the measure μ and by C of μ its covariance. Okay, then instead of just looking at the squared norm of V, we define a norm depending on the covariance, which is the quadratic form given by the inverse of the covariance matrix of V. So, in this little picture that I drew here, you might think that so covariance in this. So, covariance in this or so this direction might be an eigenvalue of the covariance matrix with a large eigenvalue, whereas this direction might correspond to an eigenvalue to a very small eigenvalue. So this would mean here that transports in this direction are rather cheap because, in some sense, we divide here or we multiply. We divide here or we multiply it with the inverse of the covariance matrix. Large covariance means for cheap transport in this direction, whereas going in this direction would be expensive. Okay. So now what What we were interested in is at least two things. First of all, we would like to understand this transport problem. So what kind of structure can we expect for optimizers? Notice that something potentially interesting is going on here since there's some competition. We already noted that, say, if the covariance is large, transports are cheap. So we can try to Transports are cheap. So, we can try to exploit this. We can try to deform our measure in such a way that the covariance becomes large in a certain direction and then move mass in this direction. So, this would favor some spreading of the mass. But on the other hand, in order to deform the measure, we need to transport it a little bit. So, this will also cost us something. So, there's an interesting competition going on. And the second aspect is. And the second aspect is if you think about Otto's interpretation of PDEs in terms of gradient flows with respect to this new covariance modulated Bush-Line geometry, what kind of gradient flows do we see and what can we say about their behavior? This is what Andre will focus on in his part of the talk. Okay, let me very briefly give you some motivation for this. Distance precisely this connects to the gradient flow interpretation. So, consider the following non-local and non-linear Fokker-Planck equation. The rate of change of a measure rho is given by the divergence of the covariance matrix multiplying the gradient of rho, and then drift term covariance multiplying rho times gradient u. So, if this covariance would not be there, this would just be the usual for covariance. Be there, this would just be the usual Fokker-Planck equation. Okay, so where does this equation come from? This arises in the mean field limit of a particle filtering approach to certain inverse problems, where you're trying to find and sample from an unknown posterior distribution, which is of the form e to the minus u. And this non-linear Fokker-Planck equation. Linear Fokker-Planck equation precisely comes up as the gradient flow of the entropy with respect to this new transport geometry. In the context of this particle filtering, as Andrei will explain us later on, conditioning the dynamics with the covariance of the particles is expected to improve the convergence behavior. We will be able to make this precise in a certain sense. Okay. Okay, um so let's observe some basic facts about this transport problem. So here I just recalled the definition of the modulated transport problem and remember that this norm of V is V C inverse times V. So first thing to note is that this problem indeed defines a pseudo distance on. It defines a pseudo-distance on the space of mobility measures, which, as the usual, Wasserstein distance matrizes with convergence plus convergence of second moments. But this distance will not be finite for any given measures, mu naught and u1. Namely, we can make very precise when it will be possible to join two measures with finite cost. Namely, the distance is finite. Finite cost, namely the distance is finite, even only if the initial and the final measure, their covariance matrices have the same kernel and the difference of their means is orthogonal to this kernel. The interpretation in this is that we already saw moving in the direction of low covariance is very expensive. Low covariance is very expensive, and in the limit moving in a direction in the kernel of the covariance produces infinite cost. But not only instantaneously, but also one can observe that it is also asymptotically not possible to move mass into the direction where the covariance vanishes. So, whenever the measure The measure is maybe supported on a lower-dimensional subspace, then at finite cost we cannot transport anything out of this subspace. We can even make this quantitative. The covariance matrix along a curve of finite action can always be compared to the initial covariance from above and below. Covariance from above and below in the sense of symmetric matrices. Okay, so in the SQL, I will only consider letters that have non-degenerate covariance. Otherwise, one can restrict the problem onto a subspace and have a so the So, we have a first result about the structure of this optimization problem, which relates this modulated transport problem to a problem concerning the optimal evolution of the mean and the covariance by itself, and a constraint transport problem. You might think that if you have several degrees of freedom, you can choose how to transport your mass and how to evolve the mean and the covariance during this transport. And this results tells us that the original problem nicely splits into these two things. Into these two things. So we have one. So our original modulated transport problem can be split into a problem that's purely concerned with the interpolation of the mean and the covariance. So that's a problem on the moments and a transport problem where we constrain the evolution of the mean and the covariance. So this is then purely, so we fixed. This is then purely so we fix the moments and we're purely concerned about the interpolation of the shape. Okay, so let me define these objects. So first of all, the constraint transport problem is looks very similar to the classical Benampigny formulation of the optimal transport problem. So we minimize the total kinetic energy along solutions to the continuity equation, but we have To the continuity equation, but we have an additional constraint that the mean of the interpolating curve is to be zero at all times, and the covariance is to be the identity at all times. This is the first part. And the second part is purely an optimization problem on interpolating between the mean at time zero and one and the covariance at time zero and one. And the cost is so for Martin. So, Martin. Sorry, I just had a question. In the continuity equation, you have a mu and a nu. Is that a typo or is that intentional? No, this is a typo. Thanks for the comment. This should also be a new here. And the cost of interpolating the moment is basically the norm of the derivative of the moment weighted with C inverse, and then there's a With C inverse, and then there's a cost of interpolating the covariance as well. More precisely, to achieve this splitting, what we do is we normalize the given measures mu0 and mu1, obtaining measures mu0 bar and mu1 bar, which have zero mean and covariance identity. Zero mean and covariance identity, but we cannot choose any normalization, but the normalization has to be carefully adapted to the evolution of the moments. So more precisely, if we take an optimal evolution of the moments, we normalize the measures by an affine linear transformation, subtracting the mean and dividing by a matrix A, where A is a character. matrix A where A is a carefully chosen non-symmetric square root of the covariance. So so on. Initially we take a square some square root of the covariance C naught and then we obtain a square root of the covariance at time t by solving a certain ODE. We will see the purpose of this in a second. Purpose of this in a second. Okay, so let me briefly sketch how this separation comes about. So the first idea is to do the optimization in two steps. So first, we fix a given evolution of mean and covariance and consider the following constraint problem where we minimize Minimize the action where here norms are calculated with respect to the fixed function ct. And we have the constraint that the mean follows our given curve mt and the covariance follows the given curve ct. And then in the second step, we optimize the result over all possible evolutions of mean and covariance. Okay. Okay. Now, if we normalize the curve by an affine linear transformation as before, achieving zero mean and covariance identity, then the resulting curve will still satisfy a continuity equation for a suitable vector field V and the original energy. The original energy of this vector field nicely splits into the classical Wasserstein energy of the new vector field plus the cost that we saw for the interpolation of the moments. Now, this is basically for any possible normalization, but if we do any normalization, we might not be able to achieve. We might not be able to achieve an optimal interpolation. So, you might remember that in the Benamouprenier formula, there's some flexibility in choosing the vector field V here. Namely, you can add anything that is mu divergence-free. And the optimal choice is a gradient of some potential. And this is where the clever choice of the square root comes in. If the original vector field was optimal, then also the vector field after normalization is optimal, provided the normalization is done with this carefully chosen square root. And the crucial property that this normalization should satisfy is that inverse a dot is symmetric. And this is precisely what. What is ensured by choosing the square root in this way? Okay. Okay, so we have split the transport problem into two parts, evolution of the moments and evolution of the shapes. So let me first discuss a bit what we can say about the optimal interpolation of moments. So first of all, we can always find an optimal interpolation. Optimal interpolation of moments. And in special cases, we can even make it explicit. So let me highlight two of them. First, let us assume we're in a special situation that the means of the two measures are the same. Then along the optimal interpolation, the means will stay constant. And the covariance has a very explicit form. So we build the final covariance conjugated from with minus one half the initial covariance, take this to the power t and then conjugate again by the symmetric square root of the initial covariance. We can also evaluate the cost. Turns out to be essentially a sum of the A sum of the logarithm of the eigenvalues of this matrix C0 inverse C1, C0 to the minus one half. The other special case that's tractable is if the two covariances are the same, but the means are different. And here we see an interesting behavior. So let us assume, so let us denote by lambda i the eigenvalues of these two. Of these two of this covariance matrix, then along the interpolation, covariance matrix will stay constant. And the cost for this interpolation has an explicit form. It depends on the eigenvalues of the covariance matrix and on the differences between the coordinates of the mean. In the form of an arcos or aerial cosine hyperbolic. So, to get some idea about the behavior, let's consider two Gaussians with different mean but the same covariance. So, in this case, there is nothing to optimize about the shapes, and the splitting tells us that the covariance modulated distance is precisely this cost for interpolating the moments. If you look at this expression, If you look at this expression, you notice that for large differences of the means, it behaves logarithmically in the difference of the means, which is quite different from the usual Wasserstein distance, which of course where of course the Wasserstein distance is just the norm of the difference of the means. Here we see this effect that if you have to, so say, move So, say, move mass, for instance, for a long distance. So, the usual displacement interpolation would just be the shifted Gaussian. But here we use this covariance structure to spread out the mass in the middle to make the transport, the main part of the transport cheaper. Okay. So the question now is what can we say about the form of shape optimizers? And this turns out to be surprisingly tricky. So let me first do a little excursion to a related problem, which is the variant model. Which is the variant modulated or constrained transport problem. Instead of making the covariance appear, we just work with the variance. So the SEPA is very similar. We define a transport problem by a variant of the Benamou-Brenier formula, where now we weight the energy of the driving vector field by one over the variance of the measure at time. Over the variance of the measure attack t. A similar splitting result can be achieved. So, this modulated distance splits into a part concerning the shape of the measures after normalization and an optimal interpolation of the mean and the variance. So, the moment interpolation problem again has some explicit cost. I have one more question. By variance, you mean the trace of the covariance, or what's the variance? Exactly, yeah, the trace of the covariance. Sorry, I should have made this explicit probably. Okay, and the constraint problem is minimizing the usual. The usual kinetic energy of the curve under the constraint that the mean stays zero and now the variance stays moment. Here, the moment problem is explicitly solvable, and the variance constraint problem has been analyzed in detail by Carla Ngambo in 2003, motivated by applications to kinetic equations, where typically along the evolution, the Along the evolution, the mean and the variance are conserved. So, here's just a recollection of this variance constraint transport problem, now a little bit more general. We constrain the mean to be m instead of zero, and the variance is constrained to be sigma squared for some sigma. Now, this can be seen as the induced distance. Induced distance that the Wasserstein distance induces on the set of probability measures with mean m and variant sigma squared. And this set of measures can be heuristically seen as the sphere of radius sigma in the Wasserstein space centered at the direct measure in M. So the picture is roughly like this. You have the direct measure in M here and And sphere of radiusigma around it. And then one of the main results of Carlin and Gangor tells us that this constraint transport cost can be explicitly determined in terms of the just Wasserstein distance between the two measures in the form of an Argus cosine. And also, the optimal interpolations are obtained in a direct fashion by just looking at the displacement interpolation and rescaling or translating the curve in such a way to meet the constraint. This is indeed very intuitive. So if the two measures are here, then the displacement interpolation would just be the chord. Be the cord cutting across the sphere, whereas here we're interested in the optimal curve that stays on the surface of the sphere. And indeed, this relation here is precisely the relation between the caudal distance between two points and the arc distance in Euclidean space. Okay. Okay, now what can we say about the covariance modulated case? And this, as I said, turns out to be quite more tricky. Let's consider the following example where maybe our first measure looks like this, just four direct measures distributed at equal distance from the origin. From the origin and the second measure with two of the points they have more mass but move a little bit closer to the origin and the other two points they get a little bit less mass but move further away from the origin in such a way that the covariance still stays the identity. Now what would the optimal transport do? This is not sort of usual. This is not so the usual optimal transport is not so hard to determine. So we have to move some mass here, some mass here. Some mass will be moved, of course, in this way. But since these two points here have more mass than the blue points, we also have to move a little bit of mass in this direction. So that now So that now at time. Okay, sorry, let me try to change the color of this to be consistent with the diagram. So now at an intermediate time, so the displacement interpolation would look something like this. Now we see now one, two, three, four, five, six, seven, eight Diracs appearing at intermediate time. This would be mu one half displacement interpolation. Now you can calculate the cost of so this curve you can re-normalize to achieve the constraint. To achieve the constraint, you can calculate this action. And you can compare it to a different curve where we make an ansat that the curve is of this form, consisting of four Diracs, and we optimize overall possible evolutions of these four Diracs. And what you see here is that the normalized W2 geodesic is different from the numerically determined optimal curve. So there's really something non-trivial going on. To shed a bit more light on this, one can look at the dual form. Dual form of the transport problem. So the constraint transport problem in a dual form takes the following form. So we have a maximization problem over Kantorich potentials phi naught and phi one, if you like, which are governed by a Hamilton-Jacobi equation. But now since we have Since we have an additional constraint in the problem, that there will be a Lagrange multiplier coming in. And this is given in terms of a matrix theta. And it intervenes at the level of the Hamilton-Jacobi equation by adding a quadratic term. Now, formally, one can obtain again the geodesic equations, so the formal optimal. Equations, so the formal optimality conditions for this problem. So it's given by the continuity equation. There is a modified Hamilton-Jacobi equation on the potential phi. And at optimality, the Lagrange multiplier theta is related to the controlled potential phi in this one. If I would have a little bit more time, I could convince you more that here's something non-trivial going on in between the interpolations. If you think of classical optimal transport, after you've chosen an optimal coupling to do the displacement interpolation, you connect the points by straight lines. Here, Here, in this modified problem, the coupled points will not be joined by straight lines, but rather by trajectories of interacting particles. And the interaction of the particles comes from the fact that they have to satisfy a global constraint, that the covariance of all the particles together has to be the identity. Okay. Let me finish by giving you a positive result on the existence of optimal interpolations for the constraint problem or the more modulated problem, which is equivalent by the splitting result that we saw. So for any two measures, mu naught and mu1, Say with zero mean and covariance identity that are not too far apart from each other. So, for instance, the modulated cost is strictly less than one, then we can ensure the existence of an optimal curve connecting them, or in other words, a geodesic with respect to this distance. Let me just briefly say what is the problem here. Of course, you can do the standard. Of course, you can do the standard calculus of variations approach and look for a minimizing sequence for this dynamical optimization problem and try to pass to the limit. It's not hard to extract limits of a minimizing sequence, but the tough problem is that these limits might lose the constraint in the limit. So the quantity to be minimized gives no control about the second moment of the The second moment of the measure in the limit. So it might happen that the covariance is not equal to one in the limit, to the identity in the limit anymore. So the crucial ingredient here is to cleverly use the fact that you're trying to optimize a certain quantity and that this allows you to propagate tightness of the second moment along almost optimizers in order. Long almost optimizers in order to preserve the constraint and the limit. Okay, so let me wrap up before I hand over to Andre. So we looked at a covariance modulated or constrained optimal transport problem that takes motivation from particle filters for inverse problems. One can nicely separate this optimization problem into an optimization of the shape of the measure and the Shape of the measure and the moments, in particular the mean and the covariance, there are still several open questions. For instance, the question of whether we have existence of optimizers in full generality to determine the explicit structure of optimal curves, shedding more light on this interaction between individual trajectories. It would also be. It would also be interesting to look at other ways of modulating the transport problem and probably many more. Okay, but I should stop here. Thank you for your attention. And I'm very happy to take maybe some questions already now and also together with Andre at the end of our joint talk. Okay, so thanks, Matthias. So maybe you should. So maybe if there's a quick question now, and Andri can maybe already start switching the screen sharing so that we're not running into kind of major time problems. So is there any problem there online or okay, Simone? Oh, please go ahead. I had a question about a formula where you had a trace. Where you had a trace of a log of a matrix with the inverse of the two covariance and the covariance of the two covariances. But if you write that as a determinant, instead of a trace of a log, if you write it as a determinant of a trace, sorry, I'm probably saying it backwards. I somehow feel like I've seen this metric before. Is it familiar to you? It's a metric on matrices. Okay, this might well be possible that it's That disappears somewhere. We haven't been aware of this. So, if you have some pointers for us, that would be very much appreciated. I'll think about it. I can't remember where I've seen it, but if I can think about it at the end. So, there's another question by Simona. Yes, just a quick question was: so, you said that if you follow the particles, then you can see interaction between them. We are not going on straight. Between them, they are not going in a straight line. So, what is the kind of interaction? You have an explicit form? So it's possible to make this explicit so that the interaction will be the particles have to solve some second order or the E, where now the Where now the acceleration is not zero, but it's given by a term which involves again the Lagrange multiplier coming from the constraint. So then in the end, this would lead to a fixed point problem where you interpolate using solutions to this kind of ODE in such a way that you achieve the global constraint. Okay, thank you very much. Thank you very much. Okay, thanks. So I'm happy now to hand over to Andrei for the second part, the dynamic part of this talk. Yeah, thanks a lot again. Thanks a lot of having us here and to report on this project. So let me directly start with the overview. So I will talk first a little bit about what Matthias already mentioned. So where does it come from? And then you also see some. It comes from, and then you also see somehow this mean feed behavior. And also, what Simona already asked, why there are somehow this interaction of the bike plays a role. And I will then mostly talk about the gradient flow part. So I will show you the gradient flows and then I will show you also some convexity properties and which we can manifest with some EVI formulations. And at the end, we also see the end. And at the end, we also see the entropy method, which works very well in this case. So, as Matthias said, this question comes from this classical inverse problem for parameter estimation. So, given some type of forward map, which for the sake of brevity is here just a map from Rd to Rk, and so it's like a model, and we observe data from this model y, and this data is This data is perturbed by some noise. And then the posterior density under some assumptions on the noise is given in terms of essentially such a distribution. So there's this exponential of some Gibbs, of some Hamiltonian f here, which is given the data. So y is in our case now fixed by the By the forward map, and then plus the uncertainty in the parameters, which is modeled here. So for the most of the target, many of the things later are specified for the special case where g of x is a linear forward map, and then this f will be a Gaussian. And okay, so for these linear forward maps, we have then a Gaussian Hamiltonian here. And then there are like two questions that you can ask about this. So one question would be: what is the best approximation? So what is the most likely Most likely x, which you can have for this Hamiltonian under this Hamiltonian pi, which is like the maximum either of this posterior density or the minimum of this function f. And or which we are more interested in is the sampling from this measure pi. And there are like algorithms developed for this. This and one algorithm which uses a particle approximation approach is this ensemble-Kalmann inversion. So that's, I mean, has already some history. And the idea is that one has a derivative free sampling for these arcmarks. And what one can do there is that one tries to take J particles. Take J particles, and this particle algorithm tries to find exactly this most likely realization. And for doing this, is that the individual particle try to do a discrete approximation of the gradient of this G. So this is, you can think of a discrete approximation of the gradient and evolve along this. So this gives you like this discrete approximation of the gradient, and then you evolve the particle positions there. You evolve the particle positions there, which works very well, especially for in the case when the xi is a linear function. Okay, but this gives you only like one single point, but if you are interested in the really in the distribution, then you do an SDE approximation. So here would be one possible realization of this Enzyme-Keyman inversion. And for the sampling, the new idea was that one here. That one here essentially does this gradient approximation of the forward map. And then to this gradient approximation, one does an adjustment with noise, which resembles the correct covariance structure. Now, the covariant structure of G is unknown. So you want to find from your sampling algorithm itself, from the random points. If from the random points, from your example, you want to find an estimator of the covariant structure of the limit. And the main idea here, going back to Reich and Kotter, was to use the covariant structure of your ensemble. And that is exactly given in terms of this. So you use the covariance of the empirical measure of your ensemble. And this is defined in this way. Okay. I'm sorry, I have a situation here at home. I have to quickly go run to the door. I'm terribly sorry. This is really worst case scenario. But yes, you may want to jump in for a minute, but I really have to go. Okay, I can try to fill the gap where we Andri left off, but I'm afraid I will not be able to switch the slides. Okay, back again. Yes, sorry for this interruption. Okay, so and having this Kalman sampling, you see that from here, if you drop more and more particles in this algorithm, that you can close this in a That you can close this in a mean field limit. This was investigated by Ding and Li and just recently appeared. And so they obtained this mean field limit of the Kalman sampler. So we are now in the mean field limit, this becomes really a gradient. And that's exactly the gradient of the Hamiltonian of the posterior density, which we saw. Posterior density, which we saw in the slides before, so exactly of this pi. And the covariance of the empirical measure becomes then the covariance of the law of the process. So this is the typical thing which you see in this mean field limit, that from this very high-dimensional SDE, you obtain self-consistently one single dimensional. One single-dimensional SDE, where then there's a self-consistency relationship in terms of this law. Good. So, and starting from here, then we can try to extract this metric structure which Matthias explained in the previous part. So, I can now write this in terms of the covariance just as Matthias did. So, for these probability measures. probability measures and also use a mean and then we observe that we have here writing here the e2 description of this ste so we have dt rho and the e2 form um so what we make use here is so the e2 form would be like the square derivatives of zero um rho plus then the divergence Plus then the divergence of this drift term rho rad f. But thanks to the fact that this covariance structure here is independent of space, we can bring this in divergence form. So this Dave divergence becomes then just So the usual divergence. So this matrix divergent becomes just sensor divergence. And here inside, we see the covariance structure, which gives us rise to this metric tensor. So and the free energy consists of an entropy part and an potential energy part. And now, if you would be given this great influence, you would have not. You this great inflow, and you would have not seen Matthias talk before, then you could do also like differentiates this in time, and then you would observe this type of entropy dissipation from where you can also then read off the covariance modulated Wasserstein distance by saying, Okay, look, this is just the product of the gradient of the free energy, and then this is a formal metric for the gradient flow description. And here, Description. And here, this is more or less equivalent as the formula definition where you would arrive, where the rigorous definition was on Matthias slide. So you have here this inner product with the covariance matrix, but now all the continuity equation has here the covariance. So that's why it does not come with the inverse here. So for in Matthias talk, we had this equal to v, and then To v and then if you substitute this here, then you would see that you get the inverse. Okay, so the main question now here is can this covariance modulation improve the conversion rates? We saw already that on the geometry side for the shape, the structure is not so. We still don't fully understand what happens to geodesics and so on, but maybe still this. But maybe still, this complicated structure could maybe improve the convergence. And we will see that this is indeed the case. So, on this slide, I just wanted to rewrap what we saw in the second last slide of Matthias talk. So, we see that we have this nice splitting, and we were also on the dynamic level, we want to make use of the splitting. So, the splitting into shape. Shape and moments. So the covariance and the mean. And then this is like again the brief description of the covariant constraint optimal transport. And the constraint geodesics, there is the additional feature of this Lagrange multiplier zeta, which occurs there. And what one can do is, if one takes this for granted, If one takes this for granted, so this Hamid-Jacobi equations, and takes this like as the geodesics, so one plugs in some internal energy and see what happens. Can we extract some convexity? And that's indeed true. So we want to check for convexity of internal energies, which satisfies the McCann conditions. Think of the entropy and Samsali's entropies, and so on. And then And then, inside of the constraint geometry, can we extract convexity? And it is, in fact, true that it's convex. So, we can bounce the second derivative from below by something which is non-negative. And we even gain something. And which is really nice and which we did not expect before was that we can gain here a term which is the pressure. So, there's a term, the pressure which is associated to this internal energy. Which is associated to this internal energy. And this is like a gain in convexity. So, usually doing this calculations, you only have some semi-convexity and you cannot use this very much. But especially for the entropy, where the pressure is linear, this is just again a one. This becomes one since we have probability densities, and the entropy becomes one convex in the constraint. So, this is a sharp contrast. So, this is in sharp contrast to the usual just semi-convexity of the entropy. So, then you think, okay, this sounds good. So, we have improved convexity for the entropy and also maybe for other entropies. But then, you have to keep in mind if you consider potentials which are of quadratic form, so here we just take a quadratic norm for the non-degenerate matrix B, then Generate matrix B, then those will be constant because exactly of the constraint. So entropy improves, entropy becomes one convex, but quadratic potential energies become just constant. So you don't see them along constrained geodesics. But if you would compare this to the usual situation in Wasserstein, then you would gain from quadratic potentials somehow something related to the eigenvalue of this matrix. Value of this matrix and from the entropy you would just get a zero. And here we now get a one independent of the choice of this matrix B. So this already hinges that we can get like uniform convergence rates. So now let's come to the second point of the splitting. So what happens with the splitting? And so now we really consider this case. We really consider this case where the energy in our Foker-Blank equation is a quadratic potential. So that's the situation where we can say most. And so the Fokker-Planck equation takes this form. So just, it's like a variant. So if you like covariant modulated orange, then we'll beg with this B. And again, for this splitting, as Matthias was also explaining. As Matthias was also explaining, and I will be brief about this, we have to use this normal symmetric square root of C of the covariance matrix. And if you do this and use the same normalization map, which now moves this density to a density which has mean zero and covariance identity, we obtain an evolution of the shape and an evolution of the moments. And both evolutions are closed in this nice setting, say, of the Ornschen-Unbeck case. Of the Orange and Unbeck case. So we have an ODE system for the mean and the covariance, which was already, I mean, for this example Karman Fitter business, this was already known. So we just, I mean, get the same because we have the same equation. But which was not known, that then also the shape satisfies a super simple equation. So this is just the plane Ornstein-Winbeck. So classical plane Ornstein-Winbeck. And now, if we recall that the entropy was one convex, this fits now very nicely together. Because this we also know that for the classical Wasserstein, this is like one convex. And this is somehow the statement on the level of the constraint dynamic. And the nice thing about the ODE, just for you to recall, I just write this once, this ODE has an explicit, nice solution. An explicit nice solution, also resembling like an Ornstein-Whinbeck process on the level of the covariance matrix. And once you have this explicit solution for the covariance matrix, you get an explicit solution for the mean. And then with some work and so on, you can make get like explicit sharp convergence rates here. Good. So let's now come to the rigorously side in this setting. So we can prove an EVI. Proof in EVI. Again, so we use now the relative entropy and we fix this Gaussian posterior distribution. And then we have for the shape, we get this nice EVI, which expresses this one convexity of the entropy. And from this EVI, we get the stability in shape. And as I said, so this convergence rate here is uniform. Convergence rate here is uniform, it's just a one, and it's independent of the quadratic potential which we choose here. So, for any of the confinement, as long as it's non-degenerate, we get this uniform stability of the shape. So, we don't see the potential at all. And then also, like for the classical Ornstein-Uhenberg, we get in this constraint geometry, we get an HWI inequality, also manifesting this convergence rate. Convergence rate. And as I said before, I mean, just recalling, and under certain assumptions, which are not so very nice, but under some assumptions, you can now plug in the ODE evolution into the optimization problem for the moment part of the splitting. And then you can also try to get estimates on the moment distance. I think there's a square missing. There's a square missing. So here you really have to crunch the numbers into some hot estimates with the ODEs. And this is a purely like, if you like, intrinsic result, very adapted to the underlying geometry. And if you compare this to other works, like Carillo Weis, they used a lot of back and forth comparison with the classical Wasserstein distance and they add up like pre-factors. Pre-factors, so they nevertheless get also these optimal rates, but then there will be like stacking of pre-factors depending on all these eigenvalues, which we don't have. So, we have in this regard a very clean result. Also, our result is intrinsic. So, let me come to the end and for the entropy splitting. So, a similar splitting holds for the entropy. So, we can have, if we have our evolution, If we have our evolution of the density and one compares this to the Gaussian having this mean and this covariance structure B, then we can use like a Pita-Accuran theorem where we compare the normalization of rho, which is eta, under this normalization map to a Gaussian with zero mean and covariant structure of one. And then this is just here a distance on the level of the Distance on the level of the Gaussians. I write this as a relative entropy, but this is some complicated function just depending on the mean and the covariance. And maybe also this was something which the last question from the audience had in mind. So this is a function depending on like the eigenvalues of the covariance of the products of C and B and so on. But it's explicit. And again, having this. Again, having these very nice evolutions for all of these ingredients. So, here we have the PDE, for this part, and then for this part, we have the ODE. We can explicitly get individually the decay to equilibrium. So, we have a theorem which gives us exponential convergence of the shape. So, the shape is an unconditional. Is an unconditional convergence. And here we also, I mean, this is again also a statement that we have this nice convexity of the shape part in the geometry. And then things become a bit more ugly for the moment part. So this moment part also, it's like just the finite dimensional part. It somehow spoils the nice evolution, the nice result because there's a pre-factor. But the pre-factor has to be there and it's also. But the pre-factor has to be there, and it's also observed in applications. So there's a pre-factor, which is essentially a norm measuring how well your initial covariance is adapted to the data. So you see that, so this just means the maximum. So if this part of the pre-factor becomes large, if your initial covariance is too small in relation to the spreading of the data, so of this B. And so you should. And so you should start with something which is already sufficiently spread out as you pay a big pre-factor here. And then there's a compatible error. So if you like, this is like a kind of condition number which measures the reverse. So the spreading of the initial data with respect to the data. And okay, behind these factors, there are some comparison of norms which occur in this quantity. And okay, just as some remarks: so this factor can be actually dropped if you already start from the correct mean. So, if you start at the correct mean, this factor is not there. And we also get this not only on the level of entropy, but also of the fission information and also get smoothing of gradients in time. Okay, so I'm getting over time, but maybe this is nevertheless nice to see. So, why is it worth to dig into this geometry? To dig into this geometry more, and why we want to understand this is if you compare this to this first idea by doing this variance modulation. So, we can again also consider this gradient flow where we have the variance as a modulation. And then this is like a non-linear time change or non-homogeneous change of time. And what you see is that you don't gain. Is that you don't gain a lot? I mean, there will be still an exponential factor which depends very much on your data and on the smallest and largest eigenvalues of the Hessian, of your Gaussian, which you want to sample. And in our case, we have now this pre-factor, which somehow has to fit the initial covariance, but the rate is just optimal. So this is really this optimal rate for the entropy. So this is really a big gain. So, this is really a big gain, which also explains why, in practice, this algorithm shows very good convergence results. Good. So, let me wrap up. So, most of these open questions summary are like in Matthias talk. So, this is also already what Matthias discussed. Let me highlight that we get this uniform exponential convergence rate, which I also explained just now. And I mean, there's still a lot to do and to learn. Lot to do and to learn here because the full existence result for the geodesics is open. At the moment, these results are only nice if we have quadratic posteriors, so like Gaussians. So this would be nice to have something non-quadratic, and then there are place to play around with the modulation and also about pathways with the stochastic formulations of this type. So, thanks a lot. Thank you very much. Thank you very much to André and Matthias. There's maybe room for one or two very short questions. Yes, is there? I can make a can I ask a question, make a remark? Yes, please. Yes, please. So, is it fair to say that whether I'm thinking about either the transport problem or the gradient flow problem, it seems like the infinite dimensional part of the dynamics is sort of the same as the classical case, and what you're superimposing upon it is a kind of non-trivial finite-dimensional dynamics involving the covariance and the mean. Is that correct? Is that correct? Okay, I would agree on the level of the splitting of the crate in flow, but for the so especially, yeah, if you look here, then this looks very much like this. But the shape evolution, I mean, the shape evolution works just very well. And I mean, there happens also something which on the level of geodesics which we don't understand, which fortunately does not play a role here. Fortunately, it does not play a role here. So, in this example which Matthias gave, this was still very symmetric. But in the geodesics, there's also a question about rotations. And these rotations of the shape, where the covariance is still fixed, so in the constraint problem, we don't have a clue how this really works. So, I mean, there are still, at the moment, we only understand this very symmetric situations for the geodesics, but I would agree on. But I would agree on the level of the dynamic, it seems like that everything which happens on the shape works in our favor. And then it really reduces the complicated and the bad slow part of the dynamic to this finite dimensional problem. I agree. Okay, and my comment was about this enhanced convexity that you get, where you get the constant one instead of zero. So this is, there's something similar that happens in another interacting. In another interacting model, which was discussed in Grégoire Lopier's thesis, where he does sort of optimal transport of particles that are interacting through a gravitational potential. And the point is that in the middle of his interpolation, things spread out and then they come back together because the particles are acting as if they're attracted to each other. And so you get better, better entropic convexity properties. And I feel like the same thing is happening here. I mean, they're spreading out for a different reason here because your interaction is different, but that's, I think, where. Because your interaction is different, but that's, I think, where the enhanced immexity is coming from. Yeah, this seems to be essential: that you need some mechanism to spread out mass. Okay. Julio has another question. Please go ahead. Okay. Thank you. Andrea. Thank you, Andrea, for the beautiful talk. Very naive question. So, the reason you are after this long-time behavior. After this long-time behavior, because I know nothing about this particle filtering, the idea is you want to recover the posterior distribution as fast as possible. So long time means like many observations, in a sense. Yes, there's an ergodic theorem at work. Yes, so you can. So improving this, you would also hope that it also improves the sampling. But yeah. But yeah, so this connection is not completely done. So, thank you. Okay, so if there are no further questions, so I'd like to thank Matthias and Andri for this nice talk. And we'll have now a short coffee break and we'll resume in 22 minutes. Resume in 22 minutes with a talk by Jonas.