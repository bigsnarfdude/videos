About connecting the measure known as space for polynomial calculus with width and resolution, and it's joint work with Nicola, who is here, and with Leo Thappen from Prague. I expect that everybody or almost everybody in this audience knows what resolution is and what polynomial calculus is, but let me just go through this briefly to make sure we're on the same page. So, resolution, system for refuting CNFs. For refuting CNFs, lines are clauses, meaning disjunctions of literals. You only have the resolution rule, so if you've derived C or X and you've derived D and or not X, with C and D are clauses, you can derive the disjunction of the literals in C and in D. A contradiction is the empty clause, which we, of course, can't satisfy. The other system we'll be discussing is: we won't work directly with the propositional polynomial calculus, we'll take the extension. Polynomial calculus will take the extension known as polynomial calculus with resolution, which incorporates the resolution rule, but it reasons about polynomial equations in the following sense. A line is a polynomial, a multivariate polynomial over a fixed field, f, where we have variables for positive literals, for propositional variables, x1 for xn, and we have separate formal variables for negated uh propositional variables. The propositional variables. Those are the barred ones. And the line, we write it just as p, and the meaning, the intended meaning is p equals zero. The rules are that if you've got two polynomials and they're zero, then the linear combination is also zero. If you have a zero, then you can multiply it by any variable that's also a zero. The variables are Boolean, so x squared equals x, and they're have mutually opposite values. So if you add x uh to x bar to negated x, you get one. x bar to negated x, you get 1. And the contradiction is the one polynomial which never equals 0. This embeds the resolution because you can translate any clause into the product of the dual variables. So if you have a clause and you write a product and you add a bar, if there wasn't a bar and you delete the bar if you had it, then This term equaling zero is the same thing as the clause meaning. Right, so resolution widths and polynomial calculus degree are defined as usual. So the width of a proof is how many variables there are, or literals there are in the widest clause appearing in the proof, and the width of imputing a formula minimizes over the widths of Minimizes over the widths of refutations of that formula. The corresponding measure with lots of formal similarity to width for polynomial calculus is degree, where instead of measuring the width of the clauses, you're measuring the degrees of the polynomials that you're in. So I assume that's more or less known. I have to spend more time on discussing this measure known as space. Lots of you know this much better than I do. Much better than I do, but probably some of you don't know it that well. So the idea is that this measures in some sense the amount of memory you need to verify a proof or to make the proof go through. So amount of memory needed by a computer verifying the proof or perhaps the size of the blackboard you need for the proof to go through, where you want every step of the proof to be verifiable from what's on the blackboard already, right? What's on the back model already, right? So, the way you model this is you present the proof as a series of so-called configurations. So, it's a blackboard state, you could say. And a configuration, this blackboard state, is divided into cells. In resolution, each cell is allowed to contain one clause. In polynomial calculus, your lines are polynomials. Polynomials consist Lines are polynomials. Polynomials consist of terms, meaning sums of variables. So each cell of memory, of the blackboard, is allowed to contain one term. And you think of the proof as a series of such configurations where you start with the empty blackboard, so there is nothing at the beginning. You finish with a contradiction. And on the way, when passing from one configuration to the next one, you can do one of three things. When you're refuting a formula, Of three things. When you're refuting a formula f, you can write down one of the axioms, one of the clauses of f on the backboard. Or studying general polynomial refutations. Yes, sir. If you use the same term in several polynomials, let's call it chart words. Yeah, you can do both. I've seen both definitions in literature. In what I do, it actually won't matter. So we're putting a lower bound on space, so we could take the liberal one. Space, so you could take them as a liberal one where you have only distinct terms. So you can also infer and write down something on the blackboard that follows from stuff that you already have on the blackboard, using the resolution rule in the case of resolution, using all the rules of PCR in the case of polynomial calculus with resolution. And if something is no longer needed, you can delete it. So that's the idea. And the space of a proof is the largest number of cells you ever needed. The largest number of cells you ever needed at a certain point in the proof, and the space needed to prove or to refute a formula just minimizes this overall refutation. So that's the usual setup for this. And now, there are known connections between space, width, and degree, in particular between resolution space and resolution width. The most classical of these is, I guess, this result due to At Simmons and Dalmau. result due to at service and almaux which says that each element of the cell in the case of polynomial outputs is a product of variables is a product of variables multiple polynomials so what does it mean uh adding a conclusion that you add polynomial right so so let's say that you had two polynomials p and q and did we write p plus q right so you have to write p plus q p plus q happens to have five terms to be a sum of five Happens to have five terms to be a sum of five monomials, you need five more cells. So here are some known relationships between these measures. So the most classical of these is, I guess, this result by Ceres and Dalmaud, which says something that's intuitively not obvious at all, at least to me, that resolution space is basically lower bounded by resolution width. Exactly by resolution width if you Exactly by resolution width, if you discount the width of the formula you're refuting. For in resolution width lower bound space. And there is a generalization or strengthening of this due to Ilario Bonacina, namely, if you take a more refined measure of space, the so-called total space, where you're not counting how many clauses you have to put in memory at any single time, but how many actual literals. So you look at how wide the clauses and memory are. At how wide the clauses and every are, then that's called total space, and this is actually lower bounded by essentially width squared. Those two relationships are for resolution purely, but there is also a known connection between resolution width and polynomial calculus space. So this big paper on polynomial calculus space by Filmus, Lauria, Miksha, Nordstrom, and Viniels, I guess. Vignals, I guess. They proved that if you have a CMF and it has high resolution width, then a related formula requires large space in polynomial calculus. But the related formula is not the original formula f, it's basically XORified F. So you replace each variable by XOR of two other variables. So it's not quite so you use high width in resolution for a formula to. In resolution, for a formula to prove a lower bound on PCR space, but for a more polynomial calculus-like formula in some sense. So it's not exactly the same thing. And all this stuff makes it natural to ask whether polynomial calculus space, which in principle could be something much smaller than resolution space, this relationship isn't really very well understood yet, whether this could be bounded by the analog of width for polynomial calculus. Of width for polynomial calculus, meaning the degree, or perhaps even by resolution width. And this question has been raised a number of times, and our main result is a contribution towards answering it. We don't get a full answer, but what we prove is the following. So we don't know if for refuting CNFs, Poland. Refuting CNFs, polynomial calculus space is nowhere bounded by width, by resolution width, but we do know that it's bounded at least by the square root of width. And I actually prefer the counterpositive form of this. The way I like to state it is that if you've got a CNF and you can refute it in polynomial calculus using S-memory, then you can actually refute it in resolution. Actually, refeated in resolution in width s squared plus the width of the original CNF plus plus plus plus the k here. Since you can basically up to say plus one, depending on how you define the rules exactly, you can basically simulate resolution width w by polynomial calculus degree w. Then this also means that for polynomial calculus, space can be transformed into square of that. Transformed into square of that degree. So degrees lower bounded, space is lower bounded by square root of the degree. So that's our main result. So we go some way towards answering this question on the space-degree relationship for polynomial calculus. And in whatever time remains, I will try to sketch some of the underlying ideas for the proof because they're actually, I think, are reasonable. Because they're actually, I think, are reasonably simple, but perhaps somewhat novel in this context. And let me start by recalling a very basic tool that's actually used to prove the space-width relationship for a solution, which also happens to be the main tool we use in our argument. So there is something called, there is a component. There is a combinatorial characterization of resolution width due to Atserius and Dalmau, which we call an Atserius-Dalmau family. And it basically says the following thing. If you've got a KCNF and refuting it requires high resolution width, at least W say, then this means that there is a family of partial assignments that somehow explains why it's easy to pretend. Why it's easy to pretend that f is satisfiable. So f is in some sense locally satisfiable, where the locally is measured by this width. So if refuting a formula requires at least width w, then there is something we can call a w at Sirius-Dalmov family for the formula. And this is the following object. It's a family of assignments, of partial assignments. Each of them has size at most w, so the domain covers at most w variables. Covers at most open variables. It's closed undertaking sub-assignments. If you've got an assignment in the family and there's a variable that's not set, then you can set it at least in one way and still remain within the family. So we can extend the assignment to set this variable, maybe not in both ways, but at least in one, and you stay within the family. And importantly, this family pretends that. Pretends that the formula you're refuting is true, namely, none of these assignments ever falsifies a clause of the formula. So this is a very useful and nice combinatorial characterization of width. And it's actually, I would say, the machine behind the original Atsterius-Dalmau proof that space is lower bounded by width in resolution. So I have to recall this argument because it's. Argument because it's in some sense a model for ours. I mean, our argument is inspired among other things, like by this. So, how do we prove that if there is a small space refutation, then there is a small width refutation in resolution. So, let's assume that F has a resolution refutation in small space. Let's present this refutation. Let's present this refutation as a series of configurations, each of them with at most s clauses. And let's assume that there is no low-width resolution, which means that you've got this, the exact number is s plus k, at serious Dalmov family. So you've got this family of assignments that pretend that f is true. But they don't necessarily cover all the variables. And now you've got this family of partial assignments, and you go down the proof from the first From the first clause in the proof towards the last line, which is a contradiction. And for each configuration in this proof, you find an assignment in the family with size that's actually bounded by S, the space of the proof, such that the assignment satisfies the entire configuration. And the inductive argument is actually totally true. Argument is actually totally trivial. There's essentially nothing to do. The only step in which you have to do anything is if you're downloading one of the axiom clauses, one of the clauses of F. If you're writing one of the clauses of the formula you're refuting on the blackboard. And you can always set all those variables in F because your previous assignment had at most S variables and you only need to set K more. only to set k more. Once you said it, you know that you're not falsifying the clause of F because you never do that. So you were satisfying it. So you have satisfied your entire configuration. Your current assignment might momentarily be slightly bigger than S, but since the entire configuration only contains S clauses, you need just one bit to satisfy a clause. So you just drop the unneeded bits and you retain an S bit assignment. You retain an S-bit assignment satisfying the new configuration, and at the end you reach a contradiction. So that's it. And I also mentioned for further reference that there are lower bound proofs, especially for width but also for other measures, going in some sense in a dual or in the opposite direction. So instead of going down the proof and having small, in some sense, assignments satisfying each step, you can go up and have... Step you can go up and have small or efficient assignments that falsify each step. That's easy to do with a contradiction. Once you reach an initial clause, you get something at least. So we need both things in our argument. And now, if we want to get a relationship between polynomial calculus space and width as opposed to resolution space and width, we want to apply something a bit similar to this, but Bit similar to this, but to a polynomial calculus refutation, small space. Okay? How much time do they have? About 10 minutes. About 10 minutes. Okay, so let me use these 10 minutes as efficiently as I can. So let's assume we've got the following picture. We've got a KCNF. The KCNF, let's call it F, has a small space polynomial calculus refutation, which Polynomial calculus refutation, which we'll write as a series of configurations m1 through mt. But let's assume it does not have a small width resolution refutation. And small in this case turns out to be 4s squared plus the width of the formula. So it has an ethical family of the corresponding domain size. Domain size. Okay, so with assignments up to 4s squared plus k variables, we'll call this number dot. Maybe let me copy this on the diaphragm. So got a k and c and f s space dcr refutation m1 through mt. And we've got w series down for it and w equals. and w equals 4s squared plus 2. And we want to show that this data is contradictory, that this cannot happen. That's array. This is somewhat difficult. Ideally, you would like to do the same thing that Sirius and Delmo did, go down the proof and keeping or maintaining an assignment from this Sirius-Dalmo family that satisfies each given configuration and we reach a contradiction at the end. But there is a problem with that. There is a problem with that. Well, here's an obvious problem. In resolution, it's the case that if you've got a small space configuration, you need few bits to satisfy it, if at all it can be satisfied. Because small configuration means small clauses, you need one bit per clause, so you need few bits of the assignment to satisfy it. This no longer works in polynomial calculus. Here's an example. You can have a single polynomial that counts as Single polynomial that counts as 2, contributes 2 to space, take 1 minus a product of many variables. And if you want to satisfy this polynomial, meaning make it 0, you've got to make all the variables, all the variables 1, which means setting n variables. So, you know, this is space 2, but to satisfy it, you need n variables. So that's a problem. And the way we overcome it is we take seriously. We take seriously this idea, which, well, at least from my perspective, it's inspired by forcing as used, for instance, in set theory and other areas. But in some ways, this has appeared in proof complexity before, but maybe not in this particular context. So let's take seriously the following idea. If you can make something true, for instance a clause, that's good. But if you That's good. But if you, and that would require you just one bit. But if you've got an assignment and you cannot make a clause false by extending this assignment, and by extending this assignment, I mean extending it to an assignment you care about, an assignment actually coming from this family age. So if you if you have no legal assignment that will actually falsify the that will make the clause true, well think of your current assignment as forcing the clause to be false. Forcing the clause to be false. So the formal definition is as follows. If you take a term, we want to say what it means for an assignment from our family of assignments to force or make a term 0 or 1. So forcing a term to 0 means the obvious thing. Your assignment should set a specific variable in the term to 0. Forcing to 1 means something different. Something different. You force a term to one if you cannot extend your assignment staying within that series-Dalmo family in such a way that would force it to zero. Okay? Yes. But H is downward calls. H is downward. Right, so basically you're right. So A sets forces T to 1 if there is no If there is no element of H of the form A, union singleton, and that's a negative literal from 1. Or it's a uniquely defined extension of alpha that falsifies T is T. Well, it's not uniquely defined because you've got many variables in T. It's in the other direction. If T has k variables, then actually there are k possible assignments you're interested in. Assignments you're interested in. That's the whole point. Yeah, so basically, you force a term to one if you can't make it zero in any way. So now, forcing a term to zero needs one variable, forcing a term to one, if it happens at all, needs no new variables at all. This matters for the rest of the argument. And then we extend this to forcing values for polynomials and for configurations in the And for configurations in the most naive possible way. So we simply say that a value for a polynomial is forced by linearity. Polynomials are a sum of terms. If you force the values for each term, then you force the value for the top polynomial, namely the sum of the terms with the corresponding coefficients. And you force a configuration to true if you forced every polynomial to zero. You force a configuration to false if you force every. To false if you force every polynomial to a value. So you give a value to every polynomial, but one of these values is non-zero. So those are the definitions. And there are some bad and some good things about this forcing relation. One bad thing is, for instance, that... Is it obvious why you want to force everything? Is that the last definition? Like, you would maybe say that if I'm forcing something to non-confident, then I should not really be done. Yeah, I would have to think about this thoroughly, but I think But I think that bookkeeping becomes much more difficult if you're dealing with assignments that somehow have not decided some of your polynomials. So here are some good and bad things about the forcing. So if you've got an assignment of maximum size and it hasn't yet set a variable, then it actually forces an axiom, this variable plus neglected variable minus one. Variable plus the needed variable minus one to a non-zero value. So it makes an axiom of polynomial calculus false if it's dealing with a variable that it hasn't set, it cannot set because of size considerations. But if the size is reasonable, then in fact good things happen. Firstly, you never force a configuration both to true and to false, that's by definition. Secondly, if you can never given a proof of refutation, you can never force... Refutation, you can never force a configuration to true and the successive configuration to false. This is basically soundness of polynomial calculus represented in this language. And finally, if you can take, given any assignment, perhaps you can't extend this assignment a little bit to force a configuration to true, but you can always, given our A space S refutation, you can always either extend your assignment a little bit. Always either extend your assignment a little bit and force the configuration to true, or extend a little bit and force it to false, where extend a little bit means add as bits as in the space. So, yeah, let me say one final thing at least. There is still another problem. You try to repeat that serious Dana argument, and you go down the proof and try to maintain an A small assignment that will make each force your current configuration to true. And you need a bound the size of this assignment. And there is an issue when you already have an assignment that forced mi to true. You extend it a little bit to force mi plus 1 to true. But then you need to cut it down to size again because if you're adding even one bit per step, you're Even one bit per step, you're in trouble because the proof can be long. And to deal with that, and the problem is that deleting any bit from your assignment that might seem redundant could cause terms to stop being forced into one. That's the issue. So to somehow wiggle your way out of that, you have to repeat this downwards and the dual upwards process a number of times. Upwards process a number of times. Maintain an assignment that keeps increasing, but the number of times you increase it is somehow controlled by our argument. And maintain an ever-smaller segment of the proof between a certain configuration MIR and a certain configuration mjr, where jr is greater, in which your current assignment behaves in an ever more special way. And at the end, The end, you get a segment of the proof where essentially your assignment has set all the terms and all the configurations within that segment of the proof to a value, and it makes the first configuration true, the last one false, but that's inconsistent with the soundness of polynomial calculus. So the details, I'm afraid, you'll have to look at the paper. We will have to look at the paper. Yeah, we get some consequences from some additional consequences, some specific space lower bounds for confidential formulas, not all nominal address, but I'll just leave this slide up to finish the effects. Thank you. Alright, so we'll have to next speaker second quantity machinery for questions. Can you put the others of the question? Can you put the open questions? Did you? Can you push the open questions? Oh yeah. Well the I just there are a number of open questions, but the this prominent one is of course whether the square is really needed. And I don't have a conjecture exploring. Yeah, so I have contradictory intuitions because this work that actually works not just for polynomial calculus space, it works for a much more general notion of system for which you can define space. Defined space. And I'm intrigued by the possibility that you could possibly improve it for the specific case of the polynomial calculus, but not for the general case. But intrigued doesn't mean I'm conjecturing it. Yeah, right. I mean, if I gave you probability values, they would sum up to more than 100%. Do you know a proof system that keeps precisely a square? System that keeps precisely a square department? No, if you could, yeah, that's still open. So we have this general notion of a configurational proof system for which a measure like space makes sense. And the question is open whether there is any system obeying this definition where space s in the system translates to tightly to a space omega of s squared, to width omega s squared in resolution. More gracious.