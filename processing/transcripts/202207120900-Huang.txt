Hi everyone, I'm Teresa. I'm a PhD student from Jobs Hopkins University. And today I'm going to talk about graph symmetry and graph spectra. But I sort of added a lot of other interesting related topic based on the conversation we have from yesterday. So hopefully you can find something interesting or inspiring, and then we can talk more later this day. So without further ado, let me. To do let okay, okay. So, here is the outline of my talk. So, I'm just gonna give a quick overview of graph representation learning and like what Liz was mentioning. So, she is basically just talking about like topological thing without really going into the graph representation learning, but I'm gonna feel that hole a little bit. And then I'm basically focusing on two aspects of graph representation learning. Aspects of graph representation learning. So we'll focus on two methods: that's either the spectral-based methods and the spatial-based method. So the spectral methods, remember yesterday, the first talk from Ira, she talks about how they used like some spectral GNN to do some, have some application on neuroscience. And also like Michael's talk on using some wavelet design spectrofeatures. Design spectral features. But then, also, the spatial methods were like more of the popular GNNs that people use, the message passing networks. So, we'll dive into those two approaches and discuss their properties in terms of efficiency and expressivity. So, let's. So, this is like a quick overview of graph representation learning. And basically, the whole idea is give me a Basically, the whole idea is: give me a graph. I want to sort of embed it into a Euclidean vector space. Then I can just do any subsequent downstream inference based on the Euclidean embedding. And so here, the rest of my talk will basically focus on one downstream task that's on no classification. And depending on what community you're in, you may also view it as community detection from like the spectrograph. Detection from like the spectrograph theory and stuff. And the idea is very simple. So basically, we want to predict no labels with the input as our full graph and some no features and also some training labels. So here what you're seeing is the adjacency matrix of certain graph. And also this XX transpose thing is basically the covariance of the no feature matrix. So as you can see here, for example, this random graph is actually sample. This random graph is actually sampled from a two-block stochastic block model. So, there are certain nice things that you can see that there is some like block structure in the graph. So, that's why, like, for example, this is like a classic literature on community detection based on like trying to cluster the notes. Um, so, and then regarding the methods to do this, as I eluded earlier, there is this classical spectrum. This classical spectral approach, where the simplest one was maybe a lot of you're familiar, is the spectral clustering or spectral graph embedding. And the idea is basically I want to do the eigendecomposition of my graph operator and use its eigenvector or eigenvalues to produce a graph embedding. And this is like a really nice statistical method because it has very a lot of nice. Has very a lot of nice properties under like random graph family. And also on the like graph neural network side, we've also seen a lot of design of spectral-based graph neural networks. And basically the viewpoint is I want to sort of after I did the graph spectral decomposition, I want to transfer the graph from the spatial domain to the spectral domain by performing the graph Fourier transform, the GFT, and then GFT. And then basically, what I need to do is to design some filter, graph filter function, which is shown as this P here. And that could be just reduced to designing a filter on my graph frequency on my graph eigenvalues. And on the other hand, we also seen this local-based, spatial-based message passing algorithm. And like more classically, maybe a lot of you familiar with or just those belief. Familiar with are just those belief propagation algorithms. And like recently, this message passing neural network has been like really popular. And then, because they're like simple and efficient, and they can be computed in the graph spatial domain. But essentially, what the message passing, they do like they're basically aggregate information from the known neighborhood. But then you can also rewrite it into say, I'm just going to pre-multiply my Multiply my previous layer embedding by this graph operator, and then right-multiply by some learnable weight matrices. So, there are some connections with these two, and I'm going to dive into them later. And so, the question we're asking here is: can we inject some global information to the local message passing neural network? And why? Because, as I guess, Because, as I guess a lot of you are familiar with, there are many issues of the local message passing network. And one prominent issue was this over-smoothing problem, which essentially says the vanilla message passing, they basically just express a low-pass filter of the graph. So they may exponentially lose some important like information, including some, say, spectral information that might be essential for a downstream task. And so there are. And so, there are a lot of existing work to try to sort of incorporate global information in the local NPMM, including like a lot of people here that, for example, we can use this sort of functional calculus spectral filters. It's basically the spectral filters that are polynomial functions or rational functions. So, those nice graph filters, you can actually express them on the spatial domain without explicitly. Domain without explicitly computing the graph eigen decomposition. So those are efficient. And also, we've seen that recently there are a lot of works on proposing transformer-based graph neural networks. However, I guess the more sort of like niche questions that we're looking for is: can we compute global spectral information from local MPNN? Because the motivation, if you recall a few slides before, it was we're A few slides before it was we were like looking at this community detection, and we know that there are like classical sort of like uh stochastic block model or a lot of random graph models that the spectral-based approach based on eigenvectors, they are proven to be the best method to solve this problem. So, here, uh, what we're proposing is a very simple algorithm called power embed that allowed us to specifically compute the top k eigenvector. Compute the top k eigenvector of your graph operator in a local message passing network. And so here is how the algorithm goes. So basically, this is your familiar message passing algorithm. And what we essentially augmented is to add this normalization step to ensure that at each intermediate, at each iteration, you will output something that's also normal. Something that's also normal. So when this algorithm converges, the output will be the top k eigenvector of your graph under some mild conditions. And also another cool features of this algorithm is the ultimate goal of what we're doing is we try to learn both local and global information from the graph. And so we want to sort of store the intermediate embedding because the way that we've Because the way that we view this is the follows. So, when we first have the no features, we view them as simply like local features. And when we first perform the first few iterations of the message passing, what you're essentially doing, you're aggregating your local neighborhood features. So you still get something that's like a local signals in a graph. But eventually, when the algorithm converge and we're outputting the top K eigenvectors, the global, the eigenvectors, they represent. The eigenvectors represent the global spectral information. So, what we need is to store the intermediate output embeddings and then we feed them in this downstream inception network module so that the say the downstream classifier can exploit and utilize both these local and global information for the downstream inference task. And so, this we've run this our like very simple methods on. Like very simple methods on a couple of benchmark data sets. And here are two graph data sets that exhibit particular hetrophily structure. And there are like a lot of work in the GNN community says the message passing GNNs may not perform so well in the graph with hetrophily structure, maybe also because they're missing the long-range information in the graph and whatever. And so here, all the methods with a norm at the end. Methods with a norm at the end are just: I use this graph operator, for example, the adjacency matrix, but then I augment the vanilla message passing with this simple normalization step. And you can see here by just adding this simple normalization, I can see like improvement over the unnormalized message passing counterpart. So it helps us to really avoid the over-smoothing issue. And also related to this issue, there is another issue called over-squashing. There is another issue called over-squashing, where the community was saying that, oh, when we continue to do this message passing and we're pulling more and more information, then the no embedding may become like indistinguishable. So the way to avoid sort of over squashing is by storing all the intermediate embedding, including the original node features, we're sort of maintain more of the information to Of the information to avoid this over-squashing issue. And also, the approach that we're proposing here is computational efficient because we don't really explicitly do the whole eigen decomposition. What we're essentially doing is just to compute the top K. And this is also nice as in a lot of time when we actually did the spectral decomposition, we may only use the top K ones because those are the global spectral signals. The global spectral signals that are most useful for our downstream task. Okay, any question at this point? Because I'm going to switch gear to yeah, so why it's named power invest? It's inspired by power iteration. And it's actually essentially how, very similarly to how you actually want to do the global decomposition using power iteration, is by having this normalization. Oh, so here is so it's like a subspace iteration. So the classical power iteration is just to get the top one, right? Like the leading eigenvector. But here I'm sort of use this generalized version where I'm getting the top K by like, so the input will be like a K dimension like. Like a k dimension, like m by k matrix, and so yeah, I can actually get the top k subspace, yeah. So I don't have to do it like subsequently. Yeah, great question, yes. Yes, uh like I would think the problem is like uh problem with those exponentially, like the neighborhood with exponential. It doesn't enable for exponential, yep, even if you use the skip connection to the number of in like entities you have limited. So, yeah, so that's a great question. So, I think it depends on how you define over squashing. So, here, I think, yes, in the sense that we actually don't sort of constrain ourselves in a fixed-dimensional embedding. So, as you can see in this algorithm. You can see in this algorithm sort of outline, I am actually storing the original no features and the intermediate embedding, and then I'm going to use all of them. So in some sense, I'm cheating in some way, as in, I kind of feel like in the over squashing literature, people mostly analyze that given like a fixed size dimensional embedding, say I have to embed them in dimension K and K is fixed. Then as I keep increasing my As I keep increasing my layers, then I will face some troubles. But here, I'm basically sort of allow K to grow as I'm storing this intermediate embedding. Yeah, but I still think like it grows linearly, how many entities for and how to enable it to yeah, yeah, it also sort of depending on the density of your graph. Yeah, yeah, yeah, yeah. Yeah, I was wondering, do you consider the natural algorithm another method that can be a population value as a vector? Which algorithm? Can you say that? Because actually, I had a work with ICM Lightning, which means describe how items. Oh, interesting. Oh interesting. I see. I see. We should talk more. Yeah, yeah. So is that something like similar to this power iteration thing or is it something completely different? Minus absolute value that costs large and because it costs no risk. Oh, okay. Interesting. Yeah, we should chat about it. Thank you. Michael? A question about the previous slide. So they were still included taking the inverse to the next point, yes. So that's a great point. So here is like a very naive normalization step, and that can definitely be improved computationally because I think there exists. Because I think there exists like better, say, like QR-based algorithms that make this normalization computationally more efficient. But also, know that if I choose K small, then this normalization wouldn't be so bad. Right. Yeah, but that's a good point. Yes. Yep. Yes. No, the thing I'm in that like inverting is basically just a k by k matrix. So it's like I once I have so after I do my first message passing having this intermediate output I further want to normalize it by doing basically you can think of this as obtaining like an offer normal version of these embedding does that Does that clarify? Was there another question? Okay. If I could ask a question in disembodied voice. This is Mike, by the way. Yes, I recognize you. Oh, thank you. One of the limitations of like GCNs is that they tend not to work well if you make them truly deep, like two or three layers, and then out beyond that, performance starts to tank. Frederick. Starts to tank. Frederick and I have tried arguing that the oversmoothing problem is related to this shallowness phenomenon. So I'm wondering, like in your network, are you able to go like deeper, add more layers without having performance decrease? Yes. So I think one trick is, as you can see in this result, is okay. So maybe first go back to the network architecture. One trick is to avoid. One trick is to avoid over-smoothing. If you can use all the intermediate embeddings that also including the original no features, then it gives you a big boost of because I think a lot of oversmoothing that happens on if you just use the final deep like GNN embedding that would result in over smoothing. But essentially, a lot of times just using the normal A lot of times, just using the node feature themselves helps to distinguish different nodes. So, I think, like, one generic design of these sort of inception network-based algorithm is you can sort of store the intermediate embedding that helps you to avoid more smoothing. And on the other hand, the algorithm eventually, when they converge, it outputs the top K eigenvectors. And so, if And so, if say, if our testbed is just some like classic random graph models where we know that the top K eigenvector basically encodes the node classification or the community detection signals, then this like you will get smoothing just within the cluster, which will be really good for downstream tasks. So, I think, yeah, so basically it depends on what your application is. What is your application, and I think just generically using such like inception network rather than just use one MLP or one classifier on your final output, that will help you a lot. Does that answer your question? That helps a lot, but I guess what I'm kind of most curious about is how do you choose capital L on this slide? Like, do you still typic it to be a relatively small number? Or yeah, great question. So, I think this will be more like based on the Will be more like based on the actual graph, and so right now for the experiment for simplicity, we just keep it at 10. But I can imagine that in practice, you may want to say, just store the first few iterations, and then maybe let L2 be a somewhat large number, but then you only keep the last few iterates. So you're sort of having the first few that is like representing the local signals and then the Sending the local signals, and then the last field that's representing your global signals. But yes, there is some arts needed to choose the capital L. Okay, thanks. That helps a lot. Thank you. Are we good? Okay, so I have like five, ten minutes left. I'm gonna do really quickly and sort of maybe like elude it on some of the ongoing. On some of the ongoing thing that I'm currently thinking, and then we can take the discussion more like offline. And so, okay, to summarize what I just discussed is we want to encode global spectral information in local message passing neural networks. And that sort of brings up the next natural question is to look at how expressive are these spectral versus spatial methods. Is spatial methods. And we can analyze them from the signal processing viewpoint and just recall how they actually look like, the spectral GNN and spatial MPNN. And so if we choose the graph filter from the spectral GNN P as say like a degree K polynomial, it's actually pretty straightforward to see that you can express a spectral GNN just using a K layer. A K-layer mpnn by composition, right? And in fact, this spectral filters, if you choose any rational function, that can also be expressed just using the local spatial MPNN. So the punchline is if you view it in the signal processing viewpoint and then selecting particular graph filters, they are essentially having the same expressive power. What if we look at it from the graph? What if we look at it from the graph isomorphism viewpoint? And so, just to recall, graph isomorphism basically are trying to answer the question whether two graphs are isomorphic, meaning if we can find a relabeling of the nodes from one graph to another. And a lot of these investigations actually utilize the concept of graph invariant. Basically, graph invariant is a function that maps the space of graph. That map the space of graph to some fixed target domain, where if the two graphs are isomorphic, then the function outputs the same thing. And so in this literature of graph invariant, there exists the spectral graph invariant, for example, like just using eigenvalues or graph angles or some like multi-sets of the eigen projectors of the graph. And also, as maybe many of you are more familiar with, there are some spatial or With, there are some spatial or combinatorial invariant that is organized under the Weiss-Feiler-Leven hierarchy. And so the natural question is: which invariants are stronger? And from a very recent work, it's shown that these above-mentioned spectral invariants, that they are less powerful than the 2WL. And why we're interested in this is, as you may have already seen, actually, this WL Actually, this WL hierarchy inspired a lot of interesting design in graph neural networks. And so, can we sort of understand and design stronger spectral invariant, which may help us to sort of inspire new designs of interesting GNN architectures? And so, okay, this will be my last slide. And this is like the very first step sort of to tackle this. First step sort of to tackle this question on designing stronger spectral invariant and by using the tools of graph spectra and graph symmetry. And what we essentially are looking at is say, give me a graph and I take the spectral decomposition of the adjacency matrix. And then if I want to tackle the graph isomorphism, I basically wanted to find the automorphism group of this graph because if I Of this graph, because if I can tell all the isomorphic pairs of my graph, I can know whether this graph is isomorphic with another graph, right? And mathematically, this can be written as if I can find some permutation matrix pi so that the pi commutes with the adjacency matrix. And so, how will the graph spectra help us to solve this problem? The way to look at it is if we can define At it is if we can define a set of the stabilizer of the eigenvector UI, and basically this set will include all the permutation that maps the eigenvector back to the same eigenspaces. So ui and uj, they're coming from the same eigenspaces associated with the eigenvalue lambda i, up to sign changes. So if I can't identify the set. If I can't identify the set of stabilizer, the permutations, then the automorphism of the graph will basically be given by the intersection of the set of these stabilizers of all my eigenvectors. So this is like a mathematical characterization on how we can connect the automorphism group of the graph to its spectral signature. Signature or to its eigenvectors. And so we're currently working on how, like, essentially, we're working on how to describe the stabilizer set of UI. And can we sort of give me like the eigenvectors of the graph? Can I extract some sufficient representation of these eigenvectors that help me read off the stabilizer set or like help me easily? Or, like, help me easily compute the stabilizer set. And we have some promising results on the simple case where the graph only have distinct eigenvalues. But we're currently still investigating the more general case where the graph have repeated eigenvalues. And if any of you are interested in talking about this, I'm very happy to discuss it more maybe in the afternoon. And so I guess this is it. Thank you. I'll take questions. Thanks a lot. Great talk. We have a bunch of minutes for some questions. Please take it away. So maybe then I'll start with one because I'm interested in the future work, of course. So how would this work? So what would you gain exactly with the stabilizer and how computationally efficiently can you? How computationally efficiently can you actually find them? Great. So let me answer the computation efficient part. This is not about computation efficiency, I think. Yeah, because well, it's that this is more like just analyzing the expressive power. Yes. And your first question remind me, was it. So what do you expect to gain from this? And this is not this. And this is not meant as an attack question. It's a dumbness question. So please explain it to me. I guess, well, this is first of all, it's not about computation, but I think it's an interesting math question, regardless. And also, there are maybe one application potentially is in certain cases, maybe you cannot really get the entire graph. Say you have a really huge graph. Say you have a really huge graph or whatever. However, you may be able to probe it and to get some, say, spectra information, like the leading eigenvectors and whatever. And then if you can use that spectra information to give you some information about what's the automorphism group of the graph, or like tells you, for example, what are the symmetries of the graph, then that may be something like useful in the application. But yeah, mobile. Application, but yeah, most of this motivation is actually just acute math question. This is great. I love loving this. So, a brief follow-up to this: you mentioned that if you have a super large graph, you might still be able to approximate some information. There's probably a lot of theory already around this, right? But can this also be approximated from sub-samples? So, suppose you have like a super large graph and everything is super dense, then probably you don't need to know everything. Know everything about these super dense things, right? It's probably sufficient to have a bunch of samples all around. Is there a theory that makes this work that tells you that, hey, you can approximate some things? I believe so, but I know very little about the sampling approaches. But I think, like, under some, say, generative graph model, there are some guarantees, uh, like the sampling viewpoint can work, but um. Can work. But yeah, I think the motivation more or less, the honest motivation is this looks like a cute math question. I know, definitely. It's super elegant. Yeah, but yeah, but if you have some suggestions, then yeah, we can definitely talk more. Thank you. Thank you, Bastian. Any other questions so far? Yep. You want the mic or should I just repeat it? The thing is the preceding. You said this is related to the tension proceedings, yeah, yeah. Okay, yeah. So here, this is related to what we so we want to sort of look at how expressive are these functions from the graph isomorphism test viewpoint. And so the graph isomorphism test basically is just determine whether two graphs are isomorphic. In some sense, this is a In some sense, this is a sort of a worst-case scenario because, uh, yeah, but I think this is like one canonical approach where people use the graph isomorphism test and see how your method can determine, whether they can determine like two non-isomorphic graphs, and to see if this method is powerful enough. And so, how this related to what we're proposing here. To what we're proposing here is when we talk about the automorphism group. So, this is basically give us the characterization of all the isomorphic pairs of my graph. And so, if I can characterize all my isomorphic pairs, I can also characterize its complement, which will be all the non-isomorphic graphs. And so, if I can completely characterize this automorphism group, essentially I solve graph isomorphism. So, your goal is to find a new non-spectral method that you can get, yeah. Okay, great question. So, is my goal to propose a new method to solve graph isomorphism, which seems pretty hard problem in general? Short answer is no, but what we're hoping here is sort of okay, now a lot back to the motivations. We see that a lot of GNN design. See that a lot of GNN design are sort of motivated from this WL hierarchy, where WL hierarchy is a very sort of important combinatorial or spatial invariant approach to solve graph isomorphism, but they do not completely solve graph isomorphism. Yeah, because like even KWL, it's not a complete graph invariant. So what we're hoping here is, can we sort of design a hierarchy of spectral invariant that first can like are stronger than this? Can like are stronger than this existing ones because the existing ones can be shown to be pretty weak. And then, can we also sort of like design a hierarchy which may potentially motivate some new interesting design of GNNs? Yeah, but like, I'm not hoping that my spectral invariant will completely solve graph isomorphism. Does that answer your question? Thank you.