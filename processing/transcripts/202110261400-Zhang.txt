Guido, and thanks the organizers, Pablo, Rongje, and Mingha, and for inviting me to speak to this workshop. So I will just do a tutorial here since people are using geometric methods, and information geometry has increasingly coming to people's recognition about one of the tools that they want to use. To use, so but I want to just give uh just go ahead and then just do some tutorial on this, and then uh uh so the topics which I planned to cover include starting from a manifold statistical models and then talk about uh no in particular statistical models, the fishing information, the change of parametrization, and then talk about the dualities for statistical inference, EM, geodesic projections, and then the maximum entropy, and general Pedagogian theorem. And general Pythagorean theorem and so forth. And then I will introduce the topic of the so-called geometrical structure, the statistical manifold structure. So, and that's the essentially the bare minimum for someone who wants to know information geometry. I plan to also introduce some advanced more advanced topic, but then I found out that it will just run way over the number of slides and for the given time, so probably will be reserved for some future. A future and our opportunity. So, the manifold of statistical models, I will start with an example, the univariate normal distribution. Univarient normal distribution with, so here omega is a random variable that can be have the sample space of R, the real line, and the mu and sigma are the parameters, right? I collectively call mu sigma, I call theta, theta, which. Mu sigma, I call theta. Theta was just a parametrization of this probability distribution. Okay, so this family, so we are interesting this the entire family of normal distributions with a random variable on the real line. So in other words, we are looking at the family of all these distributions with all possible parameters. Okay, so if you choose theta, mu theta, you get one distribution, choose another mu theta, get get another. Choose another mu theta, get another one. So we want to look at the whole set, the whole set. So let's look at some of the boundaries for this set, okay, in terms of the parameters. So these at extreme, when sigma go to zero, this would be a delta function, right? This would be a delta function, which is an extreme case of a normal distribution. This is actually a distribution as opposed to problem. Really, a distribution as opposed to probably density functions. No longer density function. On the other side, if sigma goes to infinity, well, this normal distribution become a uniform distribution on the real line. Okay, so these are the extreme cases for this whole set of normal distributions. So we are now interested in this object of this entire set of normal distributions with theta as the parameter, you know, those mu and sigma as a parameter. So one A parameter. So, well, normal distributions arise in many different settings, but we're going to talk about this. It's an exponential family, but also it is a location scale family. It's a location scale family with mu and sigma, right? So we want to see how these different aspects of a normal distribution may be captured by the study of the geometry on this entire set. Okay, so that's the kind of a very first introduction about the. The first introduction about the motivation to how we deal with this, what we are going to study. So, how we are going to deal with this is that now let's first have a way to present, to represent the entire set of this normal distribution, the entire family of normal distribution. So, what we do is on the right-hand side, these are the normal distributions, one member, another member. There are four members here. So, on the left, we look at the parameter space, namely, we put the Parameter space, namely, we put the mu as the x-axis, sigma as the y-axis. Okay, so that's a very natural way of thinking, this parametrization of the normal distribution. So in this case, mu can go to minus from minus infinity to plus infinity, right? And the sigma can go from zero to plus infinity. So that would exhaustively represent our entire family of normal distributions. Okay, so any member can be found. Can be found by one point on the left, which is indexing normal distribution, right? So we have four normal distributions here, these two normal distribution, like here, and these two normal distribution, these two. Okay, so we have each normal distribution just becomes now one point in the parameter space. So now, having this representation of the entire set of normal distributions, we want to see the geometry of it, what kind of a The geometry of it, what kind of a geometry we need to put on this space. So, but immediately we realize that for simple statistical considerations would tell us that these two distributions are closer to each other than these two distributions, because these two with smaller variance and these two with larger variance, even though the separation of the mean are the same. So, the difference in means are the same. So, this gives rise to So, this gives rise to a kind of a requirement that we want this geometry such that these two points are kind of farther apart and these two points are closer, right? Which means that immediately that says that we can't just impose a Euclidean metric on this space. So, we need to do something else. We need to prescribe something else to capture this fact. Okay, so that's the motivation for the use of a non-Euclidean kind of. Motivation for the use of a non-Euclidean kind of a geometry. Okay. So now let's in order to consider what kind of a geometry we need to want to put on to this, so let's just have some general study or understanding of statistical models, for statistical models. So say we have, say, a sample space. I use the capital omega to represent the sample space. And suppose the sample space, we are provided. The sample space, you know, we are provided with some kind of a reference measure, you know, and then well, pd mu, you know, this is just one of the probability, the probability on the sample space. And this small omega is the data or the random variable, the data on this set. So suppose we have the data is generated by, say, this P, and then we have a property model Q. A probability model, Q, which Q can possibly be different from P. So Q is our model, and P is a data generating probability distribution. Okay, so probability density for generating the data. So clearly, upon observing data, there is a loss, which we can use the log loss, which is minus log of a Q of any model. So Q is our model. So Jensen inequality provides that we have this inequality. This inequality, which basically the expected loss for any other model would also always be larger than the loss incurred by the true, the true probability, the data generating process. Okay, so this is a very straightforward kind of, I know. So the expected velocity is a minimal when the model Q really matches the data generating process, right? So that's very standard. That's a very starting point for statistical inference. Now, if we take the difference between the expected loss of our model and the minimum loss on this, this is nothing more than just the KL divergence, right? So in a sense, that KL divergence comes out very naturally when we study the family of statistical models, right? So that's a measure for the difference between, you know, how good or how the difference between. Or how the difference between two models, two models. The reason the KL divergence is non-symmetric, and this non-because Kpq is not equal to KQP, and this is actually a nice property instead of a nuance, because in statistical inference setting, we do differentiate between the data generating distribution and the model distribution. So, this is something it turns out to be quite nice. And in fact, this Be quite nice, and in fact, this you can see this, you know, why this comes out, right? So, so for the KL divergence, well, later we're going to talk about there may be a more appropriate form to use the so-called extended KL divergence, because this whole integral itself would be greater than zero for any random variable, but we're going to reserve it for a little bit for later. And important, another important thing to say is that I use the sometimes I write P and Q, well, sorry, I. write p and q well sorry i i in the later i used this uh this p and q and used the two parallel lines in between that i should have changed this notation here because i when i whenever i use the two density functions i use this two parallel lines here and whenever i use parametric parametric representation i use the x x being the parameter so x indexing the distribution p and x indexing the distribution q so that so in this case kl divergence become just a two variable function right so kl diver function, right? So KL divergence become a originally it was a functional on two density functions, but then it becomes really a function of two variables. So for normal distribution, we can calculate the KL divergence, which is essentially just, no, just do this, plug this calculation, you have this formula, okay? Now, if we were to look at the two models which are close to each other, The two models which are close to each other, so you know, those mu1 is close to mu2 and sigma one is close to sigma two. Okay, we can perform TL expansion, and after performing TL expansion, what we get is an expression like this, okay? An expression like this, so which is basically, you know, you have the zeroth order term vanishes, and also the first order term also vanish, okay? And then the only the first surviving term is the second order term, so which is can be written in this format, the second order. Written in this format, the second order term, okay, which delta theta is just not delta mu, delta sigma. G is this matrix, and like this, these are two vectors. Okay, so this quantity, well, in general, this quantity i here is nothing but the fish information, okay. So, fish information in general is defined in this way, in this way, and we will show later on. In general, you can have a fish information. A fish information, uh, you can have the fish information to uh just you know have a lowest order expansion, then you get this, I mean, KL divergence, lowest order expansion of KO divergence, get the fission. So it was, uh, I think Rao and also Jeffrey at that time studied this and proposed to use this I as the Riemannian metric on M, as a Riemannian metric, because the Riemannian metric is something that can be used to calculate the line elements. Calculate the line elements on this manifold. And after integration, this gives the distance. This gives the distance measure on the manifold. Okay. So, but I want to note here that we use here x. Okay, yeah, here, I use x instead of theta. Okay, so later on, we're going to use x generically as meaning a parametrization. Okay. And so in general, we can use any parametrization. any parametrization x for parametrized density function. But the kind of the so this x provides a natural kind of coordinate chart for the manifold. So in other words, because x is valued, it's taking its value in Rn. So it provides a very natural chart, geometrically speaking, a natural chart to index a distribution. Okay, that's straightforward. But the next thing is a little bit of interest. A little bit of interest and needs some calculation. So, but we can certainly change parametrization, right? So, we can change, say, the same density function, we can change it to x and when we can use u as long as we take x as a function of u, for instance. It becomes another parametrization of the same density function. Okay, so this is viewed as being a change of coordinates on the manifold. Okay, now when we change the parametrization, the functional form of The functional form of density changes. So that's why it's important to study how, what are the invariants in this manifold that in some way that co-varies as you change parametrization. What are the invariants which are truly reflecting, truly something which is intrinsic to the manifold itself, but not to what you happen to parametrize it? Okay, so that's the important first question that we want to resolve. So let's give So let's give an example for this. Let's say, in general, let's look at, say, for instance, for fish information. So, for fish information, right? Fish information, we can write it this way. Okay, so because it's the outer product of this. So, have this column vector and then the row vector here. It's outer product. So, one, one is just this one, two, one, three, and two, one, and so forth. Okay, so it's like this. So, first, this fisherman is, oh, it can be, it's used. Can be it's used as the Riemannian metric, but for the Riemannian metric, we need it to be positive definite. So let's check that it is indeed positive, uh, well, at least positive semi-definite. Okay, so let's check for that. Well, we can take any arbitrary vector and then we form this number, okay, which is basically we move by the row and column, and just and then you just turn this into a number. And this, you have the square here, and this is always greater than or equal to zero. always greater than or equal to zero and this is non-negative and so therefore this is always non-negative. Okay, so the fact that the Fisher matrix is positive semi-definite allows us to use it as a Riemannian metric. Okay, so that's an important side note to realize. Now, let's now look at how this metric itself, because the metric itself depends on the parameterization. Depends on the parametrization of x, okay, right? Because depends on, no, because we're taking the derivative with respect to this x, okay? So, let's look at how does that work with the change of parametrization. So, now let's re look at our re-examine our normal distribution. So, normal distribution. So, we can expand it out. We expand the normal distribution out. Okay. And we can write it this way. Okay. Now, we can, now what we do is that this is the random variable. So, we write, we put this one, we write. So we write, we put this one, we write this one as x1, and we write this one as x2. This is omega, and this is omega square. Okay. And the remaining terms we just put together to be like this. So we get a representation of normal distribution, univary normal distribution in this form. And this is the so-called exponential family, the exponential family form. So this is the location scale representation of the normal distribution. But this way, we use the X parametrization. We use the x parametrization, makes it an exponential family. Okay, so here x1, x2 can be taken to be new parameters, right? Because they are related to the old parameter in this way, and you can solve it in the other way around. So this is just a simple kind of a corny reparametrization. In geometrically speaking, it's kind of a reparametrization. I mean, a corny transformation. Corny transformation, which is what reparametrization is in statistics. In statistics. So, this phi function, this function is just here. Okay, so this is the. Now, well, certainly we can have other reprimatrizations. Okay, so we can have other reprimatizations. Oops. Did I lose anyone? Oops. Hello? Yeah, okay. Oh, I there. Okay, so sorry, no, some. I oh, I did okay, so sorry. No, somehow my screen kind of just flashed out and all the people disappeared. That's all. Yeah, the same happened to me, but I can't see your screen. Okay, right. Yeah, no, no. It would be nice to have actually have some face there so I know. Okay, or if you don't see, if something gets wrong, just let me know. But on the other hand, I would not know without if my connections went bad. Anyway, so we can have another permatric. Uh, we can have another parametrization, right? So, we can have a parametrization, say, using use. Okay, so for instance, we can use the uh say let's take the moment, the first moment and the second moment. Yeah, we can do the first, we can use the first moment and we'll do the second moment, right? Okay, then we can do the two, so first and second moment. So we can write note then the use that you can write them as a function of mu and sigma, yeah, or we can write the use in terms of the x. To use in terms of the x here, x1. By the way, this one, two is just the two components. Now, use, I use the subscript, x, I use superscript. You may wonder why I'm not consistent with this, but there's a good reason for that. You will find out later on. Okay, but I'm intentionally making the superscript for X and lower script for U, and it will be explained later on. But it's just a different parametrization at this point. So this is the first component, no X1, X, U, when you. The first component is another parametrization, okay, like this. Now, we can use this parametrization u1 u2, but then you can check. It turns out that u1 u2 is related to this phi function. Phi function is what we had earlier, this phi function, right? This phi function here. It is related to the phi function by this first derivative, okay? So this is a feature which is a very universal feature for the exponential family. Exponential family for the exponent. So the U parametrization, the moment parametrization is simply the first derivative, first derivative of phi. So now it turns out that this phi is a convex function in x. So we can compute its convex conjugate expressed in u, which is turns out this is another convex function in u. And these are conjugate, convex conjugate functions of each. conjugate conflict conjugate functions of each other now if you look at the u it's interesting because i mean if you look at the phi star u phi star u what is phi star u when you can write it out well in u right but i if i rewrite it out in terms of you know sigma this is minus log sigma sigma write it as a function of u okay so because this thing is just sigma this nothing this is the square no i mean square root of this is just sigma so but the sigma is the actually the entropy The sigma is actually the entropy of this. Okay, so the entropy of normal distribution, you can write it out, it's just minus log sigma. Okay, so now S of theta, this is not strictly convex in this, in theta, but the phi star is strictly convex in u. Okay, so now you get something that's kind of interesting that in for normal distribution, if we use if we stay with the theta parametrization, well. Beta parametrization, well, we just, you know, this is how we start with, right? We can derive a very nice, simple form of the Fisher metric, but it is not an exponential family. But if we use the U parametrization or X parametrization, turns out they are associated with these potentials and they are exponential family. Okay, so let me explain this situation. So we have three sets of parameters. Okay, we have theta, we have x, and we have u. They specify. They specify one another, right? They can specify one that you can do, and then we can do coordinate change or reparametrization. This is controlled by Jacobian. The Jacobian is the two by two matrix here in this case that controls the coordinate change, right? The reparametrization. So, but we can show that the Riemannian metric in X, in U, in theta, they all appear different. They all appear different. Okay, I'm going to show the next slide. They appear different. They appear different. And in X and U, they are hashing, but in theta, they're not Hessian. Okay. So G can be expressed as a Hessian of a convex function in the X and U coordinates, but not in the theta. So theta turn out to be the bad coordinate now in terms of the geometric property, right? So that creates a lot of puzzle. Okay, and there are interesting questions. Puzzle, okay. And there are interesting questions. These are the questions that people sort of like, you know, haven't really thought about, even for a lot of information geometers. And these are the questions that we now start to think. And it turns out that there are some very deep kind of significance for these questions. And I have to go through everything and then get to the end. Towards the end, if I have some time, I can talk a little bit about this because one of our recent papers is on explaining this phenomenon. And it turns out this phenomenon is. Phenomena and turns out this phenomena is linked with the very deep theory in geometry, like so-called mirror symmetry and so forth. Okay, so but I will leave this as an example for you. So the example is very straightforward in theta. If you use theta, the okay, I can show you this. Yeah, so if we write out, so if we write out the theta, so you have the fisher metric is very simple, right? But if you write in terms of the x coordinates, this is a this is a Hessian of this one, but this is looks kind of Of this, but this is looks kind of like this, it's kind of very complicated. You know, it's another simple metric. But then, if you use u, it becomes like this, it's also complicated. Okay, this is a very simple kind of Riemannian metric, but then, but all these three Riemannian metric are related to each other by coordinate transformation because Riemannian metric is a tensor, right? So, let me go back to talk about if you have, say, like, well, remember, G is the outer product of the partial law. Outer product of the partial log p, right? The partial. So we let's see how this partials transform, okay? The partial partial. This partial partial act because of the chain rule, they would transform in this way with this Jacobian. This is the Jacobian. Jacobian is a matrix like this, you know, from X11 and then N1, you know, 1N and so forth, right? So the partial of this transforms by multiply, pre-multiply, Jacobian, you know, that's how. Pre-multiplied by Jacobian, you know, that's how. So, therefore, if you look at the G, the new G compared with the old G, it's just pre-imposed multiplied by Jacobian, right? By this thing, so you can really test out. So, these three G's are really related to each other by pre-imposed multiply that Jacobian that converts theta to X or theta to U or X to U and so forth. Okay, so it's just a simple verification. By the way, I was teaching informing geometry class this semester, so I had. Information geometry class this semester, so I had a lot of experience of having students verify that, and once they verify that, they start to see what we are talking about and why parametrization is important, and how do we transform the metric tensor when we do the reparametrization. But anyway, so this is the first thing I want to raise it as a very elementary kind of features for this normal distribution, but then the explanation of that has to come very, very The explanation of that has to come very, very late. Still, at the moment, we are not fully understanding this phenomenon. But this phenomenon turns out to be a very important phenomenon. Okay. So let me now go to the second part of this, the duality. So informing geometry is full of duality. It's about duality in the statistical inference, duality of the statistical inference. So let us start with a familiar KL divergence. KR divergence. So you have this P log P over Q, you know. Oops sorry, this is it's a surrogate to distance. It's surrogate to distance, right? Because it's non-negative, but then it violates the symmetry axiom and triangle inequality, okay? But it's known that, well, you actually can derive it very straightforward. If you just plug in this thing, you have three distributions, P, Q, and R, they satisfy this identity. So this is the identity. So, this is the identity. So, P to Q, Q to R, P to R. Okay, you have this identity. This is just a plug in and just you find this identity. Now, under certain circumstance, right, the right-hand side will be zero. And I'm going to explain this circumstance. Okay, so under some situation, the right-hand side will be zero. If that's the case, then we have this so-called triangle relation. The triangle relation: P to Q, Q to R is P to R. Okay, so it's these things. P to R. Okay, so it's these things act as if it's the distance squared, and this is like the Pythagorean law, okay? So because of that, in fact, we can even derive a more general quadrilateral relationship. Okay, we can derive a more general quadrilateral relationship. If we had four distributions, if we have these, these, these, these, if a PQRS, we can write down this thing. QRS, we can write down this thing, okay, which is like more general than this. And when two of these, when any two of these become the same, it becomes the pedagogy, the three points. We have a four-point relationship here, okay? So, but on the right-hand side, still, you have this kind of quantity. So, we are going to look at the conditions when this quantity would become zero. And that would lead us to some, you know, either to this triangular inequality or the quadrilateral relation. Okay, so let's look at this to explain the condition. This to explain the conditions, we need to explain this notions of so-called E and M geodesic. Okay, these are the words because we didn't talk about what geodesic is. E and M E stands for exponential, M stands for mixture. So for the moment, these are just the two names. So let's see, what is M geodesic? Well, M-geodesic, essentially, it's a parametrized family of A family of density functions that's connected in this way. So, in other words, PQ, say you give me PQ density function. If I form this family of density functions, this family is parametrized by T. T is any like real number, or maybe we can take it between zero and one, you know? Okay, so in this way, this is a family of distributions, a family of densities, right? Family of densities, right? Starting from Q and then ending with P when T takes zero to one. Okay, so this is your parametric family. This family is defined as M-geodisic. Just take that away. So it's just a mixture family. Okay, very straightforward. It's a mixture family, but this mixture parameter is a one-parameter mixture family. Okay, so that's it. What is the E-geodesical? What is the E-geodiesical? It's also a mixture family, but the mixing in the log of the density function. Okay, so the log of this is a mixture. Of course, in this case, you have the normalization factor, the normalization function here. The normalization function itself, we need to satisfy some conditions. And it turns out that the derivative of a normalization function is the KL divergence at the two ends. Uh, at the two ends, okay, but this one parameter family is the e-geodesic, okay. So, we have these two basically families, okay. Mixture, this is mixing in ordinary density, this is mixing in the log of density, okay? So, M and E and M. Now, this E and M mixing family, so this is M, no, so they actually are related to the general. The to the general kind of a m like general kind of n parameter mixture family, the more general mixture. Okay, so let's here we need to use the notion, we need to introduce the notion of so-called m flatness. M flatness is the space where basically the collection of any two points, then their linear mixture is also in that space. So, M flat space, M flat space has a property that if you Has a property that if you have any two densities in that space, then their M-geodic is also in that space. So that's kind of space is called M-flat. So now you here, you have a family, the family of density functions, which can be multi-dimensional, so to speak, right? Multi-dimensional mixing, okay? And then this is called M-flatness. So in general, well, you can anticipate that this kind of a, so even given the p. A so, even given the piece of ice as being the fixed reference densities, then I have this eta i. Etas can be any positive number, non-negative number, you know, and then but less than one, the sum to one, that would be a mixture family, right? The mixture family that we know about. Okay, so this mixture family is necessary, so for the M flat, because why? Because we can, for any two P and Q, we have this, they have a family. We have these mixing coefficients. We construct this mixing coefficient of this, which is also in this, in this, uh, in this space, right? Because we can have this. So, so this is a straightforward. Same thing we can do with the E. E is basically, we first define what is the E flat space. E flat space is linear mixing in the log space, in the log density space, and then we define the exponential family to be like this. Be like this, okay, of this form. So, and we know that any exponential family is E-flat, okay? So, we can show that. So, with this two, then we can explain the condition when the right-hand side would vanish. So, the condition for the right-hand side to vanish, right? We want to see when would this thing vanish. Well, I have some other intermediate. Have some other intermediate steps that which I which I skipped. But eventually, what it comes down to is that you can, after some derived steps, what you find is that you have these things, and then these are like the velocity vectors for the two curves. And then this is the Riemannian metric, the Fischer metric, which is this. This is Fischer metric here. So essentially, the condition for this thing vanish is that this Q point, Q is a density Q. Q point, Q is the density Q, P to Q is connected by the M geodesic, and the S to Q is connected by the E geodesic. Okay, and one, two, and this, these two directions are orthogonal, are orthogonal. Okay, so there are the three. So it's better to explain by this picture. Okay, so this is a picture of the so-called because the so-called E-connection. So the condition. The E connection. So the condition of that is that this they are orthogonal. So the E geodesic and M geodesic, they are orthogonal. Okay. So say if we start with a point S, a density function on the outside, and we look at all the M flat, we have an M flat space. These are all the densities, which are, you know, as I said, there's M flat. Any two of these can be connected if you have M G D C connecting that, that's entirely within this space. So now we can look at the projections of this. Can look at the projections of this density to this m-flat space defined by the shortest, say, KL divergence. Okay, in fact, we can construct KL balls to grow out from the center and gradually, and it's to see the shortest radius that will touch it. So it can touch. Well, if you grow, you can just have basically from here, I can. Have basically, I from here I can draw this E geod6. I can draw this E geod6 and they will intersect with this space at many many points. And one of them is the minimum. One of them is the minimum point. So the minimum point is when you're minimizing this KL divergence when the first element is being varied and second one is being fixed. Okay, so that's called the E-projection. E-projection is when you are. E-projection is when you are fixing the second one and then project to the find the one that minimizes the KL with respect to the first one. Okay, so that's the when you find the minimizer, the minimizer is called the E projection onto this space. And because of the Pythagorean theorem, because this in this space, they are all M-geodic. E-geodic and M-geodic, when they are orthogonal, then you have this holds and this becomes the minimum projection. This becomes the minimum projection. So, this is a generalization of the minimum norm projection in Euclidean space, except that now I have to separate out the E geodesic versus M geodesic. In Euclidean space, E and M are just the same. You have one notion of geodesic, but in the in this, for you using the KL divergence, because of this non-symmetry, so you have to separate out there are two versions of this print. It out there are two versions of this projections, either M or E. Okay, so the same story holds because of this duality thing. It's just you know, you have you can flip the other side, the same story would also hold. So now, if we look at the M geodesic, if you look at the M geodesic projecting to the E flat space, you get the same kind of a conclusion you get, you can find a unique. So it's a very due. Now, so in general, so this, yeah, this is a picture of saying that the E projection, okay, so the E geodesic projection. Okay, so the E geodesic project in the M flat space, M geodesic is the M geodesic project to the E flat space. So that's the general. So okay, so this is also say showing that in general, from any starting from any point, you can have a lot of geodesic coming out. So connecting two points, P and S, you can have E geodesic connecting, which is this blue curve. You can also have M geodesic connecting the points, which is the red point. Okay, so in connecting the two. In connecting the two points, right, because it depends on what you mean by how you mix them. This is you mixing them by just the linear mixture, this is mixing them by their log of this. So we have different notions of mixtures. And yeah, so this is about, okay, so I have to watch out for my time. So the KL ball, if you look at the KL ball, KL ball is that if you fix, say, starting from If you fix, say, starting from Q, and you look at all the P's whose KL divergence is less than equal to C, so that's a KL ball, right? You have the E ball and have the M ball, and then these E ball and M ball, you can, so you can have the E geodesic going out, connecting Q to the E ball, and then they will, the minimum, you know, radius, you can get one, and you can deal with. So, in all of this, you have the duality, and you have to be very careful in dealing with this. Now, what I want to just very quickly kind of keep you in your keep you in your uh in your understanding is that in the e projection in the e projection what the meaning of this is that the k is the minimum divergence or maximum entropy inference okay whereas the m projection is the maximum likelihood projection okay this is you can just check out by yourself you can check out by yourself you can check out you can check out by yourself okay so uh they so we recently So, we recently actually made use of this and linked that to some methods for divergence triangle. So, in machine learning algorithms, I don't have time to go into that. But let me just quickly talk about how this links with the maximum entropy inference. Okay, maximum entropy inference. So, in maximum entropy inference, we want to maximize the S, the entropy, and subject to the constraint. So, here the constraint F sub I's unknown, P is this. And so, And so we introduced the Laglange multiplier, Laglange multiplier, and we solved this modified unconstrained optimization problem. And after we do perform the calculus of variation, we fix the this apply normalization constraint and the moment constraint can fix this lambda and the x. Uh, x right, these parameters, and then turns out that we obtain an exponential family as our solution, an exponential family as our solution, okay. And the second derivative of this function, of this function, is just actually the variance. This is the variance covariance matrix of the exponential family. So, what the maximum entropy inference method tells you is that immediately this kind of Immediately, this kind of there is a Legend duality between the X parametrization and the U parametrization. And the X and U, they are two parametrizations linked through this Lejang transform. Okay, so the log generating function, the phi, and the dual is the entropy function, is the entropy function. Okay, so the so you have this Legend dual. So, you have this Legend duality between the phi and the phi star. Okay, so this is the special part of the exponential model, exponential model. And this is the correspond to the Riemannian metric, or the fish information has a very simple form, namely the fish information is simply the second derivative, the second order derivative of this function. Okay, so that's the later on you know, this is the so-called the fish, no, the Hessian. Uh, the fish no, the Hessian metric, the Hessian metric. Okay, so uh, well, I should also mention in passing that actually for the mixture family, just like the exponential family, mixture family are also very nice. We have this mixture family, right? We have this parametrized by this eta and eta sum to one. And then you have this kind of you re so let's look at these independent parameters from eta one to eta n. And the entropy function for the mixing. function for the mixing for the mixture family if you perform its calculation the first derivative second derivative the second derivative again it is the fisher information so the if you use the entropy function of a mixture family calculate the second derivative it's a fishing information and the reason is that if you actually do the calculation you find out this second term vanishes so this is exactly the fishing information so you see when you entropy function has a When you entropy function calculate the first derivative, it looks like this. Second derivative, well, you just calculate the second derivative, but this part becomes zero because of the fact you have a mixture family. So the second derivative with respect to the parametrization is zero. So you only have the first term. The first term is the fissure, the fissure information. Okay, so now for the exponential family and mixture family, they are very nice because Nice because the KL divergence for two members of the exponential family they just become the Bragman divergence. They become the Bragman divergence. The Bragman divergence is defined this way, right? We're all familiar with the Bragman divergence. But if we use the exponential family, the two members of exponential family indexed by eta and eta star, this KL divergence, sorry, this is equal to the Bregman divergence using this. Oh, it's this is a Oh, gee, this is a sorry, this should be X. Okay, X and X star. This X and X star. And for the mixture family, the same conclusion would hold. For the mixture family, the K L divergence between two members of mixture family is also the Bregman divergence. But now the parameter is the mixing parameter and the order is reversed. Important, the order is reversed. Okay, so you see that that's why the exponential family mixture family has intrinsically. Intrinsically, they have a lot of things in common, they have a lot of connections. So, one wonders what sets them apart, and is there a way to sort of like look at them under the same lens? And this problem recently is solved. We, in collaboration with Leonardo Wong at Toronto, we actually solved that problem about building a bridge between the exponential family and the mixture family. And so, this is something look like I'm not going to have any time to do this. It's already four. To do this, uh, it's already 43 minutes, so I haven't come down to geometry, I haven't come down to uh okay, divergence functions. So, Cesar's F-divergence is the most general kind of a way of looking at its most general homogeneous divergence function between P and Q, between two density functions. And so, Amaris alpha divergence, it could be one of its one special case. One special case, the alpha divergence itself includes the Herringer distance, the chi-square divergence, the KL divergence, a special case. Okay, so this is just you have whole class of divergence functions, alpha divergence, but then which is then a special function, special case of the Cesar's F divergence. So now the question is, how do we generally construct divergence function? What's the principle behind constructing divergence function? So this is the question that I have investigated many, many times. Question that I have investigated many many years ago, and uh, it was actually first from a paper by Zhu and Rao in 95. So, what they look at is this alpha divergence, Amari's alpha divergence, they cast this alpha divergence in this way. So, Amari, see, Amari just put this one here, see, like one here, one. So, they had cast this so-called extended. So, they put this term here, but this term is integrates, see, if integrates to one and one and one. One and one and one. Okay, so, but this move is very significant because it allows you to look at this divergence function, the alpha divergence function, as a difference between the arithmetic average and the geometric average of two density functions or two numbers. So actually, this because of the fundamental inequality between the arithmetic average and geometric average, we have this hold for all. Hold for all sample values of the Z. So this is a, you don't have to integrate. You don't have to have the integration. This holds the non-negativity stands for any positive values because pz and qz are positive values and they always hold. Okay. So this points to a link with the actually the convex analysis. Okay. So the same can be found in KL divergence. KL divergence basically just you can look KL divergence basically just you can look at this when we extend it, we put this extended KL divergence, and it's essentially this inequality. The integral is really this inequality, which is a convex inequality, which is really so, okay, I have, so we, so, okay, so we actually, I don't have time to actually go to talk about convex analysis. If you know convex analysis, it turns out that the definition of the convexity allows you to construct a kind of divergence function. This is called a d-alpha divergence, and I studied in my paper. Alpha divergence, and I studied in my paper. So, this d alpha divergence, this is a for any strictly convex function f, you can construct this non-negative number and taking to extreme alpha go to plus minus one, that becomes this extended KL divergence, you know. Okay, so this is a, let me see. Okay, so this is the convex duality for this f function, which I have to skip. Yeah, yeah, so eventually, by this line of arguments, we can construct a two-parameter. We can construct a two-parameter family of this divergence function, alpha, beta, alpha, beta, where alpha is this mixing parameter, but the beta is indexing the here. This is basically the weighted mean, the weighted mean of the P and Q. So now the advantage of this alpha-beta divergence is that, number one, when alpha goes to some special values, like as alpha equals one, you can take the limit of this expression, it becomes this amount. Expression, it becomes this Amaris alpha divergence. Okay, this Amaris alpha divergence. Now I use the beta to index it. When this parameter goes to minus one, it becomes Amaris alpha divergence again. When beta go to one, it becomes Amaris alpha divergence again. But when beta go to minus one, it becomes what is known as the so-called Jensen difference. So it's a different member, Jensen difference. Okay, so the introducing of this. The introducing of this two-pronged family of divergence allows us to separate two kinds of dualities in a divergence function and in the geometry to follow. So one duality is that you see when the for p and q, if I switch the alpha to minus alpha value, I switch q to p. Okay, so this is alpha is a reference duality. It allows me to switch the two. Switch the two. Beta has the role of a tool called a representation duality. So beta allows you to switch from one minus beta two to the one plus. So, okay. So in the extreme case, this is to the p to the log of p. So if you ever are curious about the EM duality, the EM duality being the mixture in the ordinary density function, a mixture in the log density function, where does it come from? Where does it come from? This is precisely the representation duality because it's controlled by this parameter, beta. Now, alpha and beta are different. So, informing geometry, there are two kinds of duality. One is a so-called reference duality, which has to do with how the two density functions, when you have reference of one and the other, right? You have a simple switch, then you can recover that, you know. And the other is a representation duality where it's switch between p and log p, essentially. Okay, so when p go to uh So, so when p go to one, okay, this is uh basically this is the p and the log p because p raised to the power of zero, you can by a limiting argument, this is log p. Okay, this is like I will show by you can take some limit. So now, uh, yeah, this is a Jensen difference. This is Jensen difference. So let me see. I have uh, okay, it's already five minutes past my allotted time, but I have the third part, uh, which is the geometry, the geometry structure, statistical geometry. The geometry structure, statistical geometry. So I would have to maybe just quickly explain to you what has happened here. And hopefully, when this is recorded, you can go back and verify yourself each of the steps. But I do this very quickly. Okay. So if you take the KL divergence, I can take the, you know, just take the first order derivative, you know, because we can see that this actually is zero. So we can see the first order derivative, partial x and partial, you know, y, and they are all zero. You know, y and they are all zero. So, this means the KL divergent is smooth at its minimum value, you know, x equals y. Okay, so so x equals y is this minimum, and then it's the first derivative are all zero. So, there it's smoothly. So, now what's important to do is to investigate the second derivative. When you do the second derivative, now you take the mixed derivative of i and j, okay, x, i, and y, j. Okay, and sorry, I should, I. And sorry, I missed the negative sign here. There's a negative sign here, okay? After taking this and then put a negative sign, okay. Now you get this quantity, okay, this quantity, okay? Now, this quantity itself is symmetric in X in I and J. Okay, but this is not symmetric in Ij. Now, remember, Kl is not symmetric in Ij. Okay, so you need to really verify this. You need to verify that if I take X I Y J and let X equal Y, it's the same if I take X. Y is the same if I take xj yi and let them equal, and this is precisely this quantity called the fisher information. So the second derivative of this becomes the fisher information. Okay, that's a so yeah, look like I am running over grocery times. Pablo, what should I do? Well, sorry, I was gonna suggest this might be a good time to maybe stop for a quick question session. And because we have an hour. Because we have an hour of break, maybe I mean, precisely. I mean, the time allocated for the talk is one hour. So, if you want to wrap up in the next 10 minutes, that would be good. All right. Okay. So, I'll wrap up in 10 minutes. Okay. Okay. Thank you. Yeah. So, yeah, I will skip through some of the quantities about like, so eventually, what you do is that you are able to actually look at the second order quantity, right? Second order quantity, right? Second order quantity is the Riemannian metric. This is the Fischer-Raw metric. Now, once you get the Riemannian metric, you can calculate so-called geodesic, Riemannian geodesic. Riemannian geodesic is the shortest path that connects any two points, right? So you can basically do a variational calculus. You have the G, and you can do the variational calculus, and that gives you the geodesic equation performing order Lagrange. Then you can calculate the geodesic, then you get this so-called. The geode, then you get the so-called uh Levisida connection. So the Levisila connection is arises in the context of the Riemannian metric, in the context of calculating the Riemannian geodesic curve. You can get this Levisavida connection. That's how you get this. Now, once you, the Levis-Vida connection, at the moment, you just look at this as being a bunch of functions, a bunch of functions on the manifold, okay? And they give rise to this Riemannian. Rise to this Riemannian geodesic curve. And this equation, the second-order ODE, tells you what is the differential equation that characterizes the shortest path on the manifold. Okay, so this is the Riemannian manifold. So you can have this second order ODE that gives rise to this, or the solution would give you the this. Okay, so here is the example. For instance, for normal distribution, you can actually calculate all these things. You can calculate all these quantities, and then you can write out the And then you can write out the ODEs for the normal distribution, and you solve this. Okay, you can solve ODEs. You know, this is just a bunch of solving ODEs, and that gives you the Riemannian geodesic, the part that connects the two points with shortest distance. Now, for the Riemannian geodesic, it has the following property, very interesting property, that if now you look at its tangent direction of this Riemannian geodesic, okay, and you look at this tangent vectors, the tangent directions of this. vectors, the tangent directions of this curve. And these tangent directions are said to be parallel transported along the curve. They're said to be parallel transported along the curve. So, and this parallel transport is using this so-called Levisida connection. So along with the Riemannian geodesic, you always have the Levisida connection. The Levisida-Savita connection is a derivative of the Riemannian metric, and they give you the notion of parallel transport, a notion of the parallel transport. So, so on the manufacturer. Period transport. So, so on the manifold, you have these things that happened that these structures which are prescribed independently. One is a metric, the other is a linear connection, and finally there's volume and there are other things. They are all independent structures. What people didn't realize is that you can have, you can prescribe a Riemannian metric, but you can also prescribe other linear connections other than the Levisivita connection. You can do other connections and information geometry precisely gives you the opportunity. Gives you the opportunity to deal with other connections other than the V Civil connection. And these other connections form a dual relationship. And how is that done? Well, it turns out we are using the third-order quantities of the KL divergence. We use the third order quantity. We calculate this gamma and gamma star. And the average of this gamma and gamma star, it turns out to be the Lavisivida connection. So, and this gamma and gamma star can have the interpretation of the so-called dual connection or conjugate connection with respect to this Riemannian metric. Respect to this Riemannian metric. So that's where this duality comes from. So I don't have time to get into detail, but essentially, this is the whole, this is the story. So the difference between this gamma, gamma, star is the amount of change of tensor, so-called third-order invariance. And it is known that on the manifold, this quantity is an invariant quantity on the manifold. Okay, so yeah, from this, you can then construct the alpha connections, which is just a generalization of the E and M connections on the manifold, right? On the manifold, right? So I guess, yeah, from this, I'm probably just going to skip through the axiomatic derivations for this FN connection, what FN connection is and so forth. So eventually, FN connections essentially would give you so-called a curve, which its tangent vectors are parallel transport of itself. So if you have a curve on a manifold, if you look at the tangent vector, if it's parallel, if you move. if it's parallel if you move it parallel to each to itself along this curve that becomes that it's called the that is called the auto parallel curve it's autopara curve f n connection defines the meaning of the auto parallelism so what is meant to be autopara curve okay so and then the next uh message uh i really want to emphasize and i'm going to stop which is for any divergence function for any divergence function any divergence function we can induce this kind of a geometric structure so can you Of a geometric structure, so we can induce a second-order Riemannian metric and a third-order, which is a pair of conjugate connections. That's for any divergence function, it's very general, not only for Fischer-Raw, not only for F divergence, for any divergence, you can think about, okay. So for especially for the, if you use the Braggman divergence, the structure introduced or induced is the so-called Hessian geometry, is the Hessian geometry. So Hessian geometry is just basically a very, very special. Just basically a very, very special kind of geometry. So, I uh, yeah, I really should stop, otherwise, Pablo is going to get mad at me. So, I mean, so I was. No, on the contrary, on the contrary, I'd like to understand all of this with a lot more detail. Yeah, yeah, it's just a lot. I'm teaching this, I'm teaching in women's geometry class. So, it turns out that I'm expanding my lecture. Maybe I can suggest that. Yeah, but I just want to finally one final word, which is recently, yeah, recently we are actually able to link this geometry with the mirror symmetry, with the statistical, we call it statistical mirror symmetry. There are two ways of looking at this parametrical model, the standard way and our new way, and then we can link that to so-called mirror germs. So what we have right now is this is what I want to show you. So the statistical manifold, the manifold probability density function somehow is miraculous. Function somehow is miraculously linked with some very interesting theories in string theory, which is the mirror symmetry and so forth. So I cannot go into that for sure. So I thank you very much for your attention and welcome comments and looking for collaboration with all of you. So thank you. Okay, thank you very much, Jun for your tutorial on information geometry. Certainly it's a lot of topics and very difficult to cover in one session. Cover in one session. So, I hope that those who are interested in the subject and following from your presentation will be able to check on some of the lectures that you have already made available on your website. Okay, do we have any comments or questions for you? Actually, I wanted to maybe repeat one question that appeared in the chat unless somebody else wants to. Yeah, so I'd Yeah, so this was actually a question when you presented the Gaussians and discussing why we wouldn't want to use Euclidean geometry. Yes. And so maybe one answer is that the Gaussians that are sharper would be easier to tell apart based on samples. But what would be kind of your perspective on this? So why would we want actually to say that these two Gaussian should? Actually, to say that these two Gaussian should be farther apart? Oh, these two Gaussian farther apart, that's very straightforward. This is like so the in statistics, if you look at this is really the difference between the you know the difference in mean over the variance, you know, statistical, very simple statistical tests. All the T all the statistical tests tells you, you know, that these two distributions are separate. So, right? So, that's a so that one is less puzzling. I think the more puzzling part for the Gaussian distribution, I really want an understanding. Distribution that I really want an understanding, and still we don't have, and I really invite other people's comments: is that we have this parametrization which gives you the fish information very simple here, but you cannot write this fish information as the second derivative. You can't find any function whose second derivative give you this. On the other hand, if you look at the x, look, treat this as an exponential family. So, this is when you treat this as a location scale family. We treat it as a location scale family, you don't get it, it's not a Hessian geometry. Now, when you Geometry. Now, when you get the, if you look at this as an exponential family now, but you use x and so forth, you can write this as a as the Hessian geometry, the second derivative of this and so forth. But the coordinates are very ugly, you know, in terms of the g, you know, it's very ugly, very ugly, like this. So right now, it turns out that mathematically speaking, the geometry of this and geometry of this is very interesting because it turns out this mirror symmetry, we link that to some very deep theories in mathematics. Very deep theories in mathematics. So, this is our recent paper with Khan and Gabriel Khan, my postdoc. And we talk about so-called Kayla mirror symmetry. And then the recent paper is an archive to follow up on our statistical mirror symmetry paper. So clearly, mathematically is interesting, but then statistically, we know this parametrization is basically the moment parametrization, right? And this is the dual to that. You know, this is the entropy function. This is the Function. This is the cumulant generating function. But why the G look so ugly? That's what. Okay, yeah, that's indeed very puzzling. All right, so are there any final comments? We are a bit over time. Final question for you. I was going to suggest if, I mean, because I would like to ask more questions, but we could probably continue in Gabbatown. Okay, sure, of course, so that people can leave here and then. So that like people can leave here and then because we also have the final session today. I don't know if you want to do the advertisement for that. So I will then go to Gathertown. Is that right? Is it right now, right? Gathertown is saying right now. Yeah, we can meet there and like continue. So okay, so okay, stop. We're trying to wrap up your talk, Jun. Let's see if there is any final question at this time. So if there is not any final question at this time, Not any final question at this time. Please help me. Thank Jun again for his talk. Thank you. All right. So thank you very much, Jun. So now we're going to have a break for one hour, followed by a discussion session in Gathertown. Gathertown is always open, so everyone who is interested in following up the discussions is welcome to do that in Gathertown. So Jun, if you are available, you know, a couple of people evidently are very interested. Of people evidently are very interested in continuing the discussion with you, and so that would be um that would be great. Sure, yeah. Don't go there. So I'm going to just log on to the it's available, right? The cataloging is still available right now. Yeah, so fine. Okay, sorry, guys, and I took up a lot of your time. No worries. Thank you. Thank you very much for having me. Yeah, thank you, Europe. Thank you. Thanks, Cabro. Yeah, thank Guido. Yeah. Yeah, thank you. Yeah, thank you.