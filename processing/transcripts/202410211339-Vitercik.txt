With the use of machine learning to improve the performance of algorithms for these problems. And so our goal is to use this structure to optimize algorithm performance on future problems. And by performance, I mean, for example, the algorithm's runtime, its solution quality, and so on. And just to give a bit of history, so the problem of algorithm selection first appeared 50 years ago with this work of Rice. The first paper I'm aware of that started to The first paper I'm aware of that started to think about how to use machine learning or deep learning in the context of algorithms or combinatory optimization was this paper by Jean and Dierik on RL for scheduling. Then in the kind of late, around the turn of the century, we started to see more and more data-driven approaches to algorithm selection and related problems. Like Eric Horbitz had this early paper on predicting the runtime of SAP solvers. Then, in that early 2000s period, In that early 2000s period, we started to see a huge amount of research on using machine learning for algorithm selection. So, for example, a lot of work by Kevin Leighton Brown's lab, as well as algorithm configuration. But these were still not using particularly like sophisticated machine learning models, maybe like regression trees and things like that. Well, in the late 2010s to present, there's been a huge amount of work on integrating deep learning into combinatory optimization. Into combinatory optimization. And there have been some great surveys on this topic, for example, by Ben Gio et al, and this really nice survey by Capard et al. published this year in JMLR. And of course, a lot of work on theoretical guarantees for ML for discrete optimization. And the key challenge in this area is that deep learning or machine learning approaches for combinatory optimization rarely work just like out of the box. Like you can't just feed like Like, you can't just feed your traveling salesman problem to feed forward neural network or something like that and expect anything to come out. It doesn't even really make sense as a thing to do. And rather, these deep learning approaches need to be aligned to the algorithmic task at hand. There are a bunch of major challenges in doing so. First of all, is what architecture should I use? So, in say like the mid-2010s, people started looking. 20 teens, people started like kind of the earliest neural models people started in this area were like sequence-to-sequence models. So there was this model proposed really motivated. It doesn't really actually make sense to use sequence-to-sequence models, really, fundamentalization. And a few years later, graph neural networks really came onto the scene and kind of took over as the neural model people were using. As a neural model, people are using for combinatory optimization. But there's been some work outside of combinatorial optimization, but like algorithmic reasoning more generally on using transformers. And even once you fix the architecture, like GNN, whatever, there's still a lot of interesting questions you need to ask about like what hyperparameters of my model are best for the particular algorithmic task that I'm trying to solve. So that's one big challenge. Another major challenge is supervision. Major challenge is supervision, which asks, like, how should I supervise on MP-hard algorithmic problems? Or should I even bother supervising? Should I just kind of hang up and use RL? And then finally, robustness. Can we provide any types of provable guarantees? And just like a quick plug, I taught a class in the spring quarter on these topics, and I worked really hard on the lecturing website. So if this sounds interesting, I encourage you to like check out the lecturing website. Check out the lecturing study. But in this talk, I'm going to try to address these three different problems for a specific problem, which is online matching. And this was an ICML paper this summer with Alex Haydari, Amin Saberi, and one of my PhD students, Andres Wakun. So online matching is a very critical problem in many digital marketplaces, including advertising, crowdsourcing platforms, ride share. Forms, ride share, medicine, like matching OVIT borgen donors to patients, and so on. And the primary challenge in online matching is that we need to make these irrevocable matching decisions online without any real precise knowledge of how demand is going to evolve over time. So, in this paper, we look at how we can use GNMs for this online bipartite matching problem, and we have both theoretical and And we have both theoretical and empirical contributions. From a theoretical perspective, we justify why GNNs make sense as architecture to use for online bipartite matching. And then from an empirical perspective, we train a GNN for online Bayesian bipartite matching. And we supervise with the actions of the online optimal algorithm. So trying to get at this question of how do we supervise ML models for online for NPR problems. For online NPR problems. And we show that it outperforms these baselines, classic baselines on various graph families. And the key challenge we face in this setting is just the sheer complexity of this online optimal algorithm, which we're trying to train the GNN to imitate. For example, it's known that it's NP-hard to provide a constant factor approximation to this online optimal algorithm for some constant alpha. Thompson off that. So, just a little bit about related work. So, there's been some work that focuses on this, like ML for online matching from a purely theoretical perspective. Some papers that use online learning to find good worst-case algorithms where we're really looking at more of an average case study. Most related is this paper by Almirani et al. who learned a tailored matching policy using reinforcement learning for every given graph. For every given graph configuration that they consider, whereas we try to provide a kind of more general model that applies to many different graph families. And finally, I'll mention a paper by Liodal that tries to bridge this gap between worst case algorithms and more average case algorithms, where they dynamically switch between computationally expensive expert policy and ML predictions. And maybe our work could be combined with theirs and use theirs to provide some robust-based guarantees. To provide some refuge-based guarantees. Okay, so now a bit of background just on online basing and bipartite matching a bit more formally. So in this problem, we have a bipartite graph with online nodes and offline notes. And so you can think of the offline notes, for example, in a crowdsourcing setting as like the crowd as the work crowd workers and the online tasks as tasks, or the online notes as tasks that are coming in. That are coming in. Each edge weight is, there's going to be an edge weight between all the online and offline nodes, which could represent, for example, the workers' payoffs for completing tasks. And each node has some probability of arriving at each time step. So node 1 will arrive with some probability P1, node 2 with probability P2, etc. And now, if a node appears, node T, online node T appears, so just to clarify, the offline nodes are there. Just to clarify, the offline notes are there at the beginning of time. Like at the beginning of the day, we know who's available to work on these tasks at the beginning of the day, but the online tasks are arriving online. And so if this task arrive, we need to irrevocably decide who's going to be matched to whom. And or should we just skip that task, skip that online node today, and not match it to anybody, leave some offline nodes available for future matches? Matches. And our goal will be to compute a highweight matching. Okay, so there is an optimal online algorithm for this problem, which I'll describe now. So let's say that we've got our graph, and so node 1 has arrived, and we matched it to this guy here. Node 2 has arrived, and this set S is going to be the set of offline nodes that are available at this unit round. Given. The online optimal policy computes what I'll call the value to go, or the Bellman equation, which is defined as follows. So the value to go, given this set of available offline notes in the current round, is the expected value of the maximum weight matching achievable on this graph, where the expectation is over the future arrivals, and the matchings are restricted to the available offline notes. And we can write And we can write the value to go function out analytically. It's very simple. So, first of all, if node t does not arrive, we don't get any value from that node. So, our value to go function is just kind of going to skip ahead to the next time step. And the same offline nodes are still available. If the node does arrive, then we have a choice. We can skip it and get no value. skip it and get no value from that known t. And again, we're just going to, our value to the function is just going to skip ahead to the next time step. Or we can match it to some unmatched neighbor. And this is the set of unmatched neighbors, the neighbors of no T, together with the guys who are actually there available. And we're going to get the edge weight associated with that online note and the offline. With that online node and the offline node we're matching it to. Then we're going to remove that guy we matched it to from the set of available offline nodes and we'll skip ahead to the next task. It could be say the value that you get from matching this worker to this task or it could be in ride share like the you can think of it maybe as revenue that's probably the third profit situation simplest way. So you could think of it as Deterministic value, yeah, yeah. And so if no t does arrive, we, to figure out what we actually do, we just compare, well, what value we would get from skipping it versus what value would we get from matching it to its most profitable labor. And so, yeah, we don't match if the blue is bigger than the yellow, and otherwise we match it to that labor. So, that's background on online basic. So that's background on online basic and biotech matching. Now, let me give you some very quick background on GMNs. And in particular, I'll tell you about this GenConf architecture, which happens to be the one that worked best for us in this setting. So in a graph, a graph neural network, we'll have this graph, and every node is going to be associated with some feature embedding. So I've just illustrated those feature embeddings here. And so I'm going to denote that initial feature. I'm going to denote that initial feature bedding as H0V for node C. And you can think of this as like the weather at that node if we're talking about a map, a road network, or it could be like congestion, some kind of features of the graph at that node. Or it could be like positional encodings like Muana was talking about. So on each iteration or layer, so GNNs operate over a series of layers. Operate over a series of layers, capital L layers. For each node V, they're going to run this message pass, simple message passing operation. So, first, each node is going to look at, is going to aggregate its neighbor's embeddings. Here in the GenConf architecture, that happens to be by doing like looking at that node's embedding from the previous time step, running it through some like one-layer neural networks. Like one-layer neural network, some multi-layer perceptron, and then taking the element-wise mass over all of the neighbors that embedding. So, yeah, just a simple aggregation of its neighbors' embeddings. And then, so that node now has its old embedding together with its neighbor's aggregated embeddings, and it's going to use these two embeddings to update its own embedding. So, for example, maybe it takes that MLP of like summing up these two embeddings together or something. Up these two embeddings together, or something like that. So, again, just a really simple message passing operation over a series of L layers or steps, where again, you're just like aggregating your neighbor's embeddings and then updating your own embeddings, aggregating your neighbor's embeddings, updating your own embedding. And then finally, we use the final embedding to make predictions about each note. Okay, so that's Okay, so that's the basic GNN architecture. So, now how do we use GNNs for online matching? So, we call our architecture Magnolia, matching algorithms via GNNs for online value-to-go calculation. And so, at each timestep, we've got this matching step, which tells us which nodes have already matched, which node just arrived, and which nodes haven't yet arrived, together with a skip node, which will Together with a skip node, which will match to if we're skipping that node. Then we're going to add features to this graph. So we'll add features like, has it arrived, is it an online or an offline node, this value pt, and so on. We'll run this attributed graph through the GNN, and we'll use the GNN to predict these marginal value to go. Values, what is marginal value to go? Value to go. It's just the value that we get, kind of the difference in the value between matching this blue node to each of these offline, available offline nodes. So the value we would get from matching to that node versus the value we would get from skipping, matching this blue online node that just arrived to any other node. And so, for example, this yellow node is the offline node that's available that has the highest marginal value. The highest marginal value can go prediction, so we would match it to the yellow note. Okay, so what about? I'll tell you about now first our theoretical guarantees and then our experiments. So the first, the main theoretical contribution of this paper is trying to provide evidence that GNNs are well suited for this online matching problem. And the intuition is that most practical instances. Intuition is that most practical instances are somehow well structured in a way that makes them very well suited for GMNs. We show this rigorously for this family of graphs, this random graph called random geometric graphs. And we prove that under these families, this value to go function is nearly local. And I'll explain what I mean by that in a second. But it can basically be computed by just looking at local neighborhoods in the graph. Codes in the graph. Meanwhile, just by construction, GNNs compute local functions. So there's this very nice kind of correspondence between what GNNs can compute and what the structure of online optimal matching on these random geometric graphing is. So starting off, what are these random geometric graphs? So they're going to be defined by our set of offline online nodes, a distribution D, and a parameter. A distribution d, and a parameter delta. And so the first thing that we do is draw feature embeddings for each of the offline and the online nodes from this underlying distribution D. In this talk, D will be uniform just for notational simplicity, but we generalize to much more general distributions in the paper. Then, we're going to connect online and offline node with an edge if their feature embeddings are sufficiently Their feature embeddings are sufficiently close. Namely, the L informing norm is smaller than this capital delta. And random geometric graphs have many applications. They've been used to model like spatial matching problems, opinion dynamics in graphs, and contagions in wireless networks. So like I said, it's known and it's kind of obvious by definition that GNNs compute local functions. And in particular, so formally, we're going to say that a So, formally, we're going to say that a function over graphs f is R local if, okay, so this is not the formal statement, so words. I'll write it formally in a second. But if f of g only relies on information in the r-hop neighborhoods of each node in the graph. And R of G is an R hop neighborhood. In other words, we're saying that there exists these functions, psi and phi, such that f of g is basically That f of g is basically you look, you aggregate the local neighborhoods of each graph and run some function over this local neighborhood. And then you aggregate these local neighborhoods with this outer function here. And it's just an observation that GNNs with L layers compute L local functions because, again, the final embedding of this node here only depends on its, say, two-hot neighborhood if L is equal to two. Meanwhile, value to go, we prove, is locally approximable. And what I mean by this, a function f, we're going to say is r epsilon delta locally approximable over a graph distribution g, if there exists an R local function, h such that the difference between f of g and h of g is small. And this should be with high probability over the draw of a graph. Over the draw of a graph from this distribution. And so, informally, what we prove is that for any epsilon delta, and for this capital delta, which we use to define whether or not we add an edge in our random geometric graph, so for capital delta sufficiently small, value to go is approximately log of the size of the graph, epsilon delta, locally approximable, over this rhythm geometry. Over this random geometric graph distribution. So, like I said, value to go is approximable by a local function. And the intuition is that for these random geometric graphs, you can prove that they can be split into clusters with few intercluster edges. So, the way I like to think of this is like in a rideshare application, it makes sense that most of the rideshare connections. Most of the rideshare connections are going to be within neighborhoods, say, like within Berkeley, within Mountain View, within Palo Alto. And there will be few rideshare connections that go across neighborhoods. Like, I don't want my rideshare driver when I'm in Palo Alto to be picking me up from, like, San Francisco or something like that. And then once we split the graph into these clusters, matching just within the clusters is nearly optimal. The one that's uh you didn't talk at all about the weights on the edges. Is this uniform weights? Because presumably, if the weights across edges were somehow much heavier than the neighboring edges, we'd be in trouble, right? Yeah, yeah, but you're we're only adding edges if the feature embeddings are close. So, so that it's somewhat, we're saying that like there won't there aren't even any like, there are very few edges. Right, right. I guess the point is it's like a very fair thing, but like somehow. Yeah, maybe an average scale setting. Yeah, maybe that. Okay, and I'll just wrap up now with some experiments. So, just to say finally, kind of the takeaway of the theoretical guarantees, GNNs are logo functions by definition, and we prove that value to go, this value-to-go function, which the optimal online algorithm is using as a spell manipulation. We prove that this function is locally approximable. So, there's this really nice correspondence between what GNS can compute and what GNNs. What GNNs can compute and what value to go can be approximated. So, finally, experiments. We look at these random graph families: Erdashraini, Barabashi-Albert, random geometric graphs, as well as some semi-synthetic graphs. So, some graphs based on crowdsourcing data, and others, so this OSMNX package allows us to create graphs which are like neighborhoods anywhere in the world, and we create ride-shares. And we create rideshare instances by randomly choosing intersections and saying half are drivers, half are riders. So I'll walk slowly through a few plots. So here on the, we're looking at four different of these Entermark data sets here. So Vertashrini, random geometric, these ride-share instances, and that crowdsourcing graphs. And along the And along the y-axis, we've got the competitive ratio, which is the algorithms referments. I'll explain these algorithms in a second, divided by the weight, the matching weight of the offline optimal matching. Is it easy to compute optimal offline? It's optimal offline, so that's easy. It's just bipartite matching. Optimal online is the one we have to be careful about how big we train on, and I'll get to that in a second. So the different algorithms. So, the different algorithms we're comparing against is just plain greedy. Selects the highest weight available edge at each stream set. Greedy threshold, where we select a greedy edge so long as its weight isn't too small, it exceeds a threshold and otherwise skips it. And then LP rounding, which is the best pen and paper approximation algorithm for this problem. We haven't used this one by Braverm et al. There's a newer one by Noor et al., but Braverman et al. is better experimentally. Okay. Okay, and now along, and magnolia is ours, the red is ours. Along the horizontal axis here, I've got the total number of nodes in the test set, or in the test graphs. So many different graphs, n is the total number of nodes in each graph, left and right-hand set. Now, because we're training on the optimal online algorithm, which I said is like n be hard to even approximate, we have to train on really small graphs, size 6 by 10. So these are only size 16. So these are only size 16 total. I don't know if they'd even show up on this press. But you can see that our performance generalizes to graphs which are up to 20x larger than those we trained on. And actually, we've generalized this even further since we published Xeno Paper. And a takeaway is that Magnolia does quite well, especially generalizing pretty well compared to other algorithms. I'll also mention one other set of experimental results. So, here now, everything's the same except what I'm plotting along the vertical, sorry, the horizontal axis, which is how many online nodes are there compared to offline nodes. And this is an interesting kind of ratio to vary, because if there are many online nodes and very few offline nodes, Nodes and very few offline nodes. So, as we go to the right in each of these graphs, greedy does worse and worse and worse because if there are many, many online nodes and very few offline nodes, you have to be really careful about who you're matching to when this starts to get really skewed. So, around one here, we have just about the same online to offline note ratio. And the takeaway of this is that Magnolia does well, even if Magnolia does well, even in these regimes where the other methods struggle. So, overall, to conclude, we look at graph neural networks for online matching. This is an NP-hard problem with many applications in online advertising, crowdsourcing, etc. And we supervise using this exponential time online uploadable policy. This requires us to train on very small graphs, but based on the size generalization properties of graph neural networks, Properties of graph neural networks, we can test on much larger graphs. And we also provide theoretical guarantees to understand why GNANs are well suited for this task. So in future directions, I'd love to perform, like, expand our experimental results to just settings where there are many more unknowns, like the graph structure, maybe the edge weights, the arrival probabilities, etc. And I think this would just require the right data set where you can learn these quantities from actual. From actual features of the graph. And then finally, other problems. So, really, actually, very little of this paper is truly specific to online matching. I think this locality analysis would likely apply to other problems where we have this same phenomenon of random geometric graphs allowing you to solve the problem locally and have that be a nearly optimal global. Global solution. Well, so that with that, I'll wrap up and take any remaining questions. Thank you. Yep.