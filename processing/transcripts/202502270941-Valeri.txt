mental health professionals to change people's behavior for health reasons, encouraging physical activity or stop drinking, stop smoking. So we've all seen these interventions from time to time. And in fact, this is the reality that you get overwhelmed by these interventions, not just by these health apps, but by others. So at least what I do is just So, at least what I do is just to turn off these internations and don't get bothered. Now, that's not good for an environmental intervention perspective. So, what we want to do is to understand how these particular health interventions, these mobile health interventions, influence people's behavior and how to optimally design them, how to optimally sequence them so that we get the benefits out of them but don't burden the participants too much. Burden the participants too much. That's the overall scheme, and we'll delve into a very specific study in this case and develop some methods to address challenges. So the plan for today is we'll review a mobile health study and the findings, we'll identify some knowledge gaps, we'll propose a new method to address these gaps, and then we'll apply the method to obtain some new insights for improving intervention. Intervention. So, this was the hard steps micro-randomized trial that Walter was talking about. But given that this is very important to set the stage, let's go through it one more time. So, the goal of the study is to develop smartphone prompts for physical activity. And these are some example prompts. And I'm not sure if you can see it back from the back, but the left one is suggesting, you know, taking a few minutes' walk. So, this is called a walking. So, this is called a walking suggestion. The right-hand side is suggesting some stretching activities. It says stretch your feet, lift heels, and press toes into the ground for 10 seconds. So in this study, we call these anti-sedentary suggestions. These are just two examples, some messages that you can receive as a participant. And there are a whole bunch of different framings so that the participants don't keep receiving the exact same words. Then we have these two types of We have these two types of products or interventions, mini-interventions that we can deliver. And this is what happens in the actual study. There were 37 participants, each enrolled for 42 days, so six weeks. And then for five times a day, each participant is randomized. So here I'm using J to denote the decision point. X sub J denotes the observations you Subj denotes the observations you collect before the randomization occurs. And then a randomization occurs with probability 0.6, you know, five times a day at each of these times. With probability 0.6, the participant will receive a prompt. With probability 0.4, they will not receive anything. So if they receive a prompt, they will receive equal probability, one of the two types of interventions I showed you on the earlier slide. So after the prompt is sent or not sent, Prompt is sent or not sent. We collect their step count information. So the participants wear a tracker throughout the study, so we have their minute level step counts. Yes. Why are these like numbers? So like why 42 days? Why organizations? Oh, why are these numbers? Yeah, was it just like decisions had to be made, or are there like... There are some rationales, but you have to make some decisions at the end of the day. At the end of the day. So, this is for six weeks, and then five times a day, these were, I believe, from a prior study to this, the researchers analyzed people's physical activity step counts and see that for five specific time periods of the day, their step counts are more variable. So, the researchers thought these are better times to send these interventions because Not interventions because since the participants' step counts are more variable in these times, maybe these are better opportunities to change their behavior. And these will correspond to like the morning commute, lunchtime, etc. And they are set specifically for each different participant. And then these numbers, the probabilities, they correspond to, on average, how many interventions to send. That's not deemed too burdensome. In this case, five times a day. So, in this case, five times a day, 0.6 would mean on average three messages per day from each day, but it's a reasonable amount. Yeah, so they're all like scientific considerations rather than statistical considerations. For statistics, we would want 0.5 versus 0.5. So, that was the data from a single decision point for a single person. This is a longitudinal data for one person over 210 decision points. Over 210 decision points, five times 42 days. So, this is the type of data. And there was a study published using actually the weighted center d-squares method that Walter talked about. The methodology was designed to analyze this study. So, they found positive effects over the next 30 minutes. And they found that, I think I actually opened a wrong version of. Actually, I opened a wrong version of the slide because this has 36 slides. But actually, I had much fewer slides. Do you mind if I switch to another five? Sorry. Yes, this was for an hour talk. Sorry. Ah, this is one. But I mean the beginning parts are the same, so you didn't Okay, yeah, yeah, yeah, this is the pedro. So they found still 28, but lots of outline slides. So positive event over the next 30 days, great. And then effect decline over the days in the study, as expected. So that's part of the reason why they choose six weeks. Part of the reason why they choose six weeks instead of, say, half a year, because people get habituated very fast to these types of interventions. Now, these are all great, but there are some unanswered questions. The first one is, was a 30-minute window appropriate for addressing causal effects? And I realize I should share screen again, given that I have messed up the link. Okay, sorry to those online. To those are like, I don't think they're able to do that. Okay, they looked at 30 minutes. That was sort of a judgment call made by the scientific team. But why looking at 30 minutes after each decision point? Is that the right window, or should we look at a wider window, narrow window? Another question is: what about a finer time scale? We actually have the rich data of minute by minute. The rich data of minute-by-minute step count. So instead of looking at the aggregate, maybe we can find more detailed patterns to inform us when the effect takes place. How fast do people respond to these interventions? So to put everything in mathematical notation, this would be the data structure for a single person. And there are arrows all over the place, but I don't want to draw too many, so I use these thick arrows. Many, so I use these thick arrows to represent all arrows from all the left nodes to all the right nodes. So I'm not making any Markovian assumption, et cetera. So the challenge here is how can we model these time-varying effects of these interventions given this type of data structure? And in particular, notice that we have many, many J's. We have J from 1 to 210. It's not like there are just three time points where you can apply collection. Time points where you can apply classical causal inference methods. So, the first challenge I talked about: how do we define the causal effect? Second is how to handle this type of functional plus longitudinal data. Third challenge is we want to use these rich information in a robust way. We don't want to require correct model specification. So, we'll see that there are some ways to address all of this. All right, so now let's So now let's first define the cause of effect. This is the data from X are covariates, A are treatments, in this case always randomized with 0.6 probability, but in the method formulation we allow the randomization probability to depend on the history, so that if you're in an observational study, you can make some assumptions and model that treatment assignment probability. And then the YJ, the outcome. Yj, the outcome at the jth decision point is a vector. Or here, I'm viewing that as a functional outcome. But when I was giving talk to some places, some functional data analysis researchers don't agree this is a functional outcome, which I actually understand. And I should say I'm omitting subscript i throughout this talk almost, so that I'm not providing too many subscripts. Okay, so. Okay, so previously Walter talked about causal excursion effect. Here I'm also going to use the causal excursion effect, and I want to show you this visual to motivate what is happening in these cause and effect definitions. So, you know, in the traditional cause and effect, right, you would contrast y0 versus y1, the potential outcomes. If there are multiple time points that you can assign treatment, you would be contrasting y0, 0, 0. You would be contrasting y 000 versus y 001 or all these combinations of 0s and 1s. But in our case, when we have j up to 210, we can't do that combination. There will be too many parameters. So what do we do? Here I'm illustrating j equal to 3, but imagine j is actually a very large number. So we have these participants each receiving, you know, because of randomness in the trial, each receiving different combinations of the treatment sequence. The treatment sequence. And here I sorted so that the left are all receiving A3 equal to 1, the right are all receiving A3 equal to 0. This is just by sorting the participants. And I'm focusing on the outcome after A3, in this particular case, in this illustration. So let's ignore what happened, ignore or average over what happened in the earlier time points and only focus on A3. So if we do that. So, if we do that average, in fact, the average of the left is rainbow-colored A1, A2, and then A3 equals 1. So, we can calculate the potential outcome, average potential outcome, for all these participants, which can be represented in this particular way where A1, A2 are written as random variables. So, when doing this expectation, A1 and A2 are averaged over, which corresponds to A1, A2R. Which corresponds to A1, A2R, rainbow-colored, or technically, following a policy that's implemented in the micro-ANRES trial. Similarly, we can do the same thing to get rainbow-colored A1, A2, and A30. So then we would contrast these two potential outcomes as the definition of the causal effect of A3 on Y3. So you can imagine this generalizes. And this generalizes easily to a large number of J's, because however large the J is, we only contrast the last treatment assignment. We're averaging over all the past. So these were called excursion effects because the rainbow color are the MRT policy, randomization of 0.6. And these highlighted parts are excursions from the MRT policy by deterministically assigning one versus zero. Versus zero. That was the reason for the name. Now, moving on to the functional outcome, notation gets more complicated. We have two subscripts, J, decision point, T, time point. But we're essentially doing the same thing, right? Averaging over past treatments and assigning one versus zero. So with that, here is repeating what I just said, two treatment policies or two excursions from the micro and micro policy. Management policy. And the S are effect modifiers that you can set. And Walter presented some nice way to do selective inference when you put in a large number of S. For our purpose, for this project, we only look at very few numbers of S. And this expectation is averaging over everything in here, all the capital letters except four variables, including the mass. So this is called an excursion. So now that we have defined. So, now that we have defined the potential outcomes, a standard next step in causal inference is to identify. And for identification, we are doing micro-randomized trials, so we're randomizing treatments. So all these assumptions hold. You can take my word for it. So that we can write the cause and effect as observed variable distributions using iterated expectation. So the next step is: how do we model and estimate the cause and effect? A cause and effect. So, how do we do that? This is the way we do it. Varian coefficient model. Alright. So, this was the cause and effect. Let's look at some examples first. If we set SJ to be empty set, which is the most common approach, or the most common first step in a microarray micro analysis. Also the most common first step in any RCT analysis. Step in any RCT analysis. You look at the average effect. So you could have a function denotes how the effect over decision points over the course of the study. Another function characterizes how the effect varies over minutes within a 60-minute window. We could have varying coefficients if we want to include, say, effect moderation by real-time location. So more generally, we just write general functions f0, f1, f0. General functions f0, f1, f2 as pre-constructed features, and then we still have these vector-valued varying coefficients. So we use v-spines in this particular project, or we could have used other non-parametric expansions. So now to the estimator, we did a two-stage estimator to utilize all the past history information. Now remember, Information. Now remember, in defining the cause of effect, we average over the history, but in the analysis, these are the history information are control variables if you're used to doing RCT analysis. So it will be beneficial to adjust for them to improve efficiency. So here what we did is first we fit a model for these kinds of relationships, how the outcome of the specific step depends on the history. It turns out we will have It turns out we will have some robustness properties, so this first step can be done in any arbitrary way using any arbitrary supervised learning method you want, and it does not need to converge to the true condition expectation. So then, in the second stage, we solve this estimating equation, or rather this objective function, to find the betas. This is the estimated Newcomb's parameter. And then, Newcomen's parameter. And then here we have the causal excursion effect that we imposed, the causal model. And we minimize this to find the betas, or more specifically to find the regression spine coefficients. Here the w is essentially inverse probability wave. We have some theoretical results, and I want to highlight this part the robustness against misspecified first stage. First stage supervised learning models. We do not require priority modeling this, and the reason is we have randomization. Okay, now let's look at what can we learn from the study. So we looked at the effects on log transformed minute level step count. We didn't have to do that, but it makes the scale a little bit easier to interpret. Otherwise, it's very zero. Otherwise, it's very zero-inflated and some pretty large numbers. Okay, so first we look at the marginal effect, and this would be the surface for beta 1 and beta 2 together. So we have, let's look at some cross-sections. To the left, this is how the causal effect of a walk-in suggestion, one of the two type suggestions I showed you, compared that with no suggestion at all. How that changes over decision points. How that changes over decision points throughout the course of the study. So we see a decline, you know, maybe an initial increase, but it declined and covers zero very fast. So people don't care about these interventions anymore. Or maybe they developed good habits to take steps so that the interventions are no longer needed. Let's hope that's the case. Another cross-section over the 60 minutes within the decision point, we see that the fact increases. We see that the effect increases, then decreases. That's reasonable. It will take some time to respond to these notifications. Maybe it takes them like 15, 20 minutes before they can finish what they're doing at the moment and start to act on these interventions. For the anti-sedentary suggestions, we see similar patterns, decline over the course of the study, and some effect early on in the 60-minute window. And we also looked at effect moderation by weekday. At effect moderation by weekday versus weekend, and we see on weekdays, this is also showing you effect over the 60-minute window of a selected day. So positive effects, maybe up to 40 minutes, then nothing. For this other type of suggestion, the effects window seems to be much narrower. And then on weekends, we have fewer data points, confidence intervals are wider, and also the effect seems to be more. Effect seems to be more to be small, which may suggest that on weekdays these are better times to send these interventions. So, to summarize, we handled these three challenges using a marginal causal effect definition, a double index varying coefficient model, and a robust two-stage estimator. And for the hard steps, we found that the effect window, should that be 30 minutes? Well, in fact, the effect window seemed to be between In fact, the effect window seemed to be between 10 to 40 minutes. So maybe expanding the effect window definition to define the outcome in the next iteration of the study is a good idea. And how the effect changes over time declines very quickly and not necessarily linearly over time. And the effect depends on weekdays, weekend. We also looked at whether the person is sedentary at the time, and that also shows significant effect moderation. Effect moderation. So that's all I have, and thanks for your attention. Yeah, uh, very good talk. I think I have two technical questions, but maybe one technical practical question. So I start with the practical question. So I got involved in it, but you received this. Probably don't have the time to Have the time to do it. So maybe I receive a message, say, hey, Jim could take a lot. I'd like to say okay, I'll do this, but I have to talk. That accounted for somehow. Or if not, then is this so the reason the researchers originally selected three minutes is because they have this this reasoning in mind. This reasoning in mind. In fact, the interventions just take like a few minutes to complete, but they looked at 30 minutes exactly because the researchers expected the people to, you know, they need to take some time before they can respond to these interventions. So that was the rationale for the 30 minutes. But in this work, we sort of empirically verified or assessed what's maybe a more comprehensive time window to look at. Window to look at. So I would say both the original study and this particular analysis take that concern into account. That makes sense. The question is about the additivity assumption. Right. So I was trying to look at your effective curve. Right. I guess the top right one. Well, let's open the tailing now. That's what's going on. Oh, over here? Yes, I blame the spine basis. It's fun. It's hard to imagine why such a message encouraging you to do like a very tiny activity would have effect going down, then up again. So, and in fact, I'm not showing this here, but we also looked at 90 minutes as the time window, and there is an even bigger swinging up at 90 minutes. That's very weird, and all I can do is to blame the spy basis, but maybe there are. Supply basis. But maybe there are better approaches to handle this. Actually, I was talking to Ken about some ideas of imposing monotonicity or unimodality to these functions. Pull up a comment. You can always test the authority, which can be split as authority. I've watched sorry. I think during your, like, I was still thinking about moderators. Like, I was still thinking about moderators, sorry, your presentation. So, I'm wondering where moderators are in this sequence. Oh, where? Oh, yeah. Did you say something sorry? You said something about them being in H, the history variable? Yes, so you're more asking about the cause and effect formulation and where the moderator comes in. So, and you said something about several moderators, and then I missed. Several moderators, but and then I missed it because so the cause and effect here is defined conditional on moderators. And here I'm allowing s to depend on j, so the moderators can be time varying. And you can still have several moderators? I can still have multiple moderators, so you would include more terms on the right-hand side, etc. Oh, okay. Yeah, but the moderator indeed has to be inside the H. Has to be inside the H because it has to be pre-treatment. Great. I had a question. You kind of alluded to that it's possible that you see the effect going away because people have kind of all become exercisers. Right.