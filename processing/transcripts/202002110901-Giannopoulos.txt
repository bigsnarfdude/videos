And give some partial uh answers. All the results come from the thesis of my are part of the thesis of my last remaining student, Nikos Karmoyanis. And I should apologize, some parts of this are harder construction, but responsible for this is asymptotic geometric analysis part two. two. Now some of the results are joint work with Yorgos Hasapis, who has been a very good company and has supported a lot Nikos. Okay, so the first question is a question from the 80s of Vitali Milman. So we have a symmetric convex body in Rn and then we have a nest of C1 Have an S tuple C1 and Cs of symmetric convex bodies Cj in Rn, and we consider this norm. So you take the average, you integrate with respect to Xi and Ci of the K norm of the weighted sum of Tj Xj. So this is a norm in T where T is this vector in Rs. In general, let me write this. We will consider only the case where all the bodies CJ are the same body. And then I will use this notation. So let me keep this here. And let me also assume that uh all the bodies have uh equal volume and equal to one, because it is uh of no importance. You make some change of variables and uh you obtain results in the general case. Obtain a result in the general case. So I will write this in this way. And now the question of Vitali is the following. We consider the case where C is equal to K. And then the question is to examine if, in this case, this quantity is equivalent to the standard Euclidean norm up to a term which is logarithmic in the dimension. So you need an upper and a lower bound that will compare this quantity with the Euclidean norm. And in particular, he was asking if under some cotton condition on the norm induced by K on RN1 has equivalence, there are Everyone has equivalence. There are no logarithmic terms, and you have also some dependence on the quota constant of the body. And it is easy to check that you have this equality for any inverted linear transformation, so you may also choose any position of the convex body you like in order to study this question. And so far there were some lower bounds for this quantity, so we will assume that all the bodies have volume one. And the first paper on this topic was by Bourguen, Meiller, Evitali, and Pazor, who obtained this lower bound. Now by the arithmetic geometric lines inequality, this quantity is smaller than the L to normal the vector t. The method Uh the method was uh doing this. Uh it seems natural that since this is uh homogeneous, what you are trying to prove, you may normalize this to have Euclidian norm equal to one and then just look for a lower bound. And so you should obtain on the right-hand side the L2 norm of the L2 norm of the vector t, but this is what they prove. And they also proved another result. They assumed Assuming that C is isotropic, I will say a few things later on. Right now, I just want to mention something. You take an extra average here. These are independent standard Gaussian random variables. And you obtain this lower bound where you also have the appearance of the isotropic constant of C on the right hand side. And you also have this parameter M of P which is the average of the norm uh Which is the average of the norm of k on the sphere. Now, this inner expectation is up to some logarithmic term equivalent to the one that you get if you replace the G j by Bernoulli random variables. And then if you use Fubini theorem and use the fact that C is symmetric, this result actually gives you what you want. What you want, a lower part with the two norm of the vector 1, 1, 1, 1. This is the square root of s here. And you also have the isotropic constant on the bound. Okay? So let me kick this, just to recall this parameter. This is why I said this from the beginning. You have something like L C uh times square root of n m of k. M of K disappears, I mean, in this result of manufacturing. Now, around 2000, Glushkin and Vitaly started the same question and obtained a good lower bound, actually in a more general context. So, they considered the case where these sets are just measurable. K is a star body which contains. Is a star body which contains zero in the interior, assume that the volumes are all equal, and then they got this result with an absolute constant C. And their proof, I will not discuss it, here you use the brass conflict-luttinger rearrangement in a cult. So in a sense, you prove that this quantity is minimized overall A, J and K if everything is a ball shape. Everything is a ball, see? And then you compute for a ball, and this gives you the result. What is the first thing I want to say is a different proof of this lower bound. So this is the quantity here. And we want to prove this inequality. So this is larger than an absolute constant. And I want to give this And uh I want to give this proof because uh it will bring us to some idea that we had uh in order to give an upper bound for this quantity. Uh there were no upper bounds for this quantity as far as I know. And uh we will use this normalization because we are dealing with uh two norms. So we assume that the two norm of the coefficients dj is equal to one and then uh we use this identity. So we consider So we consider uh the distribution given uh t. Okay, we consider the distribution u t of this random vector, t1, x1, t plus tsxs, where xj are independent random vectors that are uniformly distributed on this convex bodies. And then this quantity that I'm writing here, actually for different bodies CJ, it takes this nice form, but then you have to form. But then you have to deal with this measure, nu t n in, in order to give a lower or an upper bound. And nu t is an even logon k probability measure, and we write gt for the density of this measure. So this is our quantity now. And we have a first lemma which says that if you use this normalization, then Normalization, then the L infinity norm of the density of this measure unit is always bounded by e to the n for any choice of t j's. And for the proof of this inequality, we use the entropy of a random vector. It was mentioned also yesterday. And a result of Bokoff and Maddy Mann, which says that if this density is located, then you have Is a local concave, then you have this uh double-sided inequality. Actually, the left-hand side inequality does not require low concavity, it is just uh Jensen, if I remember well. The other one is closed with an argument based on the preco-Line inequality. Use the low concavity of the function. And we also use the following observation because the bodies are similar. Because the bodies are symmetric, we may also assume that the coordinates of this vector p are non-negative. It is independent of Putin signs, what you will get by a change of variables. And then an equivalent form of the Sandon-Stam inequality says that if you have independent random vectors with these densities, then you have that the entropy of this combination, so the sum of the squares of the t. So the sum of the squares of the tj's is equal to 1 is bigger than the sum of tj square of each of the xj. And to prove now this, we just write the lower bound of this inequality of Bopkoff and Maddiman here. This inequality that I mentioned just before. To go to the We will go to the entropy of this random vector. And then the upper bound in the Bokovmer diemann inequality. And if you take the first term and the second, rearrange everything, you will get this inequality. And in our case, the densities are the indicator functions of some sets. And so the L infinity normal the G j's is equal to one. So this is equal to one and you have your lemma. And you have your lemma. So we have this for any T, and then we use a standard lemma, which appears in the paper of Hillman and Pasor, for example, that if you have a bounded positive density on Rn, then for any symmetric convex body k of volume one, you have this inequality. And this is all, then you prove your result. Prove your result. Recall that our quantity can be written in this form. Gt is the density of this measure. Then you will use this inequality. And what you need is an upper bound for Gt, which you have by lemma 1. That's all. Over this is the proof of the lower bound. I have proof of the lower bound. Now uh let me just uh say briefly some definitions about isotropic convex bodies. So if you have a convex body, this is called isotropic. If it has volume one, it has its variation there at the origin and uh you have a constant, the isotropic constant of the body, so that this is true for every unit we have. And uh if you have a local K probability measure, logon k probability measure with the density Fν. It is isotropic if it has by the density has barriers at the at the origin and the covariance matrix is the identity matrix. And in this case you define the isotropic constant to be the nth root of the density of the measure. I also need to recall some definitions about the C alpha directions. So if I have a sense So if I have a centered convex body of volume 1, then I say that the direction is a C alpha direction for some alpha between 1 and 2 with constant rho. If you have this inequality, reverse here inequality for all Q which are greater than 2, and you can give similar definitions for a measure. And we know by low concavity that every direction is a P1 direction with an absolute constant for any convex body C and any low concavity. And any low point probability measure. Now, we were interested in giving an upper bound. We assume that C is an isotropic convex body here. Because again, if you are interested in C convex body, you will apply an invertible linear transformation to bring C to the isotropic position. Then what you get will give you some result which will depend on the corresponding position of the body. On the corresponding position of the body K. So we assume that C is isotropic. And as before, we consider this measure mu T and now we observe that the covariance matrix of this measure mu T is a matter of the identity. This is what happens. And so if you consider the function f, which is this, L C to the power is this, Lc to the power n times GT at L C x. This is the density of an isotropic log concern probability measure, mu t on R n. And then something that we know is that for all these measures, if we consider T which has a two norm equal to one, we have an upper bound for the isotropic constant. It is less than E times H C, so they have something common. common. And what we also know is that our original quantity, uh because of this cha if we we make this change, uh will be exactly equal to this thing. So you have this parameter C to appear immediately. So you have that E C to the S A K. C is isotropic, K is something is equal to L C and then you have this integraph. And then you have this integraph. And the problem is what to do with this integral, which we may also denote. I will do this later on. And the information that we have is that the all these measures are isotropic and we also have the some upper bound for the isotropic constants. Their isotropic constants. This is what we know. Probably, in order to solve this problem, one should go deeper and know more. But I said I will discuss some questions. So we have this. And just to see what one should expect, let us average over rotations. Let us average over rotations of the body k. So, you consider this the average of this quantity is over all rotations of k. And then it is simple to see that this average is equal to square root of n m of k. This is the other quantity here. And so, on average, you have exactly this thing. Now, in order to have some Now in order to have some uh I will say this later on idea of what we expect here, assume that uh C is equal to K and this is a problem, because this is the original problem and we are interested, we can choose the position for the voltage. So K has volume 1. Ideally M of K should be of the order of 1 over square root of M over L K. L K with L K cancels. With alkalic ancestors, and you get the answer to the problem of retarding. This is what happens. This is what we would like to have. But as I said, there are some problems here. That if we achieve this, then you would get this upper bound. But it is not known what is the best upper bound you can give. The best upper bound you can give about the n parameter of an isotropic convex body. And this is an open question. And one can hope that something like this is true, and then you get what you want for the original question. But the currently best known estimate is uh something much weaker. You have a tenth root of n here. because you have a tenth root of n here. And you would like square root of n here. And even if you assume that your body is C2, which is much better with some constant rho, then even in this case you have some sixth root of n here. So this is another open question to obtain an upper bound for the n parameter of an isotropic complex body. And of course for this is on the average you would like to prove the distance To prove the estimate for k as it is given to you, and not to take an average over all the proteins. So, this is what we can do in general. This is the theorem that we can prove. So, if C is isotropic, K is any symmetric convex body, then you have this upper bound. You have L C here, you have M here, and you have something of the order of force to the. You have something of the order of 4 to 2, and this has many similarities with what is known about the isotropic constant right now. And for the proof, we estimate this quantity using only the fact that μ t is an isotropic local k probability measure. Plus, we know that since it is obtained for a convex body C, which is also isotropic, it has convex. Isotropic. It has compact support, and actually, we know something about the circumvrades of the support. So, this is the only things that we use for the proof. And it is done with an argument that resembles the Bougain's proof of this bound and makes use of De Lagrand's comparison theorem. Okay? So, this is the first thing I would like to say. There are, of course, some special cases. If you assume that your body is PC2 with constant rotation, Your body is C2 with constant rho, then you get this. One of the two rho terms here is the isotropic constant of C. If you have a quotape assumption about your convex body K, then again you get what you want, up to the quotape 2 constant of your space. And this is because of a result of Emmanuel Milmar, which exactly gives you. Milmar, which exactly gives you upper bounds uh for this quantity. And also, uh so this uh gives you that in the case where C is equal to key to C is equal to K, you have uh exactly what you want. You have the quotient II constant here, but also this is another open question. Even if you assume that your body has quotient II, you you don't have I don't know a good upper bound for the n parameter. So this The n parameter. So, this is even for the special case question. And let me also say that in the unconditional case, you have a precise result because you can estimate if all these bodies are unconditional with the same basis, okay, and isotropic. Then you have some good upper bounds here, which in the end give you some logarithmic steps. Actually, Actually, these results were proved long ago with the something happened here also probably just use the computer. I mean, I'm also left and right. With uh Mariana Herdulaki and uh Cholome. Herzulaki and Solomitis. So, in some special cases, you can come close to what you want. Now, the second question I want to discuss concerns the expected volume of some random convex sets. So, we have a symmetric convex body, and then we consider a matrix and time. matrix n times n capital, whose columns are vectors x1, x2, x, n capital in Rn and the complex body Tx, where Tx is this vertex of k. Some examples are if you take k to be the cross polytope in R n capital, then this Tx of K is the absolute convex color of the vectors. If you take K to be the cube, it is the zonotope, the sum of The sum of the line sequence minus xi xi. And the question that we want to study is to estimate the effective volume of this body when these vectors x1, xi and capital are independent random points that are distributed according to the nisotropic logon k probability measure mu. And there is a very nice result on these things about Gregoris and Peter. And Peter, that if you have some probability densities whose L infinity norm is less than one, then if you consider this expected volume, it is at least as large as what you get if your densities are the indicator function of the center that the averaging completion volume one. So this uh gives you something about the uh lower bound. Lower bound. But our question was: how to estimate this thing? If there is some formula which is general, gives you this expected volume. And for example, if this measure is the measure, the uniform measure of the ball, with Nicholas we have proved this. Because we have proved uh this uh double-side estimate. So what you would like to have is I think uh square root of n capital over n times v rad the volume radius of k. In the upper bound you will get the the mean width of the body k and if k is in some good position then these things are closed. Okay? So our main question is if you can So our main question is if you can have this uh uh for any measure mu. Uh these bounds that I described are uh exact because if you choose k to be the truth and mu is any isotropic uh measure, then this expectation is of this order. I think this had been proved by Bourgué, Megier, Pazor, and uh Milman. Also Peter has done some stronger Has done some stronger results where the isotropic constants probably appear in the lower bound. I don't remember the details, but this is the order of this quantity. So in general, the lower bound should be something like this. And if you consider the case where k is the cross-polytop, then this is another well-studied question. You take the absolute convex hull of n capital independent random. n capital independent random points with the same local conclave distribution and it is known that this expectation is uh of this order and uh then uh this gives you again square root of n over n times the mean width of the of the cross polytope. So this is the the upper bound. Now because of the results of Bobkov and Nazarov if you use these two things then you can Is too thick, then you can give estimates for unconditional isotope convex bodice k. Now, unfortunately, the upper bounds that we have do not contain the term square root of n capital over n, but the term n over n. So, this is another question. Let me just say a few things about this estimate. We start with this formula, which is well known. well known and gives you the volume of Tx of K, so that it involves the determinant of Tx Tx star and also the volume of the projection of K onto the subspace CX, which is the orthogonal subspace to the kernel of Tx. And then you combine this with the Cossibin formula if you want to work with this determinant. And then this last formula. And then this last formula is very old and it goes back to Blasky. And also if you assume that your measure of mu is isotropic, then the covariance matrix of mu is the identity matrix, and so this is equal to 1. And so you may obtain an upper bound for the expectation of this determinant. And then you use Cauchy's variance inequality, and what you have to estimate is the expectation of the square. made is the expectation of the square of the volume of these projections and this is something that one can probably do in a better way than us. For example you can use an upper bound for the volume of each such projection which is this one and this follows in a standard way from Sudakov's inequality and this gives you this n capital over n or in a similar way Or, in a similar way, if you assume that k is isotropic, then you get something like this. You will follow the same argument, but now you will use the classical inequality of Rogers and Shepherd, and you also want the lower bound for the sections that appear on the right-hand side, which you have because your body K is isotropic. So, it is easy to find some upper bounds which have the term n capital over n, but my question is if you can have a question is if you can have a boundary square root of n gaffer n. And now let me just briefly say something about the third question. I just want to show a simple inequality of scarmogenesis. This is another topic that was studied by Gregus and Peter. So you have independent random points distributed according to some say low concave probability measure or the you probability measure or the uniform measure on a convex body, and you consider the intersection of these balls. Okay? And Paurz and Pivovaro has shown again some using rearrangement equalities result that says that this thing, the volume of this thing, of the expected volume of this thing, or more generally the expected jth intrinsic volume of this thing is maximized when you're Is maximized when your measure is a uniform measure on the ball. In fact, they saw something more general, but now I don't have time to discuss this. They prove a more general results, and the increasing volumes are a consequence by the Alexandrofenkel inequality. But my question is to estimate this thing, okay? To estimate this expectation. And does Nikos Seoul And as Nicholas uh showed, there is nothing particular about balls here. Uh the fact that you use intersections of balls is important, if I remember well in the work of Peter and Gregoris, because they want to prove that some function is Steiner convex or whatever to apply the other method and then they need the balls. But you may consider this question if you want just an estimate. You take a symmetric convex mode, C, it will stop. It will stop. I will finish in two minutes. You may consider any convex body scene. You want to compute this expectation. And this Karmogen has proved this double-sided inequality. So this is the expectation. It is the volume. You have, okay, you have your points from a convex body K, you have any symmetric convex body C, any radii, R1, R2. Radii, R1, R2, Rn capital. Then you have the same thing on the left and on the right. There is a constant that depends only on n and n capital. And R here, here I have a spring. This is R i, okay? R is the minimum of R one and two R n categories. Uh I think it's a nice inequality, maybe you know this. Uh the proof is uh easy. The proof is uh easy. I mean he uses standard things from convex uh geometry. So he computes this. It is uh exactly equal to that. Uh the proof is by a Fubini argument. Uh now for the upper bound it is clear that all these volumes in the product are smaller than the volume of k times intersection with Ric and this is how we get the upper bound. So let me just say something about Upper bound. So let me just say something about the lower bound. How do you prove this? It is again standard, it is some Roger-Seppert type argument. So if you consider this function which is for each i which is an even concave function, you use polar coordinates. You use the fact that these functions are concave. You integrate and you get your result. There is no time but you understand that it is something that you have seen. That it is something that you have seen before, I mean, from proofs of rather several type results. So, a natural question, and I want to close with this, is what is the best constant in the lower bound? Because this argument is very simple. What we like is that the expression on the left and on the right is the same, but what is the constant? And the behavior is different for small and large values of r. For example, For example, if all the arrays are equal to r, and if you take uh large r, the limit as r tends to infinity, this ratio tends to 1, so the constant is 1. And this quantity behaves like the volume of R C, because k in the section with R C will be K, and K has volume 1 with R is big. And also the volume of K plus R C is the volume of R C and And also for small r, you have the same thing, the constant is 1. And now this quantity behaves as the volume of RC to the power n, because now this becomes 1, the volume of k, and this becomes RC when r is small. And so for small and larger, the constant is equal to 1. But what is the best constant in the lower bound in general? Thank you. Any questions, suggestions, to our post towards the if you like it, you're welcome to come to Kent in April and we will torture together pollsters for four or five hours and formal analysis. This was free advertisement. Let's think about. This was free advertisement. Let's think a post-hope. It's already there? Yeah. I don't understand what's going on with this thing. It's based on that.