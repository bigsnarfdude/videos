Please welcome him. Thank you. Can everybody hear me okay and see the slides now? Excellent. All right. So first like to thank the organizers as well for inviting me and giving me the chance to give this talk here. As promised, I thought it'd be nice and appropriate and fun to end the conference that's dedicated to. Conference that's dedicated to the memory of Richard Guy with a description of some joint work with him. So we started that way, we're going to finish that way too. In particular, I'm going to talk about a project that I did with Richard with Kevin Chum, who was an undergrad research assistant, did a couple terms with me working on this stuff, and Anton Mozunoff, who was a master's student of mine, who I recruited at some point to help sort of finish the job that we started with Kevin. So most of the talk is going to be about that, but I'm going to Most of the talk's going to be about that, but I'm going to end with a description of some follow-up stuff that Rich and I had been talking about for a while that I hope to get to and finish up this summer at some point. Okay, so first, what is an aliquat sequence? That's what this is all about. This is yet another of Richard's big interests. So had been looking at these for many, many, many years. And I always found that whenever I would wander in his office. Whenever I would wander in his office to talk to him about something, he always had on his screen at least one period session open where he was actively computing terms in these aliquot sequences. So, this was definitely a passion that he'd been doing for forever and ever. What are they? Basically, it's just iterating the sum of proper divisors function, s of n. So, s of n is just the sum of divisors minus n itself, and you just simply iterate that function. Now, of course, Now, of course, the question then is: once you start doing that, you wonder, you know, what could happen to such a sequence? Well, there's a number of possible things. So, one possibility is that if you're chugging along, iterating one of these things and you stumble across a prime number, then you see that the sequence immediately terminates because the next term is going to be one and it's now flatlined, and that's the end of it. But there's some other possibilities. So, for example, if you happen to stumble across a perfect number. Happen to stumble across a perfect number. Well, remember, the property of a perfect number is that s of n is equal to n. So subsequent terms in the sequence are going to be stuck at n from that point on, and it's also basically finished then. We also have some neat phenomenon called amicable pairs. So those are numbers n for which if you iterate the s of n function twice, you get back to n. So one example of those are the numbers 220 and 284. So if your sequence does So, if your sequence stumbles into an amical pair, then it's now in an infinite cycle of length 2 and is not going to go anywhere either. There's longer cycles as well. There's aliquot cycles. The longest ones that we know of are of length 28. One example of that starts at the number 2856. So, if you hit that one, then your aliquot sequence is now going to be stuck in a length 28 cycle and will never go anywhere. It's effectively done too. There's other cycle lengths. There's other cycle lengths. I think there's four and five. I think eight exists. There's a bunch in particular, we don't know if there are any alquat cycles of length three, for example. No one's found any and been able to construct them. But in any event, that's another way your sequence can end. Now, finally, another possibility, we wonder whether there are sequences that are unbounded. So for instance, the sequence that starts at the term 276 has been computed out to over 2,000 terms. Over 2,000 terms. The next one has a composite factor of 205 decimal digits, hasn't been factored yet. And we wonder: is that one going off to infinity or is that one eventually going to hit one of these four things and terminate? So no one knows the answer to this, in fact. Now, I sort of also promised to say some things about Richard and anecdotes and things as part of this. So I think I'll mention how I actually got started. Mention how I actually got started in this project. So, Renata said that Richard was really good at getting students and other people involved in his stuff. And he was very subtle about it. So, the way I got involved in this was at one point I got an email from him that says, on the subject, would you like to factor a number? And so, I opened the email, and there's this 100-digit number sitting there. And says, Well, you know, I've been trying to factor this. Paris has been taking days and days and not finishing. You know, days and days and not finishing, can you do this? And of course, I said, Yeah, sure. You know, I'm a computational number theorist, I could do these things. So, so I did. You know, I found some special purpose factoring software and did it. And Richard says, oh, thanks, great, thanks a lot. And then a day or two later, I get another email. How about this one? So I factored it, sent him back, and then another one came. And so at some point, I started to wonder, you know, well, what's going on, Greg? Where are these things coming from? So I wandered over to the math building and asked him. And Richard showed me. And asked him. And Richard showed me what he was doing, which is stuff like this. So, this is the sort of thing you would see on his desktop all the time. It's basically, this one's a little slightly annotated from essentially a pre-session working out sequences and alquat sequences. So, he had been with Karl Palmer and they were interested in the sequence that starts with 46758 and just wondering what would happen to it, whether it would eventually turn. Would you know eventually terminate or what? And he had had it out to about 627 terms and had Piris sort of gamely chugging along trying to compute subsequent terms in this. Well, what would happen is that, so Piris pretty good, right? But it's not special purpose factoring. It just has implementation of the special or self-initializing quadratic sieve. And it would go along, and then every once in a while, it would give them a warning like this: this sort of ominous message that says, well, I'm. Sort of ominous message that says, Well, MPQS factoring is going to take many hours. And in fact, it would take days at some points, right? And so that's the point where Richard was reaching out to me. He says, El Paris stuck, you know, can you help prod it along by factoring something? And nevertheless, I mean, he had was able to take this one. The last email I saw from him with this one was to the 962nd term in that sequence, where the numbers were all now, you know, almost 110 digits long. So I was looking. Long. So I was looking at this and I sort of offered to rich, look, you know, I can factor stuff probably more quickly than Pirie. Why don't I automate this for you so you don't have to do all this by hand? I mean, he always continued to do it by hand anyway, because I think it was fun for him and he liked it. But in any event, so I took that sequence to him and I ran it out to about 3,500 terms. And this is kind of what hooked me on it. It was really fun. So I would, you know, started the thing up and I was watching the sequence sort of chug along. Was watching the sequence sort of chug along, and remember, I picked it up at around 900 digits, so sort of about here. And so, it was going along a day for a couple days. It's still taking hours, you know, to produce every new term in the sequence. Then suddenly I come back and the thing's just spewing out terms. You know, it starts and the terms are plummeting. And so, something had changed going on. What's going on? Is it finally going to crash or what? And it didn't, but it got below 60 digits and started. It got below 60 digits and start kind of, you know, just sort of dancing around a bit, starts climbing again, and then it plummets again. And you wonder, oh, is this finally it? Is the thing going to, you know, finally give up the ghost and terminate? But in the end, it didn't. And something changed again. And, you know, it started climbing and climbing until the point where it was just getting too painful, even for me, to factor these things. The numbers are about 130 digits at that point. And I don't know, this is just really fun. I could totally see, you know, the, you know. Totally see the fun and amusement Richard was getting out of these things. And so I was hooked. And yeah, and that's how I started into the project. Now, the fundamental question then, as I think you can all appreciate after seeing some of this stuff now, is whether or not every aliquot sequence is going to terminate. And this was sort of the question that was at the heart of what Richard was trying to do with these things. And there were two competing conjectures. So a Catalan or R. So Catalan, or R, I guess, none of these have been resolved yet. So Catalan and Dixon in the early 1900s figured that every aliquot sequence would finish at some point. And I guess the rationale is that, well, to finish, all it has to do is hit a prime or, you know, one of these cycles, a perfect number, Alacox cycle, and it's done. So it seems that just probabilistically, as you go on for a long time, eventually one of these things is bound to happen. However, However, Richard Guy at his collaborator, John Selfridge, had a counter conjecture that there actually exist aliquot sequences that are unbounded, that essentially sort of go off onto infinity, and that they manage to avoid all these things. Now, I'll talk about a couple rationales for this as we go on that, you know, Richard, that caused Richard to believe this. A couple of them here are that, first of all, even terms tend to produce subsequent even terms. Subsequent even terms. And of course, there aren't very many even primes, especially as you get big. So, you know, if you acknowledge that, that an even term is going to persist, then as the things get bigger and bigger, you might expect that it's going to be less likely for this to happen. This is coupled with the fact that for a aliquad sequence to change parity, the only way that can happen is if you hit a perfect square. So you have even terms, and if you hit a perfect square, then the next term will be odd. Then the next term will be odd. But Richard's observation was that, well, you know, as the sequence terms get bigger, you're going to be less and less likely to hit squares. They get, you know, further apart. So he kind of thought that some of these factors would make this, his version of the conjecture, the reality. So what's known about this? Well, there's actually still 12 numbers, less than a thousand, so seeds for these sequences for which termination is unknown. 276 is the smallest of these. Smallest of these. We Bosma has done some computations for the aliquot sequences starting with the first even numbers up to a million. He's ran them out to 100-digit terms. And think about one-third of those have not terminated yet. So Richard's quest then, and what he was trying to cleverly and subtly wrote me into participating in, was to try to convince the world that he and Selfridge's view was right. View was right. And basically, this talk is going to be describing my efforts with Kevin and Anton to try to help them do that as much as we could. So the first thing that Richard was concerned with was the idea, and an important thing in the study is the idea of amplification. So amplification is just measuring some growth. Just measuring some the growth rate of a term. You're just looking at the ratio of s of n divided by n. So seeing if it's bigger than one, then the term's gotten bigger. If it's less than one, it's gotten smaller. Now, one thing that had been bugging Richard about this was that Bosman and Kane had a really nice analytic result where they proved that the geometric mean of this amplification for even integers was less than one. Less than one. And of course, Richard's belief is that he wants to show that aliquot sequences tend to increase. And this result is sort of flying counter to that. It's saying that on average, if you have even terms, you know, they're on average S of n is less than n. They tend to would suggest that it might tend to decrease. Carl Palmerns did some follow-up work on that too. So he looked at the geometric mean of second iterates of aliquot sequences. So he was looking Of aliquot sequences. So he was looking at S2 of n divided by S of n. And he found that this Aliquot constant, this geometric mean of amplification, was the same. He also worked out this constant or the log of it to 13 decimal digits, nice accurate approximation of that. So, what Richard, though, was a little unhappy with this, and he didn't really believe it. And one of the reasons was that. And one of the reasons was that he pointed out that, well, like the Bosman-Kaine result is taken over all even integers. But there are, it's known that there's a lot of even integers, about 36% of them, if I remember the number correctly, that do not occur in any aliquot sequences. So there's integers for which there is no n, such that s of n is equal to m. And he thought that these sort of analytic results didn't account for stuff like that. So that's why. Like that. So that's why Carl had looked at second iterates because at least those ones are guaranteed to come from some aliquat sequence. But Richard asked us to do some numerical work on k-th iterates. So just compute a whole bunch of sequence terms, look at ones that are sort of k terms into a sequence, and compute the geometric mean of the amplification at that point. So these are ones that are somewhat more established, you know, firmly, you know, gone along in a sequence at some point. However, At some point. However, our numerical work also showed that even k-the iterates up to k equal to 10 appear to also be approaching this alacot constant of Bosman-K. So that was not good news for Richard. And here's sort of a picture of our data that's sort of showing that. So the red line is the Bosman-K constant. And actually what that is, is that's the observed the observed geometric mean. The observed geometric mean that we did for computation of terms out to 2 to the 37. So it approaches it pretty quickly. The green one is the second iterates that's coming up from below. Looks like it's also approaching the constant. And then the blue ones are for k from 3 to 10. Now, when we were doing this, this was pretty exciting at first, too, because as we started out, as our bound on the terms was increasing, these geometric means for a kth. Geometric means for k-theorites were going up. And we thought, oh, Richard's going to be happy. And we wanted to make Richard happy, I guess, with all this stuff. So this was good. And then it was even more exciting. They started to get bigger than zero. So what I'm plotting here is the log of the geometric mean. So if it's bigger than zero, of course, that means that now things are tending to increase. And so this is looking good, but then we went further and oh no, it starts sort of leveling off and eventually starts sort of going down again. So, you know. Again, so you know, we were out of luck, and Richard decided we needed to look at something else at that point. Now, Richard, of course, is very persistent. I mean, Renata alluded to his sort of mountaineering life as well. And I guess he's climbed just about everything in the Rocky Mountains. So he's clearly not someone who's going to give up really early. So, this is just one idea, and we proceeded to move on. This failed, so let's try something else and see what we can do. So, the next So, the next notion that comes into play then is the idea of guides and drivers. So, these are written up in a MathComp paper with a guy in Selfridge from 1975, but it's something Richard had been playing around with and knew about for years before that as well. Now, what these are are basically particular divisors of terms in aliquat sequences that tend to persist from term to term. So, you get a guide, you know, in one term of the sequence. You know, in one term in the sequence, subsequent terms are probably also going to have that same guide. There's a technical definition of a guide. So these are divisors of the form 2 to the k times m, where 2 to the k is the exact power of 2 that divides the term, and m is a divisor of sigma of 2 to the k. Now I say these persist. I give one example here. So 6, for instance. So if you have an aliquat sequence where 6 is the guide of the term, 6 is also going to divide the next. 6 is also going to divide the next term unless the term you have is of the form 2 times d squared times p, where d is odd and p is a prime that's 1 mod 4. So that's a pretty restrictive condition. And if you happen to land on that, then the next term will no longer be divisible by 6. But this is not the more likely scenario. It's the more likely probability is that this condition doesn't happen and 6 sticks around. And some of the other guides and drivers have even more restrictive conditions. Drivers have even more restrictive conditions for them to leave. Like 28, for instance, is another particular guide. And that one, when you get that, that one really tends to hang around. The picture I showed for the aliquot sequence I had done with Richard, when it started to climb, you know, up to the end, up to 130, 28 was actually the driver that it had that was not going away and steadily driving it up, which leads to the notion of driver as well. So drivers are guides that have the So drivers are guides that have the additional condition that 2 to the k minus 1 divides sigma of m. So Guyan Selfridge's paper was showing that the only drivers are the even perfect numbers and this little list of numbers given in the slide here. Now these ones are guides as well, so they tend to stick around, but other than two, which tends to cause terms to decrease, these drivers all tend to cause terms to get bigger. So like two Bigger. So, like 28, for instance, that one really has a very powerful effect on making the terms increase. So, Richard's other complaint with Pomeranz's and Bosman-Kaine results is that they were completely agnostic of the effect of these guides and drivers and what actually happened to sequences. Carl worked out one little illustrative example of this. So, he also looked at computing the geometric mean of amplification, where he Amplification, where he split the even numbers into two classes. So he looked at the ones that are exactly divisible by 2, so 2 mod 4, and the ones that are divisible by 4 are greater powers. The ones that are 2 mod 4, so that means that 2 is sort of the driver or guide that's in control of the sequence, as they say. And we expect that that drives the sequence down. And indeed, when Carl worked out the geometric mean for numbers that are 2 mod 4, he found that its log is less than 0. That its log is less than zero. But when you looked at the zero mod four case, you work out analytically the geometric mean, and it's bigger than zero. And so, given that, and the fact that a lot of these drivers here are divisible by four, you might expect that when you actually look at what's happening in actual aliquot sequences, that maybe Richard's got a point and that there might be some tendency for things to want to increase. Now, the thing, though, is that other than The thing, though, is that other than this little example of Carl's, I think there's nothing proved about the effect of these guides and drivers and estimates or analytic results given that. So what Richard had done, though, back in 1968, he had a master's student named Stan Devitt who also did some computations for Richard and trying to statistically estimate the average behavior of terms. The average behavior of terms. So instead of the analytic results, things that you can prove, you're just looking numerically. And the idea they had was to basically use Markov chain analysis on this. So how does that work? Well, you could imagine that guides and drivers are the states and sort of this Markov process. So you have a particular aliquot sequence. There's only just one guide or driver that is in control of the term of the sequence. And you can model that with the Markov process. You can ask, well, what is the Process. You can ask, what is the transition probability of a term in the sequence with one guide to another one? And what he had Stan do was basically compute as many sequence terms as he could, empirically estimate the transition probabilities from one guide to the other, compute the mean of the amplification for terms with a particular driver, and then form the transition matrix, do the Markov powering it up until the things. Powering it up until the things converge, and then get an estimate as to the average behavior. So, what David found was that the arithmetic mean empirically seemed to be bigger than one. And he actually also worked out analytically what the arithmetic mean was and also got a constant bigger than one. Now, the issue of this, though, is that it's really because we're trying to measure ratios and growth rates here, it's the geometric mean that's the more relevant sort of mean here. So not the Relevant sort of mean here, so not the arithmetic mean. So, this is one problem with Devitt's work. And Bosman Kane and Karl Pomerans were working in geometric means and getting constants being less than one. The other thing of Devitt's work, so this was 50 years before we had started with this, and it's a, I don't know, I like the quote in Stan's thesis. He says that they use a new method of factoring called Pollard Row, which in 1968 it was new. And this must have been. And, you know, this must have been a really impressive computation at the time, and a heck of a lot of work to get working on the machinery back then. And he was able to use factoring numbers of 18 decimal digits with this on, you know, probably some IBM mini computer or something like that. So Richard and I, though, were brainstorming about this. And, you know, the Kth iterate thing wasn't working. And then Richard sort of pulled out this thesis from his voluminous library and showed this to me and said, well, we thought about it. Well, we thought about it and said, well, maybe we can do something with this. And so we agreed to try to reboot Devitt's work. And so the second part of our contribution was to just try to do the same thing pretty much, but with modern, latest tech, the best algorithms. We, of course, have things like quadratic sieve, number field sieve, much bigger and faster computers, and we figured we could do a lot more stuff. So we did. We looked at a lot more sequences of starting terms. Sequences of starting terms. One thing Richard added to this, too, was he really wanted to see the effect of the starting term and the sequence. So his idea and philosophy was that you look at sequences that start at larger places, that it's going to be even less likely for them to terminate. And he wanted to generate numerical evidence for that, too. So we varied the starting point. We started at numbers at 2 to the 16 and 2 to the 16. 16 and 2 to the 16 plus 32, and so we had eight different sort of starting levels for terms as part of our computations. We looked at a thousand sequences for each of these eight levels, and we ran the sequences until factoring became too painful. Too painful for us meant that the numbers were about 86 decimal digits. And then we did the same thing pretty much that Devitt did. So we just empirically estimated the transition probabilities from one guide to another. From one guide to another, empirically computed the geometric means. We did the geometric mean instead of arithmetic mean as well. It's the other thing Richard wanted to update. And did the Markov machinery to get the expected geometric mean of a term in the sequence. And so here's the data that we got. So this first one here just shows the percentages of open sequences broken down by the various starting levels, so the size of the Starting level, so the size of the starting terms and overall. So the first thing you observe is that, as Richard figured, you know, as the starting terms get bigger, you know, it becomes increasingly less likely that sequences terminate. So this is kind of what Richard always talks about, the small of law numbers, right? Or law of small numbers. What did I say? At the beginning, you know, it's more likely that something's going to go wrong. I mean, there's sort of more primes, more squares are easier. primes, more squares are easier to find and you certainly get more possibility of termination, but as you start getting further out, it's less and less likely. The other thing we did here at the request of the referee for the paper about this was to break out the results in terms of properties of the starting terms. So the second group here are starting terms that start off driving the sequence up. So for terms divisible by four, you know, the next term is going to be You know, the next term is going to be s of n is going to be bigger than n. And more generally, an abundant term, an abundant number, is one for which s of n is bigger than n, meaning that the sequence is starting off going up. The third group, these are all properties of the initial terms that are making it go down at the beginning. And so initially, for small starting seeds, if you are starting going up, you're more likely to keep going up and not terminate. But if you start going down, you're more likely to terminate and crash out. You're more likely to terminate and crash out. But we also observed that as the starting term sizes increase as well, that the effect of these things becomes more diminished as it goes on, and that it doesn't seem to play as much of a role. So what about the Markov analysis? Well, here we give the expected amplifications of terms, geometric means, both the observed ones, just the numerical ones we calculated. Just the numerical ones we calculated, and the expected ones computed via the Markov analysis. So, observations, well, we observe that, and these are logs of amplifications, I should say, of, and again, only even numbers. And you see that the amplifications are all bigger than zero here. They're logs. So, meaning that on average, you know, what we observe and what the Markov analysis predicts is that the expected behavior of a The expected behavior of a random term in an aliquot sequence is that it's going to increase. We also broke out the results on the different starting terms. And again, this does seem to have a little bit of effect. But the Markov analysis, which is trying to kind of in some sense push things out to infinity, seems to suggest that as things go out, the effect of the starting term also kind of diminishes a little bit as we go as well. Little bit as we go as well. So, this was pretty, you know, good news for Richard as well. And again, what his philosophy was, is he wanted to show that if you're really looking at real sequences where guides and drivers are playing important roles, that it's not really these agnostic averages and means that you can do analytically, but the real behavior is something like this, where things seem to be going up. However, I should point out the one fly in the ointment here is that as the Here is that as the starting term sizes increase, our means seem to be decreasing a little bit. So, this kind of you know makes us a little bit nervous and uneasy about this. I'm not sure what's going on with that, if that's just an artifact of us not having enough data to get a meaningful conclusion when the terms are getting big or what. But it's the only worrisome sort of part about all this stuff. And so, that's basically the first thing. And so that's basically the first thing that we did with Richard. Some things we could do again, it might be worthwhile to try to extend the computations a little bit just to see what's going on with those expected amplifications decreasing. Richard's been trying to get people to prove something about this stuff with the guides and drivers for a long time. I've seen many emails of him addressed to Carl Pomerance and Andrew Granville, for instance, just asking, well, can you prove something about this? And Carl. Prove something about this. And Carl's been doing some things. I mean, looking at kth iterates and things, but still with the guides and drivers specifically, not much yet. What we decided to do, though, is, well, you know, look at something else that Richard had suggested. So I'm going to switch to part two of the talk, which talks about some unfinished work that Richard and I had been talking about for the last little while and haven't finished yet and hope to get done this summer. Now, this one, I apologize. Now, this one, I apologize for the dense slide here, but I wanted to put Richard's description of this sort of observation and problem on here in his own words. And this pretty much is, as he wrote it. I did some minor edits just to make it fit a little bit better here. This is an excellent example of what Renata was talking about, of Richard's uncanny ability to just identify patterns and interesting facts from data. So, what he had been doing, I mean, he had been computing. Been doing, I mean, he had been computing all these aliquot sequences. And again, one of the things that he was concerned about, these geometric means that people had been computing, was that they don't take into account numbers that don't occur in any aliquot sequences. So in this sort of description, he refers to those as orphans. So these are integers that n for which there exists no positive integer m, such that s of m is equal to x. So he calls them orphans because they've got no parents. Parents. And so he had been observing that in all his data that about 36% of even numbers seem to be orphans. But then he started noticing other things. He was saying, well, you know, he's enumerating, he decided to figure out, well, how many parents do particular numbers have? And he was noticing things like, well, it seems like about the same proportion of even numbers only have one single parent. Then it went further. He was looking, well, Then it went further. He was looking. Well, you could find a sort of fixed proportion emerging for numbers that have exactly two parents or three. And he even came up with some sort of fitting to a function that sort of roughly predicts what the density of numbers with the p parents ought to be. And he was seeing, observing that it seemed to be one over p factorial times e. And so then he wrote this up and he was asking, well, you know, can we prove anything about this? You know, is this reality due, you know, the Reality, do you know the numbers with the p parents really do converge to some particular density? And again, you know, this is just him, you know, working out these sequences by hand, sort of recording them and just sort of noticing these cool patterns emerging and then posing these problems to people and see if anyone can figure anything out about it. So, what is known about this? Well, density results, there's a little bit. So, orphans have Little bit. So, orphans have been considered in the literature. So, these are also known as some people call them untouchable numbers, some people call them non-aliquit numbers. So, it's known, so again, these are even numbers for which there is no aliquot parent. There's no m such that s of m is equal to n. So, Chen and Zhao in 2011 proved that there is a lower density, and they proved that it's bigger than 0.06. Now, remember though, Richard's observation was that about 36% of all even numbers seem to be orphans, or about 18% or so of all integers, right? And so Pollock and Palmerenz figured out a conjectural density, what they think it ought to be, that takes about this form. If you work out this limit here, you find that it seems to be about 0.175, which is indeed pretty close. Which is indeed pretty close to Richard's sort of 18% of all integers. And that's pretty much all that's known for these sort of density things. To the best of my knowledge, nobody's worked out densities for one or more parents. All we know about is this lower density bound of Chen and Zhao and this conjectural density, but only for orphans. What about numerical results? So, Richard has done some of his own. Richard has done some of his own experiments, of course. So he showed me at some point a really beautiful handwritten sort of table, I think, of results for tabulation. I think the table, his handwritten table didn't have all 2,350 of the even integers. It had a portion of them. But he'd done this for all of those. And he worked out the number of parents for each of those. And he gets possibly from zero to nine parents and the counts for each of them. And that's it for the general case. There's been numerical work on the orphans, however. So, Pomerance and Yang developed an algorithm to count the number of non-aliquot numbers up to a bound. They did the computation up to, I think, 10 to the 8. Pollock and Palmerns extended that to 10 to the 10, wanting to, of course, have some numerical evidence showing that their conjectural density was perhaps accurate. And one other thing that came to the point of view of the fact that And one other thing that Kevin and Anton and I did for this was we extended that computation up to 2 to the 40 as well, just to see if things are still proceeding as they're supposed to. And so we used the Pomerans and Yang algorithm as well. We did some sort of minor sort of technical improvements to make it work better in parallel and things like that. But it wasn't really any new math in that. We just did this anyway. But we just did this anyway. And here's a graph sort of showing what happens in this case. So here the red plot is the observed number of non-aliquote numbers up to 2 to the r divided by 2 to the r, so the observed density. The blue one is the Pollock and Pomeran's conjectural value. And you see as the bound r, so 2 to the r, is sort of is growing as it's getting closer, I think it looks fairly convincing. I think it looks fairly convincing that those two are starting to come together at some point. So it seems to be right. But again, we only know this conjectural density for orphans. It would be cool to have something else for the other familial structures. And so that's basically what we're going to try to do. So I've hired a summer student, Gavin Guin, and I. Hopefully this summer are going to try to tackle some of those questions of rich. Tackle some of those questions of Richard. So we'll look at the numerical data. I think we actually have the numerical data, make things stand on. Just have to recover it, maybe try to extend it and see if Richard's predictions are bearing out. Probably take a shot at looking at some of the density results, maybe the provable lower bound, maybe the conjectural one, see if it's not too painful to adapt them to the general case and see what we can come up with with this. So, the last anecdote I want to say about Richard is that. Say about Richard is that I also did go visit him in the hospital as well, like Renata did. And I was there sort of talking to him about, you know, stuff and that we missed him at the meetings and things like that. And he was sort of drifting in and out of consciousness. And every once in a while, he kind of looked at me like, you know, what is this guy blithering about? Can't you see I'm trying to sleep? The sort of the impression I was getting from this. But then I started to talk to him about this stuff. And I said, mention, you know, hey, I remember what you were talking about this. I remember what you were talking about this, and I'm still planning to do it. I've got a student, we're going to try to finish this off. And as soon as I started talking about that, it's like laser focused. I could tell he was trying to talk, and he was really, really into it. So one thing about Richard that I think we all know is that he had this undeniable passion and love for mathematics that was driving him, you know, sort of constantly. And I just found it just really cool that sort of even in the end, you know, when his health was really failing, that this. Health was really failing. That this was still, you know, top of his mind, and that he was really clued in and really interested, and still loving it, even in the last minute. So what are we going to try to do with this? Basically, finish the job for Richard. So I think I'll end the talk there. Thank you all for your attention. Any questions? I want to ask a question again. Sorry, I keep asking questions. Do you have any intuition for why as the numbers go up, the initial value seems to have less impact? Because somehow saying that a certain percentage of numbers don't occur is somehow not enough to account for that. Well, I think, like talking about this with Richard, too, I think it's just that the With Richard, too. I think it's just that the initial term, all it does is basically say what starts happening at the beginning. So you give an initial term that's abundant, and so all that means is that the first term is going to get bigger. But as you go on in the sequence, you know, at some point, the expected behavior that you expect to happen, I think, starts taking a hold. And unless, well, if you start going down, I mean, unless you get unlucky and things terminate, I mean, at some point, it just sort of writes itself and the typical thing, you know, Itself and the typical thing, you know, just going to happen the further out you go, I guess. So the initial term just sort of nudges it in a certain direction and then it becomes irrelevant. Is that kind of what you're saying? I think so, yeah. Okay. Do we have more questions? So, if there's no more questions, I do have one final slide here. So, being the last speaker, the The last speaker in the conference, I just wanted to thank the organizers for kind of rolling with the COVID-19 punches, persisting, persevering, putting all this together, and pulling off the first virtual Alberta number theory days. So thanks, guys, for putting this on. This was a lot of fun. Thank you. So