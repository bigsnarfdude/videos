I can still wait for you. Oh, sure. Okay. Yes. No, that's good. So I'm already. So, Hendrik, take it away. So thanks for the invitation. I'm going to talk about discrepancy estimates and some other stuff, which is already mentioned here in the topic. We've seen a lot of discussion about discrepancy today, and I'm talking about. Today, and I'm talking about Monte Carlo integration at first. So, if we have a point set, let's call it P, and it's somewhere in our unit cube, we want to calculate integrals by an approximation which is based on the arithmetic mean of our function values. So, and we want to calculate the error of this method. method. And what we need, we need two values to bound the error. The first one, which we have seen before today, is the stack discrepancy. The star discrepancy is not a measure at all, but it tells us something about the quality of our point set, which is compared to our Lebesgue measure on a subset in our unit cube. In our unit cube. And the second value, which I will not discuss in detail today, is the total variation of our function in the sense of Hadian Krause. It looks a bit messy, but it's really important in the error bounds, but we will not go into detail here right now. So, what we get for our error. So, what we get for our error is the star discrepancy multiplied with our total variation of F. So, of course, we want to hold it as small as possible. And this is the task Christian Weiss, Michael Gneboff, and I did in our last work. We wanted to keep the style discrepancy smaller than it was before. It has a It has a greater history in its calculations. First of all, it was shown that for a given point set, we have a constant C such that we can bound our star discrepancy by C multiplied with the square of D divided by N. D is our dimension and N is our endpoint set. Endpoint set or the amount of our endpoint set, and yeah, it was not known what C is. So Christoph Eisleitner began with the first proof and bounded it by 10, or more precisely by 9.65. Christian and I just got it down to 9. Christian dare to Christian Derr to 2.7868 and Michel Gnebuch and Niels Hebinghaus even to 2.587. And what we got is a new bound, which is 2.5968, and additionally an expectation value if we just take a typical Monte Carlo point set uniformly distributed in our Points that uniformly distributed in our unit cube. It's close to the C above, and we will see that it's just running into the C if we take our D into infinity. What we need to bound the star discrepancy are the so-called delta covers and bracketing numbers. Numbers. It's some hard stuff at first glance to understand what they do, so I will just keep it short and say they are some anchored boxes in our unit queue. They're anchored in the origin. And we can just bound so-called delta cover. Cover with bracketing cover, and we can calculate bracketing covers, bracketing numbers much better than delta cover. So, what we need is a relation between those two sizes. And what we have here is we can just bound the delta cover with two times the bracketing number. So, this is really. So, this is really short. I know you can just look it up later. I guess we will send you the presentations. So, I'm trying to keep it as short as possible now. I added a bibliography in the end, so you can just look it up all and I can keep it as short as possible. So, Michael Gnevo was able to show. Michael Gnevov was able to show that we can bound the bracketing number by this term above. And as you can see, this can grow very fast if we've got higher values for D or small values for delta. So the task was to get better results for the bracketing number. This was Bracketing number. This was the first joint work together with Christian. We just, this is a slight, this was a slight improvement of the bound, but we reached a new bound which has to be published yet, but we are finished so far. Just a thing which is Which is, if you take a look at the first factor here, which is much better than our two in the theorems before. So I cannot go into detail because we still have no approval for our paper, but I guess it will not take much more longer. So you can look it up later. This new boundary, this was some hard stuff to prove because what we had was Michael Gnevo's theorem from 2008. And what you can see, we've got double sum here. And Michael Gnevo just bounded or estimated this red part with a typical integral criteria. But this was not sharp enough. But this was not sharp enough. So we had to improve these estimates. And oh yes, of course, we need the B D minus K, which was an exponential of 2 and now is our maximum here. And what we did to estimate this red part here was to prove a generalized Foul-Haber inequality. Quality. I'm not sure if you have ever seen Follharba formula. It's somehow interesting, but just look it up at Wikipedia. It's funny somehow. And what we did, we proved a generalized formulation or a generalized inequality. And we were able to get a better bound on this red part here than. Bound on this red part here than with the integral criteria. So, and what we got in the end was the first result we were able to say that there exists a constant C equal to 2.54968. And if we have a Monte Carlo point set uniformly distributed in our unit. Uniformly distributed in our unit cube, we can just say the following. The probability that C is less than or equal to 2.5 has a probability of 0.0528. And the probability that C is less than or equal to 3 is, as you can see here, 0.99 and so on. So we can, and this is Can, and this is the second result. We can expect that our discrepancy lies somewhere in this area here. And this shows the expectation value as well. So we have an expectation value of 2.55648 for D equal to and 2.53657 for D greater than or equal to 3. So, and if our D goes to infinity, D goes to infinity, this value here will just go to this value here. The last result, and the last result closes a great gap that existed before. We can say that we can just bound the expectation value from above and below. The values from below are. The values from below are from Benjamin DÃ¼rer. He just proved lower bound somewhere, I forgot the year, but this is also in the bibliography. And we can just say that our discrepancy lies somewhere between those two values with a certain probability. It's right here: one minus two exponential of minus. Exponential of minus omega of d. So these were our results in short. Oh, this was really fast, I guess. It's okay, thank you. Thank you. More time for questions, I guess. Interesting. Sorry, I was still trying to absorb the. Sorry, I'm still trying to absorb the last result. One? Oh, I see, I see. Sorry, is this a new result, the last one, or this is all in Dorr's 2014 paper? What do you mean? Oh, just this last slide. Are you stating Dor's result or do you say something new here? This is a combination of two results. The upper bound is our new result, and the lower bound is a result from The lower bound is a result from Benjamin Durr in 2013 or 14 or somewhere. And do you know what constant he gets, or is it something too far from the upper bound too? Yeah, K1 was somewhere, oh, it was really small. 3 divided by 2020, something like that. It was really small. I see. More. I see. And I guess one should be able to get some upper estimates from, I guess, like things like VC dimension. But I guess those are probably very far from these small, like those probably give pretty bad constants compared to what you guys get. Or you don't know of the top of your head, maybe. Sure. Okay, fair. Sure. Okay, fair enough. I see. Yeah, I guess I'm just wondering what one can get from kind of generic techniques, but maybe that's homework for myself. Okay, I see one raised hand. And if anyone else has questions, please also either write them in chat or raise your hand. And yes, Matthew, feel free to ask a question. Thanks. Yeah, I raised my hand. Sorry, I may have missed. Sorry, I may have missed something. Do you have generalized Bernoulli numbers to go with your generalized Faulhaber inequalities? No, we just cut those values. Just a second. Ah, here it is. No, we just cut those negative values. I guess this are only the I guess these are only the first terms we used. Hendrik, may I add something on that question? Yeah, so the point is that in our proof, we have serious problems if there appears any negative coefficient in the Paul Harbor formula. And those first three coefficients are just the same as in the usual Paul Harbour formula. Formula. But we may not use more than those because then there appear negative coefficients and therefore we restrict it to the first three terms. And moreover, what is new here is that we have this error term which we call R here. Thanks. I can see how that might make things difficult. Question, complete. Thank you, thank you. I'm just looking at some discussion in the chat, which is interesting, I guess, to my question with Christophe and Michael. I see. So the answer to my question is that if you use the generic machinery, you get unspecified constants. Unspecified constants. Once again, please. Okay, Talogrand apparently said that if you have the energy and enthusiasm, you can compute some constants. Okay, I see. Thank you. That's interesting. I guess one question that also comes to mind is: is there any hope to get something with explicit constants from other constructions? Not necessarily. Not necessarily a uniform point set, something like jitter sampling. Instead, you hope to get better functions out of that in this regime. Yes. I only talked about Monte Carlo point sets, but we have used some additional point sets like negative dependent point sets. Or Christian may tell you something more about the point sets we use. I only focus. I only focused on Monte Carlo points. It's here. Thank you. Okay, any other comments or questions? If there are no more, I wanted... So if there are no more, then I'll stop the recording first.