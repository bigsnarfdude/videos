Welcome everyone to the second lecture by Tom Hatchcroft on uniform scanning forest in high dimension. I would like to remind you all that this lecture is being recorded, so please turn off your microphone and camera if you do not wish to be seen or heard. We would like to ask you to use your real name on Zoom's. Use your real name on Zooms so that it makes for a nice atmosphere. The lecture would be about an hour long with a short break at half a point. Feel free to use the chat to ask questions. And there is also a longer question period where the recording will be turned off. Okay, yeah, please, Tom, go ahead. Great. Thanks. So let me just set up my screen share. My screen share that should be working. Okay, great. So, last time I was telling you about sampling algorithms mostly for the uniform spanning forest. So, in particular, we saw the Alder Sproider algorithm and Wilson's algorithm. And at the end, I defined the infinite volume limits, the free and wide uniform spanning forests. Wired uniform spanning forests as weak limits of uniform spanning trees on finite subgraphs. And these always exist, and they're for Z D, which we're mostly interested in, they're the same, so we won't worry about boundary conditions. And right at the end, I told you how Wilson's algorithm can be extended to infinite transient graphs. This extension here, due to BLPS, where again we just take an enumeration, we run We run a random walk, we stop it when we hit what we've already generated, which might never happen, and then we loop raise and we add that to the forest and we keep going. Okay, so we saw in particular that we generate a new component exactly at those steps where we don't hit what we have already generated. Okay, and I finished last time by stating this theorem of P-mantle, which says that there's P-mantle, which says that there's a transition from connectivity to disconnectivity in dimension four. So up to and including dimension four, the spanning forest is really a spanning tree, it's connected, whereas in higher dimensions it becomes disconnected. So by the way, you might wonder, you know, I defined this thing as a limit of trees which are connected. How could it be that a limit of connected things is not connected? Well, it's just connectivity is not a closed condition. So what could happen is that in these final What could happen is that in these finite subgraphs, at every step, there's a path. Say, if I fix two vertices, there's a path between them. But as I take the limitless path, it's getting longer and longer. So if the distribution of that path is not tight, they might not be connected. So connectivity is not necessarily preserved by taking them. And I also mentioned this theorem of Erdős and Taylor, which is very closely related, and it says. Which is very closely related. And it says that four dimensions is also the critical dimension for two independent random walks intersecting each other infinitely often. So intersecting means that just the paths intersect, they don't have to be at the same time. And you can see, in fact, that the high-dimensional case of Ernest-Taylor basically implies this theorem of Pol, or it implies at least that disconnected with positive. At least this is connected with positive probability, and it's infinitely many components, almost surely a thing is not too hard. To deduce the low-dimensional case of B-Manzel's theorem from the low-dimensional case of Erdős-Taylor is actually quite technical, but we won't go into that. I'll just mention the result that lets you do it quite later. So, let me start today by quickly going through the proof of Hades Taylor so that. Proof of Addish Taylor so that I've sort of proven something in detail for you. Sorry, so very fine control of this on window. Um let me just stop trying to get that aligned 'cause it doesn't work. To get that aligned because it's a word. So, right, so let's see how the Erdős-Taylor theorem works. This is not too difficult. In particular, the high-dimensional part is just a first-moment correlation. So, if we take two independent random walks started at two points x and y on zb, and we look at the expected number of intersections between them. Okay, so if this is finite, they certainly intersect. If this is finite, they certainly intersect at most finitely often almost shock. Okay, but we can compute what this is in terms of the transition probabilities. So I can sum all over all times and all locations where they could have a transition of the probability that x is at z at time n and y is at z at time n. Okay? But by the detailed balance or time reversal, I can flip the y and the z here, and then I get the sum pn. Get the sum pnxz pnzx, and I'm summing over z, well, that's just the same thing as the transition probability from x to y. Sorry, this would be zy, right at time n plus m. Okay, and I can rewrite that in terms of just k. And if I change variables from n plus m to k, I get a factor of k plus 1 here because that's the number of ways of decomposing k as the sum of two factors. Okay. Now, the classical estimate for random walk is that the probability of transitioning from x to y in k steps is roughly of this order. So I get this on-diagonal term, which is like 1 over k to the d over 2. And then you also get this Gaussian off-diagonal. And actually, just to earnish Taylor, we don't really need to worry about this. It's enough to look at this and take x equals y. Look at this and take x equals y. But if you use this whole thing, you can go a bit further and you can compute fairly straightforwardly from this that this expected number of collision, sorry, the intersections is bounded by a constant times the distance to the minus d plus 4. Okay, so in particular, the probability they intersect at all is bounded by this expectation, it also decays at this hydrogen. Okay, so when d is So, when d is strictly larger than 4, this means in particular that if we take two points that are far away from each other, the random walk is unlikely to intersect. Which is what you need to prove that there are infinitely many components in the spanning forest in high dimensions. Now, note that this proof also shows that this expectation is infinite when d is less than or equal to 4. d is less than or equal to 4. In particular, if you take x equals x, this pk is like 1 over k to the d over 2, and you get this extra factor of k, so this diverges exactly where d is less than or equal to 1. Okay, so to do the low-dimensional part, you take that first moment calculation and then you do a second moment calculation on top of it. So if we look at the number of intersections that Look at the number of intersections that, so now we'll take both x and y to start at the origin, and look at the number of intersections up to time n. Okay, so the same calculation we just did, basically, lets us evaluate roughly what the expectation of this is. And we find that, you know, as we just saw, it's bounded when d is larger than four, and then it grows logarithmically when d is equal to four, and like some power of them when d is strictly smaller. When d is strictly formed. Okay, so in particular, the only thing that actually matters is that it diverges when d is less than four. On the other hand, we can also compute or bound from above the second moment of this quantity. So the second moment of this number of intersections is going to be the number of pairs of pairs of times where you both have where the walks are in the same place. Are in the same place. So if we write this in terms of the vertices, so we want there to be two vertices x and y, where both walks go to x and both walks go to y. Okay, and this can happen in one of two ways. Either they both go to x and then to y, or they go the other way around. Or there are more ways, of course, but up to symmetry, these are the only two ways of happening. So, in particular, the probability that, or the expected number of times that the Number of times that the random walk starts at the origin visits x and y up to time n can be bounded as follows. So if we take this truncated Green's function, so g n x y is the expected number of visits to y starting at x up to time n, then the expected number of times the first walk visits x, or pairs of times it visits x and y, you can bound like this. So this sum has the two different orders. So, this sum has the two different orders. Either you first go to x and then to y, or first go to y and then to x. And then we take a square because there are two odds. So using Cauchy-Schwartz, you can take this square and put it inside. And then you just see by algebra that there's some symmetry I can get rid of to pull out another factor of two, just get this. But then this is actually just equal to this sum is equal to the first moment squared. Is equal to the first moment squared. Okay, so what we've shown, so I'm doing this quite quickly, so it might not be, it might be a little bit difficult to follow, but what we've shown is that the second moment is always bounded by four times the square of the first moment. Okay, now what you can do is use the Paley-Zyglund inequality. So this says that this applies to any random variable. It says that the probability that some non-negative random variable is at least Non-negative random variable is at least half its mean, is at least a quarter times the ratio of the first moment squared to the second. So this is this kind of standard second moment method in equality. And in particular, in our case, we get that this is always at least 1 over 6. Now this 1 over 16 is not very important. It's just the important thing is that it's positive. It doesn't depend on that. Now, because we know in low dimensions, Because we know in low dimensions that this expectation tends to infinity, we can apply Fatu's lemma to get that the probability that the total number of intersections is actually infinite is at least the Lim soup of these probabilities that the number of intersections up to time n is at least half its expectation. Okay, but this is at least one of the 16, which is positive. And so to deduce the full theorem, you need to invoke a zero-one law to go from knowing that. Law to go from knowing that this probability is positive to in fact that it is one. So I don't really want to go into this, but one way is to use the Hewitt-Savage 01 law, which is for exchangeable events. This event is an exchangeable event for the sequence of increments of the two walks. Anyway, there are various ways you can conclude using zero, one log and then deduce the fortitude. Okay, so once you have Erdős Taylor, the high-dimensional part of P-mantle is easy because if I just start k random walks very far away from each other in ZD, where D is at least 5, then I know with high probability none of them will ever intersect each other. And in Wilson's algorithm, I have the freedom to pick any enumeration of the vertices that I want. So I can pick these k very well-separated vertices to be the first k vertices. Separated vertices to be the first k vertices that I use in Waltz's argument. And then with high probability, none of these paths see each other, so I have at least k components with high probability. K was arbitrary, so I in fact have infinitely many components with high probability, but this high probability is also arbitrary, so it's probability function. The low D part, as I said, is more difficult. So one way to do it is by invoking. One way to do it is by invoking, which is not what Pinat will do, but is to invoke this general theorem of Lymes, Paris, and Tran, which says that, in fact, intersecting a looper, so right, just to recall the issue why we don't immediately get it is that when we run Wilson's algorithm, the new random walk needs to hit the forest which is already generated, which is made of loop-raised random walks, not simple random walks, right? And maybe I know that. That's right. And maybe I know that two simple random walks intersect, but maybe it's not true that a simple random walk intersects a blue parcel. The blue parcel random walk is smaller. But in fact, these two probabilities are always at the same order. So this is a very nice theorem due to Lyons, Prison Tran. And we may come back to say something about the proof of this on Thursday, depending on time, because the ideas are also used. Also used later on. But for now, let me just state it. It says that for any graph, if I start two random walks on the graph, and in fact, this works with Markov chains that aren't reversible, the probability that y intersects the loop pressure of x is always at least some universal constant times the probability that the two simple random walks intersect. Okay, and moreover, if the two random walks intersect infinitely often on Moshawley, then the random walks in the Often almost surely, then the random walk y intersects the blue praiser of x. This is a very nice thing, and so this together with Erdős Teller immediately implies the result of Pimantel. Just a nice thing to mention, in three dimensions, there's a much easier proof of Demantel's theorem, which is a nice exercise if you haven't seen it before. So, in fact, in three dimensions, a random walk will A random walk will intersect any infinite connected set infinitely often almost surely. So you can just apply this fact with A equals to the brace random walk. Okay, so that finishes at least our sketch of the proof of Penanzalsterum. So what I want to do next is to tell you about a new way of sampling the wide uniform spanning forest. The wide uniform spanning forest, which is called the interlacemental spreader algorithm, and which turns out to be very useful for analysing some more detailed questions about the geometry of the spanning forest. So in particular, I'll use the interlacement cover spreader algorithm to prove the D greater than or equal to three case of this theorem of Benjamin E. Lines, Proudhon Tramp, which says that every tree in the uniform spanning forest. Every tree in the uniform spanning forest is one-ended almost surely for every d of its two. So one-ended means that the tree does not contain a double infinite part. So from every vertex, there's exactly one way to go to infinity in the tree and no other way. And there are actually many proofs of this theorem that are available now that apply at different generalities and have different levels of quantitativeness and so on. The proof And so on. The proof that I'll show you of this is actually one that's not published anywhere, but it's based on the same ideas that are used to analyze the to compute the critical exponents in high dimensions. So before we go further, let me just mention that a useful thing. So when we define the wide uniform spanning forest, remember it was the limit. Remember, it was the limit of these uniform spanning trees of these graphs Gn, where I took some exhaustion of my gratitude. It's here on the left. I took some exhaustion of my vertex set by finite sets. I contracted down everything outside of Vn into this one big boundary vertex delta. Okay, and then took the record. Now, what you can do is at each step of this exhaustion, you can orient every tree in the Can orient every tree in the uniform spanning tree of GN, sorry, every edge towards the root. So every edge sees which of its endpoints is closer to this boundary vertex, partial n, and it orients the tree appropriately. So there's a question in the chat, are the trees always connected to infinity? So this is true. This is always true that every tree is always infinite, almost surely. This is just the property that every tree has size at least. That every tree has size at least n is a closed condition. So the event that the origins is in a tree of size at least n is a closed condition. It holds full sufficiently large n, so it holds into the limit as well. That's a good question. So every tree is confined. Okay, so if we orient every edge towards the Every edge towards this boundary vertex before taking the limit, turns out that we still get a well-defined limit object which has these orientations on it. And it's the same thing as if you run Wilson's algorithm rooted at infinity. And whenever you add one of these Libra's random walks, you just orient it in the direction that it's going. Here you get the same thing. It's oriented wide uniform spectral. And this gives us a nice way of thinking about this one-endedness question. Question. So once you've oriented the tree like this, every vertex has exactly one oriented edge pointing out it. Okay, and some random number of edges pointing into it. So in particular, every vertex has a unique infinite oriented path that starts from it. And we call this the future of vertex. Okay? Now, if v is in the future of u, we say that u is in the past of v. Makes sense. Makes sense. And the claim: so the trees are one-ended if and only if every vertex has finite parts. So the future is always one way to get to infinity. We want there to be no other ways to get to infinity. And so once you have this formulation, one nice thing about it is you can start to formulate more quantitative questions, for example, about Quantitative questions, for example, about critical exponents. I'll talk a lot more about this later, but you might say, well, I want to know not just that the past is finite or more surely, but I'd like to say that the probability that the past is large, it has some polynomial things like that. Okay, and we'll come back to this. Okay. So, as I said, the key to at least our proof of this. key to at least our proof of this theorem, which is not the original proof, is going to be what's called the interlacement Albert-Sproder algorithm. So the idea of this is, remember yesterday we introduced the Albert Sproder algorithm where we run a random walk on a graph and every time it enters a vertex for the first time we include the edge that it used to enter the vertex. Now you might think, how would you generalize this to infinite graphs? Well if the graph is recurrent, Well, if the graph is recurrent, it's fine because you just run the random walk, it does visit every vertex. This operation makes sense, and it in fact does give you uniform spin entry. In the case that the graph is transient, which yeah, as Russ pointed out, I'm going to assume the graph is transient throughout now, it's not clear what to do because you run this random walk, maybe it doesn't visit every vertex, and then you know, this tree is not well defined. So, the way we get around this is by using, instead of a random warp, we use what's called the random interlacement process. So, this was introduced by Snetman, and roughly speaking, it's a Poissonian soup of doubly infinite random walk trajectories. We'll see what this means in more detail later. So, in order to introduce this, let me first give some definitions. So, I'll take g will be an infinite. G will be an infinite locally finite graph, and we're always going to be interested in the case that g is transient. And for each n less than m, which might be plus or minus infinity, we define wnm to be the set of all transient parts in the graph that are indexed by this interval. So transient, just like before when we define the loop ratio, it means that you visit each vertex at most finite graphs. So in particular, if n and m are both finite, then this transient condition holds automatically. This transient condition holds automatically to define this script w to be the union over all these guys, so it's the set of all transient parts indexed by arbitrary time. So this space or the set carries a nice topology. So I don't want to talk too much about this, but it's worth mentioning because the most obvious thing to do is to put the product topology on it. Product topology on it, you know, thinking of each of these things as a set of functions. But that's actually not the right thing to do for our applications. So what we want to do is for each set of vertices k, every finite set of vertices, we'll take w sub k to be the set of all the parts in w that hit k. So the sum index where they're inside k. Okay? And for each w that's in the set, we define w sub k. We define w sub k, small w sub k, to be the portion of w between the first and last time that you visit. And then we generate the topology on w using the sub-basis, so where we take k to be finite and some w prime hitting w, and we take the set of all parts that have the same restriction. So if you look at the So, if you look at the path between the first time and the last time it visits K, that's the same as the given path. So, we declare each of these sets to be open, and then we take the smallest topology that contains all of these open sets. This is a slightly funny thing, and if you're finding it hard to digest, don't worry about it too much. But the point here is just that this is not the most obvious topology, but it's the one that will make. Uh, but it's the one that will make the proofs work, okay? Um, so basically, this is the smallest topology that makes this operation sending W to WK continuous. Right, I want in particular, I want the first and last hitting times of every finite set to be continuous, and I want the local times of vertices to be continuous, so the total number of times you visit the vertex, and also evaluation. So, if I So, if I say where is this path of time n, that should be continuous as well. And this makes all of these things continuous, whereas the product obg doesn't make it makes evaluations continuous, but it doesn't make the other two things continuous. Anyway, so now that we've defined this set W and the topology on it, we define W star to be the quotient space where we consider two parts to be equivalent if one Parts to be equivalent if one is the shift of another. So if they're indexed by intervals that are shifts of each other and they're related in this way for some. And we equip W star with the quotient topology and everything is nice here. This is a Polish space. There's nothing horrible going on here. You can treat your spaces in ways that you're familiar with. And elements of W star are what we call trajectories. So trajectory. Are we call trajectories? So, trajectories is like a path, but modded out by translation, but not reflection. Now, formally, the random interlacement process is going to be a Poisson point process on the space W star times R, where R is a time coordinate. Okay, so as time progresses, these double infinite. Progresses, these doubly infinite trajectories are going to rain down from the sky in a certain way. So, in order to define this more precisely, I need to tell you what the intensity measure of this passing process is. So, let's do that now. So, first of all, whenever I have a path indexed by some interval, I can take its reversal in the obvious way, right? So, it's indexed by Way, right? So it's indexed by the flip of the interval, and I just take the steps of the path backwards instead of forwards. By the way, I should say that everything I'm talking about is for simple graphs where you don't have multiple edges. If you do have multiple edges, everything is the same. It's just notationally more complicated because paths you need to say which edges they're crossing, as well as just the sequence of vertices that do. Now, for each finite k, we define this measure q sub k. So it's not going to be a probability measure, but it will be a finite measure, via this formula. So this is probably going to take a minute to digest, but it's quite simple actually. So it suffices to describe what the measure does to sets at this point. So I'm going to look at parts that start at some vertex u, that were Start at some vertex u that belongs just an arbitrary vertex, and I'm going to say I want the negative part of the path to belong to some set and the positive part of the path to belong to some set. So if I specify this measure for every A and B and U, I've specified the whole measure. And this is the formula. So first of all, this vertex U has to be in K, otherwise I get zero. And if it is in K, I get the degree. I get the degree times the, and then basically these two probability factors where the negative part of the walk is like is the reversal of a random walk started at u and that does not return to k. And the positive part is just a random walk started at u. Okay, where this tau plus sub k is our notice. Sub k is our notation that we use for the first positive hit of the time. So this means that you never return to k. Okay. So this formula, it's not too important for us actually, but this is what it is. We'll see a more intuitive explanation of what's going on in a minute. So this defines a measure for every k, q k. So, what I'll do now, so we define W star as a projection of W, right? So, W star is W mod the translation equivalence rate. Okay, so I'll define pi to be the projection there. Now, we want to define our interlacement process, the intensity measure should be a measure on W star, right? So, we've just defined a bunch of measures on W and we're going to construct Construct a measure on W star via all these measures on W through some kind of consistency relation. This is quite technical, but we'll see a much more intuitive definition of the emplacement processing. Okay, so and we define w star sub k to be the set of trajectories that we see. Now the theorem of Snitman, so Snitman proved it for Z D and it was generalized to arbitrary. It for Z D, and it was generalized to arbitrary transient graphs by Tejara. So it says that there exists a unique measure Q star on W star satisfying this relation. So Q star of A of the set of trajectories that visit K and belong to A is the same as Q sub K of the pre-image of the set A under the projection. Okay, and this holds for every Borel set A and every finite set of vertices A. So, again, this is quite a technical statement, but don't worry too much about it. And this measure is locally finite, so it's not a probability measure. This is going to be the, so we call Q star the interlacement intensity measure. And the interlacement process is defined to be the Poisson point process. The Poisson point process on this product space on w star times r, where the intensity in the first coordinate is q star and then the second coordinate is just the Rebecca measure R. So again, what this means intuitively, I have this distribution or this measure on trajectories, and as time progresses, these trajectories are appearing. Each trajectory has a time associated to it, and they're all just raining down from the sky. They're all just raining down from the sky. And they're sort of like double infinite random walks. Okay, so let's now make this intuition that they're sort of like double infinite random walks precise. And then we can mostly forget about these formulas for the intensity. So I'll show you another equivalent way of constructing the random intelligence process. So how does this work? So we take, again, same thing as usual. Again, same thing as usual, transient graph, the transience is important. And we'll take an exhaustion by finite sets. And we define these graphs g n star as before by contracting everything down to a special boundary graph. Now what we're going to do is we're going to we want to say that in there's a sense in which the random interlacement process is just the limit of the random of the normal random walk on this graph. Okay, but in order But in order to make sense of that, we're going to have to time-change the random walk here in an appropriate way. So the way we do this is as follows. So first of all, we take a Poisson process on R, so just these are going to be arrival times, with this intensity, the degree of this boundary vertex. This turns out to be the right scale. Now, every time we get at some time t that we get a point in this process, what we do is we sample a random walk excursion from the boundary to itself. In other words, we just start a random walk at the boundary and we run it until it returns to the boundary for the first time. Okay, and we call this W2. Okay, and every such excursion I can consider as an element of W star by just W star by just clipping off. It has the first and last step is at this vertex, which is not a vertex of the original graph, but I can just ignore that. The portion of this where it's actually inside the graph I can consider as a trajectory in the big graph G. Okay, so if I do this, so I get a random subset of w start times r. Of W start times R, where the points are, these points that I drew from my Poisson process, and then at each point I had this random walk trajectory of W2. Okay, so again, all I've already done here is I've taken the random walk or a double infinite random walk on G N star and I've parametrized it in a funny way where I broke it up into excursions so that the pieces between So, the pieces between the visits to this boundary vertex. And then I think of every excursion as arriving all at once, at just one time, and then the excursions have these exponential times between them. So I'm just taking a random walk and parameterizing it in a weird way. And the proposition is that as n tends to infinity, this point process converges to the This point process converges to the random interlacement process on the graph. Okay, so again, what this means is we can understand the interlacement process on G intuitively. It just says you take a doubly infinite simple run walk on G n star, you break it up into excursions, you apply a Poissonian time change, and then you scale time in such a way that basically there's one way that you can get a non-degenerate layer. Okay, and that this gives you the And this gives you the intellectuals. So, this is how you can think about them without needing to think about these formulas for intensive pictures and stuff. And this limiting definition is going to make the connection to spamming trees clear. So, again, usual assumptions on the graph. I'm going to take a random interlacement process, I. Now, for each vertex and time, Vertex and time, I define sigma t of v to be the first time after time v that v is hit by some trajectory, which we call w sigma t of v. So it follows from the formulas that the number of the times when v itself, yeah, I do mean traffic. So, the set of times where V is visited by some trajectory is itself a Poisson process, and it has finite intensity. So, there's some well-defined first time where you're visited, and you're visited by exactly one trajectory at that time. And then we define Et of V to be the oriented edge that's traversed by this trajectory, this first trajectory that visits V after time t, as it enters V for the first time. And the theorem. And the theorem is that if I take, for any t, I take the collection of first entry edges to all vertices, but again I have to flip them, then this is distributed as the oriented wide uniform spanning force for every t. So you can take t equals zero if you like, but each this is going to give us a dynamical version of the orange. Chemical version of the oriented product. Okay. So if this will actually let me scroll, I can recall the usual others prototyping. Right, so remember on the left here we have the how to spread algorithm for finite graphs where we just ran a Graphs where we just ran around the wall, and we did roughly the same thing. So, every vertex is hit for the first time, every vertex other than where you start visited for the first time. And we took the first entry age and we put it backwards. Okay. So, in fact, this theorem really just follows from the standard Albert Sproder algorithm for finite graphs, together with this limiting picture of Beinzlessness. Of being sweetness. Because if I apply this same operation but to i n, right, this finite approximant, this is actually just doing exactly the same as the usual other spray algorithm. I've done all these funny time changes, but it's completely irrelevant to the way the algorithm works. So I get that applying this operation to the finite approximant gives me the uniform spanning tree of g n star, and I know that this converges weakly to the Converges weakly to the uniform spanning forest, the wide uniform spanning forest of Jupiter. So in order to complete the proof, you just have to prove this kind of continuity type property of this operation. This is not totally immediate. This map is not actually continuous, but it has enough continuity properties that you can prove this in a fairly straightforward way. Straightforward way using standard weak convergence arguments. So, the nice thing about this theorem, as I said, is it lets us think, one of the nice things, is it lets us think of the wide uniform spanning forest in a dynamical way. So, for every T, so we start with the insulation of the process, then for every T, we get a different sample of the wide uniform spanning forest. And in fact, this evolves. And in fact, this evolves in a very nice way. And the key to understanding the high-dimensional spanning forest will be to convert sort of static questions about the spanning forest into dynamical questions that are actually easier to answer. This is really going to be the key idea driving everything else that I'll tell you about. So maybe I can take that quick break now or a bit I've got. Quick break now. We're a bit like a lot of time, but if anyone has any questions, this big please feel free to ask questions. Yeah, so Russ asked about dynamical versions of Wilson's algorithm. I guess you probably can. I've never thought about it. Yes, the stacks evolve. I'm not sure for infinite graphs, can you really compute Wilson's algorithm from the stacks, or does it only make sense in finite graphs? I guess in finite graphs, you can certainly have evolving stacks and then try to take some kind of limit. I'm not sure that it's an interesting question. Okay. It might turn out to be related to this, I don't know. Related to what? So, Russ was asking about, so there's this version of Wilson's algorithm where you start with a stack of arrows on every vertex and you run all the random watts using these given stacks, and then it actually doesn't matter which enumeration you pick. And you could think about dynamically evolving these stacks in time, and you would get a dynamical version forces out of them. I don't know whether it'd be, it's interesting, I don't know whether it'd be useful for analyzing the forests or not. Do you think of just growing the stacks by adding or removing yeah, there's lots of ways you could do it? Yeah, there's loads of ways you could do it, I guess. There was a question about why the intelligent process is called the intelligent process. The answer is I don't really know why it's called that. It doesn't seem to be related to other uses of the word interlacement in mathematics. Which probably answers your question. You have to ask Nimlin why you call it that. Yeah, so I think it was motivated by questions Itai was asking about the set of visited vertices by random walks on say blocks in ZB, but I'm not sure I don't think Itai used this term initially so I guess you just needed a word that sounded like some kind of path and wasn't didn't have a pre-assigned meaning. Meaning for a random instance of it. Shall I stop right here? Okay. So what I want to start to do now is tell you about how you can use the interlacement of the sproder algorithm to analyze, in particular, this one-endedness property in the unicorn spanning process. Company products. Now, the key observation that makes things work, and you'll see why this algorithm is useful for this problem, is that the past of a vertex evolves in a very nice and relatively understandable way under these dynamics. So, in fact, we'll basically always want to think about time going backwards. So, if we define f of t to be ab sub t. To be AB sub t applied to I, and we'll define pt of v to be the past of v in this forest f at time t. So what happens is that if I decrease p t of it, you know, this ab t of i, it's a function only of the part of the incisement process that's after time t. Okay, so if I decrease t a bit, I have some new interval. I have some new interval that's completely independent of f of t, but these new independent things they arrive before everything in f of t. So they have priority when you compute the algorithm. So these new, as t decreases, these new trajectories arrive and they overwrite the parts of the forest that they see. Okay, so as an example here, I have some forest here. Some forest here, and the red, so let's say I decreased here a little bit, and this red trajectory falls down from the sky. Okay, so with these arrows, they're all, every vertex is looking at the way that it's entered for the first time and then putting the arrow in that edge backwards. So, every vertex that gets visited by this red trajectory, the red thing arrived before anything else, right? So, everything's going to get rid of wherever it's. Get rid of wherever it was pointing its arrow before and then point it backwards along this red path. So you can see what happens here is that you know these guys here that were pointing up here before everything just points backwards along this red path like this. For example, this edge here gets deleted. So you could imagine this was maybe another tree going off here and I'm now not connecting into it anymore. Everything is just going off here. And similarly, all these guys here got a couple. All these guys here got cut off and they don't feed into this part anymore, they've been rerouted through here. Okay. So this is very nice for the past. So let's see how this works. So for every interval st, I'll consider this half open interval. We define this I sub st to be the set of vertices that hit at some point in this interval. Points in this interval. Okay? Now the lemma says that, this is a mistake, if V is not in a set. So if I have some vertex that's not hit between time s and t, then the pass to the vertex at the earlier time s is equal to the component of v in the past at time t where I delete everything that was hit. Okay, so this is the picture. Okay, so this is the picture. So suppose I have this vertex V, and this is its path. So these are all the guys that, if I follow their axis forward, they get to V. Now, if V is not hit, but this path here is hit by something, then this path sort of steals the part of the past that lies under where it hit. So these guys here that were feeding into the past of V before, instead they get fed up here in a way. They get fed up here in a way. So, what this means is that the past of a vertex, as I run time backwards, it's like monotonically getting smaller, except at the times when v is actually hit itself, at which point it jumps up. And in fact, in high dimensions, what happens is at that instant, it jumps up and becomes infinitely large. And then it kind of has a coming down from infinity where an epsilon after the time when it's here would be finite together. When it's here, it would be finite together. So it's kind of jumping to infinity and then falling back down, and jumping to infinity and falling back up. The fact that it becomes infinite when you're hit is not so obvious, but it is true in high dimensions. In any case, we have this monotone thing where you're jumping up possibly when you're hit by a path, and then montane and critical corresponds. Okay. And the proof of this is just kind of a formal thing going through the definition. thing going through the definitions. So the point is if we take some vertex u and we look at its future at some time t, so in the forest f of t, so these are just I follow the arrows forward, right? If I can then, remember this arrow goes from some u to something that was hit before you, right? Because I crossed that edge when I entered you. So that means in particular that these hitting times are decreasing as I go along in the future. Along in the future. Moreover, if I take the future of u at two different times s and t, then these will always be the same until the first time that they find a vertex that was hit in this interval between s and t. Because everything you compute, until you see something like this, everything's just the same. The trajectories that arrived in between are just not relevant to the computations are doing. But as soon as you find such a vertex, But as soon as you find such a vertex, you're now stuck the future in S is stuck forever in this set of vertices that were hit before time tier. Okay, and these things together tell you this however it's true. So, this is going to be the idea for proving these one-endedness theorems. We're going to analyze this sort of Markov chain. This sort of mock of chain, mock of process where the past is evolving. And to do this, we'll need to know something about how quickly the interlacements hit a set. So if I take any finite set K, then the set of times at which K is visited by some trajectory is itself a Poisson process, and the intensity is given by Q star applied to this set of all trajectories. Applied to the set of all trajectories that you have, right, just by the definition. And this is the same thing as q sub k of the whole set of all trajectories. And again, just by taking the definitions, this is a very simple event, and we compute that this measure is equal to this quantity where I sum overall velocities in the set of the degree multiplied by the probability that the random walk never returns. Okay, but hopefully, for many of you. Okay, but hopefully, for many of you, this quantity will look familiar. This is the same thing as the conductance to infinity, aka the capacity of the circuit. Okay, so this quantity is something that comes up all over the place in random walks and electronics. Okay, so this is all, we'll call it the capacity. So we'll write cap k for the same quantity that's all three of these things. So we get that the set of times that k is visited is a Poisson process with intensity given. Is a Pluson process with intensity given by the capacity. And this is nice because there's loads of tools for estimating capacity as a set. This is a popular thing. In particular, the most relevant thing for us will be this variational formula for the capacity due to Jeng and Ori. It says that the reciprocal of the capacity I can get by informizing this quantity where I Infimizing this quantity where I take the infimum over all probability measures on the set. Basically, I pick two independent random points according to this probability measure, and then I look at what the Green's function is between. And there's also this normalization by the degree, which is not very important. Okay, and remember the Green's function is the number of times in expectation that a random wall starts to leave UV. Now for zd the Green's function we know is of this order. So it's like the distance to the minus d plus 2, okay, when d is at least 3. And in particular, if you just want a lower bound on the capacity, you can upper bound this by just considering mu to be the normalized counting measure. So you get that the reciprocal of the capacity is always upper bounded by one of the Bounded by one over the volume squared times this sum up to constants. And it's not immediate, but it's not too hard to see that the worst case is when k is a ball. And in this case, you get that this sum. Let's see, it's like, let's not do it. Sorry, don't mix to mistake and embarrass myself. But you do the computation as if this is a ball. As if this is a ball, you find that the capacity has to be at least the volume of k to the d minus 2 over d. And you should compare this to this isoparametric inequality, which says that for any set of vertices in Zd, the boundary of the set has at least the volume to the d minus one of the d. These two things are actually very related. There's a theorem. Related. There's a theorem of Lyons, Morrison, Tram, in particular, probably some older things as well, which tell you that, in fact, this inequality implies this inequality in general. In any case, we have this inequality on the capacity in zip here. So using this, let me try in the small amount of time that I have left to prove this theorem for you that when D is at least. View that when D is at least three, the trees in the spanning forest are one. So to do this, I need a bit more notation. So I'll define, so remember we have this forest f of t for every time t, and p t of v was the past of v in the forest. And I define p t v n to be the vertices that belong to the past, and this path of oriented edges. Path of oriented edges connecting them to V has length at most n. So, in other words, if I take the graph metric on this forest f of t, then the distance is at most n. And similarly, I have this with a partial, which is just those vertices for which this distance is exactly n rather than less than okay. So, what I'll prove to you is that the probability The probability that the past reaches level n, in other words, that this set is non-empty, is bounded by this constant times log n times n to the d minus 2 over d. And in particular, this says that the probability that the past survives for infinitely many levels is zero, and therefore, in particular, every tree is one. So, the point of this is not. So, the point of this is not to get the optimal exponent here, but just to do sort of an easy proof that gives you some polynomial bound. And I should say that there's an earlier theorem of Lines, Morris, and Schramm, which does a similar thing. It gets a polynomial or upper bound. They do extrinsic harm instead of intrinsic harm, but the results are quite similar. Okay, so in fact, this proof is going to be quite easy, which hopefully means I can get it in before I run out of time. I'm going to need, however, the mass transport principle, so let me just remind you of that quickly. So this is a way of doing formal exchange of summation for functions defined with two coordinates in Zd. So it says in this special case that if I have any function from pairs of points in Zd to non-negative point Of points in Zd to non-negative numbers, which has this property that if I shift both the coordinates by the same amount, then I get the same thing. Then fixing one coordinate and summing over the second one gives me the same thing as fixing the second coordinate, summing over the first one. That's the mass transport. And there are versions of this for KD graphs and so on, but let's just stick to this. So just to illustrate this, let's see one. Just to illustrate this, let's see one corollary which I'll use in the proof, which is that the expected number of points that have distance exactly n from the origin, in the past of the origin, is exactly equal to 1 for every n. Why is this true? Well, let's define one of these functions. f of xy is the probability that y belongs to level n of the past of x. Now, if I fix the first coordinate and sum over the second one, I get exactly the expected number of points that belong to level n of the part. Just true here. So mass transport says this is the same as if I fixed the second coordinate and sum over the first one. But what's that? Well, it's the expected number of points x such that the origin lies n steps in the path. n steps in the past of x. But in fact, this set is just deterministically equal to 1 because this is the same thing as saying that x is the nth point on the future of the origin. So there's exactly one guy x that gets mass that's this one I get by following the arrows pointing up as the origin of x. So this expectation is one, but that's the same thing as that's by the mass transform. By the mass as well. So now let's quickly prove this theorem. So the idea is going to be that I have this event that I want to estimate and I want to, as I said, kind of convert the static question about having a big past into a dynamical question. So I'm going to do this with this union bound. So I'll say, well, if I want the past to survive to level M, either there are two things that could happen. One is that. There are two things that could happen. One is that the origin is hit at a small time. Then I have this intuition that that's going to be helpful to have a big past. Okay, so let's write that down. Otherwise, you know, I have a past of length. The past survives to level n, but the origin is not it. Clearly, this down. And moreover, this first term is order epsilon, right? Because I have a Poisson process, the number of times that the set of times. times the set of times where the origin is hit is a Poisson process whose intensity is the capacity of the origin which is a constant. So the probability I'm hit in a small in a window of length epsilon is order epsilon. Now for the second term I know by the lemma that this event happened if this event happens that means that if I look at the forest at time epsilon instead of time zero then in order for this Of time zero, then in order for this to happen, I must have the forest at time epsilon survives to level n, and there exists some path of length n going into the past that's not hit in between times. Because if I'm hit in between times, right, the the it breaks it. Okay, but I might have more than one path into the past, so maybe it's fine. Maybe this one, one of them got hit and another one didn't. Okay, but Another one didn't. Okay, but I need at least one of these parts to not be hit going from time epsilon to zero. Okay, now I can shift time just to make it look slightly more familiar, and I can say this is the same thing as the past surviving to level n at time zero, and there existing some path of length n going into the past that's not hit between time zero at negative specs. Zero and negative spec zeros. Okay? Which I can bound by the expected number of how many of these paths there are, right? Just by markup. Well, what I do now is I notice that this interlacement process between time negative epsilon and zero is independent of the forest at time zero. So if I condition on, just because the forest at time zero is computed only in time. Time zero is computed only in terms of the enclosure's after time zero. So it's independent of the ones before time zero. And in particular, if I take any particular path here, then the set of trajectory, the times when trajectory is hit it is just a Poisson. It's independent. So given the path at time zero, the probability that it's not hit is like e to the minus epsilon times the capacity of the path. Capacity of the path. Okay? But this path has length n, and therefore, deterministically, it must have capacity at least this power of n. So I can pull that out, and I get the expected, this exponential factor here, now just multiplied by the expected number of points that are in the past. But I know that this term is equal to one by the mass transport. Okay, so if I put these two things together, so I had the first term was order epsilon, the second term has this exponential, I get this bound, where the probability of surviving to level n bounded by a constant times epsilon plus this exponential thing. Okay, and I can finish the proof by just taking epsilon to be n to the minus d minus t over t times log n times a big constant. Times a big constant. And then that will make this term negligible because I'll have this, this one will cancel with this one, and I'll get a big power of 1 over n. And here I get the epsilon. Okay, so that completes the proof. And in fact, you can get this to work for all transient, transient graphs, for example, among many other classes of graphs. This covers. This covers results of Lyons, Morris, and Tramp. So on Thursday, what I'll tell you is how you can start improving this argument in high dimensions. So using the fact that this loop race random walk is not a ball, that this capacity load bound is not going to be of the correct order. If we can get a bound if we can massage this argument so that we can use capacity bounds of the correct order, then in fact we can get sharp. In fact, we can get sharp bounds on the probability of the path surviving to a large level. So that's what I'll talk about on Thursday. And I'll end there next day. Let's thank Tom for his nice talk. I think we will be unmuted by Sarai. Okay, I think, yeah, let's give it a. I think, yeah, let's give a round of applause. I think the recording will be stopped now. So feel free to ask more questions that you might have been trying to ask. You can now unmute yourself or ask on the chat. Yeah. 