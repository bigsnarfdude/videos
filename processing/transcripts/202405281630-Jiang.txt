So today I'm going to talk today I'm going to talk about the online local differential private functile inference well self-normalization. I understand that probably most of you are not familiar with privacy. So the first half of the lecture will be focusing on some of the background information about privacy. So why should we protect data? So in the past So, in the past, for a long time, people believe that once we remove the directly identifying information like name, address, and the postcode, those kind of information, then we're safe. It is not. One of the famous example is this, at that time, Sweeney was a MIT PhD student, but now she is a professor at MIT. So she did. MIT. So she did an experiment showing people that actually by using the public available data and the insurance data that actually we can identify which particular insurance data belongs to this at that time Massachusetts governor's data. So at that time insurance company believes that they can release, they can probably release their insurance data by removing just By removing just name and address, and then data bursts, those kinds of information, then we can actually put the data set, upload the data set somewhere on their website, and it's safe. But then Sweeney actually pays some money to the city and then get some information where that include all the residents' information and then maybe some address and then things like that. And then she figured out a way that can match the public available topic. Public available, public accessible data set with the insurance data set. And then she figured out this record belongs to Massachusetts. This record is Massachusetts governor's data point. And then from there, even though the public accessible data set doesn't contain confidential information, probably just the house address and then those kind of things, but in the insurance data, they have actually the salary. So they figure out how to do it. Actually, the salary. So, they figure out how much the governor made at that time. So, this is one of the examples, and it turns out that 87. So, if the US Census release their census data, then 87% of US citizens actually are unique, meaning that if the intruder are able to access some of the information, then they can accurately identify 87% of the U.S. 87% of the US citizens. So, again, this is why we need to protect people's privacy when working with the data, because releasing the raw data, then the intruders or the attackers will be able to identify whether this particular record is your record. And then from there, they can identify confidential information. So, and then we're entitled to actually protect or not release this. Actually, protect or not release this kind of information to the public. Okay, so another famous example is actually 2006 Netflix $1 million prize competition. So at that time, Netflix actually invite researchers to work on their training data set to see whether they can improve their recommendation system by 10%. So the winner has to be able to improve their existing. Be able to improve their existing algorithm by 10%. So, the training data consists of 1 million user data and their ratings on the movies. So, they have some kind of information about the users and then their ratings on different movies. So, this is a classification task. And then the testing data is 1.5 million user data without any rating. They want to test their developed algorithm on the testing data set to see whether they can improve 10% or not. Can improve 10% or not. And again, because we have this Video Privacy Protection Act, then actually, when the Netflix released the training data set, there's no information like z-code, birth date, or actually the names. So it's believed to be a safe data set to release for the public to work on. But it turns out, again, a PhD student. A PhD student and his supervisor found out that actually they were able to identify the individuals in this release the data set for training or being able to identify who is in the training data set. So this is what they did. So there's a for movie lovers, they probably know that there's a this called IMDA website where users can actually post. Users can actually post their readings on the movies. And for some reason, some users actually post their true names on this website. So then, based on this matching again, for example, so this is the released training data set for the researchers to work on. And they have the rating on different movies. So for example, in this, for example, we have one, two, three, four, five, six, six movies. One, two, three, four, five, six, six movies. And then, so this is the ratings. And then this is the MDB data set where they have users actual name and the rating. So by matching, they are so in this training data set, there's no name, but in this public available data set, there's a name, there is actual username. So then by matching based on the ratings on the movies, they can do this. They can do this kind of thing. So, they can link the individual person to the rating. So, this is a breach of the privacy of the users. So, for example, in here, because this is not exactly much, but for example, if in this public available data set, there's a person who's reading much exactly with this. Match exactly with this record, then people will be able to say, Oh, this is Alice's rating on the movies. But again, people probably don't want other people to know their opinions on certain things. And then for this reason, there was a privacy breach and actually a big lawsuit. And of course, there's no Netflix challenge number two anymore. So, this is actually two very famous examples. Researchers realize that by removing directly. That by removing directly identified information, it's not good enough for protect privacy. But there are many good things about sharing data, right? So for example, it promotes transparency, reproducibility in scientific research, and then information sharing. And also, we want to reduce the barriers in terms of access to the data, especially if this is the Especially if this is the taxpayers' money for the research, right? So, but we need to protect the privacy. So, in this talk, I will focus on this differential privacy approach. So, what we do is actually add some kind of random noise to the released final results. Instead of releasing the raw results, we add some kind of random noise. And then, this is known as the differential privacy. But there are other frameworks. But there are other frameworks that actually also protect privacy. For example, the synthetic data method, where instead of releasing the raw data, we perturb the data values in the data set before releasing. So most of the time, we don't actually directly perturb, but we fit a model to the data and then use the predictive value or the sample value from that model as the synthetic data for public release. Synthetic data for public release. So, this is known as the synthetic data method. And then there are also distributed learning, federated learning, where we want to run a model without actually sharing the raw data by some summary statistics. And then also there's K-a-nominality. So this is probably one of the oldest approach where they create generalization, meaning that, for example, if you are the only back to this example where. Where yeah, I don't know what so yeah, put the Tao wishes on yeah, yeah, sorry Okay, good, good so back to exact this example let's see so this k-anomity is a So this k anonymity is a very intuitive concept in the sense that if there are 10 other people have the same rating as you, so you couldn't tell whether this is your rating or not, right? So that's the idea behind K-anonymity. You want to create, perturb your data set such that there are K other people sharing the same information as you. So then a tiger cannot tell which one of the K records belongs. Of the K records belongs to you in particular. So, this is anonymity. Any questions? I think. Okay, so. And then there are other very traditional statistical disclosure control methods like data swapping. We swap the, so for example, instead of using my value, I switch my value with some other people. So this is known as data swap. This is known as data swapping or top or bottom coding. For example, if this is a like data set about salary, instead of using the exact salary, I can probably use something like top coding, like 10. I don't give you a specific salary value, but use like 100K or something like that as the top, as the truncate the value at a certain value. So those are the very traditional methods. Very traditional method. Yeah, exactly. So, but this is those are the very, very classical method that U.S. Census Bureau has been using for a long time, long, long time before they adopt this differential privacy approach. Now, Privacy is not, it's never free. It's not free. Even though people say we do need to protect the privacy, but we have to understand there's a price we have to pay. That's the accuracy. So our utility. So in the privacy literature, people always say the trade-off between utility and risk because utility is the price you have to pay when you impose some kind of privacy protection mechanism. So there are global measures in terms of, say, if Measure in terms of say if we want to share the data using synthetic method, then the global measure will measure like overall similarity between the synthetic data and then the original data. And then there are also very specific measures. For example, if we're using the synthetic data to do some kind of specific analysis, then there's a measure that quantifies the differences between the traditional, the true value, the value obtained using the original data, and then the value obtained using the sensitivity. And then the value of 10 using the sensitive data. So, those are the utility. And then the risk refers to the probability that you actually learn some confidential information. And specifically, there are two kinds. This is now the different, this is still the general framework that whenever you want to impose privacy protection, there's a price we have to pay, which is the utility or accuracy. Utility or accuracy. I will show you actually the mathematical notations. I know many of you are from the math set, so I will show you, but this is just a general introduction of privacy. Because when I was working on this privacy work, I had no idea what's privacy. It took me some while actually to understand why and what privacy concern people have for real. Concern people have for real applications. So, and then the risk in terms of risk, then there are identity disclosure risk, which means that how likely people can figure out which record belongs to your record. And there are also attribute disclosure, meaning that the likelihood that they learn some confidential information in the release data set. But most of the time, people are concerned with the identity disclosure. Concerned with the identity disclosure because the privacy research was originally motivated by US Census, the census. They don't want to release the original information, original data, but because there are so many information in the census data, then by identifying whether your record is in this particular role, that actually allows the attackers to learn more sensitive information from you. So, for that reason, people. Uh, from you. So, for that reason, people are not so much concerned with the attribute disclosure. But nowadays, there are like identity disclosure risk and attribute disclosure risk. So, in general, high privacy, low utility, low privacy, high utility. So there's always trade-off. And then if you look at the literature, people always focusing on like, let's see, if I maintain the same level of privacy, then my micros will obtain higher utility or something like that. Higher utility, or something like that. We focus on maximizing the utility when being able to maintain the same privacy level. So, this is the classical statistical distribution control framework. So, for example, we have a standard data set where all the directly identifying variables are removed, like name, address are removed. But we now understand this is still not safe. So, then in a typical data set, we classify the variable. We classify the variables in terms of the quasi-identifying variable and the non-identifying variable. So, those quasi-identifying variables refers to variables that are not directly identifying you, but indirectly identifying in the sense that, so for example, this is age, gender. So, in the life products example, they may have access to some public data set where they contain. Data set where they contain those demographic information about you. And then you have a confidential data set. Then, by matching based on those demographic variables, they will be able to figure out likely which roles belongs to you. And then from there, they can learn other information about you, which is in Z. So those non-identifying information can be sensitive or not sensitive. For example, if this is a medical data set, by doing the matching. By doing the matching, they will learn that you got this particular rare disease or not, but people probably don't want other people to learn this medical information about the people. So for that reason, usually if you look at the statistical literature, we focus on perturb the value of the demographic variable because those are believed to be indirectly identifying about your information. So, by perturbing your values, then you, for example, in one data set, I always explain this example. For example, we have, let's say, a Chinese who is 85 and then Chinese 85, what's more. And then, yeah, so maybe you are the only person who is Chinese and 85. So, then by looking and somehow the attacker will be able to get those information. Will be able to get those information, and then even though the data set, the data that the attacker has access to doesn't contain other information, just 85, your age, and then the race. But then in this confidential data set, there are other information by being able to match exactly based on your X, based on your identifying variable, they will learn other confidential information. So those are the confidential information we are trying to protect. So for that, We are trying to protect. So, for that reason, we perturb the X. So, instead of having you being a Chinese 85, we let you to be a Chinese, maybe 60. Then they couldn't match exactly. They couldn't tell this is your record. And then they couldn't learn your confidential information. So, this is the idea behind like perturbing the identifying variable X. So, again, the idea is that we want to prevent from We want to prevent from learning which particular data record belongs to a particular subject. Yeah, question? Like, it's the way that we're going to mathematically formalize this is that the prior probability that record i belongs to person j is not equal to the probability that record i belongs to person j, conditioned on the value of x. That's a good question. So they said actually why they actually. Question: So, this is actually why they actually need a new framework. This is why differential privacy is so popular because, in this framework, we do have some kind of important when we calculate the risk, your risk, we do have to make assumption that the attacker actually have access to your X information. Yeah, yeah, they have to. So, that's one assumption we have to make. And also, you have to decide whether these couple variables are X or you have to. X, or you have to decide which are your X variables. So those are all the assumptions. And then for that reason, this is why actually USS Bureau is no longer using the statistical disclosure control method that they have developed for so long, but focus more on the differential privacy. But differential privacy also has issues. So I will talk about that later. So this is exactly what trying to, this answers your question. We do need a This answers your question. We do need assumptions on attackers' knowledge on the participants, and then we have to make assumptions whether attacker will have access to all the information in X. And then in terms of risk, right? There are so many different definitions of risk. So there's no universal agreement which one is the best. So here comes the differential privacy. Okay, so when we say differential privacy, this is When we say differential privacy, this refers to a random algorithm. So let me just go through this definition and then I will give you a specific example. So we say, so we have a data set. So in here, we have a data set that the value falls into this density X, and then there are capital N of them. So this is a, you can consider this is a mechanism M is some kind of summary statistics. M is some kind of summary statistics. So we want to extract some kind of summary statistics based on a particular data set. And then we say this mechanism M is differential private if it satisfies this condition. So we have this mechanism that works on X, the original data set, or you have the same mechanism that works on another data. Mechanism that works on another data set. But the differences between data set X and the Y is that they differ only by one record. So there are different definitions. So in some definitions, they say two data set X and Y differ by one record. That means that they have the same number of individuals, type of n of them, but value in terms of value. One contain 90, the other one contain 80 or something. So that differs in terms of the value. But sometimes they use a different definition in the sense that. Different definition in the sense that you have two data sets, X and Y. They differ by one record in the sense that one data set has exactly one less data point. So people use this different definition, but we always say X and Y are neighboring data sets because they differ by one data record. But sometimes they refer to the case they have the same sample size, but sometimes they refer to one less data point. Okay. Okay. Good question. This is a probability in terms of what? Yeah, yeah, but this is so this is exactly what statisticians have been like. Or so this is a mechanism where the random is not coming from the sampling random. It's not coming from the sampling. So we have the data. So this is a specifically speaking. A specifically speaking, this is a conditional probability, even though people don't put conditional on x. So, here we fix the data set. We don't talk about the randomness of the data set. We only talk about the randomness of the mechanism. So, you have a data set X, you release the same result. The probability that you release the same result is approximately the same as working on the neighboring data set. What you have the probability that you have the similar. The probability that you have the similar result either working on x or on y. So, by this way, your results are safe. So, the most simple example CS people always gave is that, let's say we are trying to release the mean value of your data point. This is the most simple scenario where in the data set, there's only one variable, and now we're interested in releasing the mean. Releasing the mean. But releasing the mean itself is not safe because a typer could have access to all the n minus one individual's value, right? That could happen. But if you are. So let's say the attacker is so powerful, he already obtained the values for n minus one individuals in the data set. And then you release the mean value. What a character can do. What a character can do is just simple calculation, and then they are they can figure out the individual's data point, right? So the differential part actually coming from there. So by doing, by really, by using the individual value for n minus one individuals and the mean value, they can figure out the n individuals value. So this is why we don't want to. We don't want to release the original mean value. We want to actually perturb the mean value or by adding some kind of random noise, so then the attacker cannot do this kind of simple calculation to figure out the individual's value, right? So the assumption is: let's say we have a data set. So this is a very interesting concept. So this is the value you want to protect, and suppose Protect. And suppose the attacker already knows this value, right? Because this definition doesn't require any assumption at all. So this can happen, even though we don't know the likelihood of making this happen. But if attacker already know those values and then you give them the mean value x bar, what a tacker can do is just some kind of calculation. Calculation and then this is the value of xn, right? So differential privacy is trying to protect, prevent this from happening. But maybe. But maybe, but this per yeah, but this is the this is why they say differential privacy has this kind of the most strong privacy protection because under this assumption that they know all n minus one values except you, then by doing differential privacy, they can still protect you, even though they already know all other people's values. Yeah. This is the privacy. This is exactly what they are trying to do, right? So we don't want attacker to know the values for x1 to xn. So each row refers to each individual person. So this is their sensitive values. They want to protect. want to protect and then it could happen that the attacker already know n minus one values already this is a very very powerful attacker and then in this case if you don't release the original mean but making this x x bar differential private so we say dp then what we Then, what we do is we release X-bar plus some noise. So, if this is the case, they cannot, the attacker can no longer do this kind of calculation to figure out the value of the value. So, this is, yeah. Yeah, I'm finished. So, probability here is the the attacker's posterior belief after observing the data, basically that their beliefs. The data basically that their beliefs that the missing record is X can't be very different from their belief that the missing record is Y. There's a Bayesian version, Bayesian interpretation of this differential privacy, but that's not the original motivation. So the randomness coming from the randomness to how you add a noise. This is a random noise. Yeah, yeah. I will show you the example to show. Show you the example to show later to convince you where this randoms come from. So, from that example, you will see better. The reason I put this like a slides here is because later on, we will use this standard approach to add DP noise in our research. Okay. It will our convention. So if you add some kind of special noise, then eventually it will. So with no noise. Yeah, with no noise, no, no, yeah. So ideally, your epsilon should be a very, very small value. So in the extreme case, when epsilon is one, right? So you have the same probability of getting the same results by either working on X or working on Y. Yeah. What. Yeah, so how do I achieve DP? So achieve DP means there are many existing mechanisms. People already know how to protect. Cynthia Duart. Cynthia Duark is a professor. I think when she discovered a group of people, but she's the leading author. When she discovered this, proposed this definition, it's probably around 2006. She was working at Microsoft Research, but now she's a professor. Microsoft research, but now she's a professor at Harvard University. Yeah, so basically, under this differential privacy framework, whenever you want to release any information based on your data, you need to kind of add some kind of noise in order to protect the values. To protect attacker doing some kind of inference based on your release raw value to learn the actual values. Raw value to learn the actual values in the data set. Yeah, so how much noise you want to add this depends on your predetermined privacy budget if so this if soon is called the privacy budget okay so one known mechanism is actually based on this Laplace mechanism. I will show you the mechanism first and then I will go back to that slides to show you why we need to figure Slides to show you why we need to figure out something. So, this is the raw statistics that we want to release. Let's see, this is the mean we want to release. This is the raw, the true mean value. But instead of releasing the true mean value, our differential private mechanism require me to add some kind of noise. So, this is a general setting where you have the summary statistic is k-dimensional. So, here you have to add independent noise to each of the summary statistics you. Noise to each of the summary statistics you want to release. So, this is the raw value. And then the so how much noise you want to add is based on this Laplace distribution with location parameter zero and then this the scale parameter. So in here, the numerator is delta F. That's the sensitivity you have to figure out. And then the epsilon, the epsilon is just the epsilon that you saw in the original definition. So this is why we need to figure out what's this. This is why we need to figure out what's this delta F. And this delta F is called the sensitivity. So that's the maximum change in the L1 norm of your summary statistics caused by changing one data point. Right, so you have again two neighborhood data sets. So they differ by one data point, either one data point less or one different value. And then this is the L1, L1 normal. L1 norm of releasing the mean value either based on x or based on x prime and then this quantify the maximum maximum potential change by changing one value. So this is any neighboring data set x or x prime. Yeah, so like I said, f is the original value that we want to obtain either the mean or median or in linear regression, the regression coefficient. In linear regression, the regression coefficients were more complex. That's a good question. Very, very good question because people exactly. So if that's really the mean value, you have to make some kind of assumption. So what people usually do is to truncate your calculation. Do is to truncate your continuous value into a range, into an interval. Otherwise, the sensitivity for the mean is unbounded. So on one hand, this is the standard, the common practice for people to truncate your continuous quantity into a range. But at the same time, if you submit your paper by doing this, then people can criticize on doing this truncation. But yeah, that's that's yeah, that's. That's how it works. So now I want to show you or convince you that if you do add noise from the Laplace distribution, then your algorithm actually is differential parity. So this is just one slide showing how the Laplace distribution look like. So it's basically has this shape. So this is the mu, and then the scale is based on this S parameter. Down this S parameter. So now I want to show you the proof that this is really the case. Again, X and Y are the two neighboring data set. And then remember, either we work on X or work on Y. So either you obtain the same result Z working with data set X or Data set X, or you obtain the result Z working on Y. So we want this probability to be bounded by E to the power E, right? Remember, so then if we're using the Laplace mechanism, that means that my value, my release value is just my release, so my microphone is equal. So my mechanism is equal to the original value. So this is the true value based on the data set plus this Laplace noise. And then now my mechanism give me the result Z. So then this probability is just based on the Laplace distribution. And then again, this is a general case where you have K, the semi-statistic you release is K-dimensional. So this is a probability that you obtain a sign. The probability that you obtain the same value z working on x and working on y, so based on the Laplace distribution, and then this is just some simple simplification. And then that one is bounded by this, right? This is very easy. You have the difference of the two absolute value that's equal less than the sum. So that's that. And now you can guess why they use the Laplace mechanism because the density follows. Because the density follows this absolute. So the density has exactly this form. Okay, so far, so good, right? And then this one, again, is now we just bring this product back to the inside of this exponential, then we get this. And remember, this is exactly upper bounded by our, so yeah, this is just another way to write the sum in terms of this our nuanced. In terms of this our norm. And remember, this is exactly our how we define the sensitivity. So if you take the maximum here, right, that's our sensitivity. So that means this quantity is equal or less than the delta F, which is the upper bound. If you don't remember, we go back. So this is how exactly we define the sensitivity, the maximum possible change by changing one data point. Changing one data point. So, again, this is so for any x and y, any two neighboring data set, this quantity is upper bounded by delta f. So the delta f in the numerator denominator cancels. So this is e to the power epsilon. So this is showing that by doing by adding a La Blast noise, then I will. Then I will automatically get the maximum differential parade. For example, here I want to release the sample mean, and then I have to make the assumption exactly if I want to make my sensitivity to be bounded, then I have to make the assumption that each record in here is in this between zero and one, so then the global sensitivity is one. So then the global sensitivity is one over n. I have n data points by changing one data point. So the maximum change in the mean is one over n, right? So I have one over n. And then if I'm using the Laplace magnetism to release the sample mean, then that means I have the real mean plus z. Z is just one sample drawn from this Laplace distribution. And then the resulting value will be. Value will be a differential private sample mean release. And then the issue is: what if I have a continuous value to work with? And this is very often happen in the practice. I have, right? So they always have to find a way to bound the continuous value into an interval. So this is something unavoidable. Okay, so this is. Okay, so this is a continuous case. How about the categorical case or binary case, the most simple binary case? Again, because this is the most related to our work. So let's see, in this data set, I have n values again, but each xi is zero one value. So I have binary value in my data set. And then how do I make the relief? How do I make the release of this whole data set differential part? So, this is slightly different. So, in the first example, I want to release the mean, but here I want to release the whole data set. But very similar, just consider this mechanism is from this xn to xn itself, right? So, in here, whenever I want to release, so instead of releasing the x, the true value of x. The true value of Xi. Xi is either zero or one. I release file of Xi. So okay, so this is general case. Let's think about the most simple example xi 01. Let's see if xi is for the i indeed variable xi equal to 1 inside because I want this the yi value to be different. The yi value to be differential parity. So I don't release the original value of xi. I release either the value of xi or one minus xi. So I don't, yeah, for some reason, I don't think I need this phi function here. So I either release the true value of xi or one minus xi with certain probability. And this certain probability is determined by my privacy budget. And then the next slide. Privacy budget. And then the next slides, I will show you: if I do this accordingly, then I will have differential privacy as well. So again, the X1 are the original value. X1 to Xn are the original value. And Y1 to Yn are the differential private version of the original data set. So that's that. And then, so this is saying that. So, this is saying that if the original data set is X and now I release Y, I want to make this ratio. I want to bound this ratio by e to the power if some. Remember, this is releasing the whole data set. So, now I want to compare this two probability. Either I have the original data set being X prime or X, and eventually I'm releasing the. X and eventually I'm releasing the same data set. I want to see whether this probability is bounded by e to the power epsilon or not. And this is the case because let's say if I'm releasing y based on x, and then this probability is just because I'm doing this random response, randomly changing the value independently for each record. So that probability is just the product of the n values based on the The n values based on the original value. And then that means only because here X and X prime differ by only one data record. And now I'm assuming they differ on the S value. So here I'm assuming these two data sets happen to differ on the S value. So then if I'm tweaking the ratio, then only this term stays. All other terms are. Stay. All other terms are the same because the two neighboring data sets only differ by one particular data point. So those are the data points that differ in these two data sets. So then I have this. And then again, because I already told you how I want to release what I given my XI prime, right? So if this is the same, then the ratio. So with probability. So, with probability this, I got my original value. With probability this, I got one minus x at the opposite value. So, that means this ratio is at most e to the power epsilon. So, that means that if you follow my rule by doing this random response, randomly giving you the value, then the release data set is. The release data set is differential privacy. Okay, so this is the differential privacy. So there are so many nice things about nice properties based on the differential privacy. For example, one is that we have the post-processing property, meaning that if you have some kind of mechanism that's differential private, and then you want to do additional things based on this released value, then that's differential private as well. Virtual product as well. And then we don't need to have any assumptions on the attacker. Even in the worst case scenario, that the attacker already obtained the values for n minus one individuals under this framework, they can still provide the last one. And that's the ideal situation, but probably it won't happen. It won't have, yeah, yeah, but then then, yeah, that's the probably the most strong privacy protection, right? If soon being equal to one. But then, on the other hand, if you want to learn any useful information based on why, you probably learn nothing as well. Why you'll probably learn another thing as well, even for, yeah. We're sort of focused on one column, but it seems like if they're if your sensitive variable is highly correlated with a bunch of other variables, and if you add noise to your sensitive variable, you'll still be able to figure out right all the other variables. Yeah, that's something people don't really study. So, for example, if you think carefully here. Here there are a bunch of information in here, right? So let's say if fx is k-dimensional vector, so there are like they must be correlated in some way. But then if you want to use differential parameters and then using the Laplace mechanism, then you add independent noise. And then based on the proof, you can show that eventually this is differential privacy, but they don't actually use any of the correlation information that they may be. Information that they may be correlated. There are some updates that you have for like you can add up these columns. Yeah, yeah. But somehow this existing, there are so many variants of the differential privacy, like considering the correlation in here. But now of them, if you look at the papers carefully, some of them are contradicting to each other. There's no universal agreement. What should we be doing if? Will it be doing if you have so, for example, in here? This is one column, right? You release a single y. And then if you have multiple columns, I don't, I don't, most of the papers are theoretical papers showing nice properties, but in terms of real application, I don't see any like, yeah. Any like, yeah, there's a recent JASA paper comparing different methods, but I don't read it carefully, but before that, there's another paper, but still, when they do the comparison, there are some work comparing different methods, but most of the case, they focus on one column. Yes, even though the US Census Bureau adopted this approach, but I don't, I don't, I have some experience, I don't think it will work on real data because. On real data, because you can get this privacy protection, the strong privacy protection, but the consequence is the utility. You don't get information from those, the release or the perturbed data set. You don't get anything. Even for our simulation. So, I can quickly show you. I only have our go-throughs, but I just want to quickly show you the example. So, we're trying to get Example. So we're trying to get the confidence interval. So see the epsilon value we consider is log of spread is so large, probably you don't get much privacy protection. So this is also one common misuse of differential privacy because people can claim that I'm imposing confident differential privacy. It feels like you get something. But it turns out if you consider it to be log or three, then go back to the definition. Right, so if this is one point something, then you don't get the similar probability at all. So it's cheating. But again, right? So if soon is one point, I don't know, I don't remember log of three. How much, how big is log of three? One point something, right? One point. 1.1 something, right? So oh, you have three, right? You can consider if you have this is three, you don't get much protection because this one is naturally bounded by one. And then this is three multiplied by probably some probability. So usually people say you should consider epsilon to be one. Consider if some to be a value that's close to zero, as close to zero as possible. But we oftentimes have to consider a large if some value to obtain information. That means in this case, our confidence interval was so wide. See, so wide that's not useful when the sample size is small. I don't remember how small this in this range, but you can. In this range, but you can see probably only around like 50,000 you get something. So this the green one is our method and then the red one is the invisible. We call invisible. That's the benchmark. That's the case. We don't impose privacy protection. So this one is so large. It's so much wider than the invisible one. Visible one. So that means that if you want to impose strong privacy protection, the consequence is loss of the utility. That means you don't get much information by working with the data that's under the differential privacy guarantee. Okay, so now I don't have much time, but let me talk about the framework of our work. So we want to obtain the quantile. So we want to obtain the quantile and then do an online version. So the very natural approach is the gradient descent because it can handle the data that arrive sequentially. So there are many existing work on the convergence of SGD and asymptotics. So here we want to obtain a quantile under differential privacy protection, but because Privacy protection, but because we are dealing with the online version, so we won't focus on a variant of the original differential privacy concept, which is called the local differential privacy. So the differences is in the differential, yeah, in the original differential private framework, we assume that all the users, all the participants in the data set believe, trust this curator or trust. Curator or trust a central server. And then they are allowed to give the data to somebody. They have this kind of trust. And then we only impose differential privacy when we want to release any summary statistics like the mean. But here under the local differential privacy framework, the users don't even trust this curator. They don't trust anybody. You have to, actually, before I gave you the data, I kind of, I have to impart. I kind of have to impose differential privacy before I upload my data to somewhere. So, this is the framework that we work with. So, that means that users don't trust the data curator. So, we impose our online estimation of the quantiles under using stochastic gradient descent algorithm under local differential privacy guarantee. And then we propose a consistent estimator. Consistent estimator and then also an asymptotic confidence interval. Okay, so this is just a slide telling you or reviewing what's quantile. So for example, if this is the CDF of my distribution, again, we consider the simple scenario. There's only one variable. And then this is the CDF. And then the tau value, the quant basically split your density curve into two parts. The density curve into two parts. So under this part, there are tau, the area is tau, and then the up the rest is one minus tau. And then for those of you who are not familiar with quantile, actually the quantile can be defined as a minimizer of a loss function. So that means the quantile will be the argument of this particular loss function. So once we have the data, then we can Data, then we can obtain the quantile based the empirical quantile by minimizing this empirical risk function. So, this is why we're considering SGD because the quantile can be sequentially obtained whenever there's a new data comes in. So, we just update the quantile value whenever there's a new data point. So, this is just the steps. So, this is just the step size, and then this is the gradient, the gradient of this loss function. Very standard. Okay, so now let's think about how we do the protection. Now, this is a case where nobody, users don't trust anybody. So, whenever they release the data or release any information, they have to protect the data. So, in here, if you look our algorithm or how we update the iterate, then We update the iterate, then you will probably realize the only information that's sensitive is this indicator function, because this is asking you to compare whether your value is bigger than the current iterate or not. Right? Oh, the battery. This is so without worrying about differential privacy, this is how you'll use the Privacy, this is how you use you use STD to update your quantile value, right? Even though there are simpler approaches to calculate the quantile, but let's say we focus on STD and then this is how I update. So the only information that's sensitive is actually this one. And the n is the same n, it's the sample size. Let's say, yeah, so suppose up to So suppose up to I have n some data points that's coming sequentially one at a time. And eventually I have n data point, but I don't want because this is an online setting, meaning that I have to update my quantile whenever there's a new data point comes in. Even though if all the data came in at the same time, I can do this to estimate my quantile, but I have to do the estimation sequentially. And this is the this is the quanta value when I have n minus one data point and then this is the update I have to this is the update once I have my x n added to the existing data set. So that means for this person the only the information the only information that's sensitive is actually this binary quantity That means if we have this differential local differential privacy framework I have to Differential privacy framework, I have to perturb this binary. I have to perturb this binary indicator. And then once I perturb this binary indicator such that it satisfies differential privacy, then using the post-processing property, that means that everything else afterwards will be differential private. So I only have to perturb, worry about how I make this binary quantity differential private. Quantity differential private. I already showed you how to do this, the random response, right? The random response allows us to actually protect the binary values. So this is exactly what we do here. So we have what we call local randomized compare, but basically this is a slightly different version than the one I showed you, but the concept of the same, whether I tell you the truth or the opposite, because this is binary case. The opposite, because this is a binary case. I whether I tell the truth or the opposite by drawing a random variable from this Bernoulli distribution. So once I have that, then this is just how I update my, this is just how SCD updates the iterate. But there's one thing we have to be careful because once you impose differential private micro. Differential private mechanism to this indicator, then this one is biased. So we have to correct for that kind of bias. It's just like a misclassification. You consider this as your true value, but you perturb it. So it's no longer unbiased. We have to correct for bias. And there's some detail I don't have time to go through, but this part, this is basically, we have to do a step for correct. Do a step for correcting of the bias. Just like in the measurement error literature, you know, there's a misclassified, the value is misclassified, so you have to correct for that. So this is again the measurement error approach. And then during the iterator, we have to save some other values because we do need them in later the construction of the confidence interval. Okay, so again, how do we construct confidence interval? Before we have differential privacy. Before we have differential privacy, we already have some tools. Those are the known results, meaning that if this is your iterative of the quantile estimate, then there already results tells you the asymptotic distribution, the asymptotic results. But if you look at the right-hand side, so the variance part, there are still sensitive information. For example, this fx, this is the density. This is the density of your variable. So that means if So that means if you want everything to be differential private, we have to find a way to perturb this density function differential private as well, which is even harder than the quanta because if you already know how to make your CDF differential private, you can just estimate everything based on that because of the post-processing poverty. So this doesn't work. And then we found the solution in the sense that we want to get In a sense, that we want to get rid of this. So, if you look at the right-hand side, the only information that's sensitive is or private is this one. So, there's a self-normalization technique that allows us to actually get rid of this FX term, the density term. So, okay, so again, this is this part. I'll try my best. I'll try my best. So, this is known as there's again known results based on the functional central linear theory that says the partial sum of my iterate converge to a rescaled boundary motion. And then using the so this is that, and then using the continuous miping and also the Using the continuous mapping, and also the result here, you can imagine the ratio the variance no longer have this term. So that's how actually we obtain this result. So just like t distribution. So if you look at the test statistics from the t-test, you have a normal, you have a chi-square, and then the ratio will become a t. Ratio will become a t, and then in that resulting distribution, there's no longer the variance term. So, very, very similar concept here. So, then we can obtain a syntactic distribution that doesn't have any confidential information from the data. So, this is our self-normalization technique. And again, yeah, this is how results for differential privacy. Yeah, there's some details I skipped, but the main concept is that we already have some kind of asymptotic result, but that also requires Fx, but then this self-normalization technique allows us to get rid of the various Fx term. So then we will have a syntax distribution that doesn't rely on any. Doesn't rely on any information from the original data. So that's the final result. And then this is just like, like I said, we have to consider a relative large epsilon in order to get the converged result. But it seems, so this is something we always worry whenever, because if you have finance sample size, the confidence is so wide. But the CS people doesn't seem to care. The CS people doesn't seem to care so much. And then, so our paper was accepted by ICML last year. So, but anyway, so I think this is something I want to let people be aware that in the finance sample size scenario, that when you construct confidence involved for differential private algorithms, you have to take into the randomness from the sampling uncertainty. On top of that, there's a DP. Of that, there's a DP, the randomness from adding the DP noise, right? Those will be reflected in your confidence interval. So, this is why it's so wide. But that's the price we have to pay. And then eventually, of course, if someone says it goes to infinity, then everything, the influence from adding the random noise will go away. But in the finite sample size case, it's affecting the confidence in the world. It's affecting the confidence, and it was a lot. Yeah, that's all for my talk. Insensitivity to changing one value, yeah, because my observation doesn't change the MX doesn't change them. But amongst the class of all epsilon differential private policies, like Right, but all the like is there an optimal data as precisely as you want you can recover it. Yes. Those are, so there are many papers studying the accuracy, but I think first I didn't pay attention to those results just because we're a statistician, we want to figure out a way how to quantify the uncertainty part. But there are study studying the accuracy. Uh, studying the accuracy in the sense that they often say the inter this is also something I want to emphasize in terms of point estimate, usually accuracy is not affected because you can imagine you are adding noise that center at zero. So the accuracy is not, even in here, the point estimate is not problematic. But if you want to consider the randomness, The randomness from the DP noise is huge, right? So, if you look at the accuracy, it's not, it's not. So, the black, no, the dark blue is the point estimate in terms of the, so we have the DP value. And then, if you compare the DP version versus the truth, the difference is not huge because you are adding the noise that center at zero. But the confidence level. But the the confidence interval is affected so much. Yeah, yeah, yeah. But it's specific to the mean, but it's sample mean. Yeah. But you can look at other statistics, right? Yeah, there are people studying that, but in terms of point estimate, it's still, they always say still accurate because the bias will go away. You are adding the noise that centers at zero, but the confidence in the Center at zero, but the confidence in the wall, this is something. It's in general true, but I think local differential privacy is, I think, probably just proposed for. I think probably just proposed for SGD in the sense that in the online version, the original definition of differential privacy data applied. So then they somehow came up with this different. So Zhang Juxi actually had just a paper talking about this local differential privacy, but he didn't term that as differential privacy, but then some other people call that as local differential privacy. But I think the motivation is to accommodate this online version. Common to this online membership.