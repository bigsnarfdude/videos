Thank you very much. I would like to thank the organizers for the invitation so I can visit this beautiful place again. But as a workshop? Well, but I have to say, summary is much better. So I'm going to talk about this approach. And this is joint work with my collaborators and two of my students. Two of my students. And sometimes it's good to speak late in the workshop because there are things that the previous speakers already talked about, so I don't need to spend a lot of time on those. So that will be the case for my talk. And the idea is kind of simple, just a one-page introduction. And when we talk about the inverse problem, that we can, in general, write it in this simple formula: that we have y equals to the That we have y equals to the forward operator and working on q. And the inverse problem, of course, is if we have some knowledge about y and we want to know something about the unknown q. So there are many methods that already doing such things like iterative method if you rewrite things in optimization fashion and if you do direct sampling method like one of the assault talking about. The SAC talking about. Of course, many other methods. So I view those as deterministic methods. You try to get some information based on the differential equation. And myself is a computational mathematician. So I actually started my research doing non-iterative type of method. But later on, there are some, well, actually not later on, because if you use Bayesian inversion, If you use Bayesian inversion, the Bayes formula actually can essentially write any sort of inverse problem as the request of posterior density function. And I think it started the day when Bayes' formula became known. But the problem whether or not you will be able to use it depend on the technology and the computational power you might be able to have and other techniques. And other techniques. I think that's the point. And recently it became popular is because we can do fast sampling or we have reduced model of those things. We can do those. So my work towards this direction is we have some data, we have the forward problem related to BDEs, and I want to see whether or not we can combine deterministic. We can combine the deterministic method and then use the information I got and start to do Bayesian inversion. Because for many problems, like the partial data inverse problem, deterministic method, especially the method I have been working on, the Zampling type method, if you have much less data, you wouldn't be able to say too much. But let's use these information obtained using the deterministic method. Obtained using the deterministic method and put it in the Bayesian and to see if we can obtain additional information about the unknown. Well, of course, with the hope, there are problems you wouldn't be able to get anything more, but there are problems if you are able to get some more information. I think that's nice. And this is the whole idea of the things I wanted to talk about. So I will be talking, show you two examples. I wouldn't go to the details. You guys know. Go to the details. You guys know these inverse problems related already. Why is the inverse scheduling problem? And we saw talk this morning talking about the scattering problem. And then I will talk about another example, inverse source problem. And the fourth part is about the non-uniqueness and the local estimator. This is something I believe it's interesting. It's simple but interesting. It's related how you characterize the posterior. Characterize the posterior density function after you have sampled things successfully. And I believe this is very important in higher dimension. Because even for a function in higher dimension, how to describe that to people, I think it's very, very important. But we will see that quickly. So this is the erected scattering problem. I simply think two-dimensional case. You have an object occupy some area. Occupy some area, and you have the incident waves. Here I use plane waves, and then the direct scattering problem is try to find the scattered field such that the total field satisfies the Hamhol's equation. And I'm thinking some soft object obstacle. So, I got the Dirichlet boundary condition, and then this is summer field radiation condition for the scatter field. Scatter field. And we know that for scatter field, we have the so-called far-field expansion. And this is often the case that people measure the scattered field from far away. So what we actually have is this term. We call it far field. And the limited aperture scattering, universe scattering problem I'm talking about here is you got a certain aperture, it could be one incident wave, and then you measure. Incident wave, and then you measure the scattered far-field pattern on some aperture, could be a small aperture or larger aperture. And now you want to recover the shape of that. And for the non-iterative method I have been working on, it's like linear zombie method or the factorization method. Usually you have a lot, you need a much larger set of data. And we call it full data. It's your instant wave, it's all around the object. Wave is all around the object, and the observations are all around the object. And it doesn't work directly if you have limited data. For example, we just have one incident wave, and then we measure the, for example, surround the target, and we wouldn't get any meaningful result if you don't do something else. So, what we did, we first extend the linear sampling method to this, we call it extended this sampling. We call it extended. This zombie method is a silly name, but we were not able to find a better word for that. But anyway, so what we do is you have a disk, then you know the far field pattern for the disk in two-dimensional if your incident field is a plane wave. And then you can set up the far-field operator for like this. So the kernel is the far-field pattern due to a disk. Far field pattern due to a disk, and this is the density. And if you're familiar with numerous scattering theory, and you're familiar with this far-field pattern, but anyway, you can use that to set up the so-called far-field equation, and you measure the data actually on the right-hand side. If you remember the linear zombie method, you measured field actually is in the kernel position. So essentially, if you don't have a full data, you wouldn't be even able to. You wouldn't be even able to set up a far-field data. But now we use the disk, its far-field pattern in the kernel position, and move the measurement data to the right-hand side. So in principle, it doesn't matter how much data you have, you just put it on the right-hand side, you still have the far-field equation. You will be able to solve that. But of course, the result depends on how much data you can provide. And then we define And then we define, solve this linear U posed integral creation for each sampling point in the domain of interest. That is that you know as a priori your target is in some domain. And for each sampling point, you define this indicator function. And then you plot the indicator function. And you see, you look at the picture. Hopefully, you will find where it is. And this is usually the way it works. But actually, it works for this case. It works for this case. So, what we do is using this so-called extended sampling method to find the location and roughly the size of the target. And that's basically all we can do. So, what actually we do, it seems like we use a disk, try to put it everywhere in the domain of interest, and then suddenly it just covers the target, and then you solve this linear U post equation. You got the norm of the indicator. The norm of the indicator is small, and this is the location of the target and roughly the size. So, this is the information we got. Now, move on to the next step, Bayesian inversion method. So, we have some prior information about that. Now, let's do this. We first parametrized the object and then rewrite the inverse problem as a statistic model, adding some linearized normal. Adding some linearized noise, and then we try to recover the posterior density for Q given pi. And we assume that the noise is normal as you are. And this is something you guys already saw a lot. This is posterior density function. And this is the parameterization we use. This is the kernel in the MCMC. So I won't go into the details. Won't go into the details, and this is the algorithm. We first use ESM to obtain the location and the rough size of the target, build that into the priors for the Bayesian inversion. And this is what we see using one single incident wave. And so you got roughly the shape, but not that good, much better than the direct method we are using. So hopefully that this looks better than. This looks better than that. I only use the director method. You probably get the similar or better things using non-optimization, but of course. So this is the posterior density distribution for the first coefficients in that expansion. So you see it's after a certain number of sampling terms started to converge. So it works for this case. Now, well, suppose you don't. Now, well, suppose you don't know the location, and you start with the Bayesian inversion, and we did some experiment. So, we assume that the location is off, and then as a circle, as the initial guess, let's move. And this is after 200,000 steps, and it still hasn't moved to the place. Well, I think the theorem, I think the theorem from statistics tells us if we go that forever, we will end up with moving to that location. But in reality, you will never be able to do that. You hope that you can get a reasonable estimate for that within a reasonable amount of time. Of course, there are other places that you can do work when you do the posterior, right? Using reduced model to improve the sampling and also. Sampling and also like the statistician going to propose some method to increase the acceptance rate. So, those are the things we didn't touch on that. Euro source problem, another problems, and actually Pedrienne is an expert on that. So, we have Hamhole's equation and with the source on the right-hand side and satisfy Sommerfeld radiation condition. And we know that actually there is anonymous. Analytic formula for that. So we use this as another example to illustrate our approach. So inverse acoustic, you also have the Fafio pattern. And then the inverse problem is reconstruct the source function. We want to know the function, not just the support from the five-field measurement of that. So, what we do is using another. So what we do is using another direct method, which is called the direct sampling method. So you guys saw two sampling methods. But this is simpler. So suppose you know your source is in some area, some domain of interest. And then you set up this indicator function, and then you just plot this indicator function. Hopefully, you will see something in the domain, and that gives you the information about the source. The information about the source actually gives you what gives you a location and also roughly the size of the compact support of the source. And actually, this is what we will see. So we're going to just use a disk to be the approximation for the compact support of the source. Because later on we will use eigenexpansion for the source functions. And the disk are simple and we know it's eigenfunction. And we know its eigenfunctions. So we can do that easily without further computation. So the goal actually, we want to find that disk, and its size is roughly the size of the compact support of F, but a little bit larger than that. So we can afford to do it a little bit larger than that, but not too large. Too large, we won't get anything meaningful. So this is the eigen expansion. The eigen expansion. These are the Dirichlet eigenfunctions. And if we have a function, we can use the Fourier series to expand that. These are the eigenfunctions for the disk. And then we use approximation, actually we use 25 terms to approximate the function f. And now the random variables are the coefficients of the. Variables are the coefficients of these. But actually, the direct method tells us what? Tell us the location, tell us the size of the thing, so we can start at a reasonable point and move on to do the Bayesian inversion. So statistic model, Bayes formula, posterior density function, and then we need to explore that. And we use, I think we use precondition correctness. I think we'll use the precondition Craig-Nicholson MH method, which is proposed by other people to do that. Of course, after your computation, we already use conditional mean, right, and maximum A post theory to describe that. But think about that. Those are the things you want to use in every case. But let's move on to look at, well, this is a PCN measure. PCN metropolis husking, similar things that you saw in many paper or books. So, the problem setting, this is the full after, this is half of the after, right? You have the radiating source, you measure all around, or you just measure a half, or you just measure a quarter. Two examples. This is a smooth example, and this is actually a discontinuous function. We would expect the discontinuous function case. Expect the discontinuous function case wouldn't work very very accurate because you use the eigen expansion which is smooth right and you will you will have some some approximation problem but this is uh for the smooth case uh so the blue line if you can see it that's actually the direct sampling method uh the disk direct zombie method gave me they should be a little bit larger than the compact support of the source The support of the source. And then this is the approximation, and this is actually the histogram of the coefficients. And one goes very large, and there are other things you will see after some samples start to converge. So this is for full after half of them and a quarter of them, and the reconstruction are reasonable. So that's nice. And this is the for the discontinuous case. Discontinuous case and the reconstruction are also reasonable. We compute the relative error of the reconstruction. For the smooth case, it's about 3 to 4 percent of relative error. For the discontinuous case, it's about 7 to 11 percent of error. Well, of course, you can make the Fourier expansion to have larger more terms, but you end up with. Terms, but you end up with more sampling time on the convergence of your MCMC. And this is something I think it's very simple, but I believe it's, in my point of view, it's interesting. How about if you have now uniqueness? And this is usually the inverse problem, especially we have partial data, and we're going to face this problem. There's no way you're going to get the uniqueness for that. For that. This is actually come back to the question: if you compute the posterior density function, is the conditional mean map? The traditional statisticians tells you the estimator you're going to use. So this is actually a model problem from the paper. I just borrowed that problem. You got a unit dish. You got a unit disk, and then this is actually an inverse-medium problem. So, you got acoustic source. This is the wave equation with the source. You have this type of boundary condition and initial condition. So, the inverse medium problem we want to recover, we want to do is to recover constant speed, very simple, but with the given data on part of the boundary of the demand on the X2 plan. So, this is partial data, right? If you think about this. Data, right? If you think about the d as a unit squared, you just have one on that. And this is a very simple model problem. And using separation of variables, we know that you can just write down the exact solution. So if you are interested, you can look at your graduate test work on PDEs. We will be able to do that. But if we know the data on the X2 equals to zero plan, look at 22. Look at 22. We know if you plug c equals to 1 and c equals to 2, you will get exactly the same thing. So what we expect is if you only know the data on part of the boundary for this problem, you will get a non-unique solution. You will get two, right? So this is our measured data, discrete, and this is a statistical model. This is the posterior probability density function, and we try. Density function, and we try to use a Bayesian inversion to try to reconstruct that. So, what do we expect? We expect that the samples are going to accumulate at one and also at two, because you have two solutions, right? This is what you see. You see samples accumulate at one, you see samples accumulate at two, but 2, but you also see examples accumulate at roughly 2.7. So this is one-dimensional unknowns, and these are the things you. So are you going to use MAP? You will only get one solution. You're going to use conditional mean? You will get something meaningless, right? But actually, if you look at the field on that one, if you Field on that one, if you use three different values, they all coincide with the measure very well. So, what the result says, 2.7 is also a reasonable solution. And if you look at the number of samples here, it's probably 25% of probability the solution will be that, right? Like you play in Texas Holden, right? You always analyze how much percentage. And if you do this process, And if you do this problem, you got quite a good chance that you will end up with 2.7. So, what we think about we can do not too much. Actually, we didn't do too much. We need local estimators. So, we will think where the samples accumulate, assuming that you are doing compute the sample roughly very well, and where this is the so-called local maximum. The so-called local maximum posterior estimator. So if you see their samples accumulate at a couple of places, then you probably want to think about that. And of course, if you decide the clusters of samples, then the local conditional mean maybe also tells you something. And well, how to find those k cluster. K-cluster by K-means are the simple ways to do one-dimensional. So we do the K-midroys or K-means and try to find those local things, and those coincide with the solutions very well. But this is a very straightforward thinking of using these clusters. They are much more challenging problems that we think how actually how to describe. Actually, how to describe or characterize the posterior density function. Actually, when I thought about local estimators, I think statisticians, they must think about this before, right? But I ask statisticians, no, they didn't. They thought about like a multiple, multimodal Gaussian, right? So you have a couple of Gaussian put together, right? That's something you can do. But you can certainly. You can certainly not able to use those to deal with this problem. Very simple problem. So, where does this solution 2.7 come from? It's come from if you plug 2.7 into the problem, direct problem, you will generate almost the same data as you plug one or plug two. This is like a quadratic thing, right? You should have only You should have only two solutions that are very close to it. Well, remember, you have the measurement only part of the boundary. So you only have partial data. It's not all around. So you might have uniqueness. You even don't have two solutions, right? Depends on the problem. But you have only one data set, right? Yeah, only one. So you you might have multiple solutions. I have multiple solutions. 2.7 is about the stability. Once those two solutions, that's for uniqueness like 2.7, you observe that if you plot 2.7, the solution is very unclose, but maybe not the exact right. Exactly. But in terms of noisy inverse problem, it makes no difference. It's so close you wouldn't be able to tell. Okay, so this is not essentially a more challenging problem. Essentially, a more challenging problem, but if you do Bayesian immersion, it will be. So, this is point source, right? And very simple: that you have a point source, you generate wave. And if you measure the data at a fixed location, physicalist data, and what do you expect? You will expect the point source actually is in the distance centered at the measurement location all around, and those are all the possible locations. Are all the possible locations, right? This is now let's use Bayesian to do that. I just not do all around, I just do a square. And actually, this is the intersection of that circle with this square. And now, this is the distribution. So, how many solutions you have in this case, if learnmail, right? How to describe your solution? You probably need like a polynomial fitting. Polynomial feeding technique from data science to do that. But this is a very simple problem. Think about a higher dimension. Higher dimension we use conditional mean, but how you can guarantee your conditional mean actually describe your solution of the problem. So how to characterize not even any posterior, any density function, probability density function in higher dimension, even to let it make sense. Let it make sense, I think it's a very challenging problem. But I think it's worth investigating, even from the inverse problem community, if we want to use Bayesian inversion to do that. Here are some reference we did and future work is what we thought about the underwater acoustic reconstruction of time demand problem moving on. Time demand problem moving obstacles, which is challenging because you need to compute a lot, and how to characterize the posterior probability distribution. And actually, I spent quite some time learn, try to learn Bayesian since 2016. So hopefully, I can find some other technique combined with Bayesian to get more information about the I know. Thank you very much. Thank you very much. Any questions for our speaker? So I'm wondering, so the case with 1, 2, and 2, 7 is very nice. So I wonder if in higher dimensions, that's one definite. But if in higher dimensions, if we fail to see, I think something is very challenging. Then if we cannot see, there is like two. If we cannot see there is like 2 or 2.7, then we cannot see anything about it. Well, we can see that, but think about the technique that you use the data clustering in higher dimension. You don't need to see that. As long as you find the data clusters, you can just pull them out and to see where those samples are clustered. We did two-dimensional things. So you actually will see two group of things. Two groups of things cluster here, another group of things actually visualize. Those like k-means or k middle still be able to tell you something. And of course, I think it will be interesting if people can develop a more effective method, try to describe such phenomenons in higher dimensions. So it's basically you see them not by like uh MCMC to to sample, but by some class k clustering sample. Well Clustering setup? Well, for this project, I saw them first, and then I think about probably I can use the cluster method to broke them. But if I didn't see at the very beginning, I already used conditional mean and I will end up with a meaningless solution. And I have to try to think about why. Great. Let's thank our speaker again.