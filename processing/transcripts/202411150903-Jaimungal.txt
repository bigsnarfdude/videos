Christophe for inviting me to be here. It's a pleasure. It's the last time I was here was about ten years ago. In fact, in the summer though, so the weather was quite different. I'm just going to turn on my timer here to make sure I stay on schedule. Yeah, so there's about twice the amount of people that I expected to be here, so that's great. So this work, it's, yeah, the title is Combat Library Biore Centers, Stochacid Processes, joint work with my collaborators. Joint work with my collaborator, Silvana Picenti. It's also at the University of Toronto. You can scan the QR code for the work for the paper that's currently under review. Still happy for feedback if you have any. And the purpose of the talk, really, I'm not going to delve into all of the mathematical details. I really want to give you the big insights into the problem that we're trying to solve and hopefully convince you that it's interesting and some ideas behind how that solution, how we solve the problem. What's the motivation here? Motivation here. So you might think of a scenario where you have a bunch of experts that have looked at some data, maybe it's the same data, maybe they have slightly different data, and they've constructed some model. In the simplest version, it's just some distribution, some distribution assumption on some random variables. And suppose I gave you model one, okay, and let's even suppose that they have admit densities. So I have model one. So I have model one, model two, up to model K. These are the densities. I'm assuming that the models they give you actually have the density, so there's no point mass there. And I ask you the question, and let's say that you have some belief in these different expert models. And then I say, well, I want to come construct a model that uses all of this information that they've given me. But I want to make sure that, you know, experts. I want to make sure that, you know, expert one, I believe maybe 10% of the time, 20% here, and so on. How could I incorporate that belief in these different models and construct a single model? So there's one obvious way to do it, one naive way to do it, and that would be kind of the bag of models or ensemble of models. I say, you know, pi one percentage of the time I sample from model one, pi two from model two, et cetera. This would be a mixture model, effectively, what it would do. And the density. And the density then would just be the convex combination of those densities. Trivial way to do it, but it's somehow not entirely satisfactory. So instead you could ask the question, because for one, this will give you some sort of multimodal model. If you think of, imagine that these individual models were Gaussians, that's what you'd end up with, these sort of multimodal things. And that might not be what you're seeking in a particular instance. So instead, what we ask is: what is the model that? Is what is the model F? What you can ask is, what is the model F that is closest in some sense to all of these models? Where the closeness is measured by the Kl divergence between F and F1, and I weight that by pi1, F and F2, and I weight that by pi2. And then I find the F that has the minimal weighted KL. This gives you something different. And what it gives you, in fact, is What it gives you, in fact, is this model divided by the integral of the same stuff. So it gives you kind of a geometric average of the densities and then normalize. So this is something that you can do fairly easily for random variables when they admit a density. You can probably, if I give you 10 minutes, you sit down and you might be able to come up with it. But now if I say, I also But now, if I say, I also want to incorporate some beliefs that I have of what this data should, some properties of the data, such as the expected value of the random variable that this model represents has to be something, maybe it's zero. Maybe the expected value of a function of that has to be something, maybe again zero. Maybe I have a whole finite number of such constraints. How would I now incorporate it? And my belief. It and my beliefs, and still be close to these other models. Turns out you can also answer that in a fairly nice way. What we're doing is effectively extending this idea to the context of stochastic processes and coming up with a way of explicitly solving it, or I should say, quote-unquote explicitly, and then a methodology for being able to explicitly and computationally give your solution. And you might say, well, if you're going to process, And you might say, well, if you're going to process these, why not look at Wasserstein distances and adapted Wasserstein and so on? Well, the basic reason is that we would like to be able to construct explicit solutions and be able to come up with a methodology that gives you a solution as opposed to proving uniqueness existence and having some dual representation of the solution in terms of some solution of some other problem, which is what you could do potentially if you used an adaptive astro sign. So that's our setup. So here's the little bit more formally what we're doing. So we say, let's take now, we have these different probability measures that are representing the different experts in the setting. And under the Kth expert, X, the process that I'm interested in, satisfies this SD, so it's a fairly simple SDE. The only thing that differs from expert one, two, et cetera, is the drift, because I'm Is the drift because I'm going to constrain myself since I'm using KL, if they didn't have the same covariance, instantaneous covariance matrix here, then they would not have infinite KL. And then you might find that unsatisfying and say, well, you can, in fact, do a little cheat to allow you to have different sigmas. For example, introduce a new process that mean reverts to the different sigmas, make that mean reversion relatively fast, but a size one over epsilon. A size 1 over epsilon, and now include that as a state variable, and now you're back into a setting where they all have the same instantaneous covariance matrix, and yet you still allow for some notion of different variances. Okay, so you can indeed do that. Okay, so as I said, we want to answer the question, how do we combine the expert models and how do we simultaneously incorporate your beliefs? So here's a little picture example. This is not necessarily even in the context of financial markets, right? So let's say that this is two climate models over some long time horizon. One doesn't mean one year or one minute or one month, one century, right? I don't really care what the units are. What you're seeing there, forget the dotted lines, these dotted red and dotted brown. What we have is a number of sample paths from model one, number of sample paths from model two. Those are the ones that are kind of. Two, those are the ones that are kind of phased out, and then we have one particular sample path in blue that we're highlighting. The various shades there and the lines are the quantiles at each point in time. So you can see that in model one, it's representing something that's kind of looks like perhaps it's geometric Brownian motion-ish, right? And the second one is some sort of mean-reverting process to this seasonality-like trend that you're seeing there. And you know, do climate scientists come. And you know, do climate scientists come to you with these two models and then you say, well, I would like to incorporate that, I would like to use those two models, but I want to create something that combines both of their expertise. Moreover, I want to say that at the end of this time horizon, perhaps the average climate change in temperature, here it's around 1.5, here it's around 0.5. What I want, because of maybe some. What I want because of maybe some regulatory requirements, I want to use this model to then do simulations to do something. The regulator says that you have to incorporate a one-degree expected change. None of these models do that. So that's a belief I would like to incorporate. Perhaps you also want to ensure that some percentage of the time, the percentage of time, sorry, that the process lies below this dotted line, isn't what we observe in Model 1 or Model 2, something else. Too, something else. That's another kind of belief that we can incorporate. So, here is, in fact, the optimal model, the one that's closest to these two, that attain those two constraints, that the expected value of the terminal price, change in temperature, is one degree, and the average time we spend below that dotted brown line is 20%. And it's the one that's closest to these two. So, that's the goal. That's what we're going to do. That's what we're going to do. All right, so how do we go about doing it? Well, it turns out that there's this one particular model that is useful. It's kind of just a tool that shows up. So this particular model is one that you might naively have constructed. Take a model where the drift of the X process is the average of the drifts of the expert models. So, take that probability measure as I'll call that Q bar. Whenever I put a Q with a square brackets, what sits inside of it is the drift of the process X under that measure. So, here's the problem, the formal version of the problem. So, we're going to find the model that is the barycenter of models. So, we're going to take, we're going to seek over a collection of probability measures, Q, that are inside of some set that minimize the. side of some set that minimize the KL from model K to Q weighted by pi K. So here are our models. We're going to first, it's going to turn out that it's useful to first move to this Qmu bar model. And then we're going to try to find this model QE, which is the barycenter, the one that minimizes the KL from all of the others. So this is just a tool to really do that. And then we're going to incorporate these constraints. Constraints. The constraint types are exactly this kind. So there's an expectation of a function of the terminal value of the process. We want that to vanish. And we want the expected running cost of something to vanish. And of course, you could do multiple versions of these. Like, you didn't have to just do one. You can have many of these constraints. But we'll see how it works in the set. So the first step in getting to the result is. Yeah. Say that again. The pies are going to be fixed. So that's your belief in the various experts. And we're not saying how you construct the pies, but maybe it could be even that you have your own data set somehow. They've given you your models. Now you compute the posterior probability that that data set comes from model one, model two, model three, and that's your pies. Or it could be something completely different, just the ratio of the amount of data that was used to construct model one, two, three. Construct model one, two, three. We're agnostic as to how you get the pies, but we are assuming that they are just fixed. Yeah, did you have a question, Antonis? It looked like you did. Okay, okay, sure. All right, so as I mentioned, we're going to introduce this parametrized set of probability measures as q square bracket theta, and the theta simply represents the drift that x has under that measure. X has under that measure. And you can easily, you know, if you just do a little quick calculation, you can easily see that this is the Radonicode derivative that you'd have to use with this particular choice of this lambda for any model K, and it will induce this particular process being a Q theta Brownian motion, and then the drift of X under that model is in fact theta. So that's a way of us to get a single probability measure. Single probability measure by particular measure changes from each one of those models. So then we want to introduce those constraints because we'd like to ensure that we optimize subject to those constraints. So we would like to introduce this Lagrangian where we have two Lagrange multipliers to implement our constraints. And if you simply utilize this representation for the measure change, then you can write it in terms of this weighted. Weighted loss function here, where these delta thetas are the deviation of the model k to theta. And the big sigma is effectively the instantaneous covariance matrix. When you set those etas to zero, that reduces to finding the pure Barry center, then the constraints are irrelevant. So we make one important assumption here, the assumption that the expectation of the exponential of this object. Of the exponential of this object here, what is that object? It looks like a weighted Malhalanobis distance between the models. Sort of a weird object. I don't know if you've ever done any kind of basic machine learning. This term malanobus distance shows up as a way of measuring, instead of a Euclidean distance between points, you have this weight that, or sorry, you have this covariance matrix that shows up in between them. And here it's a weighted version of that because we have multiple. Version of that because we have multiple models and we can weight them differently. So we assume that the expectation of the exponential of this thing under that average drift measure is finite. We need this in order for any of the results to hold. So we'd have to check this first somehow. Okay, so first result is that a candidate for the value function, where I'm not going to show you how we go through and derive and get this result, but the solution for the candidate value function. But the solution for the candidate value function has this representation. It's the expectation under this average drift model of this exponential of that same Mahalenobis distance object. And then you incorporate your running constraint as well as your terminal constraint here. This turns out to be a candidate value function. And we have a verification that tells us, in fact, it is the true value function. Based on this, under some other Based on this, under some other technical assumptions, we can show that the Barycenter model actually exists, it's unique, and the random Nicodine derivative that corresponds to the optimal probability measure has this very nice form. Again, this object shows up, this weighted mahelanovis distance between models. The denominator is simply a normalizing factor, right? This is really the important, interesting object here, clearly. Yep? So it's still the theta is unknown, right? This is like the control control. The theta is like the control parameter. Because theta is the control parameter of this, when you write it in this form, in terms of the Lagrangian, theta becomes your control. That's right. And when you go through and you find your optimal control and the value function corresponding to it, this is the candidate value function that has the square representation. And you can use this in order to construct what the actual probability measure changes. The actual probability measure changes. So, for the case when there's no constraints, it has this particular form. And I'll show you what the theta, the optimal theta, is in a second. I think it might be on the next slide. No, not quite. So, in order for us to construct the theta, what we first wanted to do is, well, we'd like to also incorporate the constraint so there's a feasibility condition. Because maybe you're giving me some constraint that is not attainable. Suppose your underlying process was strictly positive somehow, and then you. Strictly positive somehow, and then you want to say that the expectation of x plus 1 is 0. Well, that can't happen, right? That's not feasible. So, the feasibility turns out to be, you can represent it in this way, again, sorry, not again. Now you have this expectation under this barycenter model of this exponential of the constraints that you have. This is kind of a cumulant generating function for this random variable, the sum of those two, effectively. And if the gradient of And if the gradient of that cumulative generated function has a point where it vanishes, then these constraints are indeed feasible. That's the way that this should be read. So we assume that this holds true. Then the constrained barycenter drift has this representation. It's given by the drift under that average drift model. So the mu bar is just the pi k mu k. And then there's this correction, which is this log of this omega guy. Which is this log of this omega guy? And what is this omega guy? Well, we saw it earlier, it was part of that canon value function. So it's this expectation of this Malhannobus distance, and then you have, again, these constraints that you're incorporating evaluated at the optimal Lagrange format, which are the ones that, in fact, force the gradient of the accumulating generating function to vanish. And the measure change, the optimal measure change, again, in terms of the optimal measure, has a In terms of the optimal measure, has the same kind of representation. So before we didn't have the eta zero and eta one for the barycenter model, if you send up the zero, and when you have the constraints, they are incorporated there. Okay, great. So I think I'm at minute 17, so I think I still have ish minutes. Okay. All right. So an interesting question you could ask yourself, if you go back to the original kind of diagram here, okay, maybe this, in the case of random variables, we have this. In the case of random variables, we have this. We say we found this barycenter model, and then we've also found another model which is kind of the one that let's call that B, this is called this one F, the one that somehow incorporates a constraint, right? Now you can ask yourself the question is, the following question. Suppose I found the barycenter model, and then I found the model closest to the barycenter model that implements my constraints. Is that the same as this, implementing the constraints and being closest? Constraints and being closest to these guys, it's not obvious that that's the case. It turns out that, in fact, it is the case. That's what this effectively says. It's easier to digest this statement by saying that this diagram commutes. You start with your underlying probability measures, P1 to PK. You minimize a KL, weighted KL, to get to the barycenter model, and then you find the model that's closest to the barycenter model, but implements your constraints. That's the same as minimizing the KL. That's the same as minimizing the KO subject to your constraints from the original models. Because this way around here, the constraints don't know anything about P1 and PK a priori. They just see the barycenter model. While going here, the original models are directly part of the construction of the criterion. So effectively, you can go there, it's equivalent to going to QB and then finding. Going to QE and then finding the one within the constraint set to Q star. It's equivalent to just going into the constraint set and finding the Q star that minimizes KO. Okay, in the last little bit, I'll just touch a little bit about the numerics. Okay, this is the same example we saw in the beginning, right? That's our model one, model two. And I'm going to just show you how the model distorts as the probability that you believe in the different experts changes. So this is where you believe 10% in models. We believe 10% in Model 1 and mostly in Model 2. Now 30% in Model 1, 50% in Model 1, 70%, 90%. So you can see how it kind of very nicely sloshes over between these two models. I quite like that. There's a nice little animation you can do. Now, you could ask yourself, how do I get these plots, right? Because I've given you a characterization of the round and a cana derivative, I've given you... Of the round and akin derivative, I've given you some representation of the solution in terms of this omega object here. Now, for those plots that I just showed you, we did a very simple thing. Well, you can use a Feynman cats representation that tells you that this particular function satisfies some PDE, and then you can solve that PDE via a finite difference. And that's exactly what we did. But what if the number of dimensions is high? What do you now do? Well, that's where we incorporate some deep learning. Well, that's where we incorporate some deep learning ideas. And don't have time to get into the details of the idea, but here's a kind of a sketch. So, what we'll do is we want to simulate under our optimal model. And there are two algorithms that we developed, and then I compared them. One is by, say, we could just use the parametrization of the drift as a minimization problem based on the criterion that we actually want to solve directly. So, that's kind of a naive approach. So, you're parametrizing. Naive approach. So you parametrized a drift as the output of some neural network, and then you just minimize the loss, which is the weighted KL, and try to impose the constraints in some way via, say, like augmented Lagrangian approaches. And you go ahead and you can find a solution. Or instead, you use our analytical characterization in terms of that omega function, which requires computing that conditional expectation. And if you want to compute that conditional expectation, well, again, you can use a deep learning method. You can say, let me now. Method. You can say, let me now use elicitability, or really it's L2 projection because it's a conditional expectation, in order for you to approximate that conditional expectation and then induce from that the optimal theta. So those two approaches are what we developed. This is just a little detail about how we go through that. So, first, it requires solving for the optimal Lagrange multiplier. So, there's an easy way to do this because we have the representation in terms of The representation in terms of the measure change from an arbitrary eta to mu bar has this form, and we want to find the etas such that the constraints find. So you can do this fairly efficiently, and you do this stage. And then, as I said, we now need to estimate this omega, and we do this by this elicitability. So, we want to minimize the integral of the expectation under our average drift measure of the squared error between omega and Error between omega and this object here, the one that we want to compute the conditional expectation of. And again, you can approximate this outer integral in time by a Fourier sum, and now you can approximate the expectation via samples, and that's just a mini stochastic gradient descent effectively. That's what's stated here. And I just want to show you an example. Let's not absorb the details of the example. What I want to show you is how the constraints actually get binded as we. Constraints actually get binded as we run this iteration. So here's the constraint that we want the sum of these two random variables to be 1.2 and their variance to be 0.05. The underlying models P1 and P2 don't have those properties at all. In fact, as you start here, the expected value of your first constraint and your second constraints are quite away from zero. And as the algorithm runs, you can see these come down and converge quite nicely. A real full sum example, I'm going to finish in a couple minutes to leave some time. Yeah. Real fulsome example is: what if we wanted to do something with implied volatility? So we took these implied volatility data at a fixed number of deltas. So the data we have was in terms of deltas as opposed to strikes. Each one of these curves, the dots is the. Curves, the dots is the data, the curves is a projection onto a functional bases. So we did for every single day, we have these dots for the data we project onto functional bases, and now we get some nice curve, and what we would like to do is create a model for the evolution of these curves. And we broke the data up into different bins. So, oh yeah, sorry, one other step. So, this is saying the implied volatility we project onto these bases functions that gives us these coefficients. And then we Coefficients, and then we learn a neural SDE for these coefficients. So, this is the experts coming up with their models part, not yet the optimization and solvement or label KR. And we broke the data into different pieces, into, I think, three different chunks. So we estimated neural SDE model one, neural SDE model two, neural SDE model three. And then we used those as our experts and continued on the process. So the A's here really are our X processes that we were talking about all along. X processes that we were talking about all along. And the implied volatility is really just some function of the underlying X processes. We had, I think, five bases, which is why it says five there. As I said, we split the data into these bits. We wrote the models. Here are the model. This is showing coefficient 1, 2, up to 5, simulated under model 1, under model 2, under model 3, and you can see they're quite different in terms of. Different in terms of their distribution. If you look at the quantiles at various time slices, this is what the average model does for you. If the average drift model, sorry. And what we wanted is we wanted an interesting kind of constraint. So we said, let's constrain, sometimes traders have beliefs that, say, the skewness is going to be on average something. So here we said, okay, we're going to enforce the skewness at the money. The skewness at the money to be 0.05. In the data, it's not at all, it's not 0.05, it's something quite different. And in fact, we'll see what that looks like in a second. And that will be my last slide. So here's a little animation of the evolution of this curve under the different models. So P1, P2, P3 are those, you know, blue, orange, and green. And this is one sample path of the evolution of the implied volatility surface. And they do what. And they do what they do. Of course, they all start together because that's the starting point of the simulation. And as time moves forward, now we'll see some variability. And under different models, they evolve differently. Same underlying Brownian motions drive the models. So it's the same risk sources. The purple one is the optimal model. And you can see it deviates quite substantially from the others. And that's because of the constraints that we've imposed. We've imposed this constraint that the slope of this thing, this has to have. That the slope of this thing has to have higher skewness than what we've observed in the past. And it so happens that higher skewness tended also to be coupled with the volatility surface also rising. And so in the optimal measure that we find, it really incorporates those features. So I think I will end there and just say that the idea that oh sorry, that shouldn't say Levieto processes. Lavaito processes. That's another paper. That's the paper that's a predecessor for this that does a much more complicated underlying dynamics, but only a single model. But yeah, so we have this analytic representation, we have this FD algorithm, but then as we go on for the diffuser models, we incorporated this deep learning methods as well as an application to LUDs. So I think I'll stop there and happy to take questions. Thank you very much, Sebastian. It was a very interesting presentation. So we have time for questions. Could you go to the remote slide? Oh, my 28. This one? Yeah, yeah. I don't know if it's just my eyes when it transform me, because when I see the screen uh the smile, it it's supposed to be a U-shaped uh, even considering the screen mass, right? But somehow it seems to me it's Somehow it seems to me it's very faintly like an on the. Yeah, there's a little bit of a W in there. Is that what you're saying? Yeah, yeah. Well, in this particular, for this, I don't recall what asset was this. Do you recall, Silvana? What it was? I don't even remember. For this particular asset, there is this little bit of a W that happen that shows up in the data. In fact, probably if we go back toward the end of the time horizon. I skipped over too many. There it is. Okay. Yeah. So you see, I think we're starting the evolution more or less from this curve, this purple one, which already has that bit of this W-ness in it. And the data, in fact, has some, you know, I'm only showing a few dates. So that's where it stems from. I was wondering what is the connection between the bias and that she's constructed and biased and model average. And Bayesian model averaging. Well, Bayesian model averaging, if I'm not mistaken, I mean, that's when you get new data and then you're updating your models based on this new data that you've just observed, right? Here, we're saying instead, no, we're given models, right? Let's go back to our problem formulation. Okay, we're given the underlying models P, 1 to P K, and we're also freezing our beliefs in those underlying models, and we're trying to construct a single model that somehow combines them. There is a kind of, I mean, you can, let me see, how would I can make the connection? I mean, I might make the connection by saying for a Bayesian model averaging version of this would be: I'd update my pies as I get new data, but then I'd still want to construct this. But then I'd still want to construct this amalgamated model out of it. Some parts of the literature, they also try to construct a single model based on combining other models. Yes, yes. Yeah, yeah, yeah. I mean, you know, the naive mixture model is a way of combining them, right? Yeah. So this is just another an alternate and as far as we or as far as we know, there there's not been something posed in the context of these stochastic processes for combining. So yeah. For combining. So what you call mixture is when you co-simulate picking paths. Take a latent variable y that's independent of everything. Discrete. Sample Y, conditional on Y, your model is FY. That's what I mean. Yeah. And then you said the density of somebody there would just be a range of density? Yeah, so the mixture model could potentially do. The mixture model could potentially just give you far too much variability, right? And maybe it gives this multimodal aspect. It may be something that you want to do. We're just suggesting another ultimate, yeah. In terms of risk measures, yeah. Mixture would be, yeah, would give you a larger risk than I would say it's likely it will give you a larger risk. It's likely it will give you a larger risk than this model. Yes. I can't characterize, I mean, I don't, do you have any idea how that might, bordering might work? And it's probably going to depend on the underlying models as well, right? What they are. So applications, it's going to be conservative, maybe. For risk management purposes, yeah, perhaps, yes. Yeah. And one other place that we've seen this. Another place that we've seen this kind of question being asked is in the context of these diffusion models, where you have different data, you've learnt the forward-backward diffusion from the different models, and now you want to combine the scores. So the mus here could be the scores that you've learned from different models, and then this gives you a way of aggregating. Yeah, for example. Yeah. I'll have a question about that. Maybe this is not relevant to your context, but in general, You can. You can't. Yeah. It trivially, it kind of quote-unquote trivially follows. Okay, yes, it has a problem because it's very complex, it's very difficult. I was wondering in your framework, you can do that. You can. It effectively just can even tell you what the solution looks like. Yeah, so oh, sorry, that, yeah, that's right. So, all that happens is that instead of evaluating the constraint to calculate. Instead of evaluating the constraint at capital T, we evaluate it at that earlier time. But the measure, the optimal measure, still has that form.