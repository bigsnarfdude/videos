But mainly, this project that is still ongoing, probably like only 60% done, but I feel like it's probably ready to just talk about this over here. It's to count how many global clusters there are in RGD Hues Galaxy using this new model that we've developed. And this is joint work with my supervisors, Gwen, Patrick Brown, Bob Abraham, and my collaborators, Phil, Josh, who is Josh, who is back with us, yes. Sam and Shani Danielli. So, am I doing right? Okay, what are ultra-defense galaxies or UDGs? So, these are actually a very weird type of galaxies, and they're super, super faint. They were first detected in massive abundance in the Coma cluster using the Dragonfly telephoto array, developed by Peter Van Doucom and my. Peter Van Dukuhm and my supervisor Bob Abraham back in 2015. And they are weird because some of them, actually a lot of them, seems to have mass that's on the similar order of the Milky Way, but at the same time only 100 times fainter. So one of the most famous examples of this is the Dragonfly 44 galaxy shown over here. So this faint blob in the middle, that's basically the galaxy. And these bright dots. These bright dots, as you can see over here, like one over here, they're basically the global clusters, or presumably, the global clusters. And thanks to Sam, who have already talked extensively about global clusters, we're not going to go into it anymore. Thanks for saving my time. But the thing is, why are we interested in them? As Sam has mentioned, global clusters are important for galaxies because they're actually some of the best indicators for Some of the best indicator for mass estimates. And so, this is also the case for UDGs. And actually, a lot of the mass of the UDGs are directly inferred using the global clusters. And also, they're useful for inferring the dark matter content of galaxies. And UDGs are actually probably one of the best avenues to study dark matter. But if we want to do this, we definitely want an accurate estimate of the global cluster counts. You might think. You might think this is an easy task because, well, any 10-year-old know how to count, but it's not that easy because there are actually quite a bunch of issues associated with getting an accurate count. And different research groups actually use quite different method in dealing with these issues. And so there's actually no, up until now, no unified or standardized or any statistically sound method. Or any statistically sound method in actually getting a global cluster can estimate. So, what this has led to is actually a very weird phenomena where different research groups actually got very different results of global cluster count estimate on the same galaxy. So, this example goes back to our Dragonfly 44, where last year, this paper by Safalahi, they estimated the global cluster counts for Dragonfly 44 with how an estimate of 30, but in 2017, Peter Van Duclumstrew got an Peter van Duclumstrup got an estimate of seven. So, very, very bad. So, what are these issues with the global cluster counting? So, the first one is what I call a global cluster membership assignment. So, when we observe an astronomical image, you might see all the bright dots in it, and presumably they're global cluster. But the thing is, we don't actually know where this global cluster belongs to. So, it could belong to a So it could belong to a giant elliptical galaxy or an ultra-diffused galaxy or it doesn't necessarily belong to any galaxy. It could just be a stray global cluster in the intergalactic medium. And how do we deal with this? And there doesn't seem to be a really good way of dealing with this upper tunnel. And the next thing is the global cluster luminosity function. So this thing is basically the brightness distribution of the global clusters. Global clusters, and or in statistical terms, it's basically a population density function for the brightness. And they're typically modeled by a Gaussian distribution. And the reason people use this thing is because at very far distance, we would not see the faint global clusters. But it doesn't mean they're not there. You still have to account for them. So, what people say is, and the thing is, for the past, I think, 50 years, I don't know exactly how many years, people have seemed to. People have seemed to reach the conclusion that this distribution is the same everywhere in the universe. So, what people would do is just simply say, Oh, let's assume this thing is fixed. And then, because we know the detection limit of the telescope, so we can figure out what is the proportion of the faint global cluster that we cannot see. And you account for that, voila, you get a global cluster count. But is it really fixed though? Because back in 2021, Shen Etel, they actually found two users. Until they actually found two UDGs that have really weird global cluster luminosity function, which they estimated to be about four times brighter than the presumably universal global cluster luminosity function. And if you assume a universal one, you will get completely wrong global cluster counts estimate for these two guys. So and also like the weirdest, probably one of the weirdest one of them all is some of the global UVGs get Glover closed UVGs get negative GC counts, which is absolutely absurd. So, yeah. So, what do we do? We basically use point process. So, this page is titled as a true global cluster point process. What I mean by that is right now, I'm assuming that you can see all the global clusters in any kinds of environment. And in order to address the GC membership problem, To address the GC membership problem, I'm assuming that the GC point process is made up of three different environments or three different global cluster population. So the first one is the intergalactic medium. So this is a not so good image of the intergalactic medium because there isn't a good image of the intergalactic medium. So for the global cluster in this environment, I'm Environment: I'm assuming this is basically a homogeneous Poisson process with some constant intensity lambda. And the next environment is basically diamond-lipped galaxy, a bright galaxy, whatever. And the last one, of course, is ultra-defused galaxies. And these two environments are basically modeled by SSHP file. And the parameters that we're interested in basically: Beg N is the average number of global cluster, RH is the Rh is the half-number radius. Small n is basically the search index. And so the actual model that we're using is this mouse full of thing. But actually there's two parts of this. So the first thing is a thin point process. So what thin point process is, is actually not a model, but rather is an operation being done on the point process. So what it is, is say, given some probability, we randomly remove a bunch of point processes. We randomly remove a bunch of points. So, this seems to be a pretty good idea, right? Because of like the faint level cluster, we don't actually see them. But the thing is, the standard thinning point process is done according to the location. So, the thinning probability is a function of the location. But right now, what we want is actually a thinning probability that depends on the mark. So, what we would need right now is actually a mark point process. So, what mark point process is is Is basically an extended type of point process where each point right now has some kind of mark or characteristic attached to it. So, in our case, it's basically the magnitude of the global cluster. And so, right now, the intensity is also extended, where it's a function of the location and the mark. And in terms of modeling, the intensity function of mark point process can be decomposed into two parts. So, the first part is the lambda. Part is the lambda zero, which is what we call a ground process, which is basically the intensity that describes just describes the spatial distribution through global clusters. And the next part is the what we call a mark distribution. So this is actually just the probability density function describing the mark. So this is basically the GCLF. So this model right now is basically assuming there's no thinning at all. So what we want is to have a model that basically thinned this entire process. Entire process based on the mark. And of course, we want a thinning probability, but this thinning probability is none other than the completeness fracture. So I think Stefan talked a little bit about this just now. So what the complete dispraction is, is basically the proportion of sources that a telescope can detect at a given magnitude n. And we over here just assume it's a logistic function and the parameter. And the parameters over here they can generally be determined just use artificial fixed star test. And so, right now, we want to figure out what the thinned intensity is. So, the thin point process is still a marked point process. So, it can still be decomposed into a ground process and a mark distribution. But we could first figure out what the mark distribution is. So, of course, it starts with the non-thin version, which is just our good old Our Good O Gaussian density for GCLF. But the thing is, we also know what the completeness fraction is, which is good old logistic function. And the thing is, the resulting thin point process, or the thin mark distribution, is basically applying Fm to this density in a fashion of truncation. So you just truncate it, and then you get this sort of skewed distribution for the magnitude. So this is what we can see from the text. This is what we can see from the telescope. And the ground process intensity right now is basically just whatever number of global clusters that we could see based on the probability that is left over based on over here. So rho is basically just taking an expected value with respect to this completeness fraction with respect to pi zero. And so our final model, I'm not going to talk about. Not done. I'm not going to talk about this at all. And bye. Results. We fitted this model onto a data set called the Piper Seventh, which is basically a Hubble imaging program targeting the global cluster population in the Perseus cluster. And here's the results. So these are all the UDGs that actually return the positive global cluster of counts in the posterior. Global cluster counts in the posterior, so all the other ones are just zero. And so the red distribution is basically the posterior for from our method. And I also did like an estimate on the counts using the Psychology's method. And you can see there's actually quite a bit of discrepancy, not surprisingly. But I guess one of the good things about this model is it's entirely variant, it's completely encountering all kinds of Encountering all kinds of uncertainty and whatnot. So we were actually able to figure out what exactly went wrong with the method. And the Devos is basically on in this figure, which is basically the posterior distribution for the mean of the GCLF. So the red distribution is again the posterior distribution for the mean. And the green line is the estimates obtained using the Sephalahis method. And the red circles are the posterior mode. So what you can see is whenever So, what you can see is whenever, say, just focus on the mode right now, whenever the mode is to the left of this green line, our estimate is higher. And whenever it's to the right, our estimate is lower. So, I guess in astronomy for statistician here, magnitude is a weird kind of thing. So, yeah, so like if it's a smaller number, that means brighter. Higher number means fainter. But what this means is that. means is that it actually kind of makes sense because if you have a fainter global cluster luminosity function that means the proportion of global cluster that's going to get removed is more so the total number of global cluster that you will have in your final estimate is going to be more as well so the reason that this is happening is because in Saifolai's method they stack their data so they were saying oh like each of the uni G has too few many of global clusters our estimate is not going to be reliable so we're going to stack all the global Reliable. So we're going to stack all the global cluster and all the UDGs and fit a single GCLF parameter. So by doing this, they completely eliminated the individual variation in the global cluster luminosity function, which in turn caused this massive discrepancy over here. And how much time do I have left? Two minutes. Okay. I'm probably just going to skip this. I have something a little bit more to talk about. I have something a little bit more to talk about, but like, I guess that that's good of it now for now. I'll I'll uh yeah, I'll take I'll take the question right now, I guess. This is some future work. Yeah. Alright. I'll record this. I haven't oh good. I'm not having that idea.