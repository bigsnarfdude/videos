Give me the opportunity to talk about uh this work here. So it's joint work with Omar Fauzi and Ayla Shajegi for the DNS View. And here's the archive if you want to know more. And maybe in the start, I should point out that this title might be slightly misleading here. So because I'm very much not a computer scientist, and so my co-authors, they Co-authors, they have selling this result in a certain way. And I'm very much unfamiliar with this kind of with this. So I will somehow talk about general mathematical results. And then, I mean, it will also apply to full-to-or and quantum computation, but it will be even more general, I think. Okay, so let's start me with a question that I want to look at. So, the question is really: how long can we compute one who compute with say fixed with a constant number of multitudes. Number of multitudes. And let me maybe say what I mean by compute. So I will use throughout the talk, I will use, I mean, okay, we mentioned different kinds of models which you could look at. But what I want to really talk about is a very, very general model of what this means to compute. So somehow like what I have in mind here is that there are a fixed Is that there are a fixed number, say, of k qubits, and then we can do some operation, some operations, from some set of operations that we will discuss. And then after these operations, the noisy part, right, I want to consider this case where on each of these qubits there's some noise channel acting acting. This noise channel, I mean, okay, I want to want a very general result, so I wouldn't. I mean, I don't want to use too many properties of this noise channel. Of course, we have to use some of the properties, but I want to kind of talk about general noise, not only about depolarizing noise that is very small probability. So then I match my other operation. Meta operation and so on, right? Probably going to be the action of the motor channel afterwards. And so on, until at some point I, after t time steps, I'm somehow I'm somehow done, or I want to want to make a statement about about the the whether this computation was successful. This computation was successful. And in particular, what I want to do is I want to even look at a case where these operations can have some classical memory system. Because when you look at these results that are in the literature on this, then they often, I mean, they make several assumptions on these operations that you can do, but I think almost all of them do not allow for a perfect classical memory. This will somehow need. This will somehow need very different techniques than slightly different techniques than what was in the literature. So, here my answer, of course, to this question is will depend on several factors. So depend on this noise channel, noise channel N, will depend on, of course, the number of qubits that they have and on. And on what kind of operations are allowed. And as I said, I want to kind of have very general results. I don't want to assume so much on this operations. Okay. So let me maybe start by a couple of examples of what people have been looking at here. So maybe an example. Maybe first of all, I want to even go to maybe a bit more specific case. So when I say here compute, maybe I'm also generating. So maybe I want to look at the easiest way, the easiest computational task is a memory. I will focus on storing storage function. This model, we usually look at You should look at this case where I have some encoding of a single qubit, and in the end, I can somehow do a decoding. I just want to have the situation where the state row that I put in is somehow close to the state row hat that I get out. This will be my complication class that I want to focus on. So let me say, so in this setting here, so when I look at the model of a memory, so I'm interested in like these statements of the following type. So I want statements of the type that if this computation That if this computation time t here, this is always the time of the computation, if this t exceeds some function f k and n, then for all e and d that I could choose here, and all um and all operations O1 to o t, all circuits that are computability. Circuits are good here. This distance of this noisy circuit of length t composed between E and D, and compare this to the qubit identity, say a diamond law, then this would be greater than epsilon zero for some fixed epsilon zero. So Some epsilon 0, and in this paper, somewhat for technical reasons, it shows epsilon 0 to be 1 over 128, which is actually quite important. Okay, so I have a question about the setting. So, if your goal is to implement setting, then what could these operations model like? Yeah, we'll say a bit about it for now. So, yeah. Um yeah. So we'll always so maybe maybe for the terminology we'll switch channels here. They will always call call this here some of the this minus third C naught C G. Okay, so let's do some examples to get some some idea. I should maybe say one thing that I'm I'm writing this a bit in a That I'm writing this a bit in a way resembling the capacity of these rates, how quantum channel theory we write, we quantify our errors. Of course, one thing you can immediately say about this is that somehow this single qubit, but I want to somehow do many qubits and I want to. And I want to encode them in this memory such that this is close here. Then, this overhead that I have in this computation is better be smaller than what is quantified by the quantum capacity of these layers of channels. So, right, so if I encode many qubits in here, then I can view this whole last part of the circuit as a large decoding for this channel m to the k. So, some of the quantum capacity. So, some of the quantum capacity will already give me a bound on the overhead that I need here. And somehow I will not really talk about this bound because I think it's rather trivial, but already this bound sometimes gives you better results than some people claim in the literature when they study this, which is somehow strange. So when we try to prove lower bounds on these overheads, upper bounds on, yeah, when you try to bound the overheads in these for tolerant settings, then make sure to compare this to the quantum. Then make sure to compare this to the quantum capacities, right? So, this is maybe the first order that you should first bound. Okay, let's do a few examples. So, maybe example one, this is an example that people have looked at really a lot, are these unitary circuits and depolarizing models. And depolarizing modes. So here, these operations, they're unitaries, and there's no classical memory. So what this means is that this O1, OT, they are unitaries and there's no classical number. And then for this, when you have this depot. And then for this, when you have this depolitic noise, so the point is now, what you can do is that this depot noise, you know, this produces some entropy in the system, and the unitaries that you will apply, they cannot remove the entropy. So the entropy will build up, and you can quantify how much this can build up. The observation is that the huge piece builds up, and you can even Builds up, and you can even quantify this. So, the entropy of this noise circuit, if you want, this noise t applied to some rho zero, I mean the rho zero would be the state that I'm feeding here. This is greater than one minus p to the t of h oh zero plus one minus one minus p to the t P to T of K, where K is the number of qubits. So P is the strength of the deposit nodes. So this here leads usually to an upper bound. So first of all, this side goes to the maximum value of k, as t goes to infinity. And then if you now note, you can now get as a continuity bound, you can get a result like. You can get a result like this, where you get this function here. What this usually leads to is that it's impossible to store longer than logarithmically in the size of this than O of log K, where K is the number of qubits. So this is like a very strong restriction on the time that you can build this. On the time that you can build this memory. I should say that this is like a very old argument. So this goes back to 1987. And then there's a follow-up of this one. So this argument, can you use it to bound the total rate of density on this one? Yeah, that would be one one possibility. I mean in in principle this is like the this this inequality. This really is a if you write in the relative entropy, this really tells you how fast this converges to the fixed point of the scale inequality to get a trace, yeah, trace distance estimate or you or you or you yeah Dynamo also has this nice results on that you can then say something that the state will have some Something that the state will have some high temperature. There are many ways on how to use these results to get some balance. Okay, I mean this is a very basic result. So the problem is that of course it's very unrealistic, right? It's very unrealistic that we only do unitaries here and we don't do anything else. So in particular this breaks down when we We go to a slightly more sophisticated model. So if we would say, for example, that sample two is this unitaries plus key incidence. And this is some of the setting where And this is some of the setting where here in this, where still we have a size of k qubits in our system. But here we can inject in the medial steps, we are allowed to somehow prepare some key ancillas. So it would be like a some kind of key ancillors. So here I say there's a K1, this upper. K1, this upper part of the K2 here, these keel answers go into these unitaries, which will also be hit by noise. They will only have low entropy. And then in e I can in each time step I can inject this these Kinunculus. So And already with this I can do some kind of error corrections. I should also say that the answers can be traced out in the end. So with this I can already move entropy from this upper part where I did the computation to this lower part and remove it to that. So here already this argument doesn't work anymore and in fact I think you can show that with this you can compute. With this you can compute actually already exponential. Well, it's impossible to compute, yeah, so it is possible to compute I think extra mention. This is kind of like where error fraction is. Also do like measurements here and do error fraction. Okay, so this that's like maybe the this. Maybe the this. It was just that I want to keep k1. I mean, the total number of qubits that should be fixed. That's my k. Just that I don't want to... Yeah. Yeah? In this model, you just trace out the qubits. You don't measure and storage this class of information. No, not for our. Okay, and this is already enough implementing error. So, yeah, so I should say, so, so, yeah, so you could also like do the similar setting that you could do if you're not interested in like quantifying the size of these answers. And you could also just say that these are CPTV maps. This is another model. An example 3, this is very similar. Where you do this O1 up to OT, there's CPTP. CPTP, and you can also prepare some low entropy states. So, yeah. To deal with these here, so think about what people did to work in these settings was to use these contraction coefficients that Christophe talked about yesterday. So, the main tool for this year, the main tool The main tool is this toolbox of protection coefficients. So for example, if you take this trace line contraction coefficient of n, which was this supremum of rho and sigma that you could put in. And then you compare n of rho minus n of sigma in trace norm divided by sigma in trace norm divided by rho minus sigma. And you remember that this is like the best constant that you can then do the data processing in related. That means that this is less than this trace distance after the channel was applied can be upper bounded by the strace long contraction coefficient times the trace distance of the two states. And then of course if your channel, if your tensor power Of course, if your channel, if your tensor power has a trace of protection coefficient that is strictly less than one, then the CPTP maps, they cannot increase the trace distance. So you can feed in two different states here in the beginning, and after a certain number of time steps, the states will not be distinguishable anymore. That gives us a lot. Of course, this depends on how the So if this trace non-contraction coefficient n of tensor k is strictly less than 1, then it's impossible for arbitrary times. So, but now you come run into this problem, right? And so, now we have to somehow talk about the straissor convection coefficient of the tensor power. And as we saw yesterday, this is not so easy to compute all the time. But if you do this, yeah. We will later see how to do this, how to get these bounds. And what will pop out is that it's impossible to compute. Impossible to compute over that scratch. Any questions regarding this? I mean, this is more or less known, I guess, to many people here. So challenge somehow to do this, right, is to quantify the straissom contraction coefficient of the tensor power. Straissome contraction coefficient of the tensor power. And by the way, we know examples where this contraction coefficient for the channel n is less than one, but for some k it becomes equal to one. So where these superactivation results is related to the zero error classical capacity, where you can have these effects. You do not have this for qubits, and this is what I will talk about. Okay, can you have some? Okay, can you have superactivation in the control? It's basically the sort of the quantum independence number that can be subtle. But not for cubes. So for higher dimensional. I should also maybe point out something that Christoph didn't say yesterday, that generically this trace of contraction coefficient is less than less than. Trace of contraction coefficients less than one. So if you take this whole noise channel and you perturb it a bit, then you will always strictly contract. So this is also an old paper by Raghinski that had this argument. Okay, so now this tool of contraction coefficients seemingly breaks down when we add again our classical memory. When we add again our classical memory system. So, first of all, the first problem is that we may not know for the tensor power that contracts, but if we know, and if we add this classical memory system here, then this tool somewhere doesn't work because I can, in the first step, I can measure my system. Say I prepare either rho or sigma. I measure something and store this in the memory, depending on whether rho or sigma was prepared. And then clearly it's And then clearly, as a memory system, this will not contract, because it's a perfect classical memory. So I cannot use these. Then later onward, I can, depending on the memory, prepare something again. So there will not be this contraction. It will not happen in the general setting where I have a classic kernel. So this is the scenario that I want to talk about. So this is a the very general case. This is the very general case. So, maybe example four. Let me copy it here because I want to say how we deal with this. So, again, we have some encoding. Then we have some keys, perfect classical memory, and this kind of This kind of thing. So, what can we do when we have this memory? And the basic idea of dealing with this, so this is the case with classical memory. And the basic idea was to use entanglement measures, because in entanglement theory, we know how to deal with classical communication. So, this is like a let's start with high-level So let's start with a high level idea. Use container measures. So I have this circle and I want to somehow prove that an upper bound on the time that this memory can work. And what our idea was to just double up the circuit first. So we take the same circuit again. The same side of the game. And we feed into it, we start here with a maximum entangled coordinate state. We feed half of it into the Feed half of it into the first instance of the circuit and half of it in the second instance. So, process of a second here. So, now if we look at what's happening here, so for some people working in tenement theory, Entanglement theory, it's kind of obvious that these operations here, right, they are what we know, I mean, they are local operations. Maybe they are these classical systems flying around here. But for sure, I mean, these are operations that are clearly not, I mean, they are clearly included in the LOCC class of prototypes, right? I mean, they are It's a bit of an overstatement that they're LCC because they're actually local. But let me just say that they're LOCC. So they have local operations that they could even communicate classically between these links. And it's known that somehow LOCC operations we cannot use to generate entanglement between these two links. So I put entanglement in the beginning. And these operations, whatever they are, they cannot generate no entanglement. Generate no entanglement, and hopefully, this noise channel will somehow destroy the entanglement. In the end, when I see, look here what state we have, somehow AP head, then this might not be very rough. If the memory functions well, then there should be a tangled or pipe tangled. And in general, we're not so you can use this. So then we can use this to upper bound the computation time. So what do we need to make this some more rigorous? Yeah, but why did you need two copies of the circuit? Can I just have yes? So we want to we want but we want it to contract. So we want to yeah, so I think it will become clear maybe so if there's no noise acting on here, on this lower circuit, then I might not be sure. then I might not be sure that um and I want to really link this to how well this memory functions. We don't want to yeah I would have trouble to prove that that the entanglement well maybe I don't know like there might be different different ways. I mean this one this setting works works very nicely. Okay. So I want to quantify the So, I want to quantify the entanglement in this state here. So, quantify entanglement in this output state, rho be head t. For this, I want to define an entanglement measure. There's a recipe for how to define entanglement measures for distance measures. So, for the distance measure, measure d. D rho sigma, we consider a tenant measure d sep of rho AB, which is just the distance of the state rho AB from the set of separable states. That we speak to the same by partition. So then with this definition, so we want two things. So we want again to somehow use this contraction coefficient machinery. And I claim that we need two things for this. The first thing is that this DSEP, this should be an LCC or could also be an LO monotone, but let's say it should be an LCC monotone. Begin the CC monoton so that we can, when we quantify the entanglement in a state where these maps act on, that we can get rid of these maps by this homotonicity. This is not a very strong assumption because when you define a distance measure like this, or then a telemetry measure like this, then this we get for free from this data processing. For free from this data processing report from PP. Just by noticing that if you have an LOCC operation, you can just apply, so if you want to quantify the entainment measure in LOCC map applied to rho AB, then you just compare it to the outputs of this LOCC map are now all separable states. And they will be separable, because LOCC maps cannot generate a tenurement from separable states. And you can just use. Temple states, then you can just use the data processing quality for D to get the monotonicity of this interaction. This kind of easy. Then we need a contraction property. So this, I want to formulate it very general. So what we need is that there exists an epsilon greater than zero such that for Such that for all states rho xyab, which is a certain type of classical quantum state, so P xyx transfer y transfer rho A B x Y they are like certain C C Q Q states States. And for this type of states, I want that if I take this DSAP of the channel and tensor 2K apply to this, rho x xyab. And this should be less than 1 minus epsilon times the obtainment. the determination of rho x y a b. And these states rho x y a b they are really the states that I have here in this if I look at the states that sit here then they have a classical two classical wires. They have the X classical wire they have a Y classical classical wire classical system and the And the A classical system here, or the B classical system here. So I have to, somehow to be very general with this classical memory, I don't have to, I mean, I thought in entanglement theory you often use this monotonicity for just bipartisan states, but here I have these classical systems flying around, which is somewhat important for the argument. Okay, so these two things I need to Two things I need. The first one is kind of trivial, and the second one we have to work with. If we have these two properties, right, then we can look at the state here, rho ABT here, and we can say that so with this, we can look at D set of rho AB T hat. t hat and we can use the first property to remove the d. Then this layer of channels will give us a factor of 1 minus epsilon. Then we remove the next C map, get another layer of channels. You can see that this gives you a bound line 1 minus epsilon to the t of D set of omega AE. And again, this goes to 0, this T goes to infinity. as t goes to three t. And you can do this more qualitatively to get again that you cannot, yeah. We will see that you can use this to show that you cannot compute for longer than exponential times. Okay. Yep. Sorry, what was Orminger AB and what's in the last line after the DSEP? On the left side. Yeah, this is into the two. For the into the 2K, the first tensor. So this is the channel tensor 2K, right? I'm using it twice here. Sorry, it's missed the ID then. Sorry. Identity on the stressable system. It's a maximum intangible state that is feeling incubated. Oh. Alright. I think it makes too many letters at once. Yeah, sorry. Okay. Quick question. Yeah, quick question. Do I need to know the partitioning? Let's say can I use my qubits as a classical wire in the television? Um what do you mean? Like at some point I say I don't want to use this qubit to measure my want to encode some class property there. Yeah, you could I mean then the partitioning is not fixed, right? It's unknown that if this second property you have is a for a fixed property for a fixed partitioning Yeah, I mean I so I think for this argument I fixed the partition because I want to say something about this memory setup. So I'm not exchanging qubits between these two memory settings. But I could use qubits to store classical information. It's just that some of the classical wires are of course much better to store classical information. Because you don't get the noise. I really like meta question first. Can you repeat? Can you repeat? I think Angus asked this question just now, but like uh why why can't you like have a different system other than that is not the same as the compass system? I mean it might not contract. So this contraction property you might not get if not if you have quantum systems flying around that are not uh hit by the channel. So like if I just have a qubit that does not is not hit by the noise, then it doesn't contract. Then I can of course you can maybe still quantify the entanglement. It's just somehow how we Um it's just somehow we are within our argument, but I mean you have to have to do something slightly different. If this qubit is hit by noise, then okay, then this qubit will fail much much earlier than memory. Why do you choose to study an entanglement? Why do you choose to study an entanglement? Well, because I want to deal with this classic information here. So I cannot use the contraction coefficients directly because it doesn't contract. So this circuit here, if we look at this, if it's this perfect classical memory, then it will not contract. Also there are some channels where, I mean, like the dephasing channel where there's also not contraption. Okay. For the last like 15 minutes or maybe 20 minutes, I don't know. Yeah, I will tell you a bit like how to how to. Yeah, I will tell you a bit like how how we can how we can prove this convection property. You can actually um use different measures here. So um so So, somehow, for curious reasons, it does not seem to be possible that we use a trace distance here as this measure. Because this trace distance somehow doesn't behave well with these classical quantum states. Now we have to do a slightly different thing, and we use this chi-square divergence that Kristen already mentioned. So, which So, which distance to use so this works well with this quantum chi-square inversions, but I should say it also seems to work with the relative entropy of entanglement or the relative entropy. Just that when we when we wrote this, we didn't know the nice results of Christoph. So, um yeah. So I think now we can do this argument a bit easier. So let's remember this. So this chi-square divergence, this looked like this. You have the trace of rho minus sigma and sigma minus one half, rho minus sigma, sigma minus one half, if the support of rows containing the support is equal. And plus infinity else. It's one of these divergence measures. And yeah, this is kind of like a generalized inverse because not invertible. And then from this we get this entanglement measure which is Which is called chi-square set ABO AB it is just same as I wrote there, infigurable over sigma AB and sub AB and then it's this chi-squared of rho AB and sigma AB. So maybe first of all we can see that these properties We can see that these properties, and Chris mentioned this yesterday, the data processing equality is satisfied. So we get this first property for free. So the PPI, this implies that chi-square z is an LSCC monotone. So we can use it here in this argument. So this first property here. Curse property here, this is fine. And as I said, we want somehow the issue is that we have to deal with these classical quantum states, so it would be helpful if this measure somehow behaves nicely. And I just want to make you aware of one property of this that we need somehow. If we have these special states, special states special classical classical quantum states then we can evaluate this chi-square set of and here we have to say where do we put the specific systems we always put the x together with the a and the y together with the b of position Position shouldn't matter for the tangent. Then there's a formula that we can prove that is a bit annoying to work with, but it's I square set of law AB xy plus 1 where minus 1. And what you should take away. And what you should take away from this is that this chi-square sep is somehow quantifies entanglement in the quantum part of this. It gets not somehow, yeah, I don't know, doesn't get distracted by the classical systems flying around. And some of this, for the trace distance, we don't know a formula like this. So, some of the trace distance, maybe you can use it, but it seems that we cannot trace distance from sample somewhat. Trace systems from several, someone gets confused by these JCP systems. Okay. So now we need to prove this contraction property for this chi-squared diversions. And as I said before, I want to have very general results. So let me write you the result that we proved because don't want to use I want to use minimal requirements Requirements or minimal assumptions on the channel. This is somehow also in the programming channel. I can get better points, but this is one of our, I guess, main theorems that this If n from m two to m two is multi-unitary, multi-unitary channel, then there exists an epsilon greater than zero such that for all rho x y AB of this form, Of this fall, we have this contract consequence tensor n squared b is less than some one minus epsilon. Um okay, so so I want to point out this is free for all channels that are not unitary. For unitary channels, of course it doesn't this this measure will just stay the same by automicity, but for unitary channels, for non-unitary channels, this will always contract. Non-unitary channels, this will always contract. So we get a result for all non-unitary channels. Okay, so now in the last part, I want to somehow maybe not prove all of it, but most of it, I think, because it's actually not so difficult. What is M2? M2 is a two-dimensional matrix algebra. So yeah. It's qubit generally. So yeah. Yeah, so it doesn't work for non-qubit generals, as you can see by just tensoring identity with something else. So it's clear that this result cannot be true for arbitrary dimensions. Can you expand on that? So why? Why you cannot? True reason, because you can take a qubit identity channel and tensor it with something non-unitary that will not be a unitary. Non-unitary, that will not be a unitary channel, and but you can you have a noise-free subspace, so you can bits, you cannot. So, yeah, there might be something that you can do this for, can improve this for Qts as well. No, maybe not. Maybe you can do some errors. Yeah, yeah, yeah. Okay. Okay? Okay, so when we prove this, let me first do a lemma. And I will not prove this lemma, but I just want to point out because in our paper, we have a similar lemma that is somehow way too complicated, I think. So when I prepared this talk, I found some simplifications of it. And so what we can show. So, what we can show is that for these CCQQ states rho x rho xy AB and quantum channels in let me write a bit more generally, in A and B. So in A X on the A part, N B X on the B part. They don't have to be Part. They don't have to be qubits. They can have anything. We have that this chi-sap square of identity xy, tensor NA, tensor NB apply to rho xyab. This is actually less than the square root of the chi-square contraction coefficient of NAT. Coefficient of NA tensor NP times the chi-square sub. And in the paper we have a very complicated expression here. So I should point out that this is much simpler and the proof is also quite easy. I think we will update this at some point soon. Okay, so we have this result. So whenever So whenever this chi-square connection coefficient of the tensor power n tensor 2k is less than 1, then we are good, right? Then we have found this way. Of course, this doesn't work for all channels because they're the defacing channels where we know that this is not contractive, 3D contractive. When we prove this theorem, we need at least two cases. One case are the Like one case are the say these defacing channels, and the other case are the non-different channels. So, we will make a colour of the colour of the colour of the colonel. So let me maybe start by the more interesting of the two classes, I think. So, proof of this theorem that I want to put here. And it's maybe more of a proof section. So the first So the first thing is, let me write if this is a star. The first thing to notice is that there's a convexity trick in this. So convexity trick. And this is just the realization that if suppose that this star This star holds for some channel L prime. Maybe if we build a channel by taking convex combination of some other channel, M and P times M prime with P greater than zero, then we see that this chi-square set of this new channel Of this new channel, and it turns out to k rho xy az. This is less than 1 minus p to the 2k times epsilon, where epsilon is the value that we have for in prime, times the chi-square set of state. This is kind of easy, it's easy to see, right? Of easy to see, right? Because this channel here, this n to the tensor 2k, this will be it will be term 2 to the k n prime tensor 2k plus something else. And this measure, which I have not said, but it's actually a convex measure. So I can use convexity to take this part out, and for this part I get this contraction. For the other part, I just For the other part, I just use this less than one. Here I can get a terrible bound by this. Of course, all these bounds are terrible because they are very general. But this will be still less important. So somehow I can use this trick now tool when I study a certain channel, I can try to split off the part that I can study. And now I use two cases, so the proof finishes in two cases where I use different splittings. And the first one is that N is unital, which contains the defasing class. In this case, I can split off an intangible breaking channel and the second. And the second part is that N is non-unital, then I can split off a non-unital extreme point of the channels. Non-unital extreme channel. Let me say, maybe, yeah, okay, I said I will start. Yeah, okay. I said I would start with the second one, but let me now start with the first one. Because it's very easy to see. So first of all, I claim that these container breaking channels, they always contract the separable as an entanglement measure simply because they destroy all the entanglement. So then I have a So then I have maximal contraction, so to speak. Observation is that if n prime is entanglement breaking, and okay, entanglement breaking, this means that it breaks entanglement with any reference. So when I apply it. With any reference, so when I apply n prime to part of a system, then afterwards it will be separable, no matter the input state. Okay, and tamar breaking channels, I mean for sure they have this property that if I take the separable distance of x, y, can I say n prime 2K say xyab then since this channel completely kills the entanglement, this will just be zero, right? So this is a very strong contraction of this entanglement. So now I need to need to show that from any unit channel I can break off this antenna breaking channel and there we remember the geometry of fuel channels. The geometry of future channels, fusion fusion channels, sorry. Maybe we can even draw it. So the picture I have in mind here is that there's an octahedron of entangled breaking channels. Sitting inside the tetrahedron of all the nitrogen. This is somehow this tetrahedron sitting around the octahedron. And for instance, you can look at this fair. For instance, you can look at this facet of the tetrahedron, so say this first triangular facet here, and draw this facet, and you'll see that the corresponding facet of the octahedron really sits inside here. So it touches these boundary points here. And this means that whenever I have, so this is the integral-britten genotype, and whenever I have any point that is not a corner of this That is not a corner of this tetrahedron, okay. Then I can always write it as a convex combination of some point in the entire breaking class at some other point. So I can always split off anything with breaking shape. Always possible to split off and say what we have to channel. On the open channel. So that deals with the humility case, including the defasing channel. This is also an advantage of dealing with this entanglement measure, because as we know, the dephasing channel doesn't contract the chi-square divergence, but it contracts this entanglement. Okay. Are there any questions? This argument? The more interesting case are the signal unit two extremal channels. So, for the non-linear extreme channels, I should say that yesterday Christoph had this nice machine he was. There's a nice machine, you know, it's just a feature. And I think this drilling coefficient, this must be always one for the extreme machines. I think you cannot show that they contract. So to somewhat deal with them, even after these maximum levels, you have to deal with them somewhat differently. Okay. So we have a non-limitable channel here. Non-limitable cubic channel. Many imagined cubic channel. So we can write this as a convex combination of some channel and p times n prime for p greater than zero and n prime in extremal non-unital cubicle. Like empty framing channel. So, and let me now, so to deal with this, right, we need to show that these extremely non-interqubit channels contract this entangled measure. And what I will use there is the lemma from before. Remember that I mentioned before, this tells you that the this tells you that the chi-square set of we need to show that they contract this measure and here I can use this lemma to first go to the square root of the chi-square convection coefficient of n prime tensor k. And then it can use the results that Christophe mentioned yesterday to upper bound this chi-square contraction coefficient by the Trayson contraction coefficient. Because that's much easier to work with. So I want to show now that this stress-on contraction coefficient is always less than one. And this is actually true for all these. And this is actually true for all these uh non-unital extreme channels and all the tensor powers. Right, in what sense are they extreme power? In what sense are they extreme of Q channels? Like amputated damping. So theoretical proof is that Is that if N is a non-unital extremal cubic channel, with there be a door, track door opening, uh cubic channel, then this trace and contraction coefficient of any tensor power is strictly less than one. Is strictly less than one. And the proof of this uses, how much time do I have? Do we have now five minutes or is it a bit longer? I mean, I get five minutes, okay. So the lemma I can use for this, this here, is a general upper bound on the Trayson contraction coefficient. Upper bounding the Trayson contraction coefficient. This is actually for any channel. But you can always upper bound this by a square root of one minus the minimum eigenvalue of the join matrix of this composition divided by the dimension squared, where I for any channel from Md to Md we have this upper bound. We have this upper bunch. Here, this joint matrix is map applied to a maximum tangent state, as you know. This is the adjoint channel in the Hilbert-Schmidt sense. So, this lemma proves the result because this upper bound is actually quite a figative. So, if I take a tensor product, A tensor product and tensor k, then this is not much, but it's computed for tensor currents. I just get a kth power of this choices. Here you get a two to the k. And now you realize that for these non-unital extremer channels, the Krauss random. The cross strength is always two. So this point matrix is always a full rank operator. In other words, this minimum eigenvalue will always be strictly greater than zero, and then you can get about. So if n is extreme curvature more unital. Then, this minimum eigenvalue of this choice, of this operator, is strictly greater than this. Then you can get this code. Yeah, I don't think I have maybe not so much time to reprove this lemma. There's a nice STP trick in relation to the fidelity. Maybe I should say that for the temperature. Maybe I should say that for the amplitude damping channel you can evaluate this. And so for amplitude damping strength P this is somehow like you get that this minimum eigenvalue is greater than one half p squared. So with this you can get a bound, a terrible bound on the Terrible bound on the contraction of them. Okay, so with this, right, I mean, I can conclude. So now with this lemma, you can now, from any non-unit to a qubit channel, you can split off this extreme channel. And since this trace contracts since the traceon contracts strictly, the chi-squared divergence or contracts strictly, and then we get this this design property. His design property. And if you go through all of this machinery, then you get, I mean, you can quantify, like, I mean, you get explicit bounds. They're just somehow terrible because they're very general. I think with this, I will stop. Questions? Yeah, I mean what happens if you constrain the operations to be for example local and have one or something like that? Yeah, people have done this. So with locality assumptions, you can prove also stronger. But what we also know with that argument I guess right because like you always get rid of one or yeah, I mean maybe there's been a bit other difficult arguments. I'm not very sure. So Uma and Ela and the Neuiden And they know they have a follow-up where they do this so I'm not sure that they maybe they don't do the classical memory. So I'm not for sure. I mean I have a really general question. So like if you didn't care about classical memory but you care about a computation, like would you would this entire system fall apart because then you could use like the capacity setting is more. Um yeah, I think I I don't know how to deal with how to really do comp I mean arbitrary computer how to use the fact that there's a computation. So I d I I really don't don't know. I mean this is of course I mean this is all I mean in the sense like the memory is the easiest computation. So if you cannot even do a memory then then you cannot do any computation. But I mean of course it's a it's a very complicated thing maybe yeah. So if you actually do a computation you expect like the time combination of the thing? uh time condition of that thing. Yeah, uh well I okay it depends on what you might might do it for, but so time for instance has these has these results right that you can sometimes classically simulate after some time. I think it should be shorter, right? But but it's um yeah. But I wouldn't know how to use it. Uh so we actually have a question online lately. So Lebbi do you want to unmute yourself? Want to amuse you yourself? So, what happens when you have larger dimensions? Yeah, so for larger dimensions, you have these first of all there are these kind of trivial counter examples, right? So, if you want to store information and your quantum channel is, say, qubit identity tens or something else. And of course, I can store in the identity part. The identity part. So, this kind of you cannot, in general, here's no upper bound on the computation on the storage time if you allow any non-unitary, for all non-unitary channels. But then you could consider, for example, these extremal channels, and there we don't know. So, our bound also works for extreme points of certain Kraus rank, but not for all extreme points. Rank, but not for all extreme points if you have higher dimension. Then there are also these, yeah, maybe last last tone, then there are these superactivation results that I mentioned, where you somehow start out with something contractive, but after some tensor powers, you become non-contractive. And that would be very interesting to look at, I think. What example of vacuum you have for one type of Except for one set of things yeah, okay. I I I don't know any explicit example. Yeah, but maybe maybe there are examples. I tried quickly to find an example for the talk, but I didn't find some class. I think we could look at the quantum zero R capacity. I think we could look at the quantum zero hour capacity and then just uh said a couple of times that uh all the bounds they are not not so good because they are so general but if you assume more specific noise models uh can you then make them tighter and get some realistic numbers? I think if you if you don't use if you use the techniques that Christoph mentioned yesterday, right, and you really do the best uh like for your specific noise model. Like for your specific noise model, you want to find maybe the exact value of this JSON contraction coefficient instead of some upper bound. Then you could improve this. But yeah. Then you would have to know the noise shadow. Yeah, yeah. And in the end, I think I think okay if you if you let the dimension grow, I mean you you can do a repression, right? So so that's a fact. So so it's not it is possible to compute uh exponentially long. to compute uh exponentially long if you can uh remove them to I think it's uh yeah so so what what point do you have to bypass from your results to the operation point? No you you can do full error correction in our results. So our result just has a bad bound because we consider very general operations that would allow for error prediction. Don't allow, I mean if you make this unitary circuits, we don't have clean answers and you cannot do it. We don't have clean answers, and you cannot do any error correction, and your bounce get better. So, more restrictive. Thank you. Maybe one of the common questions that this is like