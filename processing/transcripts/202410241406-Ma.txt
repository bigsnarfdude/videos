My name is Will from Columbia Business School. I'd like to thank the organizers for having me here. I guess typically attend OR conferences and that's why I apologize for being here late and missing a lot of your talks. We had our operations research annual conference right before this. Generally, I work in online algorithms, both the theory, proving things like competitive ratios and approximation ratios, but also their applications, in particular in e-commerce with e-commerce companies. Commerce with e-commerce companies. So, this naturally leads itself into trying to measure like the sample complexity of learning different online policies. So, generally, learning theory is fascinating to me. So, I'll present some stuff about learning online policies, and this is based on two papers that are joint works with my fantastic collaborators. Okay, so this is a talk about sequential decision making. You can think about a T-Horizon. A T horizon reward collection problem given some initial state S1. You know, classically, when you see something like this, you probably think of the hammer of like reinforcement learning. You take your favorite algorithm based on policy optimization or Q learning. So for subclasses of this very broad class of problems, you can have what's called perfect hindsight evaluation, which means your Which means your reward and your next state from any given state and action, if you just know some kind of underlying realizations, CT, then it's assumed that you can sort of perfectly evaluate the counterfactual of what would have happened under any alternate set of actions if you know the function f and you assume that you observe these underlying CTs for each trajectory. For each trajectory. So, in the problem where you can do this, and I'll give you several examples, you know, you can think about this like a dynamic pricing problem. If you know everybody's valuation, then you could actually replay what would have happened under any sequence of prices you would have offered. And when you do this, you can actually use the Hammer supervised learning now. You don't need to have these intermediate states or try to do some policy gradient that stays close to the existing past trajectory. To the existing past trajectories. You can basically just evaluate the loss, which you can think about as the negative sum of rewards of any policy on any trajectory C. And then you can just try to find a pi with low average loss over your past trajectory. And this is an idea. It's not particularly new. It has its roots in like financial engineering of backtesting policies on the stock market dating back to the 90s. But recently, I'd say it's. To the 90s, but recently I'd say it's really taken off in OR at least. People are using it for inventory management, revenue management. Amazon is using it for their inventory management, claiming it does much better than the previous approaches and comparing it to reinforcement approaches. And also queuing. And so in this talk, I'm going to try to give some kind of theoretical analysis of supervised learning for sequential decision making. Decision making. And I'm in particular going to focus on inventory management and revenue management, which is just my way of saying dynamic pricing of limited inventory of problems like this. Okay, so that's the backdrop for this talk. So I'll just tell you now the learning theory setup. I guess this is probably very standard to people here. So this is the implicit loss of a policy pie, an online policy pi, on some trajectory. Trajectory. In all the examples I give, C is just going to be a trajectory of length t, where the unknown at each time is just a real number between 0 and 1. So this is implicit. It's defined as the outcome of a bunch of state transitions and actions of what your policy does. There's going to be some unknown distribution from which, yes, do you lose anything if you go to the Supervisor next segment? Oh, okay. You mean? Oh, okay. You mean in these practical examples? Yes, you have to make some assumptions. Like, I have to assume that, you know, I look at however long my inventory arrived, like how many days delay my inventory arrived when I ordered. And I need to assume that if I ordered three times as much inventory counterfactually, it would have still arrived just as fast. Which you could argue isn't a perfect assumption, but I have to make assumptions of this form that my evaluation. I just want to know whether, you know, should we think? Should we think still in the sequential decision-making setup, or just go to the supervisor and just take all the results there? Yeah, so that's what I'm basically going to do, but I'm going to direct results that are specialized for these problems. So that's theoretically what I'm going to do. I'm saying in practice, you do lose something because the perfect hindsight is not a perfect assumption. But it's been shown that at least in this new way of work, waving my hands a bit, roughly speaking, it performs pretty well. Roughly speaking, it performs this supervisible approach. Okay. Good. So there's some unknown distribution from trajectories. So I'm going to let L d of pi denote the true loss of pi under this unknown distribution. I'm going to let pi star denote the Bayes optimal policy, the best I could have done if I knew this distribution over trajectories. So then I'm going to assume I have n IID trajectories that are drawn from this unknown distribution. I can define empirical risk. I can define empirical risk minimization. I'm going to let pi ERM denote the empirical risk minimizer that minimizes the average loss over these n pass trajectories. What's your set of minimal according to? Good. I'm going to clarify that soon. Right now, just think about it as an arbitrary policy. An arbitrary, yeah. I'll give examples. Now, okay, in this setup, there is two different types of empirical risk minimizing. Different types of empirical risk minimization. There's the standard empirical risk minimization, and then there's what's called product empirical risk minimization, which is if I assume that my random shocks are independent across time. So then, the fact that, so then I can basically, you know, create a marginal distribution for ct at each t, and that I can effectively expand my n trajectories of length t into n to the t sample. n to the t samples, I basically just take the empirical distribution for each time t, which has n samples, and then I draw randomly the coordinate for each one, and I can try to minimize the average loss over these, which is often computationally easier if I'm going to do some kind of dynamic programming based on the empirical distributions. So this is a distinction I'm going to make, and I'm going to let pi perm denote the policy that minimizes the product empirical risk. Is the product empirical risk. Okay, so now I'll give you some examples of the two main problems I'm going to study. I'm going to study single-item post-it pricing in one paper and inventory replenishment in the other paper, both of which fall under this framework. So let me define the problems. I'm in single-item post-it pricing. I have a single item I'm trying to sell. Each time I decide a price to sell, and then I observe this valuation, that's a priority unknown in this 0-1 range. Range. If the valuation exceeds the price, then the customer buys the item. I stop. There's two possible objectives. Under welfare objective, I basically collect the valuation of the customer. So I'm just trying to make the person with the highest valuation buy the item. Because it's a loss, technically I have to put a one minus, but you can think about this as maximizing the valuation of which customer got it. There's also the red. There's also the revenue objective, which is when the item is bought, I collect the price PT that the customer paid. Okay, so two variants of this problem. Inventory replenishment is another problem where I'm trying to make decisions over a time horizon of length t with unknown CTs. In this case, this is a classical OR problem where I have an initial stockpile, and then each time I get to immediately replenish my stock. Replenish my stock up to any level that's at least my current inventory level, IT. And then the demand occurs. This is the total amount that's purchased at this. And then my inventory updating is simplified. The starting inventory at time t plus one is just the level that I stock to minus the demand. And then it can't be negative, so I take the maximum of the distance. And my objective is basically to match supply with demand. So the loss of. Supply with demand. So the loss will be measured as the average over the time of how much my stock differed from the customer demand that came. You can handle more general what are called news under loss functions. I won't get into the details of that. Okay, so these are the two examples I'm going to study. So what are the goals of the learning algorithm? So okay, this will answer Simon's question. So okay, so there's two possible goals. One is I can learn the best policy in some class. Class. And so basically, I want the loss of my algorithm minus the best possible loss within some class to be less than or equal to some epsilon. You can think of the pack setting where I want this to happen with probability 1 minus delta, and I want this to be true for all distributions d, arbitrary distributions over sequences of length t. I can alternatively assume. Alternatively, assume that the distributions are independent, which is what a lot of past works have done, and try to learn the optimal policy. So here I'm just comparing with the Bayes optimal policy, but I'm restricting D to be a product of independent marginal distributions of zero and D. Okay, so if my policy class contains Policy class contains all plausible optimal policies for product distributions, then the second goal is strictly easier than the first. And for both of these problems, there will be a natural policy class that satisfies this. So let me explain this now. In single-item posted pricing, if you know the values are independent, you can just do backwards dynamic programming and apply that. And then you can get these static descending thresholds that don't depend on observations. And it just says at time t, this is the price you should charge. You should charge. In inventory replenishment, there's also a class of policies that are defined by 01 to the t. Basically, at each time t, depending on the demand distribution for that time, you set what's called the base stock level S. And the policy is simply, if my inventory is above that, I can't do anything. If my inventory is below that, I've replenished my inventory up to that level. And one can also show using dynamic programming that if the distributions are independent, If the distributions are independent, then this class of policies, their optimal policy, is always defined by a sequence of T non-adaptive base stocking levels. I should say that in general, if you don't restrict the policy class and you allow for arbitrary distributions, it's impossible because you can have a distribution where the first realization is some secret encoding of the entire future of the sequence, a Bayes optimal policy. A Bayes optimal policy would just sort of be able to read the future, but you can kind of never write thatself. Good. Okay, so what are the results? So first, I'll tell you what are the results you would get for the posted pricing problem using fairly general hammers. So you can use a pseudo-dimension and get this type of sample complexity guarantee. So I'm ignoring log 1 over delta terms. Log 1 over delta terms. And if you have product distributions, then you can also use this paper by Guo, Huang, Tang, and Zheng to get some kind of sample complexity guarantee. But both of these, as you can see, I should say both of these are general purpose hammers. They work for general problems, and you end up with a sample complexity that is like t over epsilon squared. So it grows with the length of the horizon. With the length of the horizon. Okay, so what we derive is, so first, so I'll tell you about some of the new results. So we show that for the welfare objective, even in the special class of problems, for the harder problem of learning best-in-class policies over general distributions, you have to lose the factor of t. For revenue objectives, For revenue objective, even in the easier problem, you have to lose the factor of t and then for the welfare objective and for the easier problem, you actually don't need to lose the factor of t, where here for this positive result, the algorithm is product empirical risk minimization. Okay, so I'll tell you about the results for inventory replenishment. Repunishment. There are some past works that show that, once again, even for the easier problem, even for the easier problem, you've got to lose this factor of t. You can also get this result from this hammer. And then we show that actually here, even for the harder problem, you can actually, you don't have to lose the factor of t. And your algorithm can be either ERM or PERL. Okay, so this is related to. So there's several related literatures, see the problems I'm studying. There's classical like sample-based inventory replenishments. A lot of these papers, they have their origins in sample-based auctions. And then there's this area of data-driven algorithm design, which is basically studying like parameter tuning for policies when you're given samples of the unknowns. the unknowns. And so I'm gonna now elaborate on some of I'm gonna now elaborate on these two results and try to explain the difference between revenue and welfare markets. Okay, that was my, yeah. Somehow those are fundamentally different from like a sample complexity in the online setting. Because in the offline setting, I'm not sure. Anyway, okay. I'll let you explain that better. Okay. Good. Okay, so I'll try to explain now the difference of this, and then time permitting, I'll also explain. The difference with this, and then time permitting, I'll also explain about inventory. Yeah, yeah. So, the one by the wall on and hello, yeah, they look at options that you can put amount of. Yes, yeah, yeah. But you can also apply the result to this or sequential post-it pricing. It doesn't have to be. Pardon? Forever running do that? So, so here, like, it's sequential. So, I'm not allowed to run an auction. I have to post a price for each person. But their result will also apply to sequential. Will also apply to sequential post-it pricing. I believe. Yeah. I think that my question is: how are you applying the result? Like, you do do that for every realm of your monogenicity. I think you can show strong monogenicity, and then you get to your realm. Okay, good. So let me quickly sketch this lower bound from revenue objective. Sorry, this is. Sorry, this is a type of LGBT over epsilon squared. So essentially, the construction is for each customer, they add roughly 1 over t to the value to go. So you only have one item, and you can think about the expected value to collect from a point onward. For each person, they add roughly 1 over t to the value to go, but you don't know what the optimal price is. And basically, you have these constant values, but the supports are either like 1 plus minus epsilon. like one plus minus epsilon, they differ by one plus minus epsilon over t. And basically the distributions differ in the optimal prices. And if you make a mistake, you pay a cost of roughly 1 over t. So basically, so this isn't particularly difficult. Essentially, the mistakes do accumulate in a non-linear fashion. If you make some mistakes on later agents, it reduces the impact of mistakes on earlier agents. Active mistakes on earlier agents. But nonetheless, you can analyze the dynamic program and show that if you make n mistakes, you kind of have to lose this much in total. So therefore, you have to avoid making a constant fraction of mistakes. And then in order to avoid making a constant fraction of mistakes, when you're trying to distinguish between one plus minus epsilon over t probabilities, you need t over epsilon squared samples. This is a fairly standard like Heleneer distance type analysis. And then, so therefore, Analysis and then, so therefore, this is what you have to do. So, the key reason why you can't make this type of counterexample for welfare optimization is because for welfare, the only uncertainty is in the future. If it's like in the present, I don't have to, there's no question about whether to accept or not if I know the future value to get. So, now I'll sketch our argument about the constant sample complexity for welfare. And okay, so I'm going to let v star t denote the expected value obtainable if we haven't stopped at time t under the optimal distribution and the optimal policy. So this just satisfies a standard dynamic programming recursion. So v star t plus 1 is 0. v star t is just the maximum of whether I should stop. So the threshold for stopping at time t should just be equal to should just be equal to the value to go at time t plus 1. So I just set my threshold at time t to be v star t plus 1, and then v star t is just equal to the expected max of these two. You can also think about it like this. The gain at time t is basically the expected positive part of the random value you draw minus the value to go at t plus 1. Okay, so I'm going to let d hat t denote the product empirical distribution for ct. You can define distribution for Ct, you can define V hat t's correspondingly, and then V term T will just define, will just be equal to the value to go at time t plus 1. Okay, so the key quantity that we're going to bound by is this. You should think about A to T. So this is based on the product empirical risk minimization solution, V hat t plus 1. And it's basically saying how much better these surpluses are under the empirical Surpluses are under the empirical distribution than the true distribution. And the key lemma is that, okay, so nothing's happened from here to here. All we've done is, so what we care about is v star one minus v1, because v star one is the, sorry, I'm okay. V star one is the optimal value to go of the optimal policy. V1 is the, is the value to go. Is the value to go from time one of the algorithms policy? So we can add and subtract v hat, and we're going to show that each of these differences, so this is the difference we care about, and we're going to show that each of these differences is upper bounded by the maximum deviation of the sum of these entities. Okay, so I'm going to now sketch the proof of one of these terms, of this one, and the And the argument is quite elementary. So I'm going to try to bound v hat1 minus v1. And the argument is just I'm going to let u be the first index where the estimated value to go based on the empirical distributions is less than or equal to the true value to go of the empirical optimal policy under the true distributions. And basically, this is just some algebra where v hat one. Where v hat one is, I'm just summing up the empirical terms over the first few time steps. And then this is just saying at time u the value to go if I reach that. Okay, so when I'm evaluating v1, the only difference is that whether I stop isn't based on the true value to go, it's based on the empirical value to go, v hat t plus. value to go v hat t plus 5. The key thing is by definition of u, I know that the vu plus 1 is greater than or equal to the v hat, and then this one is less than or equal to. So basically, once you use these inequalities, everything just collapses, and you end up seeing that v hat1 minus v1 is less than or equal to the max of this. And essentially, this is a Martin Gali. Essentially, this is a Martin Guill. You have to reverse it, but one can show that you can do that because conditional on v hat t plus one, the expectation of eta t is zero. So essentially to bound this. Now, if you just use that, this isn't going to be bounded. You're not going to get a bound that's independent of t. But you can also bound the total variance. Bound to total variance essentially because this sum is actually equal to the sum of the differences between the v hats, and this is just v hat one. And then, so basically, you have to apply a Bernstein version of a Martin Guille concentration inequality, Friedman's inequality in particular, to get this. Okay, so that was the sketch of these two results and why welfare differs from Differs from revenue. I think, in the interest of time, okay, I'll just talk about how to prove this result. So, the thing that I find interesting here is what we showed up here is essentially that, so both of these results depend on uniform convergence. And essentially, what we showed here is you can't use uniform convergence. You have to actually analyze carefully the dynamic program and sort of leverage. And sort of leverage specifically the independence in order to get a sample complexity result that doesn't depend on t. If you use uniform convergence, you'll lose a factor of t. But for the inventory replenishment problem, what we showed is actually the previous works were analyzing the dynamic program in a fairly elementary way, similar to the argument I showed you. But actually, we showed that if you analyze uniform convergence, you can actually get a better result. So essentially, Get a better result. So essentially, when you analyze uniform convergence, you have to use these powerful tools like VC dimension, but you sort of lose the structure of the independence. But somehow, and I don't have a good understanding of this, somehow for the post-it pricing problem, this was a bad thing. But for the inventory republishment problem, you actually get a better result by analyzing the hardware problem. And so I'm not going to go into the details, but I'll just say the steps are essentially, yeah, we're using uniform convergence. It's actually not hard to show that the expected Hard to show that the expected sort of generalization error for ERM just upper bounds the one for PRM. So it doesn't really matter that much which one you analyze. At the end of the day, we actually show doing pseudo-dimension doesn't work, so you have to do a fat-shattering dimension. And finally, instead of analyzing the loss, we have to analyze the inventory level. So yeah, so we use a bunch of learning theory. So we use a bunch of learning theory hammers, but at the end it reduces to sort of analyzing the behavior of trajectories of inventory policies, of these base stock policies on different sequences of demands. And it boils down to showing that the gamma shattering dimension of this is big over one over gamma. Okay, so three different problems. Three different problems. So the first set of results is from this paper. The second set of results is from this paper on inventory. So I like this because it actually shows all three results are possible for these three sequential decision-making problems over a time horizon of length t. So basically for inventory replenishment, even the harder problem has a sample complexity that doesn't depend on t. You get it through uniform convergence. For the welfare objective, For the welfare objective, for the harder problem, we show that you have to lose a factor of t. But for the easier problem, by analyzing the dp and not relying on uniform convergence, you get a sample complexity that doesn't depend on t. And then for the revenue objective, we show that even for the easier problem, you have to lose a factor of t. So now, in terms of coming back to the sort of practice thing, so we are actually working on switching their current. On switching their current reinforcement learning to a more supervised learning approach. We're working on this with Ali Baba. In general, I'm very excited about this direction in both theory and practice, although I should say there's definitely a main disconnect between the theory analyzed here and what people care about when implementing these inventory things in practice, which is, you know, we're doing a very classical VC theory thing where the thing you care about is like the hypothesis class and, you know. Hypothesis class, and you know, what's the parameters? Whereas in reality, they're just using deep neural networks. And the thing that matters is how you do the differentiation to do the gradient descent. And no one that we talk to in practice cares about like what is the number of parameters in the policy class and what is the VC damage or whatever. But nonetheless, I'm excited about this because I view it as at least a first step of using very classical learning theory tools to analyze sort of what's going on with this. Analyze sort of what's going on with this switch from RL to supervisor in practice. So, thank you very much. Yeah, that's your question. I'm trying to think about this compared again to the offline setting. Is this somehow saying that, like, if I've got, if I'm going to like be selling T IID bidders, pick a price that will distinguish for welfare, only takes a number of bids that's independent of T or takes a number of samples that's independent of T. Of t or takes a number of samples that's independent of t, but to pick the right price in this offline setting to maximize revenue, I have to like see order t many samples. Yes, yeah, or the certainty. I'm just trying to think about the offline sample. Yes, so just they're not, they're independent but not okay. Okay, yeah, yeah. Yes, so sorry, what is the result in the result for the author? What is the result for the auction center? It's t over epsilon squared as well, right? Yeah. This is additive. So here we're doing additive, we're not doing multiplicative, and we're assuming 0, 1 value valuations. Yeah. Let me think about it. Okay, okay, yeah, yeah, but yeah. We're assuming independent, non-identical. But yeah, we definitely need N for revenue. Yeah, yeah, yeah. So here we're showing even for the posted pricing, you need to your epsilon squared. A punitive of epsilon squared, but for welfare, you don't need, it doesn't grow with a number of because you only have one item, this is important. Like, if you had a bunch of items, it would clearly also grow with a number of widgets. Because, yeah, but the key is you only have one item to give away. So, yeah. I was just saying, in auctions, it's just like offline you don't even need to. Okay, okay. But for like that. Okay, but if you do revenue, you have to do learning. Yeah, you can say welfare, like oh yeah, yeah, sure, sure, sure. Welfare is trivial. Yeah, welfare is trivial, sure, yeah, yeah. But I think, yeah, for revenue for offline, I think it's like geo website, by the way. Yes. For the non-I. I didn't get the I. Oh, sorry, sorry. Different I. Oh, sorry, sorry. Yeah, yeah, not. Yeah, yeah, yeah. We have everything, so I have to do something.   Okay, so I'm going to talk about interactive learning with discriminative feature feedback. This is joint work with Chandrow and several students, Nick Roberts, Khan Chade, my colleague student, and Regina Ross. Yes. So, um, general question is: what can we say about learning with rich feedback? About learning with rich feedback. So, we have many different learning settings that all essentially assume the same type of feedback. So, examples and labels. We have standard learning, we have online learning, we have active learning, but they all just use examples and labels. And of course, we know that you can have many different types of feedback. Bailey just mentioned asking people about their idea about different scores, and they can explain why they think this score is wrong, etc. So, this is really what. So, this is really what we're asking here: what can be done with actual feedback? And specifically, we're going to talk about learning with explanations. So, it's important because explanations is something that's used quite a lot in research. I want to make the difference between explainable AI and learning with explanations. So, explainable AI is when we want to provide explanations about a model to humans. So, we have maybe a neural network, it's very complicated, we want to explain to people. We want to explain to you people why it's doing this in a specific case, etc. So, this is not what I'm going to talk about. Now, I'm going to talk about learning with explanation, which means that we want to use explanations from humans to improve the learning of models. Okay, so again, similar to what Bailey mentioned, exactly kind of suitable here, is you want to learn, but you want to be able to exploit more than just examples and labels. You want to be able to exploit the fact that people can explain. To exploit the fact that people can explain to you why something has a circular. So, to explain kind of the setting, let's start with a very kind of toy example. And I'll start with an example where we only have examples and labels, and then let's see what happens when we add explanations to labels. So, let's think of it a very simple case. Toy example, we have this kind of digital assistant that helps schedule tasks, and we have a user. The user asks for different tasks to be For different tasks to be scheduled, and the assistant needs to decide when to schedule these tasks. So, our user asks to schedule a jogging session, and the assistant decides to schedule it in the morning, and in this case, the user is happy about it. Then the user asks to schedule a weightlifting session, and our assistant is also trying to schedule it in the morning, but now our user says, no, actually, I prefer this to be in the evening. Okay. And then the next session, our user asks to schedule a biking session. User asks to schedule a biking session, and now, okay, our student is pretty confused. Should it be in the morning or in the evening? Well, let's go with the evening. But again, this is a mistake. The user actually wanted to schedule into the morning. All right, so this is a very simple toy example, but the main point to make here is that really, our user knows why they prefer something, but they cannot kind of give this information to the assistant. The assistant has no way of using this information. And of course, it could learn. And of course, it could learn how to schedule tasks, but it would need a lot of examples. After many, many examples, it would be able to completely find out which properties of these tasks are relevant, but it will take much more than maybe we would need if we had explanations. So let's see an example where we do have explanations in this example. So same interaction, it starts the same. Jogging is scheduled through the morning, then weightlifting again. Then weightlifting again is scheduled. The user asks to schedule weightlifting. But now, let's say that our assistant doesn't just schedule the session, it also provides some sort of information that would help our user understand why they scheduled it to serve. So our digital assistant says, I'm scheduling to the morning like jogging. So in a sense, our assistant says to the user, I think what you're asking is similar to jogging. I know you asked jogging to be in the morning. I know you asked dogging to be in the morning, so you probably want this one to be in the morning. Now, this gives our user an opportunity to actually explain the mistake. So, our user says, I want to schedule it to the evening because this is inroot. Okay, so now this is a very useful information for an assistant. The reason why weightlifting should be in the evening is because this user prefers, maybe in this case, in interactivity, to be in the evening. And then, of course, this is very helpful because then when our user Because then, when our user asks to schedule a biking session, our assistant can tell that, well, it doesn't, it should compare more to jogging and not weightlifting because it's outdoors lecture. So again, this is a toy example, but it does show the protocol that we want to be able to formalize and study. And what this very short example shows, again, is that using explanations, we can, in principle, speed up learning, but again, we want to formalize it and see how far it can take us. So let's now define a general protocol. So we have a learning algorithm and we have a teacher, which is essentially a user, a human, who can provide the labels and explanations. So here's the protocol. At every time step t, an example xt arrives. Our algorithm predicts the label that's like t hat, and it also provides an explanation. The form of the explanation is some previous example, previous observed example, that had the same name. That had the same name. So, this is the explanation. Now, the teacher, if everything is good and this is a correct prediction.