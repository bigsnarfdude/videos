My talk is about factorization of general n by n matrices. When we have a given matrix and in the vicinity of this matrix, we can, for arbitrary small epsilon, we can select some matrix which we can factorize explicitly despite of partial indices of this matrix, of this given matrix. Although we don't claim that the Although we don't claim that we keep this factorization indices, I will explain this later. So, so far, before we have done this for two by two matrices, and in this talk, I am going to generalize this for arbitrary dimension matrices. So, this is the definition of Wiener-Hoff factorization, which probably everybody knows here. And I would like just to compare spectral factorization, which I have been. I have been working for a long time, for decades before. And in spectral factorization, we have a positive definite matrix, and this factorization has such form that these two matrices are Hermitian conjugate of each other, and there is no partial indexes. Partial indexes are zero. And for decades, we have been developing such algorithms for spectra factorization, which nowadays is called Janashi-Lagulova method of matrix spectral factorization. And this was Spectral factorization. And this was my first talk when I joined this group five years ago on the Cambridge conference. I will briefly describe this Janashiel Laguelova method very briefly, just in one page. That method first produces the lower-upper factorization of the given matrix with factorized diagonal entries. So we assume that the scalar factorization is formed. Factorization is formula exists and it's possible also to do numerically. So, we do not claim that there is any new step here in scalar factorization, but for metrics, there is significant improvement that first we do this type of factorization, and these are already spectral factors of corresponding determinants. And these are of diagonal entries, they are not analytic, and by multiplying By multiplying on the corresponding unitary matrix, we produce this spectral factor. So, by multiplying all these unitary matrix, these non-analytic entries of diagonal entries become analytic. And this is done step by step. First, two by two matrixes became analytic, then three by three, four by four, and so on. And main computational step is this: that if you have any such matrix where this is analytic, analytic function, and these are anti-analytic. These are anti-analytic. They have just minus terms in classical complex analysis terms, not in it as it was in before. We find such unitary matrix of this special form. All these entries are analytic and just the last row is anti-analytic, minus terms, such that this product becomes analytic. And this is unitary matrix with determinant one. Determinant one is also important because here determinant is already factorized and we do not touch any more determinant and we produce this factorization. Produce this factorization. And classical computer, in computational terms, we approximate all these functions, cutting the tails of Fourier coefficients, negative indexed Fourier coefficients. And if you keep just n negative coefficients, then this internal matrix consists of polynomials of same order, and this product becomes analytic. And step by step, we make approximation, and we produce finally this internal matrix U. Unitary matrix U. And now we tried to contribute and to generalize this method for general Wayne-Hoff factorization when matrix is not anymore positive definite. And we achieved results which was published here. And actually, what I'm going to tell now is just a direct generalization of this method. And mostly I will describe it for two by two matrices. And then I will explain what kind of generalization. Explain what kind of generalization we have: that having a general two by two matrix, which is not anymore positive definite, we assume again that whenever we encounter scalar functions, we can produce Wienerhof factorization of such functions. Namely, we assume that this function and determinant are factorizable. And if this factorization exists, then this instead of lower, upper factorization, we can factorize. Factorization: we can factorize in this way: that this is instead of L L star factorization, we have L D U so lower, then diagonal, and then upper factorization. And here again, determinant is this is analytic and this is anti-analytic here. And these are not partial indexes, but we should produce partial indices later. And what we do again here, we cut tails of these functions. Of these functions, this function instead of all Fourier coefficients, we take just n negative Fourier coefficients, and here just n positive Fourier coefficients for this function. So, this is approximation of this function. And then we factorize this function explicitly. In explicit factorization, in explicit factorization, we act like this. We act like this: that I will copy this here. So we have this matrix which we have to factorize explicitly. And we split this into positive and negative parts separately. And we have just n minus index negative coefficients here, and here n positive index coefficients. And this is plus type function, this is minus type function, and we need. This is minus type function, and we need just to factorize this middle term. And here we multiply this matrix by unitary matrix, which becomes this, which makes this analytic by Janashiel Laguilova method, and multiply back conjugate of this unitary matrix. And this product does not change by this. And this term becomes now analytic, and this term becomes anti-analytic. And so the problem is reduced. And so the problem is reduced to the factorization of this middle term. And this middle term has the property that the determinant of this matrix is monomial. This is polynomial matrix, Lorentz polynomial in this case, and which can be multiplied by T to power M and become a usual polynomial. And it has a monomial matrix. And finally, we contributed to in the In the paper which I showed, we used some existing algorithms for factorization of such matrices, namely Jobateriev's algorithms we have used. But in the end, we developed our own algorithm for such factorization of such matrices. And this can be described in few words: that I will emphasize the existence of such new algorithms. The existence of such new algorithms that we start with this trivial factorization of this metrics and exploiting the condition that this determinant has just unique zero in the origin and it has multiplicity m, but step by step we reduce the multiplicity of this determinant to zero, and then we have a Wienerhof factorization of such matrix. Of such metrics. So, okay. So, if we discover, yes, as I told, we will do this step by step. Then this order of this matrix will be increased, and the degree of the determinant of this matrix will be reduced step by step. And if we find that we observe this matrix, and if in the zero we have a zero column, then directly we multiply by t and this we will be this will be increased by one. If this is zero here, then this will be increased by one. But if none of them is zero, anyway, we have just zero determinant in the origin, as I told, and therefore we can they should be independent. They should be independent. These two columns of G0 matrix should be dependent, and therefore, there should be some dependence between them, either this or this. And in the end, we can multiply by this matrix or by this matrix in such a way that make one of the column of G0 matrix zero. And we choose either this matrix or this matrix in such a way that this will be depending whether M1 is more or less. Depending whether M1 is more or M2 is more. So, in the end, when we move this constant matrix from the left to the right, this should be negative power, and we can achieve this. And this idea is generalized for any dimension also. And finally, we make one of the columns of this G N matrix at zero exactly equal to zero, and then we increase its power. So, when we move this matrix from the left to the right, This matrix from the left to the right, this will be negative minus type polynomial, and this will become minus type. And the degree of determinant of this matrix will be reduced by one. And to summarize, briefly, to summarize, so from this step, from this step where determinant is T to power K, so if one of the column is zero of this at the origin, then At the origin, then we multiply by t to the power and this will be increased by one. And if it is not none of the columns is zero, we multiply say by this matrix and this will become minus type matrix. And the determinant of this will be increased, will remain the same, but degree of this determinant will be increased by one. Increased by one. And this procedure will be stopped whenever we get that the determinant will be not without zero in the origin. So this will be just it to power zero. And this will be final, we will arrive final Wienerhof factorization of this matrix G. Yes, and this actually everything, this is a signal example of illustration of application of this algorithm. Application of this algorithm, for example, from this matrix by simply producing the steps which I described, we will reduce, we will arrive to this factorization. And here I will describe some problems which might arise that if epsilon is in practical applications, if epsilon is too small, question arise that whether this epsilon appears from round off errors or it's the it's related with It's related with the factorization itself. So we can say that if epsilon is too small, we can neglect this epsilon and we can change and introduce the concept of regularized factorization. We can neglect these zeros and declare that we have changed partial indexes. We do not produce exact factorization in this case. But pay attention that when epsilon But pay attention that when epsilon is too small, these factors are too large here. So we should we can control during our process, we can control growth of factors and we can change a little bit partial indexes and we might not arrive to exact solution and exact Binerhov factorization of original matrix. But if this matrix arrives appears from application, we can A pair from application, we can say that anyway, it's not exact, and we do not need exact factorization of such matrix. But, however, we will produce factorization with bounded factors, whenever factors have bounded norms, and we call such factorization regularized factorization. Probably Gennady will explain this better in his talk, also. And our final step is the generalization of Final step is the generalization of this method for n by n matrices. That initial steps are the same, that we produce instead of L L star factorization, we produce L D L L D U factorization. So this is on the diagonal determinant also here is factorized. This is plus, determinant is plus and determinant is minus here. And these diagonal entries appear. And such factorization is possible in if. Is possible in if you follow usual steps of such factorization for numerical matrices, we can produce this factorization and then we cut also tails here and here and we produce such matrix and we approximate this. We call this SN matrix and we can produce exact factorization of this matrix by the same method. That we can multiply this by unitary matrix. matrix and this becomes plus and this becomes minus type and in the middle term should be factorized and middle term also has determinant because they have determinant one and this is a determinant one and it has a determinant monomial of this type and eventually we produce the factorization of such sorry factorization of this matrix Factorization of this matrix, factorization of this type matrix, and idea is the same as it was for two by two matrices. That if we have such constant matrix and such diagonal matrix, and we need to move from the left to the right such matrix, if we are assured that this is always small than this, then this will be minus type matrix. So, this is constant matrix, some constant matrix, and diagonal entry does not change, and we will have this type of. not change and we will have this type of equation and we can always always achieve this that if we select maximum maximum column so if we column independently of how these indexes here are ordered I don't assume here that they are in decreasing or in increasing order they might be in any order and we can select this dependent independent they are This dependent independent, they are dependent since it should be in this vector. These columns are dependent, and we can choose such order that select maximum column with maximum index. And then when we represent this column as a linear combination of other columns, then such matrix can be moved from the left to the right that it will produce negative powers here. And this is all actually what I wanted to tell you. Actually, what I wanted to tell you. Thank you very much.