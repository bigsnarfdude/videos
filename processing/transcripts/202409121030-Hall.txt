We are talking about Google FileNets and the differential folks. Okay, thank you very much for the invitation. It's a pleasure to be here in person after being in one virtual meeting in band. And I want to mention some results of mine that will be in the second half of the talk, which are joint work with Ching Wei Ho, who may be on Zoom, and Yonas Yalavi and Zakhar Kabuchko from Munster. And I also want to point out my I also want to point out my website. It's here, sites.md.edu. You can just Google me, you can find my website. And if you should happen to desire the slides for my talk, they can be obtained there. And I'll just mention there are some animations. And if you want the animations to display correctly, you need to use the Adobe Acrobat thing, not like the Apple Viewer app. The rest of the slides will display correctly, but the animations need real Adobe. Okay? So, I'm going to So I'm going to ask, am I standing in a good place, by the way? Not for me. I think you have a screen that you can look up. But I'm going to need access to the laptop, so is this okay here? That's fine. Okay, so if you change a polynomial in some way, you can ask the basic question: if I change the polynomial in some way, how do the roots of the polynomial change? And the examples I'm interested in, in this talk, there are. Interested in. In this talk, there are others that we have studied, but the heat flow and repeated differentiation. This means you just differentiate until the polynomial becomes constant. And in each of these two kinds of flows, if you will, I'm going to consider two cases, real roots and complex roots. So it'll be four parts at the top. And in all of these cases, all four of these cases, we're going to find a connection to random matrix theory. And we're also Theory. And we're also going to find some partial differential equation that hopefully governs, which hopefully sort of answers the question of how the roots move. Okay, so just as a warm-up example, I'm giving you here a polynomial, the roots of a polynomial of degree 100. So there should be 100 dots here on the screen. And I'm just going to differentiate the polynomial until it becomes constant. Basically, there are no more. Constants and basically, there are no more roots. And we just let's see what happens, okay? Nowadays, every theorem has to have a simulation for me. So I like to see how it looks, okay? So what can we learn from this simulation? Well, one thing that's maybe not really not obvious from the beginning is that the roots sort of seem like they're moving continuously. I mean, of course it's not. It's a discrete thing, right? You take one derivative of another, the roots move a finite amount. Derivative of another, the roots move a finite amount. But when the nut degree is large, they seem to be moving continuously along some curves. And the goal would be: can we find those curves? How do they move? Okay, that's basically the question. So we'll come back and try to answer that question later. But I'm actually going to start in terms of the details with the heat flow and the real-rooted case. Okay, so let's start by the heat. What is the heat flow? So let's start by the heat. What is the heat flow or the heat operator? I use those terms more or less interchangeably. So I'm going to take the second derivative operator. It's like holomorphic. P of Z is a holomorphic function of Z, just polynomial of one complex variable, taking the second derivative. And then I'm going to exponentiate the second derivative operator. And I define it as a power series. And if you apply it to a polynomial, that power series will terminate. And therefore, that means that this formula. And therefore, that means that this formula makes sense for any tau in C. At the moment, there's nothing special about taking tau to be real and positive. Take any tau in C. Now, on the other hand, if tau is real and positive, and you further restrict this complex variable z down to the real axis, so I take u of x and t, where t is now real and positive, and I apply it to this p of x, x is in r, then this function u of x and t will satisfy. And t will satisfy the heat equation. Let's differentiate that series, and you will see that it will satisfy the ordinary plain vanilla heat equation the way you see it in an elementary differential equations book. So that's why we call it heat flow, right? Yes, in the simplest case, it is actually heat flow. Now, an important point is that we eventually would like to let the degree of the polynomial go to infinity. It's all about that. And to let the degree go to infinity, you have to scale things correctly. You have to scale things correctly. And the correct way to scale things is to put an n in the denominator here. So basically, I replace tau by tau over n. So it's really time. When I say tau, it always means that there's a tau over n in the denominator. And the point of this is, yes, this is the scaling that makes the evolution of the zeros have some chance of having a limit. That's n goes. Okay? Okay, now. Okay, now this section is supposed to be about real roots. And for real roots, it's actually better to take the backward heat operator. Generally, backward heat operator is bad, but on polynomials, it makes perfect sense by a power series. So I'm going to do this e to the minus t over n there. d squared dz squared t is possible. That's backward because of that minus sign. Okay, and there's the theorem of Polly and Benz from 90 years ago that says, well, if you start with a polynomial. It says, well, if you start with a polynomial with all real roots and you apply this backward etc, then the roots will all remain there. Okay, so we can have a case where we stay in the world of polynomials with real roots. Everything's on the real line. And it's a big simplification to the problem. If you know the roots are all on the real line, then it's much easier to analyze. Okay? So now our goal is to sort of understand: well, how do they evolve? How do they evolve? And before I go to that slide, I'm going to ask you a question. I want an example. I like examples. So let's take the simplest example we can. So I want you to tell me the simplest example you can of a polynomial of degree n has to be degree n, arbitrary degree n, that has all real roots. And you tell me a polynomial of degree n that has all real roots, right? All the real approach. James? Anyone? X to the N. X to the N. X to the N, thank you. Okay, excellent. So we're going to take C to the N. I lived in Canada for a while and I trained myself to say Z, but I progressed. So I'm going to say Z to the N here. If you apply the backward heat flow to Z to the N, its roots are all 0, 0, 0, 0. And when you apply this, they flow away from 0 along the real axis. And well, what you get is a scaled version of the Hermitian. What you get is a scaled version of the Hermite polynomial. When you look at the Wikipedia page for Hermite polynomials, one of several equivalent definitions is it's the backward flow applied to x n. Okay, and it's a theorem that the zeros, as n goes to infinity, the zeros are known to have an asymptotically semicircular distribution on the interval minus 2 root t to 2 root t. Okay, with this scaling of the heat equation, you get that. Of the heat equation, you get that semicircular shape. Okay, and many of you may know at least a little, maybe a lot, about random matrix theory, and you may have heard of the semicircular law and random matrix theory, and it's an interesting coincidence, or maybe it's not coincidence, that, oh, the semicircle shows up here also. Okay, now let's do another little more interesting example. Oh, that one's already interesting, right? Semicircle, that's pretty cool. Let's take a polynomial that has, let's say n is even. That has, let's say, n is even, and we take a polynomial that has half of its roots at 1 and half of the roots of minus 1. In this case, there's no formula, really. There are not any special named polynomials. But if you just do it, say, with t equals 1 for definiteness and n equals 500, you get some kind of interesting shape. And a goal would be, well, can we calculate what that shape is? Like, let's take this example as a test case. Like, can we compute the shape? As a test case, like, can we compute the limiting root distribution for this example? And the answer is, yes, we can, and I will tell you. We, being someone, can. Not necessarily me, it's not my theorem. Okay, so I said there was going to be a connection to random matrix theory. Okay? So this is my one-slide course on random matrix theory, in case you're not familiar. Gaussian unitary ensemble is all I'm going to talk about. So what is Gaussian unitary ensemble? It means you take an n by n Hermitian random ensemble You take an n by n Hermitian random matrix in which the entries are independent as much as possible, subject to the constraint that the matrix is Hermitian. So that means the entries on and above the diagonal are independent. And then the entries below the diagonal, of course, are the conjugates of that corresponding entry above. And we take the entries to be Gaussian with mean 0 and variance 1 over n. And for some reason, we like to make them common. And for some reason, we like to make them complex Gaussian off the diagonal, real Gaussian on the diagonal. Okay, and then I think it's accurate to say that this is the first theorem in random matrix theory is the semicircular law, which is that if you pick a matrix at random like this for very large n, the eigenvalues are going to resemble the semicircle distribution on minus to the two. The Varchenko-Pastur law gradiates this by thinking by 35 years. Okay. Alright, I learned something. Okay, not sure what's happened there. So the talk's going to come to an abrupt end here. Because I didn't accurate this yesterday. It is your punishment. Okay, well, I'm truly not sure what it is. It can't be discovered by Pastor. Yeah, card cover. Pasteur, who's still alive? It's discovered by Wishart. And then proved rigorous by Marchenco and Pasteur later. Pasteur is not that old. It's very old. It's almost unfortunate. Marchencism. Okay, so now that we're back in business, let's hope it stays that way. No, sorry, I'm passed. Okay, but it seems to be working at the moment. But it seems to be working at the moment. Okay, so what's the connection to the GUE? So, why do I bring random matrix theory in here? So, how can we connect polynomials to matrices? Okay, well, here's a simple way to do it, which is I'm going to take a sequence of polynomials with real roots, and I assume there's some kind of convergence as n goes to infinity, which is that the root distribution has to converge to some probability measure u on the real line. Okay, and now I'm going to connect and make a matrix. I'm going to connect and make a matrix. How to make a matrix? I'm going to make a diagonal matrix x0n that just has the roots of Pn on the diagonal, in whatever order you like. Okay? Pretty simple. And then I'm going to take Xn with ant of 0 to be a G V. And the sort of concise way of describing the theorem, I'll describe it slightly more precisely in a minute, is that if you take the original polynomial Pn and you evolve it by the backward Heath. Evolve it by backward heat flow for time t. The roots of this p-devolve polynomial are going to resemble the eigenvalues of this matrix, which has the roots of Pn on the diagonal, plus square root of T, that's the T here, goes here, square root of T times Gu. Okay? So that's a connection. This title of the slide is a connection, the random matrix theory. This is the connection. Backward heat flow resembles. Backward heat flow resembles this. Of course, that connection doesn't really answer the question you want to answer, which is, well, what is the limiting root distribution? So the next thing you say was, well, okay, but how would I compute the eigenvalues of this sum? And the answer is you compute it using something called free convolution, which I will attempt to explain in the next slide or two. Okay, but the connection, what's the connection between backward heat flow and random matrix theory? Backward heat flow is like out. Matrix theory, backward heat flow is like adding a G loop. You put the roots on the diagonal and then you add a G loop. Okay, so now I would like to try to tell you how do you compute the free convolution of two measures, but specifically with the semicircular distribution, because the GUEs are semicircular, the other ones are arbitrary. So that's really what I need to tell you is free convolution with a semicircular distribution. Okay, so we're going to use the Cauchy transform to do this. Transform to do this, the Causal transform of a measure on the real line is the integral of 1 over z minus x d mu x. And if we take z to be in the upper half plane, then z and x do not collide. x is on the real line, z is in the upper half plane, which means this is a holomorphic function of z in the upper half plane. And it's useful because you can recover the measured mu from its Cauchy transform, right? Otherwise, not so useful, which is you take. Not so useful, which is you take the imaginary part of the Cauchy transform and you evaluate it sort of infinitesimally above the x-axis and take a limit, that gives you back the measure. Okay, so now I want to present a kind of combination of a theorem of Kobluchko and a theorem of Wojpolesky. So if you have a polynomial with real roots and the root distribution is converging to mu, then the root distribution of Then the root distribution of this heat-evolved, backward heat-evolved polynomial will converge to a measure. And the Cauchy transform of that measure, which now of course depends on Z and T, yes, will satisfy this Pd. Okay, and really what it is, I mean, another way of saying it is that it's the free convolution. I mean, basically, what's said on the previous slide is the μt is the free convolution of μ, free convolution, which is the Free convolution, which is denoted by this box, is the free convolution μ with the semicircular measure of variance t. Okay, so it's the theorem of Kaplushka that mu t is this free convolution, and then it's a theorem of Vojkolescu that the free convolution with the semicircle law satisfies that. Okay? And you can solve this PDE by the method of character. By the method of characteristics, and that gives you a somewhat explicit way. If you work it out, spend a little time with characteristics, you can get a more or less explicit way of computing the measure mu t. And so, for example, let's go to our example of the measure that has mass a half at one and a half at minus one. Okay, you take the roots half moon at one, half and minus one initially, and you And minus one initially, and you run it with a factory heat flow. And well, okay, so I'm computing this measure, which has mass a half at one and half at minus one. I'm doing the free convolution with a semicircular guarantee. Let's say t equals one. And you can do it explicitly. There's an actual explicit formula, which is messy. I won't write it down, but it is an explicit formula, and that basically replicates the histogram. The histogram that I showed you before was some kind of approximation. Was some kind of approximation for this curve. Okay, so that's the answer. Backward heat flow for polynomials with real roots is free convolution, and that free convolution can be computed using a PDE for the Cauchy front. All right, that's the end of the first part of the talk, but I will entertain questions if you have any questions. Okay, so let's go on to polynomials with real roots, and we're going to do differentiation. Okay, so let's start with a polynomial Pn. You know, always there's an N, right? And we're going to let N go to infinity. Polynomial Pn of degree n with real rates, and we're going to differentiate it nt times, t between 0 and 1. Okay? Integer part of nt times t between 0 and 1. Meaning that the number of derivatives is proportional to n. The number of derivatives is proportional to n. If you just take one derivative of your polynomial and you let n go to infinity, then the root distribution won't actually change. But if you take 10 derivatives or a fixed number of derivatives, then in the large n limits, the root distribution won't change. I want to just take all the derivatives, all the way down until the polynomial is gone. So I want to take the number of derivatives proportional to n. And, okay, well, you don't really need a Poly-Benz theorem. It's an elementary thing. The roots remain real, right? Real, right? Follow along with all real roots, the derivatives still have all real roots that interlace with the original roots, okay? So then we, if the roots of the Pn converge to mu, we'd hope to find the limiting root distribution of this n t root. That's our objection. All right, so again, what's the answer? Use random matrix theorem. Okay? And a PE. Both, as well as that. Okay? Alright, so let's. Okay, all right, so let's assume at first that t has this special form. t is 1 minus 1 over k, where k is a positive origin. Okay, then the answer is that this mu t is a free convolution of mu mu mu mu k times and then rescaled by a factor of 1 minus t. Okay, and you say, well, what does that really mean? It just means that I basically make k independent Hermitian random matrices, all of which have Matrices, all of which have limiting eigenvalue distribution u, and then you just add together, like K, II, V, Hermitian random matrices. That's what this reconvolution is supposed to be computing. That's the significance of this free convolution. Okay, but then you say, well, okay, but T may not have this special form. So then you extend it. You know, you make some formulas that you somehow show that those formulas, if you massage them into the right form, that you can define it for arbitrary T. That you can define it for arbitrary t or arbitrary k. Meaning, t can be anywhere between 0 and 1. It means k is not necessarily an integer. And this is called the fractional free convolution. But it's like self-convolution. You're convolving mu with itself, k times, that was introduced by Schlachtenko and Tao. Now, Schlachtenko and Tao were not talking about repeated differentiation, okay? They were just talking about, well, how can you extend this free convolution to fractional k? But that turned out to be the But that turned out to be the tool. Okay, so that's the claim here, right? Is that if you can define this mu box k for arbitrary k, then that's what the repeated differentiation is going to do. So if t is between 0 and 1, then k is bigger than 1 in that relationship. Yeah, k is bigger than 1. So it's not. Can you say a little bit more about what Shape Penn Quintaux actually did? More about what Shai Penkuan Tao actually did because free convolution powers for any real power bigger than one was introduced in the 90s by Schweiber and Nika, where they show that it's given by free compression, by the corner of a random matrix. Okay, then it may be my mistake that it was originally due to the Schlachtenko and Tau, but there is a theorem in Schlachtenko and Tau that, as far as I know, was not in any of those earlier papers. It's not in any of those earlier papers. But yes, I may have been incorrect in attributing the actual construction to Schlechtenko on top. But I'm going to use a result in your paper that was, I think, not in this earlier pay-based page. So thanks for you two, Brian, Zeros. I will correct that the next time I do this. Okay. So the theorem about repeated differentiation then is, I already stated it roughly. I already stated it roughly on the previous slide, which is if the polynomials Pn have real roots and the limiting root distribution is μ, then the mtth derivative in the limit will be this new tensor. I don't know how you pronounce that symbol, k. Okay? Where k is 1 over 1 length. And then rescaled by a factor of 1 minus t between 0 and 1. And this theorem. And this theorem, I should emphasize, was motivated to a large extent by the work of Steinerberger. I mean, Steinerberger's work wasn't rigorous and it didn't explicitly use the free convolution, but it had a PE that's sort of equivalent to what's in Leichtenko and Cap. So it was sort of the motivations for this thing. All right, so let's do an example. I like examples. And by now, it's all your favorite example. It's a polynomial that has half of its roots in one and half. has half of its roots in 1 and half of minus 1. So the mu is the half of a delta measured 1 and half a delta measured minus 1. Okay, and let's make it simple. Let's take t to be 1 half, which means k is 2. Okay, so that means I'm going to take n over 2 derivatives. So think of some large n here, do this polynomial with some large n, and then take n over 2 derivatives and say, well, what do the zeros look like? Okay, and in that case, it's just mu tends to, it's And in that case, it's just mu times, it's mu free convolution mu. K is 2, so it's just free convolution in itself. You can calculate this explicitly. You can do it, I think, for any k also explicitly, but it's a little simpler in this case. What you get is this arcsine distribution. 1 over squared 1 minus x squared. So this arcsine, because when you integrate that function, you get an arcsine. And then these yellow part is an actual histogram. Like I actually took n equals 500. n equals 500, I think, and I just asked my computer to differentiate it 250 times and find the roots, and you see it gives a very nice approximation to the R sign well. It takes a few minutes maybe to write on a computer. Okay, so that's the answer, right? And it is a connection that random matrix did, right? You just use sums of independent Hermitian matrices, essentially. That's what the free prompt is. But then I also say, But then I also say, well, now we also want to have a PDE. So I hope I'm accurate in attributing this part at least to Schlachtenko and Klein, which is they have a PDE for the Cauchy transform of this mu to the k as a function of z and k. Okay, and then we just have to translate that into t. k is 1 over 1 minus t, t is 1 minus 1 over k. You change variables. 1 over k, you change variables. And then there's one more thing, which is I want to use the rescaled measure. So usually when you talk about the limiting root distribution, then by convention, you make it a probability measure. But when you're differentiating, that's not really natural because you're killing off the roots as you differentiate, sort of proportional to t. So it's really more natural to say that the measure at time t should have mass 1 minus t. And then at 0, all the roots have gone. So we rescale the measure to have mass 1 minus t, and we take the Cauchy transform of that. And we take the Cauchy transform in that rescaled measure, and then you use the PDE of Schlachtenko and Tau, and you get this PDE for the Cauchy transform, which looks a little bit like the PDE that we had for the heat flow, right? I mean, somewhat sort of similar first-order nonlinear things, right? This one has a minus sign, and this one has one over C instead of C, but they're in the same ballpark. Okay? All right. All right, any questions before we leave the safe harbor of real roots and go to complex roots? I have a question only from part one, actually. Take a slide back. So the equation, this PDE that you have down below there, right, that was from the backwards heat flow. So if you use the forwards heat flow instead, at least in the random matrix context, then you get the same equation without the minus sum, which is the Edwissen-Berger equation. Minus sine, which is the infused burger equation. But does it make sense in the polynomial context if you use the forward heat flow on a real-rooted polynomial? Does it stay real-rooted? No. Thank you. And thank you, because that would be my next slide. Okay, so let's try for, let's make the roots complex, but we're going to start with real roots and we're going to make them complex. Okay, so let's take the characteristic polynomial of GUE. It's semicircular, right? It's asymptotically semicircular. Right? It's asymptotically semicircular. And if you do the backward heat flow of that, it's a simple application of the theorem I told you that it will remain semicircular. The semicircular free convolution is semicircular, a semicircular with a larger variance. So backward heat flow will just stretch out the semicircle to a bigger semicircle. Okay, so it starts with semicircle on a width t and it gets bigger. So now let's run time the other way. Then you might reasonably think that the semicircle would just shrink, right? Right, all right. Well, let's see, okay, so let's do it. So, this is folks, these are the eigenvalues of a GV, they're minus 2 to 2, you can't tell, but they are semi-circularly distributed. And now I'm going to run the heat flow and see where the COS go. And the answer is they do not stay. Now, actually, in principle, they will stay on the real axis for some very short amount of time. Because the roots initially are distinct, and then they Initially, they are distinct, and then they move the differential equation for the roots moves on the real line, but they start colliding and moving off the axis sort of very, very quickly. And so, conjecturally, the zeros at time t will be uniform on an ellipse with semi-axes 2 minus t and t. So, if t is 1, it becomes 1 and 1, and you get uniform on the unit disk. And so, you could say, again, it's conjecturally, that the heat flow changes the same. Conjecturally, that the heat flow changes the semicircle law to the circular law. Okay? All right. Now, but in general, if you just have any polynomial with real roots, complex roots, how can you understand how the zeros evolve? So we're going to introduce again the Cauchy transform given by the same formula, but now we're integrating over the plane. And the big difference is this time you can't stay away from the support of the measure, or you just don't have. Support of the measure, or you just don't have enough information if you only do that for outside the support of the measure. You have to do it everywhere, okay? And if you do it everywhere, then it will not be holomorphic inside the support. So for example, if mu is uniform on the disk, then the Cauchy transform equals 1 over z outside the disk, that's holomorphic, but it's z bar inside the disk, which is not holomorphic. And then the counterpart of the Stilchi's inversion formula is that the density. Is that the density of the measure is the z-bar derivatives of that. So in this case, we get 0 out here and 1 over pi inside, which is the uniform measure we started with. Okay? All right, so Cauchy transform. So, Ching Wei Ho and I have a conjecture from a paper that's on the archives from 2022 that, and I'm just by convention to avoid being inconsistent. Being inconsistent. You're putting a minus sign here, but tau is an arbitrary complex number, so just the minus sign is just so that the formulas are similar to what we did before. But remember, tau is any complex number. So this could be forward, backward, complex, any kind of heat flow you like. Okay? And the claim is that the limiting root distribution will satisfy, quote unquote, the same PDE that we had for the real rooted x. Except, well, it's not really exactly the same PDE. Exactly the same PD because tau is a complex variable for this to be true as stated, tau has to be a complex variable. But the Cauchy transform is not in general holomorphic in tau. Inside the domain, the support of the measure, it's not holomorphic in tau. So this means the Cauchy-Biemann operator. This d by d tau means in the sense of the Cauchy-Biemann or Virti-Mir derivative sense. And the Cauchy transform is also not holomorphic. The Cauchy transform is also not holomorphic in Z either. So the Z derivative also means Cauchy remonabular. But it's still in some sense the same PDE that we had in the case of roots on the real line in the backward Heathlab. All right, now why would we believe this conjecture to be true? Well, if you just take any polynomial evolving in time by the HEFLA, you can make this sort of discrete version of the Cauchy transform. And this is like the Cauchy transform. Transform. And this is like the Cauchy transform of a measure that has mass 1 over n in each root. Okay, and now you can ask: well, how does this finite n Cauchy transform evolve in time? Just start differentiating, it's not so hard to see. And you get this PDE, that's exact PDE for any n, which is this part is the PDE that we want. That's the PDE on the slider. And then this part has a 1 over n in front of it. 1 over n goes to 0 as n goes to 0. 1 over n goes to 0 as n goes to infinity, so there you go. That's a truth. Except, well, it's not. It's definitely not a rigorous argument, okay? But heuristically, that's a motivation for why we might believe such a thing. All right, so let's now, what am I supposed to do now? I add the PDE, right? Any section I'm supposed to have: a PDE and matrices, random matrices. Matrices, random matrices. Okay, so let's make some examples from random matrix states. So the point is that there are a lot of interesting examples that come from random matrix states. Okay? So let's take two independent GUEs. It should, I guess, be a superscript n thing. These are n by n G V's, but I don't write the n. And then some complex number, tau, and I'm going to make this thing, which is an elliptic random matrix. I'm taking some combination of x and y with an i. Okay, so if tau is zero, Tau is zero, then it's just x plus Iy, which is the Genie matrix, gives you the circular bottle. If tau is not zero, then you get eigenvalues that are uniform on an ellipse. There's some semi-axes, and the argument of tau will rotate the ellipse. Okay, and now it's a theorem, it's not a hard theorem, but it's a theorem that says that if you now take the limiting eigenvalue distribution, which is the uniform measure on these ellipses, okay, take the uniform. On these ellipses, okay? You take the uniform measure on the ellipse with parameter tau, and you take the log potential of that uniform measure, then it will satisfy that PDE as a function of the space variable and the tau. The log potential or the Cauchy transform? Ah, Cauchy transform, excuse me. Yeah, yeah, okay. The log potential satisfies A P D E. Yes, Cauchy transform that satisfies the P D E that I wrote for. Okay, so. Okay, so that means, right, that I should be, the conjecture says, I should be able to replicate that behavior with polynomials evolving by the heat flow. So I just, let's say we want to start at tau equals zero. Then I just build a matrix that replicates the circular law, which I can do. It's the characteristic polynomial of the random matrix with. Of the random matrix with parameter 0, the Geneva matrix. Okay? So I start at tau equals 0 and I take the characteristic polynomial. Its roots were the eigenvalues, the Genieva matrix, they'll be asymptotically uniform over the disk. And then the conjecture says, well, if I flow it by time half, it should be evolving according to that P D E, which means it should be giving me uniform on a ellipse. On a list. So the conjecture basically says you start with a characteristic polynomial Geneva matrix, you evolve it for time t, then it should give me roots that are uniform unless. Okay, so let's have a picture, okay? So look, I just took the I took a Geneva matrix, I find its eigenvalues here, and then I evolve the polynomial by the heat flow, and we see how the roots evolve. How do the roots evolve? And the circle turns to an ellipse, and the distribution is still hopefully, plausibly uniform on that ellipse until the ellipse collapses down to. Okay, so that's an interesting example. It's a conjecture. I don't know how to prove that that is actually true. Okay, and here's another example that you can do, which is, okay, instead of just elliptic matrices, you can take a look. Of just elliptic matrices, you can take elliptic plus some other matrix, for example, elliptic plus R unitary. Then you can compute the limiting eigenvalue distribution. And the Cauchy transform still satisfies the P D. I mean, it's not uniform on ellipse. It's more complicated. But whatever the limiting distribution is, if you take the Cauchy transform, it will satisfy the P. And you can predict what happens. And basically, this picture is supposed to be saying that. Saying that our conjecture holds. These roots are evolving the way they're supposed to, according to the analysis of this random matrix model. Okay, so you can make lots more. It's not just elliptic. It's elliptic plus any other matrix. Okay, now that's a conjecture. I told you that gave you a nice formal sort of physicist-type variation of the result, but rigorously from the But rigorously, from the PDE, pure PDE standpoint, we don't really know how to control that error term. And so to do something rigorously, we're going to switch from random matrices to random polynomials. This gives a setting in which we can prove something. So there's a, I think, by now quite well-known paper of my collaborator, Zakhar Kabuchko, who Zaporozhets, that studies a very large class of random polynomials. Large class of random polynomials with independent coefficients. Undoubtedly, various special cases of this have been studied previously. So these CJs are IID random variables with essentially any distribution you like. These CJs, CJ and CJ, these CJs are deterministic constants that have some reasonable behavior as n goes to infinity, and then z to the j, cj, cj, zj. CJ, CJ, Zj. It's a good time twister, okay? So you take these random polynomials, and then they gave a mechanism, again, assuming that the CJ's, the deterministic constants have some nice behavior as n goes to infinity, they were able to calculate the limiting root distribution of this random polynomial. And in all cases, the measure you get is rotationally invariant measure on some disk or annulus. And essentially, any rotationally invariant measure at a disk will occur. Invariant measure and a disk will occur for some choice of the Cj's. But the very, very general result, this is telling you is a very large class of Ramian polynomials that fit into this framework. Okay, so let's do an example, which is the bile polynomials. So I have my arbitrary IID things here. It can be whatever distribution you like. And then my deterministic constants are here. Some overall scaling, and then the square root of j. Overall scaling and then the square root of j factorial in the denominator. And these polynomials have a limiting root distribution that's uniform on the disk. So it's like the circular law, but instead of circular law for a random matrix, it's circular law for a random polynomial. Okay, so, but the general theorem is that if you take any of those Kambluchko's operations polynomials, and you evolve them by the heat flow, then sort of with probability one, oh my. probability one, almost surely the Paul Ho conjecture will hold. That the limiting, the limiting, almost sure limiting root distribution will satisfy for a sufficiently small tau. Should have said that previously, only for a sufficiently small tau. Okay, so now I'm going to show you a picture. So this picture, you say, well, didn't you show us this already? Like, well, no, this is the random polynomial counterpart. And if you look closely at this picture, you can ask Todd. At this picture, you can ask Todd about this. It does look slightly different from the picture for the genre matrices. But also, this one is going to be much cooler because it's a theorem. Previous one, it was just a conjecture, so that's boring, but this one's an actual theorem that when n goes to infinity, this is really how it works, that you get the same transition from circular to electrical. Electrical to semicircular when it collapsed. Okay, now very briefly, because I'm out of time and the last section of my talk may have to be rather brief, but one important part of this, though, is you really want to know not just, well, what is the limiting root distribution in time t, is you want to know how things evolve in time. It's one thing to say, all right, the roots start like this. It's one thing to say, all right, the roots start like this, and then at time t they're like this, but how did they get there? So, how do the roots move as a function of time? And what we claim is that the roots basically move in straight lines with constant velocity, and that constant velocity is just equal to the Cauchy transform of the measure at time zero. And you ask, well, why would they move along these straight lines with that particular velocity? It's because those are the characteristic curves. It's because those are the characteristic curves of that PD. So the concise way that you might actually remember is the roots involve along the characteristic curves of P. So that means the PDE, first of all, is a useful way of calculating things, but it's also very conceptual that, okay, it's a PD from which the method of characteristics applies, and the roots are supposed to be moving along those characteristic curves. Okay, and what we prove, we don't actually really prove that. Prove that. We prove is that the bulk distribution of zeros is consistent with that. Okay, that if you push forward the initial distribution under a map that consists of flowing along the characteristic curves, then you will get the correct measure of plenty. That doesn't mean the roots have to follow those curves either. It doesn't follow, but it's sort of true at the bulk level. All right, so let's see a little picture, okay? So let's see a little picture, okay? So these blue dots are the initial roots of the polynomial, okay? And those gray curves are the straight lines along which the roots are supposed to be moving. And now I'm going to evolve in time, and if we're lucky, work rehearsal, these blue dots will follow pretty close to the straight line. Almost most of them, most of the blue dots should be following along pretty close to their straight lines. Pretty close to that straight line. I didn't run it all the way up to the real axis, but I could have. Okay, so I'm essentially out of time, but I will just very briefly mention about polynomials with complex roots. And so it should be PDE and random matrix one. So the PDE is this one. Looks like the same PDE. The slight complication is there's this other term there. Is there's this other term there. And why is there that other term? It's because, well, t is real. I'm taking nt derivatives. And so it doesn't make sense that t is complex. And the variable, if the time variable is not complex, then you need this other time. You're supposed to think of it as basically the same PDE. And a short version is that, well, we get rigorous results for random polynomials. What you would predict from this PDE actually holds if you start with one of those called literacy-sapparoshias. Okay, but if you remember in my very first, second, third slide, right, I showed you a picture of some roots of some polynomial that we differentiated and we saw that the roots seem to move kind of continuously down toward the origin and then they all eventually disappear at the origin. And so we asked, well, how do they move? Like, can you give a formula for how they move smooth? And the answer is yes. It? And the answer is yes. This is the formula. Okay, they move radially. If the initial distribution is radial, then the roots will move radially inward, and they will move with constant speed. If you track one single root, it's moving radially, and its speed is constant as it moves along. And what is that constant value of the speed? It's given by this part right here, which is the negative reciprocal of the Cauchy transform. Of the Cauchy transform at that initial point. Take the initial Cauchy transform at the initial point, and that determines the speed, and then they all move that way until they hit the origin. If T is big enough, eventually this curve is going to hit the origin, and when they hit the origin, they die. They have to be a mechanism for killing the roots, right? Because differentiation reduces the degree. So, how do the roots get killed? Well, they get killed when they reach the location. Okay. Okay, and our theorem says that somehow at the bulk level, that picture is correct. If you look at the limiting distribution in the bulk, it's consistent. It's pushing forth under a map that's constructed that way. You move radially and you kill before it gets to the origin. Okay, so this picture is supposed to show what happens. So I started with some polynomial. With some polynomial. And I ran t not all the way down to t equals one, maybe the t equals a half or something. And over here are the roots that started out small. This is the beginning of the chain. So this root, you know, these were the ones that are originally pretty small. And this picture is supposed to mean that these roots ran radially down to the origin with constant speed, but they hit the origin before the time. But they hit the origin before the time t that I'm calculating, so they're gone. Okay, and these roots over here are the big roots that initially are bigger, and they run radially toward the origin, but they terminate, you know, that they haven't reached the origin yet at front t. So these red dots are all the roots at time t. So they should be all be sort of at the ends of these chains. And these guys just die off and they're gone by time t. And that's essentially what the theorem says. Essentially, what the theorem says. And this picture I'm going to compare the evolution to the straight line motion. So the blue dots are the actual roots, and they're moving along. And then they have a little companion green dot. The green dots are moving exactly along straight lines. And hopefully, the blue dots will keep with their partner. And that bell means I'm out of time, so I'm going to just. I'm going to just make this our last slide. Okay, if you look, particularly when they're farther away from the origin, each blue dot stays pretty close to its green dot. And when they get near the origin, things start to get a little more unstable. But on the whole, that is the case. Okay, so that val means that my time is over. But I will give you a quiz. What did I not do that I was supposed to do? Did I not do that I was supposed to do? Random matrix model. Okay, so there's a few more slides. It says yes, there's some random matrix model, and it's similar to the real rooted case. I said it was fractional convolution, but it's the same as truncations, and it works in that case as well. Okay? So, but I should try to respect that. Questions? Yeah, so we kind of approximately follow these characteristic curves, which in this case are lines. So does the remainder look like if you know? The error of how much it deviates from the straight lines. In a polynomial setting, we don't really have a formula. But we also have a paper about the Gaussian analytic function, which is like a bio. Analytic function, which is like the Lyle polynomials, except the degree is infinity, like the sum all the way to infinity. And in that case, we do have an error estimate. And the error estimate is basically order one, independent of the starting point. And the point is the roots move a long ways, they move a long ways in time t if you start with a big root, and then you have that long. You have that long motion with an error that's order one. So, like, there is some kind of estimate that says in that case that the roots do really stay close to their straight lines, but I just don't know how to do it for the polynomials here. Yeah, maybe. Someone, maybe almost the same question. So, in the pictures where you have the roots and all inside all approximately. All inside, all approximately ellipse. Some of them are outside. Is there anything to say about how far outside or how many or with what probability? Well, I mean, whatever, it depends what you start with at time zero. If you start with a random matrix, then there's a theory about how many outliers they may be and how far away they are. If you start with a random polynomial, then I imagine. Then I imagine that there's a theorem about how many outliers and how far away they are. The answer, but what you can tell immediately if you do a simulation is the random polynomials have way more outliers and they're way, way, way, way, way farther out. I mean, there can be roots that are like distance two. You know, unit this, and this one was like distance two from the margin. They're way out there that you would never, ever, ever have for. Perhaps for the average operator. Those far outlier roots should still move, basically, according to some straight line. But yeah. So yeah, it's an interesting question. But if you do your quantum first GUI and you do add to it a rank quantum matrix, At once a outlier. So if you make your forward knee flow, how does it look like? About that case, I mean our theorems are basically about the bulk. You know, for the case of complex roots, then if you somehow make like a machine of matrix and you somehow push one IP value way far out there. Value way far out there, then it will just move according to the same predicted formula. It'll just move according, I'm, you know, not a theorem, but I believe that it will still just move according to that formula. It'll just move in a straight line with velocity equal to the reciprocal Cauchy transform evaluated at that way, far away to one. I think the simulations would be consistent with that. It's not a human, but it is. It's not a human, but they should just move the same way. The same formula applies, even if it happens to be very far out of line. So in the beginning, half of the talk, you had an explicit example where if you put point masses at the zeros of the polynomial, you can write down an explicit, exact finite MPDE, which had an error correction, an error term of order 1 over n or 1 over n squared. Term of order 1 over n or 1 over n squared, I don't remember which. So is the PDE there, is the generator of that PDE elliptic or parabolic or not? Could you do a viscosity solution to improve the difference? That's why I said it's not rigorous. It said, yes, it's not. You really should make time to be real instead of complex so that it looks like it. It's what people usually do. And then look at the error chart and it's not going to be And it is not one of the viscosity terms. It's not the Laplacian. It's the, it's like v squared vx squared minus v squared vy squared. It's nothing like viscosity. Despite the formal resemblance to a viscosity approximation, it's not that. But we we have a PEE expert out of Notre Dame who wants to talk to me about these things. Maybe he can give me some ideas of how to do it. Some ideas of how to do it feedback, but right now we don't know. Other questions? Problems back as we've got a small one.