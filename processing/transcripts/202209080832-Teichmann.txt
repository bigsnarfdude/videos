Otherwise, please give me a sign if this is not the case. I think I can see at least my slides in the video view, so this seems to be okay. Yeah, very good. So this is a very small, recent, quite technical result, but I think it might be of interest for you to see this detail. To see these details a little bit, and also maybe to add some thought to it if you want. So it is about another way how to represent the signature transform of paths. For simplicity, here we can think of a continuous Think of a continuous finite variation path, but actually the results all carry over to more general notions of path where a signature transform is defined. And well, to go right away to the goal of the talk, we all know that in machine learning, signature transforms are Signature transforms on path space are used as a regression basis for continuous with respect to some topologies path space functioners or as a regression basis in order to represent dynamics driven by the given path and this is one regression oriented Application of signature transforms to machine learning. But you all know that in machine learning there have been alternative techniques also to represent path space functionals or dynamics. And usually these methods work with recurrent neural networks. So recurrent neural networks can be seen as just discretizations of controls. Controlled ordinary differential equations, controlled differential equations. And signature is then, in that sense, a very particular example of such a recurrent neural network, where two features appear. One is that the hidden space in terms of the recurrent neural network is so large, in case of signatures infinite dimensional, that any dimensional that any path space functional with satisfying certain continuity properties can be approximated. So it's an exhaustive list of features of the path. And on the other hand, it's something which is not trained. So you only have to do a regression. So there is no way how you can modify signature. Whereas recurrent networks, of course, have trainable parameters and can be teamed towards a certain situation. And in between, this approach of this universal feature selection, which signature transform can be seen, and recurrent neural networks, you can see in the middle the approach of reservoir computing, where you somehow use recurrent networks or controlled ordinary differential equations with randomly chosen kinds. With randomly chosen characteristics. Randomly, I will explain later what that means, but you can see somehow that in between this regression-based and fully network-based approach, in between you have this random recurrent networks, which actually finally also lead in most of the cases to a regression, but you have some tunable parameters when you. Have some tunable parameters when it comes to the choice of the randomness. And I try to provide the representation theoretic viewpoint why actually this randomized recurrent network, and later on we will call it randomized signature, is actually as expressive as signature itself in a precise mathematical sense. So let me go down. Go directly to the notations of signature. Everybody knows that here in the audience, we consider the free algebra generated by D indeterminates, E1 up to Ed, and we take actually all former power series in monomers of these indeterminates. This is a Hopf algebra having many beautiful properties, all well known by the audience much better than by myself. By myself. Additionally, we have a nice locally convex topology on it, making it easy to talk about ordinary differential equations. It's just a sequence space with countably many couples of the real numbers or complex numbers, as you like. And therefore, you have a nice locally convex topology thereon. And therefore, it makes sense to talk about such a differential. sense to talk about such a differential equation where u is just for simplicity continuous finite variation path ei is multiplication from the right by one of the indeterminants so this is a linear vector field on our algebra and signature is the solution of this linear ordinary differential equation which apparently has a unique solution after a moment of reflection which After a moment of reflection, which is given by the former power series where the coefficients are the iterated integers of the path. And here I made a little mistake at time. S you start with A, therefore you have on the right here multiplication by A. So it's a dynamical system, a non-autonomous dynamical system driven by the path on this algebra, and this contains all Contains all iterated integrals. We call this collection of all iterated integers signature, and we all know that signature has many beautiful properties. In particular, it characterizes the path up to tree-like equivalencies. And if you add additionally one component, which is time, then apparently you have characterized the path up to its initial value. Reservoir computing or Reservoir computing, or the choice of a dynamical system driven by this path, which is random, has been successful in machine learning for some time. So this is these not fully trained recurrent networks, but randomly chosen recurrent networks. And one can try to make a connection with reservoir computing when you look at truncated signature. You can say if you have You can say if you have any evolution driven by our path U, and you have some characteristic vector fields V1 up to Vd, then it is possible, and this is just this Taylor expansion theorem, as a linear combination of truncated signatures, so signature up to depth m, up to order t to the power m plus one to approximate any. One to approximate any test function on the solution of this equation. So you can consider that this explanation that signature can be considered or truncated signature can be considered as a reservoir, even though we do not have these random features and therefore it is only halfway in the direction. But this is a theorem which also explains why. also explains why tool signature is exhaustive when it comes to features of the path. As I try to say, in the spirit of reservoir computing, the reason why signature is not fully considered as a reservoir is actually that it is a high-dimensional object, even though the component Even though the components, the higher components, are small, they can contribute depending on the way how they enter into the regression. And therefore, this high-dimensional character is something which makes it or can make it difficult from a computational point of view, even though several techniques have been developed to shortcut here. Another aspect of reservoir computing is that often the Is that often the calculation of the reservoir, which would be truncated signature here, is done by a physical experimental realization. And these experimental realizations are often realized as so-called random experiments. So you have to imagine sort of random optical experiments, which are carrying the information of the input path. And of course, signature is a well-defined algebraic object, and you cannot write. Object, and you cannot, or it is not known how to realize it via a random experiment. It's not even clear what that would mean precisely. But I come later at one point back to this. And of course, signature is a polynomial-inspired feature selection of the paths and therefore satisfies all disadvantages polynomial regressions might have when it comes to condition numbers and so on. It comes to condition numbers and so on. One way how to leave this trap, which sometimes appears to be a trap, is information compression of signature. This is via the Johnson-Lindenstraus projection method. I will not go into detail here. I just show how one can see that. The Johnson-Lindenstrauss theorem tells that there are actually randomly generated matrices. Generated matrices with a particular choice of randomness, which have the property that when you apply these random matrices on an end-point set in some high-dimensional space Rn, you can imagine n completely irrelevant in this consideration. What is important is the capital N. When you apply this random matrix to an endpoint set and the range spaces are k, then Then the geometry of these endpoints of this cloud is preserved up to a factor one plus epsilon, one minus epsilon. So you have an almost Lipschitz map on this endpoint set given through the restriction of this random matrix. It's an easy statement out of concentration of measure inequality. Measure inequality, but still something which is very useful because the dimension k can be chosen logarithmic in the points n. And since in case of signature, we have an exponentially growing set of features, it makes sense to compress this exponentially growing set of features by a method which goes to a logarithmic dimension in the number of points you're considering. Of points you're considering. And when you formulate that out, you finally end up in the following quite beautiful situation that actually signature acting on the truncated pencil algebra Tm can be it's a dynamical system there, and this dynamical system can be compressed into a dynamical system. Into a dynamical system on Rk, which has, of course, characteristics depending on this Johnson-Linderschaus map F. But since the Johnson-Linderschaus map F is a random map, and since you have factors which appropriately work together, finally you end up, at least in case of high M with a system which looks in the following way. system which looks in the following way let me directly go here a system on Rk which looks like that so you have a random vector Bi so this is of course an asymptotic result and the random matrix Ai and the elements of these matrices and the elements of these vectors are independently sampled from a normal distribution with characteristic variances and expectation zero and Expectation zero, and this dynamical system approximates from a point of view of regression. So, when you do a regression on the different components of this dynamical system with respect to time, you get approximately the same as if when you do a regression with respect to signature. So, this gives an object which we call randomized signature, and we have been applying that in several circumstances. That in several circumstances. And what I want to give now in the last 10 minutes is actually a representation theoretic viewpoint on this differential equation and how one can understand without reference to compression of information that actually this equation contains the same information as signature. As signature, it's not completely correct what I say, but you will see the theorem very soon. So I have a quick question. These theta hyperparameters, where do they appear? Well, hyperparameters appear in the choice of the randomness of the matrix. Okay, so when you say it's not a normal distribution, just mean and covariance. It should be the mean and the covariance, right? Okay, okay. I did not specify. I did not specify it. You can calculate it in the limit. You have certain choices for theta. But when you do it in reality, you actually choose the covariance and expectation of the normally distributed entries here as hyperparameters which you tune to the system. Does this answer the question? Yeah, thanks. So now they represent. So, another representation theoretic point of view, Harris, of course, knows that I have a background, that there's a big love for algebraic and also differential geometric viewpoints. And instead of applying now this Johnson-Lincolnstraus lemma approach, which compresses information, one can also consider this random dynamical system from a completely different point of view. So consider manifold M and the vector fields on the menu. And vector fields on the manifold, and look at the following representation. I mean, we all know that the Hanzo algebra is chosen in order to capture the non-commutativity of vector fields of dynamical systems. So therefore, you can insert any vector field instead of the indeterminants EI, and you obtain a valid hormones at least asymptotically. Now, of course, the question is, and we imagine for a certain moment that we have already found that, we imagine that we have a manifold and we have vector fields which are satisfying, which are not satisfying any relation. So, this means that this map which I described here from Map which I described here from the tensor algebra. The algebra generated from the vector field, so this would be the algebra. Let me write it in easy terms. This is the algebra of differential operators on the manifold, differential operators of finite order. Yeah, I have to be precise here. I should leave away this guy. I should leave away this guy. So I only have polynomials, and from the polynomials, I go to the differential operators of finite order. We'll imagine for a moment that this map is actually injective, faithful. There's no query, that there are no non-trivial relations among the Lee brackets of the vector fields, or even more generally formulated that the differential operators which you obtain. Operators which you obtain always generate linearly independent objects. Linear independence with respect to the real numbers, not with respect to the modular functions. Of course, the situation would be different. So in that case, if we have such a representation, formally speaking, you could say the following thing. You could consider the transport equation. The transport equation The transport equation, and I make a little trick here. I introduce a parameter s, but I have here the vector fields from before, which do not satisfy any relation, and I have f on which the vector field acts as a transport operator. So this is a first order partial differential equation driven by our signal u, which is of finite variation. Of course, we know by the method of characteristics that this equation can be solved. This equation can be solved by just solving this equation. This is the flow of the controlled ordinary differential equation driven by our paths and driven by the electrical fields Vi and having again this little factor S here. And I insert the flow into the function. And if the function is the initial value of the transport equation, this solves the transport equation. The method of characteristics says having a solution of the transport equation. Having a solution of the transport equation in this very regular setting is the same as considering the flow of the ordinary differential equation on the manifold. Of course, here, and always imagine the manifold to be compact, the vector field C infinity. In that case, you do not have any problems, or otherwise the vector fields grow at most linearly, such that you have always global existence. Then the statement is completely true. And all regularity you need in the vector fields. All regularity you need in the vector fields. So this means solving this system here is the same as solving this system here. And algebraically speaking, since the representation from EI to VI is a phase 01, everything you can formulate in the EIs and you can express via the EIs should also be able to be expressed with respect to the Vi's. Now, what did we do before with the EIs? We were defining signature. We were defining signature. What do we do with the V i's? We define the transport equation. So, morally speaking, the transport equation should contain the same information as signature. What is the problem with this argument? The problem with the argument is, of course, the little thing which I said before. I consider here differential operators. I consider here the polynomials. In order to formulate signature, I need formal series, and there is a topological problem. Series and there is a topological problem. So one has to be a little bit careful in this whole consideration. And this explains why I get my parameter s here. My parameter s, if you take the derivatives with respect to s, it allows to crank out of the flow, out of the flow of this equation or out of the transport equation actually the order n part for any n by taking the derivatives of order. Any n by taking the derivatives of order n. And like that, you can make it just get everything which is related to order n, and you do not have these topological problems here. But you see, this is one way how do you, what you need to do in order to get that. Well, the statement which you can formulate, then, in case that you find such vector fields, then you can say that the collection of all curves. Of all curves up to time t for all initial values little x and for all times s contains the same information as signature because you can crank out signature by choosing appropriate functions f, applying them to x and choosing appropriate s's such that you approximate the derivatives which give you the order n level of signature. So by linear independence you get the coefficients out. Linear independence, you get the coefficients out of the vector fields. This is, of course, not only what we had before, because first, we don't know the vector fields here. And second, we also have functions applied on the flow, which might make the whole method a little bit more cumbersome. Before, in the case of randomized signature, we were only taking the components of x, but not functions on the components of x. Therefore, we need a little strengthening of the theory. Need a little strengthening of the theorem and also a concrete statement. The concrete statement is now the following: take sigma a random real analytic function. So you can imagine any real analytic function and an appropriate small perturbation by independently sampled numbers such that you remain real analytic in the neighborhood of the given function. Given function and imagine some matrices A1 up to ED, B1 up to BD, randomly chosen with respect to the vector measure absolutely continuous, and take then the vector fields which are component-wise sigma applied on the outcome of this affine function. So A i's, b i's are independently chosen, sigma is a random real analytic function, then the following statement holds. Following statement holds, which I don't prove here, but I try to convince you that it's true that any polynomial in differential operators applied only to the identity, not to any function, only to the identity not equal to zero actually means that all coefficients vanish. This, of course, has to do with the fact that the sigma are random real analytic functions, so they should not satisfy. Functions, so they should not satisfy any finite polynomial relations, and these are random with respect to Lebesgue-Measure objects here. And in this case, you can now say that the solution of this equation for any X and for any S contains now precisely the same information as signature in the sense that you can approximate any signature component by linear. Component by linear combinations of Xt for different S's and for different T's. Right? How is the proof working? The proof is, well, there's first the topological argument, generic vector fields are, if you have an appropriate metric structure, of course, meager in the set of all non-generic vector fields are meager. So, I mean, you have said Tricia that you know you have about two. Um you have said Trisha that you know you have about two minutes left. Yes, no, I need uh I'm already uh finished here. Uh, so they are media in the set of all uh vector fields and they can also be constructed by random procedures and this construction by random procedures would be now a little algebraic argument which I try to write on this on this last slide. It is just that you have to take your You have to take your random function, random real analytic function, take the derivative, insert in determinants d times k1, and show that if it is randomly chosen with an appropriate randomness, you will never have a relation satisfied. This, of course, can be done, and therefore, this is sufficient in order to get a statement of this type here. Of this type here, which guarantees you that actually these vector fields are generic. And this closes now the circle to random nice signature before. If sigma was activation function identity, s equal to one, it would be precisely a randomized signature, which only contains approximately the information of signature. This compresses the information of signature. It's not the same information. Information of signature, it's not the same information. And here, if we can allow S to be chosen arbitrary and we have a random activation function, we actually have the full information of signature. So that's another representation of signature in terms of flows of differential equations on Rk in this case. And now I'm finished in order not to overdo with the patience of my channel. The patience of my chairman, I provided two views on randomized signature. One is from a compression point of view, the other one is representation theoretic. And this shows a link between signature, as we know it defined on the algebra, signature now defined via a random dynamical system on RK and recurrent neural networks. That is the usual use in machine learning. Thanks a lot for your attention. Attention.