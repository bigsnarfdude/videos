First off, it's by European who is at Imperial Couch. Now, I would say actually points, right? So, this is your third talk on potential gains. Okay, so thank you everyone for being here for my talk. And first, I would like to thank the organizer for the kind invitation. And thank you, Urfa. And thank you, Urhan, for the kind introduction. So, in today's talk, I'm going to present a recent work with Singo. Present a recent work with Singuo from UC Berkeley, where we try to introduce several analytical frameworks to study dynamic potential games. And on Monday, Xin has already given a very nice talk about how potential game approach can simplify the computation of Natchez equilibrium for dynamical games. And in today's talk, I'm going to focus on dynamic games with closed-loop controls. And I'm going to give you more technical details about this approach. This approach. So, a quick introduction. And as you all know, the M-player games play an important role in social science, finance, and also engineering. And these type of problems models competition between several players, where each player optimizes their objective functions based on the strategy of other players. So, in many cases, we are interested in Nash equilibrium, which is essentially a collection of strategies where given the strategy of other players, Strategy of other players, no player has the incentive to deviate from this pre-specified strategy. And as you can already see, this definition of Nashi equilibrium introduce a strong coupling between each player's strategy. And hence, it is well known that the Nashi equilibrium is difficult to analyze and also it's difficult to compute because of this strong interaction. And often people need to use some fixed point to show the existence and then. point to show the existence and then numerically there is often no guarantee that certain algorithm can find Nash equilibrium. So as a result it is very natural to search for subclass of games where this computation of Nash equilibrium is more tractable. And Minfield game is a particular approach for large population games, but they essentially simplify the computation by assuming the weak interaction and almost homogeneous. Interaction and almost homogeneous between each player. And this type of assumption is typically quite restrictive for practical applications. So another tractable of unplayer games, which has been introduced by Moldera and Shapley in 1996, which is called Inno Static Setting, which is called the potential games. I think by far you have already heard this concept several times. Several times. So in a static setting, they consider n players with potentially heterogeneous players. So let IN be a finite index set of n players. In the static setting, each player optimizes over a certain action set, AI, and they have their objective function, which is essentially a mapping between the joint action profile of all players to a real number. So this is just a general setup of. So, this is just a general setup of static M-player games. Modern and sharply focused on a special class of M-player games, which there exists a potential structure. What it means is that there exists a function, phi, which maps from the joint action profile to a real number, such that this phi predicts the change of each player's deviation. So, you can see from this identity, this essentially shows that if player I shows that if player i unelectro deviate from their action, then the change of player i's value function is essentially the same as the change of the potential function phi. So this framework become a very popular framework to study static n-player games with heterogeneous players because you see that from this setting, there is no assumption between each the homogeneous assumption of each player AI. Assumption of each player AI, and there's no homogeneity between each value function, and you don't need to send n to infinity. So, because of this existence of the potential structure, you can actually verify that if you find, if you optimize this potential function, then the optimizer of this potential function gives you a Nash equilibrium of the original game. Okay, so essentially, you don't need to apply a fixed point argument to show the existence of the Nash equilibrium. And then there is essentially you can. And then there is essentially you can do this finding the Nash equilibrium becomes an optimization problem. And also, because of this potential structure, it is shown that many decentralized algorithms can converge to the Nash equilibrium. By decentralized, I mean that each player optimizes their own actions based on their own objective function, and there is no control about other players' action. And as you can see, that in general, this Can see that in general, this case more or less, each player are essentially optimizing over a non-stationary environment because other player is also because all other players are changing their actions. And in general, such decentralized algorithm may not converge to the Nash equilibrium. Essentially, this algorithm can form a circle across a Nash equilibrium and never converge, and they can possibly diverge. So, but if you know that this is a static potential game, then this, and you can show that because Then this, and you can show that because of this potential function, it can predict the change of each player's value function. And then essentially, you can show that this type of algorithm can work. So then there is a very natural question is that whether we can do this thing, the similar things for dynamical game. And this indeed has been done by recently by some computer science, which essentially they extend this potential structure to discrete time state dynamic with. Time stay dynamic with Markov transitions and Markov policies. And they call this type of game Markov potential games. And as you can see, this essentially become very popular framework for analyzing multi-agent reinforcement learning problem. Because that in this multi-agent reinforcement learning, it is very natural to optimize over so-called closed-loop controls or policies, where each agent is going to observe the corresponding system state. The corresponding system state, and then has no control about how other players are doing, and then they just optimizing according to their own value function. And with this potential game structure, you can show that many decentralized algorithm converges. For example, a policy gradient algorithm converge, base response algorithm converge. And if you don't have, if you know that the game is not a potential game, there is this type of algorithm may not converge. There is this popular. This popular or famous observation that there is this so-called algorithm collusion, where this multi-agent equilibrium multi-agent learning algorithm can produce something which is not the equilibrium of the original game. However, there is a missing piece about all these more computational approach papers. In a sense, there is limited study about the analytical structure of Markov potential gain. So, in particular, there are very few examples of There are very few examples about market potential games beyond cooperative game. So I will introduce this cooperative game on a general version of a cooperative game later. But you can think about this. In this game, all M players, they are trying to optimize the same objective function. So there is no competition actually. And in particular, you can see that if you have competition, and if you consider Markov gain with competition, Markov game with competition. And then, even at each state, this Markov game, you are essentially a cost function, is a static potential game. In general, this dynamic gain may not be a Markov potential game. So it seems that there is basically there is no systematic approach to certifying whether a given game is a Markov potential game. And also, there is no analytical approach about how to construct a potential function. A potential function. Because if you're coming from a control perspective, not from this learning perspective, and then this whole idea about Markov potential game approach is to simplify the computation of Nash equilibrium to an optimization problem of the potential function. So, but if I give you a game, there's no way to construct this potential function. This Markov, this potential game approach seems to be less interesting. And this is one. interesting. And this is what we are going to do in this paper. So we are going to provide some analytical characterization of general market potential games with general dynamics, possibly discrete and also continuous, and general policy class. And so for different characterization, using this characterization, we are able to identify a new class of continuous time Markov potential gain. Markov potential gain, and we are able to construct the potential function analytically. And this analytical tool is useful even though the original for the general M-player games, which may not be a Markov potential game, can be, but can be close to a Markov potential game. And we call this alpha a potential game, and the detail has been already discussed in Sin's talk. So I'm not going to discuss much about that. So you can think. That so you can think about this work, it's more like lay down the analytical tools for unplayer for to characterize dynamic potential games with closed loop policy. So in this work, we essentially consider a general dynamical game with closed loop controls. So here we have the usual, we have the index side in. Usually we have the index side i n which is just some finite number finite n numbers because we have a dynamic game we have s which indicates the system state at this moment it's just a general boreal space and then we are each player is going to optimize over feedback controls or the closed loop controls where we are in particular they are going to optimize over a certain measurable policies which we denote as a map between the Denote it as a map between the corresponding system state and to each player's action set, AI. So there's no assumption about whether these AI are the same or not. And then we, as usual, we deny this product pi n be the set of the joint policy profile of all players. And each player is going to consider optimizing pi i over for the optimizing certain value function, which is essentially a function mapping from the system state. From the system state and also the joint action profile. So here we use this symbol VSphi to denote the player I's expected cost if the underlying system state starts from a particular state S and all players take the policy profile phi. So here we essentially, if you consider a finite horizon game, then this S essentially including both the time and the space variable. So basically this is enlarged the state space. Space. And for simple, we don't loss generator. We assume that each player minimizing the value function Vi over the admissible policy, admissible policies. But you can equivalently consider maximizing a value function. So here as a comment, I didn't specify how the value function are defined, depending on whether the underlying system is a discrete time or continuous. Essentially, this is just depending on different definition of the VI. Definition of the Vi. Also, I didn't specify what are the regularity conditions of pi i and also vi, so I'm going to emphasize the additional regularity of the if they are if they are needed for our characterization. At this moment, it's just some measurable function as long as these are well defined. So maybe some integrability condition if you consider a continuous time setting. So I'm going to present the first characterization. Okay, oh, okay, I need to introduce the Markov potential gains. I introduced the Markov potential game. Sorry. So, here we call, according to literature, we consider a general game, a Markov potential game, if there exists a potential function such that this potential function is going to the change of each player's value function is equal to this potential function. So this is just extension of this definition of Markov potential game from the discrete games to general games. And as you can imagine, that this imagine that this with the help of this potential function it simplifies the computation of Nash equilibrium to a minimization problem in the sense if you are able to minimize this but if you are able to find a policy profile which minimize this potential function then this potential function is a mark of Nash equilibrium for the original game and essentially this essentially the computation of the Nash equilibrium become a control problem of the A control problem of the potential according to this potential function, provided that this potential function can be fine. Okay, so then there is a very natural question. If I give you a game, when a game is a Markov potential game, the second is that how do you construct a potential function given this game? So I'm going to present the first characterization. The first characterization is more like an algebraic characterization. It does not require an It does not require any differentiability. What it essentially says is that if a game is a Markov potential game with a particular potential function phi, if and only if each player's value function can be decomposed into two terms. The first term is just a potential function which depends on all players' policy profile. And there is a profile. And there is a remaining term, a remainder term, UI, which depends for the for player I's remainder, it only depends on other players' policy profile, but does not depend on, does not necessarily depend on player I's policy. And in general, this remainder can be different for different players. So this is more like what can I call an algebraic decomposition in the sense you don't need to have differentiability. And based on this And based on this decomposition, you can see that a cooperative game is a Markov potential game. So, by cooperative game, or people in the literature, it's also called a team Markov game, which essentially says that all player have the same objective function, they have the same value function, but they may optimize over a different strategy class. And in this case, you can simply set this potential function to the common value function, and then there is no remainder. function and then there's no remainder the remainder is zero and then you can see that this is a markov potential game and actually this is the most widely studied uh markov potential game in the literature and then beyond that there is no systematic uh characterization about the mark uh about uh markov potential game and in our work i'm going to find a new class of uh markov potential game based on this decomposition uh character criteria Character criteria. So, in particular, we are going to focus on so-called distributed game, where in this particular game, each player state, they have the decoupled state. Each player state is going to evolve according to their own private state. And each player is only going to optimize according to their own private state. But all the players can show it's going to couple only through the cost. So, in particular, I'm going to consider a finite horizon. going to consider a finite horizon game where i as before i as i introduced before i'm going to introduce the the enlarge state both introduce the both the time and also each uh the product about each individual private state each player going to have their own action space and i assume for simplicity it is a set of rn and each player is going to optimize so-called a distributed policy where essentially it's only a function mapping from the time and also the Mapping from the time and also the private state to the corresponding player's action set. And each player has their own private state process, XI, which evaluates according to this controlled SD. Both the drift and diffusion can be controlled. But the key thing here is that the drift and also the diffusion only evolves according to their own position. And also only evolutes according to their own control. According to their own control. So you can think about this as more like competition between several companies and they are producing the same, a similar good, okay, a similar product. Each company's state is essentially the magnitude of how many products they are going to each come and essentially the product line about each company. Of course, that each company, the basic production of each company, they can only depend their own basically their own production. their own production management and then they are going to going to do feedback control based on their own observed system state. And then but the essentially the performance of the of the product is essentially depending on the total number of products available in the market. And which is essentially given in the cost function. So which are the basically the each company their value function depends on a function depending on cost depending on the population Depending on the population state and also the population control process, and there can be a terminal cost which depends on the terminal value of the population state. So, for this distributed game, basically I'm going to apply this particular decomposition criterion and to see when such a distributed game can become a Markov potential game. At this moment, there is no differentiability assumption about the BI. Differentiability assumption about the bi and the sigma i. Okay, so they are just some general measurable function as long as the state and the value function are well defined. So, but in particular, you can see that for this distributed game, because of the state are decoupled, you can show that if the running cost and terminal cost has this potential game, has this potential structure, then essentially the dynamical distributed game become a Markov potential. Distributed gain become a Markov potential game. And you can define this corresponding potential function by using this common function f and g from the terminal cost and also the running cost. Essentially, what it says is that essentially to construct a potential function for the dynamical gain, which essentially in general is an infinite dimensional optimization problem, you essentially, because of the distribution structure, the constructor potential. The construct of potential function becomes a finite-dimensional problem. You just need to construct the potential function for the cost function. And then you can essentially construct the potential function by essentially integrating according to this cost function. And you can see that essentially this criteria allows you to simplify the computation of the distributed game with the potentially heterogeneous players to a distributed control. To a distributed control problem, where the control objective function is essentially depending on the potential function of the cost, of the original cost, and but the state process is essentially the original state space. So there is no lifting about the, you don't need to enlarge the state space. You just need to essentially, this control problem has the same dimension as the original game. Okay. Of course, then there's a very natural question. How do you find this? A very natural question: How do you find this F and G? Okay, then we are going to give for special case. I'm going to give two criteria about how to find this F and G. For example, if you have this running cost F and G, which essentially have this particular mean field interaction, then you can essentially, the F and G can be defined by essentially some aggregating about all players' cost function. So here I'm essentially assuming a player. Essentially, assuming the player fi they have their own the cost function have their own private individual cost and also have their interaction. And this interaction, each player I potentially can interact with the different players. But this interaction essentially have this, have the certain symmetry conditions. Essentially, it means that the player IJs for different two players, the players IJ's interaction, essentially helps a certain symmetry condition. Essentially, it has a certain symmetric condition. Then, essentially, the cause function, you can see that this potential, the cause function has a so-called potential structure, and then this can be fine by just aggregating according to each individual cost. And in the existing literature, people often focus on the even function, about every one has the same have the same integration kernel. And then you can see that a particular And then you can see that a particular case is this synchronized game studied recently by Kimona and Sona. And then you can, if you have different interaction, this potentially includes a suitable games or a directed graphs. You don't need to assume a send n to infinity. So there's no, and you don't need to pass to a graph limit. And in general, if you don't have such if you don't have such a structure perform, we essentially propose a differential characterization about how to construct the function. Assume that this running cost function are twice differentiable and it satisfies this symmetry condition of their hygiene, then you can essentially construct the potential function f and g by essentially using certain antiderivative about their first order derivative. Okay, a key take-home. Okay, a key take-home message is that this high-shain function, you don't need to assume homogeneous about all players' high-chain function. Only pairwise, we only need to assume that the high-shain of the cost to be symmetric between any two players. You don't need to assume homogeneous about any three players. So essentially, maybe it's good to summarize a little bit at this moment. So here we have decomposition characterization. Have decomposition characterization. Based on this decomposition characterization, we are able to show that if you have a distributed game where all the system and controls are decoupled, then you can apply this criteria and characterize the Markov potential game. Now, of course, the very natural question is what happens if the system state are coupled? In this case, it's not easy to use this decomposition, and we are going to use propose a different second criteria. Propose a different second criterion based on differentiability of the value function. So, the motivation of this differentiability characterization is coming from the static game. So, in this seminar paper by this model and Shapley, they consider a one-dimensional static potential game, where everyone is optimizing over a y-dimensional action set, and they assume that all players' value functions are twice differentiable. They show that if the value function of the If the value function of the any two players satisfy this has a symmetric Jacobian, then you can construct a potential function of this. This original game is a static potential game. And you can construct a potential function based on this antiderivative of each player's value function, a first order derivative of each player's value function. So actually, you already see a similar construction here. A similar construction here. We essentially just apply this criteria to this dynamic distributed game. Of course, there is a very natural question: if it's not a distributed game, can we apply this extend this criterion to dynamical game? The technical question is that what is a suitable notion of the derivative in order to with respect to this policy class. Okay, so here we essentially introduce this so-called linear derivative. Introduce this so-called linear derivative. Sin has already discussed this in the own mandate. Essentially, this linear derivative is just perturbing the value functions, the change of the value function, if player I unilateral deviates from their policy. So you can see that if the value function is only a finite dimensional, then this linear derivative is just the classical derived multiplied by the change of the perturbation. So here Of the perturbation. So, here this is more like an extension of what happens for this infinite-dimensional setting. And then here we don't need to impose a topology on the infinite dimensional policy class because it's not so clear to us what is a suitable topology on these measurable functions. And you can define this second-order derivative similarly by perturbing the first-order derivative. Then, with this notion of the linear derivative, Then, with this notion of the linear derivative, you can extend this notion of this differential characterization to the dynamical setting, where essentially if all the player's value function has a symmetric Jacobian or symmetric Haitian, then the corresponding game is a Markov potential game, and you can construct the corresponding controls objective function or the potential function using the first-order linear durity. Okay, so in the remaining five minutes. So, in the remaining four minutes, I'm going to discuss how do you apply this, compute, apply this criteria for the continuous time games. In particular, I'm going to consider a general game, a general continuous time game where player I might try to optimize this general value function depending on the population state and also the joint control profile. And then there will the population state will just satisfy the. Population state, we will just satisfy the general controlled SD with general controlled drift and controlled diffusion. Then, mainly the question is that how do you compute this first order derivative? So you can see that there is essentially there are two approach. You can stick to this probability setting and then use a probability approach. What you need to do is that you just need to write down what is this perturbed value function. Where this perturbed value function is just value function is just you perturb the player i's policy by some epsilon and then you can see the corresponding perturbed state and also the perturbed uh perturbed control profile and what you do is that you just differentiate this value function with respect to the parameter epsilon okay and you set the parameter epsilon to zero and then by using if you have this value function uh cost function and also the policies are twice differentiable then you can essentially do Then you can essentially do a chain rule and you can write down what is this first order linear derivative. They are nothing but just it's here, it's just a chain rule. I differentiate the cost function over the state and also I differentiate the state over the parameter. So here this x, this different this x is just a pathway sensitivity of the corresponding state with respect to a parameter, with respect to player i's policy, which satisfy a linear SD because you are, if you are. A linear SD because if you are familiar with the maximum principle, you already see that in the maximum principle. And the corresponding A is just the perturbation of the control process. And here, in order to apply this characterization, you need this policy to be at least to be differentiable because it will appear in this. Because when you differentiate the cost the control function, you need to first differentiate the policy. To first differentiate the policy and also differentiate the state, so there will be a chain rule. So, if you don't have a differentiable policy, if you just want to work with the general, and for example, Lipschitz policy, you can use a corresponding PD approach. What you need to do is that you just write down the corresponding Feynman tech representation of the players of the value function. And then, because here we are optimizing over a Markov policy, you know the value function satisfies a linear PD and then the Pd. And then the corresponding perturbed policy is just satisfy the linear PD with the perturbed coefficient. And what you need to do is that you just need to differentiate this PD over the parameter epsilon and send the parameter epsilon to zero. And then here, this is that you are going to see the linear derivative going to satisfy a linear PD where it involves this Hamiltonian with this perturbed Hamiltonian. Okay. And you can see that here, this derivative is only Here, this derives is only related to the derivative of the value function, but not the derivative of the policy. In general, this approach allows you to work with general non-differentiable policies, maybe just holder continuous, but the rigorous analysis, which requires this value function to be differentiable, more or less required of the non-degeneracy condition of the state process. So, maybe it's good to summarize. Here, I propose a potential game framework for general dynamical games. game framework for general dynamical games over a closed-loop policy. It allows you to simplify the computation of Nash equilibrium to the minimization of the potential function. We propose a tool characterization and that which allows us to show that the distributed game is a Markov potential game and general game you can also characterize using linear policies. And this analytical tool has been applied to characterize general end-player games which has been showed which has already been given in Sin's talk. The first talk is based The talk is based on the first reference and the second part, which is essentially how we apply this approach to the general employee. So thanks for your attention. Any questions? How about the sound games? So here you consider that there's a game. So, here you consider that there's a game, say, with respect to time zero. How about subgames? Can you repeat? I didn't hear the question very well. How about the subgames? Here you consider the original game. How about the sub games? A cyber game. No, subgame. Say you look at the game at each slave each time, not just the initial time. Not just the initial time, initial state I see that okay, I think here this is what I mean by this Markov potential game, right? So here you see that this by the Markov potential game, this potential function works for general, works for general initial position. So here you see this by when I say this is a Markov potential game, this potential function works for arbitrary initial time and initial space. Of course, this is just This is just characterized whether a Markov gain is a Markov potential gain. It does not discuss how do you solve this potential, minimize this potential function. If you want to find a minimizer of this potential function, it will become much become trickier because this control problem become trickier. So here our characterization, in short, our characterization works for general games with a different time and the initial state. And then this Different time and the initial state. And then this potential function, as which essentially you can, if you go, you see this potential function, it's already indexed by the s, which includes both the time and the space variable. Thank you.