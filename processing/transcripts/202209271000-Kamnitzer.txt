Thanks very much for the invitation to speak. Sorry that I'm not there in person. And so, and happy birthday to Bernard. And in fact, this talk is sort of based on the last conversation I had with him, which was in 2019 when I visited him in Cannes. And well, I can't really report that much progress since then. I can't really report that much progress since then, but I'll explain what the problems are and background. Okay. Please let me know if you have trouble hearing or any other problems. And of course, feel free to interrupt with questions. Okay, so let's begin. So, of course, I come from more from representation theory than cluster algebra, so I'll begin there. So, let's start with a semi-simple group over the complex numbers. simple group over the complex numbers. So the canonical example is the group SLNC. Then as usual we have its set of dominant weights which for SLNC would be sequences of decreasing integers. To each dominant weight we associate the irreducible representation V lambda. And these representations, there's two sort of basic things you do with them. The first is you have a weight decomposition. Weight decomposition. So you decompose the representation under the action of the maximal torus of our group, for example, diagonal matrices when it comes to SL and C, and decomposes your representation into weight spaces. So from just a fixed notation, I'll write those as V lambda mu. And the other thing that's related, as we'll see, is you can take two different representations and have formed their tensor product. That will decompose as a direct sum of other irreducible representations with some tensor product. Representations with some tensor product multiplicities. So, a basic question in representation theory would be to determine combinatorial formulas for these weight and tensor product multiplicates. So, this problem is solved, but somehow was not solved that long ago. So, in SLN, the solution is older and involves counting certain young Tableau. Young Tableau. But for general G, a nice general way of solving this problem was only accomplished by Berenstein-Zalovinsky and by Littleman in the 90s, basically. So let me review the idea which led at least to the work of Berenstein-Zwelinsky and sort of closely related to the work of Little. So the idea which proposed, I think, by Gelfand. By Gelfand and Zelovinsky, maybe in the 80s, was to find bases for representations which were particularly nice. So, particularly nice means adapted to these multiplicity spaces. So, it's clear what I mean when I talk about weight multiplicity spaces, but it's a little less clear what it means for tensor product multiplicity spaces. So, let me explain that. So, remember this C lambda mu nu, that's our little Richardson coefficient. That's our little Richardson coefficient, the tensor product multiplicity. And we can encode it as a vector space. We take the set of all homomorphisms of G representations from an irreducible VÎ½ into the tensor product. So this is a vector space. This Hom space here is a vector space whose dimension is the tensor product multiplicity. And there's a neat trick which embeds this tensor product multiplicity space. Tensor product multiplicity space inside one of the representations, inside one of the irreducibles, of at a certain weight space. And the idea is this HOM space, a vector in this Hom space is determined by the image of the highest weight vector in the representation v nu. That highest weight vector goes to something of the form v tensor v mu plus other terms. And you just remember this v. So that's the math from left to right. It takes the highest weight. From left to right, it takes the highest weight vector to just v. Or slightly more formally, it takes phi, thought of as a homomorphism, to you apply phi to the highest weight vector v nu, and then evaluate with v nu star in the second factor, v mu star in the second factor. So some dual vector dual to v nu. In any case, that embeds this hom space into this weight space of v lambda. And in fact, it's easy to describe the image of that embed. To describe the image of that embedding, the image is all vectors in that weight space, which are killed by certain powers of the Chevrolet generators. So we have Chevrolet generators inside the upper triangular part of our Lie algebra. So in the case of SLN, so our standard example, the Chevrolet generators look like these elementary matrices with a one just above the diagonal. So our condition is that we're looking for those vectors. Is that we're looking for those vectors which are killed by a certain power of each Chevrolet generator. The power is determined by the pairing between mu and the coroot corresponding to alpha i. So this is a relatively elementary lemma, but the advantage of this lemma, it means that when we want to describe this tensor product multiplicity, it's really about describing the representation, just one single representation. Representation, just one single representation v lambda, one single weight space, even, but we have to pay attention to the fact that we want vectors killed by certain powers of these EIs. So this motivates the following definition. A basis be for representations called good if it's a weight basis, so compatible with the weight decomposition of the representation, and compatible with all these kernels. So, all possible kernels of powers of the Chevrolet generator's EI. So, compatible means its intersection with those subspaces gives you a basis of those subspaces. So, that's a notion of good, again, introduced, I think, by first by Galfan and Zelovinsky. And the advantage of this good basis, as I sort of said before, it will give you. As I sort of said before, it will give you a basis for every tensor product multiplicity space. So, because it's compatible with all these kernels and because of this lemma above, it therefore will give you a basis for every tensor product multiplicity space. So, if you have a good basis, you can find the tensor product multiplicities just by counting basis vectors. And effectively, that's what Bernstein's elevation. Bernstein Zelovinsky did. Okay, so this notion of good basis is not quite good enough for our purposes. We need a stronger notion. And in the last talk, Fan talked about bases for cluster algebras. Later in the talk, we're going to come to a comparison a little bit between the cluster algebras and the bases in representation theory, these bases. One warning is that this good is not going to correspond to good bases in the cluster algebra sense. Classroom algebra sets. So, in order to formulate the next notion, which is an enhancement of the notion of good basis, I formulate first the following definition. So for any vector, we're going to nil potents degree with respect to the Chevrolet generator EI. So epsilon I of, the maximum number of times you can apply k and not kill the vector. So here's v. If you keep a So here's V. If you keep applying this Chevrolet-Genner EI, it keeps raising the weight. Eventually, I'll get to zero. But you remember how many times you can apply EI before you get to zero. That's epsilon I of V. So here comes a sort of long definition. A good basis. So it's already compatible. It's already good. It's already a weight base. It's already compatible with kernels. But now we want an additional condition and we'll call it perfect. This definition is due to. This definition is due to Berenstein and Kajdan from the mid-2000s, although perhaps it was known before to Berenstein, Zelmensky, or other people. A good basis is called perfect if for each basis vector and each Chevrolet generator, so here I didn't say this before, capital I is the indexing set of the simple roots, and which is also the And which is also the indexing set of those Chevrolet generators EI. So for each basis vector in each Chevrolet generator, either the basis vector is killed by the Chevrolet generator or it's sent to another basis vector, almost. Or there's another basis vector called EI tilde of B. And EI of B is EI tilde of B times the scalar, that scalar, plus V. That scalar, plus V, where V has lower order. What does lower order mean? It means it has smaller nilpotence degree with respect to EI. So it's killed by a smaller power of EI. So the leading order behavior of EII of B is this captured by this EI tilde of B. Okay, so EI tilde of B is another basis vector. One reason why we really like these perfect bases is because every perfect basis gives us a very strong combinatorial structure. The structure is we remember the set, the basis itself, but just as a set. We remember the weight of every basis vector, and we remember the information of these EI tildes. So EI tilde is a partially defined map, that's why it's dotted arrows from B to B. From B to B for every I. Partially defined because it could have been the case that EI of B was zero, in which case there's no EI tilde of B. But when EI of B is not zero, there is an EI tilde of B. And it's possible to prove that this EI tilde is necessarily an injective map. And this combinatorial data allows us to define a combinatorial structure called a crystal. Called a crystal, sometimes called crystal graph. Let me show you an example. Okay, let's take the represent, take V to be SL3. That means V is SL3, the Lie algebra. And let me show you this here. I think of this SL3 as the 3x3 trace zero matrices. So capital SL3 is acting on the trace zero, three by three matrices. zero three by three matrices. So it's an eight-dimensional vector space because of the trace zero. There are nine basis vectors. Sorry, there are eight basis vectors, namely, let me not try to do two. There's eight basis vectors because this is nine-dimensional, then I impose trace zero. The basis vectors are you can have an elementary matrix, so just a one somewhere in the matrix and zeros elsewhere here, here. Here, here, here, here, here, here, everywhere off the diagonal. And that leads to these crystal elements. So these black dots, except for the middle, they correspond to certain elementary matrices. Let me just draw one so we see an example. So this black dot here corresponds to the following matrix. And then And then in the middle here, we have two black dots, and they correspond to the fact that the zero weight space, which corresponds to the diagonal matrices, is two-dimensional in this representation. And it turns out that to make the perfect basis condition satisfied, those two black dots must be chosen to be the following matrices. Following matrices. I should have prepared this ahead of time. I hope I don't make a mistake here. But I believe it's this matrix and the one you get by flipping it. I think these correspond to these two black dots. So that's the unique perfect basis of this representation. And then again, this graph structure that I'm drawing, this crystal structure, these arrows denote the action, these EI tilt. Action, these EI tildes, EI tilde, EI tilde, and E2 tilde. And they are, again, the leading order behavior of acting by EI in the spaces. So, for example, if we pick up this black dot here, under the red arrow, it goes nowhere. So, it's EI2 tilde is not defined, or you could say it's EI2 tilde zero. And under E1 tilde, it goes to the And they're E1 tilde goes to the sky. Okay. Are there any questions about that or the notion of crystals? Maybe it's not familiar to everybody. Okay, I continue. So we have the following remarkable theorem of Berenstein-Kajan, which I think deserves to be better known. So if we have any two perfect bases for a fixed representation, so one representation like this one, and I have two perfect bases. Like this one, and I have two perfect bases, possibly different bases. For this representation, there aren't two different perfect bases, but for other representations, there will be more than one possible perfect basis. And the theorem is that there's an isomorphism of crystals between the two crystals. So combinatorily, the two perfect bases look just the same. In fact, and if the representation is irreducible, this isomorphism of crystals is unique. So a lot of times people think about So, a lot of times people think about crystals as coming from quantum groups. I haven't mentioned quantum groups at all, nor will I. And well, maybe I will a little bit. But this shows you how crystals can be defined without appealing to quantum groups. It's just purely a property of finding these perfect bases for representations and examining their combinatorial structure. Okay, here are some problems which are not open. Problems which are not open. The first problem would be: how do we know, can we find such perfect bases? So, is there any way to see that representations have perfect bases? How can we construct them? And the second problem, which turns out to be a bit easier, is to give combinatorial descriptions of the resulting crystals. So, we have each representation giving us some crystals, some graph like this. How can we work describing combinatorily that graph? Combinants while they die graph. Okay, so problem one turns out to be quite hard. There's no elementary construction of perfect bases. Historically, the first construction is due to lustig. It's the dual canonical basis. There is a related construction called the dual semi-canonical basis, which, well, related in the sense it was discovered by Luz Degen soon afterwards, but actually it's not maybe less related than the name would. Not maybe less related than the name would imply. And then there's a third construction called the Mirkovich-Vologen basis. So the dual canonical basis uses the geometry of quarter varieties. The Mirkovich-Vologin basis uses the geometry of affine grassmannians. And the dual canonical basis has a number of different constructions. Maybe one approach is using quantum groups, another approach is using categorification, which Which Lucy's original approach involves perverse sheaves on certain space of quiver representations, that's closely related to the categorification approach. And I should also say that the dual semi-canonical basis was closely studied by Geis-Leclerc and Troor in a series of many papers. We'll come back to that a little later. But in any case, there's no easy way to construct these perfect bases, nor is there even Perfect bases, nor is there even a way to prove the existence of perfect bases without doing a lot of hard work of involving geometry or algebra. On the other hand, somehow it's easier to give combinatorial descriptions of the crystals. So there's many different approaches for such making such combinatorial models for crystals. I'll highlight two of them. One is the theory of Lilum and Paz, and the other is the theory of MV polytopes. Theory of MV polytopes. And well, since I'm giving the talk and I like MV polytopes, here is a picture of the MV polytopes corresponding to that crystal that we mentioned earlier. Okay, so based on that theorem of Verenshine and Kajdan, we know that these different bases, so I mentioned here three different perfect bases, these three different perfect bases. These three different perfect bases will all leave combinatorics. For any representation, they will give us the same crystal. And so then you might ask, well, is there any, are they the same as bases? And this was first proven, the difference, the non-equality of the dual canonical and dual semi-canonical basis was first proven by Kashiowara and Saito in the 90s. And then the non-equality of the dual semi-canonical basis. non-equality of the dual semi-canonical basis in Mv basis and also the MV basis and dual canonical basis was proven by myself and some co-authors quite recently. And the first counterexamples to these non-equalities come in SO8 and SL6. Maybe I just mentioned my co-authors, I guess. Pierre Beauman, Andre Nowski, who's our next speaker, Alan Knutson, and Calder Morton-Ferguson. And Calder Morton-Ferguson. One problem, which surprisingly is open, as far as I know, is to find the smallest example of a representation with more than one perfect basis. So these, as I said, these counterexamples occur again in SL6 and SO8, but and for some specific representation, maybe there's a smaller representation where there is also multiple perfect bases, but I'm not aware. Basics, but I'm not aware. So, this was all about studying one representation at a time. But one trick that we can use is we can group all the representations together. So the way we do it is we consider the upper triangular subgroup N of G. For example, when G is SLN, N is exactly the group of upper triangular matrices with ones on the diagonal. With ones on the diagonal. And we can embed all of the represent irreducible representations inside the coordinate ring of this upper triangular subgroup. By the following trick, we take every highest weight vector to the function one, the unit, the one element in this coordinate ring, and we take any other vector to the following expression. A function who assigns to a group element g or the following thing. You take the The following thing: you take the you act by g on v and then take the coefficient of the highest weight vector. For example, if g is also two, these irreducible representations are homogeneous polynomials in two variables of degree n, also known as sim and c2. And we can embed this polynomials of degree n inside all the polynomials in the obvious way just by Uh, in the obvious way, just by well, you get rid of one variable, now you just have polynomial and the thing, and that corresponds to thinking of n as being such upper triangular matrices. So, this process is relatively elementary. Just embed all your representations together. They don't sit separately inside this thing. If you were working with G mod n, then you would get the direct sum of all the representations separately. When you work at the cornering of n, then you get the representations. Then you get the representations, well, sort of sitting on top of each other. As you sort of can see in this example, because of well, this will become polynomials of degree homogeneous in two variables with degree n will get mapped over to polynomials of degree at most n in one variable. Okay, so now we're working with this algebra. We maybe have some more structures. So one structure we have is instead of just the left action, we also Action, we also now have a right action of n of its Lie algebra. And for that reason, we can consider two families of Chevrolet generators, sort of the original family, which are acting on the left, and the second family, which I'll denote EI star, acting on the right. And it invites us to introduce the following notion, which at first I think occurs in our paper, of biperfect basis. So a basis of this coordinate ring of n is called biperfect. Is called biperfect if it's perfect for both the left and right actions. So that means it should be a weight basis, and that means that it should be compatible with kernels of EI and EI star, and the actions of EI and EI star will each give you some other basis vector plus lower order terms. And then we proved an analog. So this is with Pierre and Alan. We proved an analog of the variant. Proved an analog of the Berenstein-Kajdan theorem. If we have two biperfic faces, then they're isomorphic as bicrystals. So, bicrystal, I just mean a set with two different crystal structures sharing the same underlying weight. And we also prove that if G is some small SL3, SL4, then this coordinate ring of N has a unique biperfect basis. SL5 is. SL5 is still open, but it's SL6 and SO8, the bifurcate basis is not unique because those special bases, those things, canonical bases, MB bases, so on, they'll give us biperfect bases for these groups, and they won't agree for the same reason they didn't agree before. So because of this. Because of this theorem, we can study what we call B infinity, which is the set with its crystal that you get, combinatorial object, you get from any bioperfect basis. And again, understanding this thing solves this tensor product multiplicity problem. For example, we can the tensor product multiplicity is the number of elements in this B infinity. B infinity with a certain condition on its epsilon, a certain condition on epsilon i star, and a certain weight. So it's entirely, entirely combinatorial expression that you can easily calculate once you know this crystal will be infinite. And I just throw in an example. It's not really related to the rest of the talk, but just for fun, if we take G to be SL3, there's a very nice picture of what this crystal looks like. So here I, the black dots, again. So here I the black dots again are the crystal elements, the blue arrows are the action of EIs, the red arrows are the action of EI stars, and I didn't draw the actions of sorry, the blue arrows are the action of E1, the red arrows are the action of E2. I didn't draw E1 star and E2 star, but you might be able to guess what they do. For example, this dotted guy is the action of this E2 star here. Using either or both. Using either or both of the theory of MV polytopes, the theory of Lustig data, you can prove that this B infinity is naturally in bijection with tropical points of the dual flag variety. And this was this idea maybe is a combination of the original work of Berenshine Zelovinsky and also myself, and then really formalized in the paper of Gantrav Shen. Paper of Grantov Shen from just about five years ago. And then from the viewpoint of the cluster algebras, this is like a special case of the Fock-Gantra of conjecture or expectation that bases of one cluster algebra will be parametrized by tropical points in the other dual cluster algebra. So here we're seeing tropical points of Langman's dual flag variety, plus a non-negativity condition here. A non-negativity condition here. Okay. Are there any questions about that? And then before I turn to cluster algebras and then a comparison of the two things. Okay, I'll continue. Okay, so let me brief for anti-cluster algorithms. Of course, I'm Cluster algebra is, of course, am the least knowledgeable of cluster algebras of anyone at the conference. So I apologize if I say things that are a little silly. So a cluster algebra, and I guess from my viewpoint, it will be upper cluster algebra that I'm interested in. I think of it just as a commutative algebra along with a collection of certain subsets, which are called seeds. And the algebra A contains the polynomial ring in any seed. Polynomial ring in any seed and is contained inside the Laurent polynomial ring in those variables. And another feature of cluster algebra, this is not meant to be some kind of definition, just telling you the way I think of them. One seed can be attained from another seed through a process called mutation. So if you have one seed, you have basis vector x1 to xn, then you can get a new seed, which keeps all but one of the basis vectors and changes one of them by a certain formula. One of them by a certain formula. And cluster algebra is the word seed, and there's a word cluster. And I tend to use them interchangeably, perhaps bad practice. Okay. So if I say seed, if I say cluster, I don't mean anything different. So one original motivation for development of cluster algebra by Fomey and Zelwinsky was precisely for studying bases and representation theory. Bases in representation theory. So, in particular, they prove, or Berenstein-Fomelsen proved, that this coordinate ring of n has a cluster algebra structure where each reduced word for w naught gives a cluster. So you get such a collection of variables like so for every reduced word for w naught. These are not all of the clusters, these are only some of the clusters. And w naught, by the way, it's the longest element of the value. And so when again, probably everybody knows this in cluster algebra. So when you have a cluster algebra, all of the cluster monomials are linearly independent. What does cluster monomial mean? So here is my picture of a cluster algebra. I have my various cluster variables. So I started with a single cluster. It's over here. It's this dotted outline here: x1, x2, x3, x4. X1, X2, X3, X4. Maybe I mutate once and I get this dotted outline in green. Then I mutate again, I get rid of X2, and I get this cluster over here. So that's my sum of my cluster algebra, some cluster variables. Cluster monomial means I take one of my groups, like the original group, and I look at all possible monomials in those variables. So that's a cluster monomial because I was made from this group. That's a cluster monomial because it was made from the Was made from the Yellow group over here. So, all those cluster minimums, so those will be linearly independent vectors. They're a candidate for being part of a basis. Here's an example, extremely simple example. I take G to be SL3 and N to be upper, this, of course, as usual, the subgroup of upper triangular matrices with ones and diagonal. In this case, there's only two. In this case, there's only two clusters. One cluster consists of the function x, z, and x, y, minus z. The other one consists of y z and xy minus z. These two variables are frozen variables in the language of cluster algebras. And then, well, when you do the mutation, you get rid of x and you get y. So the possible cluster monomials are monomials in the first group and the monomials in the second group. So these are all the cluster. Group. So these are all the cluster monomials. Here, A, B, and C range over the natural numbers. And these two cluster monomials correspond to the two reduced words for W naught. So this corresponds to the reduced word 1, 2, 1. And this corresponds to reduced word 212. It happens to be the case, well, many things happen to be similar in this case. Things happen to be simpler in this case. The first thing that's simpler is that there's only finitely many clusters. And also, another thing that's really special is that every cluster corresponds to a reduced word. Two reduced words, two clusters. That's very unusual. And another special thing, which is that these cluster monomials, well, they form together, they make a basis for this coordinate ring of n, and this coincides. ring of n and this coincides with this basis is easy to see it's biperfect and in fact it's the only bi perfect basis for c n so in this case we have an exact no problems an exact match of everything so what what kind of results do we have in general so here's a theorem i attribute it to guislicler shorr and to kang kashiwar kim o so the theorem is that the two of for two of the three For two of the three bases I mentioned before, dual canonical basis and dual semi-canonical basis, those bases contain all of the cluster monomials. So those cluster monomials, remember, that was a linearly independent set. So it's natural to expect it to be contained in a basis. And these guys prove that those two of those bases I mentioned before contain those customs. Maybe if I of course. Of course, this let me just. I just wanted to make a little sketch cartoon of this thing. So here I have list of monomials. And here maybe I have canonical basis, dual canonical basis. And here I have semi. Semi canonical basis. Okay, so remember those two bases are not the same by the cache-where side to counterexample, but they do coincide on a very big set of all-cluster minimums. And of course, since the way I've set up the talk, you probably are already naturally wondering this question, does the MV basis contain all of the cluster minimals? Terminals. So we have a little bit of progress recently in this direction. But these guys, Pierre Beaumont and StÃ©phane Boisson, Peter Lillman, proved that for all reduced words for W naught, which satisfy a certain condition, and I don't know of any reduced word which doesn't satisfy that condition. So as far as I know, this might be every possible reduced word. All the cluster monomials in the corresponding cluster lie in the MV basics. Cluster lie in the MV bases. This sounds really great, but you have to keep in mind that most clusters don't come from reduced words. For example, there's finally many reduced words, and as we'll see in a second, there's infinitely many clusters usually. So it's not as maybe great as it might be, but well, it's still a nice result. Okay, so a general fact about cluster algebras. A general fact about cluster algebras is that when your cluster of algebra is finite type, which means finally many clusters, then the cluster monomials form a basis. So, in those cases, for example, and another fact is that this Cn is a finite type, not very often, only for the Not very often, only for the following low-rank groups. That's types, if you like, that are to write in link in types. That's types A1, A2, A3, A4, and B2. And I guess this was met probably first, well, maybe by Bernstein, Fomi and Zelavinsky, or by guys like Clark and Schore, this observation here. So, in those types, we all know that this, well, these two bases certainly will have to coincide because they both coincide with. Coincide because they both coincide with cluster minomiums. If you were following my talk before, you might have noticed that I said before that for some of these groups, in fact, all but these two, there was a unique biperfect basis. So it looks again matching, and maybe there's a unique biperfect basis also for these two groups. So it's tempting to think that there is a unique biperfect basis exactly when the cluster algebra is a finite type. And in those cases, the unique biperfect basis is the basis of cluster. Make bi-perfect basis is the basis of cluster monomials. But well, I don't know that that's true because I don't know about SL5 and SO5. But I also, and even worse, I don't know any like conceptual reason why that should be the case. Besides that it seems like it would be nice. So, a general problem in the theory of cluster algebras, which basically was the subject of Van's talk, or Fan's talk, or and subject of many people's work, is if you're given a non-finite type cluster algebra, can you extend the cluster monomials to a basis of the whole thing? And this is sort of a very rough version of what Fan said. So if I have a cluster algebra with a given cluster, then any cluster, any element in that cluster algebra is called pointed. Called pointed if it's equal to a monomial in the cluster variables. This is to keep in mind is actually not necessarily a monomial but a Laurent monomial. So g is a integer, it's not positive integers. So it's a Laurent monomial in X's plus certain lower order terms. So that's what it means to be pointed. And a basis for our cluster algebra is called good if it consists of elements which are pointed. If it consists of elements which are pointed with respect to each cluster, this is again my version of my simplification of definition. And one good thing about good basis vectors is that they also have a nice combinatorial structure because they can be parametrized using these exponent vectors, which are called g vectors, which were sort of invented by Fohmin and. Sort of invented by Fomin and Zelovinsky. The G-vectors transform when you change clusters. So when you change this cluster to a different cluster, they transform according to a certain mutation rule that's invented by Fomy and Zelvinsky. So in this way, good bases for cluster algebras have a nice combinatorial parametrization using these g vectors. And even more precisely, It's expected that this good basis leads to maybe a known in many types too, in many cases, that there's a bijection in this way between good bases and the tropical points of the dual cluster variety. So this is called the Fock-on-Troff conjecture, which is now a theorem in many cases. Again, I'm not expert on this part. And a theorem of Fan is that every good basis contains all the cluster monomials. Now, as far as I understand, again, this is mostly from reading Fan's work, there are three constructions of good bases for cluster algebras. There's the generic bases. So these are bases made using cluster characters of generic modules in some cluster categorification. And the work of Geislickler-Schur proved that this generic basis was actually the same as the dual semi-canonical basis for Cn. So Cn, this dual semi-canonical basis constructed using equivalent varieties, and they proved that this construction matched the construction of generic bases. Second family of secondary bases. Family of second construction of good bases for cluster algebras is what is called common triangular bases. And one way to think about these is that think about they come from simple objects in monoidal categorifications of cluster algebras. And the work of these authors, Ken, Kashiwara, Kim, and O, related these common triangular bases to dual canonical bases. Canonical basis. And then there's a third family, which is the theta basis due to gross hacking kilican savage. So it's tempting to guess, since we have a third family of bases here, it's tempting to guess that this theta basis matches the MV basis. I just wanted here to point out that I haven't really talked about multiplicative properties of the MV bases or any of our bases. MV basis or any of our bases, but we did prove that this MV basis is positive for multiplication. So when you multiply two MV basis vectors in this ring, the result is a linear combination of MV basis vectors whose coefficients are non-negative integers. That's another piece of evidence in its favor. And now comes another piece of evidence in this favor, which is the substance sort of of my conversation. Conversation with Bernard three years ago. So we proved that in SL6 and SO8, the MV basis and the dual semi-canonical and dual canonical bases differ. And in these examples, they differ not just in like a random way, but a very precise way, like so. So let's take B to be the MV basis vector, C to be the dual. C to be the dual semi-canonical basis and D to be the dual canonical basis vector. And V is some other vector, which I don't want to talk about, but it's like some kind of frozen vector. It's in every basis. And the way the bases are different is that D is B plus V and C is B plus 2 V. So this MV basis vector is like the smallest one. You add one V to it, you get D, and you add two V's to it, you get C. And that's the way in which the three bases are different. Bases are different. And Bernard observed, had already observed, that the same pattern is true in rank two affine type cluster algebras, where the roles of B, C, and D are replaced with, well, the theta basis, the, I forgot the names, the generic basis, and the common triangular basis. Although, I think in these two types, Basis. Although I think in these two types, all three bases have like some different names, whatever, whatever, but they correspond to those basis vectors. For example, this theta basis in that type is the same as the greedy basis. That was proved by some large group of authors. Anyway, so Bernard observed, had already observed that this same pattern like this holds in those ranked two affine type cluster algebras. So it's natural, this is natural to think. This is natural to think again that this one should guess that this ambient basis vector could be the theta basis vector. So sorry, this is not quite out of place. This problem is slightly out of place. Let me erase this problem for a second. Okay, so we, anyway, so since the So, since this discussion from three years ago, we've been working, but not super successfully on this project of trying to, well, both prove that the MV basis contains all the cluster monomials, try to give the MV basis a kind of cluster, give cluster mutation a meaning inside the MV basis, and relate the MV basis to the theta basis. And I can't report very much progress, but a little bit of ideas in this direction was from this. If we look at the going back 10 years ago. If we look at the going back 10 years ago to the paper of Geist the Koch-Schore called Generic Bases for Cluster Algebras, where they prove the equality of the dual semi-canonical basis with the generic basis. And we adopt their ideas in the language of the MV basis, it suggests that we study certain chains of MV cycles. So I didn't talk about much about MV cycles, but MV cycles are sub-varieties of the African Grasmanian, which index these MV bases. Which index these MV bases. And from the viewpoint of this GLS paper, we should study such chains and try to classify exactly which chains can occur, what are the maximal and minimal chains, and so on. And if we can do that, at the very least, we'll get some progress. So that's sort of the state of the art in that direction. Okay, in my remaining time, I'll just report on a little bit of Just report on a little bit of state-of-the-art, just on the purely combinatorial side, which is just to observe that the theta basis and v basis really do share the same combinatorics. And it sort of was implicit in what I already said, because I already said that both of them will give a bijection with the tropical points of the dual phag variety. But I think the connection is like way stronger than most people realize. So I wanted to say it. So I had never seen exactly what I'm about to say written. Seeing exactly what I'm about to say written down. So, say we have a theta basis, or actually, probably any other good basis would do for this cluster algebra. Then, if we fix some reduced word, here's our reduced word for W naught, we can consider the G vector with respect to the cluster labeled by that reduced word. That will give us a bijection between the theta basis and a subset of Z to the M. I think in the usual literature, in the cluster. I think in the usual literature, in the cluster algebra literature, you would just get Z to them, but it sort of relates to the fact that the Z that N, the N is not really, this is not really a cluster algebra. It's really like, as in fan stock, compactification of a cluster algebra or whatever. So that's why you only get a subset here. And on the other hand, if we start with the MV basis or any other biperfect basis, there's a combinatorial construction called Lustig data. Construction called Lustig data using the same reduced word, which is a purely combinatorial thing. You can define it using that crystal B infinity that I talked about. And it gives you a bijection with n to the m. And so here is on the other hand, let's say, on the right-hand side, there's a, well, maybe many ways you might try to make a bijection, but here's a bijection between this. Between this and this. So the bijection takes some n, which is just a list of m natural numbers, and produce a list of m integers. And what's the definition? Every integer is the difference between two of the natural numbers. So that means the kth integer is nk minus n k plus. k plus means writing words, k plus is the. A plus is the next occurrence of IK in the word. So for example, if you took the standard word 1, 2, 1 for SL3, then 1 plus would be 3, because the next occurrence of a 1, of I1, which is 1, is 3, the third spot. Anyway, it's not very important. Anyway, it's not very important the exact definition, but just some combinatorial thing, very simple combinatorial thing. And if you essentially, this is contained in the work of Gens Klashavoy Schumann from a few years ago, is you could say there exists a unique bijection on the left side such that this diagram commutes for every reduced word. Sorry, it doesn't quite all fit. Okay, I'll just cut that part. Such that, so there exists a unique bijection here between Here between the MV basis and the theta basis. But it's not really a property of these specific bases, but actually would be true for any of these bi-perfect bases here and any other good basis here, making this diagram commute for every reduced word. So it really shows that the comet torics are matching perfectly on both sides. Okay, that's all I have to say. Thank you. Okay, so thank you, Troy, for this wonderful talk. And so are there questions from the online speakers or participants? Sorry. Hi, hi Joe. This is Hen. Hi. So concerning your last result, so have you checked that apply Shabbat generators to them, if I understand that correctly? So Francin, maybe you can repeat it. I think it was a little bit cut your we could not hear you completely. Okay. Okay, not sorry, maybe there are some problems. So yeah, maybe I maybe you can type your question into the chat or is it not too complicated? Sorry, sorry, it seems that my internet has some problems. So now I think. I had some problems. So now I think that you can hear me now. Yeah, that's good. The question is: have you checked that the theta basis is also a biperfect basis? No. Or maybe in SL5, I think in SL5 that can be checked directly, perhaps. How? So, so actually, so your last theorem is about this commutative diagram tell you about in SL5, I think it's. In SL5, I think it's in SL5, I guess the theta basis probably coincides with the other bases. So therefore, it is. Okay, yes, you are right. They are perfect. Yes, you are right. So maybe you should start with answering. But actually, it's an interesting question. And I don't know how to do it, which is to prove that the theta basis is bi-perfect in general. Yeah, you replace the action of the root operators by the product with some special basis element. Basis element, which hydrocretal is. It's pretty closely related to the action of the root operators. Like it's somehow dual to it. So if you can control the action of the root operators, I guess you probably can control the action of the multiplication by of the. Sorry, this is the action of the root operators. Sorry, say it again? So basically, you are your root operators. So basically, you so So basically you so you so in a in a dual semi-continuum basis or dual continuum basis actually I think you can root operators can be interpreted as you multiply another basis element. Well I understand this from a categorification point of view. So you multiply some basis element and this is kind of analogous to the root operators. EI is like multiply some something. And then you can try to do the same for the databases and see what And see what terms can appear in the decomposition into the theta bases. And you should have a leading term, and you should have a last term, and you have something in the middle, and you try to using this competitive commutative diagram to see what are these g-vectors appearing in the middle. Possibly, well, I think at least in small examples, it might be much easier to check than then you try to study the chain of the eyes. I think that is a huge project, but this is like a small project. Lack of small criticism. Yeah, it's a good idea to try to prove that the database is a bi-perfect basis, but I don't know. Okay, so are there further questions from online participants? Okay, if not, then maybe. So there's a question from Bernard. Hi, Joe. Can you hear me? Yeah, hi, Bernard. Yes, so I think I have already asked you this question, but maybe you made progress. So if you consider this MV basis, do you know how to, I mean, if you take the square of an element of the MV basis, when does it belong again to the M V basis? Does it belong again to the MD basis? Is this a kind of simpler question? In other words, what are the real elements? Are there some way of thinking about this problem? Yeah, I should not remember you asking me this question. Maybe it is. No, I don't. Anyway, I don't know the answer. Yeah. Yeah and it's a good it's a good question too but I don't know yeah and just to clarify also about what you said about the fact that semi-canonical basis and canonical basis I mean dual one differ so it is not due to Cashiwara and Saito actually in their paper they don't mention at all semi-canonical basis they they disproved another conjecture of Lucy Disproved another conjecture of Lustig on the irreducibility of singular support of purple sheaves. That's true. And then it's in our paper with in our first paper with Christophe and Jan that we found a discrepancy between semi-canonical and canonical. And it turns out that it is for the same vertex of the crystal graph. So we also noted that somehow it was a minimal example which could Of the minimal example, which coincided also with a minimal counterexample for this other conjecture which appeared already in Kashiwa's Saito, but they didn't compare semi-canonical basis with canonical basis in their paper. I see. Okay, good. Thanks. I'm just trying to find where I said that. Oh, here. Okay, good. Okay, so are there both questions? Yes, so correct. Hey, Joel, I was curious. So I know you have a characterization of the structure constants for the MV basis, but do you have a particularly nice characterization of the coefficients of a Laurentic expansion in a specific cluster? No, that's kind of a little bit related to what I was saying about those chains of the MV cycles. Cycles. Okay. But actually, one approach that I'm still curious about. And as you said, we have a characterization of those structure constants as, well, there's certain multiplicities in the valence and drinfeld degeneration of a product of MV cycles. Cycles. On the other hand, I understand that for the theta bases, there's some characterizations of the structure constants, maybe due to Tony Yu and other people as well. I'm not an expert, but I've seen it in their work. I was sort of like thinking that one could try to directly relate those two characterizations and structure constants, but maybe that's, and I did think a little bit about that, even talked a little bit about it with Tony, but maybe that's. Maybe that's very difficult. Yeah, I guess I had a more specific, a simpler and more specific question. Do you happen to know if the MV basis has the proper Laurent property? This is some property the theta basis has that the other ones don't? Yeah, I don't know the property. Why it's a property? This is that every element of the theta basis. Let me try to get it right, is either a cluster monomial in the initial cluster or every monomial that appears. Cluster, or every monomial that appears has some negative exponent. So if you're either your initial or every monomial is properly Laurent. And this is, it doesn't characterize the theta basis, but at least compared to all the other bases around, it's the only, the theta basis is the only one with this property. I don't know. It's possible it could be easy to show. That's very interesting. I'll take. That's very interesting. I'll think about that. Thanks. Thanks. Further questions? Further questions from online participants? If not, that's thank you Joel done. 