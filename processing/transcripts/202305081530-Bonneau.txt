Half genomics and half protein design. But for the last year, I've been really focused on protein design. So I'm going to give this a shot. And I think it's still relevant for this crowd because there's a lot of language model, generative AI, active learning. And then I can give away all the good parts of Natasha's talk, and hopefully she forgives me for that. This is obviously this talk. Obviously, this talk and all talks today are devoted to Cheko. Let's hope he becomes world champion soon. So, an introduction to me and where I'm coming from, and I think where a lot of my favorite things in this area of computational biology are coming from, I really think that there's a lot of power when we're able to, and it's rare, cross between the big modalities, right? And so, when can we figure out something about the evolution of a sequence family based on a pathway? When can we On a pathway, when can we use some positive selection estimate from a sequence to actually fold proteins? And for 10 years before alpha fold, right, that was the Debbie Marks, you know, main way of doing structure. Still important for assembling interfaces. So when can we cross over and get these different major, three major data types for biology to actually interact and really generate very powerful hybrid methods where the biases cancel out to some degree. The biases cancel out to some degree, and we get the right answer. And today, I'm going to really talk about sequence and structure working together to design proteins. So, I'll talk a little bit about where we're at, the context for what we're doing. We were a startup company and we're now acquired by Genentech and it's become a bigger effort than we initially thought it would. So, I'll introduce that. I'll talk a little bit about our lab in the loop, the active learning framework. Active learning framework. It's almost a lead-in to Natasha's talk tomorrow, but not quite, just one picture, don't worry. And then I'll talk a little bit about how we put it together to actually design new sequences. And one thing that's not, you know, the things that I won't really talk about, at Genentech, we've recently taken all machine learning for the different drug modalities, small molecules, peptides, and large molecules, and we've put them all into one department. All into one department, and we were this startup called Prescient, but now we're the machine learning for drug discovery department. And we think this is really exciting because there's a lot about the genetics and the sequence families for human targets. There's a lot about molecular dynamics. There's a lot of things that we use in generative AI that actually work equally well on smile strings and protein strings. You might have to tune them and train them differently, but a lot of the frameworks are the same. A lot of the frameworks are the same, and definitely things like what Natasha will talk about with active learning are very or multi-property optimization. A lot of the mathematics is at least very similar. And so, we think that there's going to be quite a revolution in drug discovery and the computational approaches to it based on just simply discharging the capacitor, taking these different columns that have worked in isolation and allowing them to work together. And we're basically seeing that a month into the adventure. But today, I'm going to only talk about proteins. teens. So prescient to some degree was something that something that came out of the lockdown. So we built some protein design codes and Kyung Young and Vlad called me up and said, we want to start a company based around this because all of our friends are starting companies, but I think it's because we were all locked in our closets and bedrooms and whatever, our mom's basements and things, and we were bored. So we started this. Board. So we started this, and less than six months after starting the company, we started negotiations with Genentech to become a client or for them to become a client and use our software. And those negotiations rapidly turned into them acquiring us outright. Since then, we've built up Prescient and now Machine Learning for Drug Discovery to be a 70-person unit that is handling most of the drug discovery. Handling most of the drug discovery and machine learning for Genentech. We also work very closely with Roche. That's a bit more complicated, but sharing data across these multiple companies that are part of the Roche family and trying to get these tools deployed across that set of two pharma is the key thing that we're trying to do. The thing that's interesting for today is that we actually have gone through now nine rounds or eight rounds of design. And so we actually get to see active learning in. In where we've actually, well, active learning, where we've done more than three iterations. My history with genomics is that we have a great bunch of students and postdocs. We do some experiments. In the interim, the sequencing technology gets 10 times better. Everybody graduates. We get one more round of students and postdocs to do one more thing. We maybe get to three iterations, but we never get to nine. So we're excited that we actually have something working. That we actually have something working. So, I'll talk about a lot of different things today, but I'll talk a lot about structure methods that include Andreas, Simon, and Jay, Natasha and Jiwan have done a lot of work on the active learning, the overarching framework. And then, of course, Stephen, Dan, Andrew, Kyung Young, Vlad, and others, Nate, have worked too many names to mention on a lot of our generative models. All right, so computational protein design. This is a crowd of biostaticians, and so you're constantly reading papers and seeing that people did things absolutely and totally wrong, right? Well, if proteins are in a 20 to the length of the protein space, you know, the reason I'm so relaxed is because everything, including what I'm doing, is definitely wrong, like guaranteed to be wrong. We're either rattling around inside local minima, optimizing. Minima optimizing something that's near a fold, or we're like guaranteably out to sea. Now, there's many different problems where there's many different problems where you can state the dimensionality in some trivial way and then make it sound like an impossible problem, but there's still many very practical approaches to the problem. But with proteins, they do have to self-assemble and then they have to function. They're very constrained systems. So, I think this is a case where this is an absurd statement of the dimensionality, but the Statement of the dimensionality, but the dimensionality is high enough that if we look at various prior methods, first of all, the space, the sequence space, is provably very functionally sparse. If we look at rational methods like Rosetta, you have to specify a fold and an objective that's in three-dimensional structure framework, and then you have to sample forever. And even for a peptide, it takes three days, right? And so this. Days, right? And so, this is a case where you could explore a local minima at the cost of 10,000 CPU hours and it would work. And a lot of a very large fraction of protein drugs have been through Rosetta just to stabilize them a little bit. And they didn't use Rosetta until they almost already had the drug. Directed evolution is also really powerful. But if you think about the number of things you can change at any given time, it's very small. It's maybe 13, 14. Very small, it's maybe 13, 14, some number. The number of variable residues in the interesting part of an antibody interface, for example, is much larger than you can explore with phage display. And so, all of these methods, you have to link things to structure or link them to fitness or selection. You can't always do that. And then, once you do that, you're also embedding things in a very, very, very complex space and really just exploring local minima. So, what can we do to get around that? And so, most of the original thought that went into the pipelines that we use now at GenITech came from seeing how far we could push the manifold assumption in the space of biosequences. And so this is, you know, this is definitely a case where the machine learning community took something that was known in several other fields of mathematics and then gave it a new name. And then gave it a new name. But so, what would you really call this if you were a statistician? Just the quiz, the manifold assumption from the late 80s. What is it called? Okay. All right. We did see something about PCA today, so there's a hint that this is not a new idea, right? But the idea is that most machine learning methods work, even if they're not expressly being used to reduce dimensionality. They work in such high-dimensional spaces and such. Such high-dimensional spaces and such sparse functional spaces that they have to be doing something that is essentially confining the latent space to a very small fraction of the total dimensionality of the problem, and that there is some smoothness or some ability to navigate through multiple wells along that space. And so what we think is really special about language models aimed at proteins is that they allow you to trans essentially transport yourself between local minima in the latent. Minima in the latent space much more effectively than you could in the original space, whether it's SO3 for structure or whether it's in sequence space or some evolutionary space. So what we did initially was to train what we called the manifold sampler. And so the idea is we used a method where we perturb and transform sequences. We also perturb selected regions in terms of length. Selected regions in terms of length, and then we have a decoder, and so this is something like a non-autoreaggressive encoder-decoder combination. The interesting things are that we have a length predictor and a function conditioning. So we have a length predictor, which can change the loops of any region, or if you specify just regions that are not selected to be preserved. And we also have a function classifier, which originally we trained on the entire go function. We trained on the entire Go function universe. At Genentech, it turns out they care about not all 2,800 functions, but they care about five or six really important drug parameters. And again, we're going to hear a lot more about how we really do this now. So we made some progress with this. The key thing here being that almost everything preceding this paper that we wrote, actually, just a few years ago, almost everything prior to that used something very cheesy for dealing. Something very cheesy for dealing with length perturbations, and almost everything really was fixed length. So you encode and decode the same length, but most of the interesting regions on proteins are loops, which alpha fold gets wrong, which are not fixed length, which have gaps and all sorts of interesting things. So if we look at multiple sequence alignments of tim barrels or antibodies, we do not see a situation where a fixed length autoencoder should do anything useful, really. The work that inspired a lot of how we put this all together was this paper that we called Deep Fry. So, and what was really special about this paper was we used this protein language model, but we used it in the context of graphs derived from structures. The structures were for the entire protein. Go annotations, 99.9% of them are applied to the entire protein. You don't know anything about design, you know. Protein. You don't know anything about design, you know, domains or sites. When we did salience mapping and GradCam and other things that let you interpret what parts of the sequence and structure were important for a given function prediction, we found that we were actually able to identify the functional sites on proteins in a very stable way and compare that with lots of databases that encapsulate protein sites and domains like PFAM, bio sites, and other things like that. And so this is a case, for example, where the whole protein Case, for example, where the whole protein went in. There was no example of this protein in the training set. All the examples of calcium binding proteins didn't actually specify the binding residues. They just specified the entire genes that had calcium binding or didn't. And when we do a stabilized version of GradCam or ensemble version of GradCam, we actually light up the loops that bind the calcium and, in fact, even have the substructure of chelation properly represented here. And this goes on and on. Represented here. And this goes on and on for DNA binding, metal binding, functions that have a structural or a punctate component to them that's understood. We were able to recapitulate a lot of that. This is what inspired us to give it a shot with design because we thought this isn't just generalizing, but it's actually learning something beyond what was in the training set. And that's what led us to then build our design method where we start with things that are, you know, in the beginning, we did things like ablate the entire loop. Like ablate the entire loop and calciumurin, ablate all the loops that bind calcium and also verify that we have destroyed the fold, right? The calcium binding loops are there to sort of modulate structure on and off. Then load in the function vector that includes calcium binding and take little steps on the manifold until the length predictor built back the correct length and also built back the chelating residue. And also built back the key lighting residues. We got excited here because not only were we restoring the sites that we destroyed, but we were also kind of making the folds more compact, improving Rosetta scores, and doing all sorts of other things just with the pure sequence generation part of this. And so the original versions of this method were things where we were using language models conditioned on function that had variable length and were not autoregressive so that we could actually generate samples and then put them through Rosetta or other methods that understood structure. Zeta or other methods that understood structure. And we decided that we would save the world and recycle plastics, break down toxic waste. And so naturally, we now have a company that just designs antibodies because that's where the money is. It's also what we really want to do deep in our hearts, right? We're not. But here's a picture of what we actually mostly work on now. Since moving to Genentech, we're mostly focused on. Genentech, we're mostly focused on antibodies. And again, antibodies have heavy and light chains, and in those, each the heavy and light chain, you have three variable regions. You also have a lot of action that happens in these green regions related to viscosity, stability, humanization. And so these are the regions that you typically modulate for affinity. And then the entire sequence is typically designed for a whole range of drug-like properties. And the three things that we do. The three things that we do, or everything that we do in the large molecule space, can roughly be designed or divided into taking a set of already known binders from the immunization of an animal or from a database or from a screen or a display library, and figuring out how to select different molecules that bind different places on the target protein to maximize your diversity with respect to activity. We also take known binders and do multi-property optimization. We'll hear about. Do multi-property optimization. We'll hear about that tomorrow to say that we don't just want to optimize affinity, but we also want to fix immunogicity, viscosity behaviors, oxidation, all sorts of things. And then the Holy Grail is, of course, de novo design. Paint something on the target protein, tell the user why they painted a region that was too small or too big, fix what they painted, and then design an antibody that binds there. And we have a lot of progress on all of these fronts. We also have challenges. We also have challenges on all these fronts because every time you make something better, when it's in an active drug discovery process, there's just huge demands on the engineering that goes into these proteins and also a lot of people testing things with a really tight design cycle. So we've got a lot of things working, but I don't think we'll ever have them working well enough. And the goal here is to actually get one way or another to a point where we never have to sequence animals. Animals bring in non-human animals. Animals bring in non-human things that we have to fix. Animals have limited diversity in what they can do, even though recombination is a very powerful method. It's nothing compared to the spaces we can explore with display technologies or with display technologies combined with computation. So we also would like it to just have just a small number fewer really expensive experiments in the loop, right? Those are very expensive experiments involving mice, rats, llamas. Involving mice, rats, llamas, and genetically engineered mice. So it's a lot of things there that we could fix if we could just skip that step. And because we're at Genentech, we do have the ability to take everything we're doing, put it through a method, which actually says, given the billions of sequences that the model likes, which are the ones that you should fill up a plate with to optimize model improvement and the chances of getting something interesting. We iterate this process, and we'll hear a lot more. Iterate this process, and we'll hear a lot more about that tomorrow. But this is a little teasier for tomorrow's talk, which is just that, you know, you often have different properties, and experimental design is an old field. We often want to do things that are fundamentally maximizing the volume of experience that we have as a classifier, as a data set, as a researcher. But this is very difficult when you have very strong relationships between the variables. Between the variables. So here we have yield and affinity. If you don't have any yield, how are you going to test affinity? If you don't have an affinity, how are you going to convince a biologist to do anything more with that molecule? And so this method, these methods end up having to deal with very strong dependencies and sequences between these variables, but they work. One thing that we seem to have a Cambrian explosion of, not just in the work that I've done. Not just in the work that I'm describing that we're doing, but just in general across the field, there's a huge diversity in the number of generative, regressive, auto, you know, there's just an endless number of different ways in which people use self-supervised methods to encode protein sequences. There's an endless number of ways people then connect those to design, but really we find that when you actually put molecular design as your end goal. Molecular design as your end goal, you find that very few of these methods that are out there have any real value. They rely on multiple sequence alignments, which there's a number of both tautological and technical reasons why that's a bad idea. They are slow at inference time, or they are difficult to fine-tune. And so, we have a number of different ways that we put these things together. Our manifold sampler right now has two main methods: one is the length. Two main methods. One is the Langevin manifold sampler, which is an energy-based method. We have something else called the walk-jump sampler, which we'll talk about soon, but we're not ready to release yet. Diffusion models play a key role. And then ultimately, many of these generative models feed into structure methods. And so sometimes we have the generative AI doing all the work. And other times we have it suggesting cassettes and mutations, which are then evaluated by things like Rosetta. So there's a number of different ways that we do that. Ways that we do that. This is now four weeks out of date, which in machine learning is ancient history. But up until maybe two months ago, the LMS sampler was our favorite. I think now it's still kind of unknown what's going on. But essentially, this stands for Langevin Manifold Sampling. This is a method that allows us to connect with energy-based methods. It seems to give good performance with respect to diversity. Good performance with respect to diversity. And it also gives us a lot of ability to preserve some segments and change the length or modulate the length of others. And so a lot of work that we're doing is actually fairly academic, just matching distributions of known antibodies, seeing if we can smoothly move between these on the manifold, and figuring out just the behavior of these when we feed these into a host of oracles that are quite demanding and quite different. We also have. We also have a pretty strong demand for structure-based methods, whether we're validating, whether we're docking and setting up the graph or the initial pose or the initial system for machine learning, or whether we're taking a known binder and trying to identify how to constrain further designs. There's lots and lots of different ways in which we need to model complexes and single proteins. And so we've built our own deep learning method for structure prediction. Deep learning method for structure prediction, and we built a corresponding suite of diffusion methods that also work to do inverse folding or design with that. Equifold, Equifold, and Multimer are the first methods that we published in this space. And the idea here, there's a few things that make this superior for design with respect to alpha fold two. One is that we don't use express torsion angles, we use these things. Torsion angles, we use these things called CG nodes, which are essentially ways of factorizing the structure into all possible triangles that span essentially centroid all the way to all atom. These CG nodes and the ways that we deal with these things are substantially more memory and computation efficient than the model in AlphaFold 2. But the other and probably the most important thing is that all of this starts with a single sequence and not a multiple sequence. A single sequence and not a multiple sequence alignment. And so, between the more efficient representation, some of the lessons that we learned by just reading the Alpha Fold paper and thinking, what if we tried things differently, and also essentially focusing on single sequences, we have a method that's substantially more useful for design. And I think, oh, I don't have the slide there. When we run this on antibodies, we do substantially better on the variable loops. Substantially better on the variable loops. And this is all released. This code is actually open source. And so you can check it out. Equifold, Equifold Multimer, and Equifold Diffusion are all different threads there. And it's still very early. This code is only about eight months old. And so if you want to check it out, feel free to contact me. And it probably does take a little bit of talking to Jay to get it to work. I think I'm out of time, so I'll run through this, but we also have. I'll run through this, but we also have a manuscript we can share here. This did not make it into ICML. Unfortunately, reviewer 3 killed it. I didn't actually see that. So that is a typo. It got really close, but it's definitely going to some obscure NURIPS workshop and everything will be fine. But again, here's the idea that we see with all of these various methods. We have ways of actually mapping what we do to this. What we do to this framework where we can add and subtract noise as ways of navigating the space and also navigating the different stages of the inference and the training process. But these methods allow us to do things that are truly de novo in the sense that we can obliterate part of the sequence. We can do things to design in regions of the sequence without a seed, without a starting point, without an instantiation. And so this is a case where we took And so, this is a case where we took trituzumab, which is an approved drug. It's got a reasonable affinity. It's not stellar. It also has an oxidizable methionine and tryptophan, which aren't perfect, and a number of other liabilities. This is just this H3 loop. When we blow out the loops and then see what diffusion does to build them back in, conditioned on the pose from a crystal structure of this thing, we actually build in a more human loop that binds just a little bit better. Binds just a little bit better than three or four times better. It's a log scale there, three times better. And we get rid of both oxidizable spots. And so we train on well-behaved human proteins so that when we generate things, we actually generate things that have fewer liabilities. But the ab diffuser and ab mixer methods in Equifold, there's a big effort now to try to bring these things together in a unified code. And we can at least release the structure prediction. The structure prediction aspects of this. Everything I've said is got a lot of things that are new and old, and all sorts of machine learning that people claim is boring if it's more than two months old, which is absurd. I think Natasha is here and has only been to conferences that have 8,000 people before, so she was really shocked that we only have 50 of us here. But in any case, one sort of way to reflect on everything we've done here is to just experiment with how well all of these. With how well all of these machineries do compared to MCMCs. So, Rosetta does various things to sample mutations that it wants to try, but it's mostly looking at loops at a time. Occasionally, it's doing cassettes of compensating mutations based on loop closure. It's got a lot of really interesting samplers in there. But how well does the manifold sampler do, not in a design framework, but just as a take a random walk on the manifold and see how fast you can minimize the Rosetta score? Fast, you can minimize the Rosetta score. And what we find is that it actually minimizes a lot of systems that we can set up in Rosetta faster than if we were just doing random or MCMC. And then there's also just a lot of cases where we put this thing through the paces. And when we compare the manifold to the manifold with Rosetta, we actually see in this particular case, I think this is her too again. We see cases. We see cases where we get substantially higher fractions of binders if we put the structure and the sort of raw AI together. And so, if structure is the baby, we have not thrown the baby out with the bathwater. We found a way to, you know, just get the temperature of the water just right. So, given the diffusion work that we're doing, it's not clear that we're going to need Rosetta for much longer, but it is clear that structure and the actual generative sequence AIs are probably going to be working together on some level, even if they're in a uniform. Working together on some level, even if they're in a unified model. This was just a plot to show that we do make a lot of changes and still preserve binding. And so we're not just making one or two mutations at a time. And the disclaimer is that nothing that I said worked had to have a phage or a yeast display library optimize it. So there are lots and lots of groups that say we fixed design. And what they really mean is they've figured out a way to make the 13 mutations they get to. To make the 13 mutations, they get to optimize with yeast display count. And that's not as cool as just loading up a plate and having half of them work. So prescient design, which might start to be called MLDD Genentech, Machine Learning for Drug Discovery, Genentech, but I think we get to call it Prescient for a little bit longer. We've been working on our Lab and Loop framework. We've had a lot of amazing collaborations with antibody engineering led by Yan Wu. With antibody engineering, led by Yan Wu at Genentech. Our language models do seem to have a lot of power in generating sequences, whether we test them empirically in the lab or whether we test them with other downstream structure simulation methods. And it does look like this is starting to work in a real world multi-parameter drug discovery setting. Although I'm able to show you everything about methods, but I have been able to show you very But I have been able to show you very little of the data because our boss, Yan Wu, she only wants us to work on real, real drugs. So, other than her two and OVA, I cannot show you most of the stuff that we've done. But I can show you all the methods and I can show you actually a good fraction of the code. And again, this is thanks to the best team that I've ever worked with. It's just been an amazing ride. So thanks for listening. So, thanks for listening. Thank you very much, Richard. Questions? Fascinating talk. Really, very impressive. So mine is kind of the most natural question that occurred to my head: is that when you're doing the oblation or ablation, ablating the sequences, you begin. Sequences, yeah, you began with an example of the binding, calcium binding, uh, or the calcium pocket around that region. So, now, my question is: could you actually arbitrarily, or is that what you're showing later? Could you arbitrarily take away arbitrary regions in the structure and kind of reproduce that structure? Does it make sense? Yeah, so that's a good question. The question, yeah, so that's and for us a related question. And for us, a related question was: you know, how much can you ablate and still build back in? Because to that, that relates to how many different sequence functions can you layer onto one thing. Here, what we did is we ablated enough to show ourselves that we weren't just picking up homology. So, we ablated enough that this thing didn't fold anymore by any method that we could use. But when we're doing these ablation tests, mostly Doing these ablation tests, mostly we did things semi-manually. We blew out active sites, we removed secondary structure elements, we tried to make it difficult. The other part of this is, of course, how much corruption you put in at each step of this iterative sampler. And again, this isn't the current method anymore. But in this early method, there were two things. How much do you ablate in the sense of what should your length step be? ablate in the sense of what should your length step be. And then with the calcium binding, that was how much do you ablate to convince yourself that you're not just picking up homology or doing something easy. And so for us, what we decided here was that we would ablate not just by mutating, but actually removing the residue so that it was also a length perturbation. And the remaining sequence would be less homologous to what was left over. And then on the ablation, or not ablation, but the corruption process, the language. Corruption process, the length scale part of how we take steps on the manifold. There, you know, we don't, I don't know what you can set that to whatever length you want, but we typically keep it for any, you know, iterative step on the sampling for the, with the original method to four or five mutations. And then over multiple iterations, in this case, I think it was six, we ended up making somewhere on the order of 20 mutations to fix the problem. So, yeah. So, yeah. Thank you. Any more questions? Yes, Robert. Given that you're only working on real targets, I probably know the answer, but have you guys looked at Cure at all? Looked at Cure Kir? I don't think I can answer that actually. Yeah. That actually, yeah. Like you're asking if that's a target, a genetic? No, no, it's so cure is a region of the genome that does the similar things to antibodies, but it's for NK cells. Not a gene name. Not a gene name. It's a region. I didn't know about it. So if somebody's working on it, they're keeping it secret. Yeah, I don't think so. Maybe I could ask you more about that after. Yeah, there's a whole bunch of sort of like cassette genes that get stuck in. There's a lot of variation, so it's very similar. lot of variation so it's very similar to and then it generates what kind of fold like a something that nk cells use to identify targets okay okay yeah no i i i haven't looked at that that's cool thanks thought you were saying hey by the way interlooking z yeah anyone in the audience uh sorry in the zoom audience that wants to Sorry, in the Zoom audience that wants to ask a question. No. Okay, nobody else here. Okay, thank you very much, Richard. So we have a coffee break now. After that, we are going to do a discussion where we can ask questions.