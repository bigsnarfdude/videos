Richards in their low latency and machine learning approach. Uh good morning everyone. Uh as Melissa just said, I'm going to give uh our attempts in estimating some bounds of the series parameters in low latency like immediately after we get a trigger. Okay, so something that even Andrew pointed out and even folks in the in the crowd have talked about is so at one point if you want very fast data products. If we want very fast data products, we have to rely on the point estimates, what the pipelines are giving, the template estimates. Then, slowly, some rapid methods of PE will catch up, and then later on, a bit more, like more systematic PE will catch up, and then we send out our improved data products over time. One of the challenges we have in Google Defense is: so, what are the things that are available, and what are the errors? What do we see when we do an injection recovery? So, this is Recovery, right? So, this is based on a paper from Becca from GSU Al team, and this is based on the O3 replay NDC, which was done just prior to 04, where the pipelines were running on their full 04 settings. And as you can see, there is the, on the x-axis are the injected parameters, and the y-axis are the recovered parameters. So, for now, let's just focus on the injected chart mass. We see the chart mass estimates are fairly along the diagonal, but this is, don't forget, this is zoomed. But this is, don't forget, this is zoomed out, also zoomed in version of this part. But we feel like our short mass estimates all the way up to 20 short mass looks very diagonal. Like what we get, what we input in the system is what we get from the low density pipelines. We cannot say the same for Q, but this is due to, this is not just JSL, it happens in all the pipelines, and the Q estimates are off significantly, and hence methods like EM Bright tries to. EM Bright tries to feed into that injection recovery and try to trace back what was the thing that was injected. And following the success of EM Bright, so I was also thinking, if can we estimate, like say, given a real time, let's say a pipeline estimates, a given short pass and a given mass ratio, or given M1, M2, what could likely be the injected parameters in that scenario? So, zooming in, like, So, zooming in, like now we are going the way this is in a BNS territory, where between 1.25 and 1.3, you can see it's fairly along the diagonal. The chirp masses, there are some a little bit up and down, but there are not that much of an error in this mass range, in this shirt mass range. Now, when we start going into EBH territories, now we can see some deviations. When we go even higher, it's basically all of the face and most. Basically, all of the phase, and most of the time, the pipeline systematics underestimates what the shop master was injected. There are cases where, let's say, when the short master injected were around 30 or 40, and the recovered cases are around 20. So that's half whatever the shirt master was injected. In future, people, like Andrew is talking about data products, that will ultimately ask us to, hey, can we do something about our point estimates and short mass and give some estimation in these kinds of things? In these kinds of things, but this is something that we need to be really wary about: our pipelines have the systematics and have the point estimates will not fall along what was objected. So we will not be able to cover the same thing. So one idea that I had is, can we quantify this? It's like at what range of parameters release or what kind of templates that the pipeline recovered, can we give certain, can we trace back to what was injected, right? Trace back to what was injected. So in machine learning, we could try to maybe train something that will try to give us from a recovered space, what likely was the injected space, what likely was the injected parameters. That would be the idea of a point regression thing. So from what pipeline we're estimating in the recovered space and what was the injected one. But this seemed to be a very difficult problem. Seemed to be a very difficult problem. We tried multiple methods and, well, the results were not suitable at all. Then we changed our approach to: hey, what if we give certain bounds that the chirp mass, instead of giving an exact point estimate, is like, hey, according to some study we did from a data set, how much the chirp mass is changing, or how much of the parameters are changing given a template for a reconstructed parameter. Template for a reconstructed parameter. So that is basically the idea of quantile regression. So rather than doing a point regression, we're giving now a quantile, we're giving a set quantiles. And using two bounds of the quantiles, you can give a bound to how much the like at what label, at what range the injected shot mass is actual in. So that's the objective is in low latency parameters, whenever the pipeline gives us our results, we'll have our masses, our spins, our chip mass. Our masses, our spins, our chip mass, Q total mass, which is basically a combination of this, and an SNR. These are the things that we know in low latency, right? And can we give a bound, let's say, in check mass in Q and in total mass, but this would be based on whatever the data that, like your O2GSL data that Michael just gave a talk about, and this is the same data set where the HASNS has remnant, it has mass gap quantities, have been trained and used in. Have been trained and used in production for like two observation runs. We split this data into 80%, 20%, among which 10% is for validation. That leaves us with 200,000 events still, because this was a very dense injection and GSLL. Very thankful that they provided us this data to play around with. And we further been these categories, like I said, because of this features. Because of this features, it's like at the smaller categories, at very small chart mass, the errors are less, but when you go higher and higher, the chart masses, the chop masses are different. And we tried binning the data in this approach. So this approach is very familiar to what Build does, the online Buildy does. And we tried to replicate that. So because ultimately, we also wanted to see if this would be used as priority in Buildy. So we tried to replicate the spin, but there's no law or there's no. There's no law, or there's no idea that we should be stopping at this binning. We could do more binning between 12 and some other steps there. But, okay, and these bins are actually inspired by our online PE bins. And for each bin, we train our model for choke mass, we train our model for cube, and we train our model for total mass, all of them. And the tests were also carried out for this. So, what is the actual So, what is the actual network architecture? It's fairly simple, actually. We have our input layer, and we have a couple of hidden layers for the drop-out and DQ related activation. We do have a sorting layer, because apparently it is a well-known problem in quantile regression, is whenever you are trying to get multiple quantiles, whenever we're trying to predict multiple quantiles across a quantile regression network at once, there is a case that the fifth world. The neural network when it tries to estimate the 5%, the 5th quantile, and where you try to estimate the 10th quantile, they might be swapped just because of the internal noise of the training. So the sorting layer helps this to be completely ascended in that scenario. And we estimate multiple quantiles across this. So we can say the chart master, 5, 10, we can go at any kind of spacing we want. And ultimately, we can get like 20 outputs of it. Ultimately, we can get like 20 outputs of the network. This is all based on what kind of training we would want to do. Currently, we were training like 27 different outputs across the from 0 to 100. So we cannot provide 0 and 100, but we can provide 0.01 and all the way up to 99. That's in the target quantile region. So, and that's the neural network output. You can see if we are trying to estimate q, because q is bound between 0 and 1. Because Q is bound between 0 and 1, so we have to put a sigmoid at the output layer of the neural network so that we can bound it between certain numbers. But for shirt mass and for total mass, we can use an exponential and followed by what the parameter was injected in the first place. What the parameter was recovered in the first place. So, the loss function, this is the standard quantum regression loss function. We have not made any changes in this. This loss function is proven, well, in a data to always, if the neural network can find a minima, will always land into a certain quantile, that certain target quantile. So, in a given distribution of data points, if you chain this loss function for a given, let's say, 75% quantile, it will try to find, and if this converges, it is proven that it will land. Converges, it is proven that it will land on that 75% quantite point for a given distribution of data. And we do minimize this with Adam Optimizer, and we train this for each short mass bins. Now, how do we evaluate the performance of the network? We'll have to look at both accuracy and width. So, let's say if we are going to evaluate certain like 99% confidence or 99%, let's say 1 and 99% quantile, which we can basically use a 99.9%. Tile, which we can basically use at 98%. And if the width is very high, our accuracy would be squared. Like if we assume that the width is around like 100 chart masses, then we will always be accurate because the injection point will always lie on that. So there are two things that we have to take care of. We have to look at the accuracy, and we have to look at the width at the same time. Come on. Okay, there we go. So, this is the performance of the model on the testing setting. Model on the testing set. So for now, let's look at the chirp mass itself. Is the target, the x-axis is the target accuracy. So, how do we determine the target accuracy? Is we take one and let's say we assume two quantiles. If we say five and 95, that gives you a target accuracy of 90%. And if we go even higher than higher, we can go all the way up to depending upon how fine the network we are training. We can go all the way up to close. We cannot get to one, but we can go close to 99%. So it is fairly diagonal. We don't see that same thing for mass, the total mass. But this is the performance across the four different builds that we are looking at. And ultimately, so if we are looking at a very high number, let's say 95% or something, the actual measured accuracy is very high. It's almost linear in charm assets across all the four pins. There are some things that go above. thing that goes above. So ideally we would want this to be diagonal, right? If we are trying to, if it is a properly described function, we would want it to be, when we try to increase our target accuracy, the amount of event accuracy that we measure should be across the diagonal. But we do see some overestimation here. But ultimately, for a use case, if we are trying to use a 95% accuracy, we are mostly getting the 95%. Like if you are trying to train to get a target. Are you trying to train on to get a target of 95% accuracy? We are actually getting 95% accuracy in our data, in our testing set, actually. Now, the moment of truth is this is trained on O2 data set, and the main thing we would need is what was the output on the NDC. Now, we did face a couple of challenges, is the same problem that Tom just described, is this network is trained on OTA data set in which the maximum masses and the templates used by GSTLAL were. And the templates used by GSTLAL were less than a certain chart mass and less than a certain primary and secondary mass. We can see in the MDC and in O2, the width of, so in the MDC is a much recent one, the short mass goes all the way up to 120, like 150 or something. But in our injection set, which we use for training, the short mass never reaches that high. So ultimately, and it's a, for any regression model, when you It's a for any regression model when you are trying to find something way out of the area that you are training, you'll find some functional results. So, we did have to constrain our MDC events that we are testing to a cut. If you are predicting these quantiles in the region where we are trained on, so it almost removed half of the triggers in this in this respective beds. But ultimately, we would want to train this on something newer in O4 so that we can capture all the all the distribution. All the distribution of M1 and M2 in the templates. So we now are trying to see the performance in the mock data challenge. And this is the performance. If we assume, if the target accuracy is, if the model is changing for a target accuracy of 96%, if you are trying to get 96% accuracy, this is what you are getting in the O3DPM VC. Let's look at just a chip class. 95%. 95% with a width of 10 to the power minus 3. Like, that's a very, because the pipeline still does a really good job at that boundary. And for bin B, we have 98%, bin C, 98%, and bin D, 99% accuracy. Now, this is not as intuitive, but I think these plots will make it more intuitive. Not this, but this. So you remember this plot that I made before? Now this is what Now, this is what the neural network predicts in the width of where likely the chart mass is. Now, for the case, let's say this point is we are estimating something lower, and if we include the width, it will actually touch the diagonal, and the width will include the actual injection. For the case of a little higher chart mass, these are the widths. And you can see even when the, even like for case. When the, even like for cases like all the way out there, the neural network estimates some bounds that actually include the chart mass. And for even higher ones, for the 30 to 40 chart mass, this is the 95% model, so we are bound to miss a couple of them, but almost all of them, you'll see that at least the bounds predicted by the neural network actually contains the injection within. Now, swing back to this plot. So, we also tried to do, so this is something like same like Andrew's mass spread plot. So, we don't, so we tried creating a fake template parameters, M1, M2, to give the training the neural network an input and see what the neural network outputs across the check mass and across the queue. And that's the width of the estimated width. Of the estimated width of the neural network. And this is estimated at 96%. And you can see, well, unfortunately, this pin is on this color scale. You cannot see this pin at all. I'm sorry for that. But you can see some systematic features. Like, for example, if you look at the VVH pins, at this point, the color scale points around almost 10 chart mass widths at a low chart mass. And as you go down and And as you go down and down, the color scheme actually changes to darker and darker widths. When the pipelines are estimating lower and lower Q values, that means the errors are higher and higher in that. And the neural network will give you wider and wider width in order to accommodate what the actual true injection was. So we also actually did try to predict the real event for which we or the collaboration have already published a paper saying, hey. Published a paper saying, Hey, what does our model give? And compared to he, what does that look like? So, our model gives a short mass range between 2.021 to 2.031, a Q range between this and an M total range between this. And on the right-hand side, you can see the checkmass Q and the M total range of the actual final PE results that was published. And they are all within the bounds. Well, they are contained within the bounds provided by the neural network. Are contained within the bounds provided by the neural network. Now, coming to the use cases, right? What could this be used for? Like, this looks handy, but what could this be actually used for? So, if we want to give some uncertainties on the estimate, on the point estimate, because a lot of our data products in low latency actually feeds in into these point estimates to give us very low latency estimates of SNS and other various quantities, then maybe. Various quantities, then maybe our short mass errors like this or our bounds could be used for those kind of things. And this network is scalable, and we can, if we get a data set where multiple pipelines are actually doing an injection recovery from a given data set, we can basically add more layers as an input. Let's say PyCDC gives a preferred event, MBTA gives a preferred event, GSLL is giving a preferred event, and Spirit is giving a preferred event. We can train that to give We can train that to give one unified bounds in shortcuts. Like how much, how much, but that would be a new network-based bound, but still, it can be done really fast, like the order of one second. So it could be something that can bring the pipeline data together to give one outcome. And something that we did, I also tried to explore, is, could it be used? Because this will ultimately, the model does a really good job of keeping the injected parameters. Job of keeping the injected parameter there within the bounds. Can we try to use this as a prior information for really fast P methods that are out there? We really wanted to test this with Rati P, but unfortunately, because of some circumstances, we could not. But I've been talking with Dan, so maybe we can now ultimately do some testing there. But we did try to do it with BILD as well. But first, this slides. These slides. Yeah, this basically feeds into the idea of maybe we can provide more information. And now we know that astronomers are more and more interested also to follow up tightly localized DVHs. And whenever we are trying to give some coarse chart mass information, we need to include these uncertainties. We have seen the case like an event recovered as a 20 chart mass might not be as interesting, but the actual injected chart mass was 40. Injected short mass was 14. And that becomes a bit more interesting for astronomers. So maybe we could try to integrate. I don't know how, I've not thought about that, how to properly integrate this into the system, but this could be a very useful tool. And yeah, so my idea of the online PE PE approach, sorry, this is very congested, but this is basically giving you a sense of how the online PE methods are trying to. The online PE methods are trying to currently estimate priors. So if it is a BNS, they are trying to go as low, if it is a BNS with a chart mass less than 2, they will try to search between whatever the recovered short mass of the template was, minus 0.01 and plus 0.01, because that's how good the pipelines are in estimating the chart mass at this region. If the chart mass is less than 4, chart mass is less than 8, but something interesting is. But something interesting is now when we go in higher and higher check mass, we go wider and wider in prior space. I was thinking if this method would be useful in a sense that now we can, a neural network can give you a bound where likely the injector parameter is, and maybe this could be used as a prior introduction for chart mass and queue. And that's what we actually try to do. So in MDC, we took like 102 events divide equally between Divide equally between BNSs, NSBHs, and BBHs. Now, there are certain, in the current online version of BILLB, we have an ROQ likelihood method that actually, let's say, if the BILLB default prior is within a certain range and our neural network method gives a certain range that is outside of BILLB prior range, our run fails because of the REQ implementation parameter. So, we also did some tests with related meaning where that. We also did some tests with related meaning where that constraint is not there. So, a couple of examples, like interesting examples, is for some of the cases, what the default neural network priors are giving and what the default TE and what the neural network is giving, with that prior we get almost similar results. But there are this is the case that is most peculiar because of the neural network. That is most particular because of the neural network, because the neural network puts a hard cut on saying that, hey, the prior must be between these two numbers. And BILD goes beyond. BILB tries to search as much as possible. A default prior range starts to search as much as possible. But the neural network will try to chop off this at any points that the neural network seems fit from the stream. This ultimately leads to Ultimately, it leads to some interesting thing: is well, we are reducing the prior space. So, hence the number of likelihood iteration that the neural network will do in order to come to a final state will reduce. And that reduction number, this is on BNSs. Anything above zero is basically that the neural network model is having to. So, the y-axis is basically the percentage. So, this is 10%, 20% reduction in the number of likelihood iterations. The number of likelihood duration and the red dots are the BNS, the blue are the NSBH, and the green are the BBH. The median reduction is around 10% for ROQ likelihood and for relative, and well, let's talk about the SkyMap statistics. The main key product that comes out of a PE that we send out public is our SkyMaps. And so let's look at how the SkyMaps look like using this prior is the default prior and the Prior is the default prior and the neural network prior are fairly similar, but at some points, neural network prior with ROQ statistics actually falls a little bit down here. And the PP plot also shows a similar trend. But for relative meaning, we also get something similar to 9% in median reduction. And SkyMap statistics look quite similar on the M. Quite similar on the MVC data. So, just concluding, the neural network can provide some dynamic rounds and CVC parameters. And this, why I'm calling it dynamic, because it depends. Let's say if you just increase the same event and have, instead of 15, you use a 30 SNR, the bounce automatically reduce, knowing that the bounce also has show some reduction in that bounce. The testing against MPC shows over 95% accuracy. 95% accuracy within O2 injection values. If you go beyond, the neural network will start housing in that sense. It is easy to implement, inference time is less than one second. And this could also be used maybe as a prior innovatency, but definitely this information could be used for some short mass information later on. Thank you. All right, we are sliding into the coffee break, so one quick question. Is it quick? Quick question? Is it quick? Okay. Depends upon the answers. Yes. Okay. This is really cool. I have a question about the mass uncertainties. I think for that to be useful, it will need to have the same mass distribution for the MBC that you're training on, and the template mass distribution in the pipeline has to be the same. Yes. And for it to be Yes, and for it to be like really useful, these all need to be some sort of astronomical distribution. So the injection set that we used were at that era, in O2 era, developed by the RNP team, used by using the base, like the, to the best knowledge at that, like 2017, 2018 times, that something like that. So the idea is, so you know the O4 VT injection sets that are going on right now, the online VT injection sets, that could fairly see the operation. Fairly see the astrophysical properties there. Now, the point about the MDC is also we have tried to be as, I remember the, I don't know, the MDC designer, Sean, I think he also tried to be as close to the astrophysical distribution, but ultimately, that's the only point of testing for us right now. So that's all I can actually show you and show at this point. Then we continue. Then we continue the discussion during the coffee break and we thank our speakers.