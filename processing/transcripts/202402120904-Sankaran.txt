Day on interview started, just giving a high-level review, a lot of topics here. So, what I'm hoping you can get out of this tutorial, first, why is interpretability an important problem in machine learning? What are some of the common techniques that are used all across in industry and academia? What are some of the major open challenges? And finally, I want to also emphasize that I think statistics has a lot to play in this space. Lot to play in this space. So hopefully you'll see some examples of where statistical thinking is very relevant. So I think by now we all kind of understand that machine learning is everywhere in society. And there are a few different aspects of the ways that machine learning is being used. The ways that machine learning is being used. So, first, think machine learning is being used a lot to assist or sometimes even completely automate decision making. If you think back, this is actually a very modern phenomenon. 50 years ago, if someone wanted to give you a loan, or if some doctor was trying to decide what kind of treatment you should be getting, it was really human expertise all the time. Now we see very often there's some sort of automation or some sort of machine processing that. Or some sort of machine processing that gives a suggestion to what we need. Second, in discovery. In many areas of science now, there's so much data that trying to do very classical algorithms is not really sufficient. People are using lots of machine learning algorithms to just navigate all the data structures. And maybe something that seems to be not in the front of everyone's mind in this area, but which is really growing, is creativity. There were lots of There were lots of ways of designing and creating things that used to be only accessible to someone with lots of expertise and lots of technical training. And now, someone who's just kind of a beginner in a certain area is able to use machine learning tools and start creating things that look almost like something that someone who had a lot of training would have had. So, think about all these tools in Photoshop that you don't have to be a Photoshop expert, and you can start sketching things out, and it will fill out a lot of this interesting design. Now, it's being used in so many different areas for these kinds of models to really be beneficial for humanity. If we want to make sure that these are not misused, it's important to be able to interpret these models. So, what exactly can go wrong if we don't use these models wisely is a very famous example. I don't know if you've already heard of this. This is from a study. Already heard of this. This is from a study. It was a study to try to understand whether you should put people who have just been diagnosed with pneumonia. The question is: should you have them go to the hospital directly or should you put them in outpatient care? Just give them some medication, give them some treatment, and hope for them to recover at home. The reason for doing this is if people are likely to recover well at home just during medication, then that frees up a lot of possible resources. So that's what. That's what they were trying to do at the very beginning of diagnosis. Do you go directly to the hospital or after? Here's the results from one model, and it's a very nicely interpretable model. So, y-axis, this is probability of death. From a historical data set, they looked at all the characters. So, people who are coming in, and are they going to die of pneumonia or not? So, see like age, older people getting pneumonia, there's much higher risk. Kind of surprising result is this one. Result was this one. For people with asthma, have a lower risk of death than people without asthma. Do people know why this happened? I guess because they have a lot of medicine and stuff already at home, or they have experience like treating that. Some of the yeah, this is the basic idea. In the historical data set, people with asthma were kind of immediately kind of alert the Immediately kind of alert the hospital staff said, okay, you have to be in the hospital in order to give you the most aggressive treatment possible. So it turned out that people knew that there's a high risk for asthma. So if you had asthma, you actually had a lower probability of dying from pneumonia. That just being a classical data set. So this is problematic if you were just going to start applying this model. Because if you had just trusted this model to assess the risk, then those people would have been sent home with justification. Been said, Oh, it's justification, and it wouldn't have given that aggressive change. That's one of the reasons why we need this sort of interpretable. Another famous example. There were some researchers who figured out how to put stickers on a stop sign so that when you place the stickers in the right way, the computer vision algorithm trying to detect what is this object thinks, oh, it's a speaking rate sign. Speaking of language. And so this is extremely dangerous, right? And this is the sort of thing that really only happens when you don't really understand how the classifications happen. If we understood exactly how these models make the prediction that is a stop sign or speed limit, then this sort of attack would not be possible. Here's a very recent example, just in the last year. Did people see this? This is from the Google Bard demo. So people, they just released this like the Google chat. They just released this like a Google Chat GPT. They just released it. They're trying to show it off to people. And in the advertisement of the demo, they said, What new discoveries from the Jane Space Web Telescope can I tell my nine-year-old about it? Ask the question. Bard replies with a few kind of interesting sounding facts. And it all sounds convincing enough. I don't know much about astronomy. But as soon as this demo was made public, a bunch of astronomers started taking over social media and they. Started taking over social media, and the astronomers are saying, This is not true. James Bud Smith, this was not the very first picture of a planet outside the solar system. The first picture was from the early 2000s. There were other technologies before this that allowed you to image it. So you think about this example, it must have gone through so much review and there must have been so many eyes making sure that this very first Google demo was very good, and yet this sort of mistake still went through. This sort of mistake still went through. So maybe it doesn't sound so impactful if it's just someone asking about fun facts, but these sort of chat AI, these are going to be used all about science. They're going to be used in legal form. Having this sort of mistake is not really going to be tolerable. So all these examples raise a natural question, what exactly makes a model interpretable? This is a very difficult question. This is a very difficult question. I think a lot of the community is still actively thinking about what exactly is interpretability. I'm sure we're going to have a lot of discussions today, throughout the week, about what do we think really makes models interpretable or not. So I want to step back and think about the question that the community has a lot of experience thinking about, which is visualization. Statistics has a long tradition of data visualization and graphics. What exactly makes a visualization good? And I'm going to try to argue that there's some. And I'm going to try to argue that there's some useful analogies here. So, just some first properties. A good visualization should be legible, it should be annotated. You should have high information density. So, this is a term that Edward Tufte coined. The idea is that you have the amount of ink that's dedicated to showing actual data, and then the amount of ink that's kind of extraneous, irrelevant information. And a good visualization should have high information density loss. Have high information density loss of ink related to the data and less kind of unnecessary annotation, unnecessary points. So, just to understand what a difference that can make, here's a visualization that comes from a government report about bowling damage in the space shuttle. Rockets is a different launch of the space shuttle, and it's giving some information about where the damage was and which shuttle it was, what the temperatures are. Temperatures are, this is a very difficult visualization to make any sense of. I can't even tell what they're trying to say with this plot. And Tufty, in his book, he replots the exact same information in just a simple scatter plot. X-axis is temperature on the day of the launch of the spaceship. Y-axis is the index of only damage. There's a little bit of a trend here. And he Here. And he highlights this is the temperature on the day that Space Shuttle Challenger was going to be launched. One of the arguments he's making here is that if they had communicated this data in a clearer way, we might have been able to avoid a space shuttle challenge or disaster. Because the challenge or disaster was because of high overwhelming damage. So this idea that we're using machine learning models in high-risk situations and there's lots of potential damage. Loss of potential damage from using them unwisely, that's a new phenomenon. The idea that we need to think carefully about how we make decisions based on data, this is an old idea. And I think we have a long history to draw from. I gave the kind of most obvious examples of what makes visualization effective. There's a lot more issues. So even data problems, right? It doesn't matter how beautiful your graphic is, if you can't really figure out what the data mean or where exactly they came from, then I'd argue it's not really interpretable in the sense that you really care about. The effectiveness of a visualization depends a lot on its audience, right? A visualization that makes a lot of sense to a group of statisticians might not be what you want to print in a newspaper which has circulation. And then And then every visualization is also prioritizing certain kinds of comparisons. Maybe it engages people to more or less extent this parameter. My point is not memorize these different characteristics. My point is that, at least in data visualization, we can think about what makes a visualization good or bad with a lot of nuance. There are lots of different properties that go into visualization. And my And my hope is that machine learning interpretability reaches that similar level of nuanced discourse. I think for a long time there's been a lot of this model is interpretable, your model is not interpretable, and just kind of very black and white thinking about this issue. And I think we're getting to a point where we're able to think about this model is interpretable for this audience, for this task, in this context, for that kind of specification. Context, but that kind of discusses the questions about the introduction before I move on to some example like this. So yeah, the goal for this section, I want to review a few different methods that are commonly used for machine learning interpretability and explainability. My goal is to be representative of a few different classes of methods. I didn't think Classes of methods. So I didn't pick any, I didn't pick the state-of-the-art method for any one area, but I tried to pick a few different areas and some relatively common techniques. Before we talk about specific methods, I just want to establish some vocabulary. The first distinction is going to be between an interpretable model and an explainability. An interpretable model is one where the mapping from the inputs to the outputs is clear just by design of. Is clear just by design of the model. So you design the model so that you can understand each component very clearly. An explainability technique, this is a technique that can be applied to arbitrary models to try to shed light on what exactly is the relationship between input and output for that arbitrary model. About tipable models, this is kind of model-specific explainability technique. It's a generic technique that can be applied to many different kinds of models. Technique that can be applied to many different possible. Within explainability techniques, there is a further distinction. So global versus local explanations. A global explanation is one that tries to summarize the entire model. So imagine this is the decision for your black box model. You don't really know exactly if you knew the decision values have been too straightforward. Like, you can't really query it. Like, you can't really query it, but you try to return some sort of prophecy, some sort of simplified version of that, and that's going to be your global summary of a complex platform, so that's a global explanation. And then there's an idea of a local explanation where instead of trying to summarize the model overall, try to summarize what features are relevant for a specific decision. So take a single sample, and for that sample, try to figure out what properties of that sample led to one decision or another. Decision or another. That's pick one example and try to figure out what directions do I push you that increase you increase one or two. So level explanation. And to illustrate my examples, I've simulated a small toy data set. I work in microbiome research, so this is kind of the first thing that came to mind. I'm going to try. The first thing that came to mind was: I'm going to try to simulate a little microbiome example. The idea is going to be, I modeled it off of some studies I've seen that are microbiome related to HIV risk. So people have documented if you have certain types of bacteria and certain kind of trends in your bacteria, then you're more likely to be able to transmit HIV. So the idea in the simulation is: I've made up these different curves, each of these panels. Curves. Each of these panels is a different species. The color is going to be disease or healthy. I saw at the end of the time course, at the end of the entire study, you get assigned whether you develop disease or not. I have this actually for 144 species. And I've done it for 50 time points and 500. So I'm just seeing some subsequent. The point is that these are very complicated looking features. There's some of them are increasing, some decreasing. Some are increasing, some decreasing, they're having bumps at different locations. So I've simulated it in some very complicated way. And as the scientist, what you imagine wanting to do is not just predict whether this particular series is associated with health or disease, want to figure out what exactly are the features that are relevant. If you were going to try to understand the disease mechanism, you might want to say something like: an increase in this group of species near the end of the time course status specific species. That's something that you can actually. That's something that you can actually follow up with. So, I mean, the point was, I just wanted to have one example that you could see all the different explained movements to be small for one example. Just a quick question to clarify. This is like at the end of your disease, is like they all end up with the same disease, so like all the red is the same disease, or could be any given disease? It's the same disease. Yeah. I mean, it's all simulated, but I've tried to make some sort of homology. Other questions about this set? Other questions about this app? Okay. So the first thing let's try to do, let's try fitting one of these directly interpretable models. So we're just going to try organizing the data so that we can fit regression. So how can we organize it so we can do regression? Each row is just a different sample. I've just stacked the entire trajectory for each species. I made it horizontally. So I have this variable. I made it horizontally. So I have this very, very wide matrix where it's every time point and every species of one components. That's this, you know, the first data organization step we can take. And then from here, a standard direct interpretable model is sparse logistic regression. And this actually already does reasonably well. You can get 77% accuracy. You think just 38 of the original, there's over 7,000 features because there are many time licenses. 7,000 features because there are many time licensing. Okay. And this is actually very good news if you were talking to your collaborators, because you'd be able to say, pay attention to these few taxa at these few time points, and that's already going to help you predict the difference between health of C's. So that might be already kind of a very nice, interpretable way to approach this problem. And this idea of what And this idea of why spark statistic regression is interpretable is similar to why a lot of statistical models are interpretable. A lot of it has to do with kind of two terms, parsimony and simulatability. Parsimony is this, you know, you only need to pay attention to a few features. So as far as logistic regression, you only need to pay attention to a few features that already gives you the decision model. But it applies in other kinds of models too. So if you were looking at a generalized additive model with interactions, often you only need to consider Interactions. Often you only need to consider a small subset of interactions. And that lends itself to a certain kind of parsimony. And if you think about graphical models, that also has a similar kind of simplicity, where you only need to pay attention to a small number of sampling steps. So for all these reasons, these sort of statistical models have that kind of interpretability right away. You only need to pay attention to a few features, a few interactions, a few sampling steps. But even in this sort of simplified setting, there is some subtlety. The main issue in this simulation, if I split the data in half, and I fit sparse-logistic regression on one half, and then on the other half, and you look at the coefficients that are selected, you get almost totally disjoint subsets of selected coefficients. So if you were giving this to your collaborator, But if you were giving this to your collaborator, you were saying, here are my 38 perfect really good predictors, this is actually not something that you should really trust, right? Because it's very unstable. And there's a kind of obvious reason in this example for why it's unstable, right? I've simulated these time series. They're relatively smooth. So you have very high correlation in the predictors. You're going to sometimes zero out some digital. So, my point of including this is that even the model that we kind of always talk about as being very interpretable and very simple, even in a model like this, some care has to be taken before we call it interpretable. If it's unstable, that's not really interpretable anymore. Okay, so what's the next thing that you might try? Instead of throwing in every single Instead of throwing in every single time point, that's what's causing this high correlations. Let's just summarize it. Let's build our own features of these time series, do that handcrafting, that feature engineering, and then throw that into our classifier. So here, all I've done is for every single species, I've extracted two different features. One is the overall slope, and the other is the curvature. So I'm just taking the second difference of its squared. Just throw in those two features. Just throw in those two features. Now it's a much smaller matrix. We don't have nearly as many columns. And we actually do much better. It went from 77% accuracy, call it out, to 86%. And my lesson here is that interpretability and accuracy, these are really not as much at odds with one another as sometimes you'll hear about. So, I think in statistics, we're relatively comfortable with this idea. In statistics, we're so used to bias-variance trade-off. We understand that a more complicated model can have higher variance. It might actually be worse to have a very complicated model. So, we're very used to that idea. If you read a lot of the interpretability literature, though, you'll often hear people talk about there's some sort of explicit trade-off between a very complicated model that's very good, but it can't be interpretable, and a interpretable model has to be very simple and can't cover well. Very simple and can't cover too well. Reality is much more complicated. Especially when you're allowed to design your own features, or you're allowed to design a model estimation that matches the real data, sometimes you can do a very simple model and get very good performance. Okay. Oh, question. Sorry, are we allowed to is that okay to ask questions? Yeah, of course, please. So you're leaving me kind of hanging about the instability. Does this one the instability? Does this one use the instability? Not sure actually. I'm pretty sure because I think the ones that it was zeroing out and positive was the same species. Okay. Right? And just different time points in the space. But I actually haven't don't have the same plot. But the code is all there. So I should make this clear. Other questions? Yeah. I'm curious if another way to say what you just said is. So, say what you just said is reducing the measurement error can increase both accuracy and interceptability. Is that kind of an accurate summary of what you're doing? I think the measurement error. I mean, like, if you have, like, data that's, like, just really noisy, right? And maybe you have, like, 10 measures that are basically measuring the same thing. Yes. And then do your feature engineering actually. So, like, if you think of it, that's right. Like, if you think of X as noisy, or X is itself random, and it's associated with the And it's associated with the true generating signal, and yes, the more you can match that parsimonious true signal, then the better you'll be able to do, and it will still be interpreted. That's a fact. So, you mentioned that this simple model with its derived features can perform maybe as well as a black box model. So, I just wonder, is that because there's some extra knowledge going on how you derive these features? Yeah, of course. Yeah, of course. So I've simulated this entire example, right? And these features are very close to what I used in the generating mechanism. I generated random shapes for a certain taxa, and I associated those random shapes with the classes. So it's true. And it's very dependent on the quality of those derived features. So is it fair to say that this is not so much a fair compiler in the sense that in this Compiler, in the sense that in this simple model, you have some extra change, and the Flat Ramos model, you're sort of relying on data normal, so that's why you can get S. So, if in this case you get, say, compatible performance, then you are using some extra knowledge to achieve some simpler performance. Yes, that's very true. And actually, when I transitioned to talking about deep learning models, the appeal there is that you don't have to use as much effort to try to figure out what exactly To try and figure out what exactly are the best features for the problem. I'll come to the question, how do you describe features? Should that more rely on domain knowledge of experts, or you should consider again learning using data? Okay, so I mean that's exactly this kind of problem, right? Like the I guess in all of these problems, it's never really I guess in all of these problems, it's never really possible to know what the true process is. If your domain expert is able to identify features that they have some real grounding to believe are going to be associated with the fonts, then obviously this should be used. But there are going to be a lot of situations where this is actually quite difficult. Yeah, I don't think there's, at least at this point, there's no universal. At least at this point, that's not universal. I'll talk about a couple. Near the end, I'm going to talk about some approaches which are a little bit of a compromise. Where you allow the expert to provide some sort of domain knowledge, but allow the model to still extract more feed some other. Yeah, there is some compromise. Expert could be wrong. Yeah, exactly, right? Or like you can kind of add to the vocabulary. That's like you want to discover things. So you should try to learn something from it. Okay, so. Okay, so this is actually a very good transition. So far, I've been talking about directly interpretable models. Now I'm going to start talking about explainability techniques. So to talk about explainability techniques, I need to have some sort of black box model going on in the general. The model that I'm going to be using here, I'm going to fit a transformer model to try to predict healthier business. So I'm actually using GPT-2, the pre-GPT 3.5, GPT-4 that OpenAI is. GPT-4 that OpenAI is exhausted. So remember, how does GPT-2 work? It takes these kind of one-hot encodings of a sentence, and it tries to predict the next word. So here it's saying, we love to learn from, and from those one-hot encodings, you somehow need to predict that the next word is going to be data. The way this actually is working in the background, it's coming up with some sort of compressed representation of the sentence. Compressed representation of the sentence so far. So it's taken that kind of one hot sequence and it's replacing it with some vector representation of all the different kind of features in that sentence so far. And from that vector representation of the sentence so far, you're trying to learn classifier for the next word. The analogy in this microbiome example, replace the words with the different bacteria. You still have time instead of the location. Still have time. Instead of the location of the sentence, it's just time. Instead of the one-hot encodings for the words, I have the abundance of the species at that location. And then instead of just trying to predict the next profile, I'm using the same kind of GPT compressed representation at the end, and I take that compressed representation to predict the requests. So you can apply GPT-2 if it's not too difficult to train. And if you do this, you get 84% accuracy. So, this is almost as good as the hand-crafted features, and that hand-crafted features example was also a little bit cheating because it has access to what I know is the true data generating frequency. So this is quite impressive. This model knows nothing about how I generated the data. It gets nearly as good as the handcrafted features. Alright, how? What's the data sample size? It's 500 people, 50 time points each. The same data set as well. The same exact data set as before. Are you using a pre-trained model? No pre-trained model, because I'm not able to use any actual language or image. It's just crunks. I reduced the the number of layers compared to the original GPT. Maybe only three or four layers instead of six or something. Three or four layers instead of six or something. When house 3.5 would rather have a list of the language. Well, because this is the one that is easiest to use in some library. But yeah, I mean, all the code, I linked the code at the very first slide, and it's also on my website. You reduced the number of layers. Did you also reduce the number of arrows per layer? I reduced the embedding dimension. So the embedding dimension on the original, but I didn't otherwise change it. Part of my reason for including this is that I think sometimes in statistics we talk about deep learning models as being some like totally out of reach and like oh you need to have tons and tons of compute to be able to do it. When people are using deep learning a lot in the real world right now, it's being used and lots and lots of people are using it. It's not just something that only experts are using. So I wanted to show it's actually in some s sense it's easier than a lot of statistical models to try. Models to train. Just to clarify, did you retrain the coefficients in the GPP2? Yeah, I started from scratch. Random initialization. Like I randomly initialized the weights and then. I just used the architecture. I used the architecture in the console. How do you begin to explain a model like this? The first idea we'll borrow, this is We'll borrow, this is from a paper about another kind of language model, is you can analyze the embeddings. So remember, I mentioned that you take the sentence so far and then you embed it in some high-dimensional vector. So you can do this kind of for every word in a sentence. You can look at the embedding of that word within its larger sentence context. So this is the thing that a lot of people have done. Visualization and machine learning. It's probably really hard to read this. It's probably really hard to read this. Each point corresponds to a sentence that uses the word fair. Fair. And what you kind of start seeing is that there are different clusters of sentences that are using the word fair. And they have a real kind of clear semantic difference. So these fair are fair in the legal sense. So any sentence that's using the word fair use is appearing in this sort of cluster. It's appearing in this sort of cluster. Words that are using fair in the sense of like world fair, that's appearing out here in this kind of cluster. So somehow this model has already learned differences in semantic context for the single word. And this is one way that people try to understand what exactly are the structure that this kind of model has learned. Has it learned syntactic meaning, has it learned synthetic? In our case, we can again do that kind of We can again do that kind of embedding. We look at for each. Here, I've just picked up one taxon instead of one word, and I've looked at that high-dimensional representation, and I just run PCA. The previous one was also just PCA on that embedding. You have the PCA of the original data, so there's not that much separation. You look at the original data. But when you look at the PCA of the embeddings that have been trained, you see very clear separation, which just makes sense, right? This is a model that's been trained to classify the two different boosts. You don't have the nice clusters. I guess there's kind of no difference in the meaning of that species in different contexts. So we don't have that pattern. Another kind of thing, though, you can do on this data to help interpret it, you can try looking at the interpolations. It's a very common technique in this case. So in this case, I've taken two different samples for the same species. I take two different samples. Different samples. I draw the line in, I actually draw the line in the high-dimensional space, and then I look at the nearest point along the nearest points along the interpolation. So I get a kind of little sequence of points mapping between those two samples. And now here I've drawn what exactly the time series for those people, for the species, between those two interpolator points. Points. The main thing that stood out to me looking at this axis is at the very beginning of the series, on this side you tend to have the larger values at the beginning of the series. Here, at the very beginning of the series, it's relatively low values. So, this somehow is a feature that's predictive of health or disease that my model has done. So, the main idea of this interpolation technique. So, the main idea of this interpolation technique is you kind of get different regions of your embedding space, you can draw some sort of axis and try to interpret what is the meaning of that axis. That's one way of interpreting these sort of embeddings. So, that's a global explanation. Any questions about the global explanation? Treat the high-dimensional representations as data themselves. You start analyzing. themselves and you start analyzing that representation as data. Is there any reason to think that the model actually uses this? I mean these are this is like the first principal component direction of the medic space. So this is like the largest variation of things. And since it does separate between health and disease, I would think that these features... My interpretation of them, that's subjective, right? So maybe that low versus high, that might not be. I mean, there's a gem between saying, well, it would make sense that the model succeeded and the model actually tried doing is you can define now that you see this sort of feature, you can define that feature, and then you try analyzing the correlation between that feature and Analyzing the correlation between that feature and all intervals. And that's like a formal way that you can choose. I guess just it was, it's very, in the, you know, example before where you were talking about the clusterings. It's like in the sentence example, it's really easy to go back to the original data, but it's a bit tricky here. Yeah, I think it's. Yeah, I I think it's it's probably one of these like language is so easy to try to make sense of and then in other applications that you do really need to understand that domain a lot more. Like this these time series they don't obviously directly correspond to the original time series, right? Oh these are the new series. Sorry, I misunderstood your explanation. Yeah yeah yeah sorry. These are the original samples. Okay. When I interpolated, I put markers along the interpolation. Markers belong in the origin. And I could be the nearest neighbor that I've marked it. So those time series are your original. They are the original candidate. If I had had a generative model that generates the series, I could have actually simulated the series along its simulation. But no, these are actually... One other technique I can use to try to interpret. That you can use to try to interpret or try to explain. Here now, individual instances, the main idea is to look at perturbations. So, the main idea with perturbations is you take a specific instance, you look at properties of that instance, and you see as you perturb a particular characteristic of it, does it change the class at all? So, there are many different kinds of techniques for doing this. We'll look at a version that generates these sort of salieness graphs. Graphs. The idea is called integrated gradients. So the main idea of integrated gradients, you imagine perturbing each individual pixel, and you look at the gradient of the class. So here I'm trying to predict bird or not, or bird and other species. And you see, does the probability of bird increase as I change each individual pixel? Per turbo the pixel in a certain direction, does the probability of bird change? This alone doesn't quite work, right? The reason it doesn't quite work is that for this kind of final classification, you're using a logistic kind of loss. So if you have too much activation, then the logistic loss saturates. So your gradient, you get almost no gradient. You just get that loss. So what people will do is they'll kind of shrink the image towards the zero image, the black image. And you'll look at the gradient along that kind of shrunken version. Along that kind of shrunken version. So it has a little bit more gradient from that loss. So you'll actually see the bird pops out the most. Yeah, it actually pops out a little bit earlier in the alpha. And then when you have the original image, there's not that much gradient at all, right? So there's almost no gradient. Pay attention when it gets to that. Here's the totally black image. So you see, the image of the bird is much more clear when you're in the bird. Is much more clear when you have that shrunken version of the image. That was the gradient. That's why they'll look at the gradient along the entire interpolation from the black image to the full blue image. And this integral is just adding up those gradients all along the series. Okay. So again, we can bring this back to the time series in a microbiome example. What does it output to you now? Now we get an interpretive, we get... Get the importance for every single time point, for every single species and every single example. So, here I've just taken some subset of people, and you can make some argument that for this person, the features that are relevant for this disease versus healthy prediction, they're kind of around the box, at least for this example. Some of the examples don't have as clear interpretation here. Somehow, this disease person, it's really important for its classification to look at the first few titles. So now for every single example, you're getting a feature-level importance measure. That's what they mean by vocal importance measure. So this is a very common technique in practice, also a very problematic. Very fraught technique. So, this is from an experiment where what they had done is they trained the ordinary MNIST classification, classify the digits. And they did this exact same thing with a version of the model where they randomized the labels. And now you have two different networks. You can calculate all these different kinds of local explainability techniques for the two different networks. Here's integrated gradients, the one I just showed you. Here's integrated gradients on the network that's trained with the correct labels. Here's the one where I trained on the random labels. And the thing that's a little bit disturbing is that the explanations look kind of similar. So the fact that they look kind of similar means that it's looking at some inherent property of the image rather than what the model is learning. Because this model has learned nothing. It's a random label, it's just a noisy network. Right? So this. It's not true that it's learned a lot about. It's learned a lot about eyes. This paper is about when you train with random labels. It does some kind of a PCA-like thing. Because it needs to distinguish images from other images. I actually don't. Or maybe I just forget. Yeah, GCA. Let me talk about it. Sorry. Yeah, I think disturbing. Yeah, it's disturbing, and maybe the bigger thing that it highlights is that for a lot of these papers, especially kind of going up to this era, is that just a lot of focus on subjective evaluation of these explanations. You look at the output and just subjectively, it looks like a reasonable explanation. It highlights the zeros, you just kind of trust it looks like a good explanation. And this paper is kind of highlighting, you really need to be a lot more careful with how you value this shit. The last kind of technique I wanted to highlight is called a concept bottleneck. This is when I mentioned about trying to bridge raw and human feature engineering with something that's learned from the data, this is the kind of technique I had in mind. It's actually a relatively simple idea. Instead of just trying to classify the species of bird, ask your annotators to provide much. Ask your annotators to provide much richer annotation. So ask them to tell you what is the color of the wing, what is the color of the tail, how long is the teeth. Ask them to provide all that kind of annotation. And now predict from the image to those concepts. That's what they call them. Very dense annotation. From those concepts, now you can predict the bird species. So why is this kind of related? How is this bridging these two different ways of thinking? At this step, this is just learning from. At this step, this is just learning from the data. You're going directly from the image to an attitude. But at this step, this is kind of the interpretable, the features that we already kind of want to work with going to the classification we care about. So what's nice here is you can have a little bit of control over how the different concepts relate to the final classification. So if you say, I don't want it to use the wing color in my classification, you can think of all the sorts of properties in your original data that you don't want in your model. Your original data that you don't want your model to use in the final presentation, you can kind of zero out that much. That's the main idea for this concept of all the neck model. I never change things right before your talk. But the idea I had taken for the analogy in the microbiome is: instead of just classifying healthier disease, think about many different kinds of disease symptoms, right? And then think about Right, and then think about many different kinds of health, like different kinds of healthy personal characteristics in the microbiome. I've hand-annotated those kinds of characteristics, so saying this person has these three symptoms, this person has these two symptoms. It's again related to the data generating mechanism. So it's, you know, how appropriate your concepts are to the data generating mechanism is going to influence the final performance a lot of questions. When you design the concepts, do you need them to be independent of each other? As independent as possible to be. As unique and as possible. I don't know if anyone studied this. For example, if the concepts are highly correlated with each other, or you just train the classifier, you might use the medium model, right? Yeah. Like, you might damage your downstream performance if these are very correct. That's true. But you could, like, you're a little bit free in this step, right? So you could try to use an approach that will first decorrelate or select variables. Select variables. So this is more, this is more like a, you can actually plug in whatever you want for these two steps. Yeah, I'm thinking about even the correct interoperability concepts are highly correlated for immigrants and redundant concepts. True. Yes. Yeah, that will compete with each other. There's actually another kind of problem where a natural thing you might see, if you see this picture, you might try to draw another edge. To draw another edge directly to the bird species, which is supposed to be the things that are learned just from the machine. But you definitely don't want to do that using naive approaches, because then some of the important information about a concept can flow through that path. So you somehow need to force those to be level. And people try that. So for the test data, do I also need an actual human penny? You don't. You don't. Right? Because it's learned the classification. Right, because it's learned the classification from the image to those annotations. Yeah, it will predict those. Right, so in the future, all you need is the image, and then it will predict these concepts. And from the concepts, it will use a pretty good concept to the final concept. Just want to add an extra layer to a multiple supervised learning path. Yeah, exactly. Yes, exactly. Yes. You're just changing. Yes. Uh you just chain together two different volumes actually. And you're using the output of one as it's recognized. So yeah, this is how in the microbiome example, now I just add in this extra model network of human-derived symptoms, human-derived concepts, and from those concepts I can make the final classification. And it gets comparable performance. And it gets comparable performance. Maybe not surprising again because this is a simulated example, but just want to highlight that this is again another one of those situations where it's both more interpretable and still accurate. Okay. So any questions about just the kind of classic methods? So we saw directly interpretable models, far progression. We saw crafting features, embeddings, integrated gradients, and concept bottom. Integrated gradients and concept bottleneck. I think it covers a range of interpretability and explainability. So, Sweden, a bird example just showed you, kind of first use the neural net to train these different features and then try to assign a meaning to each of these features. Or you define your features in advance from a human concept, right? So it's not like you have to interpret them afterward. You know the meaning at the beginning. So you have to know what kind of features. So you have to know what kind of features to look for. Yes, that's true. You do have to have some sort of language for talking about the image already. But it somehow it's just from, you know, something like bird wing color, that would be really hard to encode mathematically. Like you could, in theory, design a feature that extracts that RGB is yellow, so I'm going to start using yellow. But it would be very difficult, so instead you use this CNN. Yeah, I guess I mean I think it's an interesting idea, but I guess it's not quite the same as actually for twin colour, right? Because you don't know what the model's, like you still have a black box in the first step. You don't actually know what the model's doing to predict wind colour. That's true, but you think of it as it's somehow, it's automating the human feature extraction process. So if you have a very good, if it does predict wind colour very well, then it's, you think of it as being a good enough velocity for that. Think of it as being a good enough epoxy for that hand capitalist very special. But it's true that you don't know exactly what it's looking at in order to get that structure. I guess I'm just imagining you could still have these kind of weird problems that some background correlation. Yeah, yeah, that's exactly what I mean. I mean, just restricted. Yes, that's true. That's true. If you keep on you could go as many layers. How is this supposed to compare to a table that we use? And after I get the class repaired, then in the back, I try to understand what the feature the CN tell me is explainable. I hope how that is compared to that. Okay, so you're asking for first is trained and then interpret afterwards. And that's how you interpret our output. I think this is a much more constrained approach, right? Which has its benefits and costs, right? Benefits and costs, right? So it's more constrained in that you can kind of know that just by looking at that last from the bottleneck to your classification, it's maybe a lot easier to understand how it's making the classification. And you can more directly control what it's using to make your classification. But the downside is, if there's important features that we don't already know about, then it's not being used. It's the idea of happening. Yes, exactly. Yeah, that's trying to get figuring out how exactly you can. So, that's just the question that I asked you earlier: how do you trust a human? How do you trust a learnable feature something from data? So, it depends on how big your data size. Do you have a humongous yeah, of course they can like data screen now for itself? And if you have small sample size, of course the human judgment may matter. Yeah, it also depends on the context that you're trying to use the model and how much control you need over each step. It's a concept to the final label you can train another classifier. Yes. And you can train them jointly, you can train them sequentially. Let me add one thought here. Second question to me is essentially you are constructing a latent space, find by human interpretable features, project data into this latent space. Yes, as far as PCA is extracting features. Yes, I think it makes it worse. So let me make a comment here and also kind of individual taste for solving the problems. I like open source and I I like open source and I like the video principles. The leaders principles generally set up the scene, the one thing, the sum of the one really good. But for here, for me, like, the bottom contact bottom line, but you have to save intermediate time. That's true. To compare with the directly the final outcomes. So how much they compromise the four layers? So why we kind of put this? Or why we kind of put these things together. Okay, I mean, I think this is kind of getting back to the interpretability accuracy sort of trade-offs, right? So kind of one of my arguments is that this doesn't always necessarily exist, but by having to force some intermediate learning, then you are again constraining the possible model at low cost cost detection. Okay, I'm wrapping up here. Five minutes. Okay, I'm wrapping up there. So okay, so I might skip a couple slides, but this, I think, is one of the more important for the final part. And I think some of this discussion is already getting at this, is that, okay, machine learning, so much of the progress has come from having very clear objective criteria where we just need to optimize that metric. And this image at there is very clear common. Very clear, common task that we all gather around and solve that. One of the difficulties in interpretability is that we're somehow realizing that there are many other values that we care about. Like we care about fairness, we care about being able to do well on out-of-domain homeless. There are these other values that are important in any real-world machine learning system, and which haven't really been part of the metrics that we put into our usual evaluation. That we put into our usual evaluation. So, this is the challenge: how can we somehow encode those values and use them to make some more systematic progress? It's not something that's happened in machine learning so clearly so far. And I think this is actually something where statistics has a lot to feature. So, why? So, first is that instead of just building methods, we tend to be a lot more careful about what exactly are the assumptions. About what exactly are the assumptions that go into that method for your interpretation to be appropriate? So, I think of the sort of issue with the integrated gradients and the random label integrated gradients. I think in statistics we might think a little bit more carefully about what properties do we expect our explainability output to have. And maybe the bigger point is that we're very used to thinking about problems in their larger context. So, this is from Future of Data Analysis, two P's. And number five, he has the dangers of optimization. So, you know, Tukey wasn't thinking about machine learning benchmarks. He was thinking about these statistical optimality results and people being so focused on proving statistical optimality in a very constrained model that they lose sight of the larger problem. And something similar is happening here, where we're so focused on just this one benchmark metric that you lose sight of the larger issues about. The larger issues about how people are going to use a podcast. And that's these are the interpretability. I skip over some of these in detail, but I've not really talked too much about evaluation so far, but this is one of these major open areas. How can we make sure that we evaluate interpretability techniques in a very systematic way? So, kind of common technique here is actually have people interact with. Actually, have people interact with the predictions from models and have them see if an explainability output helps them better reason about how the model will perform. Actually, I should talk about this. This example is kind of fun. So, this example is they have a rating, they give like some text ratings. It's your job to say, is this a good rating or a bad rating, just from the text one? Positive sentiment, negative sentiment. You're allowed to read the text. You're also allowed to look at the AI. To read the text, you're also allowed to look at the AI prediction, the trains of sentiment analysis model. And what's interesting is that if you give the human plus AI output, then they do pretty well. If you give them an explanation along with the AI output, they do a little bit worse. What's happening is that people started distrusting the output of the AI even more when they see the explanation, even when that output is not actually that good. Output is not actually that good. So, when you start doing these human studies, you start noticing these nuances, and how are people actually using vaccinations? And I'll skip over this, but just kind of the main point is the AI that we're seeing now is different than AI from just a few years ago. They're more multimodal, they're more interactive, they're supposed to be generalists rather than focused. Supposed to be generalists rather than focusing on any individual task. And all of these are issues that really haven't made their way into interpretability. And just to close by previewing the rest of the day, so later today we'll hear from Cythier Rudin, Hong Ku Zhu, and Yuan Ji, who are going to be talking about expanding our class of directly interpretable models. So we don't have to necessarily rely on explainability. Have to necessarily rely on explainability techniques as much. And Hubert Benecki and Devish Moda are going to be helping us think: can we have a better language for auditing models? And not just kind of the individual model, but the entire machine learning pipeline. So it's making sure we understand that the data going into our system is clear and that we have some rich suite of techniques. Some rich suite of techniques for once you get some output, you can actually explain it. So that's the preview. Thank you for all the questions, and I appreciate it. I'm also happy to answer anything offline, and looking forward to the rest of the day. Thank you everyone. Briefly, the workshop. Next half hour is going to have a coffee film. Um right outside the stage.   Yeah, which I can't probably just check and see what saving your browser is crazy. You're laying one on the students. Yeah, I'll just get the large. So, yeah. Yeah, it's gonna happen. I am not going to tell you because I have to talk to you about it. No, it's not the control.   Or you can go back to the future.   Yeah, that's the message. I think the idea will not provide it for the Yeah, I think that's what we did. I think you could talk about the session. I did send it to you. We're talking about selling. I just don't know. Yeah, great. Yeah, did you send me the slides? I thought I was more comprehensive. That's great. Yeah. I didn't want to interrupt your thought, but I think the concept of the slides. Lots of issues. Very controversial. We trust the machine and not trust. Machine doing everything better than you. Always word design. Why is that judgment I would trust more? In terms of anything, I would develop. Yeah, but it's not because I can do the reverse thing and get the things I try to understand why the machine is doing, but why the machine makes a mistake. That's also XAID XAI is all about that. I did like the work. So that's just one expert to settle by the phone. All the word design tasks you can be machine compiler. Word design, you can quite enjoy it. Even quite a bit. Yes, yes. So then we did actually exist on the social license. Actually, it's for them. I was surprised though. Yeah, so don't interest those kinds of stuff. Yeah, you're talking about your topics. And do it. Like, Turkey has a lot of problems. If you kind of try to stay submodules different model and then maximum C Here's the importance, here's the channel. I mean, it's all makes sense. I think it's all mixed. If you only look at one point bytes, you can use that bytes. I mean, in our study, there was some when we are trying to analyze, okay, the document task is like predicting if the model is wrong or not, which is hard points. But then it's actually like 0.5 messing down and makes some model predictions. Now we give them explanations and see if they better. And see if they better than they decide to do it, which is the so we give them like one and they have to once again say the model is set. Yes, exactly. And then after that, so the decision is set. Then he gets another support. And now so of course there is a lot of like psychological scientist. I'm a computer scientist. I don't know how to science, but we work with my psychologist to make it more like it. But it was fun. And then at the end, you can change the picture and then you can show it more quickly. So you can imagine like the charge pushes can be put in verbal buttons. So you give like different buttons or whatever. For example, even data. Yeah, exactly. Or we like to compare local to global simulations. And then actually we have seen in some cases. Yeah, yeah, exactly. So before, okay, this is the hardware. Like, if there is a signal, then maybe it makes sense. And we did that to. Yeah, and we did that to kind of um persuade people that it's interactivity because it's kind of 100 every minute. So it's actually like published the longest compute time. So preferably not quickly but as you start to read. This is a landing phase. I think what is the science textualization. Yeah, I like it's like just custom story. So, yeah, I said like three or four hours.