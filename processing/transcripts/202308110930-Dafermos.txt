I very much enjoyed this meeting and I've learned many things. So thank you very much for giving me this opportunity. I wish I were there with you, but it's very far. It's morning for you. It's night for me. But I can see you even from a distance and this makes me very happy. Me very happy. So, I will talk about hyperbolic systems of balance loss with stiff source, but I will begin with some background information. So, my starting point will be to recall a fundamental theorem that we all know. And this is the well-posedness of the Cauchy problem for hyperbolic systems of Systems of conservation laws, the famous theorem of William, that of course has been improved, amended, extended by many other people, by Type Liu, by Alberto Bressan and his students, his collaborators, and so on. And I remind you that we have a general system of strictly hyperbolic conservation laws, capital U taking values in Taking values in Rm is a state vector. The system is hyperbolic in the sense that the Jacobian of F has real distinct eigenvalues, characteristic speeds of the system, and n linearly independent eigenvalue vectors R1 and Rn should remember this notation because it will play some role in what follows. And if we start out with initial data with small total variation, there exists a unique solution of class BV. That solution is also stable in L1. The total variation remains uniformly small in time. And I want to remind you two important features. First of all, that the restriction to small total variation is unfortunate. Variation is unfortunately at the present time necessary. We don't know what happens when the total variation gets big. And also that the reason that this theorem has been proved is for the insightful idea of GLIM of the role of dispersion. So when you have two waves interacting, the total variation may increase at that time. Variation may increase at that point locally, but two waves have only one chance in their lifetime to interact as they're moving away from one another. And this is what keeps the total variation bounded. And this was achieved by means of the so-called GLIM functional that is global in character, does not control the total variation at each point, but controls Each point, but controls the total variation throughout the flow. All right, now I want to move to the case of systems of balance laws, in which we have also this source G of U. And usually in PTEs, this may turn out to be boring problems in the sense that if we know how to handle the principal part of the system, then adding a terminal Then adding a term of lower order does not change things much. And this is still the case here, so long as we are staying locally in time. And it's an exercise, a tedious one, but an exercise to prove that indeed, if you start with initial data with sufficiently small total variation, again, for a short time at least, there exists a BV solution. APV solution and the total variation remains bounded for each time within that time interval. And so long as the total variation stays small, we can keep going and we can even construct a globally defined solution. But of course, the restriction that the total variation remains uniformly small is not easy to enforce. Now you see instead of Now you see, instead of having the estimate we had before, we have a different kind of estimate. The bound depends on time. This parameter rho is often positive. So after a while, the total variation increases beyond the range where we can really deal with the problem. And the challenge is to characterize the class of sources for which this thing will not. For which this thing will not happen. This is what we have. I want to point out here that there are two reasons by which the source may induce increase in the total variation. The obvious one, the trivial one, is when the source itself has big variation and therefore increases the variation of the solution. But sometimes it turns out that the source itself is Source itself is small or dissipative, and still it may force total variation to increase. This is a subtler phenomenon, is the scattering of waves. Because now when two waves interact, after they interact and they start moving away, they are scattered by the source and may return and interact again and again and again, thus increasing. Again and again, thus increasing the variation. And this mechanism has not been studied sufficiently. It's a very complicated geometric problem. So the only hope for retaining the variation bounded is to have some source that it is sufficiently dissipative to prevent this phenomenon. Okay, so now what sources are dissipative? What kind of, what makes a source dissipative? What makes a source dissipative? Well, I should start with the easy case, the dream case in a certain sense, of strong dissipation. A strong dissipation is characterized the way that you can see on this slide here. You construct this n by n matrix A by using the eigenvectors of the Jacobian of the flux. Of the flux F, you take the eigenvectors, you put them together, and you form an n-by-n matrix, and then you define this A here. And when A as a matrix is strictly diagonal dominant, then indeed, whenever you start with small total variation, the total variation stays small and in fact eventually decays exponentially to zero. Exponentially to zero. Now, what is the function of this assumption here? The function is the following. If the system were linear, then this condition is what it takes to render the semigroup generated by the system a contraction, a strict contraction in L1. So, of course, then the total variation also is bounded. Also, is bounded. And even in the non-linear case, it's not difficult to show that this property is preserved somehow. So, this is, of course, the dream case, but unfortunately, it's also a dream in the sense that this condition is not satisfied by the vast majority of systems of interest. Systems of interest usually arise when you are dealing with the phenomenon of relaxation. The phenomenon of relaxation. We want to model some type of relaxation. So, when we have relaxation, it is true that the source is dissipative, but not as dissipative as this assumption here requires. What usually happens, as I'm sure all of you know, is that in the systems of interest that model relaxation, some Relaxation, some of the equations are in conservation form. They are not affected directly by the source. And some are affected by the source that may be dumping. But because only some of them are affected by this source, you don't have this strong dissipation property that this thing requires here. A thing requires here. So, the challenge here is to see how we can solve the problem under the assumptions that usually arise in practice. Now, here is a theorem that deals with this situation. Now, you want to deal with sources that have the following property. First of all, First of all, they're dissipative in a gross sense, namely, you have a convex entropy that produces positive entropy, and as a result, the total entropy is decreasing. So, this is, of course, a sensual dissipation. This is not enough for our purposes. The other condition that you need is Condition that you need is that the matrix A that I had before that was supposed to be diagonally dominant, now it does not have to be diagonally dominant, just its element along the principal diagonal have to be positive. The off-diagonal elements may be whatever they want, but still the elements along the principal diagonal have to be positive. This is a condition that more or less tells us that there is some kind of Or less tells us that there is some kind of collaboration between the various equations of the system. In fact, it can be shown that the presence of entropy together with that condition is equivalent to what is called the Kavashima condition for systems with relaxation. So, this is indeed the condition that usually provides dissipation, but Dissipation, but only in a gross sense, not in a point-wise sense, as was the case when we had strong dissipation. So, that theorem tells us that it doesn't matter if you have gross dissipation, then it is possible to convert it to local dissipation and to have again a global existence theorem. So, you can see what the theorem says here, starting with initial data with small total variation. With small total variation, but in addition to that, assuming that the decay at infinity as x goes to infinity is sufficiently fast to render this integral here small. So, under these conditions, indeed, there exists again a B V solution, and that in the total variation remains bounded. And in fact, as you can see, the contribution of the total variation of the Of the total variation of the initial data measured by delta is forgotten as t goes to infinity, but you stay with the contribution of the total mass of this function here, of the initial data, that stays there. Initially, eventually the total variation goes to zero, but may go to zero very slowly, not exponentially fast, as in the case of strong dissipation. Okay, now how does this neuron mean? Okay, now how does this theorem is proved? How can you manage to deal with the situation in which you don't have strong dissipation? It's by using a device that I call the distribution of dumping. So the distribution of dumping, you will see of course as I go along exactly how it works, is a device by which you change the state vector in such a way that equations that were not dumped Equations that were not dumped are borrowing some dumping from the equations that are dumped. So you take from the rich and give to the poor in a certain sense in order to have some kind of socialist regime. Unfortunately, this change of variable cannot be local because it's impossible to make the things locally dissipative by its nature. By its nature. It's the same philosophy as the gleam functional, somehow, that you have to give some kind of change of variable that is global in character. And you will see in an example that I will use as a model case how this thing works. Of course, here I'm interested in the case of the zero relaxation limit, the case of a stiff source. In other words, Stiff source. In other words, now I have what I had before in the previous theorem. And what I have here this parameter in front, mu is the relaxation time, if you wish, and mu is very small. And I'm interested in the case in which actually mu goes to zero and see what is the zero relaxation limit. This is a standard problem in mathematical physics to understand. To understand these stiff systems, you see as μ goes to zero, this source term becomes stiffer and stiffer. And the question is to see what happens. And for that purpose, we went to show that there is some class of initial data for which you have existence of solutions for all mu. Same initial data, variable mu. Variable mu and that the total variation of the solutions is bounded and small uniformly in time and uniformly with respect to the small parameter mu. And then, of course, in that case, we have a limit and there are other questions, questions of uniqueness and so on. These problems were set in a seminal paper first by Chen Levermore and Yu. And of course, as you all know, And of course, as you all know, there's a lot of research that has been done on various aspects of these questions. So my problem here is to have a B V type theory that can deal with these questions. Now, I said before, what I will do actually, I will present everything in the context of this model system. This system System arises very often in the applications, describes, for instance, the propagation of planar waves in a three-dimensional elastic, non-linear elastic medium, describes the oscillation of a flexible string in three-dimensional space. So U and V takes values in some Rn, so it's a system of two n unknowns to any quarter. unknowns, to n equations in two n unknowns. This s of u given function is the gradient or some convex scalar function phi of u. And this induces, of course, an entropy for this system, the mechanical energy, which is this function here, which as you can see is convex. And for n equals one, this reduces to a system that is perhaps familiar. To a system that is perhaps familiar to all of you, this system here, but governs the flow of a gas, one-dimensional flow of a gas in a porous medium, right? So this is the standard system here. The reason that I want to use the system, the bigger system than this model system as my example, is because, of course, for this system here, there's many. This system here, there's many special tricks that one can use, many special features, which do not apply for the general system. And what I want to present is a blueprint for something that has general applicability, does not require any special structure of the system, requires very little structure of the system, the presence of a convex entropy, of course, and this Kavasima type. This Kawashima type condition imposed on the source. Okay, so this is the model that I will use throughout. And in the case m equals 1, in the case where p of u is 1 over u, this is the only case that up to this point to have a definitive answer to that question. In that case, if you start with bound If you start with bounded initial variation, not necessarily small, you do have a solution for all mu, right? And this was done by two separate teams many years ago, Amadori and Guerra, and independently by Luo, Natalini, and Young. But of course, this, as you can imagine, uses very strongly this special. Very strongly, this special form of this equation is the famous Nishida case that has very special properties. What I want to do is, of course, to do things in general. And for that, I have to work very hard, as you will see. Okay, so let me tell you first of all, what's the main result? So, I start with the system here. I impose them here, I impose initial data and I make the following assumption of initial data. Unfortunately, there is a misprint here. In front of this term here, I should have a weight factor 1 plus x squared. So I want initial data that are in H1. And in addition to that, they decay at infinity sufficiently fast. So if I multiply this by one. So if I multiply this by one plus x squared, it is still integrable and the integral is small. Why these weights will become clear in a minute, but I want to perhaps tell you at this point the general reason why you will see these weights coming over and over again. It will become, as you know, the natural spaces for hyperbolic conservation also in one space. Hyperbolic conservation also in one space dimensions are of the L1 type. It's L1, BV, et cetera. Unfortunately, the only estimates that we have, general estimates, are of the L2 type, because these are estimates induced by the entropy. The entropy is of quadratic growth near equilibrium. So it gives us estimates in L2. L2, right? So we have to pass from L2 to L1. We have to use L2 to get information on L1. And locally, of course, L2 gives us very good information on L1. But at infinity, L2 is much weaker than L1. So this is why I have to use spaces that are L2, but are weighted at infinity in such a way that they contain. A way that they contain L1. Now, this would be used over and over again. The reason that even though the entropy gives you a bounty ne2, you can also get estimates that at infinity, heavy weight one plus x squared is that the That the speed of the waves propagate with constant speed, right? And therefore, the support of the solution, if you wish, if you start, let's say, with initial data with compact support, then the support of the solution increases, but increases linearly in time. And there are enough estimates, space-time estimates, that allow you in that case to get bound. You in that case to get bounds on weighted L2 norms. So, this is again a general feature of all the methodology that I have been using for dealing with such problems. So, this is also reflected in the assumption in the initial data. You want the initial data to be essentially of compact support or to decay fast at infinity. But then once you Infinity. But then, once you have this condition, it's true again that we have a globally defined solution, the total variation is many small, and also you have the additional properties in the time direction that give you the necessary compactness to pass to the zero relaxation limit if you want to pass to the zero relaxation limit. So, this is the main reason. So, this is the main result. But the important thing is not the result, but rather the methodology on how to get to this result. So, this I will try to explain now. Now, I told you earlier that the method that I was using to deal with problems in which I don't have this stiffness difficulty was a distribution of dumping. So, now I want to explain what is the distribution of dumping. Explain what is redistribution dumping in the context of this particular example. So, you see, in this example, we have two equations, two vector equations, and the second one indeed includes the source, which is of dissipative type. Immediately, you can see that it has a dissipative character, but the other equation is in a conservative form. So I have to. So I have to find this change of state vector that will take some of the source from the second equation and pass it to the first equation. And this is how this is done. You consider this phi, which depends on the solution, and you see it's a non-local transformation. And you change And you change, you keep the variable u, but you change the variable v into y, which is given by that. And under this transformation, the system is transformed into this form here. And now you can see that the dumping has been redistributed between the equations. Half of it stayed with the second equation, but the other half Equation, but the other half moves to the first equation. So this system here, this part of the system is strongly dissipative and ready somehow to employ the thing that we had before. However, the new difficulty that arises is that this new term here, the source, that's induced by the change of variable. So let me predict. So let me pretend that I know this in advance. So what I would wish to know on this term in order to be able to push my program through, I want the total variation of this term here to be small. Now, what is the total variation of phi? Is this term here is 1 over mu times the L1 norm of u. Now, think for a Now, think for a minute that I know mu. Mu is, let's say, one. Then what I need is that the L1 norm of u, the total mass of u, remains bounded and small. So in order to employ this technique, this idea is first to get some kind of bound on the L1 normal dissolution. And this Solution. And this indeed can be done under the assumptions that I said before, under the assumptions on that this diagonal elements of the matrix A are positive, etc. So, this is more or less how the method. Or less how the method tries to work. Okay, so if mu were bounded, the method works. Unfortunately, when you want to apply it in the case of the stiff source, the method fails. And the reason that the method fails is the following. You have, of course, one over mu in front of this term, one over mu in front of this term, and one over nu in front of that term. This mu can be scaled out. This can be scaled out. Just by scaling space and time, this can be scaled out. Incidentally, one would have thought from the beginning: why don't you scale space and time to get rid of this one over mu? Unfortunately, if I do that, my assumptions on the initial conditions go bad, right? You can see here. Can see here in the main result that I have to make this kind of assumption. And you see, if you change the scaling of the space variable by mu, it's in front of here, you are getting a one over mu squared, right? So you cannot rescale mu at this level. However, However, um after this uh redistribution, when I reach this stage here, the only thing I need is that the total variation of the initial data is bounded, and this is invariant under the scaling. So, this one over mu does. So this one over mu does not bother me at all. Unfortunately, there's another one over mu here. You can see that as mu goes to zero, this term here gets enormous. For instance, in here t equals zero, u is close to its initial value, which is some given function. So this thing is independent of mu near t equals zero, and it's multiplied by equals zero, and it's multiplied by an over mu. So that's what makes the method of redistribution of dumping to fail when you are dealing with stiff sources. And what I have to do now is to explain how you can overcome this difficulty. So the idea is the following. Let me try to do the following thing. Break the state Break the state vector into two parts. So I break U into P and W, and I break V into R and Z. And in such a way that the bulk of the mass of the solution is associated with the P R part. So P and R are smooth, let's say, for... And R are smooth, let's say, functions, and they carry the bulk of the mass of the solution, whereas W and Z carry the oscillations of the solution, but they have very small mass. So it's oscillating wildly, of course, within the requirement of bounded total variation, but the total mass, the integral, is small. The integral is small. So the integral goes to p and r. Now, if you do that, I'll explain how you do it and why, but for the time being, let's say naively, yeah, that's a good idea. There are many choices of P and R that you can think of that carry over the bulk of the solution. What is the difficulty? Well, I suppose you. Difficulty. Well, suppose you choose P and R for the time being. What is W and Z? W and Z satisfies a system of this type. So this is, again, as you can see, a system similar to the original one, except that it's no longer homogeneous, right? Instead of having here S function of P only, you have a function of P plus W, and etc. All right. All right, and you want this system here to carry the oscillations of the initial data, but very small mass. So the question is: how can you choose P and R so that the resulting system here has very small mass? Of course, you can start with initial data. You are free to choose the initial data in such a way that the mass is small. And you will see how this is done in a minute, right? But this is not enough. Enough. The mass has to stay small, uniformly small, with respect to mu and with respect to time. And this cannot be done by arbitrarily chosen P and R. So after a lot of experimentation, I found out that a choice, a choice is not necessarily unique, I don't know, but it's not obvious in a sense. And after And after a lot of experimentation, I find out a method that works. And this is to choose P to be a solution of this system here, which you can see is like the porous media time of equation. If P were a scalar and this term were not here, this would have been the porous media equation. Perhaps a posteriori, this is not. A posteriori, this is not surprising because we know that the porous media equation is what describes the long-time behavior of solutions of the separate system of the hyperbolic porous media system that I had before. And here we have Ron Hua, who is an expert, an expert in that area here. So perhaps Rong Hua would have guessed this right. Rompois would have guessed this right away that perhaps it's this particular kind of equation, particular system that will give us the answer, right? But it turns out that it takes a lot of work and it's a very delicate situation that things balance out exactly right to give us the correct answer. Perhaps somebody with deeper insight than I have. Than I have would have guessed that by devising some kind of scaling that explains why this thing works, right? I could not find a single scaling that could give the answer. There are partial scalings here and there, but it's not enough. So to me, it's magic in a certain sense, but perhaps for somebody else it will be easily motivated. Okay, so now. So now, how about the initial conditions? As I said before, the initial conditions, of course, have to reflect this property that the initial values of P and R must take a lot of the bulk of the mass, and what's left will take the oscillation of the initial data. And you do this by using modification of P0. Of P0 and Q0 by using a kernel, but again, this kernel has to be very special, it turns out. It has to be compactly supported and it has to be symmetric, right? If it's not symmetric, we think they're not working. Okay, so that's how you start out, and you can see that you, of course, now P0 and Q0. Now, P0 and Q0 are smooth functions, so they have derivatives of any order. But let's not forget the presence of mu here. So the integral of p and its first derivative, of course, are bounded uniformly with respect to mu because of my choice of initial conditions. But if you are going to higher derivatives, they grow like one over mu squared or one over mu to the square. Mu squared or one over mu to the fourth, and one has to keep track of this very carefully, as you will see. Okay, so now here is the problem. Here's the algorithm, if you wish. I want to start by solving this initial value problem for this parabolic system. For this parabolic system, starting with the initial conditions of P0 that had before, I think that I have found P. It's no problem with solving this problem. This is another soft problem. Then you also find R by using this formula here. And then you insert them into this system as known functions. And you try to find W and Z, hoping that you will be able to. Hoping that you will be able to find the distribution of dumping that works for that particular system. So, this is the program. Okay, now as far as the first problem is concerned, it turns out that you have to find a number of a priori bounds on solutions. And here I have listed the quantities that have to be bounded. Okay, this is the This is the system that I have to solve, and this is what I have to prove. I will need in what follows all of this information. Of course, I have not time here to convey why I need all this information. The reason I listed it is that you will take some idea of the variety of information that you need. You need some. That you need. You need some, let's say, pointwise estimates like these ones here. You need estimates on the variation. You need estimates on integrals. Also, to point out that there is a particular dependence on the parameter mu of these quantities. Some are independent of mu, some grow like one over mu, one over mu squared. one over mu squared, sum, one over square root of mu. And this is extremely important. In other words, if you change this a little bit, the thing does not work. So as I said, it has to fit very precisely into the picture in order to make the things work. Now, deriving these estimates, on the other hand, is not so difficult. It takes a lot of work, as you can see, because there are so many of them. Are so many of them, but you are just using standard energy estimates for this parabolic system. You only have to be careful to make sure that you don't miss some power of mu, because as I said, it's extremely delicate. It has to depend in a very particular way on this mu. Okay, so suppose that I have done this work and I have found bounds, all these bounds, what do I do next? What do I do next? So, what I do next, now I have to use energy estimates for the reduced system, this system here for W and Z. So, here you need some Yapunov estimates, Yapunov functions that are not obvious, right? The previous ones, it does work. Here you have to try and you have to be lucky. You have to try and you have to be lucky somehow in order to get these estimates. They're very long, very, very technical, so I'm not going to bother you, to torture you with explaining what these estimates are. Here I have how one starts getting these estimates. You start by estimating, by taking the time derivative of these quantities here, using the system. Using the system, and you find that you have terms that you can bound, you can estimate, you can bound, and the end result, the goal that you want to achieve is to obtain this particular estimate here, that the L2 norm of w and, weighted by this factor 1 plus x squared is bounded by C mu. is bounded by C mu square epsilon square. So the mu square here is crucial, right? Why? Because this will give you this estimate here for W just by sources inequality. And therefore, one over mu times this integral here would be bounded. You remember what Induced the failure of a redistribution of dumping is that I had to deal with this term, the total variation of phi, that was one over mu times this quantity here. And I told you this quantity here, of course, this thing goes to infinity, and mu goes to zero. Well, by this breaking the state reactor into two parts, State vector into two parts, I reach the point in which one over mu times the L1 norm of W is now bounded. And therefore, I can perform the redistribution of dumping that I said before. Things, of course, become more complicated because we have all these extra terms, but you hope, and in fact, the hope is materialized, that because of all the estimates that I have. That because of all the estimates that I have obtained before, you are able indeed to get this bound here and the result here, which is the key result for providing solution to this hyperbolic system here. Okay, now, of course, what we need is a bound on the total variation before, and one would have thought that, okay, we have the system here, and now Here, and now we have to check that the conditions that give bound to the total variation work. They don't work. So, this system here was devised in order to get the a priori estimate. But in order to get the bound on the total variation, you have to perform still another transformation of the state variables, which, of course, as you can see. Of course, as you can see, it's very complicated. You have to go from X to a new variable Z, which is related to X through that thing here. It looks quite mysterious, and of course it is, but after working very hard with the problem, you gain enough experience to devise what the change of state vectors should be in order to get to. State vector should be in order to get to the answer. And upon performing this change of variable here, you end up with this system for w and x. And what you have here on the right-hand side now can be controlled. It's very complicated. This is this capital theta here. Capital Omega, the total variation is exactly what we know that is bounded before. And it turns out. And it turns out that also the total variation of theta is also bounded, and this gives us the final result. I wish I could show you some of these calculations, some of these estimates, because some of them are cute and interesting. But unfortunately, this is not longer a one-hour lecture, would have been a five-hour lecture. In five hours later. So I think I'll stop here. Thank you very much for your attention. And as I said before, I very much enjoyed the meeting. We still have two lectures to go, of course. And thank you very much for giving me the opportunity to be part of it. Okay. Thank you. Thank you.