Thank you very much for the invitation. It's a pleasure to be here. It's not to make people who is online jealous, but the city is great. The food is great. People is great. So it's really a pleasure. So I want to talk about equations defining paleogenetic varieties, which most of you know, but a new method to find these equations. To find these equations. Okay. No. Is that one right? Okay, otherwise they can do it less. No, no, it works. Yeah. Okay, so for those of you who haven't heard about phylogenetics, phylogenetics studies the evolution of species in the earth and it's usually represented in a phylogenetic tree or maybe in networks. Maybe in networks as well. A phylogenetic tree represents species that live nowadays at the leaves of the tree and ancestors at the interior nodes of the tree. So the root of this tree would be the common ancestor to all living species, and then each interior node would be an ancestor of this living species. And there will be these speciation processes and the branches represent. Processes and the branches represent all these evolutionary processes. Okay. So, phylogenetics is not only useful to know the history of our living organisms in the earth, but also it has been used to detect which species have to be preserved in order to have a larger biodiversity or to detect the origin of certain viruses, for example. This is how they. This is how they knew that SARS-CoV-2 in humans came from bats by building the phylogenetic tree. It has been used also in some tracking of cancer cells and in linguistics as well. So it has different applications. And the goal is, I'm given some data nowadays. This data is usually DNA data or amino acid data, which would be part of the genome of this disease. Which would be part of the genome of this species. And I'm given this data in the form of an alignment. An alignment means that each DNA sequence I have, so as DNA is a double-stranded molecule, we just need to read one strand and represent this molecule by nucleotides, A, C, G, and T. So we have a word in these four letters for each species. And we have an alignment. It means that each column Each column, I don't want to use this. Okay, that way. Each column in the alignment represents that this nucleotide that we observe at this species has come from the same nucleotide at the common species, at the common ancestor of these species. So maybe in the first position, this common ancestor had an A as well, and had an in. But maybe in the third position, you will see some changes here in the chimp sequence. chimp sequence so we don't know whether it was a C or a G or some other thing in the in this common ancestor so this is our input data and then we want to guess which is the most suitable tree most likely whatever you want to measure it and how do you want to measure it it doesn't matter now so I would like to know whether it has been that a gory lanchima speciated separately from human or It separately from human or not, human and chim separated from gorillas and other primates, and so on. So, we all know which is the correct tree nowadays. And we do this by modeling evolution. There are other ways of doing this by putting some distances on these sequences, but one of the most common ways is to model the substitution of nucleotides through a Markov process on a tree. Okay, so. On a tree. Okay, so we are going to assume that all sites of this alignment evolve independently and in the same way, independently and identically distributed. This is an assumption that can be made in some parts of the genome. And then once you have this, you can go for more complex models that account for that remove the identically distributed part. Okay, but for the moment, let's do this so that at each note of the So, that at each node of the tree, we assign a random variable that takes values on A, C, G, and T, the four nucleotypes. Okay, and the parameters of this Markov process are going to be a distribution at the root of the tree. Here I'm drawing a rooted tree, but we will see afterwards unrooted trees. And then we put a substitution matrix at each edge of the tree where we write the condition. Where we write the conditional probabilities of substitutions of these nucleotides. So these are parameters of our model because we don't know these parameters. And then we let a Markov process on the tree by meaning that the random variable at each node is independent of its non-descendants given the previous node, okay, its parent node. And with this process, we can write the joint distribution at the leaves in. Joint distribution at the leaves in terms of these parameters, like a formula like this. If I want to know the probability of observing ATC at these leaves, then I will have to sum overall possibilities at the interior nodes, Iyr, Y4, and multiply by the probability of having a certain nucleotide IyR at the top, and then by the corresponding entry of the transition mate. Corresponding entry of the transition matrix. Okay, so we have a polynomial expression in terms of the parameters of the model so that we have, here is where algebraic statistics come in. And actually, we can view this process, this Markov process on a tree as a parameterization map, as a polynomial map that goes from the set of parameters of this Markov process. This Markov process or this Markov model, which could be the entries of this distribution at the root and the entries of the transition matrices. And the arrival space would be the set of distributions on the four leaves. Okay, so four leaves, I'm sorry, I said four leaves, and leaves. Okay, that could be either observing AA at the leaves, AAAC, AAA, and so on, up to TTT. So that this, we start with... So, that this we start with a linear space here of parameters, if I don't assume that they are positive. That's something I'm not going to talk about this in this talk, but can be done afterwards. So we have this map that sends it to something, some set that doesn't need to be an algebraic variety, but if we take its closure, then we can. Take its closure, then we can talk about ultra variety. Okay, this set of distributions, if we don't think about positive distributions, would almost fill the whole variety. We move to C because we want to work with equations of these varieties, we want to work with algebraic geometry. So that would be the phylogenetic variety associated to a tree. So I have this tree, I associate to it a Markov process on it, and we have this parameterization map, and we take the closure of. And we take the closure of this image, and we have this variety. And the ideal of this variety, the elements of this ideal, are usually called phylogenetic invariants. Okay. So when I have a data point, my data was an alignment of a number of species. From this alignment, I can estimate the jump probability of observing AAA, AAC, AAT, and so on. AAT and so on by counting the relative frequency of these columns in the alignment. So I have a data point in this arrival space. Okay. And if this data was generated by this model, then this point, data point, should be close to this variety or close in some sense. Okay, so actually, the most interesting thing would be when I have different tree structures and I have a data point. And I have a data point, I will have different phylogenetic varieties for each tree. And the idea could be try to guess to which variety this point is closer, closer in some sense, again. Well, of course, these varieties are not circles, I mean, are not spheres, and it is not easy to get the equations of these varieties. Okay, I'm gonna, this is what my talk is about, more or less. Let me tell you about some of these equations. So, there is an easy way to get equations for these varieties. It's by flattening this joint distribution vector. So, if we have for the rest of the talk, I think I'm going to concentrate on four leaves, but everything I would say works for a larger number of leaves. Yeah, I think. So, So, but it's easier to work here. So, let's see that. Let's say that we have this 256 component vector of the joint distribution at four leaves on a tree of four leaves, on a quarter tree. And we flatten this joint distribution, this vector, into a 16 times 16 matrix, where I'm going to call the flattening that the splits one, two against three, four. speeds one two against three four by putting the levels at leaves one two at this at the rows and um at leaves three four at the columns so that each entry of this metric is at the has the levels at the row corresponding to the observations at leaves one two for example in this in this row we have ac always and at the column the corresponding um n tip for the leaves three and four okay it's a Leaves three and four. Okay, it's another way to express this vector. Or if you think about this vector as a tensor, then you are just flattening this tensor by splitting or grouping two of these tensor spaces separately. And it turns out that if your distribution came from this tree, from a Markov process on this tree that splits leaves one and two against three and four, then And two against three and four, then this platinum matrix can be expressed in something like this: the chronicle product of these two matrices at the leaves. And here, again, the chronicle product of the matrices at the leaves. And then in the middle, we have a 16 times 16 matrix, but a matrix that has rank four. It contains the entries of the distribution pi and the entries of m5 in just in four rows. It's a very simple matrix. simple matrix so this matrix the whole product of these matrices is going to have rank four by four no no these are 16 times 16 matrices all of them okay so this is what harmon and roads proved that if you have a point that arises on a on on this tree as a markov process on this tree then the rank of this platinum matrix would be less than or equal to four Than or equal to four. And if you, instead of considering this flattening, you consider the flattering that puts leaves one and three at the rows and two and four at the columns, then you could have a round 16. If these parameters were general enough, it means that if these matrices were invertible, actually. So this says that the five by five minors of this flagoning matrix are elements of the ideal of this phylogenetic variety. Of this phylogenetic variety. And actually, they are what we call topology invariants because they vanish on this phylogenetic variety, but do not vanish on the ideal of a phylogenetic variety for another tree. For example, for the tree that splits leaves one and three against two and four. Okay, so this for me, from the goals I have in mind, that it's always phylogenetic. mind that it's always phylogenetic at topology reconstruction, these are the most interesting elements in the idea. And we recently found that these flattering conditions, these rank conditions and flattenings actually hold also on networks. Okay, if you have, for example, this network that has this split of leaves one and two here at the bottom and then three and four outside. And then three and four outside. So, this network can be thought as a mixture of a distribution on these three and a distribution on the same tree topology, but with different transition matrices at the interior edges, then it is very easy to see that the flattening corresponding to one, two against three, four has still run four. Okay, the important thing is that there is this edge here splitting one, two against the other. Splitting one two against the other. Or actually, this can be proven more in general. If you have a tree clade that has leaves A, a certain set of leaves, against the other set of leaves B, then if you flatten your distribution like this, then this would still have rank four. Okay. So these rank conditions on flattening have been used in different for In different have been used to propose different methods of phylogenetic reconstruction. Why? Because they are very easy to evaluate. Instead of taking the five by five minors, we should take the singular value composition of the matrix and then compute the distance of this matrix to the set of rank four matrices, okay, which is done with the singular values in a very easy way. Also, because this Because this theorem of Almond Rhodes not only works for nucleotides, but also works for any number of states. So if you want to deal with amino acids, you could do that as well. The problem is that if you want to work with amino acids, that would be my next goal in my research, then we cannot deal with this so much, so general models like the general Markov model I was introducing. So we should look for more simple models. We should look for more simple models. Okay, so I'm going to try to talk about these more simple models and how these flattening conditions translate into these models. So that's why I'm going to talk about G-equivariant models, and then we will talk more about flattenings. And I will approach these flattenings in two ways, via representation theory or just by intersecting with. intersection intersecting with some linear spaces okay so we have been talking about the most general um model that would be the general markov model we let the transition matrices be whatever so the only restriction is that the sum of rows is equal to one and we let the distribution at the root also be anything okay any distribution so we have three parameters And distribution. So we have three parameters at the root and 12 parameters per matrix. If we think about this in terms of amino acids, if we want to generalize it to amino acids could have 20 by 19 parameters per edge, that's too much. On the other side, that could be a very simple model, that could be the Jux-Cantro model. It could be just a single three parameter per edge, that could be the probability of no mutation. A probability of no mutation. So actually, the transition matrices have this shape: one parameter out of the diagonal, and one parameter in the diagonal, but the sum of them A plus 3B must be equal to 1. And then the distribution at the root is assumed to be the stationary distribution of these matrices, which is the uniform distribution. Okay, so the distribution at the root, it's not a parameter in this case either. So we just have a single parameter per edge. Okay, and that would be the most simple model. It was proposed by biologist Jux and Cantor in 69. And if we look at this matrix, this matrix has lots of symmetries. It has so many symmetries that it is invariant by any permutations. Any permutation I do at the rows and at the same time at the columns leaves the matrix invariant. Okay, so if I take S4 to be the set of permutations of The set of permutations of A, C, G, and T, and take any of these permutations and call PG the permutation matrix. Then, if I apply this permutation at the columns of the matrix or and I apply this permutation at the rows of the matrix, I get the same matrix. Okay, uh, this is called when a matrix satisfies this, this it is said that this matrix is equivariant, okay, according to this. Okay, according to this group action. And the distribution at the root, if you observe, as it's a uniform distribution, is also invariant by any permutation we do of the nucleotides. So we have in this model equivariant matrices with respect to this group S4, and the distribution at the root is an invariant vector. This equivariance of the matrices can be also written in this way: that if you have an entry of the matrix xy, and you apply any permutation g, then this entry is going to be equal to the entry gx gy. Okay, gx mean that you apply this permutation to the corresponding index, so to the corresponding nucleotide. Okay, and this translates to And this translates to an action on the joint distribution at the leaves. Because if we have that this, we had that this distribution was written in terms of the parameters of the model. For example, that we put as example before ATC was written in this way. Now, if these matrices were Juxanto matrices, if I have one of these permutations, permutation G. Permutations, permutation G, and I write the corresponding coordinate, the probability of observing GA, GT, and GC at the leaves, then I would express it in the same way, okay, just putting GA, GT, and GC here. The thing now is that if these matrices were JuxCantor's matrices, as they are equivariant, I have that equality there. I could put this is the same. I could put this is the same as G minus Y, G minus 1, Z R and A here, and here the same G minus 1 Z R and so on. So that when I sum over all this Z4 and ZR, it's the same about summing all G minus 1, Z4, and G minus 1, ZR, so that I would get exactly the same expression. Okay, so if your matrices in the matrices in the Markov process where Jux can't draw matrices, then you would have this equality. The probability of observing AAAC would be the same as observing GAAA or GAGAGAGC for any permutation G. Okay? So this gives us some equalities on the showing distribution. So what we saw is that when we have these parameters or this parameterization map that was parameterizing our phylogenetic varieties, we had this arrival space that was C4 times C4 n times. Okay, we can rename this space C4 as This space C4 as the space generated by four elements, A, C, G, and T. So I'm going to think about this A C, G, and T instead of nucleotides, I'm going to think about them in the standard basis of my C4, okay? So that this Jung distribution that is in the arrival space, actually, if I take the natural basis of this tensor product from that basis. From that basis, then this joint distribution can also be thought as a tensor in this arrival space. And it's important to think about tensors here because there would be some invariance of tensors. And in the same way that S4 acted on the set of four nucleotides, on the set of ACGNT, now it acts on this basis of this arrival space because we knew how to. Because we knew how to apply a permutation to A, C, G, or T. So we know how to apply a permutation to this tensor product of ACGs and T's. Okay, so we have this group S4 acting on this arrival space. And what we saw before is that from this equality here, actually, and from this definition here, we get that any point in this arrival space, if it was generated by a Markov. Equo-generated by a Markov process on a tree with Jukes canton matrices, then we have this equality that this tensor in the arrival space is equal to any permutation applied to the same tensor. Okay, so for Jux counter parameters, the image of this polynomial map lies not only in this tensor space, but in the set of invariant tensors in this space. Okay, we call it L. Okay, we call it LS4. This is the set of tensors in C4 times C4 times C4 that are invariant by this action of this permutation action at the components of the tensor. So in this way, we are seeing Juxanton model as an instance of G. Jukes-Canter model as an instance of G-equivariant models. Okay, to talk about G-equivariant models, we consider G a subgroup of the group of permutations of four elements. We consider W as before, the vector space C4 with the standard basis ACGMT. And we call WG the set of G-invariant vectors. It means vectors in W such that this permutation. Such that these permutations of the group led the vector invariant. Are these symmetric tensors in the usual sense? Symmetric tensors? No. No. No, no, no. No, I don't think so. So we consider G-invariant vectors and G-equivariant matrices, that means matrices that commute with. matrices that commute with the permutation matrices when I take a permutation in this group G. Okay, so a G equivariant model is an evolutionary model that has a root distribution, transition matrices, but the distribution at the root is a G invariant vector and the transition matrices are G equivariant matrices. Okay, for example, the scantor was a G equivariant model when G is equal to Equivariant model 1g is equal to the poor group of promotations s4. So the distribution at the root in that case was uniform, is the only g invariant distribution, and the transition matrices were equivariant matrices. Yeah. Can you just like, how does this compare to speeding cloud phase? So is like every clue phase. Okay, I'm gonna tell you a little bit. Actually, I wanted to explain this approach because it's something I had to be to get familiar with. Something I had to be to get familiar with. It's not my approach, I will tell you. But I wanted to explain you this to give you another way of working with group-based models instead of group-based models. Not all, well, I'm going to present some equivalent models here. Not all of them are group-based models. Okay, this is a more general framework. So for example, other G-equivariant models, we could consider the strand symmetric model. We could consider the stand symmetric model, which is not a group-based model. Okay? The stand-symmetric model, the distribution at the root satisfies these symmetries, that the probability of observing A is the same as observing T, and the probability of observing C is the same as observing T, due to the double strand symmetry of the DNA molecule. Okay, an A is always paired to a T and a C is always paired to a G. And for the transition matrix, And for the transition matrix, the same. If I permute AC and GT, then at rows and columns, I would get the same matrix. Okay, so the group acting here is this group, the transition IT composed with the transition transition, is it called? Transposition, no? Transposition, okay. A transposition IT with the transposition CG. And the matrices have this. Have this, um, have this look. Okay, so we have actually three parameters per row, and we have six free parameters only here. Okay, we have, or these are also called central symmetric matrices. The Kimura parameter model, which is a model that you may know better, is also an instance of a G-equivariant model. In this case, these matrices hold the These matrices, well, the distribution of the root is uniform as well. The matrices have this shape, these symmetries. It also has a chemical explanation because nucleotides can be grouped into purines and pyrimidines, and wherever you are changing from between different groups, or you are within the same group, you put a different parameter or not. Okay. And within this model, there is the Kimura two-parameter model if you let this be equal to D, and there is also. V equals D, and there is also the Jux-Cantor model, of course, as a special case of this. Okay, so in this case, this Kimura Fi parameter model is invariant by permutations ACGT and AGCT and the group that is generated by them, which is an abelian group, it's Z2 times Z2, which corresponds exactly to the group-based model when you view this model as a group-based model. However, with the cumulative parameter model, To parameter model, we have this group. This is the hedral group. And when you view it as a group-based model, you have to put this group and some lava links on this group. Okay, so it's a different way of dealing with these models. And with this framework of geo equivariant models, we can talk about the varieties that arise. Varieties that arise in this setting. If we have a group within the symmetric group of permutations of four elements, and we restrict the parameters of the polynomial map or the Markov process to these G equivariant matrices and the root distribution invariant by the group action. Then we have this map that we are going to call with this separate index. It's this super index G. And then the phylogenetic variety is the closure of the image of the map, and we're going to denote it also with the G on top. Same for the ideal. As a special case, we have the general Markov model. When the group is the trivial group, then we have the general Markov model here. So everything I say also works with the general Markov model. And as we saw for the Jukes Cantor model, in this case, the image of this map lies in the set. Lies in the set of invariant tensors. Okay, so the arrival space is much more smaller, is the space of tensors that are invariant by the action of the permutations in this group. Okay? And this arrival space is actually defined by model equation. So it tells us whether we're working with a model or with another model in the sense that if you have a point in this affirmable space in this Arrival space in this have one of these gene by M tensors, then this can be written as a mixture of distributions on trees evolving under these models. Okay, so the space of mixture of distributions of trees evolving on these models is this Lg. So this space doesn't tell us anything about the tree structure, whatever, but tells us many things about the model. Model. So, the equations for these models are well known. There has been work from lots of people here, for example, for different models, for group-based models, for the fancy matting model, for the CFN model. And then Raisman Katler gave this model. Catler gave this more general framework of using G-equivariant models to deal with all the models at the same time: general Markov, group-based models, all models. And they came up with the same conclusion as everybody. That was the first work of Hermann and Rhodes that actually could see that the ideal of the phylogenetic variety for any tree can be written or can be. Or can be derived from the ideal of clot trees or chipots and from flattening grant conditions on flattenings on edges. The only problem is that the purpose, from my point of view, is very theoretical and is for me was difficult to read and to understand. And it's more theoretical than practical. I would like to use the equations for some G-equivariant models, maybe not for. Models, maybe not for nucleotides, because for nucleotides, maybe they are not so interesting, but at some point for amino acids models. So actually, what I'm interested in is in phylogenetic topology reconstruction. So as we just saw, that the phylogenetic variety for this group lies. Variety for this group lies inside this space of invariant tensors Lg, but also lies inside the variety of the general Markov. Okay, testing whether a data point lies in this LG, in this space of invariant tensors, is very easy. This is a linear variety, so we can do exact maximum likelihood computations use. Well, we did it with Matthias actually now. In a paper. But I don't want to test for the model. I want to assume that the model is correct or that we are projecting now to the model that best fits the data. And then afterwards, I would like to test for the tree. Okay? So wouldn't the equations of VT suffice to test for the tree once I know I'm already in the model space? It would make sense. It would make sense, wouldn't those strategy equations be the same for testing whether I have this point in this smaller variety? So my question was, is this smaller variety equal to the intersection of these two varieties? This is something we already asked. Yeah, Bt is the general Markov variety. The general Markov, and that could be Bt Gida, Shukskant or Kimura, Stan Simetic, whatever. Or Kimura, stand symmetric, whatever. Actually, this is one of the questions we asked at the beginning, beginning, beginning, probably with working with Luis. And when we were working with these questions, and we said, no, no, no, it's not true. Okay. And I can tell you it's not true in general. But even if it's not true in general, because, well, would there be a way to adapt the run conditions from flattening to these G models? G models in an easy way, I would like not Drais McCuller approach, an easy way that I could do a singular value decomposition immediately, for example. And let me give you an example for the Hughes canter. This is an arrival space that could be the smallest one in this case, because the group is the bigger one. So the model is the smallest model. It's very easy to see that if you have, you let this group S4 acting on. This group S4 acting on the coordinates, then the probability of observing AAA is the same as observing CCC because you let, for example, any promotation that sends A to C, and you have this first equality here. And you can continue with all coordinates, and you would have these equalities. Actually, for example, for the second row, the important thing is where you have the distinct element. It's in the fourth leaf, then. It's in the four leaves, then you can permute anything. And then I put here a times four because you can put this thing element in any of the four leaves. And in total, you would get 245, 241 equations. These are 256 variables? Yeah, 256 variables. But if you go to the ideal of this variety for the Jux counter case, it contains some models. Case, it contains some other linear invariants. Like these are called Lake's equations. There are two independent of them. One is this. And as you see, this equation is not a linear combination of those. And in this ideal of the general Markov model, there are no more linear invariants. So it's clear that in terms of ideals, the ideal of this variety is not equal to the ideal of the general Markov plus these linear equations. It could still be true that in terms of varieties this would hold. So, in terms of ideas, I could mean that the radical of this idea would be the other one. It could be true. I guess it's not true. But here we have dimension five because we are dealing with juxtaposed four leaves. Okay, so dimension of the variety is five inside the much bigger. Yeah, yeah. The dimension count doesn't work. Yeah, that that's the that's the thing. But what we have been able to prove is that our small variety VTG is an irreducible component of that intersection for any tree for any G equivariant model. Okay, and actually what, well, the way we proved it is by proving that there exists an open set in the space of parameters. In the space of parameters quotation and Markov, that if I cut my parameters with this open set, then the image is exactly the intersection of this open set of parameters with the space of invariant tensors. Okay, so the difficult part is starting from here, you have a tensor, a G-invariant tensor that is in the image of the general Markov. If you take matrices whose diagonal entity in each column is different than the rest of the N. Different than the rest of the entries, then you following Chang's result on the well, Chang gave a way of computing the pre-image of each data point or of each point in the image. Okay, so by checking carefully his result and adapting it to G equivariant models, you can prove that the prey image is a G equivariant. Is a G equivariant is formed by G equivarium matrices. Okay. Good question. I haven't checked. I haven't checked. I still don't know if it's true or not for Jup Scantor in four leaves because I haven't put the questions and I haven't made the computations. But I guess it's not true that, so I guess that it's not. That so, I guess that it's not irreducible. That's my claim. Because if you don't do the stick of the open set, you it's I don't think you can prove it. Ah, chang is the one that has a paper in 96, more or less, that gives a method to recover the parameters. If you have a point, a distribution that has a reason on a tree with certain marks. Horizon on a tree with certain Markov matrices, general Markov. Then he gives a method to recover the parameters. It's an spectral method. So you do the marginalization over some leaf, you invert that matrix and so on, and you can recover the parameters. And starting from a G invariant tensor, you can check the proof and prove that actually the parameters are G equivariant. Actually, the parameters are gene equivalent. So, I would say that this intersection is not irreducible in general, because as I was saying, I really need this open set. But for example, for Claude's, I've been using the Smalltrees web page, which is a FAIR research data. That's the best example I know. And it's maintained by Luis. And is maintained by Luis. In there, you can check that if you put your K81 equations and cut with the equations of the linear space for K80, then you get exactly the ideal of K80. And the same from K8 to Jux counter. So this process here, I could do it not only from General Markov to any other submodel, but from any submodel to any other. I could do it in steps if I want. Okay? So open. Okay, so open question for the workshop. In which cases is this intersection irreducible? In which cases do we do not need to take radical here, for example, as well? Okay, so here are some examples. I didn't check it for general Markov because the questions for general Markovs are not so easy to write down. Okay, so and it's not in the in Louis webpage. I couldn't use it. I couldn't use it so easily. Okay, so, but for the rest, I would like to still deal with G-equivalent models, but about plan conditions and flattenings. Because as I said, these are for me the most interesting equations. So now Now we view these J aquery models as well with the tools of representation theory of groups. This space C4 generated by ACGNT, we can view it as the permutation representation of the group of permutations on four elements, which just means that we have a group morphism from Group morphism from S4 to the linear group on this space C4 that sends each permutation to the permutation matrix. Okay, this is a very easy group Morse phase. And if I have any subgroup of this S4, then restricting this map to this group G, then I would get that W is also a representation of this other group. Okay? The action that we had before. The action that we had before on vectors on this space, where I was letting the permutations act on ACG, which were the basis of this vector space, is the same as now applying this permutation matrix to the vector. Okay, so it's exactly the same as we were doing before, but in another setting. And there are other linear representations of a group. And whenever you have a map like this, from the group to the Like this, from the group to the linear group of the vector space, you have a linear representation of the group, or if you prefer, you can think about linear representations as G models. And then equivalent maps are G maps. Okay, so for example, the tensor product of W, W, W is also a representation of G. And then here, all this machinery that Deisman Kudler introduced in the framework of phylogenetics. In the framework of phylogenetics, this irreducible representation characters, Schurz-Lehmann-Maschki theorem, which I didn't know about, comes in and allows to work with all the models at the same time. And to tell you about this, let's put this most simple example, which is always Kimura T-parameter model, and that's why it's my favorite variety, as I mentioned yesterday. So this group. So, this group in that case was this abelian group, Z2 times Z2. And in this case, there are irreducible representations of the group in the sense that we couldn't find inside sub-representations. In this case, the reducible representations of the group have dimension one and correspond to the dual elements of the group. And that's why I called these representations. These representations with SAP in the C's A C G and T because this group that two in here we could associate A with this element, C with this element, for example, G with this element, and T with this element. And then the dual group. The dual group could be in correspondence with this ACG and T as well. That's why I call these iridescent representations A C, G, and T with a small letter. Then any other representation you have is going to be written as direct sums of copies of this irreducible representation. That's the important thing. This is Mashke theorem. And these copies of irreducible components are called isotypic components. Isotypic components. Okay, so you have any other representation, for example, W times W, and this can be written as a sum of some copies of this erodition representation. The number of copies are going to be collected in a vector of multiplicities. Okay? And in which sense are either this representation important or this decomposition important in the sense that any equivariant matrix Any equivariant matrix is going to be blocked diagonal if I find a basis adapted to this decomposition. Okay, and that's the good thing about this decomposition into isotypic components. So for example, in this case, my first representation was W. W is not an irreducible representation in this case. It has dimension four, but it can be. Case it has dimension four, but it can be written as one copy of each of these irreducible representations. Okay, so the multiplicities in this case are one, one, one, one. And there is a basis of this double these components that when I group this basis into a basis of W, I would get a block diagonal matrix. In this case, as these subspaces have dimension one, this would be in the Spaces have dimension one, this would be that my matrix would be diagonal matrix. And the eigen basis for all K81 matrices is the Fourier basis that is well known in the context of root-based model. Okay, so if I get the Fourier basis, which I denote by A bar, C bar, G bar, and T bar, then my matrix is going to become a block diagonal, actually diagonal in this case. Okay? And what if I want to deal with the flattening of a Flattening of a tensor in four copies of W. Then this flattening could be a matrix or a map from W tensor W to W tensor W. And if this tensor is in the is a G-invariant tensor for the group of Kimura model, then this flattening is a G-equivariant matrix. It means that there is a basis in which this matrix would Matrix would be block diagonal matrix, okay? Which is the basis. So I have to decompose W, tensor W into isotypic components. I look at the multiplicities. In this case, each of the isotopic components is four copies of the irreducible component. Okay, so I have to find a basis for each of these subspecies. And in this case, it's very easy. I just tensor the Fourier. Or the Fourier basis against itself, and in this component corresponding to A, there could be those tensors that sum to A. Sum in which sense? In this sense? Yeah. So this W tensor W. So the irreducible representation could be quadratic monomials CG. And you can sort of see that in your face. So in which sense can you index? So this is not an air. And you index. So, this is not an irreducible representation. No, no, no, this is not. I'm writing it as copies. Each W cancer W can be written as isotypic components, one corresponding to the same character. So, the same irreducible component. This is four copies of the irreducible components. The characters are the characters are monomials in A, C, G, and T. So the relevant characters. So the relevant characters here would be quadratic and you sort of see that in your base. But in which sense is this an isotopic component? In the sense that, okay, let me see. For example, in the sense that this component corresponding to this copy of the trivial irreducible component, that would be the trivial one, this is the set of. One, this is the set of G-invariant tensors in W tensor W. Okay, but these are not, these correspond to other irreducible components, okay? So in this case, finding the basis is very easy. If you want to mimic the approach of group-based models, you can observe here that the sum of the two Fourier bases here or indexes is A in this case. Is A in this case if we take some in this group. Here it's A, here it's C, here is G, and here is T. Okay, that's why I put this here. And then this flattening matrix can be written as a block diagonal matrix in these bases. And we have four blocks, each of size four by four. These four blocks correspond to those coordinates that some. Correspond to those coordinates at some A, C, G, or T. And from the work of Teresa Man Kohler, you can deduce that what they claim is that these blocks each have rank smaller than the multiplicities of W, the corresponding multiplicity of W. So we're going to get quadratic equations here. Okay, rank smaller than or equal to one. When you go to Jukes Candor model, To Jukes-Cantor model, things are much worse because we have five irreducible representations and zero from N4. And the problem here is that the irreducible representations do not have dimension one. We have some of dimension one, n0 and n2, and n1, n2 has dimension two, and n3 has dimension three as well as n4. And our representation w is only a copy of w0. only a copy of w0 and w3 okay has dimension four this had dimension one and this has this had dimension three so the multiplicities are one zero zero one and zero okay but in the same way if I find a Fourier a certain basis of this W adapted to this decomposition equivariant matrices would be blocked diagonal in this basis in this case I could still use the free case I could still use the Fourier Fourier basis but then grouping T G and T in this in this W in this copy of W3 and then diagonalization here we get the same eigenvalue and actually maps from an irreducible representation to itself due to Schurlema are just homoses so it's what we see exactly here how does the platinum matrix look from play we will see it okay that's not We will see it okay. That's not next slide. That's the problem. Okay, if you want to see the protony metrics, then you have to do exactly the same for W tensor W. You have to write it in isotopic components. Look at the multiplicities of the irreducible, so how many copies of each irreducible representation you have. So you have these multiplicities: 2, 0, 1, 3. Multiplicities 20131. And you know that in a certain basis, the flattening is blocked diagonal. The problem is now the basis is not so easy as take intensive products of the Fourier basis. You have to work a little bit more. You have to project onto these isotopic components. But there is an algorithm, right? There is an algorithm. Yeah, true, true, true. Yeah, yeah, there is. There is the projection. The projection is known. Yeah, that's an algorithm. The problem is that you want to do this for The problem is that you want to do this for larger flattening, then. And there is an extra problem that, as these irreducible representations do not have dimension one, we know for sure Lemma that these maps among an irreducible representation itself are homorphices. So we have to eliminate redundancy. And instead of dealing with a 16 times 16 matrix, we can deal just looking at this with something that can be read from Can be read from these multiplicities. We can deal with two zero one three one times two zero one three one matrices. Okay, seven times seven matrices. And the thing is, the run of this new flattening, this thin flattening, taking the redundancies off, because I don't want to repeat equations. Equations in blocks must be smaller than or equal to the multiplicities of w, which were one, zero, zero, one, z. Okay, so here we get some quadratic equations, but here we get some linear invariants because of these zeros, no, because of these zeros, we had some linear invariants which are exactly legs invariants. Okay, so you can recover them from here. Okay, and okay, I'm almost finishing. Okay, I'm almost finishing. Sorry about this. What if there would be an easier way to do this? So I have my flattening. I write it in free coordinates if I want, if not, it's not a problem. I apply the group action on this coordinate so that I would get some symmetries. If I'm writing it in free coordinates, I would like to translate these symmetries into these coordinates. Okay, it's not exactly the same as working with the Okay, it's not exactly the same as working with the original distribution. And in this case, the two permutations of the generate the group act on this Fourier basis in this way, by, for example, sending t to minus t or whatever. And then you can guess, we can deduce that all these Fourier coordinates must be zero if I have an invariant tensor here. Okay, and this is exactly the same that you get when you work with group-based models. Work with group-based models that the coordinates must be zero when the sum of nucleotides in this group is different than zero. Okay, so again, just imposing the action on the coordinates, you get again this block diagonal matrix without knowing anything about representation theory. Okay, just imposing this action. And then you have these four blocks. It's the same as imposing rank four. As imposing rank four on this matrix, because the general Markov model we had this theorem that said that this must have rank four. From the Eisma Cutler, you can guess that each block must have rank one. It's not exactly the same. So we could have a block of rank four and the other three zero. But if you, for example, choose these coordinates and impose that them are different from zero, sorry. them are different from zero sorry then you would get exactly the same conditions okay so this is the this is the main theorem okay here i'm i didn't put the example of juxander this is the main theorem there is an open set where the rank of the flattening matrix is uh if you have a tensor that is g invariant then the rank of the flattening matrix is smaller than or equal to four if and only if the ranks of the seam flattening The ranks of the seam flattening in terms of the ismacolor is written in terms of the multiplicity. So you just need to do an SVD with to detect the distance to rank for matrices. Okay, and this is also useful for polyphorescent networks, as I mentioned before. So we could also get invariants for any equivariant model on this. So thank you very much. So, thank you very much. Thank you very much. Um, so we have a number of questions already. Are there any more? Yes, thank you. It's a pretty interesting talk. Okay, let's we have to click to you can give that one. Okay, that's a pretty interesting talk. Okay, so for kids, right? So, for kids, right? So, So it'd be interesting to build like these are the exact same rank constraints you would get if you want to be able to do it. Yep, we have covered um the results. Yeah, yeah, we recovered the results. I don't know if there is any group-based model that is not G-equivariant in another, not in four states, but in more states. I don't know if there is one of the group-based models that could have been studied and doesn't fit in this framework. I don't know. I don't know. I don't know. No, no, no, I I couldn't claim that. Yeah. The ones that we work with, yes. Yeah, the classical ones, yeah. Classical ones. Yeah, yeah, sorry. Okay, this is on an ongoing project. We have I I just put this proposition here that with the same um theorem I had before, I can prove that these networks are identifiable. These networks are identifiable, so are distinguishable, okay? And as you proved in that paper, this could serve to prove identifiability for more general networks. Oh, I see. So there's still like a couple of more steps to do. Yeah, this is what we're working on now. Okay, awesome. Thank you. Third and last. Hi. I had a general question. I had a general question about whether or not there are connections between Markov processes on trees and tensor networks. So, in your talk, you have a vector at the root and then on edges of the tree, there are matrices all together producing a tensor. Whereas in tensor networks, one would have tensors on nodes, which would be contracted, altogether producing a tensor. Wow. I'm sorry, but I never heard about tensors. I'm sorry, but I never heard about tensor networks, so we can think about this afterwards. Maybe I'll answer this. So, this, so, what you're referring to is what the physicists and the numerical analysts like. On the other hand, what Marta spoke about is what the statisticians and computer scientists like. There's a very, very beautiful article by Anna Siegel and Elena Robeva that explains the connection between the two. So, it's a very, very nice. The labeling is different. There are two communities: there's physics. Two communities: there's physics and numerics on one hand, and CS and statistics on the other hand. I recommend that, Mike. Let's take a four-minute break. That would be until 10.07, and then Pyotr will continue.