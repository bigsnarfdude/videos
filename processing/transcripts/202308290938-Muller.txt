So Valerio already said, basically the last two talks perfectly set the stage for my talks now, my talk now. So a lot of the things that especially in the first part I will explain to you will be very familiar. And the last part will be then more about the numerical linear algebra and especially the integer servers that I used. So I should mention that this is work that I did during my time as a Work that I did during my time as a PhD student together with my two supervisors, Super Simon, in Darmshot. So, first of all, let me give you a quick introduction to the model that I'm considering. So, it's the Stokes flow model with random data here, which is set in my case in a two-dimensional physical domain that has a boundary with the theoretical and the Neumann part. And yeah, I also need this probability space here to probability space here to define the randomness in the equation properly. And then I'm looking for random functions with the velocity and the pressure u and v that fulfill the random flows model, which structurally is very similar to the deterministic one except of the dependence on the small omega here, which is an element of the big omega in the probability space. And we are working on these product domains here. And as the title already And as the title already said, the randomness originates from the data, so some of the input data to the equation must be uncertain. And if we take a closer look here, the data are the DLT boundary data, the right-hand side in the moment equation, they are both deterministic as well as the domain, but the viscosity is carries small and meager dependent, so this is uncertain. Just a quick comment: so adding Just a quick comment. So adding uncertainty to the right hand side and the DNA data would totally work for the approach that I'm considering, but random domains that can also be treated are a bit more intricate, and I will not talk about that. So having established that the uncertainty originates from the viscosity, we need a model for that. And this is the next point. So this expansion will probably look quite familiar to you because we just saw it a few minutes ago. We just saw it a few minutes ago. So, what you usually assume is that you have some sort of a fine representation of your random data given by a deterministic mean field and a potentially infinite sum of spatial functions and random variables, where spatial functions are usually assumed to be suitably bounded and the random variables are assumed to be independent. And then for the latest numerics part, you usually want to cut the sum after a finite number m. And for this to be a reasonably good approximation of your initial random field, you need some additional assumptions, but I will comment on that a bit later for a specific example. Yeah, so now you have this input model, but there is still one more choice to make, namely what Still, one more choice to make, namely, what kind of distributions you want to use for the random variables. So, in the work, we consider two different choices, and the first and arguably simplest one is to use just uniformly distributed random variables with an image domain, which is this m-dimensional unit cube that I will denote as psi m in the following. And in order to ensure that we end up with a positive. To ensure that we end up with a positive viscosity, we impose this condition here, which basically states that all potential negative influences that come from this sum here on the right must be compensated for by the mean field. And the other choice that also already popped in Elizabeth's talk is the log normal case. So you post this affine representation with uniform random variables. Uniform random variables which have an unbounded image domain, and then you use the exponential of this to ensure positivity. And what you also want to do, or yeah, in the following, or what we want to do is to work with the y's as variables instead of the omega. So you do a variable transform, transforming the problem into a from a stochastic Into a from a stochastic into a parametric one. And what I also want to restrict myself for the sake of this talk is mostly on the uniform case because it is much nicer to write down. But I will comment on the differences that the dog normal case has on the considerations I will present. Okay, so now that we have set our model for We have set a model for the physics as well as for the input data. I need a weak formulation to define my Galurkin answers. And since I do Galurkin both in the physical as well as in the image domain, you need the functions to be in these Bogner type spaces. So the solutions are functions that are square integrable with respect to the image domain. Respect to the image domain, or we can also call it parameter domain now, with respect to the measure of the random variables. So, this is the uniform measure in the uniform case and under Gaussian measure in the Gaussian case. And the values of these functions are in H1, 0 for the velocity and L2 for the pressure, respectively. And then you can write down such a big formulation here with bilinear forms A and B and linear function is L and T. L and T and the bilinear forms, like in the inner integral, this looks very much like the deterministic bilinear form, but you have the additional dependence on y and you have additionally this integral over the image domain with respect to the measure of the random variables that you chose for your input. Yeah, the same for the divergence linear form, bilinear form here, and also for the linear functionals, where this u0 here is just a linear form. Where this U0 here is just the lifting of the boundary conditions. A nice work for a priori analysis of such setting point problems can be found here in this paper from 2012, where they considered the mixed diffusion problem with random data. Then I also did some or produced some similar results for the Stokes case, also for the log normal. Input model. Input model. Yeah. So now that we have the variation in formulation, we can continue with discretizing this problem. And for the spatial domain, I use standard finite elements. So on a regular triangulation, Taylor Hood get finite elements, meaning quadratic basis functions for the velocity and linear basis functions for the pressure, so that you end up with a total of n degrees of freedom. Total of n degrees of freedom, so nothing special here. And for the image or parametric domain, I'm using a so-called stochastic lurking discretization. So this is something that did not explicitly come up yet. So this is an approach where you basically solve for all the parameters at once. So you have a global polynomial basis. So the polynomials are defined over the whole image domain. And in addition, And in addition, they are orthonormal with respect to the measure of the input variables. And to construct this multivariate basis, I use a complete basis of total degree k so that the total degrees of freedom end up to be q, which is m plus k over m. And this, of course, grows quite quickly with the numbers m and k. The numbers m and k, so you should not have these numbers to be too large. But the good news is that, like for these simpler problems like Stokes, if you have reasonably well-behaving input, then you can expect your solution, or you can expect the solution also to be comparably nice, and then the convergence with respect to these values will be very quick exponentially in the ideal case. Should be the ideal case. So this should be manageable for the right problems. And then if you plug in this discretizations into the reformulation, you end up, if you order the coefficients in the right way, with a settlement point problem. So structurally, this is the equivalent to the deterministic case, but the system of equations is Q times is large. And of course, you are. And of course you also know a bit more about the substructure of these matrices A and B. So A, the diffusion matrix basically mirrors the structure of the input. So you have like a mean part here, which is the quanical product of the identity and the mean diffusion matrix. And here you basically see the orthonormality of the stochastic or lurking functions. And you have the sum here. And you have the sum here with the stochastic Holurkin matrices that are not diagonal but have a very particular sparsity structure. So for uniform random variables they are, I would say, rather sparse, but for the log-normal case, they are quite dense. And also for the divergence matrix, they have the structure. So ending up with this large settlement point problems, we considered iterative solvers to To treat these problems, because there are like well-established approaches, and we chose two particular pre-conditioners or structural answers for these pre-conditioners, which is a symmetric block diagonal one on the left, with suitable approximations of the diffusion matrix and the show complement on the diagonals. And structurally slightly richer, non-symmetric pre-condition here on the right, which has this block triangular form and the divergence matrix in the lower left corner. And the iterative solvers we use to solve these preconditioned systems with are on the one side the classical minus method and this has been there has been a similar study again for the mixed diffusion problem with random data here. Problem with random data here in this 2009 paper. And for the non-for the block triangular precondition problem, we decided to use this specific conjugate gradient method, which dates back to this 1988 paper by Bramble and Pasciac. So this is often referred to as a Bramble-Pasciac conjugate gradient method. And in order for this to be applicable, you need two small modifications in your preconditioner, so you need to add the scaling factor A. To add the scaling factor A. Here this linear one, and you have to negate the approximate true complement part. And I will come back to that in a minute. Before that, I want to tell you what we chose for the substructures of these approximations, A tilde and S tilde. So we essentially followed existing approaches and used a mean-based approach, which is known to work. Approach which is known to work quite well, especially for the uniform case. So your approximation is a Kronecker product of the identity times a suitable preconditioner for the finite element part and luckily for the Stokes problem there are really nice well-behaving preconditioners. So a multi-bit recycle and the diagonal of the pressure mass matrix, which are both cheap to apply and are known to be spectral. And they are known to be spectrally equivalent. In the case of the log-normal input, you might want to use something more elaborate here on the left. And there are some approaches, but I will not go into details on that. Okay, so now back to this case on the right. So in what scenarios can you use this specific conjugate gradient method to solve this? To solve this non-symmetric Curly free condition problem. And the basic result for that is the Faber-Manteuffer theorem from 1984. And this is basically a very easy deduction from this general result. So that fits our setting. So we basically need the scaling to be in such a way that this difference here is positive definite, and then you know. And then you know that the precondition system is H symmetric and H positive definite. Where H is defined here below, so this is this symmetric positive if this condition is for just matrix. Yeah, and then you know you can use such a Bram-Bescher conjugate gradient method which lives in a, or which is performed in a particular inner product to solve your problem. And these conditions are not very hard to check, so the similar. Hard to check, so the symmetry you can just calculate, and for the positive definiteness, you can, for example, look at such a matrix factorization. And then the critical part is the upper left block. But this just boils down to the condition that we imposed for the result before. So, in the end, you need some information on the smallest eigenvalue of the preconditioned diffusion matrix. Matrix. This, of course, means that in the worst case, you need to compute numerically at least an estimate on the smallest eigenvalue of this preconditioned matrix. But what you can also do, like in very particular cases, and what we did is to compute an analytical bound for the eigenvalues in these dead. And this is one of the reasons why we derived some eigenvalue bounds for this. We derived some eigenvalue bounds for this particular problem. So these three sets of eigenvalue bounds, among others, have been derived by us. So the first one is the preconditioned diffusion matrix, then up on the bottom the preconditioned negative true complement, and in the middle something that looks like the preconditioned true complement, but there is the x hilda in the middle. And what are the quantities that turn up in the bounds? So C1, 2, C6 are just constants independent of modeling and discretization parameters. Then there is underline new 0 and the new 0 bar. So these are the lower and upper bounds for the mean visposity field. Then you have the square root of 3, which is the interval bound of the uniform random variables. And then you have this. Variables, and then you have this psi, this chi and bar, which I have forgotten to mention explicitly, so let me go back. Yeah, this is just here the sum over the n-infinity norms of the functions nu m. So these quantities turn up here in the bounds, and what you can also, of course, do based on this bounds is derive error. On these bounds, is derived error estimates for the deterrent solvers. So, first, using existing results, you can bound the eigenvalues of the preconditioned point problem. May I ask a question? So far, where where does the the stochastic the random farm play a role for the convergence? Um I mean you have because you decided not to have the the stochastic part and then the condition alive. Yeah, I mean I have it to some extent, so it's basically covered like by the identity matrix. So I mean you you end up with a deterministic problem, a deterministic linear system of equations that where like the entry sum somehow can depend on the discretization parameters that you chose for the stochastic space. That you chose for the stochastic space. So, I mean, the square root of 3 basically comes from the stochastic influence because it is the bound of the uniform random variables. And of course, these bounds look much more complicated for the log-normal case. So, this means that in the original system, the random part is really not very not so much for this uniform case. This is true. So, this is why this mean. This is why this mean approximation works so well in this uniform case. Because I mean, you are this mind, this is not necessarily obvious, but this condition is quite restrictive. So we will also see this in the numerical results later. So this does not allow for the randomness to be too big. Otherwise, you will not be able, at least not by this particular condition, to ensure that you end up with a positive discourse. The number of uh uh variables in uh the the random space doesn't matter in terms of uh uh not for the uniform case, but um because basically I mean what it boils down to is the the eigenvalues of these GM matrices here, and I mean in the in the uniform case, of course, also the eigenvalues. From case, of course, also the eigenvalues are bounded by the interval that I chose. So, this will not, at least not for larger values of k play a role, but for the log normal case, the dependence on the degree is not so simple. So, there you see a see for if you use this uniform and not this uniform is mean approximation, this will be of much bigger influence. Yeah, so you can use existing results to derive standard error estimates based on these existing results and you can do the same using the first and third bounds for the conjugate gradient method. And I mean what you can also to some extent do with these bounds is use them to explain the results that you see for your numerical. The results that you see for your numerical experiments. And yeah, we also perform some comparisons between these two approaches. And I will show you one of these experiments in the last part of my talk. So we considered this flow backwards facing step here with a parabolic inflow profile on the left and an outflow boundary on the right. And on the top and the bottom, there are steroids. Top and the bottom, there is the origin boundary. And for the viscosity model, I'm using this representation which results from a Carohn and Riff expansion that Elizabeth also mentioned already. So starting with a second order random field, so the viscosities seem to be a second order random field, and then you can use this Berlin-Werf expansion to produce such an expansion. Expansion. And in order for this to be a good approximation for a reasonably low number m, you need a rather smooth covariance operator for the second-order random fields. Otherwise, this will not be manageable in this stochastic or lurking setting. But the nice thing is that it produces exactly this affine representation that we assumed in the beginning. So this fits the other setting. And yeah, then we considered the number of iterations that are necessary to reduce the relative residual below 10 to the minus 6 for these three configurations. So the minres case and two different cases for the CG, one where I used the scaling based on the analytic eigenvalue bounds, where the scaling is computed numerically. And before I come to the actual iteration, Iteration counts. I have a small table visualizing the influence of the scaling. So in the first line, there is a relative scaling. So A is varied and this A ref is the approximation of the minimum eigenvalue, just preconditioned diffusion matrix. And you see that the closer you are to the actual value or the approximate value, the better, or the the fewer iterations you need. Fewer iterations you need, and this bold value here is the value that results from the analytic bounds for this particular setting. Okay, so back to the number of iterations, which we considered for varying different discretization and modeling parameters. The first two are the mesh width and the number of terms in the input representation, and as expected, you don't see. As expected, you don't see a media dependence on the mesh width because of the spectrally equivalent preconditioners for the finite element parts. You see a slight increase here in the beginning when the number of terms is increased, but this flattens down, which is also to be expected because of the decay properties of the input representation and comparison. And yeah, comparing the different configurations, you see here that minerals need the most iterations, then the conjugate gradient method with dynamical scaling and the one with the numerical scaling needs the fewest iterations and the same here on the right. And there is also additionally this dash dotted line here, which you can see as a correction of the min-res iteration count if you factor in that it is. That it is cheaper to apply than the conjugate gradient method in the sense that you don't have this lower left B block in the preconditioner. So this is in that sense cheaper. But still, even with this correction, the conjugate gradient method converges faster. And similarly for the total degree of the stochastic alurking, Of the stochastic and lurking basis, I have here results in then for an increasing standard deviation of the input representation. So I also forgot to point that out. So here, this is the standard deviation of the input random field, and this is varied here. So you also see this flattening with respect to the degree of the pull. Respect to the degree of the stochastic lurking basis, and this is what you already asked me about. So basically, you only have this interval from minus 3 to 3, and at some point, not much more is happening. But of course, you see like a dependence on the standard deviation because we have not taken any additional measures to deal with that. And especially for the numerical scaling, we see here that this DT. Scaling you see here that this deteriorates because this is, in that sense, even more influenced by that. Okay, so let me summarize what I just talked about. So we considered in this particular work a Stokes flow model with a uniform and a log normal viscosity, then used the stochastic Allergen. Then use the stochastic OLERGIN finite element discretization, which leads to a structurally a settle point problem as in the deterministic case. And then with an appropriate scaling, you can use this conjugate gradient method to solve the problem. And also, of course, you can use the minerals method with the symmetric pre-conditioner, as I showed you. Then we have derived some error estimates for the editorial dissolvers based on these eigenvalue estimates for the submit. Eigenvalue estimates for the submatrices and in the end compared the two approaches based on iteration numbers for different varying modeling displacation parameters and overall the convergence behavior is quite similar which was to be expected by the error estimates but at least for the experiments we performed the conjugate gradient method is faster at least in terms of iteration cost okay so Okay, so with that, thank you very much for your attention. Questions? I have just a much more comment. So now congratulations should be accompanied by simplifying applying the Brando plus recognition is much more expensive than it's not so. I mean it's it's not so ma I mean it you basically only need I mean okay at least concerning matrix vector operations you only have the additional multiplication by B I mean you probably have some more inner products but but besides that I mean it's because these are becoming huge matrices because you have a chronic care right so these uh sorry huge vectors have a chronic care for and this is true but I mean you have the same for the minrus method. You have the same for the minrus method. I mean, I have it, this is basically the algorithm. So you have multiplication with A, application of the show complement, recondition of B transpose B, A inverse B. So B is basically. So I mean it would be, of course, to some extent interesting, but I mean computation times always to some extent depend on implementation. So yeah. I felt like comparing it with respect to this, I mean, taking this additional factor into account, this may be somewhat more implementation-independent. I mean, I also did, like, with my implementation, the CG method was faster, but I mean, that doesn't mean if you implement it maybe in a different way or smarter, then it can turn out difficult. For graph of Patient, you need these eigenvalues and how much of a overhead is there. Yeah, that's true. So this is potentially a bigger overhead, but what again works for this particular case is, I mean, you have the spectrally equivalent finite element preconditioners, so you can solve the eigenvalue problem on a potentially much coarser spatial. Are spatially good, and then it's much more manageable. If not, we thank her for the first time.  So what