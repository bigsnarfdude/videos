Okay, so I'm going to talk about generalization bounds for message passing networks. So basically, the situation is that I have a training set of many graphs. And let's say I do graph classification or graph regression. And suppose I train a message passing network and it performs well on my training set. I want to be able to prove that it also performs well on the test set. So I'm going to focus on message. Focus on message passing networks. This is probably the most popular class of graph neural networks. So, let me give you a quick reminder of how convolution is defined in message passing networks. So, we have the graph and we have a graph signal. We have a feature in each node. And the way we update the value of the feature in the next layer is we combine the information from the neighbor. So, we define. So we define a trainable function that we call the message function. We plug in the feature. Oops. I think the laser pointer is not going to work then. How do I. Okay, I think it's better that I point with the cursor anyway so people in Zoom can see what I'm doing. Okay. Okay, so we take, we plug in this the feature value in the central node, and we plug in the feature value in each of the neighbors, right? So this is, we can think about it as a message sent from each of the neighbors to the central node. And then we need to collect all of these messages to update the value of the node. And we typically do something like max aggregation or sum aggregation. And I'm going to focus in this talk on average aggregation. Okay, so then we take this. So then we take this averaged message and we plug it in a different trainable function called the update function. And this is going to give me the feature value in the next layer. Okay, we also plug in the previous value of the feature. And of course, we do the same thing for all of the other nodes. And because we want to do graph classification or graph regression, at the last layer, we are going to do global pooling. So we are going to do to simply average the features. To simply average the features of all of the nodes together. This is going to be the output. Okay, so the goal is to show that if we train a message-passive network and it performs well on the training set, we want to give a guarantee that it also performs well on the test set. So let me first give you a reminder from statistical learning about how we deal with generalization. So we have a space, a theoretical. Space, a theoretical space of all possible graphs in the universe that I want to consider, G. We assume it is a probability space, and for each graph in the space G, we have a desired output, for example, the ground root class, the ground root label of the graph. And then we define a loss, right? So the loss takes, let's denote by theta the message passing network. So the loss tells me how much the output of the network Of the network approximates the ground truth output. And to define how well the network performs, I integrate the loss over the space of all possible graphs. Okay, so the goal in machine learning is to minimize the risk. This is how I define the risk. But of course, I don't have access to the space of all possible graphs in the universe. So, what is done in practice is I have a training set, right? I only have a finite number of samples. I only have a finite number of samples which I assume are uniformly and independent from the space G. And then instead of computing the risk, I compute the empirical risk, which is simply the sum of the loss overall of the train data. So this is the goal is to minimize what we do is we try to minimize the risk. Sorry, the empirical risk instead of the statistical risk. Okay, now suppose that. Okay, now suppose that I found a network that leads to a small empirical risk. The goal is to prove that the statistical risk is small, right? Because I want to do what it does outside the training set. So one typical approach for bounding the statistical risk is to decompose it as follows. We subtract the empirical risk and add the empirical risk. So if I know that So, if I know that my empirical risk is small, and I know it because this is simply the loss on the training data, right? I can compute it. So, if this is small, and I can show that the difference between the statistical and empirical risk is small, then I know that my statistical risk is small. Now, because this is a random variable, the choice of the data set T is a random variable because I randomly generate it. I need to somehow compute the expected value of this. So, this is going to be difficult because the network depends on the training set, right? I fit my network to the training set. So, if I want to somehow compute this expected value, I need to understand how training works, how training transforms the training data to a network. And this is, it's difficult to model. So, one standard approach is to bound this difference by what is called the generalization bound, generalization error. Generalization bound, generalization error, sorry, which is the supermum over all possible choices of message passing networks of the difference between the statistical and empirical disks. Now, this is going to be easier to bound because I don't need to know anything about how training works to bound this. Of course, this thing bounds this difference. Okay, so if you look at standard bounds from the literature, they usually take the following. Literature, they usually take the following form. The generalization error is bounded by something divided by m to the one-half. M is the size of the training set, is the number of examples. Now, this makes a lot of sense because if you look at the empirical risks, empirical risk, it looks like a Monte Carlo approximation of the statistical risk, right? I simply replace the integral by a sum over independent random variables. Independent random variables. So it makes sense to have m to the one-half in the denominator. Now, the thing is, this is not really a Monte-Colo approximation because I want a uniform bound. I take the supremum over all choices of networks. Right, so somehow if the network was fixed, I would get, right? If theta was fixed and I didn't take supermove here, the bound would be something like one over m to the one-half, right? So I can prove that. right so i can i can prove that we could prove that there is an event of high probability such that given my network the statistical risk approximates the the empirical risk approximates the statistical risk now the thing is i i need to i need a uniform bound i need to go over all networks so in some sense i need to intersect many events right for each network there is a different event and this is why we have here in the in the numerator a term that describes A term that describes the complexity of the hypothesis class. I mean, it describes the richness of the space of message passing networks. Okay, so what we are going to prove specifically for message passing networks is the following bound. So the bound looks a lot like the typical bound. The difference is that here in the denominator, we don't have to be a benefit. Denominator, we don't just have the number of graphs to the one-half, we also multiply by the average number of nodes in each graph to some power which is less than one half. So what is the intuition behind this bound? The intuition is that we are going to think about the empirical risk as a Monte Carlo approximation of the statistical risk, but the random variables are not only. But the random variables are not only the graphs that we sample. The random variables here are going to be all nodes of all graphs unite together, right? Because the nodes themselves are going to be also sampled from some random graph model. And that's why, in some sense, the number of here in the denominator, we're going to have m times the number of nodes. Now, the power is alpha, it's less than one half because nodes, of One half because nodes of graphs are not independent random variables, they are dependent on each other in view of the connectivity of the graph. And this is why the power here is not the perfect one, it's not one half. Okay, so to prove this, we need a good model for how graphs are generated. And I'm going to base the model on graphons. And before I continue, I want to convince you that it is a good idea. That it is a good idea to think about graphs as being sampled from graphons. I want to explain or convince you that this is general enough. Okay, so one standard approach in data analysis on graphs for analyzing graphs is computing statistics of graphs. Okay, so what we do is we take simple graphs that we call motifs, for example, triangles, and we count them. And we count them. If I want to compare these two graphs, I'm going to count the number of triangles I see in each of the graphs. So I don't really count, I somehow compute the density of triangles in the graphs. This is called the homomorphism density of the triangle. And if I have approximately the same homomorphism density, I say that with respect to triangles, these two graphs represent in some sense the same phenomenon. Okay, so I can do it for every type of simple graph. Of simple graph, right? Every motif, and if for every motif the homorphism density is the same for both graphs, approximately the same, I think about them as representing the same phenomenon. Somehow, this extracts the relevant information from the graphs. Okay, so suppose now that I have a sequence of graphs with increasing number of nodes. I have more and more nodes, and suppose And more nodes, and suppose that for every motif, the homomorphism density, the number of times I see the motif in the graphs, converges to something. So in this sense, I think about the sequence of graphs as representing the same type of phenomenon. Now, Graphon theory tells us that if all homorphism densities converge to something, then there is a unique limit object to the graph sequence. limit object to the graph sequence. And this limit object is called a graphone. So what is a graphon? You can think about a graphon as a continuous version of a graph. It's like a graph with a continuous index set. So the index set is a standard probability space, for example, the interval zero to one. And you can think about the graphon as a continuous version of the adjacency matrix. So you take pairs of nodes. So, you take pairs of nodes, right, and you return a weight how much these two nodes are connected. This is a graphon. Now, graphon theory, one of the basic results is that if you have a sequence of graphs with converging homorphism densities, then there is a unique graphon up to some symmetry for which the homophysian densities of all motifs converge. Of all motifs converge to the homomorphism densities of the Graphon. So there is also a way to count how many times I see triangles and other motifs inside the Graphon. So in some sense, a Graphon is a natural way to describe graphs that represent the same phenomenon. So we are going to think about Graphon as what describes a certain phenomenon. Now, Graphons can also be seen as a generative model for graphs. Model for graphs. So there is a way to generate graphs from graphons. You take random samples from this index set, independent and uniformly. You think about these points as nodes of the graph. And then you generate the graph. The way you decide how to connect the different nodes is by defining the ages matrix, either as a weighted graph, you simply take the graphon and sample it in all pairs. Sample it in all pairs of the nodes. This is going to give you a weighted graph. Or if you're interested in an unweighted graph and the graphon has values in zero to one, then you can sample edges as a Bernoulli random variable according to the value of the graph. Okay, so hopefully I convinced you that it makes sense to think about to model different graphs as coming from different graphons or different graphs that represent different things as coming from different graphons. Coming from different graphons. And this is going to be the basis of how we are going to model how data is generated in the data set. So we are going to assume that we have a finite collection of graphons. Okay, we have a finite collection of templates. Each of these templates is a metric space that has also a probability measure. Now, on each such space, there is also a signal that we are going to assume is Lipschitz continuous. To assume is Lipschitz continuous and a graphon. I call it here a kernel because I assume topological things on it. I assume that it is Lipschitz continuous. So typically in Graphon analysis, you just assume that the Graphon is measurable. So here I assume it is Lipschitz continuous. And this is why I work with more general metric spaces. Now, how is a graph generated from this data? So, first of all, I pick one of these templates. All I pick one of these templates, one of these graphons at random under in some probability. So, suppose I picked this metric space, then I picked the number of nodes of the graph at random from some probability measure, and then I pick the nodes of the graph uniformly and independently from this metric space, from the probability space of this metric space, and I connect them according to the graphon. According to the graphon. So, if I'm interested in weighted graph, I simply plug in all of the points that I sampled inside the graphon. This is going to give me the adjacency matrix. I can also work with Bernoulli edges, as I said before. And then the graph signal is sampled from the graphon signal, simply by evaluating the continuous signal at the points that I sample. So, this is how a graph is generated. Then I generate many of these, and this is my data set. Data set. Okay, yeah. Yes. Okay, so the question was about the signal. Okay, so when I do message passing network, when I define a message passing network, right, I need to. Right, I need to start with the input signal. I need the feature at each node. So, what I assume is that each of these metric spaces has a continuous signal, right? And the way to generate the graph signal is simply to evaluate it on the nodes, according to this picture. Okay, now before I'm going to show you the generalization theorem, I'm going to show you a convergence theorem. Convergence theorem, we are going to base the generalization theorem on the convergence theorem. So, the convergence theorem is going to say the following thing. So, suppose I have one of these templates, and I randomly generate graphs from this template with more and more nodes. So, the number of nodes goes to infinity. Now, I can apply the message passing network on each of the randomly generated graphs. What we are going to prove is that this is going to converge to something. The application of Going to converge to something. The application of the graphone on this sequence of graphs is going to converge to something. And this something is the application of the message passing network on the graphon. Okay, so first of all, I want to define what it means to apply a message passing network on a metric space or on a graphon. And this is actually quite natural. So suppose I have a point X in my metric space and I want to update it in the next layer. To update it in the next layer. So I want to send messages from all of the metric space. So I simply take the signal evaluated at all other points Y, take the signal at X, plug it in the message function. This gives me a message from every other point. And the way to aggregate it is, first of all, I need to weigh the messages according to the graphon, right? Because the graphon tells me how much y is. Me, how much y is connected to x. Okay, so I multiply this message by the connectivity, right, by the value of the Graphon, and I instead of doing average pooling, I do, right, instead of averaging, I define it by an integral, right, which is the continuous version of an average. And then I have the aggregated message, and I simply can plug it in the update function, right? So, yeah. Uh can I understand? Can you repeat the question? So the question if there is a relation between homophysiism density and the expressivity, I don't know, could be. So do you have do you ask because you have an answer? So, this is a question you've thought about. Okay, so maybe we can discuss it afterwards. Okay. Okay, so it's very natural to apply a message passing network on this random graph model. And then we can prove the following convergence theorem, which is a uniform convergence theorem. It says that given one of the It says that given one of these templates, given one of these graphons, if I fix the number of nodes of the graph and I sample a graph randomly from this model, then the supermoom over all message passing networks of the difference between the graphon message passing network and the graph message passing network is bounded by a constant that describes the complexity. That describes the complexity of the space of message passing networks divided by the number of nodes to some power. So, first of all, how do we define the complexity? The complexity is defined, so we assume that the message function, that the message function and update functions are Lipschitz continuous with some Lipschitz constant. And the complexity is going to increase exponentially in the depth of the network and polynomially in the Lipschitz bounds. The Lipschitz bounds of the message and update functions. Okay, so in some sense, this describes how rich the space of message passing networks is. So it's not directly the VC complexity, it's something different. But you can think about it, if you just want intuition, you can think about it as the number of parameters, some function of the number of parameters. But we don't directly limit the number of parameters. What we do limit is the What we do limit is the Lipschitz constant of the message and update functions, which in some sense also limits the number of parameters. Because, so if I told you, for example, that I want to represent a Lipschitz continuous function with Lipschitz constant one from the interval zero to one to the interval zero to one, you would and let's say I want to represent it up to accuracy of one over 100, you would probably choose 100 parameters. You would probably choose 100 parameters to represent such functions, not 1 million. Okay, so the fact that we limit the Lipschitz constant in some sense also limits the number of parameters, but not directly. Okay, so now in the denominator, we have n to 1 over d plus 1, where d is the dimension of the metric space. So the intuition here is that if I have more dimensions, Here is that if I have more dimensions, then I'm going to have more correlation between the different nodes, and that's why they are less independent. And we are going to have a power which is further from one half, the more dimensions I have. This is the intuition. Okay, so just a side note: this is a uniform convergence result, right? Basically, we can find an event of high probability such that for every message passing network, That for every message-passing network, this difference is bounded by something. Now, you can find other results in the literature which are pointwise, which are called either convergence or transferability. So the pointwise results, I mean, we can actually prove something better than this bound. If we are only interested in the point-wise result, we can get order of n to the negative one or order of log n to n to the negative one in case we don't assume. To negative one, in case we don't assume any topological assumptions on the Graphon, if it's just a measurable function. Okay, but this is just a side note because we need the uniform convergence if we want to use it to prove generalization. Okay, so what is the idea of proving generalization from this convergence result? The idea is that my data set cannot be too crazy. Data set cannot be too crazy. I know that each of my graphs come from one of these templates. So I can take my training set and I can take my test set and partition it to pairs of graphs that sample the same graphon. And because both of these graphs sample the same graphon, they are going to approximate each other. The message passing network applied on these graphs is going to approximately be the same due to the convergence theorem. And this is how to And this is how to transform the convergence theorem to a generalization theorem, right? So then we can prove that the expected value with respect to the choice of the training set, the choice of M graphs from this model, of the supermoom over the difference between the statistical and empirical risks is bounded by some term that let's ignore this. Some term that let's ignore this term for a second, plus again, the complexity, right? You can imagine it as the number of parameters, it's actually not the number of parameters, divided by the number of graphs times the number of nodes to the power one already plus one as before. So, what can we learn from this bound? So, one thing we can learn from it is that, so if we imagine this complexity as the number of parameters, even though it's Is the number of parameters, even though it's not really the number of parameters, it means that we are allowed to take a network with more parameters than the number of examples we have in the training set as long as the graphs are large, because this term here is going to save me. And this is also the reason why I can ignore the first term, because the first term is a universal constant divided by the number of graphs. And because I'm interested in Because I'm interested in high-complexity networks, so Cl is going to be large and M is going to be large to be large. So this term is going to be networkable with respect to the second term. Okay, so basically what we proved here is, if I'm not mistaken, that the first generalization bound for graph neural networks in which it is beneficial to have large graphs. The larger the graphs are, the better the generalization bound is. The generalization boundaries. Okay, so you can compare the results to other results in the literature. And actually, one of them is by Brangia is going to present later today. But it's a little bit like comparing apples and oranges because each paper has a different setting, right, and different assumptions. But in all other papers, basically, the bound looks the same, more or less. It has m to the negative one. Less, it has m to the negative one-half, the number of examples. But either the bound increases if I have larger graphs, or it doesn't depend on the size of the graph. Okay. Okay, so I have here some example where we took a graphon and we took many random message passing networks. We sampled graphs within. We sample the graphs with increasing number of nodes, and we see that the supermoom over the choice of the network of the difference between the statistical and empirical loss is going to so sorry, this is about the convergence theorem. So, the difference between the output of the graphone message passing network and the graph message passing network decays in the number of nodes, and you can see that the decay is actually. Is actually faster than what we have in our theoretical bound. So the decay is more close to the decay in the pointwise bound. So it would be interesting, this is interesting future work to try and come up with additional assumptions and try to improve the decay rate here. Okay, so basically this is all I wanted to tell you today. This is all I wanted to tell you today. Thank you for the invitation. So, thank you, Ron. And also, thank you for coming here. I think Ron has the record for the longest duration of travel here with 22 hours. Yeah, do I get some prize for that? Yeah, we should give you an award. Jet leg. Yeah, one gets the jet leg award. Jet like award. So any questions? In one of the inequalities you showed, you mentioned there was an exponential dependence on the depth of the network. So I've run into a similar thing for something I'm working on. And so I was wondering, do you think this is something that's fundamentally impossible to overcome? Impossible to overcome? Or do you think this, yeah, I guess just do you think this is something which could be improved? Or do you think this is just a fundamental dependency? Yeah, I think to overcome it, you would need a completely different technique because somehow what we do in this technique is we study what happens in each layer and then take the composition of all of the layers together, and you get some recurrent sequence. Get some recurrent sequence, and you have some term that increases exponentially in the depth. The good news is that typically message passing networks are not that deep, in many examples, at least. So it's not embedded in, I don't know, convolution networks on images. And each layer is not, it's not the layer of the, I mean, each layer is a layer of the message passing network, not the layer of the multi-layer perception that defines the message. Perception that defines the message function. So the message function can be deep. Okay, thanks. Yeah, that helps a lot and makes me feel a lot better. Yeah. A question which might, I hope, somehow well aligned with this. So one of the best Europe papers 21 was about showing that. About showing that current neural nets are severely underparameterized. So, there were some new bounds, and they said they showed that the current, even those very deep nets, they are still underparameterized given the number of data straight on. So, do you have a estimation or do you have the same feeling that current genes are also underparameterized, or are they well enough parameterized right now? I have no idea. For me, it's a difficult question. Me, it's a difficult question. Um, my feeling is that somehow, if you take more and more parameters, I mean, in the first steps of training, you're going to learn something meaningful with just a combination of not so many parameters. And then the rest of the parameters are only used for interpolation of the data, and this is not going to be generalized. But this is my intuition about what happens. I don't know. Okay. Okay. Yeah, thanks for the amazing work. So, I have a quick question about your assumption. I maybe lost some part of it. Like, if I understand correctly, you assume the graph on curl is IID for training and test. Is that true? The W, the curdle of the graph. Is that the same for training and the testing graphs? Yes, it's the same. graphs or yes it's the same distribution i mean it's not out of distribution so i see and then what's the the the what's the purpose of introducing those so-called templates is that uh the purpose is i mean we are trying to define some partially realistic uh generative model so for example if if your task is to do classification we think about each of these templates as being a different class or each collection of templates Different class, or each collection of templates is going to be a different class. I see. Um, because we are not able to prove that message passing networks and I mean, if you have if you have only one graphon, then any message passing network is going to do approximately the same thing for any graph that comes from this graphon. So how can you do classification like that, right? I mean, it's going to be this is not the language to use if you're trying to do classification on graphs that come from the same. On the graphs that come from the single phone. I see. Yeah, thanks a lot. And the last question is about your final numerical experiments where you show the convergence. Is that the actual values of the bond or is yeah, it's okay. It's not an approximation of this uniform band. What we did is we randomly generated many message passing networks. Many message passing networks. We computed the error between the continuous version and the discrete version. And then we took the max over all of them. So it's not really the theoretical boundary. In some sense, it approximates it or describes it in some sense. I see. Yeah, that relieves me because my calculation of the bound value is much larger than one. But yours seems to, I mean, the log 10 is likely, which I think is quite a small. I think it's quite small in some sense. Oh, so you have a faster decay? No, no, actually, I computed an exact generalization bound, which is much larger than one, makes it useless in practice somewhat. Yeah, but yeah. Yeah, thank you very much. Quick question. Yeah. Thanks for the great talk. I just wanted to. The great talk. I just wanted to ask whether you have kind of empirically investigated, like in real data, like how this assumption would hold up. For example, let's say you talked about graph regression, graph classification for molecules, for example. Is there evidence that maybe graphons would be good generative models for molecules? Okay, so I have two answers. The first one is that my intuition tells me that not, because molecules are not a phenomenon that you can. Molecules are not a phenomenon that you can increase. It's not like there is a limit, a continuous limit to a molecule. The molecule is what it is. So I wouldn't model molecules as graphons. But if you think about variational autoencoders, in some sense, what you learn there is a graphon. So if you learn an encoder, which basically gives you a node embedding, right? Each node is embedded to some point in a feature space. Is embedded to some point in a feature space. And then you learn a decoder that takes you okay. After you encode, you forget about the connectivity and you want to reconstruct the connectivity. So you learn a function that you plug in pairs of points and it tells you how much to connect them. Okay, so basically what you learn is a graphon, right? What is a graphon? A graphon is a function that you give it a pair of points and it tells you how much to connect them. Okay, so then, I mean, if you want to generate graphs, I mean, if you want to generate graphs in this approach, you randomly sample points in this feature space, plug it in the function that you trained that tells you how much you want to connect each pair. So basically, these methods learn response, right? So this is not a new, maybe it's a, I'm not sure if, I don't think they wrote this interpretation in the first paper at least, but basically it's not a new idea. Yeah, thanks. Just a very quick Yeah, thanks. Just a very quick question because you kind of answered it because you said, like, every graph on represents a different class, kind of, but would it also be possible to do a add an operation which inside of the graph on samples different classes? Or would that make sense? Not in this approach. Because this approach, in this approach, what you prove is that the message passing network does approximately the same thing for any two graphs that sample in high probability for any. Graphs that sample in high probability for any two graphs that sample the same, the same graphona. But so if you somehow want to have sub sub-classes or something, you would need a different analysis for that. Sure. Thanks. Thanks for the very nice talk. I have a question. Maybe it's not very related. So, say that you have a large graph and you want to sample it to, like, say, 10% of the nodes. Would this methodology tell you how? Would this methodology tell you how to do the sub-sampling? So, this under the assumption that the initial graph is randomly sampled, then you can always sub-sample it. It's just like assuming that it's initially sampled less points. So, I don't know if this methodology can tell you exact numbers of what would be. Yeah, I mean, I suppose you can take the approximation theorem, right? But say that you don't know what distribution generated the. Know what distribution generated the large graph. Yes. Okay. Even if you don't know, okay, so this depends. If you already trained your network and you have one fixed message passing network, then to get a convergence result, which is point-wise, you don't need any assumption on the Graphon. It is simply a measurable function. It doesn't have to be continuous. It doesn't have to be anything. So the classical Graphon theory is purely measure theoretic. So you don't really need to assume anything about it. So, you don't really need to assume anything about the Graphon, except for suppose that it is bounded by some values, right? It doesn't go to infinity at some points. I didn't write the constants here. Let me try and remember what they were. I don't remember at the point. We don't have the complexity. I think what we have is probably something like the infinity norm of the sigma and stuff like that. But I have to look it up. I don't remember at the moment. But I have to look it up. I don't remember at the moment. But it could be, it could, it could be, I don't remember. I mean, I need to check, but it could give you an explicit. Yeah, okay. Thanks for a very nice talk. I'm wondering, can you sort of from your analysis show that what's the benefit of using the graph just as versus if I'm just running a classifier? I'm just running a classifier on the node feature. Because it seems like your random graph model is also modeling signals on the graph. And I think there are some debates recently on some empirical finding of whether the graph structure is actually helping us for the downstream statistical inference. Okay. So the problem I'm dealing with here is graph classification or regression. It's not node classification or regression. Classification or regression. So, what you suggest would be to somehow have a separate network applied to each feature of each node, but then you need to aggregate the information. So, we probably average everything together. Right. And you say, I mean, looking at this analysis, what can you say if you forget about the graphone? Let me think for a second about this question. Think for a second about this question. Would that give you like it seems like then you lose some information? Yeah, and how, how bad or how, like, how would that be? Okay, so I mean, in some sense, what you're saying is that I simply have a collection of features. I don't know anything about how they are related to each other. So, what we do in this theory is we are able to prove that the message passing network is doing is going to do something different for different graphs. So, at least it's going to For different graphones. So, at least it's going to do the same thing for graphs sampled from the same graphon. We're actually not proving that it's going to do something different for different graphons. So, this analysis doesn't directly tell you. Let me think about this question for a second. Yeah, I mean. Yeah, I mean it's al it's almost like it's almost like what you're suggesting sounds like the graphon is simply a constant because you apply something in each of each of the nodes and then just sum everything together. In some sense, this is assuming that this is a like a random graph model where the model where the yeah yeah exactly exactly i see um i need to think about if if the analysis directly i mean this is something we thought about all of this what this analysis does it tells you that if you have two graphs that come from the same graphon the message passing network is going to do approximately the same on them it doesn't it doesn't tell you if you have two different graphons and two graphs coming from different graphons the message the message passing network is going to be The message passing network is going to do something different. So, in some sense, this is like only one side of the story. So, this is like an open question, I think. Thank you, Bill.