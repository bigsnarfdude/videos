In this session, we got three talks. The first talk is by René Cabrera from the University of Texas in Austin and is entitled An Optimal Transport Problem with Interaction. Thank you. Thank you very much. Thank you for the introduction. And thanks to the organizers for An Inwon and Mr. John. And also for the people coming in. And also for the people coming, thank you for coming. It's a great workshop. Okay, let me hear my voice. Okay, that's better. Okay, so let me start by saying that this is working collaboration with Nestor Guin. He's at Texas State and his student Jacob Hoborowski, a grad student of Nestor from Texas State. And I want to say that I'm surprised. And I want to say that I'm supported by the NSF grant number 1840314. Thanks, the NSF. And of course, thanks for this conference that was also supported. Okay, so let me start. Let me just jump in an overview here. So this is the famous Ben Amu Magnier. Did I say it right? Frenchman? Okay, good. So this, well, this is the The formulation that was proved by Benamu and Perenier in the 1990s. I think it was 98 or 99, maybe 2000. Okay, there we go. So yeah, so this is the what we're doing here is you have this minimization of the Munch-Kentorvich transfer problem, which coincides with the minimum of this dynamics here, which is the infinite mum over this pair. Over this pair of the rho and velocity. So is a transport distance between two measures and subject to the transport equation of the continuity equation. So you have this velocity. Oh, I have this, right? I totally forgot that I can. Okay, cool. So we have this velocity and the density which evolves through this velocity field. And so what I want to do today is sort of What I want to do today is sort of say this: what can we do if we add this a version of the Benamou Benier formulation or linear functional, but we add an interaction term? Okay. And so we have this Monskintorvich problem, and then you get this Benamu and Brené. Benamu and Brenier. So, what is the Moonshken Torvic problem that corresponds to this? Okay, so we're going to see this and see how we can sort of demonstrate this in a moment. Okay, so first I'll introduce what we call or what I call the path-dependent optimal transport. So, it depends on paths. And then I'll introduce the congestion, but I The congestion, but I actually want to call it the interaction trim. So here I should probably say interaction. And you'll see it in a moment, okay? And then, of course, the existence of minimizers and then characterizing the minimizers via duality. And then the last thing is the Benamo formulation of the optimal transfer problem plus the interaction curve. So, I want to avoid congestion because there's this paper by Jimenez, Carlier, and Santa Ambrogio, where they used this congestion theme. It's a bit different, their theory with ours. Okay, so this is the classical optimal transport. You've seen it in the talk of Suyong and Hanukkah a little bit. You talked about it. Oh, you folks warned me. Yes. Yes. So I'll do the Kenturovich formulation, which in a sense sort of contains the Munch problem with some conditions, of course. So we seek this plan. And what happens is that you are allowed to split the mass through this task for. So you sort of project on the source measure and then on the target measure. Okay. And you want to minimize for this case. Minimize for this case the quadratic cost because I think that's the one that is the famous one, and so this is the picture here of Licha's described. So this is the blue one. Looks more like green, but all right. This is green. Okay. Okay, Ketrovich, 101. Sorry. Okay, so now what I want to do is, I forgot where I got this picture from, but it's not from me. So you can think of It's not for me. So you can think of this as your source measure and your target measure. And you have this cost function that says how much does it cost to transfer the unit of mass from location X1 to location Y1. And sort of you don't, this is not dependent on path. And a priori, you could see that, well, it sort of is a path or a straight line. And so what we want to do is I want to take into account. Um, I want to take into account a path from x1 and y1. Okay, so I'm doing this transport that incurs a path. And so, in order for me to show you what I'm talking about, so now suppose that I have this transport problem and it depends on paths now. And so, remember that I have this sort of straight line, and you can see that that's the best one of them all. But why is the path important? The mob, but why is the path important? Well, I don't know, maybe you're transporting some stuff through a forest and you don't want to hit trees, maybe, or on a crowd of people, you know, when you're trying to find the restroom, sort of avoiding people. So, so paths can be somewhat important. Okay, so this is, let me put them, I was going to say back to back, but up to down. And an example of a cost in Example of a cost in, let me call this one, in this configuration, you can think of the cost between x and y as this distance, p bigger than or equal to one. And then while the cost here on the second one, you can think of the, I have this path and it's the total kinetic energy. Okay, and actually, why not? I can put a p here where p is greater than or equal to 1. Okay, so you can sort of see the difference of using this cost function between x and y with this distance, and then the kinetic energy for this p, which is the one half the requirement. There's a one-half there, but never mind, let's leave it like this. Okay, so now using the interaction term or congestion. So now the relative position of the masses, we care about the relative position of the masses, and we want to transport them so that they don't hit each other from this initial configuration right into this final configuration. Okay, so you have these boxes with expensive. So, you have these boxes with expensive items, and when you have a fleet of vehicles, I don't know, Amazon, you want to make sure that you don't hit each other, right? That'll incur a penalty. Okay. Okay, so let's set the stage. So, I will have some probability measures, and then we'll be working on the path space, where I have that the paths here are absolutely continuous. the the paths here are absolutely continuous absolutely continuous yes and so the the trouble here is what topology to put on these and it really there's there's a lot that don't work well but for all intents and purposes uh we'll just stick for the lipschitz norm i think that that works oh that's thank you x is rn no thank you yeah i just wrote a previous slide Okay, and then how do I evaluate these paths? Well, we do it so that I have a path base, path space. It's going to fit here. Okay, so suppose that this is my space of paths, and this point here corresponds to a path. So, what I do is I So, what I do is I evaluate through this evaluation map ET in Rn, also known as X previously, and I have the path here, and so I evaluate E of gamma. Sorry, not E of gamma, it doesn't make gamma of t. So I have a path and I just evaluate at this point. At this point. Okay, so we want to put a measure on the path space. And so, how do we do that? Well, we do it in such a way that, right, I want to push pi by these evaluation maps from my source to my target measure. And so, I don't know why I have this, ignore that. Know why I have this, ignore that. Okay, so the other picture is that, well, I have this sort of configuration where I have this, maybe I should put the probability on this, right? So I have this pi on the path space, and I sort of project to Rn like this. So with my source measure here, and then my target measure. And then for ET, I don't know if you. For ET, I don't know if you can see this. That's ET. I'm pushing pipe or by this under ET. And I think of it as McCann's famous interpolation that I have this configuration and it sort of malleably comes to, you know, from the source measure to the target measure. Okay. Okay, so we have that. And oh, I thought I heard it's my echo. Oh, I thought I heard it. My echo. So now let's introduce a cost function or an energy function, an energy function. And so these are the examples that I sort of gave before. So you have an idea of how these, the energy of the path and the one where I have the distance between the initial and the final point of the path. Okay, so these are some examples on how I will evaluate my energy functional through this path. Path. Okay, and an explicit path that we sort of looked at was the kinetic energy and the background potential here. But I think I will, this is so you see that there's an explicit one that we worked on, this theory here, but I don't think I'm going to use it because it gets a bit involved and I'll probably get confused while I write and therefore might confuse you. So we'll stick to. We'll stick to the kinetic energy, the total kinetic energy here. Okay, so now what do we want to do here? Right, so we want to sort of minimize the total energy on this against pi on this path, subject to these constraints. And so this here is capital pi of path. Basically, what it means is it'll be the sets of what you've seen before, right? So it's the Push forward this pi through E0, which is your source measure, and then your target measure. How many is okay? So under these linear constraints, and so actually, if you pay attention to this a little with this, if you let this be your total kinetic energy. Be your total kinetic energy. It's basically the same theory that stems from the classical Monschkentorovich. But there's nothing that we have done. It's just that we're looking at it from a different perspective through paths. And so what we want to do now is look at what we call the endpoint cost. The endpoint cost, sorry, the effective cost, which is basically what's the best under this path, gamma, going from the initial point zero to the final point, gamma of one. What is the best one among these, right? So for example, if you have some path, right, something like this. So the best one, well, you could see, right, that I don't have to go around, I could get something. Have to go around, I could get something that is like this, okay, from the initial gamma zero to gamma one. We could visually see what this is sort of representing. And the corresponding endpoint cost is basically what I call the C sub E between these points X and Y. And I'm taking the infinite among these paths that go from the initial to the final point. Initial to the final point, along this path. Okay, and such paths will be called C minimal, for want of a better word. Okay, so I think the picture sort of illustrates what we're trying to do here. I hope. Certainly for me, it does, but okay, what's next? Oh, yeah, so this is this result. And so if this pi here is optimal, and I'll sort of say that if pi here solves the optimal transfer problem. The optimal transfer problem that includes the paths, then what do we have? Well, first, that pi will be supported instead of C minimal paths, the paths that we saw before. And the other results, well, what it says basically is that you're lifting to the path space. That's where you have your optimal measure. And then what you're doing is that you're pushing forward to a solution of the classical Mohnschen-Sorbitz problem. Problem. And this is the coupling that sort of achieves that, which is a probability measure in Rn cross Rn, which solves the controversy problem for these source and target measure with this effective cost. Okay, so you're lifting it, there's an opt over here, you push forward or projecting, I should say, you project to a solution of torture problem. Okay, so now I think this is interesting, but I think in general, it's not so interesting, right? Because, oh, what It's not so interesting, right? Because, oh, what did you do, Renee? Sure, there's some paths, and you still have the solution to the classical contours problem. So, what else can you tell me? All right, fine, I'll tell you something. Let's add interaction and see how it changes the dynamics of the problem. Okay, cool. Okay, so that's what I explained before, right? Where we're going to add an interaction so that the The we're now interested in the position of the relative masses and see how they're transported. Just how I explained before. So, this here is where we add some interaction. And so, we're going to consider a kernel, call it even positive definite. You'll see in a moment why. It has to do with convexity. So, this sort of gives you, it induces a natural interaction kernel. And the ones that we're going to be looking are the That we're going to be looking are the Gaussian, okay. And I'll explain in a moment why. Because, well, suppose that you're responsible for transporting some items and they're sort of delicate and you don't want to break them, right? You want to make sure that they don't crash. And so the Gaussian sort of tells you that, well, if they get close together, you pay a little price. But furthermore, you don't get convexity. You'll see with a picture. The other one. You'll see with a picture. The other one is a coulomb potential. I think the coolant potential, you can see that if the interacting paths get closer, then, oh, you pay a high price. And of course, we don't get convexity. Okay, and so this here gives rise to a nice interaction kernel, which is the total potential on this k. And this k is. And this K is interacting, it affects how the paths interact against each other. Okay. And so, what we want, all right, here's a picture. So you see how you pay a little price here, and then furthermore, you don't get convexity there, so we avoid that. And then for the Coulomb potential, oh, that's even more dangerous in the singularity, right? We don't want that. And so I can integrate this. I can integrate this against the pi, say, gamma one. And so this gives me the total interaction between paths gamma and a distribution of paths that's given by pi. Does that make sense? Are you thinking about all these paths then? And okay, well, and so here, let me say something about this kernel K. So, what I want is that this This matrix, which is the collection of finite points, call it xi and xj, the requirements here, right, is that this matrix is positive definite. Can you read that? Okay, cool. Thank you. Yeah, because I sure can. So it's a positive definite. So it's a positive definite. We're good, right? Why are we good? Because we have a positive definite? Well, we get the convexity, but furthermore, the Gaussian is and the Coulomb are positive definite, right? So it's even better with a Gaussian when you integrate against this pi, right? Because you get enough compactness or the Gaussian is good, right? We can all agree. The gaussian is good, right? We can all agree, okay? So, yeah, that's right. That's why maybe we have something to do with this. I can't remember now, but I think you avoid that. Yeah, for N3. Yeah, you get away with that if you invite. With that, if you invoke, sorry, folks, Bachner's theorem, where the four-year of the Gaussian gives you some positive type that grows polynomially. It's very technical, and I don't even understand it, but I know that if I say some technical, it does work, and we can talk about it if you want. Okay, so now the natural question is: well, we have this, right? This matrix K or this kernel K, which This matrix K or this kernel K, which is positive definite in the Gaussian case at least, then what we want to do is: okay, now we want to minimize not just the total kinetic energy that comes from this initial integral, but we also want to include the interaction term. And you are integrating this, excuse me, you're minimizing this linear functional subject to this constraint. Constraint. So the constraints sort of don't change necessarily, right? Because you still have that. This is exactly what we had here, right? Where we have the push forward, this pi, which is a source, and the target measure. Okay. Okay, and so, okay, so actually, for the work that I did, this is based on a dissertation in UMass. U mass ignore this, the potential. We're gonna stick to the kinetic energy for what comes so that we can have a very nice interpretation of the Benam-Mu and Birunet formulation. The cost is just this. Yeah, yeah, that's right. That's why I said that there's no. That's right. That's why I said that there's no, it's just incurring paths. The interesting part here is that we're adding an interaction. No. Yeah, but although you can get it from this background potential. Yes. But yeah, that's right. It's already included in the Yes, that's right, yeah. And so that's right. So I'm sorry. So I'm calling it the objective here, objective functional where I'm minimizing subject disconstraint, I'm calling it the OT plus interaction problem because I don't know the name. We haven't figured out a good name that sort of describes it. Okay, cool. So we showed that, or I showed. We showed that, or I showed that it has this at least one minimizer. And the proof is, I'm not going to go over it right now because I will probably confuse you, but it's the lower saving continuity and getting enough compactness, the traditional way of using proving minimizers. Okay, so the cool thing that I want to talk about is the characterization of minimizer. And this is my favorite part because it was so natural. Favorite part because it was so natural. And you've seen something like this in De Jung's talk where we talk about the duality problem. Okay, so what do we have here? So if pi zero is a minimizer for the problem, the LT plus interaction, then this is equivalent as saying that you get two potentials, right? So let's call them. So let's call them conturban potentials if you want, but so you can think of something that you've seen before. And it says that for all path gamma, when you evaluate phi and psi at the respective initial points and final points, it's not much bigger than the total energy through this kinetic plus this interaction term. And the new part here is that on the right-hand side, Is that on the right-hand side, you get your pi zero, right? So you can get this, and you don't need to care about what pi zero is, right? Because you get this for free, so to speak. And the other part, you get equality provided that the path here, the gamma, is contained in the support of pi zero. Okay? Yes? Yes. Yeah, that's with the property, I can't connect any two points, yeah. And uh, so yeah, so one thing that I want to say is that these potentials, they remain in Rn because I didn't change the variable of the potentials, right? And in the I only changed the objective. So that's quite nice. So here's a sketch of the approval. So, let me see something else before I give you the punchline. So, this here, what this result is basically telling you: well, first of all, it's a consequence of duality. And then, second of all, perhaps probably equally important, says that this result allows you to relate whatever optimal measure, right, to a solution to the transport problem. To the transport problem, right, with our interaction for an effective cost, and you'll see that in a moment what that means, I hope. Okay, so here's the proof. I think the proof is kind of cute, right? So you get this Lagrangian and you end up getting the phi, psi, and this lambda here. So you write it down, you write this functional, and this here, these are the marginal. These are the marginal constraints, right? And then this one, this is the non-negativity constraints. So the lambda here, I'm just putting it so that I have the, it comes with the non-negative. The non-negative non-negativity constraint because I have a measure, right? So, yeah, non-negativity constraint because I have a measure, and it's really natural how it comes. So, we're going to get this condition. And so, let me just make sure that I put here that it's really positive. Okay, so what I do now, it's pretty nice, right? And yes, question? Yes, and so the fee and the sign. The fee and the sign. Yes, exactly right. And you can see that these here, I keep referring to the screen old habits here. And you can see that this here sort of gives you the E0, right, push forward through this, or a pi push forward through this E0. Okay, so now you rearrange the integral here like this. And what you do now is we're going to get a specific curve and Specific curve and note that these are linear, yes. And so this is quadratic. This is the new thing that we saw here. And so we want to take a derivative through this special curve with respect to S. And what we end up getting, so there's probably a two here. Right. And so now we have this tangent vector. Right. And Right, and because we had already that pi zero is the uh the minimizer or the critical point, then what this means is that there is there must be these psi, phi, and lambda, right, maybe, such that this linear functional or the above is equal to zero, right? And so, what I'm basically saying is for those of you that know a little bit of optimization, I'm basically doing the, what's the name, KKT, right? What's the name? KKT, right? The Kadushan Tucker conditions, slackness, if that means anything to you. And furthermore, what we get is that this lambda here is zero in the support of pi. But the cool part here is that we know that this is positive, right? And so this tells you exactly the inequality that we wanted, right? That the sum of this is less than, right? And then there's more to the proof, but I think. There's more to the proof, but I think I wanted to do what I call a nice proof so you can see that okay, this makes sense, right? I don't know. Okay, so this is the effective cause that I was talking about. Yeah? Yes, Matt? So, what do you mean all the time? So, what do you mean, all these things? Yeah, yeah, this is. Yeah, that's right. But now, so it's a generic, I'm just integrating over this pi through these paths. Right? Oh, the space of paths. Such that, yeah, yeah, that's right. Yeah. So, so, yeah, omega is the space of paths such that path is absolutely continuous. Such that the path is absolutely continuous. So, this is greater than zero. Okay. Okay, so this is the effective cause that I talked about before, but now with the interaction, and the cool part is that now you see that we have this pi zero, which is the newest thing of this theory here. And the corresponding endpoint cost function is this, what we call C sub E and pi zero, which is. Sub e and pi zero, which is the infimum of these costs to these gamma. So, what we can do here is, because we have equality, yeah, so I can talk about like the infimom along these paths here, where sigma is just a generic path variable. Okay. Okay, good. So there we have this effective cost. Check and then the corresponding endpoint cost function. Exactly this if you mum the cost of the effective cost along the initial and final evaluation of the final and initial. Okay, so again, we have the same results where you lift to the path base and you project a solution of the Katrovich problem when Problem when I have this endpoint cost. And number one is the same, right? That the pi supported on these C sub pi minimal paths. So which pi zero? Yeah, so pi zero is the optimal map that minimizes your total. Your your total right, so what was it? Uh sigma and pi gamma. And of course, subject to the constraints that pi is this measure that's greater than or equal to zero. And I also have the conditions where I push pi forward through E0, which I get my source measure, and then I measure and then I push pi through my target measure sorry through E1 and I get my target measure that's right that's right I'm putting a measure on the on these path space exactly right so this if if pi here solve the OT plus interaction problem then I get this condition where the the pi supported on the C sub pi zero and all that and then the same thing happens right I lift to the measure of pi The same thing happens, right? I lift to the measure of paths where I have my minimal here, pi, and then I project to the solution of the Kentucky launch problem through this cost here. Okay, cool. And so now let's return to the, I think, one of the first slides. Oh, wow, great in time. How's everyone doing? Yeah, any questions? Yes, Matt, please. Yes? That's right. So, good question. So, the path space, this capital gamma, it's an infinite product space, and I'm looking at the marginals. No, because as long as we have, I think, convexity. Yeah, that's right. Uh, yeah, that's right, because this minimization problem is an infinite-dimensional problem, right? And the constraints are linear and convex, so I think that's what we're here. We do this theory, and we just, it's important that we have convexity, strict convexity. But I can yeah, just think of it that you're just integrating against this pi through genetic. Against this pi through generic path. Yeah. The path are segments? Well, the best ones are lines. Yes. But and I also needed to say that we're in the space of simply connected so that I don't break the path. I'm also avoiding paths that sort of are super long. Sort of are super long, and I'm avoiding paths that oscillate a lot because, yeah, there's some restrictions there because otherwise, I mean, let's say I want to get from point A to point B and I have this, what is it, one over X sine one over X. It's crazy, right? So I'm avoiding all that, and I'm also avoiding the passive superline. Yes, actually. Yes. Yes, and also one of the reasons why we chose a Lipcho's norm is because I want to make sense at infinity, right? On these paths, and I want to make sure that the derivative of this path makes sense. Yeah, there's more technicalities. Excuse me? What about boundary variation? Yeah, I guess it can be. Yeah, I guess it can be. So that we have a good, like a well-defined. Yeah, I wanted to get away with just saying that the paths are absolutely continuous. But yeah, there's probably more conditions that we need to invoke here. So yeah, thank you. More questions? We can great in time. And then after a while, I'm going to be late. So let me just continue, okay? Okay, so this is the result that Nestor and Jacob. And Jacob worked on, which is pretty cool, right? So, the minimal value of the OT interaction problem gives you the minimum value of your benamu-bin functional here, right? The original that we talked about, this dynamics, and then plus this. And subject to these constraints, right, of this transfer equation over the pairs of rho and v. And the cool part is that the constraints are the same, right? The constraints are the same, right? From the Benamoon-Brenier, and we're adding this interaction term. And the proof is really cute because it relies on, let me use the word because I'm doing great in time, the change of variables formula, right? Or something like this. So it also comes from the measure-preserving map, but it's, I call it a change of variables formula. So what is it? So we're going to get some, let's call it. So let's call it, what is it? Yes, please. Yeah, I'll close it. Oh, I said please. I meant to say yes, of course. So what is it? T of x and then of x coincides with of the units. Thank you. And this, right? So this T is just a general T. I'm just describing how the I'm just describing how the proof will come into play. Okay, okay. So, here's the idea, right? So, of course, just like they did, you do a change of variables and where this energy E is, well, this energy field is rho V. And so you get a convex functional in this pair of rho and the energy. Okay. And so you just plug it in and we get. Plug it in, and we get this convex function. So, which is good, right? It allows us to work on the things that we usually do. If you consider the modifiers, I think. And so, you define these through this modifier, you define these convolutions, okay, with this velocity field, and you end up with smooth approximations that satisfy the transport equation. So, this is things that I hope we've seen. That I hope we've seen. Okay, so I start with the generics, and then so you take this smooth vector field V. Okay, and so it flows through this map, which I call capital gamma on X and T. And so it solves this ODE or this equation, and which we prescribe an initial condition, right, for all X. And so you can now equivalently define this map. Equivalently define this map. And this map here takes, you can think of it as taking x to the path, which I will call capital gamma t of x. Okay? Which solves this equation here. Okay, so using this map here, so we can produce some measures. And so this is what I was talking about. And so, this is what I was talking about. And so, to prove this, we're going to basically just apply these definitions. Okay, of course, I should also note that rho of zero is the same as your source measure. Okay, and so we start by plugging stuff in. So, this is your total kinetic energy, right? And then I apply, you know, what this is, that's just the derivative. Derivative of this gamma map. And then I, well, I know that this solves this equation, right? Sorry, I'm sorry that I'm scrolling up and down. I usually get, what is that word? Dizzy. So if you're getting dizzy, I'm sorry. Yeah. Okay. So you see it, right? How I'm going to substitute this here. And then, so what I can. Here and then, so what I can what I should do here is: well, I applied here. Um, what was it? Oh, it's right here. I applied this. Okay, and then again, what I what I do here is say, Well, let's do a change of variable. Y is this t of x, and then of course, I move the. And then, of course, I move the integration with respect to t. And so, in this step, I'm using exactly this change of variable formula, okay, as well as using. Okay, and cool, that's we get it. We get the first term of the benamoe embrena. No, it should not be over omega. No, it should not be over omega. Thank you. It should be over a yes, thank you. Okay, and so for the other one, right, on the other hand, let's not forget about this interaction functional. And so you do the same thing, right? You apply this. What we're using here is just an application of this. Okay, so well, we know. Okay, so, well, we know what this is, right? That's your total potential energy that interacts among these paths, that affect these paths. And you just apply exactly this. Yes, so from here. And then here, you apply exactly what. Rho of t, right? So, which is basically what I wrote on the green board here, just change of variables. Okay, and so what we have then is for this measure, which is given by this map, push forward, mu zero through this gamma, and with the other conditions here that I use a change of variables, you end up getting a very nice Benamu and Benaneaf. Uh, Benamu and Beninea formula of the that comes from the control of itch Munch problem, which is pretty cool, right? Because you end up with the original one and then plus this interaction effect. And then there's more stuff, right? Of course, that apologies get because I was flirting with it. Oh, you still have time, but it's maybe lunch in a bit. And so we went enough time. So I also want to talk about the other direction in this case where. In this case, where so the classical with the Benamo-Bernier, right? So you have this infimom of this dynamics. And if you assume everything is smooth and you take the gradients, you sort of get this nice Hamilton-Jacobi equation. Now, for this case, the minimizer of the pair rho and V, it yields a solution, a variation of a solution to this Hamilton-Jacobi equation. And the only difference, I'm sure that you speak. And the only difference I'm sure that you spotted was this convolution. For the folks that I've seen this in Meanfield Games paper by think Filippo Sacambrogio, where you take the derivative with respect to this V, right? And then you put the, anyways, the fential or whatever, you end up with this nice configuration. I thought that was a cool thing that we got from this. thing that we got from this. We're still exploring this and see what this gives us. And so this brings us to, well, another thing that we began to explore is, suppose that we have multiple phase, the multiple phase problems here. So even the two-phase problem is quite difficult, right? But this is the next thing that Nestor and Jacob and I were sort of working and saying that, well, you have this, the, the, the, uh, The mob denier, right? And then the addition of this interaction term with these makes it somewhat complicated, right? It's pretty hard. And I don't even, I think that we exclude the possibility that X and Y, they come together. This is not modeling if the particles come together, right? Because we're going to lose convexity. So I think this is an interesting problem to tackle finding the. Finding the minimizing of this with the constraints that we've seen before via the transport equation. So, yeah, so hopefully, this turns out something nice. Even I think existence of minimizers was okay, but yeah, we're still working on that. So, what do you mean, the initial? Oh, yeah, so so. So let's see if I can look great because I'm going to have I should have something like this and then maybe I should put an I and an I with your source and yeah that's right yes I think this makes the problem a little it's fine Makes the problem a little, it's fighting back, which I think is interesting when you have a problem that sort of gives you a bit of a problem. That's why we, well, I mean, right? I mean, you don't want to do, of course, a low-hanging fruit is also good, right? Okay, and let me see. Oh, Rosang, thank you. Please, question, comment, suggestions. Hi, thanks for the talk. In just the ultimate slides of the previous slide, your interaction was with, like, I'm thinking of row one as the density of species one and row two is the density of species two. So that term I'm thinking is how the difference. So, that term of thinking is how the different species interact with each other. But I guess, could you also include species one interacting with itself and species two interacting with itself? Of course, this cross-determines the hardest one, but yeah, if I think, let's see, if species one interacts with itself, then you no longer have, oh, didn't. Yeah, so. Yeah, so I'll just. Yeah, I don't know. I think I want to say that you can go back to the original one, but you still have the transport equation that incorporates both the species. So it might be the case that, yeah, because you're talking about the so you have this one, right? Something like this. Something like this, right? Yeah, I don't know, right? I think you can get something that, yeah, because we don't care about the, for the, at least for the potential here. But for the energy, you still have, unless you consider the energies, the kinetic energies that interact with the same species or different species. I guess it depends. Yeah, that's a good question. We can chat about it. So, I mean, I guess you just started looking at this two-page. I guess you just started looking at this two-phase thing, but can you do a version where for instead of having two pairs of initial and final densities, you have one pair, but then like you allow so because you have two phases, right? So you also allow the two phases to vary initial. No, no, I think that's way too complicated, yeah. I think that's way too complicated, yeah. But maybe I'm looking at it, we're looking at it from, or I'm looking at it from a different perspective. Something that's natural from the model is that this vector field P is going to be divergence. But is this something that falls from your mathematical model? Is it something that is like that describes or? Like that describes or I mean you you you are taking this this problem minimize this function and on this uh continuity equation constraint but uh for me it's not clear that this v should be divergent you know that there is no for example a spontaneous creation of mass and then spontaneous uh annulation you know is this does this make sense or i and i can do this probably in a probability measure Probably in a probability measure. I can create something here and I can if the B has a positive non-trivial divergence, no? Yeah, I see. I don't know. I think about mass preservation doesn't exclude that you can create something here and exploit here. Right here, no, it's the total mass is preserved, right? Are there two continued equations? Yeah, yeah, that's right. I just wrote them as I'm talking about the first problem, no? Oh, sorry. I thought that the model, right, that was my question as well. The model is basically right that you have to put this on for, you know. Coefficient, or you know, say co-century, you know, you want to avoid right. So, do you actually see from your results that velocity vector you create? I mean, I wanted to ask you to put some regular strengths on the head space. Ah, yeah. Is it really yeah, the regularity, yeah, I'm having dealt with. So it but but you put in the path Yes. Yes. But it's, yeah, it's under. So you're saying about the penalty, and that comes from what do you mean? Yeah, we have, yeah, the paths, there's some technical conditions that we put on the path. Yeah, because, like, for example, if you're thinking about the kinetic energy, I want to make sure that I want to make sure that it's an L2, maybe, and otherwise an infinity. I don't know if I'm answering your question. I don't even know if I understood your question. When you take K0, you take the equivalent over all the flows, you get the vast system. Yeah, if k0, you get you go back to the kind of a metric. What is k or take you take the integral that the namu I'm bringing into square flow and you but you add this term, right? Yes, this this interaction term. What's a flow? Rho T is a flow. Oh, yes, yes, that's right, yeah. Oh, yes, yes, that's right. Yeah. So you take the infimum overall, this flow, would you get some? In k is zero, you get the vast nice distance. Yes. Also, just this term, the infimum of this term? Or collectively with the collectively with the other. Is this the metric? Oh, uh um Yeah, now I understand. So you want to relate it to the like the D2, right, or the Wasserstein metric. I don't know. I think if I look at it more, probably get, yeah, because I looked at them as functionals. You need some properties in K to make it the metric or something, I guess. Yeah, maybe. I mean, because K is for the For the specific ones was the Gaussian. And exactly, yeah. And so it behaves somewhat nicer. Yeah, exactly because the difference would be both, obviously. Yeah, although, let's see, with the case of Gasha, but you get something. Yeah, I think you get you might get some triangle version. Yeah, yeah, with the caption. I remember I computed it, but it was a long time ago. Yeah. Yeah, but yeah, I think Jeremy, yeah. Yeah, I'll just make a very quick comment, I guess. I mean, I think, yeah, this type of system is very close to what study did. Is very close to what study did they build games with interviews, you know, a lot of different types of interactions, but it has different boundary conditions. I mean, maybe Albert could comment more about that. But I think there's a lot of regularity results over there. That's what you might be able to. Yeah, that's I just started thinking about it through the mean field games and it's quite nice. But yeah, that type of system is quite common and they study lots of interaction costs. I mean, repulsive is generally better. Yeah. Cool. So you alluded to this a few times, but if you change the C gamma, so it's not, you no longer take the square of gamma dot. If you change the square of gamma dots to like some Lagrangian depending on gamma and gamma dot, still integrated over time, you can cover that. I think so. I think that, yeah. Yeah. And my impression is that you can. I'll just have to fill in the details. Thank you very much. As we were saying, this kind of problems totally fall into the category of mid-field games. There is a huge literature on it, and in particular, the Hamid Panchakovi formulation. Jacobi formulation is quite well understood. So, if you have this kind of property on K, that means that that energy is convex, which means that you can write down the dual formulation and you can use potential look-peller theorem to have duality and you have like a pretty clean understanding of this kind of thing. Yeah, actually, the paper mefuld games more like the planning problem from this has. And this has been for the paper from I think Santa Ambrosio and Felipe Santa Brocho and someone else, or I use the calculation. I think that that paper is by myself and Santa. Oh, sorry. So I actually used, that's how I got the Helsinki and Jacobi with the convolution. Yeah, yeah, exactly. I use those. Thank you. So please, please stop it. So, please, please study those kinds of things because you might find lots of answers. Cool. And the coming three questions: if you have the characterization with the Hamiltonian equation, you can write down Euler-Lagrange equations for the curves and you can study regularly. I am not sure if that has been said. If you have the Hamilton-Jacobi formulation, those curves will be also optimized in the expression. In the expression, you need a control program. Cool, thank you. All right. Any more questions? Can you say anything basic about like the way entropy behaves on? No, I don't. I haven't seen, or I haven't at least myself explored this. Yeah, but entropy is quite important. Yeah, but entropy is quite important. I use it somewhere. I see it somewhere else, but in particular, this, yeah, I haven't sat down and investigated, but it would be nice to see what we can say about it.