Perfect. Yeah. Okay. So, first of all, thank you so much for having organized this conference and for like putting me in this program. So today I'm talking about multi-study approaches and multi-study techniques to handle multiple study together. And first of all, I want to give you some motivation behind this proposed approach. Behind this proposed approach and what it has been done until now. So, the data came from, as you can see in this picture, different studies or different population where you can measure the same violence. You cannot see the picture. Okay. Let me retry. Right now we try. Yes, now we can try. Now you can see it. Yes, now we can see it. Now you can see it. So I will avoid the full screen. Thank you, Ale. Okay, this is the picture then, where you have different population in different color, and you can measure exactly the same variables. And like there are a lot of different approach. Here I put the multi-study factor analysis where you are able to handle multiple populations together. There are several examples where you can find this. Find this situation where you have a different population, Boston, Italy, Switzerland, and where you can measure the same food variables. Or in other settings, you can find the same gene expression data measured on breast cancer patients coming from different hospitals. So today, I'm focused. So, today I'm focusing on a different approach in the sense that the data are not normally distributed. And of course, this is more challenging because now what we have is mutational counts. And you have different populations, of course, where all of these populations have different cancer types. And you have And you have seen a similar approach from the talk of Jeff Miller, where he explained what mutational signature is. And I will go really in deep, even in a multi-study setting. So where the mutation and where this method came from. So mutation is a SNV, and the SNV is a change in the DNA sequence. So you have this DNA and like. This DNA, and like instead of a G, you will have a mutation in a T. And this will create mutation. And why these mutations are so important? Because, like, of course, accumulation of these mutations can cause cancer. And recently, they discovered that there are some mutational processes. And for this, is coming the name of mutational signature. So there are. Mutational signature. So there are some mutational processes where they see that there are various types of substitution at a particular rate, and these substitutions are caused by external phenomena, like, for example, like environmental health or smoking cigarette or other different things that could cause this sort of mutation. And today I'm talking. And today I'm talking about this single base substitution. So, what you can see here is that you can have a substitution and annotation only with the C and only with the T. And so, this will lead us to six different substitution types. And then, the initial even the four nucleotides before the mutation site and the four nucleotides after the mutation site. Mutation side, and these will lead to a total of 96 variables, or for doctor and genetics, 96 mutation categories. Remember that so we have 96 variables in all of these different population. These 96 are mutation, and what you have is mutational counts. So these are in like count variable and. And I will start from like the beginning and how generally people will approach with this mutational signature and mutational counts. Generally, they use these non-negative metrics factorization that is very similar to a factor analysis approach, but of course, we are not with continuous data, but we are with count data. Here, this is Here, this is the matrix of my k that is again our 96 variable times n, that is the subject, could be the composer in p, that is the signature matrix. So it's a k, the number of variables types the number of signatures or factors, and the exposure. The exposure is an expose. The exposure is an exposure matrix of R, that are again our mutation and signature times N, that are the number of subjects. In a certain way, it's like factor scoring factor analysis. The important thing of this exposure is that these metrics will give us a weight for each subject associated to the exposure for these particular mutational signatory. Signature. So, this is like the frequencies approach. But, of course, there are, and as you can see, that's a very recent paper in even Abaysian settings. And one famous one is Rosales in 2017, so a very recent paper. And what they use is a gamma prior on both the exposure and the signature metrics. And the signature metrics. Like this, Zido and Miller prior, they developed this prior that actually put an automatic selection of mutational signature. And this is a very recent paper. There are, of course, some limitations. First of all, we are decomposing a single data matrix at a time. What does it mean? It means that if you have multiple populations, you cannot handle these multiple populations. you cannot handle these multiple populations together but we are handling one population at a time and we cannot estimating we cannot estimate the novel signature so you cannot discover new mutations process that are in these signature metrics and the third point that is very important is they do not consider cobalts. Not consider covariates and covariates could be really important because they have an impact on the exposure metrics. Like imagine if you live in a city that is full of pollutions. Of course, it's different than living like in a city that is not so heavy polluted, for example. And so, like, we are going in a multi-study settings and Study settings. And there is only one paper from what I know, recent paper that considers multi-study for non-negative metric factorization. And this is Grabsky et al. in 2023. So right now we have our metrics of K that is mutational counts times the NS subject that of course vary from study to study. And this is the component. And this is decomposed with a P matrix of mutational signature. Again, this AS that is a diagonal matrix where the diagonal element will tell us if there is the signature in that study or not. We have the exposure metrics again that, of course, vary from study to study. Course, vary from study to study because we have different subjects. And finally, they incorporate even these Ws that is a normalizing constant. And they put prior in all of these elements. In particular, like what they have, they put a gamma prior on the mutational signature matrix with alpha and beta that are exponential in gamma as hyper priors. As huffer priors. This is the diagonal matrix with zero and one. So they put a beta Bernoulli prior. And on the exposure, they actually mimic the same strategy that they adopt for the mutational signature matrix P. So they put a gamma where there are hyper priors exponential in gamma for alpha. Exponential and gamma for alpha and beta, and they do not put any priors for this normalizing constant. And there are three main issues for these settings. One is that the mixing for these AES metrics is very poor. There are a lot of identifiability issues between P and ES. And finally, they do not put any prior on WS. Put any prior on WS and these remain constant. So, like we try to solve these three big issues and by reformatting these mutational counts and redo some sort of strategy, even incorporating some covariates. So, our mutational counts can be decomposed with again our mutation. With again our mutational matrix, mutational signature, K times R. We have the exposure, and we have, so our model is more simple than the one from EDSI, Gramsci, and this is like a diagonal weight matrix, NS times NS. And our prior differs a lot from the one that I present. A lot from the one that I presented until now. Because, first of all, we put a Dirichlet prior on the matrix of mutational signature with this alpha as parameters. The exposure will change a lot because, like, what we put is a mixture of a Diverchlet, where this eighth or this parameter is coming from a probit model. Coming from a probit model, where the probit is modeling with our covariate. What does it mean? It means that we are actually putting a prior that is driving by the beta coefficient. And this makes particular sense in the exposure because the exposure is a weight that each subject will have on being. Subject will have on being exposed to a particular mutation. And of course, this could vary from the different observed variables that you have, like even the gender could have a different mutational rate as an exposure, as a probability, and as a weight. And finally, we even put Finally, we even put a gamma exponential gamma prior for these weights. So, like we solve the three issues that I presented before, but first of all, and I want to do step by step, we solve the identifiability issue between P and ES, because right now we have the Dirichlet priors for both the mutational signature and the exposure. We use the covariance. We use the covariates in the probit as a prior, even to see if a signature R can have a high or low concentration for specific subject. We are doing even variable selection, and that's an important step in mutational signature. We put a prior for the W, and this will add more flexibility. Flexibility. And how can we interpret now WSJ? So, this is the total number of mutations in the genome of subject J for the study S. And we are using an efficient and very fast variational base algorithm. So, I want to show you like a couple of simulations. Of course, like we are doing more. We are doing more, like a couple more in our paper. So, the first scenario is more generic. So, we put the number of signatures that are equal to 50, but imagine that this k could be even good or different variables. The important thing is that the variables are count. These are the number of singles. These are the number of signatures, or the number of factor, Latin factors. And we have five different studies. And we have our sample size that is between 3000 and 4,000. This scenario is made to, like we make this scenario in order to closely mimic real data application like nutritional epidemiology data. And scenario two instead. And the scenario two instead closely mimics a mutational signature setting where you have 96 variables, so mutational counts, six different mutational signatures, four different study or cancer types, as I told you from the beginning, and the sample size will go from 38 to 98. And the specific scenario two is coming from this data. Is coming from this data set here, PCEWG, where you have four different cancer type. And in all of these cancer, you can see different mutation. And some of them, like SBS2, are common in all the cancer. Some are particular to specific cancer type. And some instead are related just to a couple of Are related just to a couple of cancer, but not all. So, we actually tried to closely mimic this data set. And what we have done, we have perform a non-negative metric factorization for each of these study. And we try actually to find like some signature, and the ones that are in common, we try to put in common, and the ones that are instead like. And the ones that are instead like specific, we put as a specific signature. So, this is a way to really closely mimic these real data scenario. So, and first of all, I want to show you the result for scenario one. Like here, what you have is the simple Dirichlet prior model. The second one is our model. One is our model, so the duisflay mixture. This is like instead with a gamma prior, and this is the Gibbs sampling performed by Grabsky, the first papers that I show you in a multi-study settings. And this is exactly the one that Grabsky has with a variational inference. And we can see that, of course, the variational inference speeds up this. Speed up the estimation a lot compared to the gibb sampling. I want to show you even the cosine similarity between like the common that are like five different common. Like we have some study specific for each study. And of course, we have some common Finature for tree study, but we have some shared that are shared only for tree study, but not all. But all in all, the cosine similarity is pretty good, is greater than 0.9 for all these like signatures. One important feature of our model is the probability of inclusion. The probability of inclusion. This probability is coming from the probit model. And why this is important? It's important because we can estimate the number of factor in this way. And so like where we put the probit, we actually consider the probability of the probit and we try to, of course, estimate the true number of signatures. And as you can see, Signature, and as you can see, our model can estimate very clearly each signature shared and even the specific one. Theme for scenario two. Remember that scenario two is more complex because closely mimics the data application of mutational signature. So, here we don't have any more factor one, factor two, common factor, but we have a mutational signature. Have a mutational signature. These signatures, these two were common across all the studies. And these two were shared among not all the studies, and these two were specific. And as you can see, even in these settings, that is more complex than the one presented before, we can closely estimate the finial of this signature. Signature and fame for the probability of inclusion. This is the mean of 50 different replicates, and we are able to estimate the probability of inclusion for the common and for the specific one as well. So, I want to drive some conclusion. We define a Bayesian multi-size. We define a Bayesian multi-study non-negative metric factorization that jointly analyzes multiple studies together and is able to capture the common mutations among different cancer type and the study-specific one. In this setting, we derive a fast and efficient CAVI algorithm with a variational-based approach. And there is a paper where we did that even in a multi-study settings for continuous. Settings for continuous data. And I want to stress that in this variational inference algorithm, there is a very fast and efficient estimation. And like in this, what we have seen is that we are able to estimate the number of signatures accurately. And like it's very fast and it will reduce. And it will reduce computational time. I want to thank all for listening, and I want to thank all of these people that work with me and are in some of these projects that I presented today. Thank you so much. Thank you so much, Roberta. Let's see if we have a question from the audience. See if we have questions from the audience, whether in person or from online. Is there anything online? All right, so I was wondering in this type of study where you have multi- A type of study where you have multi-studies, so you kind of expect that they count move away from zero. I was wondering whether maybe transformation to make data continuous then becomes still visible because as you move away from zero, this transformation should be better and better. So I was wondering whether using continuous method gives completely results completely off. That's one. And a second one, more general to this multi-style. Second one, more general to this multi-study setting. What is your view? What is the thoughts on measurement aloe in different studies? How this could propagate? Whether there is already something that you're thinking about it, or just maybe one general, one moderated. Thanks. Yeah. So thank you for this question. These are very two big questions. So the first one, there are some papers that they have done this. They call normalizing the data. So use continuous. Data so use continuous as count. It depends from the application in the sense that, of course, you can miss some specific thing, and sometimes you can have some error. So, we use this procedure. And I think that in this particular setting, you can lose some information. Some information. Of course, when you have, for example, big data, that's like big, big data, that could become like I think that normalizing could be a little bit better. So it depends from the data that you have in hand. And for the second question, so I think that this is most interesting in the sense that there are a lot of lot of there are a lot to do and right now we are starting actually a project on doing this sort of measurement error in like a multi-study settings of course it's not easy because each study came from a different population and there are no literature at all that perform measurement error in these multi-study settings. Like what I did, for example, in one of my first paper, is, but it could be very different, is how much robust these methods are. And that's a pretty robust, especially because like when you merge different studies, you can have more power than just considering one. But of course, there are a lot to do in this area. Thanks a lot. Yeah, thank you. Thank you so much, Roberta. And again, we can always move the discussion offline. Thank you again.