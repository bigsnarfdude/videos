Sorry, this title is just the title of this paper by Bill Helton and Igor Klepp and Yuri Volchich. I realized I didn't put down the citation, but this was Advances in Math 2018. I hope 2018 isn't considered old news. But anyway, so what is it about? Well, it's about basically What is it about? Well, it's about basically about zeros of non-cummative polynomials. So, let me get going here. So, let's begin by recalling Hilbert Smolstellen's eye. So, a classical theorem from algebra and algebraic geometry. So, if I have a pair of polynomials in some number of variables over the complex numbers, or you could say algebraically closed fields, I'm just going to say complex numbers. And you know that F2 vanishes everywhere that F1 vanishes, then, well, okay, some power of F2 belongs to the ideal generated by F1. To the ideal generated by F1. Instead of having a singly generated ideal, you can have an ideal with more generators, etc. But this instantly has some consequences. So, of course, the first is if the first polynomial is irreducible, then you don't need this sum power business. You can just say that F1 actually divides F2. And then, if you assume both polynomials are reducible, well, you can apply that first fact in both directions and conclude. First fact in both directions and conclude that they're just constant multiples of each other. Okay, so polynomials that have this, if I have two irreducible polynomials with the same zeros, then they're really the same polynomial. Okay, well, all right, ostensibly, this is a conference and analysis. And if somebody's first slide is the statement of Hilbert Schmostlenzatz, you have a right to sort of pound your fist on the table and demand an explanation. I guess one advantage of this online format is that, I mean, if you give Is that, I mean, if you're giving this in a lecture hall, this is the moment when somebody's arriving one minute late for the talk and they pause in the doorway and look at your first slide and turn around and leave. So I can avoid that. But anyway, so why am I quoting this? So what I'm going to talk about and what this paper is about is zeros of non-commutative polynomials in some sense. And for people who do or have recently started doing this sort of non-commutative function theory, it's very popular now, and there's going to be lots of talks about this. Popular now, and there's going to be lots of talks of this at this conference. A non-commutative function, whatever that is, is well going to be some kind of generalization of a non-commutative polynomial. And just like in classical function theory and classical complex analysis, if you take a graduate course in complex analysis, well, in order to talk about fancy things like zeros of analytic functions, you first need to talk about elementary things like zeros of polynomials. And you need to understand how zeros of polynomials work in order to get a feel for what to expect from analytic functions. A feel for what to expect from analytic functions. So, the motivation here is then if we want to do fancy things like talk about zeros at NC analytic functions or things like that, it's worth first understanding carefully what happens with polynomials. I guess for two reasons. One, because you want to avoid making silly conjectures. You want to know what's true and what's false and what kind of counterexamples exist and so forth. But equally important are the techniques. The point is, if you're going to prove things about these polynomials, You're going to prove things about these polynomials. Well, the ideas and the tools from algebra that you use to prove those things are very likely going to be useful in trying to move to this sort of larger analytic kind of category. So the motivation is simply in order to do analytic function theory, you should first understand polynomials to avoid being too pretentious. So the plan of the talk is I'm going to introduce polynomials and non-connuting variables. That will be on the next slide. We'll have to decide what we mean. We'll have to decide what we mean by a zero of a non-commutative polynomial, but once we decide that, we can ask for a sort of for theorems like this. If I have two polynomials and the first one vanishes everywhere, the second one does, how are they related? So let's try to make that a little bit precise. So first, the setup. So I'm going to talk about polynomials in G non-commuting variables. And sort of trivially, everything you say, you could just take G equals one and get things about polynomials in one variable. Polynomials in one variable, but if I want variables to not commute, I need at least two of them. So g is the number of variables. So I wrote down some examples here. I mean, just the point is you can just write down the variables in any order. And the point is, like the first one I wrote down is a commutator. And the point is that object is not zero because I don't allow the variables. I don't assume the variables commute. And then the second one there is just some arbitrary polynomial. So I said I want to talk about zeros of these things. Things and you have to think then for a minute about what you ought to mean by a zero of a non-commutative polynomial. So, for polynomials and commuting variables, well, okay, you can obviously evaluate them at g tuples of elements of the ground field, and then you'd say that it's zero and so forth. You can do that here too. I mean, obviously, I can just plug in complex numbers for x1 and x2, or x1, x2, and x3, and evaluate these things. But clearly, that's not going to work because this first polynomial. Work because this first polynomial will be identically zero if I just plug in scalars. And so evaluating at points won't sort of tell me what the polynomial is. So you need to figure out where to evaluate these and what you would mean by a zero. Where could you evaluate them? Well, you could evaluate them in any sort of mathematical world where you can take products and sums and multiply by constant scalar coefficients. So any kind of algebra over C. So this could be very general. I mean, Could be very general. I mean, we'll talk about matrices or square matrices of arbitrary size, but as was pointed out already in Orr's talk, I mean, in some situations in this kind of analysis, conceivable you want to go beyond just matrices. You want to, I mean, you could certainly evaluate these things on bounded operators, for example, either on a Hilbert space or some other box space or even more exotic kinds of rings and so forth. Nonetheless, for this talk, I'm only ever going to evaluate things on matrices of finite size, so I don't need these scary infinite cardinals or anything. Scary infinite cardinals or anything. So, when I talk about evaluating non-commutative polynomials, I'm only going to evaluate them on G-tuples of square matrices. But the point is, the size of the matrix is flexible. So, I think I wrote the annotation. Okay, so more formally, so if f is a polynomial in g non-commuting variables, and then for each n, which the n is the size of the matrix, I get a function from g tuples of matrices of that size to single matrices of that size. We just plug in the matrix. Matrices of that size, where you just plug in the matrices and do the matrix multiplication in the matrix algebra the way you know how to. So, in very fancy terms, this is a graded function. So, it's really not a single function maybe, but I can think of this matrix universe as sort of graded into levels. So I have a level one matrices, the one by one matrices, which you can identify with the scalars. You have level two, the two by two matrices, level three, and so forth. And the function really acts level-wise. Nonetheless, having a Nonetheless, having observed that, I'm going to, for the rest of the talk, ignore this n and just write f, and we're going to understand the level from the context and so on. But again, as War already hinted at, in this sort of non-commutative analysis, you observe two things about these polynomial evaluations. The first is they respect direct sums. So if I took, so my x without a subscript here really denotes the tuple. Maybe I should write that somewhere. X means this tuple. Somewhere. X means this tuple, Xg at whatever level. And by this direct sum thing, I really mean the tuple whose entries are X1, direct sum, Y1, X2, direct sum, Y2, et cetera. Well, just by the way matrix algebra works, you see immediately that you can pass this through direct sums. And also, just by the way, matrix algebra works. If I fix Matrix algebra works. If I fix a similarity S and some level N, and I, so this notation, if I write S inverse X S, this just means take each one of the X J's and conjugate it by the same similarity S. And again, just by the way matrix algebra works, I can pull that similarity outside the polynomial. So the evaluation, so the point is this, this tells you, the first one tells you how these evaluations at the different How these evaluations at the different levels are tied together. And these two properties together are also equivalent to a third single condition about intertwiners. If you've seen that before, I'm not going to bother with that. But then the point is, this is the motivation then for the definition of an NC analytic function, which is basically a function defined on matrices of all sizes that obeys these two properties. And that's supposed to generalize what polynomials do. But I think, again, nonetheless, for this talk, I'm only going to talk about polynomials. For this talk, I'm only going to talk about polynomials, or well, maybe not polynomials, I'll go a little bit beyond that, but I'll wait. So, anyway, for we're talking about zeros, I'm talking about evaluating polynomials, I'm evaluating them on square matrices of arbitrary size. So with that established, oh, actually, sorry, I did want to say one more thing. It turns out to be very useful in the theory, even if in the end you only care about the kind of polynomials I wrote where the coefficients come from the scalars. Wrote where the coefficients come from the scalar field. It often turns out to be useful to consider more general polynomials where the coefficients come from some matrix ring. So, what you would do is you would fix a matrix size D, say. So, this one I just wrote down this commutator, but with matrix coefficients. You take coefficients from that matrix ring. And then, if I want to evaluate on X's, now the X's are a different size from D. The size D is fixed. Those are the coefficients of the polynomial. I can still I can still evaluate this on matrices of any size, n, and the way you multiply the matrices of two different sizes is by the Kronecker tensor product. So this is familiar, I think. And the output then, so I input N by N matrices, and I output matrices in MD tensor MN, which is natural identified with matrices of size DN. Matrices of size dn. So evaluation with matrix coefficients is no problem either. But we will have clause to deal with polynomials with matrix coefficients, like I said, even if we think we only care about ones with scalar coefficients. Okay, so before I dive into anything about the actual, sorry, I mean, I think I think they before I dive into anything about the actual Before I dive into anything about the actual paper, though, I want to give some prehistory. So I said we're going to be looking for some kind of Milstellenzox that describes zeros of non-committed polynomials. Now that we know how we're going to evaluate them, we can ask what a zero is. But since we're evaluating on matrices, it turns out there's more than one way to define a zero. And so, in fact, there are at least three different ways you might define the zero of a non-commutative polynomial evaluated by. Non-commutative polynomial evaluated on matrices. And I'm going to tell you about all three ways. And the point is, the paper in question is about sort of the third way, but I'll tell you about the first two first, just because, again, in the spirit of trying to understand carefully what happens for a non-commutative polynomial, you can even ask yourself, again, what is a zero? So perhaps the simplest and most naive definition you could come up with for the zero of an NC polynomial is to simply say, well, I'm evaluating my polynomial at n by n matrices. The n by n matrices. n-by-n matrices. The n-by-n matrices form a ring. That ring has a zero element. So you say f has a zero at a point x simply if f of x is the zero matrix, which isn't entirely reasonable. So in this theory, so again, there are going to be different species of zeros. So in this theory, this is sometimes called a hard zero. So you say that your polynomial f has a hard zero at x. If you evaluate the f of the x's, and you get literally the zero matrix. So you can ask for Nostellanzats. Can ask for Nostellenzatz. Okay, so Nostellenzatz, the Zatz means theorem or statement or sentence. And so it's not a statement yet, it's only a question. So maybe I should call it a Nostellen Frage. But anyway, you can ask a sort of question about hard zeros, which is exactly the sort of basic question that's inspired by the commutative case. If I have a pair of NC polynomials F1 and F2, and I know that F2 has a hard zero at every place that F1 has a hard zero. That F1 has a hard zero, how are they related? How is F2? You hope to know that F2 is somehow generated by F1s. Like I said in the Hilbert-Nostellenzatz, it's a statement about membership and an ideal. But I'll leave it vague for the moment because the question is, if you know this, how are F2 and F1 related? There is a theorem about this. In fact, I put this one first because this is historically first. This is a rather old one. This is a rather old one. But before I state the theorem, let's just look at an example that shows you what you have to worry about. So let's take a very simple, very general example. So let's look at these two, well, not these two polynomials. Here are my polynomials. F1, so I'll work in two variables. So g equals two. I'm a polynomial in two variables. I'll call the variables x and y instead of x1 and x2, but just to save the subscripts. So for my polynomial f1, So, for my polynomial f1, just let that be anything. So, just take any two-variable polynomial you want there. And for f2, I'll take a commutator, x, y, minus y, x. So now it's immediate that if for the moment I just restrict myself to level one and look at hard zeros at level one, so just zeros that I get by plugging in scalars for everything, it's trivially true that F2 has a hard zero at level one every place that f1 does. That F1 does, for the stupid reason that F2 is just identically zero at level one. But what this means is that you sort of have no hope of saying that F2 is going to be generated by F1 in some sense or remember something about F1 because F2 is just zero being zero for its own selfish reasons that have nothing to do with F1. So any theorem you're going to hope for is going to have to account for this phenomenon. On the other hand, of course, you could have the same hard zeros of F for non-trivial reasons. I mean, you could have the same F for non-trivial reasons. I mean, you could have the same hard zeros of f by belonging to the ideal generated by f, which is more what you'd want. So you could belong to the ideal generated by f, but you could also have hard zeros for this trivial reason. And the theorem that helps explain what's going on is due to Amitsur in 1957. And it's the sort of those. Okay, I've only put one statement here. He actually proves a bunch more things, but this isn't really. Actually, he proves a bunch more things, but this isn't really the main subject, and so I'm just going to. The actual statement here isn't important for anything that goes further. But the point is simply, you can prove something. So, here's one of the things that he proves, among others. One thing that you can prove is, okay, let's just fix one level n. Look at matrices just of one fixed size n. And suppose that F2 has a hard zero at level n, every place that F1 has a hard zero at level n, then, well, basically the two reasons that I gave on the last slide are the only things that can happen. Last slide are the only things that can happen. What you can conclude is that F2 belongs to the ideal generated by F1 plus the ideal of things that are zero for trivial reasons at level n, meaning they vanish identically at level n. So this Froctor Mn is the ideal of polynomials, NC polynomials that are identically zero up to level n. So basically, once you account for this sort of trivial phenomena, that's the theorem. I'll mention in passing, so when n equals one, what is m1? Well, m1 is familiar to everyone. M1 is the concept. M1 is familiar to everyone. M1 is the commutator ideal. But at least for me, I think until four and five years ago, I didn't realize, I didn't know this fact that for every n, these mns are non-trivial. So for example, there is a non-trivial m2, which means there are polynomials you can write down in, say, two non-commuting variables that vanish on, well, they will necessarily vanish on all. Well, they will necessarily vanish on all scalars and will also vanish on all 2x2 matrices, but don't vanish at all 3x3 matrices. In fact, you can find such polynomials going up at every level, which is a thing that don't ask me to write down the two by the identity for level 2. I mean, it's possible to write it down and I could have looked it up, but I didn't bother. It's enormously complicated, and you'll sort of never guess what it is. So I'll pass by that. But the point is, you do sort of successfully get an But the point is, you do sort of successfully get a Nolfdellensatz here, and you can, like I said, you can say more things, but I'll pass that by. Okay, so that's one definition of a zero, sort of the most obvious one, the hard zero. But it turns out for the applications that people have in mind in the sort of non-commutative function theory, the hard zeros are often sort of not what you want. You want sort of weaker or softer notions of zero. So let me look at the second kind of zero. These are called by people who work in the field detailed zeros. The field detailed zeros. So, what we're going to say now is that, okay, well, I don't, I'm not going to ask fx to be the zero matrix, but if it's a matrix, well, it can still be sort of partially zero in the sense that it can have a null space, it can be singular. But if I take the same point x and I look at different functions f, f of x can be singular, and so f2 of x can be singular, but they might have different null spaces. They might sort of be singular in different directions. So, when you talk about detailed zeros, So, when you talk about detailed zeros, you want to keep track of the direction of the null space. So, a detailed zero isn't simply a G tuple of matrices at some level, but it's a G tuple of matrices together with a non-zero vector that sort of witnesses the null space of the matrix. So, a detailed zero is a pair xv where f at x, to evaluate f at the matrix is x, you get some matrix f of x, and then that matrix f of x is singular, and v is a non-zero, a null vector. Okay, so similarly, you could. Okay, so similarly, you can pose a question about detailed zeros, which is exactly the same question. If you fix a polynomial F1 and you happen to know that F2 has a detailed zero every time F1 does at every level, how are F1 and F2 related? Is F2 generated by F1 in some sense? And again, it's easy to see at least one way this could happen because if I take F1 of X. f1 of x and it has a detailed zero at xv and I multiply it on the left by any polynomial p and I call that product f2 obviously f2 will also have a detailed zero at xv so a trivial way to have the same detailed zeros as f1 is to be in the left ideal generated by f1 uh but again you have to sort of think But again, you have to sort of think for a bit and decide if there are other ways for this to happen. It turns out the answer is no, and it turns out the answer is no, there are no other ways for this to happen. And this is known as Bergman's Nolstellensatz. And so because this is an analysis conference, I should emphasize this is the algebraist George Bergman, not the analyst Stefan Bergman. I don't know that this is published elsewhere, but it appears in a paper. Uh, elsewhere, but it appears in a paper of Helton Cull in 2004. But it says exactly what I just said. So, if F2 has detailed zero everywhere that F1 does, then F2 must belong to the left ideal generated by F1. So there's no other way for that to happen. And in fact, again, what's actually proved is more than this. What's actually proved is that you don't really need to verify this on all detailed zeros. You only need to go up to some threshold size that depends on the degrees of everything. Of everything. And that's not so hard to see once you sort of dig into the proof. And again, you don't need a single F1. You can look at an ideal generated by several F1s, et cetera, but that's fine. Okay, so that settles the question for detailed zeros. So there's a third kind of zero, and that third kind of zero is sort of the actual subject of the paper and the talk. And that's sort of intermediate between the hard zeros and the detailed zeros. So what could you do? Well, in the detailed zero, you have an X where F is. zero you have an x where f is singular and you keep track of the vector in the direction in which it's singular but now we're going to say forget forget that so forget the v and simply look at all the x's where fx is singular uh and so again you can do this at every level so at each n will form what's called the zero locus of f at level n which is just again like i said all the x's where fx is singular and then you can organize the zero locus again you put them all together at every level Put them all together at every level, and that's your sort of this is your third notion of the zero set. And so, again, you can pose the same kind of question: if the zero locus of F1 is contained in the zero locus of F2, how are they related and so forth? And then so is there some kind of Nostellensotz there? And that's the sort of the main theorem of this paper is that, yes, there's a theorem there, and I want to state that theorem. Before I state the theorem, though, I want to sort of, oh, I need a couple of definitions. I want to sort of. Oh, I need a couple of definitions, and the best way to introduce the definitions is to kind of sneak up on them via an example. So, the example I want to look at are these two polynomials that I've written there, f1 and f2. So 1 minus xy and 1 minus yx. So again, just in two variables, x and y. Again, they don't commute, so these are obviously different polynomials. But it's pretty easy to see that these two are going to have the same zero locus. I have a I have a if I the determinant of one is going to be zero if and only if the determinant of the other is zero. And this is easy enough to see just by doing some linear algebra. So of course, in fact, something stronger is true. The determinants are literally going to be equal. They won't worry about that. But so you remember the fact for linear algebra that just says if I have a pair of matrices, then x, y, and yx have the same eigenvalues, which means 1 minus xy and 1 minus yx have the same eigenvalues, and therefore the determinant. Eigenvalues, and therefore, the determinants. Okay. So that's great, but that's sort of very special to this example. And what I want to show you is that there's a very different way to arrive at this conclusion without knowing this fact from linear algebra, but that points in the right direction. So I'm going to show you a second proof, which is completely different. And the second proof goes like this. So it exploits sure complements. And so if you're the kind of person who thinks about or uses short complements a lot, as soon as you see one minus. A lot as soon as you see one minus xy, almost your Pavlovian response is to recognize that as a short complement of a two by two matrix, by which I mean this computation. So if you just do out this, so this matrix multiplication, you can verify that this is true. This is sort of the computation that underlies the theory of the shirt complement. I should say something about how to interpret this. So these are really matrix polynomials. So what I mean is, how would you evaluate something like this? What I mean is, well, if X and Y are going to be n by n matrices, Y are going to be n by n matrices. You evaluate this by thinking of the one as the n by n identity. And so then if I plug in, when I evaluate these at matrices of size n, these matrices are all of size 2n by 2n. Okay, so the ones are identity matrices of the correct size. Okay, so you can do this. This is the sure complement calculation. Well, you could take this and you can obviously interchange the roles of x and y and get a similar computation for 1 minus. similar computation for one minus yx. So one minus xy appears as a sure complement of this matrix and one minus sorry xy appears as the short complement of that one. But now if you look at these two matrices on the right hand side you see that they are similar because you can just conjugate them by the unitary that swaps the order of the basis right so if I use that unitary oops yeah sorry if I use that unitary If I use that unitary 0, 1, 1, 0 and conjugate them, I can conjugate one of these matrices over the other one by that one. So maybe I'll skip writing it out. But if you do a couple more lines of calculation, you arrive at this thing at the end, which says that if I take my 1 minus xy and direct sum it with a 1, or look at 1 minus yx direct sum with a 1, well, they're not similar exactly, but you can move one over the other with a pair of With a pair of always invertible matrix polynomials. This Pxy and this QXY are going to be polynomials, two by two matrix-valued polynomials. But the way they're built, the point is they're going, this is what's important. The PXY and the QXY are always going to be invertible no matter what X and Y I plug in. And you can see this up here. So if I look at this one, this 10y1, that's obviously invertible for any Y of any size because, well, no matter what Y I plug in, this thing is simply low. What why I plug in this thing is simply lower triangular with ones on the diagonal, so it's determined to always one. And the same thing with the upper triangular thing on the left, and the same thing with conjugating by the unit theory and so forth. So the point is these pxy and qxy are always invertible for all xy. So now, once you know that 1 minus xy and 1 minus yx stand in this relation to each other, you can take determinants of this whole equation. The determinants of p and q are always non-zero. Always non-zero, which means that the determinant on the left-hand side will be zero only when that of one minus xy is, and similarly for one minus yx. So you conclude that the zeros are that the determinants are zero at exactly the same time because they stand in this relationship. All right, so the reason to go through all of that is that motivates sort of the key definition. So this is a very natural thing for people to do in ring theory, this kind of stabilization where you take your thing and direct sum on a bunch of ones. It shows up in K-theory because it's Shows up in K-theory because it's the right notion of equivalence of vector bundles and whatever. But anyway, so at least for people coming to this from algebra, this is a very natural and reasonable thing to do. Anyway, so you say that these two matrix polynomials F1 and F2 are stably associated if they stand in this relation. If I direct some on a bunch of ones and I direct some on a bunch of ones, I can move one over the other by P's and Q's that are always invertible, no matter what X's I plug in. So again, I'm using here X as shorthand for this tuple. X is shorthand, but it's tuple. So this is stable associativity. And by the discussion we just had, we conclude that if I have F1 and F2 are stably associated, then they have the same zero locus at every level, just by taking determinants of both sides. So one way for two polynomials to have the same zero locus is to be stably associated. So it means any Nostolan zots you're going to hope for is going to have to account for this phenomenon. For is going to have to account for this phenomenon. And I need one more definition to state the main theorem. Maybe I didn't say this clearly. If you look closely at what I wrote here, I wrote different numbers of ones there, M1 and M2. The reason is that I want to allow here F1, and this is, I can do this not just for F1 and F2 polynomials with scalar coefficients, but I mentioned at the beginning, I want to allow for matrix coefficients as well. And the point is you can do this exact same thing for polynomials with matrix coefficients. Exact same thing for polynomials with matrix coefficients and get the exact same kind of conclusion. So, this allows F1 and F2 to be matrix coefficients of different sizes. And the point is, you direct some on a bunch of ones and make them the same size and then move one over the other. So, since I'm dealing with F1 and F2 that may have matrix coefficients, you want to do factorization in that ring. So, you say an F is an atom if it doesn't factor into non-invertible matrices G and H. And this is. G and H. And this is, so forget Adam, this is really just the correct notion of irreducibility in this ring. So it's just what irreducible means. Okay, so with this definition of stably associated and with the definition of Adam, I can state sort of the first main theorem of this paper, which is the Nolstellensatz for this zero locus. There's three statements here. I'm suppressing the first one because I'm going to come back and talk about that later. I want to focus on the second two. So this is true not. To. So, this is true not just for polynomials with scalar coefficients, but also with matrix coefficients. We need one non-trivial hypothesis, which is we need them to sort of not certainly not be identically zero at level one. And really, what you need them to be is invertible at some level one point. So we'll just say f of zero is the identity. So item two sets. So again, we saw already that if things are stably associated, they must have the same zero locus. And item two says the converse holds as long as the polynomial. Says the converse holds as long as the polynomials I start with are irreducible atoms. So, if f1 and f2 are atoms and they have the same zero locus, then in fact they must be stably associated. And then once you have that fact, it's not too much more work to get the general theorem about inclusions, which says that if F2 has all has one of these weak zeros at every place F1 does, if it has its containment at the zero loci, then well, what can you say? You can't say that F1 divides F2. You can't say that F1 divides F2. There's not a statement about ideals here because you have to account for the stabilization. But what's true is that every irreducible factor of F1 is stably associated to some irreducible factor of F2. And that's sort of what your Nostalin san says. So that's the statement. So there's a lot more that goes on in this paper as well. And if there's time at the end, I'll hint at it. But I thought the best thing to do is give some. I thought the best thing to do is give some hint of the ideas that go into the proof of this. I won't actually do the proof, of course, but I'll sketch the line of reasoning, taking a lot of things for granted. But part of the, I said at the beginning, that part of there's a twofold motivation here. One, we simply want to understand what's true about zeros and non-commutative polynomials to guide our intuition about analytic things. But we also want to understand what tools and techniques are available. Are available. And for me, the real selling point of this is that when you see what the tools and techniques are, if you have any acquaintance, any prior acquaintance with any kind of this matrix operator theory or this matrix function theory, this kind of non-commutative analysis, the key idea will be instantly recognizable. And hopefully that will sell it. So what is the big idea? The big idea is to linearize the problem. And I will now spend And I will now spend a few minutes telling you what I mean by linearizing the problem. So, we need to import the one big important tool about non-commutative polynomials and more generally non-commutative rational functions, which are linear pencils and realizations. So, here's a definition. A monic linear pencil is simply a matrix, a polynomial, NC polynomial with matrix coefficients. It's linear. The x is only occurred to the first power. I'm allowed a Power. I'm allowed a scalar term, but the scalar term I'm going to insist is the identity. That's monic, where the aj's are matrices of some size. That's a monic linear pencil. And the crucial basic starting point fact is that every NC polynomial FX has what's called a realization. So what you do is there exists some linear pencil built out of the A's like this. You take the inverse of that linear pencil anywhere that it makes sense and you cut it down. And you cut it down with a vector B and a row vector C, and this so that this will hold for all matrices X. So this may look mysterious, but you can certainly believe it's possible to represent at least some polynomials like this, because one way you could get a polynomial out of this recipe is, for example, if you take nilpotent A's or A's that are jointly nilpotent. Because if you take this L of X inverse, that's really a geometric series. If you expand that in a geometric series, and Geometric series, and the A's are jointly milpotent, then that series will truncate. You'll get a polynomial. So it's clear that you can represent at least some non-commutative polynomials this way by choosing jointly milpotent A's. And then the theorem that you have to prove is, in fact, every non-commutative polynomial admits such a realization. Not a unique one, of course, but you can say a few things. So you call the realization minimal if the A's are as small as possible, the smallest size that you can get away with. And then if you insist on a minimal realization, those. And then, if you insist on a minimal realization, those are unique up to just a change of basis in this D-dimensional vector space where the A, B, C live. And you get a converse. So, if I have a minimal realization of a polynomial, then necessarily the A's are jointly nilpotent, and then this thing is sort of obviously a polynomial. So, all across this kind of analysis in this non-cummitative function theory, people deal with realizations. And the point is, if I write such a thing down with nilpotent, jointly nilpotent days, it's obviously a polynomial, and then the theorem says they're all. A polynomial, and then the theorem says they're all like that. All across the subject, you have theorems of this sort. We have an NC function that has some kind of property, like being monotone, or convex, or contractive, or positive, or whatever. And the game is always to try to prove that it has some kind of realization that makes that property obvious. This is a very pedestrian case just with polynomials, but that's a far-reaching principle that cuts across many parts of the subject. So, realizations are a very, very big deal. And I think in some other talks this week, Very big deal. And I think in some other talks this week, we'll see other instances of realizations in other settings. But they're a big and very important tool. Okay, so polynomials have realizations. So start with a polynomial that has a realization. And again, we're interested in understanding the zero locus of this polynomial, where these things are singular. Well, okay, so suppose, again, I'll make this hypothesis that it's not zero at level one, which I really needed to get that realization to begin with. So here's the trick. To begin with. So here's the trick. I'll look now at f of x inverse because I want to know sort of where f is singular, it means where this doesn't exist. Now, if f was a polynomial, f inverse is something like a rational function. And an important theorem of Schutzenberger going back to the 1960s is that rational functions also have realizations of some size, maybe not the same size as the original f, but the same deal. There's a matrix pencil and a b and c that realize this. If f isn't the power, if f inverse isn't the polynomial, of course. If f inverse isn't a polynomial, of course, then this isn't going to be nilpotent anymore, but if the realizations still exist, morally, what you do is you sort of formally invert the power series and solve for the coefficients. And there's lots of ways to do it. But anyway, rational, these inverses of polynomials and more general NC rational expressions that are regular at the origin are always going to have realizations. But you should be careful interpreting this because I have an f of x inverse on the left and L of X inverse on the right. These will both be invertible for x is sort of near the. Both be invertible for x's sort of near the origin, but these may not have the same domain if you're not careful. So, you'd like to do this in a more controlled way where you can control the domains. And that follows from a much more recent theory of Yuri Volchich, based on an earlier result by Vinnikov and Kaluzhny Verbovetsky, which turned out to have a gap in it, and Yuri went and filled the gap. But anyway, if I look at this f of x inverse, there's a notion of minimal realization there. If I take a minimal realization for f of x inverse, then in fact the domains coincide. Then, in fact, the domains coincide. So, f will be invertible if and only if the pencil in the minimal realization of the f inverse is invertible. Well, this has a very important consequence, which you can see by this comment about the domains being the same, because what's the domain? The domain is just all the places where fx is non-singular, but that's exactly the complement of the zero locus. So, if the domains are the same, that means the zero loci are the same. In other words, the determinant of f of x is zero if and only if the determinant of the X is zero if and only if the determinant of this linear pencil is zero. And therefore, starting with any polynomial that's non-trivial at level one, its zero locus coincides with the zero locus of some linear pencil. And this is what I mean by linearizing the problem, because now if I want to try to compare zero loci of general polynomials, what I should do is invert them and then realize the inverses and then compare the zero loci of those linear pencils. And hopefully, because now I'm only dealing with linear things, I can get some. I'm only dealing with linear things, I can get some traction. So it's very much like the philosophy in classical complex analysis, the kind of problems that people put on qualifying exams where they ask you some weird question about zeros of a polynomial, and the trick is to invert the polynomial and apply the residue theorem. So the idea is instead of asking for zeros of F, I'm going to ask for poles of F inverse. And by what we just said, the point is the zeros of F are places where F is singular. The poles of F inverse are the places where this linear pencil is singular. So that's the So that's the trade-off. And then now you need to start doing some. Now that's where the actual hard work starts because now you need to start sort of inspecting these pencils very closely and trying to understand how their properties reflect properties of the polynomial. And in fact, you can prove something stronger than just this sort of trick I did with the inverting the realization. You can prove that in fact, any NC polynomial is going to be stably equivalent to a model. Is going to be stably equivalent to a monic, stably equivalent to a monic linear pencil. So, not just that they have the same zero locus, but you can actually force this by a stable equivalence. You can prove this using some souping up of the short complement trick. And then the really, really important thing, and it's something that's non-trivial to prove, is that if I start with a polynomial F that's irreducible, I can always arrange that it's stably equivalent to a monic linear pencil where the pencil itself is irreducible. Now, irreducibility of the pencil simply means that the coefficients generate the full matrix out. That the coefficients generate the full matrix algebra. So, the second lemma is quite important and again, not trivial to prove and involve some more machinery. So, I'm just going to import this as a black box. But the point is now this is making it more likely that we can sort of solve this problem by appeal to some kind of linear theory. And the last ingredient is again an analysis, a close analysis of zero loci of linear pencils. And this was done in an Uh, was done in an equally interesting uh earlier paper of Kleppen Voltich. Uh, and they analyzed again this sort of Nostellenzatz problem or this Nostellenfraga for just for linear pencils and analyzed inclusions of linear pencils and so on. And they proved a whole bunch of things. And so this is just one consequence of one of their theorems. They prove a lot more. But one consequence is that if I have two irreducible monic pencils and they have the same zero locus, then in fact the pencils themselves are stable equivalent. Fact, the pencils themselves are stable equivalent, and in fact, even the L's are similar. So, this is again a highly non-trivial theorem, and there's lots of hard work behind this in the previous lemma. So that's the stuff I'm skipping over. But once you have these ingredients, now the reasoning of that proof of part two is clear. So, again, part two says if I have two irreducible polynomials and they have the same zero locus, then they're stably associated. And the point is, with all the work that's been done, the linearization trick. That's been done, the linearization trick works. So, how do you prove this? Well, you say, first, like we said, F1 and F2 are each stably equivalent to irreducible pencils. By the Nilfellenzats for the irreducible pencils, the pencils, okay, well, the F1 and F2 have the same zero locus, so L1 and L2 have the same zero locus. By the Nilstellan, thank you. By the Nestellenzots for the pencils, we have that L1 and L2 are stable equivalent, and then by transitivity, F1 and F2 are stable equivalent. So that's how you. Enough two are stable equivalents. So that's how you prove two, and then three is a consequence of two with a little bit more work. So the point is linearization is good, and realizations are very good. And so the realization machinery is extremely powerful. And it sort of from you should take away that it's the right way to approach these problems. Okay, so in the five minutes I have left, like I said, there's lots more proved in this paper, and I'm just going to skim over a couple of the highlights. I've been suppressing this first item, but that's quite an impressive result by itself. That's quite an impressive result by itself. So, maybe in the last five minutes, I'll just tell you what that says. It's called eventual irreducibility. Okay, so we have the zero locus at each level, the set of places where f is singular. So, what I'm going to do then, if I think about these x's, these matrix variables, I can introduce a single complex variable, one for each matrix entry. So if I look at how fx depends on the individual entries of all the x's, if I take that determinant, that determinant is going to be some giant polynomial in some huge number of complex variables. Some huge number of complex variables, g n squared complex variables, one variable for each entry of each matrix. And therefore, at level n, this very complicated, this is some very complicated hypersurface algebraic variety in a domain of dimension g n squared. And you can ask about this variety and its relation to the irreducibility of f. And the point is, by the linearization stuff we just said, we know that this is, we can replace this. This is, we can replace this variety by the variety determined by a moniclinear pencil. We also can assume that the pencil is irreducible if the F was irreducible. And the key fact, which is again, this is a high one, this is theorem C in the paper, non-trivial to prove, but the point is, if I take an irreducible pencil and form this determinant and view it as a polynomial in this, all of these complex variables, that as an ordinary polynomial in Gn squared complex variables, will it be. G n squared complex variables will be irreducible once the size is large enough. And that's important. You can't reduce, we get rid of that quantifier. But let me skip ahead. So now I can explain this first statement, which is that F1 is an atom. If it's irreducible, then this ordinary polynomial formed as a determinant of the function of all the matrix entries is irreducible for large n. It's easy to see that the large n is necessary, by the way. If I look at this example, 1 minus x squared minus y squared, that is irreducible as an N C. That is irreducible as an NC polynomial. But if I looked at it just at level one, at level one, it's a dense difference of squares and I can factor it. So the point is, even though I can factor it at level one, the algebraic variety I get, the zero locus at level one, is reducible, but once I go high enough, it will be irreducible. And the theorem is much sharper than this. You can actually get bounds on n in terms of the degree and so forth. But this theorem, I should say, by the way, involves a large amount of. A large amount of non-trivial machinery from sort of pure algebra, from the theory of non-cummitative rings and from invariant theory and so forth. And so I'll skip talking about that entirely. I had, in case there was time, a little bit more about flip-poly pencils, which I won't talk about on this slide. But what you the linearization trick said that I can replace the zero locus of my polynomial F by the zero locus of some linear pencil, but it's But it's not clear that you get every pencil that way. In fact, you don't. And what they prove, which I'll skip the statement because I'm out of time, but it turns out you can give a very precise characterization of exactly the linear pencils that arise by this linearization trick. They're called flip-poly pencils, and I'll leave the definition there. The point is, when you do this inversion, you take the F and you invert it and take a realization of the inverse. In the realization of the inverse, In the realization of the inverse, basically you get rank one perturbations of the things that occurred in the original pencil. The original thing was a polynomial, you had nilpotence, and you get rank one perturbations of nilpotence. And you can go on to say very detailed things about that, which I'll leave as an advertisement for reading the rest of the paper. But I'll stop there. Thank you. Thank you, Michael. Anybody have any questions? I have one. Yes. So, for the irreducibility in that previous result that you had for when you consider each entry of the matrix as a variable, do you have a lower, sorry, a bound for which n you should take? Yeah, so in the paper, they prove a very precise bound on how high you need to go. So, I think for the, there's several related results here. For the one I've stated here, well, okay, so you have to be a little bit careful. Well, okay, so you have to be a little bit careful. So, if I just look at some scalar-valued f's ones, for f with scalar coefficients, large n, n needs to be, I think, if you go as high as degree of f over two or something like that. So it's going to be built out of the degree of f. So there's a converse to this, in fact, which says that, actually, no, I'm saying that backwards. Yeah, so the short answer is yes, there are bounds, and then they give explicit bounds on how high you need to go, and it's going to depend on the degree of F, the bigger the degree. On the degree of f, uh, the bigger the degree of f, and I don't recall the exact results, but there are precise bounds for that. Uh, I will say, yeah, there's a converse to this. So, in the case of scalar value, scalar coefficients, there's a converse to this, which means that if I look at this determinant viewed as a function of the matrix entries, if I know that that's irreducible for large enough n, say bigger than half the degree or something, then that forces F1 itself to be an atom. So, in the scalar case, this is actually if and only if, and you can get degree balance both ways. Yeah, it's a very impressive result. Yeah, it's a very impressive result, but there's some serious algebraic machinery behind it. Thank you. Any other questions? I have a question for Stefan or Michael. Is polynomial singularitate null stellensatz, one word or two? This is a German noun with an English adjective. Although I think that no, I met my. I meant Michael Hartz. If you said that. Yeah, so I think you can turn it into one noun if you really want to, but maybe leaving it as an adjective in German would be more easier to read, perhaps. But yes, you could turn into one noun if you really wanted to. Yay. Okay. Yeah, so I was thinking the each of the rest of the talk is to come up with a longer word than that. Yeah, that's right. Okay. Okay, let's thank Mike again. We have another coffee break, and the next talk will be