So to put it short, lifting is a way to prove lower bounds by proving easier lower bounds first. So what do I mean by this? Let's make this plan a bit more explicit. The idea is that we want to start with some formula and prove a lower one for it, but maybe not a hard time. It but maybe not a hard lower bound. Let's prove a lower bound in an easy model. Then we'll take that formula and compose it with a function. I'm saying compose because if we were not talking about formulas but functions, this would be normal function composition. We'll see what it means for formulas. And then we get the formula that, well, maybe it's harder. And the way to prove that it is actually harder is to prove it. Is actually harder is to prove a generic lifting theorem that maybe applies to any formula. This lifting theorem would say something like: if you have a lower bound in this weak model, then we get a lower bound in the hard model. And then we're happy because we have a lower bound in the hard model. Of course, well, this is a pattern that has been used in proof complexity a lot, really. Here are a few examples. Few examples. This is just to scare you. I'm not going to talk about all of this. But maybe you notice a few patterns in this. That lots of these things talk about separations, trade-offs, these kind of things, where we have one hard result, one easy result. So this is something that lifting is very good at. Because if we start with a separation or trade-off in the weak model, then it often happens that lifting translates both the lower bounds and the upper bounds. It's both the lower bounds and the upper bounds. So then we get separation, straight-offs, whatever in the strong model. Yeah, but without further ado, let's start lifting. So I'm going to start with an easy uh lifting theorem. Um probably you've seen that before, but um But let's prove an overbound for the resolution size of setting formulas. Recall that setting formulas, we have a graph, we have one variable for each edge, and for each vertex we have a constraint saying that the parity of the incident edges is something. If sum of something is odd, then that formula is answered. So, what is the plan that we had before? Before tell us now, specialize to this particular formula. Well, what we want to do first is to prove a width lower bound. That's an easier task than to prove a size lower bound. Then we're going to lift the formula, compose it with a function. In this case, the function is going to be XOR of two variables. A very simple function. And then we're going to prove a lifting theorem. In this case, Lifting theorem. In this case, the lifting theorem is going to say the following: that if you have, if the composed formula has a proof of size s, then we can translate that into a proof for the original formula without lifting of width about log s. Or the converse. If you can prove a width lower bound for the original formula that's linear, then that implies a lower bound for the lifted formula that is exponential. Lifted formula that is exponential. So then we'll get our exponential orbound form by following these three steps. The first step, I don't really want to talk about it. There are you can see with your bound for taking formats in, say, Eli Van Sasson's paper or somewhere else. It's it's not really hard to do, but uh in the interest of time I'm going to skip it. But in the interest of time, I'm going to skip it. Let's move to the second point, which is how to leave the formula. So, in general, this is for any formula, for any function, any gadget. What we want to do is take uh the original variables of the formula, and then whenever one of the original variables appears, we replace it by a gadget. By a gadget, which is a function. In this case, we have a gadget of RETK. So we're going to replace every variable by a function on k variables. Let's see an example of this. See that the original formula is CNF over here with three clauses, x or y, n f x or y not y. And we want to compose it with the xor function. Well, then, as I said, Well, then, as I said, we take our original variables, and every time that x appears, we replace it by x1, x or x2. Every time that y appears, we do the same, but with y. If there are negations, well, then we want to negate the output of the function, and so on. Uh now this is a perfectly valid formula, but uh often we will want to talk about CNFs uh and this is not a CNF. And this is not a CNF. So, in that case, we have to expand whatever we got from lifting into something that's actually a CNF. And you can see from this example that the formula size might increase quite a bit. So, this tells us that we should ga use gadgets that are not too large. So, in particular, So, in particular, if we have gadgets of width about log n, then the blow up is not going to be too large. To the blow-up will be something about two-to-the-grid gadget. So, to the log n is fine. If our gadget had n variables, then maybe it would not be not so fine. Okay. But in this case, uh this is how we get the CNF. Uh this is how we get the CNF by composing the now this completes step two. Step two, we know how to tiff set it. Uh by the way this doesn't uh matter that much, but since our original constraints are parities and we are composing with parity, then what we what we get by doing this cheating composed with parity is just uh cheating over a different graph, which would be a graph that has now for each original edge. Has now, for each original edge, now we have two edges. So you think for a moment, and you'll see that these are the same thing. Great. So now let's do step three and prove a lifting theorem. What we want to prove is that if we have a proof for the lifted formula, we can get a proof for the original formula of small width. And we're good to use. And we're going to use tool that we all like, which are random restrictions. Maybe not so random in this case. So the kind of restriction that we want to use is the following. For each of the original variables, say x, we are going to pick x1 or x2 uniformly at random. So the variable that we did not pick, we leave it as it was. Was the variable the variable that we picked, then we flip another coin and we set it to zero or one again at random, uniformly. And now that we have a restriction, well, let's hit our proof of F-composed exorbitant with that restriction. So, what we get, we get another resolution proof, pi prime, and And since for each of the variables, original variables were killing one of them and we are leaving the other one as it was, then what we... the original formula gets transformed into sorry the lifted formula gets transformed into something that looks like the original formula except that maybe some uh variables got flipped so that the signs got uh went from x to not x. But this uh doesn't really matter. But this uh doesn't really matter. So we what we get is a proof pi prime that is essentially a proof of the original formula. And now we want to claim that this new proof has small width. So let's do that. What happens when you have a clause and you hit it with this restriction? What's the probability that it will survive? Well, first let's Well, uh first let's uh let's for a moment assume that all the um all the variables uh in that clause are independent in the sense that we don't have x1 and x2 in the same clause. Then what we'll get is that the probability of hitting one clause is one half, and then the probability of hitting it with a sorry, hitting one variable is one half. The probability of now choosing the value Of now choosing the value that will satisfy the clause instead of just removing the literal, is then one half again. So that means that the probability of hitting that is one quarter. Now what's the probability that one clause survives? Well, that's one minus that, the number of literals. The number of literals, because all the choices are independent. It's three-quarters to the width of the clause. Now, this was assuming that we didn't have x1 and x2 in the same clause. If we do, then the probability are not independent, but are only higher. So we don't really need to worry about that. And now that we have this, well, we can. We can do a union bound. So, if our original formula has small size, then we need that the probability that some plus survives is at most is something that's less than one. So that means that there is some restriction that so that no white clause survives, and that means that f has a proof of which at most it floats. Most loads. So that's great. We prove our lifting theorem. That means that we're done. We just take our with lollar bound, compose with check-in, apply the lifting theorem, get a size loller bound. Can you repeat the motivation why we do this for changing for resolution? This is just an example. So you could for this case. Example: So, you could for this case, you could just use the size width relation and be done. Yeah, but now that we know how to lift, let's uh start lifting with communication. Right, so most of the results I talked about just use proof complex techniques. Use proof complex techniques. In this talk, we're going to focus on the kind of lifting theorems that we get from communication complexity. So, if you remember the general scheme, it was this take a formula, compose, proof of lifting theorem, now the composed formula is hard. If we want to use communication complexity in this mix, well then we have to put it somewhere. So, what happens in this case? So, what happens in this case is that we take our formula, we translate that into some search problem, and then we do all our lifting in the communication complexity world. So, we will end up getting a lower bound in communication complexity. And then, after that, we translate the lower bound in communication complexity into a lower bound for proof proof complexity. Right, so well, in order to explain this, then we'll have to do two things. So, first, figure out which are these search problems that we want to work with. And the second thing is, of course, talk about communication complexity. Let's do these things in order. Let's talk about the falsified search problem. And this is something that you might also have seen. It's a very natural search problem that comes associated with any unsatisfiable search. That comes associated to any unsatisfiable CNF. So the idea is that we have a formula that's universal, so everyone can see it. And now the input for the problem is some assignment, some assignment to the variables of this order. And our task is then to produce a clause that is falsified by the Use a clause that is falsified by that assignment. So there's always going to be at least one clause because the formal is unsatisfiable. So this is not a hard problem. It becomes hard when we are working in some particular model, for instance communication or query complexity. Yeah, by the way, Yeah, by the way, let's give an example of what this kind of problem is. Say that we have this small C in F from before, and I give you an assignment, x equals 0, Y equals 1. Well, which clauses does these assignments falsify? Turns out that only one of the clauses is falsified, which is not why. So we can answer not why. That would be the answer to this problem in this case. And why is this problem interesting? What's the connection to proof complexity? Well, that is because proofs and search problems are very related. So if you take a proof, let's say a resolution proof in this case, you can easily build a precision tree. Be the decision tree for your search problem. Of course, for that to be a tree, your original proof has to be tree-like as well. We'll see what happens when you're not tree-like. But for now, let's see how that works. This was also observed a long time ago, that if you have a small proof, you get a small decision tree. How does this happen? Well, again, this is a resolution. Again, this is a resolution proof. So at every node, like this constriction over here, we get it by resolving over some variable, in this case y. So we can build the decision tree by querying what happens with y. And then if our goal is to find a falsified clause, what we want to do is to go down the branch that falsifies that. So if the answer is that So if the answer is that, well, our variable was uh equal to zero, then we want to go to the place that had the literal y. If the literal if y equals one, then we want to go to wherever the literal not y came from. We do the same for x. And you can check that this is actually a decision tree for the falsified search problem on this formula. Right, so that's a falsified closed search problem. We could try to do lifting with this. The problem is that if we only work with the decision tree model, that's not great because we would only be able to prove depth-load balance. So since we cannot balance proofs, then the kind of decision tree that we get is something. Then the kind of decision tree that we get is something that might not be balanced. So we cannot really prove size lower balance, we can only prove depth lower balance. A way to fix that is to move to a more powerful model that actually can be balanced, such as communication complexity. So let's do that. Let's define communication, deterministic communication. Maybe you've seen that before, maybe you haven't, but the idea is Before, maybe we haven't, but the idea is that now we have distributed problems. We want to have two players, Alice and Bob. Alice has an input x, Bob has an input y. They now want to compute a function that depends on x and y. How do they do that? That's by sending messages back and forth. So Alice sends some message that depends on her input only. Then Bob is going to And Bob is going to reply. Now Bob's uh message depends on his input and whatever message he has sent before. They keep doing that until in the end both players know what's the output of the function. We assume that the players have unlimited computational power, but for deterministic communication, we ask them to be deterministic in order to And the cost of computing a function then is how much they had to communicate. How long is the sum of their messages and bits? So, Mark, I didn't understand this motivation for moving to communication complexity. Could you just say that again? Yeah, sure. One motivation is that if you want to prove size or bounds okay, so if you take a decision tree, a decision tree you really you cannot really say anything about the the size of the decision tree by just looking at the depth yes but then we could try to blow down the size I mean sure we could do that I mean that that's that's also a way to do it and we'll actually talk about that later but proving Mm but proving deflower ones is of course easier than proving sashlo ones. I don't know if that answers your question. Well you did say that communication complexity can be balanced. So can you say in what sense? What does it mean that it can be balanced? Right. It means that it can be balanced in that if you have a communication protocol of size S, then you can always S, then you can always balance it to have depth log S. Maybe for this to make sense, I have to do a couple more slides. So I'll go back to that in a couple more slides. Right, uh, this was the definition of communication. Let's see an example. So, imagine that we want to compute the very easy function, does C. easy function does 6 divide x plus y well in this case Alice can start by telling to Bob what's the parity of her input and now well Bob looks at the parity of his input and if the sum turns to be odd then he already knows that 6 does not divide x plus y so he can say that if the parity is even The parity is even, but then he says maybe and now tells what is his input model of 3. Now Alice looks at her input model of 3 and that's enough information to figure out the answer to the problem. So she can announce that. It's very convenient to look at this thing as a tree or something like a decision. A tree, or something like a decision tree, except that no, to a normal decision tree would query a variable at the x0. In a communication protocol, instead of querying a variable, we can query any function that depends on x, or any function that depends on y. So that's something that's much more powerful than a decision trick. And now I can answer your question a little bit. It's that look at But it's that look at this decision tree. Now if the size of this decision tree is S, then you can balance it so that the depth of the decision tree is at most log S and that would be the same as saying the communication is at most log S. Okay, so that's deterministic communication Now let's see how to use it to ask you about balancing again so when can you balance and when you cannot because we know that sometimes we cannot balance right we cannot balance proofs but we can always balance the mystic products but only deterministic Yeah, but only a little mini stick. I don't think we have balanced communication protocols, can you? We can. But it doesn't even matter, does it? It should be possible for random messages as well, because they're just combinations of the doctorists. I don't remember if you can. I don't remember if you can balance randomize, but you can definitely balance the mistake. So you can definitely balance hash and redistribute against the correspondence. So if your protocol is a cache and redistribution, then it's here to balance because it can always move to forward. Because you can always move to formulas and back, you know, solving the same problem. Sure, but you don't need to move to formulas. You don't need to have both caching in some games. In general, for any communication problem, you can balance. Just frequency can balance, perform as you can balance. Yeah. The same thing. So protocols are two like Same thing, so protocols are two like objects. No, you can verify. You can see that if you only reach this point, it doesn't matter what you're about to say, right? It doesn't matter right. Yeah, so let's move on. Fair enough. Okay, so we can talk about this offline. Let's see some examples of how to use communication complexity to prove results in proof complexity. So the first result I want to talk about is one of the very first results that ever used uh communication complexity or and lifting. Or and lifting to prove results in improved complexity. So, this was a separation between tree-like resolution and cutting plates. Saying that the opposite, so tag-like resolution and three-like cutting plates. So, saying that there are some formulas where if you restrict your cutting plates proof to be tree-like, then you could do better. then you could do better with normal cutting planes, but not only that, you could also do better by just doing a resolution. Which is what I'm writing here. Some formulas that have polynomial proof in resolution, but require exponential proofs in categories. So when this was proved, this was when the first, the original lifting theorem of Raz and McKinsey had just Razzon McKenzie had just came out. And well, at that time it was very nice, but people didn't quite see how to use it except for these four nice people over here, Bonet Estevan Galesi Johannes, who figured out actually we could use this for cutting plates. So let's see how to do that. So if you remember the plan, what we wanted to do was start with some formula, translate that into a search problem, get use lifting, get lower bound for communication complexity, then translate that into a lower bound for proof complexity. What we mean is that we should figure out a way to do this last step. A way to do this last step, which is going from communication lower bounds into proof complexity lower bounds. Or what would be the opposite, start with a proof in cutting planes, build a communication protocol from it, build an efficient protocol. For which problem? Well, for the falsified closed search problem, in this example, I'm distributing the inputs. Distributing the inputs a bit arbitrarily, so I'm just giving some of the inputs to Alice, some of the inputs to Pop. In general, there's going to be a very clear way to do that because we're going to use a gadget that has two sides. So, and we're going to give one of the sides to Alice, one of the sides to Bob. But let's see how to translate this proof into a communication protocol. Again, what we want to find is a falsified clause. So let's start by something that's sure to be false. Let's start at the top at the contradiction. And now we want to figure out whether it should go left or right in the proof. So for that, we're going to take one of the children of Of this node in the proof we are at, the line in the proof we are at right now. And we're going to evaluate it. It doesn't matter which one we take. So to do that, well, what we can do is the following. So Alice knows the values of the variables that she has. So she can look at the occurrences of her variables in the formula and just sum everything together. If there were coefficients, she would If there were coefficients, she would multiply her variables by the appropriate coefficients as well. And now she just goes and sends that number to Bob. Bob, on the other hand, has the remaining variables. So by knowing the sum of Alice's variables plus his own variables, he can figure out whether this linear equa inequality is true or false. Inequality is true or false. So he does that and tells Alice, well, this one is false, according to the assignment that we have. So now both players move down this line in the proof. And now they do the same, so they take another ch uh child at random, they evaluate it, well in this case it turns out that uh it is satisfied, so now they know that they sh they should move to the other child. That they should move to the other child because the proof is some, so there's always going to be at least one of the children that is possible. So, this was fine if we had small coefficients. Because then the numbers that the players send while they are communicating are not too large. It doesn't take too many bits. But since we are working on cutting But since we are working on cutting planes, and for all we know, the coefficients might be very large, maybe exponentially large in the size of the proof. That's the magnitude of the coefficients. The number of bits is, of course, polynomial. But it could happen that the amount of communication that we need to send the bits is larger than the size of the proof, the depth of the proof. The depth of the proof. So, well, that's not great. We wouldn't be able to say anything in that case. What's the solution? Well, notice that we didn't really care. So, Bob didn't really care about the sum of Alice's variables. The only thing that they care is evaluating this greater than function. So, if instead we use a communication model where it's easy to evaluate the greater than It's easy to evaluate the greater than function, then they can do that and they will not be paying for the size of the coefficients. So there are a couple of models where you can evaluate the greater than function easily. One of them is randomized communication, which I did not define, but it's similar to deterministic communication, but now the players can also see a random string and now they ha they have to communicate with some error. They have to communicate with some error. The problem is that back in the 90s, there was no such thing as a lifting theorem for randomized communication. Now there is. So what they had to do was to use a different model, which is deterministic communication with a greater than oracle. Let's see what that means. So now, in addition to Alice and Bob, we have a third party, which is the oracle. It's an oracle that It's an oracle that just knows how to compute Greater Dan, doesn't think, doesn't do anything, just computes GreaterBan. But now, in order to communicate, the players send messages to the Oracle. The Oracle compares them and tells the answers back to the players. So uh in this model we don't charge them for the amount of communication that uh goes to the oracle. Uh That uh goes to the oracle. As in if they're comparing real numbers, uh that's fine. It takes infinite amount of uh bits to to send them, but that's fine. We only charge them for the answers that the Oracle gives. So this has cost one. And that's great because now we can compute greater than very cheaply. And we can do our reduction from before very efficiently. By the way, if you think about this model for a moment, you will see that this is more powerful than deterministic communication and it's less powerful than randomized communication. The fun thing about this model is that there is a lifting theorem for it, which is the following: it says that take any function, relation, anything, compose it with. Anything. Composite with the indexing function, the indexing gadget. What is that gadget? Someone mentioned it yesterday, I think. But it's just the following. So the inputs are two parts. So we have something that's a pointer and something that's a string. And then the pointer indexes a position in the string. The string is zero one, so we just uh the the output of the function is whatever position. Is whatever value that string has on that position. So if we compose with the indexing function and we have a protocol that has cost D, even in this weird complication with a greater than oracle. By the way, I didn't mention that this is something that comes from Jan Krychik. Then you can get Then you can get a decision tree for your original function that has depth the overlog n. This overlog n might be a bit surprising, but it's actually not that much if you think of how how we will do the opposite thing. How say that you have um you have a decision to build the communication protocol. with the communication protocol. Well, what you would do is that uh every time that that you're talking about some variable, you would instead talk about the variables that uh are in the gadget. And then evaluating the gadget takes us logging bits of communication because Alice has to send her pointer to Bob and Bob says one bit which is the value of his string. So if we're trying to do go in the opposite direction we'll get that at this Direction, we'll get that a decision tree of depth d gives us a protocol of depth or cost d times omnipotent. And this theorem is just saying that this is optimal. This is the best that you can do. This is always split as the index is for Alice and the rest is for Bob? Yeah, we are allowed to choose how we want to do it, but the Do it, but the best way is to give all the indices to Alice and all the strings to go. I don't know if doing something else buys you anything, I don't think so. And so, using this theorem, we can now prove our resonance separation. How we take a Kebling formula, we prove We prove um depth, lower bound for it, then run all of our reductions and we will end up getting a lower bound for cutting plates. And the good thing is that even if we did this lifting, the upper bound for solution doesn't break, so that's how you get the separation. Okay, now this was what was happening somewhere. This was what was happening some 20 years ago. Let's see something more recent. We're going to see a separation between polynomial calculus and cutting plates. And now I'm talking about normal cutting plates, nothing to relax about this. So our goal or the result I want to talk about is that. Is that we can separate polymer calculus and cutting planes. So there are formulas that have short cutting plate proofs, require exponentially long cutting plane proofs. And since we don't want to be tree-like, well, then we need models of communication that are tag-like. We'll have to do something that's tag-like lifting and that's going to talk about tag-like communication protocols, whatever that means, and back-like this isn't true. That means, and dug-like decision trees, whatever that means. Maybe we should call them decision dugs. Right, well, let's see what this is about. Again, I claim that something that proof complexity people should be very used to. So, remember a few slides back when we talk about how would you take a resolution proof and translate it into a decision tree? That was just reversing the armor. That was just reversing the arrows and figuring out what to query. Well, we want to make this a bit more general. So in order to define DAG-like protocols, we are going to forget about queries. We're just going to look at let's say the dual of the proof. What do I mean by that? So l take take any of the lines, let's say white, let's say contradiction. Let's say contradiction instead. And look at what are the assignments that falsify this line. In this case, every assignment falsifies contradiction. So we have a node that's labeled by all the assignments that falsify it. That's all the assignments. And if we want to look at this node, x or y, well, this is only falsified by one of the assignments, which is 0, 0. So what happens when we do this transformation? So, what happens when we do this transformation? What properties do we have? Well, first of all, all of the nodes have the same shape. So, in this case, we were working about with resolution, so each node has the shape of a sub-cube. If we were working with cutting planes, say, then each node would have the shape as a half space. All the assignment does. All the assignment that's falsified, it looks like a half space. What else? So, we always start with a contradiction on top. That means that we always start with a full set of assignments on top. And then, well, since the proof is sound, we know that if each point in a note over here, well, either goes to the left or to the right. To the left or to the right, maybe both. But at least it goes to one of the places, and that's just because it proves itself. Well, now let's just take these properties and abstract them. So forget that we come from a proof. Let's just ask that we have a DAG that has these properties. Every set has the same shape. We start with a contradiction. With a contradiction, each point goes to at least one of the children. Then that's the definition of Ducklight Protocol. And the shapes then really matter a lot. So, depending on what your shapes are, your protocols are going to be more or less powerful. We've seen, so I mentioned shapes coming from. So, I mentioned shapes coming for resolution in cutting planes. If we're going to work with communication, then there are some other shapes that are more friendly to communication. So, these would be rectangles and triangles. By a rectangle, I just mean a set of assignments that is the product of some set on Alice's side and some set on Bob's side, with the Cartesian product. Triangles is slightly harder. Uh triangles is slightly harder to define. Let's say that it's just a generalization of half spaces for an object. So these are Doug-like protocols. Are you happy with this? Let's see what our decision dugs then. So this since we defined the DAC-like protocols, well, we can view this as a particular kind of Particular kind of lag-like protocols, which are protocols where the shape is a sub-cube. Remember, these were the kind of protocols that we got from Presolution. So the set of assignments from each node is something that's equivalent to the set of assignments that falsify some particular clause. So for some people. You can also think about this as. As a decision tree where you are allowed to forget variables, for instance. And since we're talking about resolution, maybe you will be not that surprised to learn that there is something called the width of a decision type. If this was a resolution proof, that would be just the width of the largest clause in the proof. Now, well, let's just look at the largest co-dimension of a sign. The largest co-dimension of a subcube. That's the same thing. But talking about width, well, probably this reminds you of something. What is this DAC doing? It's squaring variables, it's forgetting variables, that's nothing but the JRISDA logic. So, also something that we've seen a lot. By the way, let's see a couple of examples of what decision tax can do. Imagine that you want to solve the fork relation. So that's a problem where you are getting one string with the promise that the first bit is a 1, the last bit is a 0. Now we want to find the 1 followed by a 0. If you wanted to solve this with a normal decision tree, well the depth of that will be low in the. Decision tree, well, the depth of that will be logging. If you solve it with the decision bug, now you're allowed to forget variables. You play the jurista mammal beam on this function, and you just need to remember some place where there's a one. And then, as soon as you find another variable, and if it's a zero, then you found the solution. If it's not, and the next one. If it's not and the next variable is a 1, then you forget that the first variable was a 1. You just need to remember one of the variables plus the variable that you're not seeing. So the width of fork is 2. Something that is not an example is, for instance, a branching problem for parity. So I'm thinking of if you're trying to compute If you are trying to compute parity with a branching program by remembering what's the parity of the first k variables. Well, this is not a decision duck because the shape of that DAC, you need to remember parities. Those are not so cubes. This is something that you're not allowed to do. So are you also happy with this is induction? Then. Great. Then we have a lifting theorem between the DAGLI protocols and DAGLI and decision DACs. Which is that, again, the gadget is indexing, so it's saying that if you have any function relation composite with indexing and have a duck-like protocol whose shapes are either rectangles or triangles, both shapes work. Shapes work. Then you can get a decision DAC for the original function search problem, whose width is O of log S. And if you recall, the very, very first example that we saw was an example where the gadget was absorbed. And we didn't have rectangle or triangle stack. What we had there were Subcube does. And we were also translating size into width, which is exactly what's happening over here. And of course, all the work goes into proving this lifting theorem. But the good thing is that once you've proved this, then you can plug it in any formula that you wish. And in this case, we can take that chat in formula composition. Formula composite with indexing. We'll get the lower bound for cutting plates, normal cutting plates. And the good thing is that again the upper bound for polynomial calculus still translates. And we get a separation from here. Separation of work? Of polynomial calculus and sites. So let's recap. So what are the nice thing of listing theorems? Well, we got the proof that we're very modular. And we just had to prove a listing theorem that works for any formula. And we also could use connections to Use connections to communication complexity. We could use theorems over there to prove results in proof complexity. That's great. What are some other things that are maybe not that good is that by doing this lifting, the kind of formulas that we get are very artificial. Maybe for setting that's not so much, but for instance you will never get the Patient Hall principle by looking at uh by by looking at uh at lifting you'll get vision whole principle lifted by something. Uh so if you come from the if what you're interested in is seeing what principles can prove some proof systems prove or not that's something that lifting will not easily be able to do. Another bad point is that since we are doing this modular thing then we don't Doing this modular thing, then we don't work so much with the proof, we work with some more abstract objects, and we lose a bit of grip on what's going on. Some things that would be nice to prove about lifting theorems. So I mentioned that there was this dagger-like lithium theorem for rectangles and triangles. It would be great to prove it for other shapes like the intersection of triangles. Then we would get lower than for resolutions. It would be also great to prove lifting theorems for multi-party communication. And then that would give us a way to attack semi-algebraic proof systems in the sum of squares meaning. Could be also great to make lifting theorems work with more gadgets. So I only discussed the indexing here. It's possible to make them work for other gadgets, but then sometimes you can't get so strong lifting theorems, you get something different. Susanna is going to talk about this on Thursday. But yeah, if you can prove any of these things, that would be great. And I will end that with this. Questions? You didn't mention PLS communication protocols. So is it clear? Can you think of them as more general versions of the stack protocols? PLS protocols are very related to the Protocols are very related to diaglike protocols. And in fact, diaglike protocols come inspired from TLS. The thing is that when we're trying to work with communication, the extra conditions that you have on PLS don't seem to work. Maybe. Uh maybe Dmitri or Mika can tell you uh something more informative than they're just equivalent. Just like in just equivalent based? Yes, at least the PLS protocols from REST were spawned. Exactly the same. So that's for local relations or only for the it's just uh it's just another definition. It's uh it's just some syntactic change. Okay, thank you. More pressure? Done then. Thank you again.