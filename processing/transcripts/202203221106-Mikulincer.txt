There's a big break in between um so we start with the first talk by uh then uh Nico himself from MIT uh who's going to talk about in transport map which will be very new transport map with couple of very nice features. So please go ahead. Okay, um thank you for the introduction. It's fun to be able to do this in person and In person. And yeah, I want to talk about the Brownian transport map, which is a recent work with Veda with El Schanfeld. We'll also talk later. And it's a nice story about optimal transport and stochastic analysis, which I hope to show. But it will take me some time before I can actually explain what is the Brownian transport map, and maybe I'll start with a bit of a motivation of w why wanting to look at it. Alright, so so just to fix here. So just to fix, yeah, to fix the jargon, we call a transport map any map which pushes forward for this specific case the standard Gaussian to some target measure on R D. So it doesn't have to be optimal in any sense. But because the standard Gaussian is such a well-understood measure with nice properties, as soon as you have a transport map which takes it into any other measure, then you can imagine. any other measure, then then you can immediately infer things about this measure. For example, if if this map is computationally tractable, then you can sample from it. And if this map has nice analytic properties, then you can infer corresponding properties for your target measurement. Okay, so in some sense, I'm interested in both points, but the talk is mostly going to focus on the second point. And I should have said, feel free to stop me at any point. I try to make the talk as self-contained as possible. To make the talk as self-contained as possible, but probably I failed. Right, so if you're looking for transport maps, well, we know that there is one distinguished transport map which comes from optimal transport. Okay, it's the one which minimizes the Vassarstein distance. It is the optimal transport map. And when the source measure is Gaussian and the target measure is nice, then we know that not only does this map exist, it means it is canonical in some way. This map exists, means it is canonical in some sense, it also has nice properties, which goes back to my point before. So, an example of a nice property was discovered by Caffarelli more than 20 years ago, and it says that if mu is more low-concave, and I'll explain what I mean by that in a second, if it is more low-concave than the standard Gaussian in Rd, then the optimal transport map is one Leap Sheets. Okay? And what I mean by Okay? And and what I mean by more low concavity than Gaussian or strong low concavity is that if you look at the logarithm of the density of your measure, it is a strongly concave function. So this inequality is just an equality for Gaussian measures, which have a quadratic potential, which justifies the term of more non-conc than the Gaussian. Alright, so let's see what we can do. Let's see what we can do once we know that such a map exists and it is one Lipschitz. So, you know, one very nice property which the Gaussian possesses, which is called the Gaussian-Pointra inequality, says the following thing. That if you take any test function, say differentiable f, and you ask what is the variance of this test function in Gauss. Of this test function in Gaussian space, and this is always bounded by the fluctuations of the function, okay, by the square norm, the gradient. Essentially, it means that in Gaussian space, a function which has small fluctuations is essentially constant. Now, you can ask for the same thing for any other measure, right? So maybe other measures satisfy a Point K inequality. So we would say that this is the case for general measure mu. For general measure mu, if the same inequality is satisfied, but you know, we allow a pre-fracture, pre-fractor in the right-hand side. And this is what we will call the Poincaré constant. Okay, so this is a crude way to measure concentration. But it, in some sense, already captures the fact that the Gaussian is highly concentrated because this is a dimension-free phenomenon. Okay, so it is true for the Gaussian in any dimension. So, a very classic. So a very classical theorem from the 70s due to Braskamp and Leibniz says that for any, well, at least in this version, for any strongly low-concave measure which is more low-concave than Gaussian, then it also satisfies the Poincaré inequality, which constant, which is not worse than the Gaussian. And and it exactly captures this idea that if you are more low-concrete than the Gaussian, then you have a better concentration profile. You you get better better properties. Better properties. And using this result of Caffarelli in 2002, Cordello Raskin produced a very, very short proof of this fact. So the point is you want to measure the variance of a function with respect to some target measure mu, which you know that it is more local than the Gaussian. So you can transport the variance into Gaussian space, measure the variance in Gaussian space, apply the Gaussian space space. The variance in Gaussian space, apply the Gaussian-Poincaré inequality, and now all you need to do is apply the chain rule in some Cauchy-Schmart, right? And because this map is a lip sheet, then its gradient must be smaller than one. But this is a transport map, so you can go back to your original measure, and in two lines you've proved this version of Russ complete. Okay? And this is true for many other This is true for many other properties of the Gaussians which can be transported. Now, the point is that this property of being Morlon-Concave than the Gaussian is restrictive. There are many measures which possess nice properties which are not Morlo-Concave than the Gaussian. And one question is how far can we take Caffarelli's result to a much larger class of measurements? Larger class of measures. Okay, so for which measures can be realized as a leap sheets push forward of the standard also. So what this tells you is that those measures must satisfy a point claim quality, for example. Otherwise it wouldn't work. So maybe the first attempt to relax the strong low concavity assumption would be to look at low concave measures. So now the potential So now the potential is not a concave function, it's not a strongly concave function, just a concave function. So the Hessian is upper bounded just by zero. And it is known that if you have a measure which is low-concave and compactly supported on a ball of a given diameter, then it satisfies the dimension free-point curve inequality, which only depends on the diameter. And there are many proofs of this fact going back to. And there are many proofs of this fact, going back to the works of Penn and Weinberger in the 70s, just as there are many proofs of the Braskan-Pleb inequality. However, there is no transport proof of that. So this leads to a natural question. If you have such a measure, which is low-concave, completely supported, can you realize it as a Lipschitz push forward of the Gaussian? Okay, and and maybe let me just comment that this question deals m with more than just finding a new proof for an existing result. For an existing result. Because once you find such a map, then you can transport other properties like phi-sobolev or log-sobolev inequalities, higher eigenvalues of the weighted Laplacian. So if the Poincaré constant can be thought of, this is the inverse of the first eigenvalue, you can do the same thing for higher eigenvalues, isopyrimetric inequalities, and I think Nathaniel Ehrstox is going to touch upon some of those applications. Upon some of those applications. Let me mention maybe another application which is close to my heart and is not in the same flavor of functional inequalities. And this goes back to Stein's theory, which in some modern formulations of it tells you that once you have well-behaved transport maps, then you can derive g very strong central limits for right and with improved rates of convergence. So like the the question is highly motivated. The question is highly motivated. And maybe just to mention another class of measures which can be interesting in this contest. So not only do log-concave measures should be realized by ellipsis marks, we can also think about Gaussian mixtures, which is in some sense another way to realize convexity. Okay, so here I want to consider additive Gaussian mixtures, just a convolution of the standard Gaussian with some measure. Standard Gaussian needs some measure. Now, this is pretty general, and so one way to restrict it is to demand that this mixing measure again will have bounded support. But the measure still has infinite support because it is a convolution with the Gaussian. So, some recent work in the years, in the previous years, showed that those measures also satisfy Poincaré inequality and also Logso-Bolev inequalities. Inequalities, again in a dimensional freeway, which only depends this time exponentially on the support of the mixing measure nu. So, this leads to another question: can we realize those measures as a push forward of the standard Lausanne? And one final motivating question, going back to low-concave measures, but this time without requiring them to have finite boundary. Have finite bounded support, but just to make the question sensible, we need to fix a normalization simply because those things are like Poincaré constants and Lipschitz constants will scale with the variance. So, just to fix normalization, it should be isotropic, which means that it has zero expectation and covariance matrix to be the identity. So, for those types of measures, there's a very, very famous There's a very, very famous and well-studied conjecture in convex geometry due to Canal, Novosh, and Shimanovic, the KLS conjecture, which asks whether it is always true that the Poincaré constant of those measures can be bounded by an absolute constant, which does not depend on the specific measure, and in particular it does not depend on the dimension. And this has many far-reaching consequences, both for sampling algorithms and in deeper areas of controversy. In deeper areas of convex geometry. And we're almost there. We're almost there in the sense that the current best bound, which is due to Yuanzi Chen from last year, gives not a dimension-free constant, but a sub-polynomial constant, a constant which improves as a dimension. Well, it does not go, it goes, the power improves as the dimension increases, so it's smaller than any polynomial. Um right so so again it is natural to ask whether we can recover Johan's chance result with a leap sheet smart. So so as opposed to the previous two questions, this is actually an easy question, okay? Because the answer is no. There's a very easy argument because if you would be able to transport the standard Gaussian into any isotropic locon k measure, this would immediately imply that the Concave measures, this would immediately imply that those measures are sub-Gaussian tails, for example. And this is not true in general. There are simple isotropic local concave measures which are done sub-Gaussian. Okay, but there is another point, and this is a result of Immanuel Millman, that if you just want to prove upon Crane equality, then you don't really need to have a Leap Sheets map. It's enough for the map to be LeapSheets on average. It's enough for the map to build leap sheets on average. So you don't need an almost sure bound on the derivative, but you need a derivative when average is small. So a more refined question would be to ask: if you have a measure which is low-concave and isotropic, can you realize it as a push forward by a Lipschitz map such that the the sobolev null of the lip of the map is uh is uh sorry so it's not a Lipschitz map but it's a map with a bounded sorbole of null. It's a map with a bound that's over. Right. Any questions? So I presented three questions and in the form they were presented, I don't know how to answer those. Those are actually open questions which come from different areas or map. But the thing we did realize is that maybe to go back to That maybe to go back to the simple argument of Cordellaskan. So, one thing to realize is that you don't really care about the fact that this map is the optimal transport map. You just care about the fact that it is Lipschitz. But also, you don't really care that it's a map between R D to R D, between the same space to itself. Because you just care about the fact that it is leap sheet and the constant does not depend. Leap sheets and the constant does not depend on the dimension. Right, so if you could transport the Gaussian from R2D to Rd, you would still have the same effect. And what we decided to look at was what would happen if instead of transporting a Gaussian between finite-dimensional spaces, Spaces, we will try to transport a Gaussian measure from an infinite-dimensional space to a finite-dimensional Euclidean space. So in this case, consider the winner space, let's denote it as omega, so it's the space of continuous path with values in Rd with the winner measure, let's denote it by gamma, node the dimension. Gamma, no D for a dimension. And throughout our talk, I'll use Bt to just denote a Brownian motion on this space. And what we will show? We will show that there exists a map which pushes forward the winner measure from the winner space to a target measure in R D with bounded derivatives in those cases of interest. But I should say something about what I mean. I should say something about what I mean by bounded derivatives, right? Because now we are in an infinite-dimensional space, it does not have the same differential structure as Euclidean space, so we should give some meaning to it, and here the meaning will be taken in the Maliovian sense. So I'll say what I mean by a Maliovian derivative in a second, but just to state the theorem. So for any reason that For any reasonable measure on R D, we can realize it as a push forward of the Wiener measure by some map. And in those cases of interest, the map satisfies the necessary inequality we need. So if the measure is more low-concave, sorry, if the measure is low-concave with bounded support, then we get the correct Lipschitz constant. And if the measure is an additive constant, And if the measure is an additive Gaussian mixture and the mixing measure has bounded support, again we get the correct dependence on the correct dependence of the Lipschitz constant. And if mu is low-concavenisotropic, then we get the correct sobolev null for the map. What I want to focus on in this talk is basically the first point, which will be very similar to the second point. Similar to the second point. The third point takes somewhat more technical work to show. But maybe I should just mention that we actually used the proof of Johan's chain of this bound for the KLS conjecture as part of our proof. So we do not give a new proof of this bound. Proof of his bound, we just reinterpret it as an a priori stronger claim, because it seems to imply what they did for transport maps. It is. Yeah. Right, yeah. I tried to reserve gamma for the winner measure and gamma D for the standard Gaussian army. So from now on gamma is the winner measure. We cannot improve this. We cannot improve this. And so we cannot improve it. We cannot improve it because we use his bound and we actually get something which is slightly worse here in the power. Basically, we've collected some extra logarithmic factors. And we actually don't know if they have to be there or not, but to be fair, we actually don't know if they have to be there for the KLS conjecture or not. So we get a Rolls one, maybe you need to get a Wolves one because it is a stronger claim. But maybe one point, even if so, any improvement on the bound for the KLS conjecture will basically give us an improvement, but even Give us an improvement, but even if the Kerala conjecture is proved with an absolute constant, we would still get something like Lobiderated non-degree. So, yeah, that's the extent of this result. But still, some polynomial. Anything else? Okay, so maybe let me spend some minutes to just talk about Valerwind derivative. Baliovian derivatives. So I need to give some sense of derivatives in the wiener space which needs to be compatible with the winner measure. Because everything is done in a measure theoretic sense. So in the wiener space, there is a distinguished subspace, which is called the Cameroon-Martin space. Let's denote it by H. And if the Wiener space is the space of all continuous paths, then Continuous path, then the camera nautic space is the space of all absolutely continuous paths. Okay, path which essentially have a derivative almost everywhere. So if we have such a path h, we'll denote its derivative by h dot. And the reason why this space is important is because it has the following property. If you take the Wiener measure and you translate it by any fixed vector in the Wiener space, you get a new translated measure. The translated measure The translated measure is absolutely continuous with respect to the winner measure if and only if your fixed vector was in the Cameroon-Martin space. Now, if you plan to do some differential calculus, so you take some functional, you want to apply it to the Brownian notion, and then you want to take a derivative, so you need to translate it by some fixed vector. Green is a bad choice here. And then take this string to zero. But then, if those two things live on orthogonal measures, that doesn't make so much sense. So if you want your derivative to be compatible with your measure, you need to restrict yourselves to vectors which live in the Camelo mountain space. And and heuristically at least, the Maliavin derivative is just the directional derivative. Is just the directional derivative of some functional on the Winner space, but when you restrict it to the h directions, only those which interact well with the measure. So this is nice, but another nice thing is that the Winner space is naturally a Banach space. But the Cameroon mounting space, which is densely embedded in it, comes with a natural inner product. It's a Hilbert space. It's just the inner product which is induced. Product which is induced from L2 on the derivatives. Okay? So now to say that something is a Lipschitz map from the Wiener space to Rd, we'll want to look at its derivative. So because the derivative is restricted to the H spaces, and you can think about it as a map from the Wiener space to the Cameroon mountain space, you can measure its length in the Cameroon mountain space with. Length in the Cameroon martin space with this inner product, and then we want to say that a mat is Lipschitz, or it's R Lipschitz, if its derivative is almost surely bounded by this constant, by R. So this might seem like an arbitrary choice if it's the first time you see it, but it is actually justified, and at least for our purposes, it is justified because the winner measure satisfies a Poincaré inequality, which is. A Poincaré inequality with respect to this quantity. So, if you have a map from the Wiener space which has small fluctuation in the sense that the norm of its Malio-Win derivative in the H directions in the H norm is small, then the variance must be small. And because these things satisfy a chain rule, then the same argument of the order we're asking would be would be applied here to push forward a point chain equality to the target measuring RB. Inequality to the target measuring RB. Okay, so this is our working definition. Okay, so how would you build such a map? Probably the first thing to try is we know that a map between R D to R D is leap sheet in many cases when you take the optimal transport map. So it might make sense to try and mimic Caffarelli's. sense to try and mimic Caffarelli's result from 20 years ago for maps from the we which go from the winner's base. But in order to do that we'll need to address two issues. First we'll need to define a Wasserstein metric on the winner space, which means that we need a metric on the winner space, but the metric needs to be compatible with the way we take derivatives. So it shouldn't be just the usual. So, it it shouldn't be just the usual metric on the window space, like the balance metric. So, that's one thing to do. And the second thing to do, we have this measure on R D, we want to take a transport plan in the winner space, so we'll need to embed a measure from R D into the winner space in a way which is easily recoverable later on. Okay, so so let's start with the first point. Um so we first let's define a metric. First, let's define a metric on the window space. This is what we want to optimize couplings over or all mappings later on. So here is a metric, let's denote it with a subscript of h, because it should be compatible with the h directions. And if you get two paths, well, you take their difference, it's a vector space. And if this difference lies in a camera mounting space, then you can measure its norm with respect to the inner product. Its norm with respect to the inner product, and this is the metric. But if it does not lie in the Kamal-Martin space, you put infinity. So now if you want to optimize couplings over this metric, we are basically forcing the couplings to live in the Kamelon mounting space. So they will be supported on functions which have absolute derivatives. Absolute derivatives, and basically I will show it in a minute. Basically, it means that any coupling will be of the form of Brownian motion plus some drift. Because those drifts must live in the Kamer-Martin space. So that addressed the first point. And now, to address the second point, how to embed a measure from R D into the window space, so there are infinitely many ways to do it, but maybe the simplest one to do it is Maybe the simplest one to do it is we have a Brownian motion, say that goes from time 0 to time 1. And what we want to do is to embed the measure at the terminal point of the Brownian motion. What I mean by that is, well, at least intuitively, we want to condition the Brownian motion to have low mu at time 1. This is also called the Brownian bridge. And in formula, if we have a measure μ with a relative density to the Gaussian dμ to d gamma, then we'll re-weigh every path in the Wiener space. So this is the new measure mu tilde, which is just a re-weighing of the Wiener measure, according to what the measure mu does on Rd, and we just look at the terminal point of the Brownian motion. Right? So this immediately gives This immediately gives us that we have some law on the Wiener space. If we project this law at time one, we get the measure minimum. We've embedded it and it is very easy to recover. There is a very simple function to recover, just project the winner measure at time one. Okay, so now let's consider the optimization problem. I'm writing it as a way to optimize over functions, not over couples. Optimize over functions, not over couplings, because there is an optimal map. We want to optimize over all maps which push forward the wiener measure to this lifted measure mu tilde and we want to minimize over this quadratic distance. So, this is the analog of the Wasserstein distance. But because we know that any such map must live in map must live in the Cameroon mountain space, we can rephrase this optimization problem as an optimization problem on the Cameroon mountain space. So every such map must look like some drift and this is exactly the norm of the drift in the Cameroon mountain space. It's the L2 L2 norm of its derivative. And this constraint of pushing forward the wiener measure Forward the wiener measure to this lifted measure simply becomes this constraint. You take the Brownian motion, and at time one, when you integrate over the drift, it must have this slow volume. So this optimization problem and this optimization problem are equivalent. Now the good news and bad news. The good news is that this is this turns out to be pretty easy to solve. Right? Why does it turn out to be pretty easy to solve? Why does it turn out to be pretty easy to solve? Because if we know the optimal map in R D, then because we've lifted the measure from R D to the window space, the optimal map is lifted along with it. Okay? So if this is the map, or rather the drift which minimizes this energy, turn out to be a very boring drift. Lift because it will only depend on the terminal point of the Brownian motion, right? Omega is like an instance of the Brownian motion. And what this drift does, it looks at the terminal point, applies to it the optimal transport map. So this is where it wants to go, and it subtracts the original place it should have landed at. Okay, so it's a very, very boring drift. It's very easy to calculate, but it's not in. Very easy to calculate, but it's not informative, and that's the problem. So that's the bad news. So we managed to calculate it, it's a transform map, it's the optimal transform map, it realizes the Wasserstein distance. But our starting point was that we did not know how to take a Farelli's result, a Farelli's proof, and apply it to those types of measures. And by solving this optimization problem, we've ended up with the same problem. To prove that this is a leap sheet in any sense, Lip sheets in any sense must go by proving that this is lip sheets. So if we couldn't do it in finite-dimensional spaces, we won't be able to do it in infinite-dimensional spaces. So the point is that this is unsatisfactory. So what could be the next step? We had some optimization problem. We solved it. But we worked really hard to make everything compatible with the window space. And somehow this optimization problem is not compatible. This optimization problem is not compatible with the winner's space. Because not only does the winner's space come with this differential structure from the camera mountain space, it also comes with a natural filtration of time. What you get from the Brownian motion, there is an arrow of time which we did not use. And this is the reason why this is such a boring drift, because it allows us to anticipate the future. Instead of considering what the Brownian motion does, it simply looks at the terminal point and does everything with respect to that. To that. So it really doesn't use the Brownian motion. So a way to refine this maybe would be to take the same optimization problem but restrict the admissible drifts we look at. We want them to play nice with the filtration. Which basically means that instead of allowing to look on arbitrary drifts, we will want to look at drifts which are adapted. So the value So the value of the drift at time t can only look at the values of the Brownian motion up to time t. They are not allowed to anticipate the future. And well, once you solve this optimization problem, then by definition this is adapted, it makes sense to write it as a stochastic differential equation and you get some process. Okay, so maybe it's not clear that there is a s unique solution. That there is a unique solution, but in many cases, there is a unique solution. And I'll say something about it in a second, but before I do that, so we get a process. We add this extra condition. So it's not only that we're optimizing over adapted lifts, we add this extra constraint that at time one, when you add it to the Brownian motion, you get the low mu. Which means that if you look at this process at time one, it has low mu. This is what we call the Brownian transport law. It takes the Brownian, it takes the Wiener measure. It takes the winner measure from the winner space transpo like the process transports the to the lifted measure mu tilde. Then when you project it at time one, you get the measure mu. So this is the main property which is important for us. Another property which I won't use at all, but it's nice to know at least that if the optimal drifts, the optimal drift encodes the Wasserstein distance, The Wasserstein distance, this drift actually encodes relative entropy. Okay, so one nice immediate implication of that, because we're optimizing over a smaller set of drifts, necessarily this thing must be larger than the Basserstein distance, which is just a way to rephrase Tallagrand's inequality. And another nice thing is that this optimal adapted rift This optimal adapted rift turns out to be a martingale, which at least is believable because it minimizes some quadratic fall. But not only is it a martingale, it has a very explicit formula. So it's given by the gradient of the logarithm here p is the heat semi-group, so you're taking the relative density of mu and you're convolving it with the Gaussian. And it satisfies some non-trivial. Is some non-trivial relationship with the position of xt, right? It's a random thing, but the thing to keep in mind, we want to prove Lipschitz bounds. So in order to prove Lipschitz bounds, they need to be true almost everywhere. So anything we will want to prove will only depend on the function and not where it's not on the argument xt, which will really simplify all the calculations. So, maybe before I give some of the more technical details, some history on that. So, this is actually one case of a Schrodinger bridge. It actually takes the delta measure at zero to the measure we cared about. So it goes back at least to the 30s, to Schrodinger. But this process itself, at least the formula for entropy, Was first studied by Hans Follmer in the 80s, but it appears and reappears in the literature sometimes, even implicitly in different guises. For example, it appears in the works of Fayelle and Stunel from the early 2000s. They cared about optimal transport in the winter space with no adaptivity condition, but but still it appeared implicitly there when they tried to calculate entropy. Entropy and in the context of functional inequalities, it was pioneered by Joseph Le Heck, which built on the work of Borel. And more recently, La Salle actually identified it as a solution to what he called the causal transportation problem on the winner space. And variations of those ideas appearing in the work of Ronald Dan and Leon Venpala, which led to the work of Joan Di Chen. And the best part for the KLS contractual. Okay, so yeah, before I spend like 10 minutes on very technical details, maybe it's a time to ask whether someone has questions. That's a good question. I I know that I don't know how to answer it because we try to think about the the analog question between if you take and this is something DA will talk about if you if you take the you can take an analog construction uh in finite dimensional spaces and there we don't understand it very well yet. So you can definitely formulate it in this sense, but I don't have anything smart to say about it at this moment. Yeah, but it's definitely a good question. Okay, so let me spend like ten minutes to explain how we can actually prove things here. And really, the only thing And really, the only thing I'm going to use for now is that I have an expression for the drift. So this is like the transfer map. This is where the process terminates at time one. We want to show We want to show that this is Lipschitz. So, what we would like to do is to take derivatives and show that the derivative is small. Now, if you've never seen or did calculations with Maliavini calculus, maybe you can believe two things about it. It's a derivative, so it's additive. So if you want to take a derivative of d thing, you can take separately the derivative of the Bowman motion. The derivative of the Bowman motion, the derivative for the integral, when integral is additive, you can put it inside. Okay? So that's one thing you should believe about it. And the second thing is it's a derivative, so it satisfies a chain rule. Okay, so let's see what those two things mean together. We have the blown emotion. Basically, it is the identity. Basically, it is the identity map on the winner space. When you take the derivative of the identity map, basically you get the identity. Then you can take the derivative inside here. You have a gradient of something. So it's a first derivative. You take a second derivative, well, you get the hessian. But it's the gradient of something which depends on the coven position. So you should also take the derivative of the common position. This is the chain rule. So this will pop out here. Okay? Alright, so this is. Okay? Alright, so this is the derivative at time t, this is the derivative at time s, and you integrate over that. But the point is that now what we can do, let's just give this the suggestive name of gradient of Vt. It's actually a vector of field, right? So it makes sense to take derivative. And what we get here, it's just an integral equation. We have a function of t and we can express it. Of t, and we can express it with the previous function, with the previous value of the function at time s. And from now on, everything I'm going to say will be restricted to dimension one, simply because those calculations are easier in dimension one, neither dimension, you just need to work a bit harder. But everything everything still makes sense in neither dimension. Right, so we want to say that this thing is smaller, it has a smaller norm in the Kami-R-Nardin space. In the Cameroon Nardin space, in the Hilbert space. So, one way to do it would be to heat it with a vector in this space and show that the inner product is small. So, let's fix the vector h. Let's look at the evolution of this torus when you heat it with the vector h. And we want to show that at time 1 it is small for any vector 1. So, we'll define this function. This is the evolution function. And we're basically taking this thing, multiplying it by some fixed vector, taking the inner product in L2. So, here product in L2. So here we just get an integral over the derivative and well here we just get stuff. And now we can take derivatives, it's a one-dimensional thing and this gives us the differential equation for this function. If you just follow the logic you get some differential equation and apparently it has an explicit solution. Okay apparently it's something you learn in the first consequence. Something you learn in the first concept only. And you can write the explicit sol solution in this way. Okay? Something that even Wolfmalpha could do. So this is the solution at time one. But this is true for any vector. And this is true for any H. But we've basically decoupled the dependence on H from something else. H from something else. If this is the inner product with any vector, this means that this is the Madeline derivative. If you want to calculate its norm, well then we simply need to square it and integrate from 0 to 1. So this is the thing we wanted to bind, and this is the way we expressed it. So to show that this thing is small, we need to show that the gradient of the drift is small. Gradient of the drift is small, almost true. So let's see what we have here. It is a Hessian of a logarithm of some integral of the relative density. If you write out exactly what it means and take the derivative inside the integral, okay, you can. Okay, you can say integration by parts three times, and then you get this expression. It is the covariance of some measure, not the original measure, and re-weight it with something which depends on t and you subtract some term of the identity. And what this measure really is, it's the original measure you started out with, but tilted with some Gaussian factor. When this is what you this is the Gaussian curl that you get from the Hitzeming rule. It's any more. So now here is a very, very simple observation. We cared about measures which have bounded support. So if the original measure had bounded support, then the tilted measure must also have a bounded support. But if the tilted measure has bounded support, then its covariance matrix cannot be too large. So we immediately get that if the original measure was supported on a polar produce R, then this is also true for the tilted measure, and the Also, true for the tilted measure, and the covariance matrix is smaller than R squared. This is just plugging it into the basic special model. But here is another easy observation. If the original measure was log concave, then when you tilt it by a Gaussian, you're only making it more log-concave. And as t gets closer to one, it only becomes more and more log concave. But we know But we know that the covariance of a log of a strongly local concave measure cannot be too large. This is basically the Braskam-Plip inequality. So if you work out the calculation, you get a bound of this sort. If the original measure was local, when you plug in everything you know from the Braskamplib inequality, you get it at time t, this is almost truly smaller than 1 over t. Right, so here we have a bound which makes sense when t is close to 0. When t is close to 0, but is useless when t is close to 1. Here we have a bound which makes sense when t is close to 1, but is useless when t is close to 0. And if we combine them together, we get something which makes sense at any time. So if you just combine those two bounds together, you integrate it in this expression over there, this is the exact dependence you will get on the support. Okay? on the support. Okay? And if you run similar calculations for um Gaussian mixtures, you it will give it will basically work out in the same way. Okay? So this proves the first two points of the theorem I had before. The proof of the Sobolev norm is more involved because the nice thing here is that we could forget about the actual path of the process. Path of the process, we just had almost sure bounds. We have a sober of normal, you're going to integrate over things. You need to account for something in the path, and we need to say, okay, bad things can happen, but with very low probability, some of this needs to be taken into account. But maybe two, maybe one other point about this result. So, we didn't really use the fact that mu is log concave. What we really use the fact is that. What we really use, the fact is that when you multiply a measure by a Gaussian, it only becomes more low-concave. So it could be the case that μ is not low-concave to begin with, but still this would make sense, at least for maybe from some T onwards. So the same equality can be extended to account for measures which are semi-non-concave. So you don't really need the potential to be a concave function. The potential to be a concave function, it's enough for it to be semi-concave, which means that the Hessian could be positive, but it will be bounded from above by a positive constant. So this is one extension. Before we finish, let me share one thought. So we've basically demonstrated that there is a LeapSheets map. Map from the window space to finite dimensional space. But this flip sheets map goes through a map from the window space to itself and then it is just projected down at time one. So it also makes sense to ask whether this map is Lipschitz to begin with. And I won't get into the exact details of what I mean by a Lipschitz map from the Winner Space 2 itself, but I just want to note a curious thing which happens here. Thing which happens here. So it turns out that there exists strongly long concave measures for which, when you think about this map as a map from the Wiener space to itself, it is not limited for any constant. So something happens to some path which really expands distances. No, H. No, no, no. In the H, no. But I'm omitting a lot of details of what I mean by lip sheets and the H normal in the winner space. Yes. And this fact is curious. So when you project it down, it becomes lip sheets. Actually, when you project it down at any time, it is lip sheets. But the whole map as a path is not lip sheets. And this is actually in contrast to what happens for the optimal transport map, which is lifted. Lifted from R D, because it turns out to be, at least for strongly local concave measures, the result of Caffarelli is simply lifted to the infinite-dimensional space. So this map, which goes from the minor space to its flat, does turn out to be leap shapes on the level of path, and of course, also when you project it down. So there is a qualitative difference between this adapted map, this optimal transport map, but somehow it is. But somehow it is exactly this difference which allows us to capitalize on it when we project it to finite dimensional spaces. Okay, and maybe just to mention some future directions. So yeah, definitely one question to ask is how far can you extend those questions? Can you go beyond semi-low concavity? Like what's the largest class of measures which can be realized as a reach is push forward? And another natural question to look at is: this was one example of a process in the winner space which gives you a measure, but there are other interesting processes. Can you prove similar or analogous results with respect to Ornstein-Uhlenberg, with respect to Langevin dynamics, with respect to entropic polarization? That there are many, many things to look at. One point which we understand better at this point, and this is what Ir will talk about, is it's nice to have a leap sheets map from the winner space to RD, and it has enough of an application, but there are applications which actually require you to have a map between the same space to itself, or in other words, there are applications which require the existence of an inverse mapping. Right, so can you get a similar result? So, can you get the similar results like the same results for map between R D to R D, and we know how to do it in some cases, but what we don't know how to do, and it's still an interesting question, can you get the same results for the optimal transport map? Because the fact that it is a gradient of a convex function, which means that the Hessian is symmetric, is still interesting. So to be to know that you can realize the same result with a symmetric Hessian is an interesting thing. Symmetric hash is an interesting thing to look at. Okay, so I'll stop here. Thank you. That's all for this very nice presentation. Are there any comments, questions? Simon? Are we in here? So my question is whether if so what is the relation between the the Schrodinger problem and this problem? Because uh it was not quite clear to me because at at one point we say that the velocity Say that the velocity is adapted, so it can depend on the whole path up to XT, right? But then in the end, you say that actually it is just so it is a marking at 3 banks on the next feed. So because the process starts, we initialize the We initialize the process, I write it explicitly, but we initialize the process at time zero to just be zero. So you're really interpolating between zero and some measure, you know, mu at time one. And so in some sense, and you are minimizing basically entropy. And this is why you get this expression for the entropy. So it's related to shorting averages, but to a very restricted case of that, where you're interpolating. Case of that where you're interpolating between a delta measure and some other measure. So what you're saying is that this is basically the Schrodinger problem, but between the zero, the delta is zero and the measurement. Exactly, so it's not a Schrodinger problem between a Gaussian and a measure of. And so you expect to So, get similar results when you get the starting point, which is instead below, which you may prescribe as a question, which can be nuttered, but also some other error. I would definitely expect that. There is like a treatment, I think from two weeks ago, which does a very similar thing, not in the winner space, but for the potentials of the entropical polarization and by And by Chewy and Pouledin, so it seems very reasonable that the same thing would be true. So they do the same thing in the caffarelli context, for strong local concave measures. It's still not clear if you can capitalize on it and go beyond it. But I'm willing to believe that it is true. Thank you very much. So I have a question. So do you have an idea of how this map looks like on RD? Can you kind of picture it? So kind of I mean that like the optimal transfer map is a very ex Transform map is very explicit in a certain sense. For example, one dimension, you can just think of it as monotone rearrangement, as well as high-dimensional branded convex functions. So this kind of very clear intuition what this is doing. So if you just think of R D, do you have an idea what your map is doing? Right, but the domain is still infinite dimensional. So. Yes, in a certain sense, yes, but you still have induced map. So. Right, so it's still getting a transport map. Well, but you d there's no way to get a well, there's no immediate way to get a transport map between finite-dimensional spaces. So so something is inherently infinite-dimensional in this map. Yes? So uh Are uh are you asking whether when you give me a path in the window space can I can I imagine where it will land out and like where the terminal point will end up? There is an explicit uh way to represent this so maybe it's somewhat similar to the case of optimal transport. So optimal transport is given as a solution to some non-linear differential non-linear differential equation, right? And here you can be given as a solution to some stochastic differential equation. This is maybe an analogy. One point is that you can potentially simulate the trajectories of this map. It's an explicit SD, it's adapted, you can take a discretization scheme and see what ways. Scheme and see what you're going to end up. What is not clear is how to actually calculate this vector of the coefficient. Logarithm of some integral, the integral derivatives of that. Maybe if I can jump on Martin's comment here. For example, if you take the gradient, gradient of the log of this ratio, so that's the Ratio. So that's like the that's like the Vashistian gradient of the relative entropy function, right? So that means a geometric picture of which way you're moving. Is there a similar geometric picture to this one, where you are taking a cluster convolution, which is like you're taking a conditional expectation, but you're still trying to somehow flow in the direction of the... Can you realize it as a flow? Exactly. As a gradient is a very important thing. I definitely want to know the answers for that. want to know the answers for that, but they don't know how to answer any. That's something we should. I have another question. So I mean sometimes, I mean, if I think about the way station would you're developing your own program, is that you know, you said, okay, there's the map that's induced by plot transport, and I can understand. Standard and analyze this. In fact, what it does, because it's not anticipated, it's sort of in fact for the way that you compute the derivatives sort of. So if you restrict that this case functionals, I have a nice rule. But if I actually want to put your applications, then what I should be doing is I should ask what is the best Brownian map if you're... Brownian map, if you want, map from Brownian space list, where the optimization cost now is given by the suitable norm of my CMALIP under it, because that's what I'm actually trying to minimize, right? So, okay, so but this is where you keep one fixed application in mind. Yes, yes, yes. So now we have a platform of maps. So now you can tailor it. You can tailor like an optimization. I I mean like like I I I showed two optimization forklens and Of weapons, and both of them are natural in some sense, but definitely those are not the only possibilities and things to optimize on. I completely agree that there is a lot of role to look on natural subspaces to optimize on. And this is analogous to what people do in finite-dimensional spaces, you know, strict. This is completely analogous to the adapted Russell's time distance. Are there any further questions or comments? Well, this does not seem to be the case, so let's uh thank them and all the other speakers from this morning session. And where is the majority of the market?