Thank you, Bruce Demin, for inviting me. I'm really glad to be here. And thank you, everyone, for coming. So, today I would like to talk to you about our recent work on a variant of the CAC bin packing problem. And so, in this variant, we will have time varying item sizes. So, this is a joint work with my PhD student, Igor Hong, at CMU. And tell me if. And Chamin from UW Medicine. So I don't have many slides, so please interrupt me if you have any questions. Otherwise, I might not be able to talk for a whole hour. So, okay, so let's get started. Let's see the problem. So, this is a bin packing problem. So, we have, sorry, let me see. Is this working? Perhaps it's not people can use it. I can't lift to the next slide. Yeah, I can use it. We have items coming into the system over time. And whenever an item comes into the system, it needs to be assigned to a bin. And we have an infinite number of And we have an infinite number of bins here. So each bin has a capacity m, and each item also has a width. So we need to fit the items into the bins. And so this problem, we study this problem because our motivation is to study the job assignment problem in the data center. So in our setting, the items would be jobs. Those are like virtual machine jobs. And those bins are servers. Those bins are servers. So the capacity of a server is like, for example, how many CPUs each server has. And the size of each job will be like how many CPUs it needs. Okay. So in a traditional model, when the job comes in, it has a fixed size, has a fixed resource requirement, and the resource requirement doesn't change over time. And each job will leave after a random time. That's the service time. So for example, That's the service time. So, for example, when this job comes in, we give it to the first bin, and next job comes in, and we assign it to the first server, and the job could depart. So, this is the basic setting, and then the next job comes in, it cannot fit into the first server, so we give it to the second server. Okay, and the goal here is to minimize the number of active servers. So, active server is active if it is currently Active if it is currently serving at least one job. Okay, so in steady state, we will have like a certain number of servers active, and we want to design this assigning policy so the expectation of that active number of servers is minimized. Okay, so in this model, there has been like a lot of work on bin packing and the stochastic versions of bin packing. I'm not giving a complete list. Here, there's just a Here, there's just the line of work that's most relevant to us. And in their work, they designed various algorithms to minimize the number of active servers asymptotically. And I'll talk about the asymptotic regime also a little bit later. Okay, so in our problem, we're going to study a new job model. So in our model, each job doesn't have a fixed resource requirement. Have a fixed resource requirement, but rather a time-varying resource requirement. So then that means, like, for example, when we give a job to the first server, if it's there, then over time it could expand. And once it expands, it could like the total resource requirement on that bin could exceed its capacity. So in this case, we do not kick out the jobs. Out the jobs. Like, we still allow this to happen. It's like temporarily, we are overusing the resource on the server, and that's allowed. However, we cannot do it like too much. So, we do have a cost constraint. So, whenever the total resource requirement on the server exceeds its capacity, we use a cost function to quantify that cost. And we want to make sure the cost is within a certain budget. So, this cost is actually. Budget. So, this cost is actually a read of generating certain costs. If you are in a bad state, you generate costs, and we want that to be within the budget. Okay, so yeah, any questions on the basic setting? I haven't specified several things, but this is the higher level question we wanted to ask. Okay, so then let me tell you why we want to study time-varying job sizes or job resources. Job sizes or job resource requirements. This is because, like, in actual computing systems, usually the resource requirements of jobs indeed vary over time and they vary a lot. So, this is just a simplified cardron picture demonstrating this phenomenon. So, each job here, during its peak hours, it could require like a lot of CPUs. And most of the time, it doesn't require that many CPUs. That many CPUs. So, here we can think of this job like a job usually in a cloud system is a virtual machine. So, although to the system it's a job, but usually it's serving some requests from the users, like it's serving some Viber requests. And those requests have like a time-varying intensity. So, the demand could change over time, and that changes the resource amount of resource needed by each job. And in this case, so. And in this case, so it varies over time, and actually, like the peak cover is short. So, if we provision based on the peak requirement, then that means we will think, okay, now every job needs 10 CPUs and we'll do that like when we assign the jobs. Then we are actually over we are a lot of times we are not utilizing the resources on the servers. So, we are going to have a very low resource utilization, and that ultimately means we will be using. Ultimately, it means we will be using a more number of active servers. Okay, so that's not a good idea. So, in reality, people, what the systems usually do is they overcommit the resources on the servers. So in this case, although we know that during peak hours, they need 10 CPUs, but we just treat them as if they need five CPUs. But if the peak covers of different jobs occur at the same time, we could again. At the same time, we could again like we could have like a possible resource contention. And in that case, we do need to pay the cost. So, our formulation here, the aim of our formulation is to capture this relationship between the utilization and the resource contention cost. Okay. Okay. So, an important thing in our model is how the resource requirement varies over time. So, this is our job model. So, this is a Model. So, this is just a demonstrating example. So, we assume the job could have different phases. So, in this example, it has two phases, L and H. So, during the L phase, it needs a smaller number of resources. So, the item size is smaller. And during the H phase, it's larger. And it transits between those two phases according to a mark of chain. And during while it's in each phase, During or while it's in each phase, it could complete. So then we use this completion phase to denote the completion. So then, like if it transits to the completion phase, then the job is completed and it departs. Okay. So under this markup chain, oh, sorry, I forgot to say this. So when the job arrives, it has an initial distribution. And if it initializes phase H, then we call it a type H job. So then the job type could vary over time. Vary over time. Okay. And then we assume the dynamics of this Markov chain is exogenous. So it doesn't really depend on how you assign the jobs, like whether the job is in the server with a lot of other jobs or whether it's on the server by itself. We assume the dynamics of the Markov chain doesn't get affected. Okay, so because we want to model the exogenous demand. Okay, so now I have the fully specified. Okay, so now I have fully specified our model, and yeah, one more thing: the jobs arrive according to a post-own process. Okay, so any questions on the job model? Usually, the question will come up later because right now, like it seems to make sense, but later on, when we do the analysis, you may recall, like, why do we make those assumptions? Okay, okay, so yeah, let's think about this problem, okay? Um, so. Problem. Okay, so in this problem, we want to do the job assignment. And if we want to look at the state of the system, the state is like how many jobs of each type each server has, right? So then like there could be, because we even assume we have an infinite number of servers, so the state space is very large. So if we just want to directly design a policy, then the state space is kind of too large. And it would be nice if we can do the following thing. If we can reduce If we can reduce the dimensionality by looking at like each server, looking at the system server by server. So we want to look at like, for example, each server, and we wanted to see, we wanted to predict whether this server is suitable for accepting a new job or not. So if we can do that, then we can just evaluate the system server by server, and the space of each server is smaller. Okay, so if we want to do that, So, if we want to do that, then we needed to answer: like, how do how should we evaluate each server? So, for example, here, we have three servers, we have the incoming job. If we put it onto the first server, like we still have one spot left, right? So, it doesn't feel like we are making full utilization of the server. So, perhaps we want to send it to the other two servers, but if we send it to the second server, those those jobs could expand at any time. Could expand at any time. And if either one of them expands, then we are overusing our resource, right? But of course, we don't know the parameters yet, like the parameters of the transitions. So those jobs, they could also leave before they expand. And if that happens, then maybe the server two is a good choice. So basically, we needed to have a way to tell whether each server is good or not. Each server is good or not, and then we needed to connect that back to our performance requirement, which is the expected number of active servers. Okay, okay, so this is very vague. This is the high-level idea. So, what we did is this. We have a framework. So, in this framework, like the infinite server system is our regional system. So, our ultimate goal is to develop policies there. But we are going to also consider. But we are going to also consider policies in a single server system. And we are going to do conversion between those two sets of policies. So we will design a policy for a single server system. And then we are going to convert it back to the original system. And we are going to show how their performances are related. Okay. And the other direction is for any policy in the infinite service system, we can convert it to a policy. Can convert it to a policy in the single-server system. So, then in that sense, if we can lower bound the performance in the single-server system, that serves as a lower bound of the original system. So, we actually did that, but I'm not going to focus on that part in this talk, but we do have a lower bound by going through this direction. So, the direction I wanted to focus more is the other one because it's more instructive, like it tells us how to come up with the actual policy. Up with the actual policy. Okay. Okay. So yeah, let's look at what is the single server system we consider. So as the name suggests, we only have one server, and then we have an infinite supply of all kinds of jobs. Okay. So in this system, the state is just like the current configuration, and the system can decide to request. Decide to request any types of jobs at any number. Okay, so but the goal here is to maximize the throughput of this system. So once you request the jobs, you serve them, they could leave. Okay, so then you wanted to maximize the throughput subject to the same resource contention budget constraint. Okay, so ultimately, we will want each server in the infinite server system to act. The infinite server system to act like a single server system. So, if each single server can have a really large throughput, that means we need fewer servers in the original system. Okay, okay, so this is a single server system. And how do we do the conversion? So, we still need to start from the infinite server system. Okay, so we need the arrival rate to be specified. So, the arrival rate here, we assume this is like still from the example we have to. like still from the example we have two types of jobs low and high and they each have uh they they each has arrived rate r times the lambda sub l or lambda sub h okay so r is a number that we're going to scale up or this is just a recorded picture so r is a number we're going to scale up so the asymptotic regime is as r goes to infinity so basically we consider the system that has a larger and larger That has a larger and larger revolver rate along certain directions. Okay. Okay. So R is our scaling parameter. And now we can talk about the policy in the single server system. So in the single server system, we are given the rev rate R times that vector. Okay, it's a vector. And we want to design the policy such that the throughput times the number n-bar. Times the number n-bar is equal to the arrival rate. So the throughput there is also a vector for both types of jobs. Okay? So basically we are saying, okay, our throughput is some number that is, our throughput is some vector that's proportional to the rivalry vector, but they differ by a scalar n-bar. Okay. And of course, we have the cost of constraint. So what does the n-bar do? So, what does the n bar do? Then, if we can, if we have a policy that has its n bar, okay, then we can come up with the policy in the original system. So the number of active servers is upper bounded by this. So this mbar times one plus a big O term. So the big O term is going to diminish as R goes to infinity. So that means it's just upper bounded by, like roughly speaking, like it's upper bounded by n bar. It's upper bounded by n-bar asymptotically. Okay, and the cost constraint is also satisfied asymptotically. We allow a diminishing gap to the budget. Okay, so then in this sense, if our policy sigma bar is the optimal policy that minimizes n-bar, then let's say the optimal number is n-bar star, then we can, and then like when we convert it back to the original system, Convert it back to the original system, we would have like an n-bar here. And we can show that n-bar is also a lower bound. So then, like, by combining them, we can have an asymptotic optimal policy. Okay. Okay. So, is it clear so far? So, so, do you see all the capacity constraints for the single table system and that person? So, there is a capacity constraint. So, one capacity constraint serves like it will appear in the cost function. Will appear in the cost function. So it's like once the total amount of resource exceeds that capacity, we assume it's going to incur certain cost. And another thing that's a little bit I didn't mention is like we also have a hard constraint saying like there cannot be more than like a certain number of jobs on each server. I think that's reasonable in real systems. You will have an upper bound on the total number of jobs. Yeah. I'm not sure if you say what policy is for single-server too. Can you decide what arrival process you would like? Yeah, so you basically just request your decision is at each moment of time. Anytime you want, I can just ask for a job or whatever. Right, right. You can ask for a job. So I could wait until someone just left and get a replacement for that. Yes, yeah, you can do that. You can do anything you like. You can even request a lot of jobs for no reason. Yeah. And then this project has shown this no-server system and people. Constraint in the single server system depends on the slowing value R? No, the budget constraint does depend on that. We assume the budget constraint is fixed when R goes up. So, how about my policy? I just keep asking for lots of low jobs. You could, like, then they could expand in the future. They could. Yeah, and then you need to make sure. I don't have that choice, so how could I have that choice in the single server system? So, you really. Uh so y y in real system you mean in practice or in the in the infinite server system okay yeah that here's that's like how we convert the policies that's what I'm going to tell you next. Okay, maybe you can put a constraint on so yeah so our policy in the original system will just take whatever single server policy you give me as an oracle and we can just maybe And we can just mimic, we can try to mimic that policy. Okay, so yeah, let me then go to this policy conversion part. How did we do it? Okay, so in the original system, we have a lot of servers. We take each server and we copy the state to a single server system and we run the single server policy. Okay, and then as Bruce said, like the single server policy can do Like the single-server policy can do like a crazy things, like asking for a lot of jobs, okay? But that doesn't matter. So, whatever jobs it requests, we will generate like a generate token of that type. Okay, so and also like you once like once you generate a token on the server, like multiple tokens could be generated at the same time, but once you generated the tokens, that server wouldn't like run the single server policy like until. policy like until like it empties the tokens okay so it will put the tokens there and kind of like freezes a little bit okay so uh yeah so this policy is like okay for every server you kind of look at uh you look at yourself you look at like your self-workload you wanted to say okay uh i actually i think i'm good for another job and you put up a sign like here saying okay i actually want an l job and then like it's there like Up and then, like, it's there, like the sign is there. Okay. And then we do this for every server. Every server has its signs up. Okay. Now, when the real job comes into the system, it looks at all the tokens available. Like if this is an L type job, it looks at all the L tokens and it's going to pick one uniformly at random and goes to that server and removes one token and basically replaces the token with itself. Replaces the token with itself. Okay. Okay. And if there are no tokens available, then we just go to an inactive server. Okay, so we open up a new server and put the new job there. So is the policy clear? Okay. Okay, so then we have, yeah, sorry, this is the question I intended to ask. This is the question I intended to ask. Then, like, we actually wanted to quantify the throughput, like, to the to, so we will through you make use of the tokens to relate the throughput of each server to the number of active servers. Okay, so there's a second part to the algorithm, which in my mind is more or less for technical reasons, but we need them to be there. Okay, so in this part of the algorithm, so first we actually don't run the So, first, we actually don't run the single server system for every server. We just like restrict ourselves to the first n-bar servers. And because anyway, in the end, we will show that the number of active servers is more or less n-bar. So that's like kind of enough. Okay. So for the other servers, we call them. Yeah, sorry. For the other servers, we call them the backup servers. Okay. And n-bar is our target number of servers. We only run the policies there. And another And another point is we won't allow tokens to build up by too much. If the total number of tokens exceeds a certain threshold, square root of R, then we're going to take the overflow token and kind of force it to become a job. But now it's a virtual job. There's no real job responding to it. It's a virtual job. But the virtual job behaves like a real job. It has all the transitions among different phases. Okay. And it could be. Different phases. Okay, and it could depart. Okay. Okay. So those two parts kind of are the regulations we need for the algorithm. And now let me show you like our main proof idea. So at the end, we are going to prove like the number of active servers is like roughly m bar and the number of virtual jobs wouldn't be too much. So they wouldn't affect our results. So yeah, let me show show you our. Yeah, let me show you our main proof ideas. Okay. So the first one is this. So we wanted to, like this first one is the key part, the one where we compare a single server in the regional system with a single server system. And we wanted to show that in steady state, the steady state distribution on each server in the original system as In the original system, is very close to the statistic distribution of the number of configuration on the single-server system. Okay, so how do we do this? Like intuitively, if each token is immediately replaced by a real job, then we have no problem, right? Like they are exactly the same system, okay? But in our problem, it's not like that because the tokens will put the tokens there, we'll wait for a real job to come into the system, and the real To come into the system, and the real job will pick among different tokens. And the real, and also we have this token overflow mechanism that looks at the total number of tokens. If it's too much, we replace them by virtual jobs. So that part is a little bit complicated. Different servers are correlated through the arrival process, through the token replacement process. So how do we overcome this difficulty? So the idea is actually very simple. So, the idea is actually very simple. So, for each type I, let's say we consider the number of type I jobs plus the number of type I virtual jobs plus the number of type I tokens on all the servers. Okay. Then what is the good about our like when we sum them together? What's the good part about it? So, now if we have new arrivals, then the arrivals wouldn't Then the arrivals wouldn't affect this number because what does a revel do? A rebel is going to just replace the token with the real job. Okay? So it moves like this is a sum. So it moves a number from token to a real job. So the rebel process doesn't really affect our K tilde here. And moreover, the overflow mechanism doesn't affect it either. Either, like, yeah, because when we have more tokens, we replace them with virtual jobs. Again, like, we are just replacing tokens with virtual jobs. So that doesn't affect it either. So then like, then it's kind of like in the dynamics of this KTOTA, we don't need to worry about the arrival part, which is the complicated part. Okay. And then we just need to like when we compare this with the single server system, which is like, and we compare it with the number of jobs of type. With the number of jobs of type Y in the single server system. So we only need to compare their departure part and the departure and the requester part. But they are very similar to each other because that's how we designed the policy. If we request a new job in the single server system, we will request a new job in the real system. We will generate a token, right? Like, of course, there's a little bit of subtlety here. Like, when we request... Little bit subtlety here, like when we request new jobs, we look at the number of jobs, and like it's like the number of jobs of virtual and virtual jobs. So they would deviate a little bit by the number of tokens, but the number of tokens is under our control. We won't let it to be too large. Okay. So, okay, so this is the requested part and the departure part also, like it's very similar. If you have, like, because the virtual jobs and the real jobs, they all have like the same dynamics. Dynamics and uh, so, like, except from the token part, they are exactly the same. We can couple them so they have exactly the same departuring and the transitioning processes. Okay, so then when we compare the dynamics, like from one part of the regional system, between one part of the regional system and this single server system, they actually like the difference is small, okay, because the number of tokens is square root of n, so for each server. square root of n so for each service like one over square root of n okay and uh we use the uh so-called science method for queuing systems to like to translate this difference in dynamics into difference uh in the steady state distributions and we can show that uh the the uh the distribution here uh the difference um the the wiser standard distance is upper bounded in this way okay Okay. Okay. So any questions? I'm still holding on to my question. I can ask it again. So suppose that your single server policy only asks for low jobs. It never asks for a high job. Right. Then the real system's got high jobs coming in. And since the single server doesn't make any tokens for highs, those highs will always. For highs, those highs will always be put into new servers, I suppose. Right. That doesn't seem like a very good policy for the so how well it performs would depend on how well the single server policy performs. So in this case, if you only request for all types of jobs, then if you want to make sure the constraint, the budget, the cost constraint is still satisfied, then, oh, yeah, so there are several things. Oh, yeah. So there are several things. One thing is we need the throughput of a single server system to be aligned with the arrival rate of the regional system. So in that case, you wouldn't like just request for one type of jobs. So that's one thing. So half the jobs coming to the big system are low, half are high. In my policy, half the time I have to request a low and half the time high. Right, right, right. Yeah. Yeah. It's. Constraint on a single server. Right, right. If it's not balanced, it's basically kind of constrained. Right. Yeah. Okay. Okay. So this is kind of our main proof idea. And there's a second part to the proof because like this handles the main part, the coupling between a single server system and the single server in the original system. And the other parts will be like the kind of we have those virtual jobs. We have those virtual jobs, and we have servers outside of our range. We have those backup servers. We needed to show that the number of virtual jobs is not too much, and the number of backup servers we use is not too much. And interestingly, those two things can actually be bounded at the same time. Okay, so this is just a recall to how we do the token overflow. If it exceeds square root of r. It exceeds square root of r, we replace them with virtual jobs. And if a job, like if we couldn't find anything within the n-bar, then like we'll send the job to the backup servers. Okay. And this is what we are going to show. Okay. So to quantify those two things, we actually just need to look at the number of tokens in the system. Okay. So the number of tokens in the system, we think of like there's a buffer here and the Buffer here and the size, like the buffer size is square root of r. Okay. And when will we change like the number of tokens in the buffer? Okay. So let's recall how it works. So basically, we will increase the number of tokens if a server requests like type L jobs, right? So we'll do that. That will increase the number of tokens. And what will destroy a token? So basically, if a type So basically, if a type L job arrives, it's going to destroy a type L token. Okay? So then what happens if our number of tokens hits the upper boundary here? That means we have like a square root of our tokens. We have to replace them by virtual jobs, right? So this is when we are going to generate virtual jobs. And when will the number of tools, when will this buffer empty, when does it hit the other end? When does it hit the other end? When it hits the other end, we have no tokens in the system, so we have to send the job to the backup servers. So, this is when we generate a job to the backup servers. Okay, so then like basically we are looking at a random walk here, and we just wanted to quantify the probability of hitting like the two boundaries. And this random walk, like intuitively, is actually very simple because like the arrival rate of type L drops is on the rate of type L jobs is on the order of R and the request rate is roughly the same. So it's very balanced. So then like the probability of in each state will be just one over square root of r. So the probability of hitting either boundaries is also just one over square root of r. Okay, so this is how we handled like those edge cases. Okay. Okay. So yeah, like I said, I don't have many slides, or this is just what I described. Described and we get that bound. So, yeah, so those are all I have. Like, to summarize, we considered a bin packing problem motivated by the job assignment problem in a computing system. And the interesting thing about our model is the resource requirement of each job could change over time. So, then, like, when we assign the jobs, we need to factor that into our decision. And we come up with And we come up with an asymptotic optimal policy. And this is achieved through this policy conversion framework, which we convert between policy in the regional system and the policy in a single server system. Okay. So yeah, this is like our framework. We call it join the recently requesting server because that's like the recently requesting server is the one that's holding up the token. So this is the name of the policy. Okay. The name of the policy. Okay? Okay. So that's all I have. Thank you. Can you describe the Apple policy then for the single server? Okay. So yeah, that's a good question. So we don't have a really good like a closed form description of that policy. So what we did was we relax it into a like a linear programming like based on the state action frequency. State action frequency, and we solve that linear programming and then generate the state action frequency we need, and then basically request the things based on that frequency. So it would be more interesting if we have a policy that can be explained in a more explicit way. Like, for example, maybe it should be related to something like you predict in the future. Something like you predict in the future whether the capacitor is going to be violated or not, and do something based on that. But yeah, right now we don't have a really good explanation of the optimum policy. So for the single server problem, do you have like the congestion cost for that? So is the same. Mean, you you have the same amount of space? The same, sorry. The same buffer space that you have. Right, right, right. And that's fixed as you're letting R go to infinity. Right, that's fixed. Yeah. And it has the same custom model as the server in the original problem. Yeah. And is the policy a stationary Markov policy? Yes, yes, yes. That's a requirement. That's a requirement. Yeah, that's our requirement. But you can, I needed to double-check. I wanted to say you can show that, like, this is because this is for the achievability, right? So we can just design policies in that space. And our lower bound doesn't require that, but they kind of actually match because the lower bound is based on the LP relaxation. So, yeah. But I needed to. Yeah, but I needed to double check. So then the policy you get for the whole system is going to be optimal within a square root of r. Right, right. And the number of servers being busy would be like r plus or minus square root of r yes, yes, yes. The optimal value is on the order of r. What happens if different servers have different capacity? Oh, yeah, that's a good question. Right now, we didn't study that. In our model, they are all homogeneous. But I believe, like, for example, if you have two types of servers, let's not make them all different, but you have two types of servers. I think the framework could generalize because it doesn't really depend. Because it doesn't really depend on what each server looks like, as long as it's the same. Like, when we construct the policy in the single-server system, as long as this is similar to that, it kind of works. So, I believe it could be generalized. Yeah, but yeah. Yeah, I think that's also a good question. We could fix the number of servers and we reject a job. Yeah, let me think. Yeah, I needed to think through what's the relationship between that and the infinite server model. Yeah, I feel like we could have fixed the number of servers to be like the To be like the uh, yeah, no, yeah, let me let me think it through. Yeah, so when you prove the lower bound, is it also sufficient to prove that on like a single server and it somehow transfers a length of it? Right, right, right. It's roughly that, yeah, yeah. And you said this is based on some LP relaxation. Yeah, can you say like a bit more about how the what the LP relaxation is? Yeah, so um. So, um yeah, let me try to recall. So, um for yeah so the LP is like uh um because for every like every copy of the single server here uh so the LP is on the probability of uh in each state whether you can serve uh whether And serve whether your action is. Yeah, sorry, let me try to think it through. So the variables would be like the frequency of being in the state, like in certain state and taking certain action. And the state is just a configuration. And the action is whether you can accept one more job or not. And so then if we want if we want the If we want the ultimate system to have like a certain distribution, stationary distribution on each server, then like on each server, those probabilities, they need to satisfy the balance equations. So the balance equations will be one of the constraints. And that's one constraint. Another constraint is the cost constraint. And I think that's all. Those are the constraints we have, and we minimize. We minimize that, you might want to use randomization. Say, I want to accept a high job of probability. Right, right, yeah. And yeah, we could have that like it could be a randomized policy. Yeah. Is it actually a relaxation or does that exactly describe? It's actually, I think it's a relaxation because it's a relaxation because like those. decision because like those those those constraints they of course they need to be satisfied in the regional system but what else didn't we uh we i think we we didn't yeah let me see i think it should be a relaxation uh in what sense uh why is it not exact right that's the question uh I feel like we must be missing something in the constraint. But what did we miss? Yeah, that's a good question. I will think about it offline. It seems like if you want to reduce the make it get rid of that wherever it opt part suboptimality, the real system would have to take into account sort of the well something. I don't know. Yeah, the square root of r more or less comes from like several minimal constant on the square root of r? I'm not sure. Optimal constant. I'm actually not sure if the square root of r is the optimal gap order. Okay. Yeah. So within r plus or minus square root of r, we could show it. So even square root of r, you don't even know if that's right. See optimal might be r plus 2.3 square root of r or something, but maybe it's 2.3 r to the smaller power. Yeah. Yeah, I think for some for some specific cases, it could be like especially for some degenerate cases. Yeah. Okay, so here, yeah, I guess I'll just steer my hand on like so the fundamental idea is that this is to so that you can only so you only need to work at a single server state and do not even try. So it's kind of like all of these different servers that kind of platform. Right. Right. And then regardless, it's going to be more or less something. Right, right, right. Yeah, yeah. Right. Yeah. So roughly speaking, the regional system behave like our NBAR independent copies of the single server system. Yeah. There could be some strange complementarity or something, but oh. Or something, but oh, what? Yeah, I guess they're these Markov processes. But maybe it's good to have on the large system, have half of your servers just serving a bunch of low jobs and the other half serving a bunch of high jobs. You're kind of assuming that all the servers are kind of homogeneous in a sense. So, actually, in the end, like we were not sure if the single server system only has one recurrent class or not. Recurrent class or not. So it could have different recurrent classes. And to realize that policy will actually divide the servers into like different types. Yeah. We thought it should be just like having one recurrent class, but we couldn't show it. So it would have to be, but you're right. I think having multiple recurrent classes could be better. Okay. Could be better. Okay. Yeah. Can you say what are we currently classing? So, yeah, so basically, like we for the Markov chain, we have several, like you have transient states and some recurrent recurrent recurrent states. And basically, if you have like, so for the for things within the same recurrent states, like you can, you can basically like from any state in the You can basically, like, from any state in the class, you can achieve a single state in that class within a finite amount of time, intuitively. Yeah. And if you have different recurrent classes, then like they are kind of separate. Like for one policy, maybe like the number of you have a lot of L jobs, small number of H jobs. For another policy, it's kind of a different part of the state space and they don't overlap. Overlap. Is this the classic employer in general? Like, for example, if I mouse considering two servers, if I sell in PL3 servers, so if we only have two, yeah, so the pink. Yeah, so the bin packing problem, like usually the definition would be like you can use any number of bins. Right. So then you want to, and there will be different versions, like static versus dynamic. Dynamic basically means like the jobs could leave. And the static one would be like, all those jobs can arrive over time, but they don't leave. And then like no matter how many, like you put them into the bins, they will. Like you put them into the bins, they will be there. And at the end, you count how many bins you have used. So, the difference between those two versions is actually quite interesting. For the static part, the actions are time exchangeable. If you put a job of certain type now into this bin versus you put a future job of the same type into that bin, they kind of lead you to the same number of bins used. Number of bins used. But for the dynamic version, because of the job departures, the actions are no longer time exchangeable. So to me, it seems like this is a big difference. And so for the static part, I think like especially for the static part, for certain scenarios, they could come up with a policy that's within like a constant to the optimal value. The optimal value. So that's why I'm thinking maybe this square root of r may not be the optimal order. And for the dynamic part, like without the time varying, like usually I think they just show that it, I believe the gap is more or less still square root of r, even without the time varying setting. So yeah, I believe that's the case of the order of the. That's the case of the order of the gap, yeah. Okay, thank you very much. Yeah, thank you, everyone.