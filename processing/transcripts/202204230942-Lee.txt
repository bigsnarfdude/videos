I am from Korea and I would appreciate all the organizers and their efforts for organizing this merchandise for shop. And it's really my first time to visit Canada. So I have a very good memory. So yes, thanks for everyone. Now I'm going to talk about the convergence of the degree distribution in general budgetary processes. This work or lots of co-workers do a job with me and Co-workers do a job with me, and some of them are there, Shu Son, and V here. So I'm going to talk about the convergence of the degree distribution. As you know, actually I believe the many systems in reality can be represented as the network. So for example, the power grinder system and the care part. This is the Chile part. Here, this is a Celia paragraph, and this video is from Chite, and the water distribution system, or the protein-protein nutrition network, or any metabolism, and the bacterial system. So, as you know, in our universe, the system is finite-sized because we don't have an infinite-sized system in the reality or nature. So, yes, so it's so natural, but the crucial point, I think. I uh the crucial point I think is to is a discrepancy between the finite size the empirical system and the theoretical models because you don't know you know in the theoretical modeling we always setting that the system will be in the thermodynamic limit I mean the infinite size system so I think the it is very important to the finite name in the model network or model system so uh So I think there are various causes of the network structure changes. For example, the network can grow or network can be destroyed or network maintain itself structures or oscillates between construction or destruction. Or if there is an original original network with full information, we can we may somewhat Somewhat observed the very kind of partial network from the original network. Yes, there are many possible scenarios, but I focus on the kind of the scenario and the destruction of the network. Because I think it is very important because, for example, the power grid system, there is a power grid system, and if one generator is very important to Or very important distribution line is the fail, then the failure can overspread and bits may cause the black like this very big and very phenomenal. So I think it is very important to what system behaves under that kind of the barriers to cope with that kind of the very terrible scenarios. So we focus on the We focus on the this, how the system behaves under the network and loading. So it is based very basic and important investigation to the what system behaves, what systems, structural changes, is degree distribution. It's investigate, yes, is to investigate the degree distribution. So there are very typical times like a degree distribution. Typical times with a degree distribution. The first one is a very broad one, and the other one is a very narrow one. The representative, the Ichi broad and narrow distribution, is a powerful distribution, and the other one is the passing distribution. And the network having that kind of the powerable degree distribution is called the scattering network. And the network having the AUSIN distribution is Eraser-Shivani-Landa network, which is most. Network, which is a mostly randomly organized network. So, my question is: if the original network is a scattering network, then how if we observe the sampled structure of the scattering network, then the sample vector is still scale or it is very kind of the very long-standing question. I mean, the very conventional, so it is not something. Conventional, so it is not something new, but yes, there are two sides, two sides, I think. So from the scatter network, we sample the structure from the scatter network, becomes scatfrey or ER network. And one of our other is the scattering network becomes scattered network. So I think the it is a very, I think it is a very post. I think it is a very post opinion, common opinion in our field. But recently, on the recent year, this paper was published. In this work, the authors insist that the scalping network becomes the toward, goes toward the era network under the suggested load remote processes. So, as I just So, yes, I'm just wondering why they exist so. So I look into this paper. In their work, they start from the Baraba-Shiarbotomodal network, which is a very representative scattering network with the degree gamma 3. So they start from the BAPA model and they suggested three kinds of node removal processes, but I just want to show you the two types of the node removal. To show you the two types of node removal process. The first one is a random removal and the other one is a preference of removal. Random removal is literally that the node is randomly choosed and they are removed in the network. And the preferential removal is a higher degree selected for the removing with a higher probability, like this. And they suffered their insists by. They suffered their insist by evaluating the Kulman-Bibula divergence, which is a so-called relative entropy. The Kulbank-Libula divergence is defined as like this mathematically, then you can understand the meaning of the measure is between the average difference between the two distributions in low-scale. So if the two distributions are very, very different, then the it's Different than the estimated value is very high, and they are very similar, then the estimated value is very low. So, it means that the two distributions are exactly the same, then the relative entropy becomes zero. So, you can see this left column is for the random remover, and the right pedal is for the pre-pressure remover. And the horizontal axis. And the horizontal axis is, you can think of the time. It is a number of the remaining nodes in the neighborhood. And you can see the mean degree becomes decreasing. Okay, is it? Okay, yes, so understandable. And the relative entropy is also decreasing. So they support their incidents by this kind of the figure. So at relative entropy decreasing, it means that. Entropy decreasing, it means that the boss, the network structure becomes to the structure having the bossing distribution. It's the scalp network, I mean the VA model, becomes the earlier network. So we have two arising questions. The first one is why they didn't compare with the power distribution. And the other one is how about The other one is how about the general processes. To answer these questions, we measure the relative entropy compared with the setting model distribution. The setting model distribution, I don't want to explain the detail about the safety model, but you can think the safety model is a more most generalized scattering network. It is kind of the generalized version of the ear network. Of the ER network. So we can obtain the unlinked form of the centimeter distribution. And for the answer to the second question, we suggested and designed the general rule with the one control of parameters. So this is a summary for my talk, so it is time to follow. You can see that we find that there are two regimes. The first one is fossil and the other one is the scattering. The horizontal axis is load removal strategy to be introduced later. And the vertical axis is the pressure of the nose remove. You can see the pink area is the bossing distribution. It means that the network structure is similar with the ear network. And the other one is scattery, and you means that. So now I introduce our setting in this work. We start from the network model. The first one is the setting model and the other one is again. The VA model is used in the previous work. And the setting model is, as I mentioned, it is a very generalized scale-free network. So and we use We use the degree response to 2.5 and 6, and you can see the power distribution like this with the degree exponent to gamma. The VA model is the degree exponent of the VA model is 6 and 3. But in the setting model, we can confirm the degree exponent very continuously. So we use the 2.5 and 3. Yes, it is for the comparison to the previous work. And the smaller degree is probably. And the smaller degree is called to means that there are many harvests in the memory. And now is to introduce the removal strategy. We design the probability, go the removal probability as like this. It is proportional to the k plus 2 d zeta 1 to the power of zeta is minus beta. The k is a degree of one mode, and the close mode is the The plus one is for the distinguishing the removal probability with the degree k equals 0 and k equals 1. And if the view you can continuously control the very parameter data, I just want to show you the three representative cases. The negative setup means kind of the hub preparation. So the higher degree of use, the will be deleted. will be deleted with a higher probability. And the beta equals zero is a random remover and the theta equals one is a common protective remover. So the smaller degree, the node with the smaller degree will be deleted with higher carbon energy. And now we want to measure the entropy. That is pullback milodon, so relative entropy. And as the And as the previous work, we want to use the compare the distribution with the boss distribution. And this is for the autonomy form of the statimodel network. It's quite complicated, but in a very large degree, this distribution follows the power law. And I just want to remind you of the previous thread. Remind you of the previous work. In the tissue BL in 2019, they started from the VA network and they showed two types of the load removal processes and they compared with the AUSM distribution. So it is a kind of a snapshot of our network. The left panel is for the VA start from the VA network and the right panel is start from the static model with degrees connectivity. The setting model with the degree is pointed to gamma 2.5. So the piece beta is a new remote strategy. And you can see, yes, it's very different, looks very different. So I just first simply investigate the degree distribution for each cases. The left panel is for the SATI model, the right panel is for the A model. And the first row is about randomly. Is about the food random removal and the lower panel is for the whole project team remover. You can see the it is a kind of a snapshot, an animation. Obligate them in the one panel, you can see the intercepting model. As Doodle is removed, the degree distribution have each kill is still have they have still the very effective. Are very effective. And yes, and the VA model is also look so still heavy, I think, in the random removal processing. And you can see that at the very heterogeneous network and the very hot protective removal strategy, the degree distribution doesn't look so changing. So we just compare the degree distribution. Figure distribution and its alternative one. The red, left, right, left pattern, left two is for the VA, and the right one is a static model. And it is for the Hubble preferential and the random and the hop of contact remote process. And this point, the point line plot is uh line point plot is uh obtained from our simulation and the dash to line is um Poisson distribution for a given mean degree. Distribution for a given mean degree, and the zone line is also the organic form of the static model network and with a given mean degree and the given exponent. And you can in these two cases, the TGB and O is that the BA network goes towards the L network. It means that the degree distribution follows the RSM distribution. But you can see that here. The network will So network, the data point is obtained our simulation. And you can see the kind of the distribution follows more than the solid line rather than the batch line. It means that our, I think, this network was similar to the scalper network rather than the ER network. And the setting model, the hub protection removal, still very, very, very long time. Very, very long time, they remain their structure at just every so. Now I show you some our results. We compared with our simulation results and with our own authentic approaches. So, this I just briefly introduced the rate equation approaches, but I cannot understand, I cannot I cannot explain the detail because I did not do this job. My colleague does a job for the derived all the processes. So I just want to deliver the concept of the verification of G. So QK tau is the load remover probability for a given loader with degree K with the time step tau and small f is a And small f is a fraction of the remove the nodes. And we, every single step, we remove every single node. So it is related to the kind of the rescue, the type scale. So tau is a type step, and n is the total, a number of the total nodes in the original starting network. And the region of NK tau right now, and n k tau is the number of the surviving nodes with the degree k. The surviving node with the degree K at the time step tau. So in this rate equation, the left-hand side is for the variance variation and the right-hand side, you can see that the first term is for the primary node. It means that the node with the degree k itself is removed. And uh the other two terms is for the secondary effect. It means the uh the node's never is d removed. Neighbor is removed and then my degree also changes. So k equals 1 to k and k to k minus 1. There is that the kind of the effect is described here. So and from using the kind of the AK tau, we can write down the fraction of the survey node with the repeat like this, and using the DFK, we can obtain the The DFK, we can obtain the degree distribution for the acceleration of the model will be removed. So, we just solve this rate equation numerically and I will show the result of the rate equation represented by the dashed line. So, I first analyze the mean degree, investigate the mean degree using very simple measure. Is a very simple measure. So the F is a fraction of the removal of the node, and you can consider it's a kind of the time step. And the horizontal vertical axis reporting to refer different cases of the global removal processes. The left one is for the static and the right one is VA model. As you can see, the mean degree is steadily decreasing, that kind of the thing. But the very straight line is for the random. Very straight line is for the random remover in both panels. And like a concave convex-like curve is for the Hobbit prepression remover. It is quite understandable because the larger degree nodes, I mean the Hover nodes are okay. Hover degree nodes are really very earlier stages. So it's very and this kind of the increasing factory is not surprising. Sector is not surprising because this curve is for the Hubble content remover, and the network is a very heterogeneous network, so the Hubble, the effect of a Hobby is very, very strong. The Hobble still, still, still survives the binary, binary, binary, so it causes the effect of increasing in debris. Now, with time for the K of Quebec livelihood divergence, we compare the We compare our network from the rate equation of our simulation and we compare these distributions with the bulwark or scale grid network. So this panel is the upper panel is for the static model and the below panel is for the WA model. And the first two column is for the beta equal to minus one and the zero and one. And then you can see the various. And then you can see that there is a kind of the crossover point in the Hubble remote preference remote. So there is some features in this result I briefly deliver to you. You can see in most of the cases, the value, the estimation of the person distribution is a word type of very, very large value, larger value. A very, very large value, larger value than for the scapegrain network. It means that the network is very similar to the setting of the scapegoat network rather than neural network. Our major point is we find the kind of the crossover point right now. It occurs when for the negative variables of detail, I mean the Hubble repressory processed. It means that the entropy with the ER network is decreasing or the scalp that relative entropy with the scraper network is also decreasing, but it crossover at some point. It means that at the time the network was similar to the neural network rather than the scraper network. So we find the kind of the crossing point and we And we bit with, yes. So you can see there is more Poisson-like reading and skateboard-like reading. Once again, I want to remind you of the previous work and the T2B et al. They insist that in these two cases, the network. two cases the perfect a network and the theta equal minus one theta equal to zero and they he says that the entropy with the boss distribution is decreasing so the network more similar to your network yes so it is correct but we find that the comparison between comparison with the scale scale network so the entropy value is more and more less than the relative entropy with the real network so we as we compare So, we concluded that their instance is also correct and alright, but we can think the network is more similar to the scale network. It is for the investigation of the giant component side, and you can see here. This is our conclusion. So, we to find how the network is similar to any types of the network. The any types of the network under the various load generator remover strategy. So, in most of the cases, the network was similar to the scammer network rather than the ER network, and which we supported by the value of the relative entropy. And we find that kind of the V diagram. And of course, our work has limitation. I think the limitation is from the V. The limitation is from the we use the K-R divergence, relative divergence, relative entropy. Because the relative entropy is dependent on what we use the reference distribution. We just use the ER network and the static model of scale brain network. But the other types of the network distribution with the bigger distribution can be used here. And our result will be changed, I think. So, and one other So and uh one other uh point is uh this uh uh measure is well defined uh only when the the reference distribution has no geom all the ranges. So to overcome this uh this limitation, uh one can use the one of the various divergences available for the reference distribution also being used. And our future work is uh pretty quality on the system to component. We didn't we just show you the analytical line and our um numerical result, but we want to know the how the near the critical point the system behaves at the criticality. So that's very short. We have time for a couple of quick questions. So, yeah, I have two questions. Thanks for a great talk. I was trying to understand why the results for the VA model and the SCA-3 network study are different. And there are two main things I think which are different between these two things. One is the C gamma value, which was green versus one Y. And another is the degree correlation. I don't know which static network model you use for this, assuming something has more complication. Because let's assume something as a multiplication model, let's say a degree correlation for the static model, but the VF has a limited degree correlation. So, do you know which one causes this difference? And also, actually, I suspect that the degree, say, one and a gamma difference between 2.5 and degree is not that important. So, this degree correlation is important, that's my assumption. Then, is it possible to develop, let's say, the mean Q theory which Let's say the mean field theory which uses the conditional distribution for scenarios. Okay, I try to my best to answer to your question. So in the first one, yes, we only use the V network and the second model with the gamma equal 2.3 by 2.5, but we also perform the setting model with the gamma equal 3. So yes, I just omit this slide, but we can see the in our appendix we the Appendix to be the handle of the GAM micro3. So GAMICO3 is a very in the point of some point of view, they simulate here, they behave like the static model cases, but the other side they behave the degree is pointed together microdegree, I mean the VA network. So, but I think the our the the it is not There is not, I think, the qualitatively, there is no big difference I found it. And to the second question, yes, the VA model has a kind of disruptive structures and the setting model is also, yes, exactly the setting model is organized very as uncorrelated, but you see that the degrees pointed to gamma is lower than degree, is a very high can be. Is a very hot can means that it means that the very high half, but the high, the very large half, it means naturally intrinsically disruptive structures. The degree is 1 to 2.5 or so. So of course in the previous work, the author, I mean the tissue PL, they handle about the degree degradation, but we didn't do actually. I can't not do any exact answer to exact answer to each. Yeah, thanks for the nice talk. I just wanted to follow up the I'm not too familiar with the static model you talked about. Is that basically like price models? Price model. So the price model is the first model in the 60s, which basically precedes the Barbace-Albert model, which allows you to tune the exponent. Ice moment. Okay, well, we can talk about that. Talk about that. The second question I had: if you go back to your last slide, where you had this the future work. Exactly. Well, I think the other one where you show the transitions, the journey. Yeah, this one here. So what are your expectations going to happen? Do you expect just standard continuous phase sensors in one case and explosive percolation in the other case, or do you expect to see something new? We expect that in our future, we want to analyze a bit. Depending on the starting network and the load remover strategy, we expect that the criticality can be changed. I mean, some regions, the system shows the ER-like behavior and or other regions that they show the stick scatter-free network behaviour with that you can you already know though that kind of the behavior can be characterized universally. Behavior can be characterized by universally professed over the system. So, we want to investigate more universal systems. Thank you. So, for the people online, as we said, if you want to ask a question, just keep in or raise your hand and choose. And if not, let's find the speaker again. And our next speaker before the cross-break is going to be Magic Palette. Okay, great. All the time for the topic.