People over the door, so we can't yeah, so we're like how did you like a private sector? Yeah, if you say order your private sector, it's not started. All right, welcome back everyone to joining us this morning. I just wanted to bring your attention to most of today's schedule. That's all I could fit on the board. I'll add more later, but we will have a plenary talk for half an hour this morning and then bring. Plenary talk for half an hour this morning and then break into the groups. Just before the lunch, from 11:30 to 1, there is the group photo, which I think they're going to just collect us here from the lobby and then maybe take us outside and take the photo, and then we'll continue on. There's more group and then the plenary before the coffee break. And then there's more stuff later, but we'll talk about that later. It is my great pleasure to introduce my friend and colleague, Janet Edwards, Dr. Jana Gedwards from the Bee College of New Jersey. The College of New Jersey. She's a full professor there at the Department of Math and Science, and she's going to be giving our first talk for today on model-driven experimental design with applications to cancer immunotherapy. Thank you, Jane. Thank you, Kathleen. Thank you, Renee and Sarah, for the invitation. 8:30 in the morning is not quite my time, but I've tried to wake up for all of you. And if you would like, I don't know what's standard here, but if you want to ask questions throughout, I'm always happy to be interrupted. Throughout, I'm always happy to be interrupted and just make this a conversation. So, we're all here right because of our interest in mathematical oncology, mathematics very broadly defined. My particular motivation, and I think it's one shared with many people, is really trying to understand heterogeneous patient response. Why can you take two people that seem to have the same diagnosis and same treatment plan and they respond so differently to the seemingly same protocol? The seemingly same protocol. And I think we're all here because, again, mathematics, broadly defined, whether we mean mechanistic modeling or machine learning, is a really powerful tool, right, to gain insight into complex biological and clinical phenomena. But, as the theme of this workshop says, right, any of the models we're going to work with are only as useful as the data that goes into them. Bad data in is going to be bad predictions out. Is going to be bad predictions at. So, that in mind, my research program right now sort of has two main strands. One thing I'm very interested in is how can we trust the robustness of predictions that come out of models? And the second direction is, how do we ensure we have the right data to trust our model predictions? And given the theme of the workshop, I'm going to focus on a talk that's going to be the data that goes in. So, the project I'm going to tell you about is really trying to answer. Going to tell you about is really trying to answer the following question. Can we identify the minimal experimental design that we would need to confidently determine parameters in an ordinary differential equation model? So I am coming from the perspective of semi-mechanistic modeling, not the machine learning side. The context in which I'm going to do this in is a model of an immune checkpoint inhibitor. I'll tell you a bit about the biology of that in a little while. The tool we're going to use to answer this question. The tool we're going to use to answer this question is identifiability analysis. I'm going to start with the tool, then tell you the biology, then tell you the method. So, identifiability analysis seeks to answer the following question. Is it possible to uniquely determine model parameters from available data? Structural identifiability asks this question in the context of perfect and noise-free data. So, as a trivial example, I want you to imagine that this is your data here. That this is your data here, and for some reason, the model you were trying to fit to this data had three parameters. It doesn't take any advanced mathematics to know you will never uniquely identify three parameters for data that falls on a line. Any combination of A and B that multiply to 2 will perfectly describe this data. Obviously, this is an easy fix. You reparametrize the model, you let A times B be a new parameter, and then you have unique parameterization. Obviously, spotting this in differential. Obviously, spotting this in differential equation models is not only so obvious, but it is an important tool for knowing if you write if you sort of have unnecessary parameters in your model. But of course you should be asking, well, real data is never perfect. Why does this matter? Well, it is a necessary condition for what we call practical identifiability, which asks the same question, but in the context of real adminoising data. There are multiple ways to assess practical identifiability. Identifiability, we're going to do this using the profile likelihood method. So, if you haven't seen this before, here's what I want you to picture. You have a model with n parameters. You're going to focus on one parameter in that n-dimensional space at a time. You're going to begin by setting sort of a restriction on the value that parameter can take on. And then what you're going to do is pick a parameter value in the domain to focus on. So, you're going to fix one value in that domain, and what you're going to do is, given that is fixed, you're then going to fit. is fixed, you're then going to fit your model, meaning the remaining n minus 1 parameters, to your data. You'll minimize whatever sort of cost or loss function you're interested in. In this talk, we're using a really standard minimizing the sum of the square error normalized by the variance in the data. So what you're going to get is an optimal set of n minus 1 parameters for this fixed parameter, and you're going to keep track of the value of the cost function. You repeat that over the entire You repeat that over the entire, over a discrete set of values in your domain, and what comes out is what we call the profile likelihood curve. Under some assumptions, you can also compute a confidence interval on a parameter. Typically, it's like the 95% confidence interval. So the question then becomes, once you have these profiles, how do you actually interpret them? So the profile I just showed you is the ideal scenario. So why do I say that? There is a clear global minimum in the cost function, so a clear best value. So, a clear best value for this parameter, given the data that you have. If you make the parameter bigger or smaller, right, the cost is going to increase significantly. And then you're going to cross over the 95% confidence threshold on your domain. We're going to call this a practically identifiable parameter. I want you to think of this as you have enough data to determine the value of that parameter. On the extreme other end of the scenario, you can get a profile like this one here where the curve is completely flat. Where the curve is completely flat. So if you think about what this is saying, it says over the entire domain, right, I can equally well describe the data by simply compensating by changing the other parameters. In other words, the data is completely non-informative for this parameter value. We're going to classify this as lacking structural identifiability. And then we have an in-between case where you have structural identifiability, but you lack practical identifiability. Practical identifiability. Our goal is going to be to use identifiability as a tool to inform experimental design. Is it automatically clear here or open yet to how to choose to obtain one of the XS? That's a great question. So similar to when you do a global sensitivity analysis, there is definitely not a science behind this. So there is a bit of playing that you have to do with the parameter, with testing parameter domains. Usually I start. Parameter domains, usually I start very greedy, and then I see how large is my cost function compared to the threshold value, and then you can adjust accordingly. There are some methods that have come out more recently that I've not implemented that actually introduce algorithms to try to find where this curve will cross that threshold to reduce the computation. A patient argument, though, is because it sounds a little bit getting there in a way, but it's not patient. Correct, it's not patient. That is right. Are there other questions? Are there other questions? So if you sort of think ahead to what I said, we need data to do identifiability analysis, but yet my motivation was to use identifiability analysis to say when to collect data. So it sort of sounds like I might have walked myself in a bit of a circle, right? How can I do these two things simultaneously? So what we're going to do is propose a method to handle the seeming contradiction. I'm going to illustrate the following. Contradiction, I'm going to illustrate the full method in the context of an example and then also try to illustrate its broader applicability. So, what I want to do next is move to the context in which we're going to do this. So, we are going to look at an immune checkpoint inhibitor, primerlizumab. For those not familiar with immune checkpoints, I want to do like a one-minute primer on that. So, tumor cells express these PD1 receptors. When these PD1 receptors are bound by their ligand, it sends a signal to the immune system that. It sends a signal to the immune system that that cell should be tolerated by the immune system. The problem is, tumor cells typically express the ligand for this receptor. Therefore, the tumor sends a signal to the immune system that you should not eradicate me. So what Pembrolizumab does is come in and it binds to the PD1 receptor on the T cells. By blocking the binding, the tumor can no longer send the signal to the immune system that I should be tolerated and that should activate an immune response against the tumor. So, if you think about the mode of action of this drug, it is going to depend on the extent to which the drug binds its target in the tumor microenvironment. You need to prevent the tumor from binding to its ligand, its receptor on the T cells. However, when you look at the data that you would typically have in a pre-clinical setting to build a model of such a phenomenon, you would typically only have plasma drug concentration as a function. Plasma drug concentration as a function of time and tumor volume as a function of time. This leads us to our experimental design question. If I am going to make a model that is considering the extent of target occupancy in the tumor microenvironment, I probably need some data on that to be confident in what's happening in middle parts of my model that go between how much drug is in the plasma and the tumor size. So the experimental design question we wanted to ask is the following. We wanted to ask is the following. How many measurements of percent target occupancy in the tumor microenvironment would you need to collect? And when should you collect them so you could confidently parameterize your model and therefore trust its predictions. So that's our experimental design question. Then it comes a step where we have to build a model with enough detail so we can answer this question, but in my opinion, not more detail than that. So what that led us to was a three-compartment. Was a three-compartment site-of-action pharmacokinetic pharmacodynamic model. So I say three compartments. Those compartments are going to be the central compartment that the drug starts in. It can distribute to a peripheral compartment, very sort of standard. And the third compartment, we're going to treat the tumor microenvironment as the third compartment. This is the site of action of the drug. Now, when you think of compartment modeling, typically you think of your compartments as having some fixed volume of distribution. But we're going to have our tumor. But we're going to have our tumor have a dynamically changing volume because, of course, the tumor is going to grow or shrink during the course of treatment. In terms of the pharmacodynamics of the model, so the response of the tumor to the drug, we keep it really simple. The tumor is just going to grow logistically and it's going to be killed as a function of the percent target occupancy of its receptor. So if you want to see the equations, I'll throw them up here. This is all published so you can look at it in more detail, just to sort of summarize what we have. Just to sort of summarize what we have, there are eight differential equations here, which for me is a little big. For other people, this might be very small. This system of eight equations has 20 parameters in it. A lot of it is looking at free drug in the different compartments, free and bound target in the different compartments. There is only one equation describing the tumor. So it's not a complicated tumor model. I just out of curiosity, I would assume that a number of these parameters are not. Of these parameters are not things that you have to identify from your data. That's exactly right. Okay, yeah, so especially the pharmacokinetic. So information that you put in. Exactly, especially a lot of these pharmacokinetic parameters of drug volumes of distribution of these compartments, clearance rates, these are things there's really standard ways to measure. So when I say 20, there are definitely not 20 with a lot of uncertainty in here. Yeah. So next thing we did was parametrize the model. So we calibrated parameters to We calibrated parameters to three different sets of data. We particularly fit to drug concentration as a function of time at a dose of 10 digs per kg at a particular protocol, and also tumor volume at the same dose. And we didn't fit to this, we just sort of looked. There was published data that looked at percent target occupancy in the TME as a function of drug concentration, not time. And we are within the range of the very noisy data they had for that. We didn't fit into that. That they had for that. We didn't think to that. What we then did was held out data at other doses and took a look at the model predictions. So, if we look at a dose of one MIG per cig, we can well predict drug plasma concentration, and we can well predict tumor size at a dose of two MIGs per cig. So, taking this as sort of the calibration and the validation of the model. Yeah, sorry, Naya. When you say you're calibrating the model, is this how it's sitting all the way? No, because the PK. No, because the PK parameters we had good estimates on from the data. So most of the PK parameters are just fixed. So I think this actually gives the number. I think it was eight, I think it was 10 of them. Actually, one of them, nine of them that we were actually fitting, and the other 11 we had reasonable measurements on. Which is still a lot to fit. I'm not, you know. Yeah. How is the carrier opens three measure experiments? How is which measured? How are they measuring? Which is what how are they measuring what? They carry out open source. That's a good question to argue, Rubin. That's a good question, aren't you, Ruby? And do you remember how they measure target occupancy? So they take a baseline of the free target estimate, and then they are doing radio labeling of the radio labeling and then doing the visualization of the percent bomb. Yeah, they wait a standard amount of time and then they look. It's very noisy, that data. See that data. Yeah. All right, so now that we have our model, the next question becomes: we need to take this subset of parameters that we've been talking about, the set of parameters, and identify a subset that we're going to use for further analysis. So here's how we're going to try to get down to a subset of parameters. We're going to, first of all, remove any parameters from consideration that could be easily estimated from data, like our pharmacokinetic parameters. We don't have to worry about those. We don't have to worry about those. We're also going to do a sensitivity analysis to get a sense of which model parameters don't really influence our major outputs that we're interested in. So, here we did a local sensitivity analysis on two outputs. So, we just varied each parameter by 5%, very simple, and looked at the log-fold change in the tumor size and percent target occupancy in the TME. So, the two downstream outputs from giving the drug. And what you can see is there's a number of parameters of the nine that we didn't have values for. The nine that we didn't have values for, that really these outputs are not sensitive to. So, when it came down to it, there were four parameters that sort of stood out to us. And of the four, we're only going to select two of these for further analysis. I'll tell you which ones we selected. So the first one I call K on T. This is the rate of drug target binding in the tumor microenvironment. That was our fourth most sensitive parameter here. And we're going to take the most sensitive parameter as well, which is case. Parameter as well, which is K-synth. This is the rate that the target is synthesized in the TND. So we chose these two for the not only because they're most sensitive, but because they're really things we're not going to be able to measure experimentally. So what do we do now that we've sort of narrowed down our consideration to two parameters? We're going to use these two parameters to generate noisy simulated data of the variable of interest. So what I mean by that is we're going to assume some underlying So, what I mean by that is we're going to assume some underlying distribution on our parameters. We're going to randomly sample independently values of these parameters, and we're going to take this and throw it into our model. Every other parameter is fixed at the calibrated value. The model is then going to make a prediction on what percent target occupancy in the tumor microenvironment will be as a function of time at that parameterization. We were going to do this 10 times to generate 10 simulated data sets. 10 simulated data sets. What I want you to do is think of it, even though it's not, as this being an experimental measurement of percent target occupancy in the TMA as a function of time over 10 replicas. What we're then going to do is despatize the data and imagine in the ideal scenario, someone could collect this data for you daily. Not saying someone could actually do this, but imagine they could. We're going to call this our ideal experimental scenario. So then we're going to ask the following questions. So then we're going to ask the following question. If someone could get this data for you every single day, is that enough information to get the parameters back in? And the parameters say the mean value of these distributions. So we perform our identifiability analysis DAO, and here you see the profile likelihood curves for both parameters. And the important thing to come out of this is both of these have this nice parabolic shape with a very clear global minimum, saying that, yes. Minimum. Saying that yes, if someone could do this experiment for you every single day, no problem. You can get these two parameters back out. Once you've established that, you're ready to ask your experimental design question. That is, what is the minimal amount of data that would preserve this feature, right? Because we're not going to collect this daily, so what's the minimum amount of data we need? And when should that data be collected? So let me sort of summarize what we've done and where we're going. So we started by identifying. So, we started by identifying an experiment that we wanted to inform. That was measuring percent target occupancy in the tumor microenvironment. We then proposed a side-of-action model complex enough to answer the question, but not more complicated. We identified a subset of parameters for further analysis, generated simulated data, ensured practical identifiability. This is an important caveat of this method. If you don't have all identifiable parameters in the ideal, Identifiable parameters in the ideal experimental scenario, you can't keep going because you're not going to be able to identify the parameters. So, your subset, which is why my subset is so small, you need to be able to do that. Then, once you say, okay, if someone can do this for me daily, right, in that ideal scenario, I could get those parameters, now you're ready to ask your experimental design question. So, we started with the following. Suppose you could measure percent target occupancy in the TME once in a month. TME once in a month, and that is it. And we can do it any time in the month. We picked the month because that was the length of the experiments that we had. So if the experiment's done early in the month, the current will be shown in blue. If it's done late in the month, it's going to be shown in yellow. So what you're seeing, let's focus on the right first. So this is the synthesis parameter. The black curve is the profile likelihood curve if you had ideal experimental data, nice and identifiable. The colors are the Identifiable. The colors are the profiles if you only had one day of data. So you can see whether the data is taken very early or very late in bright yellow, you do not have a practically identifiable parameter anymore. One day of data is not enough to learn this parameter value with any confidence. It is even worse if you look at the binding rate. Effectively on this scale, we seem to lack structural identifiability, right? We could equally well describe the data no matter what value that parameter takes on. No matter what value that parameter takes on. So I want to do a question. Just for the likelihood curves on the right side, can you conclude from that that if you at least take it early, it's more identifiable? So like if you have two data points, one should be early. Exactly. And I'm actually going to show you, sort of interesting. I thought that first when I looked at that, and I'm going to show you something interesting that that's actually not quite the right conclusion, even though I thought it as well. Yeah. Yeah, so what we did was sort of follow on that idea, like say you could only do it once, say that was the limitation. What would we recommend? So, what we did was say, let's take all parameterizations we got out of this that fall within the 95% confidence interval. This could be a reasonable parameterization. And what we're going to do is extrapolate the model forward and say, what would the model predict over those parameters? Not for percent target occupancy in the TME, because we don't have that data, for tumor volume, which we do have data for. Have data from. So here you can see the data is shown in the star, and the blue curve is sort of our predictive range of the model if we can only collect the data at day one. And okay, what's good is the data is nice in the middle of our prediction, but you're predicting nothing here, right? Is the tumor eradicated? Is it growing by an order of magnitude? Your model has no predictive ability if you take that data point very early. If you take it in the middle, I was a little surprised by this. A little surprised by this. Now, we sort of miss the data, but not by a huge amount. I mean, it's not awful, but obviously, I don't like missing the data. And if you do it at the end, you capture more of the data, but again, your predictive region is incredibly large. So just to sort of illustrate, right, if you only have one data point, it's hard to sort of trust where the model goes after this. So then we said, okay, what if you could have two dates of data? So there are 30 choose two ways you could design these protocols. Ways you could design these protocols. I could agree. Of course, I have. So, in the right house plot, for example, if you collect it on day 29, you have already five tumor volume measurements. I mean, that's a great question, right? So, the question becomes: how much does changing the tumor volume? Because all those other parameters are fixed from the beginning, right? So, could we go back and Could we go back and recalibrate the tumor parameters based on the findings? Because you'd assume we have 829. Right. Yep, we haven't tried that. That's a good question. That's something we can think about because this is not adapting at all, right? This is all fixed parameterization. So, no, but we can try it. Yeah, parsh. If you go by one mod story, so which is one data point, your parameters are not identified, but would you look at all these parameters? Are these parameters in some sort of a practically identifiable combination, though? Yeah, that's a good question. So, no, it wasn't. Oh, so you're saying, is there a correlation between what's happening here? I have the plots. We looked at it. I don't remember seeing anything, and we threw it in a supplement and stopped talking about it. But I don't remember seeing an interesting correlation there. I'd have to look again. Good question. Okay, so now we're on the two days of data. Of data, and you're going to see therefore 435 profile likelihood curves for each possible two-day protocol compared to, again, the ideal experimental scenario. And there's improvement, right? Two days of data helps. We definitely have structural identifiability there, but this is a little hard to do. So I'm going to take this data and I'm going to show it to you a little bit differently. On the horizontal axis, you're going to see the time at which we're going to do the first experimental measurement, and the vertical is the second. There's nothing in this region. Second. There's nothing in this region because the second has to come after the first. The marker is simply going to indicate how many parameters were identifiable given that experimental protocol. And you're going to notice a bunch of these red and orange markers throughout the space indicating that either none or only one of your parameters are practically identifiable with those protocols. I will note there are six exceptions sort of floating in that space. Kathleen. Sorry, how did you quantify the shape of your proto-like with it? The shape of your likelihood to say that it was. Great question. I asked: do we cross over the 95% confidence threshold on the domain? And I did play with, like, would expanding the domain help that? Is it just a restriction of the domain or not? So there was some extra simulations for that. But that's what the algorithm does. It's say, have I crossed the threshold or not? So what we sort of found was interesting is there's six sort of exceptions that just sort of float in this space. And I give sort of when those experimental measurements should be taken. Sort of when those experimental measurements should be taken. And I'm going to show you the predictions for tumor volume if we were to use those three of those procedures, and it's quite good, right? So we do have a lot of sort of ability to predict forward on volume if you just had those two days of data, which is great. But I'm going to look at this a little closer. And I want to argue that I still don't like this recommendation because it's not robust. So say you pick that protocol, but say day four was a weekend or you ended up sick and you had a Weekend, or you ended up sick and you had to come in on day five, it's gone, right? So we've lost that feature. So we would really not want to make this recommendation given that there's so much uncertainty that goes into these calculations. I don't feel good about a lone point in this space. So we went to three-day protocols. Now it starts to get computationally intensive. We did a random sampling of 28% of them. And here are the profile likelihood terms. Seem to have identifiability here. So what I'm going to do is just Here. So, what I'm going to do is just pull out the identifiable curves here, and it's only 7% of them. So, it's not like this is going to happen very often if you have three days of data. But I want to point out some interesting things about these profiles. First of all, if you look at where the global minimum occurs when you have three days of data compared to the black curve where you have ideal data every day, you're basically going to make the same prediction on what the parameter is. Yes, your 95% confidence interval is a bit larger, but given how much less data you have. But given how much less data you have, that's not really a problem here, right? So I thought, oh, that was really interesting. So this says, okay, maybe three days is the way to go, but it doesn't tell us if it's robust. So what I did next is took these protocols and plotted them in three-dimensional protocol space. And just focus on the blue markers. Those are the ones for which both parameters are identifiable. And we noticed when we plotted this that most of those blue points lie along Most of those blue points lie along a plane with the first experimental measurement needing to be taken at day four, and the second one by day 16. So we're going to project into that space. And what we found is when you do that projection, 90% of the protocols in that space result in practically identifiable parameters, 90 of the ones we had sampled. Not perfect. So what we did is we looked further at this and we ended up identifying this region in the box where everything that we sampled resulted in practice. That we sampled resulted in practically identifiable parameters. So then we focused in on that region and we tested every protocol. And the model does predict that every single protocol in this region will result in practically identifiable parameters. So what we would say is that this is quite a robust design. Yes, you do have this hard restriction of day four, but otherwise you have a lot of freedom. And it also says that you don't have to test every protocol to draw these pieces. Have to test every protocol to draw these conclusions, right? Just doing a random sampling of space led us to this conclusion. Yeah. Just so that I understand it. Yeah. You're talking about if you have sparse data, but not if you have noisy data, right? It's still perfect data. Synthetic perfect data or whatever. Well, okay, so it is noisy data from a model, right? Because the data we're using is simulated data out of our model. But we assume noise in that data by having some assumed underlying distribution. Some assumed underlying distribution on the parameters we're focusing on. But doesn't it depend on how much noise you allow? Oh, absolutely, right? So we did, we did like study how, you know, if you, I mean, the obvious things happening, more noise, this region gets tighter, less noise, this region gets larger, but it still all focuses around this point. But yes, absolutely, it depends on how much noise there is. All right, so sort of to wrap this, we're ready to make our experimental design recommendation. We would say if we want to confidently parameterize this Say, if we want to confidently parameterize this model, we would need percent target accuracy in the tumor microenvironment measured three times over a 30-day period. Again, that hard restriction is you have to schedule to be there on day four. That seems really important at that data point. But otherwise, anywhere between day five and 10 inclusive for day two, second collection will do. And anywhere from day 16 to the end of the month will do for the third data collection. This is the largest region of protocol space we could find for which every We could find for which every protocol in the space results in practically identifiable rounds. So, to wrap this up, right, we proposed a method based on identifiability analysis to really ask the question of how you can get the right data for your model. Cool. This method absolutely requires close communication with your experimental collaborators. You need to know what can be collected, when can be collected, how many times before you could set this up. But the idea here is, right, to try to. idea here is right to try to really both increase the efficiency of the experimental design and the predictive power of our models simultaneously. So let me end by thanking my collaborator who you've already talked to on this, Irena Kareva. I was fortunate enough to spend a year-long spatical working at her company and on the translational quantitative pharmacology team at EMD Serono. The papers published, if you're interested, the QR code is there. Thank you all. There. Thank you all for your time, attention, and questions. I appreciate it. Michael? Beautiful as always. I do have two questions. So, your tumor bulky data finished lecturer date 26. What are the implications of data after the GMO one experiments? I mean you're asking the right question. I don't have a good answer to it. Irina, do you? What are you trying to achieve? So sorry. What are you trying to achieve? Understanding your question. As you like in trying to make it produce, put you more volume of a time. For tumor volume over time. But if your sample from the tumor microlibrate after the last tumor volume has been collected, sure, then everything is identifiable. Right, right. So how much could we do with less information? So although everything after time was 16 might be identifiable, it's not all useful. Yeah, so what we were doing is saying, like, these are two easy things. We're doing is saying, like, these are two easy data sets to get, right? So, you can measure easily plasma drug concentration and tumor volume, and we're trying to use that to constrain the parameterization and say, can you get the hard thing in the middle? So, we weren't really asking the question of sort of adapting as new data came in, which I still think is the question around. No, so if I could, please, in terms of what we're trying to achieve, one of the things that I get asked, for example, if we're doing experiments. For example, if we're doing experimental design, they can say we have money or we have resources to do only bits and mouse experiments, a little bit easier, but still kind of labor-intensive and expensive to do collections here. The goal would be to then extrapolate to biopsies. If I can only have two biopsy measurements when I'm proposing something for a clinical trial or trying to make any decisions, what is the most strategic time that I can ask for those? And if I'm asking for three, I better have a really, really good reason to do that. Have a really, really good reason to do that, which is I will be able to, I don't know, predict your trajectory better. So that was in terms of resource allocation. Second thing, which I kind of really love about this, is the parameters that we focused on here, there is no freaking way to measure them any other way. There is no experimental design to measure the synthesis rate of the target, for example, with two microenvironment. But this from actually measurable data. Measurable data allows you to get an estimate for that. And when I'm doing translational modeling, I'm taking, oh, this is a bus model, and now I need to do some magic and predict how it's going to be in people. Well, I have something to actually extrapolate from and to hang my hat on. That, hmm, this is at least I get a sense of the value in animals and then we can do translational things. So these would be the questions that would be answered with this. But I feel like. But I feel Heiko's question is going into if pissing on a certain value or you know that it's not available, a scanned or whatever, it's not available before or after, which is also a reality. What is the utility that can we modify the utility? I'll say there's other work, so hiring Cho along with other people. Here, were you involved in that work before? Okay. Okay, other people who have worked on like questions of if I take like an image right now, when should I take the next one? Right, and I do think that is a clinical question. We are more focused at the pre-clinical level of being able to parametrize this model so you can make a first and human dose projection with some confidence. So, ideally, right, thinking about how to expand this to having real-time clinical data is an interesting question, but it wasn't actually what motivated this originally. It was increasing. This originally. It was increasing confidence in the pre-clinical prediction to make a first-in-human protection. Sorry. It's just really fascinating. The other thing that I would probably look at, and maybe all we did, if she did a practical identifiable analysis, like the 10 parameters, even though some of them are not sensitive to outcome of interest. Outcome of interest. Because you can sometimes get at you can sometimes narrow down your field of, you know, you're trying to do optimal, you're trying to optimize experimental design. And if you look at if some of your parameters have a flat line or identifiable from the left or from the right, you could sort of already pre-guess maybe that's the sort of data, like whatever biological branch of data I'm designed by. Biological data design biological data. We did only look at the four parameters that were most sensitive with any detail. Truthfully, the rest. Yeah, so we didn't try to go beyond a subset where we did have identifiability with ideal data. So we didn't look any further at sort of what else we would need there. But even like looking at, oh, I think it's quite interesting that Brownos didn't have a combination. That's quite. I want to look at that again, though. I want to check that. I want to look at that again, though. I want to check that data tomorrow. What special go-time date was? That's a great question. Not only it also showed up in the two-day protocols, too. What I would say is it must be enough time for the drug to actually have moved through its compartments and get to its site of action and started to have some effect. Right? So, if you're giving the first, let's call it day zero, right? It's injected, it goes into the central compartment, it moves between compartments, so it probably takes some time. Between compartments, so it probably takes some time until you see anything. So, my guess is four is the first sort of interesting day where you see a little bit happening. But then, day five is too late. Day five, yeah, it's come down too much. Well, so actually, to refine on that, this is a feature of the particular experimental protocol that was there, because they were giving, I think, every three days. So, you give two doses, and this is after you give the second dose. And so, now you know what happens in terms of our distribution. You know what happens in terms of how it's treatments after the first dose, after the second. So, that was that is a question that comes up. Can you look at the drug concentration for the compartments to identify a priority, like what days? Like if you know you're giving drug day one, two, and then there's a gap, whatever, seven, eight, and then there's a gap conference. You can look at through the PK what your drug profile looks like. There's a peak on day four, so. There's a peak on day four, yeah. It was really, yeah, we can. I mean, certainly, so we do have the PK curves, right? Um, and what we definitely see is right, so you get this spike at day, you know, get the spike, and this is PK in the plasma, it comes down, go up again at day three. So, at day four, you're just starting to come down from that second spike in the plasma. You're just starting to come down. And it's going to be proportional, right, with a delay, right? Can I ask a question from the hi, Norda? Hi, Adorda. Beautiful talk, really beautiful. I have a question about the sensitivity of the results, the robust results, to things such as how the confidence interval is computed. For example, there's some Bayesian assumptions that are hidden in the way that one computes the confidence interval. Also, the threshold is set to 95%, could be set to 96% to 94, whatever. 