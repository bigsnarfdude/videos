And it would really kind of blur different processes going on in these different individuals. So, by analyzing the subject at their specific time point and getting more accurate measures of their functional brain activity, we were able to discover these longitudinal trajectories that hadn't been discovered before of this inverted U-shape with hyperactivation followed by hypoactivation. And so, if we were to have averaged across subjects at a Have average across subjects at a particular point in time, we wouldn't have those hypo and hyperactivations canceled out, and we wouldn't be able to see this kind of nuanced pattern going on in this disease. And then finally, I would say the kind of golden egg is biomarker development, clinical translation, as well as pre-surgical planning. So, in cases, we really do care about the individual as an individual, not just for the advancement of discovery science. But I would argue that in general, But I would argue that in general, caring about the accuracy of functional brain features in individuals matters across the spectrum of research and clinical care. Okay, so when we say precision functional neuroimaging, what do we really mean by that? And I think when I would argue that we should mean something different, what we often mean, or what the field often means. So if we look up the definition of precision in the dictionary, there's definition, there's part of the definition that says it's the refinement Part of the definition that says it's the refinement in a measurement, calculation, or specification. So, precision functional neuroimaging simply means that we are obtaining accurate, precise measures of functional brain features. And so how can we do that? What contributes to the precision of functional brain measures in individuals? So the most common factor that people really think about is the time in the scanner. So when we talk about precision functional imaging, we often think about studies collected, say, five hours, 10 hours of rest. Say five hours, 10 hours of resting state fMRI or other measures on individuals. That's only one factor, and it may not even be a necessary factor, is what I'm trying to argue in this talk. Another factor is the quality of the acquisition or processing. This certainly plays an important role. For example, the move towards multi-echo imaging is a big, is a major advance in terms of signal-to-noise. But also, as many people in this workshop have been talking about, better statistical models. Can we account for? Statistical models? Can we account for the actual structure and the data rather than drastic simplifying assumptions that lead to loss of efficiency, loss of power? What I'm going to talk about here is actually the use of auxiliary information. And this has been done in the past in terms of, say, using brain structure to inform analyses of functional connectivity. But I'm actually here focusing on a different source of auxiliary information, which is other individuals in a patient. This is actually In a quotation. This is actually quite natural from a statistical perspective because we often do this in hierarchical Bayesian models, right? So we fit subjects together in a hierarchical Bayesian model, and our subject level estimates represent sort of shrinkage towards the population average. But here I'm taking a slightly different approach, which is the use of empirical population priors in single subject analysis. Okay, so why should we consider the use of empirical population priors? Okay, so compared with traditional hierarchical Okay, so compared with traditional hierarchical models, which are a natural approach to estimating both single subject and group average quantities of interest, the use of empirical population priors provides a very fast model estimation. This is hard to overstate. It's much, much faster. It also allows us to make complex model formations so we can incorporate additional sources of information than would be feasible in a hierarchical model simply computationally. Simply computational name. These models are also applicable to new subjects without having to refit an entire hierarchical model. They can be clinically applicable because they can be fit to a single object. And they're also practical for a range of sample sizes. So hierarchical abayesian models have kind of a limitation in high-dimensional data, which is that if the sample size is too small, then you don't have enough information to estimate the group population parameters, right? But if your sample size is too large, you can't put But if your sample size is too large, you can't fit the model. So, hierarchical Bayesian models are practically limited to moderate sample sizes in neuroimaging. But this approach is practical for all sample sizes from the smallest to the largest. Okay, this has been done in a number of contexts in fMRI already. So, in terms of functional partialation, the paper I showed earlier by Ruby Kong and colleagues, as well as an earlier paper that I worked on with some colleagues, including Martin. Some colleagues and including Martin. Functional connectivity. We had some previous work using shrinkage estimators or functional connectivity, which is kind of another way to frame the same problem. Also some work by Rahman colleagues in prediction. Martin has a paper from back in 2017 on using group kind of population priors in a prediction, expand prediction. And then what I'll talk about is in the use of independent component analysis in the context of independent component analysis. In the context of independent analysis, I'll be talking about a couple of papers that we published, 2020, 2022, but I'll also be talking about some more recent work. And then I really think the possibilities are quite limitless here. So I think that empirical population priors could be in basically any context in fMRI analysis where a higher cultivation model could be used. And so I haven't really seen that being done yet in, say, task activation or network analysis. I'm not to say it hasn't been done, but I'm not. Not to say it hasn't been done, but I'm not aware of the work. And I think that there are a lot more possibilities that could be pursued. Okay, so I'll be talking about ICA. So just a really, really brief primer into how ICA is used in functional connectivity in fMRI. And also, I tend to run long, so please keep me warnings. Okay, so ICA is used to decompose the fMRI data into a temporal mixing matrix A and a spatial independent. And a spatial independent component matrix S. And there's a residual here, but I'm just ignoring it for the purpose of this slide. So these spatial, these rows of S contain maps that represent functional brain topology. And the columns of A can be correlated or used some other measure of functional connectivity, study the functional connectivity between the regions represented in S. Okay, so it's you get functional connectivity and spatial topography in kind of one. Topography in kind of one with one tool. Okay, so what we're proposing is you doing ICA, or what we have proposed is using empirical priors in this ICA. So we're basically fitting a hierarchical ICA model, but with known priors at the group level. So we take the subject level data. This wonder doesn't work. Maybe it does. No. Okay, so we take the subject level data. We want to model it as this product of the temporal. Um, this product of the temporal mixing matrix A and the spatial signals S plus a residual. And we are assuming that S can be represented as a deviation from the known group average, SO, where those deviations have a known variance. Okay, so there's a number of kind of the doubles of the details. This is an extremely simple model, but there's a number of things you have to do, you know, to There's a number of things we have to do to get this to work. The first is pre-processing. So we have to remove additional components. So we assume what we call template components, but there can be additional components in the data, like such as noise components. So we want to remove those beforehand. In the for 2020, we also considered estimation of those simultaneously using a mixture of Gaussians that actually are worse than just simply pre-processing. Processing the data to remove those components, and it was much, much slower. We also do some kind of pre-whitening, in a sense, to standardize the variance spatially. And then we also do dimension reduction. Sorry? Well, so, so you're getting into the this is so we are not currently doing pre-whitening to remove temperature correlation, which we should be doing. So, that's something that we need to add in essentially to the software. We have the software to do it, we just need to add it. The software to do it, we just need to add it. What we do do is standardize the sigma-to-noise ratio so that we can assume homogeneous variance with some level of being reasonable. And also sometimes pre-whitening is used to refer to dimension reduction in ICA, which I don't love. So I just call that part dimension reduction. So that's not what I mean by pre-whitening. Okay, and then we have to estimate this empirical prior. So I'll get into that. Estimate this empirical prior, so I'll get into that in a moment. But in terms of model estimation, if we fix the what we're considering, the known population prior, we just need to estimate the latent variables, which are our S's, as well as the model parameters, which we're considering the temporal matrix and the noise variance. So, in order to estimate the empirical prior, here's the approach that we took. So, we use a set of existing group average IDs. So, that can be from Human Connect In Project or you're a collection. Connect them project, or your collaborator estimates those. We then use existing ad hoc methods, which we're going to be later compared to, to obtain a set of noisy, repeated measures of the ICs for a set of training subjects. So you can see like this ad hoc measure produces quite noisy estimates. These, you know, clearly have a lot of noise in them. And then we can use something like ANOVA to estimate the mean as well as the between subject variants. So this basically compresses the This basically compresses the empirical prior model. So, using the HTTP, here's two different ICs. So, we did this with six different ICs. You can see the first one represents a default mode network component. The second one represents an attention network component. And where the mean is higher, we tend to see that the variance is higher as well. So, those hotspots representing areas where subjects tend to be engaged in this particular network are also where the subjects tend to vary a lot. So, this model is going to allow. A lot. So, this model is going to allow subjects to vary more in the areas where they tend to differ from each other, and it's going to perform more shrinkage in the background. But because the model is additive across the ICs, that shrinkage in the background for one component is actually going to help us in terms of estimation of areas of engagement in other components. It's kind of a targeted shrinkage. Okay, so some results of this from our previous papers compared with this. Papers, compared with this ad hoc method, the regression, which is used quite commonly in practice, we get a lot more kind of clean-looking components, but they're also a lot more reliable. So here I'm showing ICC at every vertex. And if we were to kind of summarize that, we go from about 0.135 CC to 0.429 CC for this particular component. So still not a great ICC, but it's protect-wise. So it's kind of a high bar. And if we Of a high bar. And if we compare with dual regression that ad hoc method, when we vary scan duration from five minutes to 15 minutes, we improve quite a bit at every duration, but actually we do better at five minutes than dual regression does with 15 minutes. And this is why I want to argue that it's not just about increasing scan duration. It's about using all the information that we have available to us. And we can actually potentially save scan time. Can actually potentially save scan time, save a lot of money, and extend the applicability to many different contexts where we can't do long scans. Okay, and then also recently we've been looking at can we produce what is considered precision functional topography with much, much less data. So, this is a figure from the Midnight Scan Club. They collected five hours of resting state data and they did a subject-level parcelation. And they were just showing in this figure how the subject-level partial Showing in this figure how the subject level partulations, particularly if you look kind of at these black islands, really differed from the group average at the top. And so they were basically saying that, you know, they need to be respecting this individual functional topography. So we applied template ICA. We kind of tried doing ICA, we're not doing parcelation, but we tried to kind of match up some of these parcels to ICs in our ICA. So we have this one IC that represents the salient. Have this one IC that represents the salience network, and that black island also represents a salience parcel. And what we saw when we applied template IC to a single session and then did some inference to threshold those maps that we got a very, very similar area to what they got from 10 sessions, which is the five hours of data. So, based on 30 minutes of data, we can get something that's very, very similar. And by increasing that to two sessions, which is an hour, Um, two sessions, which is an hour, or three sessions, which is which is an hour and a half, we really don't see much change. And so it looks like, and we need to kind of continue this line of work to kind of quantify this, but it looks like we can get really stable estimates of these functional areas with a single session of data. Okay, then as I mentioned earlier, one of the advantages of this approach is that we can extend the model to build more complex model formulations because not really. Model formulations because not really constrained by as much by computational considerations, we can really incorporate multiple sources of information into the model. And so, one of the ways we did that was by incorporating surface-based spatial priors. And so I'm showing here an image of one lateral side of the cortical surface. You can see there's, I don't know if you can see, but there's triangles. So this is actually represented as a triangular mesh. And so we've done some work using stochastic partial differential equation priors that are built. Equation priors that are built on triangular meshes in order to incorporate the spatial dependence along the surface. So, this spatial dependence is basically inversely related to, or the prior and the prior is inversely related to the geodesics along the surface. And so this is basically the same model as I wrote earlier, but now I'm vectorizing the prior on S. And so that the covariance structure of that vector actually has two components. Actually, it has two components. This D is from the prior that I already estimated. This is basically the variance, so the standard deviation. And then we have the correlation matrix. So the correlation matrix is actually what I'm presenting as an SPDE process prior, but with unit variance. So I only have a single parameter, whereas SPDE priors have two parameters, one controlling the scale and one controlling the correlation range. So now I have to estimate kappa as well. So we also did this with EM. With EM. I'm not gonna lie, it's not very fast. Probably went from one minute per model estimation to about 12 hours. And that was after a lot of work with a lot of linear algebra tricks. And we had to use a numerical optimization for Kaupa here, but it does work. And if you're patient enough, you can use it. I'm not. So I haven't used it. I haven't used it much in practice, but it does have some pretty nice advantages if someone really cares about it. If someone really cares about having maximum power, for example. So here I'm comparing the spatial model to the non-spatial model. And I don't know if you can tell, but the areas of activation are quite a bit larger when we do difference. And that's largely driven by the reduction in the variances. And then we just did a small reliability study on this, and we found that the scan-weight correlation was quite a bit higher with spatial template, what we're calling spatial template ICA when we increase the spatial priors. When we increase the spatial priors, especially the shorter scan. This is five minutes of data, and this is 15 minutes of data. So if you have more data, they tend to kind of even out in terms of accuracy of these spatial maps. But if you have less data, using the spatial prior helps a lot. So that's nice because less data, also the model runs faster. So, but if you have a lot of data, then maybe you don't really need it. We also can actually identify with this model the deviation. Identify with its model the deviation, right? So, in the model, we have this deviation term that represents how the subject differs from the average. And that deviation might actually also be even a more scientific interest than the maps themselves, right? Because it's representing this subject is over-engaged or this subject is under-engaged in this particular network. So, these, you can see, but in the spatial model, these are a lot bigger, these areas are a lot bigger than the non-spatial model. So, that just reflects increased. Non-spatial model. So that just reflects increased power we have to detect those deviations. And then this reliability study showed that overlap of those deviations from test to retest was substantially higher with the spatial model. Even when we can kind of do the dice overlap, which basically it's harder when you have the spatial model because the areas are larger. So you're expanding the areas. And so it's a kind of a harder problem to have good overlap. Okay, so the main thing I want to talk about in the five minutes, okay, is our new work, our ongoing work, which is working to incorporate population information, functional connectivity side of the model. And so remember this slide from earlier, we said this ICA gives us functional topography and functional connectivity. Turns out, though, this functional connectivity is often what people are primarily interested in. I think in part because these maps have I think in part because these maps have kind of been ridge with dual regression for a long time, and they don't, they can't really be, there's no difference possible with like dual regression. So I'm hoping that the field will move more towards you this information, but functional connectivity is certainly an important part of what's produced with ICA. So we said, okay, we do want to try to do this. So we are now building kind of a closer to closer to full-base. Closer to a full Bayesian model on this, although there's still kind of an empirical Bayesian flavor, where A now, the mixing matrix, is also considered to be a random variable. So A here is modeled as alpha plus B, where B follows a multi-normal with a covariance matrix G. That covariance matrix G follows an inverse Wishrup prior, where we estimate a priori at the parameters of the inverse Wishru prior. So that now forms part of the Now it forms pure priors. So, and we added some more priors here. I made a prior like on the noise variance. So, we kind of extended the model to be a little bit closer to a fully model. And we can try to estimate this with EM, but now my latent variables are A and S, right? So, now this is going to be a little bit more difficult, maybe a lot more difficult. And my model parameters are the north variance, and these two parameters on the temperature. Two parameters on the temporal prior. So, how can we do this? So, how do we estimate the prior? I'll just mention that very briefly. So, we take a sort of approach where we take kind of ANOVA to separate out the sources of variance. But we actually are trying to be more conservative here. We don't want to overly constrain the functional connectivity. We know it's dynamic. And so we actually use the total variance rather than trying to separate the variance. Variants rather than trying to separate the variants to allow for within subject ranges. So, from test to retest, we don't want to consider that nuisance. We want to actually allow for those, allow for that level of variability in functional connectivity. And then we just use like method moments and to make the inverse Witchert parameter new such that the inverse variance is larger than the imperial variance for all functional connectivity pairs. So, the inverse Witcher variance is tied to the mean, so we don't have. mean so we don't have kind of the level of flexibility we would like in order to to um produce the kind of um the empirical variance so instead we kind of inflate the variance so that the um so that we never underestimate the variance for any of the functional conditive pairs okay so we're currently testing two approaches for model estimation the first is a gibbs within and i really appreciate the um any feedback on this from the from the beijing contingent The Bayesian contingent. So we have two approaches that we're currently working with. So Gibbs within Baym. So we iteratively obtain MAPs of model parameters, the noise variance, alpha and G, and the conditional posterior moment of the variables S and A. And that's the hard part because at the E step, we need to simple from the joint posterior. We need to estimate some quantities involving the joint posterior S and A. So the way we're doing that is through. So the way we're doing that is through Gibbs sampling. And so because it's this Gibbs with NEM, it's quite slow, even though we've programmed it in C. And we run this until it converts with the expected log likelihood. Okay, the second step that we're second approach we're working with is a regional base. And so we assume the posterior factorizes over these five components. We run that to a conversion of the evidence lower bound. Okay, so I'm not going to go into simulation study setup, although I Setup, although I think it's pretty cool because we use some real data. But I can talk about that offline if anyone's interested. But I do want to get to just some results. So these are some deviation maps that we generated. So this is the true deviation map. This is the template ICI estimate, the first model I talked about. And this is the new model. So interestingly, although we changed anything about the empirical prior on the spatial maps, we actually get cleaner estimates through this. Estimates through this additional set of information with the wires on the functional connectivity. And then, if we look at the mean scored error, it is lower generally with this new model. So the VB here, the EM here, but the VB does a lot worse in the actual areas where we'd like it to do better. So it's not doing as well as we'd like it to. The EM does quite well, but it's quite slow. And then in terms of the functional connectivity, this is what we're really pleased about. So it turns out template ICA actually does worse. It turns out template ICA actually does worse, so our earlier model actually does worse than ad hoc model in terms of functional connectivity. So that's not great because if this is what people care about, then we need to not do a worse job of the, you know, compared with ad hoc. So a new model actually does better than both progression and our previous model. And the VV actually does well in terms of the functional connectivity estimation. The EM does a little better in the areas of stronger activity. Okay, so just to summarize. Okay, so just to summarize, because I believe I'm out of time, compared with popular ad hiks that are used in practice, the use of piracle population priors and subject ICA, we've provided more accurate individual level functional topography, more accurate individual level functional connectivity estimates. And also we can do input. So this is something that by taking a statistical approach, we get these nice guarantees of estimation and efficiency, but also we can. But also to be inverse, which is, I think, a big advance over the ad hoc. Several challenges still remain for this ongoing work. So the Gibbs within AGM approach performs very well, but I don't think it's going to be scalable to real data. So this takes over an hour in simulated data really to converge to kind of a high standard. So the convergence right now is set to like 10 to the negative six. So maybe we could relax that a little bit, but it really does. We could relax that a little bit, but it really does take a long time to do the Gibbs samples at every step because these are high-dimensional objects. Could we somehow refine the variational Bayes approach to improve the estimation of the spatial maps? That would be nice because it's super fast. It takes about 20 seconds in simulated data. How can we consider dynamic functional connectivity? So we're printing this prior on functional connectivity, kind of assuming that it's, you know, that a single distribution is the right representation of the population variability. Representation of the population variability. We know that it's not. We know that it's dynamic and that there are different functional kinds of states we should consider. And then finally, we really are starting to work towards showing the utility of this for biomarker development, specifically in Alzheimer's disease. You know, we're getting a lot of features at this point. So how can we use these features to advance biomarker development in different contexts? Okay, so with that, I'd like to thank my collaborators, Ani Leyon from Brown, David Bolling from Caust, Brian Capo, John from Johns Hopkins. Brian Capo, Johnson from Johns Hopkins, Ying Guo from Emory, Mary Beth Nabel from Johns Hopkins, as well as Ryan Yue from CUNY. And thank you guys so much for listening. I think we have time for a quick question before lunch. Moo is very excited about this. Independent component you are talking about. So this is independent in what sense? Is what dependent in what sense? The independent component. In what sense? The independent component analysis using, yeah, okay, that's a good question. So, ICA actually does not guarantee independence, it's supposed to be independent of the spatial components. So, you're yeah, ICA aims to detect, to estimate maximally independent components. And if there exists independent components in the latent space, it's supposed to estimate them, but they actually don't turn out to be independent. Just that they that um just that they they kind of are there's the the um the uh the the optimization function basically maximizes like the negatrophy it's highly non-Gaussian and they tend to be not overlapping or not not very much overlapping so so how many independent components you're using here yeah it's a good question so currently we're using a smaller number so we typically working with like say 20 maybe 30 but for functional connectivity analysis I think For functional connectivity analysis, I think people often need larger numbers, and so we would need to examine the scalability. But to answer your question, I mean, people do anything from, say, 20 components up to 300 components. Oh, then from the talk, I idealize how to the topological portion of the topological eye share where independent component actually come from topological independent. I think we can do that. So it will be topological I share. So it will be topological eye share. And I think I can get into the proposal actually better than the traditional eye share. Yeah, yeah, yeah. I think. Awesome. I'm looking forward to it. I saw Jones first. Thanks, Annie. Great talk. Can you give a bit more intuition as to what these empirical priors are capturing? Capturing yeah, so I think go forward um to you back. Do you mean on facial maps or in general? So I think the facial maps are more informative than they are for the functional connectivity because we took a lot of care in the functional connectivity not to overconstrain it, given the structure of the infrastructure prior. Um, but with the okay, so with the um spatial maps, basically it's saying that. It's saying that we expect the background areas. So, okay, so this is the mean, right? So, this is kind of the on like the group ICA map. Like, if you're in group ICA, the higher mean looks a lot like a group ICA, kind of the average. And the variance is saying how much do we see the individual's engagement levels deviating from that average. And because they, you know, those patterns show kind of very Know those patterns show kind of very low variance in areas. It's basically saying that we expect in those areas everyone to kind of be the same, basically zero. So we expect a lot of smoothness and smoothness in those areas. So we're going to get a lot of gauge towards zero in the background areas, which are the components. Does that help at all? Yes, each component, right? So here I'm showing two, but in this analysis, there were 16. Yeah. 16. Yeah. And the spatial, the variance varies spatially all though. So we're estimating a lot of parameters. That's one downside. We need to have a decently large training data set. So we're currently working because it's our simulation shown that this is a really easy to estimate. This is not so easy to estimate. You know, it's not surprising. The mean is easier to estimate than the variance. So we're, but we know there's a relation between the mean and the variance, right? So can we use that to kind of improve? To kind of improve or adapt this variance in smaller samples. Okay. As we've already done the housekeeping, I'm going to go for one more question. Yeah, thanks for your talk. And as you know, I care about the reliability these days. And the idea here is that you're going to be like shrinking your data ICs to the population ICs, right? It is possible that you have a higher reality because you use the same population. Because you use the same popular prior for the test that like scan and rescan, because I mean, right? Um, can you probably take care to well, probably take care to with that? I mean, in principle, I like to avoid doing that. I'm part of ICA because there's no scale or anything. So, um, okay, so what I usually like to do is I'd like to, I'm gonna do like a reliability study. I like to do the Bayesian shrinkage on one, on the first session, but not on the second session so that I get like an unbiased estimate. That I get like an unbiased estimate. That's one approach, but it's hard to do in ICA. We haven't figured that out. But what we did is we looked at the ICC. So we're looking at individuality, not so we're looking at like mean scored error. I would completely agree with you that that would be an inflated measure of reliability because both are shrinking. But if we're looking at individuality, then I think feel like we're on safe ground. But I'm not sure everybody agrees with me on that. But we also have the simulation studies, which show that we do get much more accurate. That we do get much more accurate estimates. Yeah, intuitively speaking, if we like a shrink everything like first to like a population mean like very strongly, then you got you'll get a high ICC. Zero ICC. Zero. Because you have no variability of your individual. Oh, oh, I see. Yeah. So that's the that's why I think ICC is kind of protective. I see. Yeah. Thank you. It's a really important consideration. Okay. Thank you very much. Can we thank Amanda and Very much. Can we thank Amanda and speakers? No more announcements. I think we're just after lunch and then we'll back at half in our groups.