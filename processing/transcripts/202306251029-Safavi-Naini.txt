Of this talk is going to be what I'd like to convey is that cryptography provides the main tools that we have for securing internet. So we really need to appreciate what we get from crypto. Without that, we didn't have any way of communicating over the internet. And the next thing is that proving security for a real-life system is almost impossible. So it's impossible. So it's it's impossible. So with all the attempts that we do and all the methods that we use to say, to formalize things and prove security when it comes to real life systems, we can say very little. Okay, so let's look at where cryptography starts. Cryptography is this art of secret writing that goes thousands of years back. It goes to, you have seen You have seen things like this in your high school. So Caesar cipher, it's a very simple cipher. You take your plaintext alphabets, you shift it by three characters, and then now when you have a plaintext, each character of plain text is replaced by a corresponding character for from this ciphertext alphabet. So it's a simple shift. Every character it gets this gets shifted. This gets shifted. And of course, so when the receiver or decryptor knows the amount of shift, they can go back and recover. So when we give them the ciphertext, that looks quite jumbled up. But when they know what the shift was, then they can go back and recover the message. So that's the notion of key. So key is this secret piece of information that the receiver has and allows them to decrypt the. The ciphertext. So the keys can be, if you look at during the history, it has taken different shapes. So this is eschitale. This is a device that Spartans use. It's a cylinder, so you wrote your ciphertext on a band of leather band and then you wrapped it around a cylinder of the right size. Of the right size, right diameter, then you could read the message across the cylinder. So the key in this case is this cylinder of the right diameter. So it's a really physical key. So if you look through the history from 2000 years ago till very recently, maybe 50 years ago, cryptography was mainly something that was used. That was used by statesmen, by general, by government. This is for secret communication, ordering your soldier to attack or do negotiations, make treaties with your friends or enemies. So this was not really something that normal people use. It was just really for um uh for this upper class or military and and uh special kind of applications. Special kind of applications. And now then, so and how we did do the secret key exchange because this key had to be delivered to the other side. There were all these methods of like, you know, putting your key in a suitcase or a briefcase and hand it over to the next person or using these pigeons and things like this. So really very physical way of delivering the key to the other side and then use it for decryption and significance. For decryption and secure communication. So that was what it was. And then we have the time that the rise of internet. So this is the 80s and 90s. Internet becomes dominant and now becomes everyone is using it. We start with email and the World Wide Web and then services going on the Internet. And now cryptography suddenly becomes the core, the main tool that we have to do anything on the Internet. Now the biggest problem that we have to do is Now the biggest problem that we have is not really encryption or security. The main problem that we have is authentication. So how do we know who is the person that you are talking to? And so this is a very famous cartoon that was in mid-90s, it was in New Yorker, it's kind of all over the web. So it really captures the problem that we have with user authentication. And so we don't know who we are talking to. We don't know who we are talking to, really. I think there's nothing that says that the message is coming from. So, this is, but then, so that is a problem that needs to be solved. And then, gradually, I think that there are other, secure communication email. Everyone knew that when you send an email, it goes through all these routers and switches along the way. And everyone can read it. And when you send, when you go to a website, you can actually, every step of whatever you're asking for, everything. whatever you're asking for, everything, every activity is going to be seen. So going to government website for services, everything requires some kind of security. And then even the mobile phone when you want to talk to the cells, however, you have to have SIM cards. So all of that requires security. So security becomes, application of cryptography becomes core in providing this security that we needed to use internet in our everyday. Use internet in our everyday life. So we go even, so then it gradually expresses through everything. So through our passport, through e-passport, through your barcode readers, through garage door opener, cars, everywhere that you use, you use some kind of cryptography. So cryptography is really one of those technologies that are invisible, but it's everywhere. So let's So let's step back and say, okay, so let's start with the very, very basic cryptography. So hopefully that showed you cryptography has many applications and many definitions of security, all security functionalities. Let's start to stay with this simplest one that is encryption. And everyone is familiar. We had it the whole time and it's something that we use every day. So effectively, so here you have a sender, you have a plain text, and sending Sender, you have a plain text, it's an encryption algorithm, it generates a ciphertext, and then we talk about a symmetric key system. So, encryption and decryption share the same key, and then so the decryptor has the decryption algorithm has the same key, and you can recover the message. So, that's the very basic key that symmetry key encryption system. And so, when we come to modern ciphers, then the secret key. Ciphers, then the secret key is now a binary string which is truly random. When we say truly random, it means that every bit has a probability of 0 and 1 equally, and they are independently generated. So we have these keys, and then the question is that how can we so and I think the importance of these keys become even more important that we have this Important that we have this Kirchhoff's principle that says that in a cryptographic system, every detail of cryptographic system should be the same so we can find them. So you cannot say that my algorithm is hidden. So algorithm is known by everyone. So the only thing that stays secret in a secure electrophographic system is a secret key. So you can see that how important that key is because every bit of the key that is crucial. The key that is crucial in providing security. We count on every bit of the key. Okay, and as we said, so in modern systems, it's a binary string which is randomly generated. And so now this is now the question for us is that, Alice and Bob, so this is what we do every day. We go, so I'm here and looking at the website of University of Calgary. So my communication goes through the internet and the adversary who is even very benign adversar who is. Even if it's a very benign adversary who is only observing the communication, how can I make this communication signal between myself and the university? Or how can we share a key secret between the two of us by communicating over a public network? And if we think about it, it looks like impossible. And it truly was impossible for a long time. So to very 70s. So, it's really people, there was no solution for this. Then we have this groundbreaking result that you may have seen. And I apologize for people who may have seen this, but I thought I should say it at least some of you may not have seen it. So, this is a groundbreaking result. The paper is New Directions in Cryptography. I highly recommend to you to go and take a look at it. It's really worth reading. It's by these two Diffiers. It's by these two, Diffie and Talman. The three inventors or public key cryptography are Merkel, Talman and Diffie. He was a professor in Stanford, who was a research associate, and George Merkel was a graduate student. So what they did, so this is 1976, and this is later when they won Turing Award in 2015 for their contribution. So this paper actually does two main Actually, it does two main things. One of the things that it does, it actually solves this problem of key agreement. And it's such an elegant solution that after 50 years, no one can compete with it. And still, it is the main algorithm that we use every day. So every time that you use your Gmail, every time that you use TLS, you are using their algorithm. But the algorithm is very simple. But the algorithm is very simple. So you have a public group, society group, with a generator G here. So Alice and Bob, each of them, so this is public. Everyone knows the description of the group and the generator G. And now what Alice does, so she chooses a random integer, 1 to the size of the cyclic group, and finds g to the x and the Finds g to the x, sends it to Bob, and Bob does the same thing. And then both of them now can calculate this shared value. So g to the power y to the power x, which is g to the xy. So you can see this common shared value is only computable by these two. Adversary can see g to the x, g to the g to the x, g to the y, and now needs to find g to the xy. So that is the that is somehow we have to argue that diagnosis cannot do this. Because so these things are actually x is in the exponent, y is in the exponent, so so how can he actually do this? So can find this value. And this is where we use these computational assumptions. Use this computational assumption. So, the computational assumption that we are using now is what is called discrete logarithm problem. So, if you have a group G with a generator and if I give you G to the X, an element of the group, then if I ask you to find X, then there is no efficient algorithm in general to do this. So there are sub at the most sub exponential algorithm and that is where uh that is why it is called the discrete logarithm problem. The discrete logarithm problem is hard. There is no polynomial time algorithm for it. You can see that, like, for example, this is quite a small example that I've written, you can see that finding x becomes, you need to do a lot of effectively exhausting, exhaustively going through the positive values of x. But this is not enough. So if you have, so, so no, it's true that. So no, it's true that if this discrete logarithm problem has an efficient solution, then the Diffie-Hellman problem is the Diffie-Hellman key exchange is insecure. But doesn't mean that if someone can break this one, it's equivalent to this. And in fact, what they did, they introduced another computational assumption, and that is that if we have g to the x, g to the y, binding g to the x, y is also. To the x, y is also hard. So, and this is, this problem was known for hundreds of years, this is more recent, but in the past 60, 70 years, a lot of mathematicians and scientists and computer scientists, they haven't made any progress and kind of believe that this is as hard as this. So, now you can see that the difficult helmet was simple, elegant. Simple, elegant, and it has this security based on this computational assumption. So when we, but that's not enough yet. So obviously the left, if you know the left solution, you go to the right. But then you're saying it's not known that if you can solve the right, you can solve the left. Yeah, so I guess, so if you could find x, then g to the y, then you can find g to the x, y. You can find it today, otherwise. Yeah, so that is that, but the algorithm may just find a clever algorithm, just takes these two things without finding x, y, can find g to the x, y. So the other, so I mean, this one requires you to find x and then raise g to the power y that you would see, raise it into the power so. My question was that if if you can do that for some x, y, can you do the other problem? No, it's not in all right. It's on the right. Yeah, so one of them, so this is what Ader C is. If he can find the first solution, find x, then he can do this one to the x and find the key. But the other way is that he knows, he sees g of x and he sees g of y. Somehow you find a clever algorithm that combine the two and find g x y without finding x. Find GX volume without finding X. So this totally. Okay, so but this is not enough to say this key exchange is secure because, okay, so the addressing cannot find the whole of the key, like the whole GX void, G to the X void, but how do we know that one bit of the key is not leaked, right? So how do we capture this problem? So that is where things like defining security become Defining security becomes one of the core topics in modern cryptography. So, how do we define security for key? So, if you remember, we wanted the key to be truly random, every bit independent, and every bit with the same probability. So, this is, we are going to define a game, a game-based indistinguishability security. So, we are saying that the security is captured by. That the security is captured by this game. You're having this experiment, there is a defender, there is an attacker. So, this defender, so I'm claiming that this algorithm is secure. I'm going to run this algorithm. So, I generate g to the x, g to the y, and g to the, sorry, it should be g to the x, y. It's not g to the g to sorry. And then I also generate a random element of A random element of I also randomly choose an element of the group. Then I toss a coin and then if I provide to the adversary this triplet, g to the x, g to the y, and if b was 0, I send the correct value, g to the xy. If this was If this was one, I just sent a random element of G. So there are two cases. And now, the adversary, after seeing this triplet, it should decide that whether it was what we call a Diffie-Hellman triplet or it was this was a random gene. So this is distinguishability gain. So we have one experiment. So we One experiment, so the experiment is like this, and the triplet is generated. But the defender tosses a coin and presents one of these two things with equal probability to the attacker. And so, if this triplet leaks anything, then it means that this attacker can have a better chance of deciding which of the two cases. So, that's the meaning of indistinguishable. So that's the meaning of indistinguishability. So we define advantage of the adversary in so this is the output of the adversary, which is another bit, and probability of these two bits equal minus a half becomes the advantage of the adversary, and we want this advantage to be negligible. So I just want to give you one example of how we define security. So it's not enough to say mathematical problem is hard. We have to define what security means and we have to show that our Means and we have to show that our algorithm satisfies this definition of security. Okay, so the second thing that actually that paper did, the new direction of cryptography did, so first was that giving solving this key agreement problem, but the second thing that it did, it actually said that for an encryption system, you don't need to have a shared key. You can actually separate it. This is quite unintuitive. If I want to send a message to you, If I want to send a message to you, you don't need to know the same, you do not need to share a secret thing because all I need, so we can actually separate the keys. So I can generate two keys, a public key and a secret key, private key. So I put my public key, publish it. If you want to encrypt to me, you use that public key to encrypt the message. I know the private key that corresponds to that, and I can decrypt. So it's quite unintuitive, but that is what they did. But that is what they did, and so this is the proposal in their paper. And a year after that, these three photographers, Shamir, Rivest and Adaman, and this is their picture of Turing Award in 2002. So they, and I should say, I guess Turing Award is one of the most prestigious prizes in computer science. So they, so they So they actually proposed again a very elegant algorithm. When I say elegant, it's simple, after 50 years, no one has been able to make something like it. And it's something that is used. And no one has been able to break it. So these are the meaning of elegance in my definition. So here you have the just going quickly through this algorithm. So n, you have two prime numbers, p and q, so large. Two prime numbers, P and Q, so large prime numbers. And then the public and private key, in fact, there are two exponents, E and D, and this modulus N. So public key, this is what we publish. Private key, secret key is something that we keep. And then now encryption is simply raising the message to this power D, mod N. And then decryption becomes raising the ciphertext to this power D. So you can see that here. So you can see that here encryption and decryption are simple exponentiation and we know how to do exponentiation efficiently. And I guess it's so how to find, to generate this E and D, you don't need to know, but this is effectively using Euler torsion function. So this is using that. But the security of this important thing is that it relies on That it relies on integer factorization problems. So, again, this is a problem that was known in mathematics, not to have any efficient algorithm for hundreds of years now. So, now this is the RSA algorithm, the celebrated RSA algorithm, and security relying on this. So, I should say that this algorithm, if you want to use it to encrypt a file which is one megabyte, you cannot use it. Which is one megabyte we cannot use, it becomes too slow. So, the way that we use this one, we use it to encrypt a short random string that will be used as the key for our symmetric key algorithms. So symmetric key algorithms are efficient ones. These are capable of allowing us to have this shared key. But it is an encryption system which. An encryption system without any share key. So you can see that so it seemed that all we solved this big problem of key agreement by making these computational assumptions. And so what the question, so as everyone should be concerned now, so how do we know that these computational assumptions are going to last? How valid they are. And in fact, there are lots of They are. And in fact, there are lots of reasons to be doubtful about all of this. There are computers that are becoming faster, cheaper, a lot of advancements in different technologies. Everyone has heard about Wohr's law. The computing goes, doubles every number of um transistors on a chip doubles every uh eighteen months or something like this. So it's now changed a bit, but again showing that But again, showing that hardware technology moves quite fast. New algorithms are found, so there are a lot of clever people like yourself, so they're doing a lot of work. New algorithms are found, so those computational assumptions that we set hard problems, there's no guarantee as being a state part. And very importantly, all these computational complexity theories relies on Turing model of computation, and that is one. and that is one model of computation and definitely the other models, biological computers, quantum computers, and which is the one that has found a lot of attention these years in recent years. Particularly with the work of Peter Schor that showed in 2001 that you can actually have a quantum algorithm. You have a quantum algorithm that can solve both discrete logarithm and factorize. Logarithm and factorization problem efficiency. So effectively, if you have a quantum computer, both of those old crypto algorithms, all public key crypto algorithms that we use today to have secure, to enjoy this secure communication, they're going to go away. So that is quite a worrying thing to have because really it's something that It's something that breaks security of the internet. And of course, that is why we have all so much investment in quantum computation. So you have almost all major companies, Google, Microsoft, Amazon, everyone, IBM, all of them have their own computers, quantum computers project. Universities across the world and Canada is investing so much in quantum computers. Of quantum computation. Our CTM category is quantum CTM. So it's really the amount of effort that is put into quantum technology really makes you concerned that what happens. And that is why the huge interest in quantum, what we call quantum resistant cryptography and being able to be prepared for what we're going to do if there is a quantum point here. So when you go to the So when you go to the immediate thing that you can say is that, okay, so what about so that there are two problems? So Schor's algorithm solves two problems. Discrete factor, many problems, the two that are concerned are discrete law problem and integer factorization. So what about using other types of problems in mathematics that have not had easy solutions? So of course, so we have So of course, so we have these two big sources that have been used by cryptography, lattices and algebraic codes. So this gives you a much more bulky, big parameters systems. They're not as elegant as what I showed you, but they do provide security. So we do not know any efficient algorithm for random code decoding. Random code decoding problem or shortest vector in lattice problem. So these are the problems that have been used to design crypto systems. Again, these are many, many years old, so these are quite considered safe in that sense. But people have also proposed newer assumptions. This is one of the very respectable, widely used assumptions for learning with errors. It's by Odette Regev. You have a secret, which is a vector over ZQn, and so you have a set of random equations on this, but these equations have a little bit of error. So these are not exact equations. So if you do this linear combination of components of S with some error, with some small error that you randomly choose, it becomes approximately. Randomly choose, it becomes approximately like this. So, when you have a set of equations like this, the goal is to find S. And the best known algorithm that we know for this is at the moment is order of this form. So, this problem is closely related to lattices and also algebraic codes. So, people kind of believe that this is a nice problem. But there are also a lot of other problems. I saw it in the May's problem. Problems, I saw a really base problem that recently was broken. So things actually came from Canada, from Waterblue people. For quite a while, it was one of the contenders for quantum resistance cryptography. Okay, so this is one trend that we want to go forward. So we are going to use this, try to use computational assumptions that are not That are not, that we don't know any efficient solution for the music quantum computers. That's one approach. But then, so you can ask a different question. You can say, okay, so suppose we don't want to use hard problems. Is there anything else that we can do? Okay? So suppose the adversary, so you're not concerned about the computational power of the adversary. The adversary is computational. Adversary is computationally unlimited. Assume that the adversary is unlimited. And again, the same setting, so Alice, Bob, and Alice, Bob, and Eve, they're connected by, so Alice and Bob are connected to this insecure network that is observed by the adversaries. So the question is that, is there anything else that we can do in this segment? So this is again, and so one thing that has been proved information theoretically. Information theoretically. So, this is the work of Uri Morr in early 90s. He showed that without making assumption, not computational assumption, assumption in general, without making assumption, precondition, it is impossible to have a shared T between any bulk. So, it's information theoretic result. So, information cannot fly. So, information cannot fly from this one to this one. But you formalize it using information theory. And so an alternative setup that people have considered is this, that assume that there is a public distribution X, Y, Z, and then there is this. Then there is this Alice, Bob, and they have samples of these correlated samples of this distribution. So Alice has X, Bob has Y, and Inc has some side information about this X and Y. So this is the meaning. So you have some private input, this is Bob, some private input. They're not exactly the same, they're related. And then Eve has some. Some know something about it. So, this is ideal. And organizing knowing something, all of these knowing is information theoretic concept. So, what do we do with this? And I guess this is not really imaginary situation. So, this is really the situation that corresponds to, for example, biometric data. So, when you scan your iris or your finger. Your iris or your fingerprint, you get the scan, you create your encoder, generate the binary string. And then, when you register, you enroll, you put this one in your device, for example, or you put it on the website. And next time that you scan your environment, you generate a string which is correlated with this one. It's not exactly the same. There are some errors. So, and if you can imagine that your fingerprints are The fingerprint, part of it might have stayed on a glass or something like that, or something that others you can also get. So you can imagine that there is something of this, this is w prime and w, there are two correlated variables, and something is left for the others. Another scenario that corresponds to this one and is also very much used is this what is called a satellite scenario. So you can have a beacon that broadcasts random. That broadcasts random numbers, random string, and this random string is received by these people through different channels. So each of us have a different receiver, so cell phone, and what we receive is going to be different. These are all correlated because there is all one source of randomness. We receive it through different channels, but then it's correlated. So the question is that, okay, so what can we do? So this is this is so there. This is so there is Alice. So I look at a very simple case of this one. The case that you have Alice and Bob, these are two S strings that they have. This is exactly like what we had in parametric case. You can see that there are the two strings in the couple of places that they are different. And then the correlation is impacted by the Hamming distance between these two. So we would like this Hamming distance to be bounded. Hamming distance is placed. time and distance places that these two as vectors, they do not match, right? So every time that this is a one, this is a zero, so this is having one added to the hanging distance. So the number of places that we do not miss. So the question is that, so now this is our setting. Our adversary is computationally unbounded. Alice and Bob have this, so you can see Alice in the server, for example, they have this. Have these correlated samples and they need to communicate over a channel which is public. So, how do we do this? How do we actually generate a is it possible for them to generate a shared key? So, this is one of the tools that is used here is called secure sketch. Secure sketch is a pair of algorithms. So, Sketching Sketching and reconstruction. So, this sketch, both of them are randomized algorithms. So, it takes a sample, so this W, and it generates some something, some helper data that will be used by this reconstruction algorithm that uses this helper data plus that W prime that was close to this. Remember that? So, these two, and then it generates. Two and then it generates W. It recovers W. So it's kind of the concept of a required code, but it's not a reference code. So you generate something. But a very important property of this is this part. So this is the reconstruction part. So this helper data allows the reconstructor to recover the string that was used by the sender. But then for security, we would And for security, we would like to have this be required. So, of course, I can send this, the whole of W to this, right? So, this trivial case, I send the whole W. So, the security is we would like to have minimum leakage of information. And this is captured by this concept of mean entropy. So, if you have mean entropy of this thing, so this I'm going to So, this I'm going to show you in a minute. So, there are these two notions of mean entropy and average mean entropy of a random variable. So, if you have a distribution, mean entropy of that distribution is the minus log of the probability of the element that has the highest chance of occurring. So, this is your worst case entropy. entropy or best guessing and the best guessing probability for that variable. So this is called mean entropy, number of bits, like you have five bit mean entropy, it means that your success chance of guessing a variable is 2 to the minus 5. And then you have when you have so when you observe something, when you observe B, which is related to A, your mean entropy reduces. So Introduces. So, this definition, which I'm not going to go through it, it actually captures this expected mean entropy of A after you observe B. So you can see that here, when we send S, the adversary sees something which is related to W. So this leakage actually reduces the initial entry. The initial entropy of W. So W was something that was unpredictable because it's our biometric, no one knows it, but it was. But when we send some information related to this, then it becomes the pieceability of this reduces. So this is just remembering that you are in information theoretic domain. There is no concept of hardness or anything like that. We just really measure the information. Clearly, measure the information that is needed. Sorry, Ray, I'm a little confused. So, those are random algorithms, right? So, but Rick, it's supposed to take, it gives W exactly with probability with type probability. Exactly. Yeah, exactly. Yes. So, yes, and I think, so this is this is a, so mean end, we would like the, so we call this one a. A small m is the mean entropy of the source, m prime is the mean entropy of this thing, and then t is the distance between w and w prime. So we say this is a secure sketch. So security if this property is called. So mean entropy of W, after we send this one, we still have enough entropy in W to be able to. Entropy in W to be able to extract the key. Because as I said, so this reconciling the value of W at W prime is not hard. You can send the whole of W, but that removes all the entropy that you wanted to generate your key. So that's what we went through. So in fact, now to just give you a quick example of how these security Quick example of how the secure sketch would work. This is what is called a code offset construction. So it's very simple. It uses an error correcting code. If you're not familiar, so basically error correcting code is a sub-space of dimension k over f2n, for example, in the simple case. And when we say this is the error correcting code of minimum distance 2t plus 1, it means if Plus one, it means if each code word is added with error, is disturbed by error, by T errors, we can go back and recover it. So this is one of the big areas of communication and mathematics, also construction of these code and code and things like that. So this is, we have two algorithms. So this is, we have two algorithms, encoding and decoding. So encoding takes a message and things with code word, decoding takes a code word which is corrupted and recover the message as long as this distance is destined. So how do we use an error correcting code to construct a secure sketch? Remember that we had W and W prime and we wanted Bob to be able to use this sketch and W prime to recover W. To recover W. So the secure sketch of W becomes randomly selecting a code word C and adding it to W and sending it. This is adding means bitwise X4 of the two vectors. So reconstruction takes the decoding algorithms. So the reconstruction, which is the algorithm used by Bob, it uses the It uses the sketch and W prime. W prime was the sample that was close to W. So Bob reviews the decoding algorithms of this code. It adds these two things, the two inputs, again binary XOR. And remembering that S was this thing, you can see that decoding becomes, so W plus W prime is a vector of at. Is a vector of at most t non-zero values. So it means that I have a code board that is disturbed by at most t errors. So my decoding algorithm can recover C, and then once I know C, Bob, can recover the data. So this is one secure sketch, and I guess one of the main security and Security analysis is to say how much is the loss of entropy through this operation. So when you mask your code word, when you start with a source, when W has mean entropy N, when you use this approach, when you add this code word, random code word and then you recover it, how much is going to be the resulting mean entropy of W? mean entropy of W. Because something is big to the other save, how much is left. So that becomes the analysis of this kind of extractors, sketches. So if we use this, so now this, if you want to actually make our fuzzy extractor, fuzzy extractor means that we are now really extracting truly random string. So it uses one of these secure sketches and Sketches and so it is very similar since this secure sketch only generated this helper data that helped reconstructing W. Puzzle extractor takes W and gives two outputs. One output is this helper data that will be used by this to reconstruct, and then there is also a local truly random string which is key. The output of this reproduction. The output of this reproduction is only a random key, which is exactly the same as this one. So, here for fuzzy extractor, we are insisting that the output is truly random. So, effectively, we are using this one, and then we use some extraction after this. So, this is what we have. So, we use our secure sketch followed by Pure sketch followed by a randomness extractor, which is again one of the big areas in mathematics. I guess I'm confused. This doesn't work, though, if the adversary also could listen to the dice, right? If they also had a W. So these samples are this dice means that these samples are private. Like your sample of private metal is private. Yeah, okay, yeah. So like the adversary doesn't have one. Okay, yeah, so like the adversary doesn't have one. Okay. Hey, so yes. So this is the only thing that the adversary doesn't know in this system. The adversary has infinite computation, right? So this is the only thing that the adversary doesn't know is your biometric sample and your registration at the cell. Yeah, exactly. Okay, so yeah, so I think I don't know how much time is, let's look quickly through this one. Let's look quickly through this one. So, to show you that, so that method of extraction here was called a sketch extract. So, first we use a sketch function to allow the receiver to recover W and then we use this randomness extractor to generate this random string, truly random string that. That is independent of the observation of the addresses, so piece of observation of the addresses. So I wanted to show you that these things can become, so that was a very elementary one, so in practice we do not, like biometric, so the sources that we have, so how do we measure by mean entropy of the Entropy of our fingerprint or our eyes. How do we model real-life sources of entropy? So, like, one of the models that people have used is this model of, like, actually they use it for iris. Iris later, you can look at it as a low entropy rate source. So, the property of this iris is that it's quite an, if you encode an iris, it is a If you encode an iris, it is a long string, like it can be 2000 bits. But not all of it has high entropy. But if you take small samples, sub-samples, these sub-samples can be, if you take these sub-samples short enough, these sub-samples are random sub-samples. These sub-samples, they are random. So this allows. So this allows us to have a different kind of extractor, which we call sample and block extractor. So the idea here is that, so remember the goal of this fuzzy extractor was to, in the setting that Alice and Bob have two correlated randomness, allow them to have a end up with a shared truly random string. So that was the goal. So what do we do here? So that was the goal. So what do we do here? If we have a source like this, if the samples that Alice and Bob have look like these long strings that have low entropy rate, but they have a small sub-sample of them, they are truly random. So, not truly random. Sub-sample have a bounded unentropy. So, what we are going to do, we are going to take, this is the random. This is the random string that we would like Alice and Bob to share. So, this is a truly random string. We take this one and we take some sample of this. We use this locking mechanism, which is a kind of encryption system, a special type of encryption system. So, we use, we do it many times. Many times you are going to maybe 500 times or 1,000 times. 500 times or 1000 times. The same R, but with different sub-samples of this. And we lock, and so our helper data is going to look like this. Now, the receiver is going to have a string that is similar. Some of these sub-samples in the so the other receiver knows which sub-samples we. Receiver knows which sub-samples we are using. So we are sharing that information. So the indexes that we have used, but not the actual value. So this thing is going to be different. This sub-sample, because the two strings of Alice and Bob are different, W and W prime are different. So this sub-sample is different, but there will be with the high probability there will be one sub-sample that is the same. And that sub-sample allows us to unwind. Sample allows us to unlock and find our key. So, effectively, what we are doing, we are using one random key, which is R. We want to send it to the receiver, and we don't have any shared key with the receiver. But we are using some sample of these things as our keys, and we are saying because this W and W prime were close, if we have in Close. If we have enough of these things, then there is a high probability. Now you have to calculate all of these things, how many docs you need, and so all of these are different stories, but effectively the idea is this, that there will be the high probability if you choose the number of locks sufficiently many, then there will be one of them that allows the, with a matching key and that allows the Key and that allows a receiver to develop. Okay? So I guess that's what I think I should. Maybe that's what I wanted to say. You can add robustness. So all the thing that I've said till now was the case that your channel was, so whatever you sent over the channel was received. The adversary didn't touch the channel, so he just looked at the channel and he wanted to make sure that information is not leaked to the adversary. Information is not unique to the adversary, but the adversary is not really like that. So the adversary can change things, and this is the concept of robustness. So we add this back message authentication code. So we have a design, for example, that we have this. This is one of our latest work that we work on and that improves security of this. It uses these components. It's a an LPN-based encryption, so LPN is a few. So LPN is a learning parity with noise, is an example of, well, you can look at it as an LWE problem, but this is the only binary vector. So we use, and then LDPC code, which is, this is low density parameters check code. This is the code that we have to use to be able to decode. And then we use information. So you to the effectively the Effectively, what we had before to this one we are going to add a MAC, that message authentication code, that detects any tampering of this. Okay. I'm not going to say, so I think one of my master's students, so he actually implemented this one for Irish data. He had to collect, I mean, getting this Irish data, real Irish data. This Irish state, a real Irish state, is quite a challenge, it has to be. Yeah, so agreement of university and this other North Carolina or one of these places they have a repository of artist data we have to get those things and then showing that all of this system actually works. Okay, so I think that's all I wanted to say. So, I think one thing that I wanted to say here is that this part is the design of FTN-based, robust, fuzzy extractor. Now, this part becomes when we want to use it in a particular setting, in a real-life application. So, in a real life application, as we said, we looked at iris data, and for that, we need to And for that, we we need to do a whole lot of mo modelling of this. So firstly, we have a repository of so many irises. Each iris has many samples. You have to look at them and you have to show that these actually satisfy the assumption that you had on your source, that was low rate in entropy and things like this. So you have so that this construction actually does make sense to be used in this setting. In this setting. So when you do all of that, then you have this scheme. Okay, so yeah, so I guess that's all I wanted to say. I think cryptography uses a lot of tools and techniques from mathematics and computer science. You have seen a range of things that we have touched to it. And key agreement is a fundamental problem. It's a beautiful problem. It's very simple to stay. Problems. Very simple to state and impossible to solve without any assumption, but you have different kinds of assumptions, and then you have all these very clever, very elegant algorithms. But I think one of the challenges is that how many of them can be actually used in practice? And that's why Diffie Hellman is actually such a star out there. And I guess the other thing is that proving security is really. Is that proving security is really a you prove, you have to model. So every time that you prove, I showed you that one way of a small, tiny sample of how we define security. So when we define security, we have to model what the adversary can do, what adversary can see, how can interact with the system. And based on that, we have to say what is the meaning of winning. So it's a game-based, the simple case, game-based definition. Definition. So, after doing all of this work to prove security, our proof is as valid as our model. So, the adversary may do things outside our model. And so there are security concerns over in here. So, it's really like that, that this becomes quite important that all models are wrong, but the practical question is how wrong do they have to be enough to be useful. So, if that really makes sense in in our situations. In our situations. But I think there is a big value in doing this analysis and knowing what are the things that can go wrong in all of this. And these are some of my students and postdocs who have helped. I suspect we have time for a quick question to answer. Thanks. In the interests of getting people to lunch, let's try to start the next talk as soon as we turn off. 