Basically in the typical protocol reference panel, where there's the interaction and the genotype, and the app height is in the QAS data set. And when we initially learned what is the TOS, we feel things in the field, there's no thorough analysis between TWAS and the TWAS. So lots of papers published in the field of TWAS, they compare with the previous DEN or fusion. Or fusion, compare the the new Chiwazo method to the existing TOAS method. But there is a lack of solar analysis between TOS with just the GOS. So now we conducted a mathematical derivation using NCP, the down saturated parameters, to analyze the power of TOS and GOS. So here we we have several theoretical results. But this of course based on the assumptions because we we used the closed form derivations. The cluster form derivations. And based on our derivation and the parameter estimation from Western Genome Project, we find that, first of all, in the pyrotropic model, the TWAS protocol is a lot more powerful than the consolidated model. And second, we find TWAS sometimes is superior to actual expressions. That means, let's say if we have genotype, phenotype, and the gene expression, we can say that. And the gene interaction with the same data set. And then we can do an IU analysis by associating genotype with it and associate the iteration with the phenotype. This we have real iteration. Since this protocol, this hypothetical protocol is not as powerful as the TOS protocol that borrows some expression from external data sets. So this initially looks kind of intuitive, but if we think harder, But if we think harder, if we think we are trying to find the genetic basis of disease through the mediation of expression, then the borrowed, the predicted expression contains genetic components only, while the real expression may contain other noise. So that's why, surprisingly, the QA protocol actually is more powerful than a hypothetical protocol that we have expressed. That we have extraction in the current GWAS dataset. And finally, we find that GWAS could be more powerful than TWAS when the extraction credibility is lower than some threshold. And remember, this is probably 6% for for consolidated model and uh and four percent for for biological model, as far as our derivation, wh which maybe maybe bad also because we have some assumptions. Because we have some assumptions when we derive NCP. But these three messages lead us to think about what exactly is. So some people interpret the QAS as a protocol that could give expressions, but we think it's probably two steps. One is feature selection, the other is a feature aggregation. These are two commonly used protocols in machine learning. So in the first step, So in the first step, when we train the GREX, the genetically regulated expression, when we train that, this is essentially a process of feature selection. We select the weight, the SNPs, and their weights that are relevant to the gene expression. And in the second step, when we apply this GIX, because we usually notice the GIX head, in the GIS data set, we use the genotype to We use the genotype to predict AG impressions for further association to phenotype. This step essentially is a feature aggregation. Basically, the features that have been selected in the first step will be aggregated in the second step. If we think in this way, T1 is essentially two things. One is selecting variance associated with interaction. The other is how to The other is how to gather them together for a joint attack to thin attack. If this were correct, then we don't have to use the same model for both steps. Here, which is the GIX. We use the same elastic net, which is a kind of a linear, weighted linear combination of selective variance for both first step and second step. So, directed by Lisi Insight, we are thinking about whether we can. We are thinking about whether we can replace one step of TWAS to something else. So here is our first attempt. The first, in this protocol, we still use the SDNet borrowed from DivaScan to predict the exertion. As a result, we got the FNIPs and their weights. But in the second effect, the which we interpreted as a feature aggregation, we no longer apply the GIX head associated with the phenotype. Associated with the phenotype. Instead, we use some other method. So, in the elastic net, we got the weights of the SNPs, and now we use this weight to calculate and rescue the GRM. The original GRM can be calculated by G times G prime. Now we have a W, which weights learned from the last n in pretty big scan. And this forms. Scan. And this forts a kernel, the matches, the virus correct matches for the kernel. And now we apply a scat using this weighted kernel. So basically in the second step, we leverage the weight learned from predicted scan, but not just the US them, just the original linear combination. Instead, we use the kernel, and this is more powerful than the original predict scan. Predict scam protocol. Actually, a couple of weeks before we finish this, Jinjin and Scrooge with Emory published their work called VCTOS in BioRP. And we are a little bit a couple of weeks later then, we call it KTOS, K stands for kernel, we call it VCTOS, or VC stands for one of the components. I think as my view, we are identical. Although we are Although we write the paper in a very different way. So, this is our attempt of replacing the second step, which we call the feature application, in QDS scan, using kernel method. So then, we go a step further, cover the first step, in the feature selection. Do we have to use ElectronNet? Of course, there are lots of ways to select features in machine learning using the Gene Thrashers object queue to select. Dean threshold as objective to select infinite weights. There are a lot of weights. And what we do is just use the marginal effect. Just regress the expression against one genetic market at a time. This is similar to the standard Q-T equal TR analysis. Now we have our four protocols. First, using GIX, which is the GACNANT for feature selection, and also GIX for aggregation. This is the only email. Aggregation, this is the only predicate scan protocol. Or we just use a marginal selection, just one sneaker time. This can be linked to GIAX aggregation or kernel aggregation using SCAT. And we compare these four protocols. And we showed that the protocol using marginal selection together with kernel aggregation is most powerful as far as the trail we have. The trail we have path is. And this shows, essentially, Twas is two thirds. And if we disentangle it the two steps, we need a more powerful analysis. So here is our colour arc. So this stands for the DNA extraction as well as the phenotype. So in our interpretation, it's not really a causal model, but I know this is maybe. Model, but I know this is maybe wrong. I think based on the discussion on Tuesday, I think our interpretation may or may not be just correct. But as far as what we see in this limited data set and the simulations, maybe based on the assumptions, we think the essential of TOS is that the expression can serve as a proxy to select the features. Select the features and then we can use whatever we like for the feature application, and that's our understanding of what it tells us. And based on this understanding, we actually open the door for lots of other analysis. So, for example, when you have real variants, the real variants are not good in predicting iterations, but in the view of a feature selection, this actually come straight forward. This actually works straightforward, but this is not part of my talk. So, here in my talk, this talk we extend the QRS for image analysis. So, the neural image is relatively expensive. And it's relatively expensive and sometimes unavailable for both the legacy GWAS data sets. And it will be nice if we can both. And it would be nice if we can borrow the image from somewhere else. For example, you have a bank. And a good thing is that the neural imager has a higher fatibility than G expressions. So then we develop a protocol to borrow brain images from the UK Black Bank and then apply that magazine data set for neurodevelopment disorders. So we call this data bridge, which means the neural image can be borrowed from other external data sets, just like the GG instruction in G-Text dataset. Now we can use the image for feature selection, and then aggregate the selected variants in our social test. One of the advantages One of the advantages for our marginal effect-based selection is that it doesn't have to evolve the whole genome regression. So for gene expression, it's straightforward to predict extraction using the c's as the c's variants. Or when we use trans variants, we have some private information about the transcription binding size, etc. So whatever we have some biological insight to narrow down. Set to narrow down. However, when we have images, since the SNPs in the whole genome could be relevant, so then loading all these variants into a regularized question model may run the risk of either overfitting or underfitting. When the number of variants is so large, it sounds for me difficult to manage this. But when we use the feature selection view, now we have more flexibility of selection. Flexibility of selection, whatever variance is more relevant. So, this has been just published this year in the American Journal of Emotionetics. So, here is the idea of the Borough framework. As I mentioned, the Provisions, Chiwas protocol, using the same strategy for both selecting variants and aggregating variants. And we separate them. And we separate them. We first do selection and then do aggregation. In our software, we actually provided all these alternatives. One is using the elastic net, that's similar to the previous scan rubber expression, or using linear mix model to analyze one mark at a time. For information, we provide both a linear combination and the weighted current of which is. And the weighted kernel, which is similar to the SCAD protocol. And we also conducted simulations using both causality and the HP models. This is quite similar to what we did in extracting data. Just here we simulated images, imager associated variants across the whole genome instead of just the C-servatory variants. Okay, so here's our result for the simulations. I will skip the details. Just the main method is that, again, in college of the model, the protocol is more powerful and our our protocol is more powerful than just uh directly associated uh the s variance to image and entry from image to phenotype. Uh again. From the image to phenotype. Again, similar to our closed form mathematical derivations using extractions, clear we use simulations to show that the borrowed image could be more powerful than the real image if the naive method is used. So again, the same underlying philosophy. If we believe we wanted the genetic basis, random image selecting variants, that's better than just That's better than just the directly associated the real ease. So here we applied our method into four neurodevelopmental disorders. Three of them are from the EBF. One of them is from MSFD Autism Sequencing Consortium. And we discovered that um a few uh IDP. This is IDP is uh immediate derived phenotypes. Immediately around the phenotypes with some biological interpretations. So we do this is the the significant result from the image associated study and these are the functional goal and the KEGG enrichment to show the functional relevance. And as a byproduct, we also compare the our protocol Our protocol against the imaging in UKBL Bank. So, in the UK Bank, we have some samples that have a label of major depression disorder. And this is the, in our IMAS analysis, we use an external GWAS data set. We borrow the imaging from UKVLM. We identify the eight IDVs. And for for match the before we match the number of sample case under control in UKVLM, But we sample that, we have to randomly sample some sample individuals. And the most trials have a very small number of discoveries. Or alternatively, if we just use all the 5,000, all the 5,000 individuals in the UKR rank, which means unbiased analysis because the semotic in the UK is actually a lot larger than our GR data set, we still install the I must discover more IDPs than the direct association using data. So this again consistently to our claim that using this feature selection method, actually when we borrow an image from an external data set, the power is still good. And we also analyze the EQTL, and the overlap between the reported EQTL. They reported the EQTR and discovered the SNIPs using images and we find that their OLF are pretty good. So finally, my collaborators, Dr. Pao Arnold and his PhD students, carefully analyzed our results and then they figured out a story about cerebral underlying these four neuropsychiatric disorders. Disorders. So, in general, many years ago, I think cerebralism is more relevant for the movement disorders such as Parkington disease. They told me in recent one or two years, there are some reports to study the cerebral in neurodevelopmental disorder. But in our results, we actually find the one IDP that is underlying only. That is underlying all these four different neurodevelopment disorders, namely schizophrenia, autism, bipolar, and depression. And they managed to undertake literature and also look at the expression, the co-intression network in the G-TACS brain tissue. And they find the mechanism, which is what they have found, the immune system, the TP53, as well as other. As well as other immune-related 19s are related in this pathway. And finally, the tech-home method is that IMAS is a new method that leverages existing image data, such as IDPs in the Juhyprovan, to edit a genotype of phenotype of searching studies. And we showed I must identify IDPs related to the fault health disorders by the BMS. And with borrowed images, it could be us effect through us, the viewer image. And finally, I know that there's another tool called Brain GAN by Hartley group. I think that the comparison is rather different things. I think our band supports genotype data only. Supports genotype data only for small samples, while brain scan supports summary statistics for large samples. So, our method is kind of more competitive to brain scan. When you have a local sample, let's say with only a few hundred or thousand genotype individuals with genotype data available, then you can use our method that is more powerful for smaller samples. It's more powerful for smaller samples. But if you and then as a larger example, with the samples that are two things, for the moment we don't support them yet. I just have a question. So there's obviously the expression heritability, and there's the proportion of variance explained by gene expression, right? Absolutely. So there's the heritability. So there's the heritability, expression heritability, on the one hand, and then there's the proportion of variance explained by gene expression. So I was just wondering how, you know, in your simulation scenario, so in your original analysis, which one were you looking at? So you mean the first server analysis? So here we actually assume different spectrums. assume different spectrums of expression habitability and also where it have uh where it is explained uh by the by the expression. So in our paper we have the picture for different extraction. Okay. Let's thank the speaker here and so our next speaker is uh Ben Han Hong. The bare hand hole this