Our next talk is by Heinrich Albert and he will talk about observability quest. That's a hope, right? Hope for observability? In some sense, yes. Alright, so thank you so much. So, I structure my presentation as follows. I will give you first some motivation which I encountered roughly 20 years ago when we started to do data simulations. It was very exotic. And so, it's all about the same questions. All about the same questions. What needs to be observed in the view that we have so many constituents? Central case also, photochemistry is considered here because it is in a certain sense very important and at the same time quite challenging. When can emissions be quantified? Initial boundary domain emissions and a practical approach to full-fledged CTM, where and what to observe, to predict and find. To observe, to predict, and finally we'll come up with a conclusion. So let me start. So the critical questions I encountered, and to some extent they still have, in a certain sense, some ability, with roughly 100 prognostic variables and only a few observations. Why do you expect chemistry data assimilation to be successful? So those questions were mostly posed by experimental chemists which have a very strong position in Germany. Which have a very strong position in Germany. And so it was a real challenge to answer to this. And how do you plan to avoid artificial chemical imbalances by correcting through single observations? So in mathematical terms, it would mean you will be pushed from the slow manifold and how you go back there, and finally, you have no control where you finally end on another slow manifold, which might even be more apart from reality. Apart from reality. The third one is: why do you approve initial values while the system is controlled by emissions? So, as a meteorologist, I was in the tradition of initial values and not other parameters. And the meteorologists, to my knowledge, just consider the interaction with the soil and so on. And the addressing of the initial values is still paramount. So, So that's the reason why we have the observability quest. And do we observe sufficiently complete? That's finally the key question. And if you see something like this, if I had started with this number, I would have lost confidence in the beginning with 200 reactions and so to get control of this. And while he met. while the metallurgists have depend on how detailed they have the the water they never come up higher than ten or so constituents so what needs to be observed and this is addressed in terms of the photochemistry and again taking the ideas which were introduced in meteorology there's one There's one, quite sophisticated one by Berlina, for example. He comes from statistical design and he minimized the analyzes error coherence matrix A via a trace by some case. So finally what is to be minimized is the operator H. So we have to optimize the observation operator as a key to A key to get the best estimate of the analyzed error covence matrix. So, this ends up to find maximal eigenvectors for the observation operators, which then configure the observation. So, that's the balance here. So, this capital L is the model resolvent, and M denotes here the tenure. The other one, which is a little bit more simpler, is introduced by PARM as the singular vector analyzers. But PARM has the singular vector analysis. So observe the maximum singular vector configuration. So you have here, some people say the Rayleigh coefficient, and then you have here the Ozelich operator in the numerator. And this is quite intuitive to go ahead. And what we have done here, we tested both, by the way, this is the PhD work of Nadine Purez. It's not real work was not done by me. Work was not done by me. So, and what is also a very old approach to this in early days of photochemistry, where the key part is that the ozone production is non-linearly composed by a combination of volatile organic compounds on the one hand and NO and NO2. So, in this case, you see a In this case, you see a VOC sensitive area, so this is the limiting factor, and this is the NOx-sensitive area. So, this gives you already a simple indication what can be expected if you are doing the jobs in terms of the maximal sensitivity or a smaller value. So, and this is quite a sketch and introducing this with the full-fledged regional air. Regional air chemistry model Lucker 2. Then you see something like this. You have to make a choice within the fixed time span. So in this case, this is HCHO taken here and here NO. This is as a representative of the VOCs. Then it is dependent on the initial concentrations of NO and HCHO per variant. Change the final concentration is given by the colour. Is given by the colour, that's the production of the ozone isoplase, and then the gradients of singular values are given here by these arrows. So, in fact, again, you have here the LOX constraint regimes. That means for your observations, better observe NOx. That's not the problem from experimental viewpoint. The problem from experimental viewpoint is that the observations of VOCs are quite challenging and costly. Quite challenging and costly and sparse, and not possible from space in that sense. So, if you are in this regime, you have to think about some practical approaches. And one is that you have the evolution of the ozone concentration. There's a certain sense of a very practical workaround not to come at odds with this. So, we have just to mention we have. Just to mention, we have to do this within a fixed time span of forecast, but then there are other subtleties of the chemistry which must be addressed. And finally, you have to make a calculation of this source for each type. So what has been done here, this is, I guess, for those of the mathematicians quite familiar, but you have some within some metric here, some initial uncertainty here, whether you have a perturbation, be it B and B or C's or not. BOCs on the NOx, and then you have some forecast time, and then you get here in this case, you have the maximized ozone production in a certain combination of perturbation of NO and VLCs. So, in that sense, the X tension here is given by the Yaponov number in this sense. And surely, this is not volume in phase space, volume concerns. Phase-based volume conservative, so not a Hamiltonian system cannot be expected. But this is not of importance here. What is of importance is the forecast time here. When do you do this? Is it directly at emissions or later after some aging? And there are lots of combinations which must be considered. Start it in the night, and do you want the best measurement in the night or the date, and so on? So finally, this. So finally, this led us to the following representation. Each pixel here is one sort of discalculation here. And what you see here, all these two types, on the left-hand side you have perturbation of the volatile organic compound, on the right-hand side of the NOX. And though here you have some initial time, you have some aging, and then you start to be interested in this. It starts at here, let's say, at 75%. Let's say at 75 hours, and then the forecast you are interested here at 175 hours, and so on, or in between, you can look here what is important to observe. So, if it is red, it is preminent that you have preference to the VOC observations, or if here it is red, then you have preference to the NOx observations here. And then to make it a little bit more confusing, sun, moon, sun, moon. Confusing sun, moon, sun, moon, sun, moon. This depends where you do it. Start with the day or the night, and your final target time is the day or night. But at least what you can see is fairly complementary. And this is for a not pristine situation, but marine. It is not some on a background which is fairly Clean. But this can be done for a lot of different conditions. So for the free troposphere, you have a picture like this, and for an urban plume, you have another picture. What's here is the signature is fading away with some aging, but here that's for the urban program. That's for the urban problem. At some time, it's the NOx to be observed because the NOx controls later to what extent the VOCs are finally combusted, in a sense. And with the VOC mix, you have short reactive and long reactive. In this case, the mixture is given by the scenario here. So, if you are confident that you can trace the Hermas. Confident that you can trace an LMS with the range and sense. This is the answer to set for the photochemistry, what should be observed. And if you make a step, oh sorry, something wrong. I would also give you an example of you doing not for the initial versus but for the emission rates, but because maybe I did not activate this slide. Okay, so let's proceed. So let's proceed. The next question: When and where can emissions be quantified? So the key point is that for reactive chemistry, total difference to, let's say, for the CO2 people to involve. We have the CO2 people have a big background and a small signal. That's the challenge. Our challenge is that background. Is that background and forcing from the emissions is in many cases or in critical cases on the same level. People have identified a dump curler number in that sense, but this is just to drop a name for those who are interested in this. So in both cases, what you measure can be emitted in your model domain some hours or a day before. Some hours or a day before, or be advected or mixed down from the free troposphere or whatever, or can be, if the geometry of the model is appropriate, introduced by the boundary values. So the question is, what can we expect under certain configurations that this is the result we have is giving you something reasonable about the emission rate or not? Reasonable about the emission rates or not. If you, as will be shown later, simply introduce the emission rates and include this as an optimization parameter, then you will get the nice result which fits with the observations which are given for the observation operator, for the assimilation procedure. But the forecast can well go astray because it is not included. Not included. So the time where we observe this is quite critical. And so what is done here in general terms, the question is what should be optimized finally? The initial values, the boundary values, or whatever. So the basic observation is that the product with paucity of knowledge times importance for the form. Times importance for the forecast. If this product is high, then you have to address this for your inversion. If your knowledge of the emissions is poor, that the positive knowledge is really high, and clearly it has an importance of the forecast if there are emission sources, then emission sources can be reasoned well. The critical automation The critical optimization parameters and not the initial values. You can construct these cases. Close to an emission site, the initial values can be negligible. So what we induced here is quite similar to what has been presented here by guests from Camps. First, here the cost function, background and observation, which is fairly Which is fairly known. And then we have to induce here an emission arrow covence matrix in this set. But this is not the key of this, but just this is a very old slide from me, but it's just to illuminate what it is. Let's assume that our observations are unbiased and fairly reasonable, and that we have here a biased emission, and this may be the true state at the time. And if we start with the And if we start with the initial values, the observations will try here nice in this area for the assimilation interval, and then the old problem will reappear, then it relaxes with that bias. And so you say, oh, the emission are the problem solution, let's optimize the emission. You quickly run up at this problem, and then the big tank is overturned, and with the emission. And with the emissions, it's a more inert system with the initial values. And the big art is to get this far. So the question is whether we have a chance to achieve this in a certain setting or not. And with this, I went. This is a practical approach where we exactly see this. This is initial value optimization, this is emission optimization, and putting Optimization and putting everything together in red. This works fine for this station aggregate, which regrettably dismantled, but other cases we have problems. So I went to Berget Jakob and those are optimal control mathematicians and Mathematicians, and finally, they told me as a naive metallurgist, I thought this would not be too big of a problem to have, but they say that we have, in mathematics, a real problem with this. What they have to do is, and it turned out in the paper, which is optimal control of observations, observation locations for time-varying systems on a finite time horizon. It turned out that the mathematician in this respect was an infinitely long time interval. Infinitely long time interval, and they had in this paper they were successful to prove under which conditions also a limited time interval may work, that it converges. So the result is for time variance in the Hilbert spaces, the existence of convergence of common locations based on the linear quantity control function on a finite time horizontal is demonstrated to be subject to conditions feasible. And the approach is the following: the optimal location of the Is the following. The optimal location of observations for improving the estimation of the state is found as a dual problem of the least square optimal problem, the control locations. And the approach here is taken as Kalman filter and Kalman smooth. So to include the emissions here is that we artificially defined a prognostic, computational prognostic volumes for the Volumes for the emissions so that it can be homogeneously extended, the extension. And what is the optimization parameter finally is only the amplitude of the layer site. On the other hand, we are linked to the fact that our emission intervention window is the full day. And so this is then. And so this is then the observation operator associated with this. This is fairly reasonable, only for the notational reasons. And the excess to go ahead is that we have to introduce the observability gradient, which is a little bit blown up. The gramians are always blown up because we have all the time stacks included. Stacks included here, and then we have also the model operator here. And then we have also over the time steps of the observation error covariance matrix include. This is also fairly standard here. And to get the link to the degree of freedom of signal approach in this, this is the way as it is produced. This is the standard case, this is normal. The standard case, this is normalized. So, what you have here, this in terms of methodology, it is the forecast error coherence matrix or background error coherence matrix. This is the analyzed error coherence matrix. And if there is no improvement, the scaling here would scale it. If there would be a perfect improvement, you would end up here with the unit matrix here. If no improvement is, then you are. If no improvement is, then you are fit to zero. Then you plug it in here, and finally, and then this singular value decomposition, you end up here with this scalar case, where we have taken the nuclear norm, which is quite familiar in the case of control for the control people, which is not the quadratic one, so you have a rhomboidal structure in this case. But this is of more liberal pseudo-T here, what what U2T here, but not too much of an almost here. And this can generalize in a direct way for the ensemble case, which I, for the sake of time, I will not include here. And what the next step here is that with this degree of freedom of signal matrix separate in that sub-matrix which is of the constituents and that one which is dependent on the Which is dependent on the emissions, and then we have cross terms which we not consider further. And then you can end up here with partial singular vectors here for the sector and the singular vectors here for the concentrations, and those one for the emissions. And then you can go ahead and associate it here the that's the nuclear norm here, in this case. The nuclear norm here, in this case, and I have a scalar value to make this. And this is the simple example of how to work this. So just to show, given we have one observation site and one window emission source location and the assimilation window is then whether it's sufficient to reach to make the link between the emission source and the observation side. And if you are very And if you are very um narrow with this, then you only have there must be this Kuhl-Penny's labor criterion satisfied that you get spurious oscillations from the numerics as a very small signal that you have this is an extreme example that you have some indication of the emission source, but surely this is. Emission source, but surely it is not reliable. But if you have the full transport here, then you can work with this and extract some data here. So in this case, it's quite in between that you have for the degree of freedom of sigma for the initial values, we have nearly a half, and here a little bit more than a half in this case. So it would work. And so in this case, the time of In this case, the time of infection is not a problem. We just made one test, which is continuous emission here, which does not vary over time. And the question is, if we have known diurnal cycles, can we extract some information from this to make a distinction? And in fact, there is some signal here. We have here the concentration part, which is a little bit smaller than here. Smaller than here, and we have that result which we wanted: that with the flat emission profile, we have some indication that the emissions are the critical part or they are even more, can be more better identified if it is clear on the cycle. On the time? Yeah. Five minutes? Okay. So, practical life. Practical life. Again, this is a settlement campaign which has the problem that we get a profile of very valuable data. These are far beyond the exactness of the accuracy of routing data. And this was operated in the vicinity of the headquarters of this. This was the Lake Constant in Germany. In Germany, and the work is done with the oil model, so we also have some legacy in contributing to the CAMS case where we developed this. We first started here with first ozone columns, and then we are part of the regional models in CAMPS. And the construction is here. I also do this in preparation of a talk for an anna later, so that she has not started with. Later, so that she has not started here at the very beginning. So we are driven by the Gwaf technology, not CHEM. Then we have an emission inventory, which gives us the background, and then we have the direct CTM, you have observations, and then we have the adjoint CTM. And in this case, the joint gives us the gradient, the initial rates, and emission rates are those which can give the result. And so finally, you can make the forecast. This is. This is uh second this is the first singular value of the profile. So you see the F the affection of the wind from this southern direction across the others and so you see the effective path. And so you see the effective part here over the target area here, where things should be observed with preference. And looking at the surface, you see here for NO observations, this is the background of NO as the emission inventory is given to us. And you see clearly here that, as expected, as it is background, that we have to observe it with preference here directly in the Preference here directly in the emission area, then there's something which is given by the vector, and then here is an emission area which is highlighted with the prevailing winds to make a better forecast on this area here. And this is for the ozone. It is not fractured by the emissions of the surface clearly. And so it is the background flow which is affected plus those which is formed. And prior to the arrival at the target area. So, these are some other details here of which species should be observed with some preference. So, the rating is clear. The most prevalent one is ozone. Then you have formalde, then you have CO, and then you have O no. And the least one is. And the least one is, and the most critical one, this is OH. In this case, they they were able to to measure OH in the field and it's not so easy. And then there's another example where it is important to observe ozone for prediction. So you have this area here, this is for the emission rate, and this is for HCOH. And this is for HCOHO and so on. And this can be done for the forecast time as you like. Okay, conclusions. So what I would say is that the quantitative, the objective quantification of algorithms proves the worst, so as a methodology, rather than qualitative estimates. We have to aspire better observability of uncertain atmospheric forcing mechanisms. So in this case, it was an example. In this case, it was an example by emissions, but also we have the deposition and other processes. And so it's not only the state observation which gives us the concentrations. We can also observe states with a focus on certain processes like emissions. And the other part is attention being driven by the weather. The chemistry network optimization is situation dependent. With these algorithms I gave you. With these algorithms I gave you, I showed you, there never was a concentration included. They only give the value by the configuration plus the weather situations. But they work without that you give any example concentration in them. So that's it. Happy to take questions if you have any. I have a bit of time for a question, but I don't know what you're doing. So, any questions? Yeah, remote phones, maybe. Can you go back to the slide? When the observations can be proper to get the estimate? So I remember there is a product of the hospital of knowledge.