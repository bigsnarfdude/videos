Kalak, can you hear me and see my screen, my slides? Yes, that's perfect. Great. So, thanks for the introduction. So, the system of interest that this workshop revolves around is Earth. And as we know, this is a complex dynamical system that features complicated dynamics, which you can measure in terms of time series. Being a method developer, my goal in this context is to develop and provide tools using machine. And provide tools using machine learning and statistics that support research in these fields of climate and earth system sciences. My colleagues and me from the causal inference group at the DLR Institute of Data Science in JINA developed methods based on the modern causal inference framework and implement these in open source software concretely in the Python package Tigramite that is available on GitHub so that domain scientists can so that domain scientists can apply our methods. And with some of them we are also working together in collaborations. Let me very briefly recall from a bird's eye point of view what causal inference is about. So foremost it goes ahead to specify a framework that defines effect and cause, cause and effect causation in a mathematical framework, which then allows to cast questions. To cast questions of the type: does a certain variable cause another certain variable? And if yes, how large this effect is in this mathematical framework. It then goes ahead to specify assumptions that connect the realms of statistics where we have correlation and dependence with causation. And based on appropriate enabling assumptions, it provides methods that allow to answer causal questions from data. For the purpose of For the purpose of this talk, I will be interested in the question marked here, which concerns learning qualitative cause and effect relationships, which is known as causal discovery. The information about these qualitative relationships can be summarized by what is known the causal graph, an example here on the right-hand side, where the vertices represent variables and the edges represent causal influences. And causal influences. So you can look at it as a type of Bayesian network that also conveys causal information. And they play a central role, these causal graphs, in the graphical model-based approach to causal inference, because these tell you how to estimate causal effects from data. Now, causal discovery per se is defined and also works without reference to time. But for the purpose of this talk, I will be interested in causal discovery. I will be interested in causal discovery in time series. Why? Well, because time series is what you typically have in many scientific applications. Then, what is special for causal discovery in time series? Well, first of all, by definition, we observe variables at different time steps, and we put ourselves in a discrete time scenario, meaning that we're interested in reconstructing a causal graph where we have several words. Graph where we have several vertices per variable, namely corresponding to the individual time steps. Another important feature is autocorrelation, very prominent here in this talk, which graphically translates to these blue-colored edges here. I also want to remark that I will be making an assumption of a stationary causal structure. Graphically, this means that the edges are repetitive. The edges are repetitive in time, as illustrated here again by blue colour. Also, I allow so-called contemporaneous edges. These are edges between variables at the same time step, again colored blue here. These correspond to causal influences at time scales smaller than the sampling interval. As a side remark for the experts, because I allow contemporaneous edges, the cost of The causal discovery problem as framed here actually formally subsumes the IID non-time series causal discovery problem. Now, however, you encounter additional statistical challenges, mainly due to autocorrelation. So the causal discovery algorithms that I'm mostly talking about belong to the so-called constraint-based class of methods, which utilize conditions. Of methods which utilize conditional independencies in the data, which are imprinted into the data by the structure of the causal graph, and hence you can infer at least parts of the causal graph from these independencies. However, due to autocorrelation, the statistical tests of independence tend to be ill-calibrated and have a low detection power. And as a result of that, standard algorithms, which are not tuned to the time series setting, often have a bad statistic. Often have a bad statistical performance on time series. This is where previous work of ours comes in. Namely, we have provided several causal discovery algorithms that are specialized to time series and to a large extent alleviate these statistical problems. These are the PCMCI algorithm by Runge et al., then the PCMCI plus algorithm by Runge and latent PCMCI or short et al. Latent PCMCI or short LPCMCI by Gerhardos and Roman. All these algorithms are part of Tigromite and they're at different stages of development with the more recent one making less stringent assumptions. For example, PCMCI doesn't consider contemporaneous edges and it disallows unobserved confounders. PCMCI Plus still disallows unobserved confounders but does consider contemporaneous edges and latent PCMIs. Temporaneous edges and latent PCMCI, as the name suggests, then moves to the latent causal discovery problem. That is, it also allows unobserved confounders. Now, let's take a closer look first at PCMCI plus and a previous performance study here taken from Runge 2020. Now, this complicated graphics, I don't want to talk about all of it, but the essential points are that on the x-axis of the individual side. x-axis of the individual subplots, you see autocorrelation strength. It increases to the right. And the six subplots then are different performance measures. In the upper left, we have the true positive rate regarding adjacencies. And there we see that the performance of PC, the famous PC algorithm, strongly degrades with stronger autocorrelation, meaning that it isn't able to recover many of the true links. Recover many of the true links. This is so, although it does not control false positives at the desired level, as you specify by the significance level that you pass to the conditional independence test. PCMCI plus resolves these issues, which we see because it has much higher true positives regarding the adjacencies. And in fact, for autodependency links, the true positive rate indeed grows with autocorrelation. Indeed, it grows with autocorrelation. So it's helpful, in fact. And the same is true for the recall of the orientation for contemporaneous links. Their autocorrelation helps indeed. At the same time, false positives are controlled at the desired level by PCMCI ‚Åá. Now, this study also involved two other methods, Varlingam, which we will be talking about later as well, and Granger Causality with then a receiver. Granger causality with then a residual PC, so PC algorithm on the residuals, but I don't want to go into these details here. Here are the references to all the methods. Now, latent PCMCI, as I said, this considers the latent problem where some time series can be unobserved. And then what you need to do is, at least that's the approach conventionally taken, is to consider. Is to consider a larger type of graphs, in particular graphs with bidirected edges. And then there are methods that are able to still perform causal discovery, although you have unobserved confounders. And one of them for time series is LPCMCI, which has also been evaluated numerically here in comparison to the previous state-of-the-art SVAR FCI. FCI. The story is pretty much the same as for PC MCI Plus versus PC, meaning that the baseline SVAR FCI has strongly degrading performance with autocorrelation regarding the true positive rates of adjacencies, for example, and it doesn't control false positives. Whereas our algorithm, LPCMCI, does control false positives and has much higher true positives regarding the adjacencies. The adjacencies and also the recall of orientations. Now, there's much more that I could and would like to tell you about this, but for the purpose of this talk to take away from this discussion is that we have several methods for constraint-based causal discovery in time series that work quite well. However, what we did not yet do is a systematic study of their performances for models with heavy For models with heavy-tailed noise distributions. And that's what I'll be talking about here today, what I prepared for the purpose of this talk, namely a numerical study of these performances on synthetically generated data. The general setup is that I consider many different experimental configurations, which means I consider models with different number of variables. models with different number of variables, different number of observed time steps, of possible time lags, and so on and so on. More on that later. For each configuration, I then randomly generate many models and for each random model apply the considered methods and evaluate their performance based on some metrics. I then aggregate these metrics statistically over the set of randomly generated models and put this into plots which can Plots which can then be analyzed. Now, to the details, first regarding the considered models and, in relation to that, the experimental configurations. To this end, I need to tell you about the graphical structure and the functional structure. First, regarding the graphs, the causal graphs, these are of the type, as we saw before, which are DAGs. Which are DAGs which have sort of repetitive time structure. And then they come with different properties, namely, for example, the number of component time series. And this is part of the experimental configuration. So I consider models with varying numbers of component time series. And the same is true for the other quantities that are colored red here. So first, the density of links and the maximum. Of links and the maximal possible time lag that I allow in the considered graph. Across all configurations, I fix the proportion of lagged versus contemporaneous edges to 3 to 1. And in addition to acyclicity, I demand that all lag1 autocorrelations are present. But apart from these two constraints, the edges are all drawn independently and uniformly. Drawn independently and uniformly of each other. Here is one example of a randomly generated graph where the maximal possible time lag is three and hence we consider four times a window length of four time steps. And there are three component time series, three variables. Here is another example where we have five component time series. Again, Again, a remark to the experts. The precisely stated causal discovery task is now to learn the CPDAGs of these graphs, or these random graphs. Now moving to the functional structure. On each randomly generated time series DAC, I impose a randomly generated structural causal model. All of these are linear, so that's a restriction that I do here in this study. Do here in this study, and this means that we have one parameter per edge, so-called edge coefficient, corresponding to the respective coefficient in the linear function. These edge coefficients are also drawn randomly, where I distinguish between lag1 autocorrelation and all other links. Lag1 autocorrelations are uniformly drawn from A minus 0.3 to A, where A is a number that I give by A number that I give by means of the experimental configuration, so it's varied and it ranges from 0.8 to 0.99. All other edges are uniformly drawn from this interval. Regarding the noise, this is both serially and mutually independent, and the distribution is constant in time but can vary across the different variables. And now to the And now, to the important aspect which puts the simulation study in the context of this work, namely, I consider, and for that, I've taken inspiration from a paper by Mijko et al. I consider t-distributions as the no distributions with different, well, with randomly drawn degrees of freedom from this interval here. So df, the mean degrees of freedom, plus minus 0. Freedom plus minus 0.2. And this df parameter, this is again part of the experimental configuration, which is varied from 1.5 to 100. So from a scenario where the variance is infinite to a almost Gaussian scenario. And that's the main parameter of interest: how the methods perform when this parameter is varied. As for the methods, As for the methods, I consider PCMCI plus, PC and var lingam. I did not consider latent PCMCI because for the purpose of this study and this presentation, I didn't want to overcomplicate things by adding this additional complication of unobserved confounders. So, yeah, I should probably say that Waalingam, this is not a constraint-based method, but rather a symmetry-based or strong. symmetry based or structural causal model based method which inherently uses non-Gaussianity of the noise in linear models to orient links. More remarks for the experts. To all these three methods, I give the information of the maximal possible time lag. And as independence test for PCMCI plus and PC, I use a partial correlation test with a T-star. correlation test with a t-statistic, even in the case of non-Gaussian noise or not even necessarily, not even closely Gaussian noise, with a significance level alpha that is again part of the experimental configuration which I give as input. Now to the perhaps most important aspect of the study, namely how do I randomly generate? What is the protocol? So for each given experimental configuration Experimental configuration, I attempt to generate 250 random models. And in order to do so, for each of random model, I try to generate stationary data 50 times at most. To this end, I generate first a random DAG, then a random structural causal model on this DAG. And with this structural model, causal model, random data with 10,000 plus T time steps. Plus t time steps, where t will be the number of observed time steps, which is also part of the experimental configuration that is varied. I then subject this data to a stationarity criterion, and if this is fulfilled, then I remove this initial transient period of 10,000 time steps, and this is then one of the 250 random realizations of the same extra. Realizations of the same experimental configuration. How do I test for stationarity? This is very simple and heuristic. I just regard a time series as stationary if it doesn't contain an absolute value larger than a million. And that's why also I chose a rather large number of transient time steps, 10,000. So at this point, you'll probably hear alarm sounds ringing. So let's discuss briefly. So let's discuss briefly. First of all, I apply this stationarity criterion after random model and data generation. So the question naturally is, does this introduce selection bias? So a question would be, do I automatically filter out the particularly extreme realization so that effectively, by this selection procedure, I make the problem again more simple. A second point, it can happen. A second point: it can happen that I do not find a stationary model after 50 replications. So there will be, and there are experimental configurations which end up having fewer than 250 successful realizations. But this isn't so much of a problem because it just means that the error bars in the end will be larger. Well, regarding this criterion for stationarity, I'm totally aware that it's extremely simple, probably overly simple, but hence, I wanted to post. But hence, I wanted to pose actually a question to the community here because the question is: Are there, and I suppose there are, are there analytical results on the stationarity of linear processes when there are contemporaneous edges and or the noise, the innovations that do not have a finite variance? So, if so, it would be interesting to be interested to hear about that. Then I could refine the study. Refine the study. Okay, before I move to the results, some sample data. Here, a case with five time series with mean decrease, a noise decrease of freedom 100, so an almost Gaussian regime. This looks quite okay. Then we move to the case with 2.5 mean noise degrees of freedom, where the data already looks much more jumpy. For example, here. Jumpy, example here and here. And then moving to the case where the variance is infinite, where in time series one, for example, we clearly see an extreme noise realization, which is then driven back to the mean value. Yeah, so this looks all quite plausible. Now to the results, where in light of time I can only show very selected aspects. Selected aspects. First, I'm reconsidering what has already been done. So I consider the, in this case, almost Gaussian regime for models with five component time series, maximal possible time lag of five and 500 time steps and significance level 0.01 for the partial correlation test for PCMCI plus and PC. On the upper left, we rediscover that the performance of PC. Cover that the performance of PC significantly drops with increasing autocorrelation, which is on the x-axis, while again false positives are not controlled. And it has the longest runtime. PCMCI is much better. It controls the false positives and still has much higher true positive rates, especially regarding autodependency and contemporaneous links. Despite finding almost all contemporaneous Finding almost all contemporaneous links, also the accuracy of contemporaneous edge orientations is quite high. And moving to Walingam, it also has very good adjacency to positives rates, slightly lower than PCMCI plus for the contemporaneous edges, but significantly higher and almost perfect for laged links. However, the price to pay are much higher by a factor of 10 false positives. And yeah, I. And yeah, I just didn't find a parameter in the method as implemented in the Lingam package to tune the false positives. So that's just what I got. Also, the accuracy of contemporaneous edge orientations is quite low, which isn't so surprising because the method uses non-Gaussianity of the data to do the orientation, and we are in the almost Gaussian regime. So this pretty much meets the expectations and what we have previously seen. And here this is just a write-up of what I've just said in words. So being reassured, we can now move to what we're actually interested in, namely the performance as a function of how extreme the tails are, how heavy tailed the noises are, which I do by plotting the mean noise degrees of freedom on the x-axis. Noise degrees of freedom on the x-axis with the All-Masgausian regime on the right and the very extreme case on the left. Starting off with Walingam in the upper right, we see that the contemporaneous orientation accuracy increases. And this is very much expected because the data becomes more and more non-Gaussian. And that's what the method uses for orientations. Also, the true positive rate of adjacency growth, it is already very good all of the time. Already very good all of the time, but it still grows. However, again, we have, I mean, the false positives are slightly decreasing, but they are still factors larger than for PCMCI plus. PCMCI plus, and that's what surprises me, is that despite using a partial correlation test with T statistics, the performance doesn't drop a lot. Drop a lot until 2.5. And then in moving, so the most significant drop in the performance comes when we move from 2.5 to 1.5. So going to the case where the variance is infinite. And by the way, this is again for models with five variables and a maximal possible time lag of five and 500 time steps. So then I also. So then I also looked at larger graphs, so 10 variables, maximal possible time leg seven, so larger graph and fewer samples, only 200 time steps. In order to see how Waalingam behaves when going from 2.5 to 1.5. And what we see there is that also Wa-Lingam suffers then when moving to the regime of infinite variance. That's also something that I did. That's also something that I didn't expect. And a natural question is: is there somehow expected that this step, that sort of going beyond the threshold of finite variance, that this makes the problem particularly challenging? Again, that's just a write-up of what I said. So pretty much expected, but Expected but for Walingam, except for this performance drop when moving to the infinite variance case, and also another surprise that PCMCI plus has almost constant performance as long as you have a finite variance. Although I use a t-statistic. There are much more things to look at, to do and open questions. So first of all, it would be good to more systematically analyze. Good to more systematically analyze also more experimental configurations. For example, also look at non-linear models and try different independence tests. Also interesting to know would be whether the performance or how the individual independence tests perform in the heavy-tailed scenario, especially whether the loss of false positive control that we see saw with PCMCI Plus for PCMCI plus for mean degrees of freedom 1.5 is already seen there, and whether this drop in performance can be fixed by using a set transformation or using permutation testing, for example. What remains is the question of whether this selection of stationary models introduces selection bias that somehow jeopardizes the results of the study. And of course, it would be very good to compare with algorithms. Would be very good to compare with algorithms that are specialized and use the heavy-tailed noise, such as extremal ancestral search by Njeko et al., I think our chair of this session, which we will also hear a talk about, I think, tomorrow. Yeah, and a vaguely phrased question towards the statisticians. What does autocorrelation do to heavy-tailedness? Due to heavy-tailedness of the noise. So, could it be that the resulting stationary distribution is somehow less or more heavy-tailed than the noise distributions of the innovation terms. All right. Yeah, I think I'm almost done with my time, but I'm also done with talk. So thanks for the attention. Of course, I'm happy to hear questions, comments, remarks, ideas, and so on. comments, remarks, ideas, and so on.