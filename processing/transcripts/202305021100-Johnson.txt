I want to thank Carolina for doing all the work, and Mark did some as well. Mark Saltmaire. His name was always on the email, Fernando, but all the emails were from Carolina. So muchas gracias. All right. So there we go. Got it started. I'm going to talk about a subayan analysis of task-based fMRI for pre-surgical planning. For my pre-surgical planning, some research I've been doing off and on for about 10 years or so with some students, and some of it was with Tom Nichols as well, and with a collaborator I'll introduce later, Andreas Bart. Oh, you forgot Zoom. Okay. Uh-huh. Now this isn't working. That's okay. I'll just the keyboard. Yeah, no, no, no problem. Let me just do it this way. So the outline, I'm going to give a little bit of background of what pre-surgical F What pre-surgical FM pre-surgical entails. Talk the three main existing software packages, then I'll do some comparison and contrast between what I'm doing modeling and what they do. And I'll use what I call the spatially adapt time-varying autoregression model. Then show some results in references. So over the past decade or so, there's been a growing interest in using FI to aid in pre-space. Interest in using aid in pre-surgical plant. And I'm going to go right here. So, electrical stimulation mapping is usually performed to map the functionally eloquent areas of the brain. And so, what they do is the surgeon will put the patient, you know, the anesthesiologist put the patient asleep. The surgeon will expose part of the brain where they're going to start the surgery, and then they wake the patient up. They're heavily sedated, so I don't know if it's really stressful for them. So, I don't know if it's really stressful for them. I think it would be stressful. And then they take this electrical probe with a positive and negative node, and they pass a little electrical current through that area. And when they're doing this, they'll maybe, if it's like a word association or a noun recognition type of a task they're looking for, they might put up a picture of a star and you say star. And they'll put up a picture of a sun and you say sun. And they'll be moving this around. They put up a picture of a moon and you don't know what it is. You can't record. And you don't know what it is. You can't recall the name moon. So then the surgeon will put one of these little markers there. And after they do that, they have to navigate around these things as they cut through the cortex of the brain to get to the tumor to resect it. Okay. And these surgeries can take a very long time. They're probably stressful for the surgeon. The patient's awake, it's sedated, but still, I think it would be stressful, especially when they touch part of your brain and you start stammering or something like that. Start stammering or something like that, you can't talk anymore. Um, so the this was from a 62-year-old woman who presented with difficulty speech, doing speech talking. My collaborators in Germany, so this is a woman in Germany, and you can see here the icleoblastoma, and there's some edema around that. So, that's the tumor they want to resect. They want to resect. And it's in the pimple lobe where your language areas of the brain are. This is just called the Wernacky-get-Jeshwin model. So if you hear a question, you hear it through the auditory cortex. It goes Wernicke's area, you do some processing. Don't know what all these areas do. Broca's area, more processing. And then your lips, tongue, and mouth have to move through the motor cortex is involved. And if you read words, you have to see them first. You have to see them first, the visual cortex, and then this is the angular gyrus processing back to Wernacy area to brochus area and the motor cortex. The one tumor is right about here. Everything's distorted. So when I show you final results, I'm not exactly sure where the angular gyrus is or where the Wernix areas because everything's distorted from the tumor. So these are the areas you want to stay away from. And they have a general idea where these areas are. Where these areas are, but everybody processes these things in slightly different areas. And if bilingual or trilingual, you probably have larger areas you use. You've used more of those areas to learn the different languages. Okay, so the main software packages that are available, I'm sure most of you have heard of them, SL, SPN, AFNI, they're all free given the links. They're all free given the links if you wanted to download them, and they all perform massive univariate analysis, which means I'm sorry, I had some slides about fMRI, but my talk's too long, and I figured most of you know fMRI. So at each voxel, you have a time series of data, and they do a regression on that time series and don't do anything spatially. So you have this massive univariate problem, and you have to. A variative problem, and you have to do some type of familywise error, control the family-wise error somehow. So, for the differences from the standard models compared to mine, temporal correlation, usually use a fixed order AR process. I'm going to use the time-varying autoregression process, and the order of that is going to be a random variable. So, each box can have its own order. Typically, it's from one to four. Or typically it's from one to four. For the model errors, they assume they're spatially independent. I am going to spatially correlate those errors via the prior parameters of interest. They assume they are spatially independent. I'm going to spatially correlate those with the spatially adaptive autoregression. Low frequency drift, you get some drift from thermal noise and other things in the machine. Noise and other things in the machine. They typically use a truncated discrete cosine basis with a low frequency. They just want to get the, you know, very slow. I'm going to use the B-spline bases, and my B-spline bases are going to have random locations and random number of knots. And I'll explain why I like B-spline bases a lot, as opposed to smoothing like were used yesterday. More time, that's one downside. That's one downside. And then for the hemodynamic response function, it's usually fixed. So they'll use the canonical HRF or a gamma HRF. And a lot of times they will include a temporal and dispersion derivative. Andreas, my collaborator, uses FSL and they only allow temporal correlation, temporal derivative, not a dispersion derivative. I'm going to use a I'm going to use a gamma HRF with hyperpriors on the parameters of the gamma. A lot of pre-processing typically goes into analyses. The only pre-processing that we do for this model is going from K-space out of the machine and transforming it into Euclidean space. So you have your image. Okay. And I might be wrong, but I kind of think of all this pre-processing. But I kind of think of all this pre-processing as we have a model, let's fit the data to it. And I'd rather take the data and develop a model that fits the data better. And then inference, they either use random field theory. You heard about that from Mu and Armin. Sorry, Armin. Or Or they'll use the false discovery rate to control the familywise error. I'm going to take a decision-theoretic approach where my loss function can put different weights on a false positive or a false negative. And in pre-surgical fMRI, false negatives are very important because if there's part of the brain that you say there's no activity, they cut through it, you could disable the person or do worse damage. Damage. And so I'm going to be able to control the false negative rate as well as the false positive rate. There's some alternative Bayesian models. Chris Genovese in 2000 had a JASA paper on time course modeling for functional MRI imaging. We have some similarities, but he didn't do anything spatially. And his errors, he assumed independent errors over time, and they're not independent. Over time, and they're not independent. There is a spatial-temporal non-parametric Bayesian model, but that was for multi-subject fMRI. I put this in here because I knew Michaela was here, but he left. He didn't want to hear my talk. No, he had to go teach. And then with Marina Bunucci, who I saw was on Zoom yesterday. I don't know if she's here today. And then last week, I found this paper by Cardonia Jimenez et al., where they use a multivariate dynamic linear model. A multivariate dynamic linear model approach. And that's what paper that I do want to compare my model to. I had this idea in the back of my mind. I tried it with a single voxel, so it wasn't multivariate. And for FNIRS data for a single channel. And I had very large credible intervals for the parameters of interest. It was difficult to do. So I want to compare my modeling to theirs. Compare my modeling to theirs. Anyway, those are some alternative Bayesian approaches. I'm sure there's many more. And I know FSL has a Bayesian module in their software if you wanted to do that, but it's basically just the frequentist model put in a Bayesian framework. All right, so I'm going to go into the likelihood now and then I'll introduce the priors. So one component at a time. V is a jerk voxel. This holds for all voxels. And if there's This holds for all voxels. And if there's a subscript on a parameter, it means it's specific to that voxel only. So we have our time series data. And here's just an exemplar example of a time series. I standardize all my time series because of the scaling differences from voxel to voxel and MRI data. And below here is the boxcar design. I think she did the same task eight times. So the task she did was, she was reading what's called. She was reading what's called embedded non-final clauses. I'm not, I don't have time to explain what that is. I asked Andreas, so he explained it to me. But I guess it works better in German and Dutch because of the syntax than it does in English. But it's supposed to activate all the language areas of the brain better than other tasks. So the first part of the likelihood is. First part of the likelihood is the design matrix multiplied by the parameters of interest beta. So x is the design. It's just if you have three different tasks, you convolve with the HRF and then you get three columns for the vector X or for the matrix X. The next part models a low frequency drift and it can also using the beast blind bases it can model very fast amplitude changes in the signal or if you have a sharp Or if you have a sharp increase and then it flattens out, you can model that as well. With b-spine bases, and like I said, the number of knots and locations are random variables. And so this is just the posterior expectation of beta eta for this particular voxel in this data set. And then I have the time-varying autoregression part of the model. So r is the p-vector of lags at time t. Of lags at time t. Just the lags, and then Rv star is just y at time t minus x beta at time t minus b eta at time t. And of course, phi is the p vector of the time varying autoregression coefficients at time t. So it can be a vector of length p. And note that it does depend on time, so these parameters can change over time as opposed to a stationary AR process. AR process. So this is just the posterior for this particular voxel of R phi over time. And then lastly, we have our errors, and I'm assuming that they are independent over time. I'm assuming that R transpose phi takes care of all the autocorrelation. So these errors are independent over time. And this is just the residual. And this is just the residuals from this particular voxel. Autocorrelation takes care of most of it. And then the normal QQ plot looks to look like it's working pretty well. Okay, so priors. We'll start with the model variant since I ended there. I'm going to use a conditional auto-regression prior on the variance. I'm going to call it a V-car because VSOG's conditional auto-regression was always on the space. The spatial parameters. This is on the variance. So it's going to depend on the variances of neighbors of this voxel V. It's going to be inverse gamma, Nv plus one over two. And then the second parameter, Nv Sv. So Nv, if I didn't have this one in here, it'd be a scaled inverse chi-squared distribution. And Nv we call the prior sample. So I'm going to stick with that terminology. Terminology. And I think that should be a two actually. SV is a weighted average of the variances of the neighbors of the sky. And I'm using a first order neighborhood system. So if you had a cube for a voxel, the neighbors are the six voxels that have a common face with that. You could use the second order or third order as well. And these weights have to sum to one. So the data were The data were acquired sagittally. So within plane, the voxels are three millimeters apart, but between planes, they're three and a half. So these weights are proportional to one over the distance between the voxels. And I did this so it gives an a priority mean equal to the weighted average of the neighboring voxels. In previous work, when we only worked with, we didn't have. We only worked with, we didn't have time series data, only a spatial image. My student Juqing Liu and Veronica Baracol, my collaborator or colleague, we use the log normal prior in previous work. I like this prior better because I can use Gibbs updates and I don't have to worry about tuning an MCMC, the parameter and the MC. So, in order to update the full conditionals of these. To update full conditionals of these, it's going to require the intractable joint prior of our betas. So we can't do that where we resort using VSOG's pseudo-likelihood approach, where the pseudo-likelihood is just the product of the conditional priors of these data. Not the true likelihood, but VSOG showed that this works very well. And I think he had some convinced theory all in there. All in there. Running out of time. So HRF, gamma. And then I put priors on the expectation of the gamma and the variance of the gamma. And a prior, if we plug in six and six here, it's equal to the gamma HRF in standard software. For the beta V, I put what's called the G-Car prior on it, and I will explain that in a minute. We need to rewrite. Explain that in a minute. We need to rearrange the likelihood a little bit. So I take the time-varying r times phi. I take part of it and put it on the left-hand side. And then there's an x-beta in that times phi. So I can rewrite this thing. I dropped the subscript v here. I put it back. I get y star equals xv. It's a function of phi times beta v plus epsilon. So now I have just a standard regression problem. Okay, so that's the regression. Okay, so that's the regression problem. And then my G-car prior depends on the parameters of neighbors, same neighborhood system, same weights. So mu V is the weighted average of the neighboring parameters. So this is the BSOG's car prior. And then I put Zellner's G prior in for the variance. Whoops, sorry, tripping here. So it's combining BSOG's car prior with Zellner's G prior. SOG's car prior with Zellner's G prior. There's been a lot of work on putting priors on this G in regression problems. A lot of them are, you can find them in Leong et al. I'm going to implicitly define the prior on GV by letting Pv equals GV over 1 plus GV. And this is called the shrinkage parameter. And then we put a beta prior on this parameter P. This parameter P. I'm going to use a uniform prior, so beta 1, 1. Again, I need to use pseudo-likelihood and metropolis Hastings to update G, and then I can update B with just Gibbs samples. All right, so the reason I like this G-car prior is because the full conditional has a nice interpretation. It's a weighted average of data. So this you can just think of as the MLE. It's X transpose. MLE. It's X transpose X inverse, X transpose Y star, and the shrinkage prior, the average of the neighbors. So this spatially smooths and correlates the parameters adaptively, depending on the value of P. And it borrows information from neighbors, which helps reduce variability in my parameters, both spatially and each beta, the variance is reduced. Almost done with priors. So, for the beast line basis, B, I put a prior on the number and locations of those knots. So, the prior on the number is just discrete uniform up to K max. And then, given K max on the locations, it's just uniform from one to t. And I'm going to use parabolic splines as opposed to. And I'm going to use parabolic splines as opposed to cubic splines. They fit better for some reason. I'm not sure why, but so this is a parabolic spline bases where I have knots equally spaced between 0 and 400 and one out here. And I can get a nice smooth curve. I just drew normal 0, 1 coefficients for this basis to get this curve. It'd be hard for a discrete cosine basis to get this type of curvature and then straight lines. Type of curvature and then straight line like this. If you had enough bases, you'd have oscillation along here. If I take a knot, say at 150, and I co-locate it with the one at 200, so I have two knots at 200, my bases change a little bit, and I can get a function that is not differentiable at 200. So you can get non-differentiable functions still continuous. But if I take a third knot, But if I take a third knot, move this one from 250 and put it at 200, a few of the bases change. Another thing I like about these is they're locally compact and it can speed up the algorithm because you don't have to do it. If you're going to change a basis, you don't have to change everything. So I have three knots co-located and I can get a discontinuous function. So this will help me if I have any large amplitude changes in the signal. I can pick those up with B-spline bases as long as you have random. As long as you have random knots, because I can put several knots close to one another and get almost a discontinuous function. And since I have a prior on the coefficients, it doesn't matter if this thing is not full rank anymore because it's just like doing a ridge regression. And I put a prior on eta. Again, I'm going to use Zellner's G prior. Theta V, phi of V has the same point. Theta v has the same form as x down here. And here, I tried to put a prior on g, but it didn't seem to be any information in the data to actually get a pull it away from the prior. So I set g equal to the number of time points in my time series. And this is what Cass and Wasserman call the unit information prior. So it's like the information that you'd have from one subject or one data point. And then I'm going to go quickly here. And then I'm going to go quickly here: the T-var coefficients. I need to rewrite the likelihood a little bit, and then I cast it into the dynamic linear modeling framework. So I have my observation equation, system equation, and then at P sub V, I have to have a prior. And then I use the forward filtering, backward sampling algorithm that was developed in the 90s. That's very fast. So it's a Gibbs up digital. It's not a Metropolis Hastings. It's not a metropolis hastings, you just update the entire vector. Well, each of these vectors over all time at once. But you do it by the conditioning. This would be a slow algorithm if I couldn't do some parallel processing. So I just use the cores on my laptop. I use 18 of them. And I can draw all the parameters. Can par I can draw all the parameters in parallel in two steps. So, in this checkerboard design, if this is a pixel, the neighbors would be the red. So, all the blue pixels are conditionally independent of the red. So, I can pull those out and update everything, all those blue pixels or voxels in parallel. And I put those back in the second step, I can update all the red independently of one another, conditionally independent of one another. All right, I only got a All right, I only got a couple minutes left. So, my loss function, I'm just going to explain the terms, and then I'm going to go through this real quickly. So, R is the true state. So, zero indicates an all-voxel. One would be deactivated or activated voxel. So, the loss function decreases by one for a false, true positive, decreases by one for a true negative. It'll increase by a positive It'll increase by a factor k1 for a false negative and k2 for a false positive. So you can change k1 and k2, and you can differentially weight in how much a false positive or a false negative incurs a loss. Okay, so I just don't have a probability that the true true True, the state of the voxel is positive and negative. So from a paper by Peter Mueller, Giovanni, Parmigiani, and Ken Rice. They suggested to use a function that's monotone, and then M is just the posterior expectation of the parameter divided by the posterior squared of the posterior variance, and then the absolute value of that. And I've been working with Andreas for several years, and roughly about false positive, false negative should incur a loss about five times as greater as a false positive. So that's what I'm going to use for the loss function. So I'm just going to show one voxel here, and this was in the angular gyrus. I'm showing the posterior expectation of the signal of interest. And it doesn't look too high, but I believe this was a box. But I believe this was a voxel that passed the threshold from the loss function. This is the posterior expectation of the low frequency drift, and you can see it can pick up this fast change in amplitude. In the FNIRS data I've been analyzing, I found out that sometimes when you have these large amplitude changes, depending on where they occur, they relate in relative. They relate in relax in relation to the task, it can bias results if you don't pick this up. This is the posterior expectation of the time-varying signal, and this is the first element of phi over time. So you can see that it changes over time. And I've looked at several hundred of these, and almost all of them change quite a bit over time. So you can see it's by positive correlation, very little, back to positive, very little. Back to positive, very little, back to positive, and little, and then a little bit negative. But and all the back, all these voxels I've looked at, they do have this time-varying component to them. So I think it's important that you don't use a stationary autoregression. Okay, so we got a last minute. I can finish this up. I have several slides here showing the sagittal slice going from lateral to medial. Lateral to medial, so from here. And so this is the beginning of the temporal lobe. I'm comparing with FSL using FDR controlled at a family-wise error rate of 0.05. And I mean, I was happy that we got similar regions of interest, but mine tend to be bigger because I'm putting a larger, I'm controlling for false negatives and I'm putting a fairly large weight on those in the last function. Okay, so now you can start seeing the. Okay, so now you can start seeing the tumor come in a little bit in here, a little bit more. So now here you can see back here is probably the angular gyrus, and here you're getting the wornx area. I'm not sure what this is, and up here, probably Broca's area. This tumor is distorting the temporal lobe so much it's hard to tell what is where. So you might think, well, my decision-theoretic approach loss function, you can do that in a frequency framework. So I took the results from FSL, the Z statistic image, and applied the loss function with K equals K1 equals 5, K2 equal 1. And it didn't look good at all. I had to go and the false positives I had, false negatives. False positives, I had false negatives, I had to wait with a factor of 15 in order to get something that looks similar to this image. And Andreas looked at these and compared them to the electrical stimulation mapping, and he seems happy with the results. This is the last slide. I know I'm a little bit over. So I compared with other Bayesian models. I took what I call Bayes FSL, mass univariate, just the Bayesian. Just the Bayesian FSL model. And I gave the DIC, the model deviance, and the complexity of the model. And then I smoothed instead of just using mass unit variant analysis. And my model with K max, that's the maximum number of knots in my B-spoin bases. I chose 8, 16, 24, and 32, 5% of the number of volumes, which were 160 over time. Where has 160 over time, 10%, 15, and 20%? And these two are probably pretty interchangeable. So I chose the model that was more parsimonious. And you can see that these three models have less complexity, but the model fits much better. And overall, the DIC is much, much smaller. This is per voxel, so you multiply by. Voxel, so you multiply by 41,808 voxels. I didn't want to show that large DIC, but you can see that per voxel it's you know about 40 times larger, sorry, 40 points larger. And this is my collaborator, Andreas Bart. She's a consultant neuroradiologist. I think he has his own company, but he does have affiliations with the University of Heidelberg and the University of Oxford. He uses a lot with the fMRI. Oxford uses a lot with the fMRI B lab in Oxford. And just a bunch of references. And thank you. Don't have much time. Sorry.