Yes, so this is going to be a joint work with James Melbourne and Sir Luberto. And it's going to be about minimizing entropy for low concave random variables when the variance is fixed. So the talk is going to be quite serious. We will have maybe 13 slides, no pictures, no jokes. Okay, so let me give you some background. So actually it's not very complicated. Well actually it's not very complicated in terms of definition. So you have a variance, which you all know. The entropy, standard entropy of random variable X with density F is this one, minus integral F log F. And then you may also consider the Reni entropy of order alpha. This is defined for alpha not equal to one. For alpha equal to one, if you take the limit, you recover this. Take the limit, you recover this. So, I want to minimize entropy under fixed variance, but also, of course, you can think of maximizing entropy under fixed variance. So, this problem is well known. So, there are independent works by these people, and it turns out that there is this thing called generalized Gaussian density that maximizes entropy when the variance is fixed. Entropy when the variance is fixed, so it is of this form. So this is for alpha bigger than one-third, and for alpha smaller than one-third, this is not even bounded. And in particular, if alpha is equal to one, the maximizer is a Gaussian. Here, if you take the limit alpha going to one, you get a Gaussian. And in other words, you can phrase this. Phrase this extremization problem in terms of the inequality. This is a homogeneous inequality. If you rescale x, this doesn't change. So basically, this extremizing problem is the same as proving the inequality like this. And this inequality, I think it goes back to Gibbs or Boltzmann. This is a super classical thing. It doesn't follow from the Prosper Mielsen. Yes, this is an easy consequence of concavity of logarithm. Alright. So how about minimizing entropy under fixed variance? So if you do not assume anything on x, this is hopeless. Because if you take this function fn, so this is basically something that Is basically something that approximates the rather matter. This is one minus one, and you have a bump here like this. Then, as you can see, the variance is almost one. And then, if you compute these entropies, they go to minus infinity. So, you have to impose some restrictions on X. So, I think one of, yeah, it's a convexity conference. So, we usually assume that X is Assume that the X is log concave. And there is a theorem in the symmetric case. So these are actually two papers. One of them is joint work with Mokshai and Tomas, and the other part is joint work with my student, Majek Bjobzewski. So this is there is a full description. There are these inequalities. So there is a number alpha, this number alpha star, satisfies this equation. Satisfies this equation. It's about 1.241. And there is a phase transition in alpha star. So below alpha star, you have this optimal inequality with equality for a uniform on minus 1, 1. And above alpha star, you have this inequality with equality for two-sided exponents. So for this alpha star, basically. Alpha star, basically, these two constants will match. So, this is basically what we know for symmetric log-concave random variables, but now then there is a question, what do you do for general log-concave random variables? So, this is a theorem that we established this year in Paris with James and Cyril. This is for our This is for alpha equal one, for the Shannon and German. So there is this inequality. This is an optimal inequality. It holds for any log concern of the variable with equality for exponential. And also if you have this one by some monotonicity properties of entropy, you can also get this for alpha bigger equal to one. Okay? Also, of course, there's also Okay. Also o of course there is also a question what happens below one? So I'm not sure what happens. We we thought it is uh phase transition like uh in the symmetric case but I think it is actually not really the same thing. There might be more difficult extramizers for alpha small than one, but we I'm not sure. We are not sure. Okay? So I want to give you a sketch of the proof of this in the club. So the proof is 30 pages. It's a tough proof, but it has some ideas. I'll try to convince you that there are some ideas in the book. So this is one idea. This is a well-known idea. And all of these things are done. Idea and all of these things are done via this approach. Okay, so this is a well-known thing. So this is called degrees of freedom. So of course, as you know, if I add two log-concave functions, the sum might not be log-concave. So this is a problem. It doesn't have a linear structure. But these guys, Matthias and Olivia, they introduced this amazing definition. So they somehow introduced the local link. Introduce the local linear structure in the space of log concave functions. So we say that a log concave function F doesn't have to be density at this point, has D degrees of freedom if there are some linearly independent functions. They do not have to be log concave. And epsilon, such that if I perturb f with this thing for With this thing for small deltas, this new function remains log concrete. And so, in a sense, f has d degrees of freedom if locally around f there is a linear subspace of dimension d in the space of local concave functions. Again, this is a very cool definition. And also, what is important about it is that you can say what are the functions. Say what are the functions with, let's say, two, three, four degrees of freedom. So there is this fact, it's not very hard. If f has at most k plus one degrees of freedom, then this f is of course e to minus v, where v is convex, but v is something very, very special. This is a maximum of k affine functions. Okay, so this affine functions. Okay, so this, so if v is this, then the e to minus v has two, it has two degrees of freedom, this has three degrees of freedom, this will have four degrees of freedom. So what do I do with this? So let's say I have a complicated function which has at least four degrees of freedom. Freedom. Then I claim that in this inequality that I want to prove, I do not have to even consider this function. And why is that? So let's say I have a function, a locong function with four degrees of freedom. It means that there are these functions g1, d2, g3, g4, such that this for sufficiently small deltas is still locong. Still local. But now I want, so I have this function f, it has some variance, some entropy, but I want to force f delta to have, first of all, to be a probability density. So for f I assume that it's a probability density. So first of all, I want uh I want to force this f delta to be a probability density, and I also want this f delta to have the same variance as f. Have the same variance as f. So, how do I do this? This is very easy. So, first of all, I define these moments of gi's. And what do I need for this to be, let's say, a density? The integral of f is 1, so I have to have the integral of this guy here to be equal to 0, which gives me some linear equation in deltas, right? And then I can do the same for the For integral of x f delta and for integral of x squared f delta, and this leads to some other linear equations. So here I have three equations and four variables. And as we know, this system of linear equations will have a non-trivial solution. So, in other words, there is a non-trivial solution. Solution. So now let's take this solution F delta. So I can also consider F minus delta with minus signs here. And this is also a solution. And F is a sum of F delta and F minus delta divided by. So the entropy of F, so this F delta and this F minus delta delta. F delta and f minus delta, they have the same variance as f. But the entropy of f is the entropy of a meme, and entropy is a strictly convex function, strictly concave function. So this is strictly bigger than this, which means that one of these two has to be smaller than h of f. Okay? So once you know that this in this problem there is a maximizer, there is a minimizer, which is easy. Which is easy. This shows that f cannot be a minimizer. So, in other words, if f has four degrees of freedom or more, I do not have to even consider it. Okay? So, what do we do? So, I have to consider functions with three degrees of freedom or less. But before doing this, But before doing this, let me also present this additional ingredient. So the function that I have to deal with is f e to the minus v with v of this shape. So it can be monotone or may not be monotone. And it is crucial for the proof to assume that this function is monotone. Otherwise, Monoto. Otherwise, the things that we will be doing are not true. And there is a lemma that allows me to just assume that the function is monitored. So there is this concept of non-increasing rearrangement. There is a unique function, non-increasing function, such that this holds, which means that for any function Ïˆ, this kind of integrals will be the same, and in particular, this. And in particular, this this uh F F arrow will have the same entropy as F, but there is a lemma. So here for some technical assumptions, we'll do it on interval 0L. But if X is low concave, then it turns out that X RO, which has density F RO, is still low concave and the variance increases. Increases. This is maybe not very intuitive, but this is true and this is crucial. So, in other words, I keep the variance the same, I keep the entropy the same, but the variance goes up. So, it's enough to prove the inequality for this rearrangement. Okay? So, this is without this ingredient, I don't think this it is possible to do this process. So, now so it is my function right now. So, it is my function right now. This is my function. So, I already assumed that it's monotone, so here C is bigger or equal than 1. So, there are some, so here I assume that this inequality is invariant under rescaling and shifts. So, I can put this point at 0 and I can also make one of the slopes to be equal to 1 by rescaling. So, basically, this is what. So basically, this is what you do. You get this function, and this thing here is just makes this density. So now the question is, so during Professor Kenning's talk, there were some monsters. So if I have this function, I can write down my inequality, because it's enough to verify the inequality for this one. So here's my monster, okay? So here is my monster, okay? This is my monster. And we have to prove that G is positive. So what do you do? Well, this is kind of hopeless. So you can run away, right, and give up, or you can just closely look carefully. Because every monster has some weak points. Weak points. There are some weaknesses here. So, what is the weakness of this monster? So, the weakness of this monster is the fact that this part, so let's think of this as a function in C. So, this is a polynomial of degree 4 in C. So, I can kill this guy by taking fifth direction. Kill this guy by taking fifth derivative. But the problem is here. There is this function. If I start hitting this function, you can see there is this linear function in C, but there's exponent and there's C here in the denominator. This is going to be horrible. Okay? I'm not in a good shape. So, but here's a Here is a third crucial idea. So here I have this function, so here t is c here. So I have a linear function, the same function is here in the denominator, and there is some other linear function in the numerator. So this function is of this form for n equals 5. And I want to take the fifth derivative. And I want to take the fifth derivative. So in general, I want to take the nth derivative. And the question is, what is it? So this is going to something horrible this is going to be something horrible maybe. But here's a miracle. This is the nth derivative of this function. And I have no idea why this is the case. You can prove it by Taylor expanding this using some Leibniz rule. It's, I mean, I don't know. This is. I don't know. This is the f the nth derivative of this one. Okay. Let me do a remark, because in the previous paper with my student, actually it was his observation, which is C kind of similar to this. Actually, you can derive this from this fact. So it is true, the following thing is true. If you take this function of this form with some alpha beta, not necessarily. Alpha beta, not necessarily natural numbers, but if they sum up to n, then if you take the n plus one derivative of this, this has a fixed sign. And you can determine the sign in terms of these numbers here. So actually from this, you can actually get the fact that this has a sign. Okay? So what do we know? Well, I know that the fifth derivative has a sign. Well, that is very Well, that is very good because now I can try to go back and prove something for G. And so here is a list of inequalities that I need to prove. Because so this means that the fourth derivative is decreasing. So if I want to prove that the fourth derivative is positive, I have to prove that at infinity it is positive. Then g4 is positive. And if I prove that this is the case, then I prove that. Yeah, I go back, right? Yeah, I go back, right? I go back, go back. Kind of easy. Yeah, but these things, I guess they look still complicated. So all of them are of this form. Okay, because as you can imagine, if I start computing derivatives of this, I will get still this exponent times some polynomials in e to x and x and y and things like that, okay? And y and things like that. So all of these inequalities, this is kind of easy, but all of these inequalities will have this form where aj bj are some polynomials is xy into x into y. This is kind of problematic because of this guy here. Look, this is the exponent of the exponent of an. I don't want to deal with this really, right? This is kind of horrible. So the problem is to find So, the problem is to find an estimate on this. So, these guys I like because these are polynomials. Well, I don't like them, but they are much better than this one. Because these guys are polynomials in e to x and x, so there I can tailor, expand maybe something. But this one is bad. So, this is an example of how this looks like, because these things are not simple. So, for example, for G3, third derivative. For G3, third derivative in C. You have this A3, B3, and these are these guys, and you know, it's T squared, T cubed, and S T are these monsters. This is, you don't even... I mean, basically, you have to derive these things using mathematics. Because otherwise, I mean, if you do do it by hand, it's it will take forever, okay, to even derive this formula. So, my goal is to estimate this exponent in a smart way so that I get rid of the exponent of this exponent. So, here is a crucial technical bound. So, this inequality is true for any x and y. So, the question is: how do you cook up such an How do you cook up such an estimate? So the way it is done is that there is inequality, there is this inequality in the list of inequalities, the list of five inequalities that I have. There is the inequality for the function itself with c equals 1 to be proved. And it is true. And this inequality is of similar form. You can think Of similar form, you can think of it as an estimate on this exponent. So, this estimate is not good enough if you try to use it for other g j. But you can play with the coefficients in this estimate and improve it to match some Taylor coefficients, and this is what you get. It turns out that this is true. True. And so, how do you prove this? So, first of all, you observe that this is invariant under shifts. So, you can assume that y is equal to minus x, and in this case, you get a function of one variable. Okay? And you do something to prove it. You, I know, take five derivatives or something like that. I just wanted to mention one thing: that this function looks kind of complicated, but this function is almost equal to one. Almost equal to 1. This function, the left-hand side, is bounded by 1, 0, 0, 0. So if you, let's say, put this thing to the right-hand side and draw a graph of both sides, you cannot distinguish this function by just looking at the graph. These functions are the same, more or less. And we actually need bounds that tight for this proof to work. Okay? So, yeah. So yeah, I still have some time. Seven minutes. How many? Seven. Seven. Yes, I will probably finish earlier. Because you know there is nothing to do here. And what can I do with this, right? So this factor, C of xy, is let it be this thing. So think of this as bound on this exponent. So this exponent is bounded by one over C, if I put it to the right hand side. If I put it to the right hand side. So now I can use this bound to estimate these guys. And I have to deal with these inequalities. Because there was AJ was multiplied by this exponent. Now I estimated this by 1 over C, so I multiply by C and it goes to B, right? So the last idea in the proof, because these are really, really complicated things. You cannot you have to do something, I don't know, something kind of cheap to prove these things. So the cheap thing is to expand in x. You can try to expand in x because now these things are polynomials in x, y and e to x, e to y. So that's why I love these guys. These are simple guys for me. These are simple guys for me. So I can try to expand in x, and I will get some coefficients pn of y, and I can try to prove that these coefficients are non-negative. So for example, if you take this G3 G three of one xy, and you do the you you And you do the you consider this A3 and B3, okay? Then this coefficient will be of this form. So there is some combination of e to y, e to 2y, e to 3y, e to 4y with some oh it's not working. So anyway, with some polynomials. We stand polynomials here. Okay? So, how do you prove that these things are positive? Well, first of all, it's lucky that it's true. And for all of those inequalities, it's true. It's not only one inequality, it's five inequalities or something like that. For all of them, this is true. And these things, these coefficients, they are very equal. These coefficients, these are very complicated functions of n. So basically, one of these, if you want to write down all of these, this is like two pages of formulas. This is not easy, and you basically just generate this using mathematica or some. But the nice thing about this is that basically if n is large, let's say n is bigger than thirty. n is bigger than 30. a0 is much bigger than all of the other coefficients. And as you can see, these functions e to y times y, e to y times y squared, y is smaller than 0. So these functions are bounded, so I can somehow bound the whole sum. I can bound the whole sum by some sum of these coefficients, and e to a is anyway the biggest. So you prove that for n bigger equal So, you prove that for n bigger equals 30, this is positive for obvious reasons. And then you are left with some number of concrete inequalities, because if n is a concrete number, this is a concrete function. And you just have to check. So, basically, for most of these concrete functions, you check it somehow still bounding, still. Still bounding, still giving some bounds on these coefficients. And at some point, if n is equal to, I don't know, five, four, three, you have to you have a function which is really kind of the inequalities start to be tight and then you well you do something to prove to prove it, but it's a concrete function. So basically the proof of this is proof. Proving this very cheaply for big n and then for small n, you have a list of maybe 20 concrete inequalities that you have to check. So we checked all of them. I don't think you have to really check them because these are concrete functions. You can easily draw plots of this. On the plot, you can easily see that this is good. So for some of them, to make the proofs easy, you just do the Easy, you just do the following thing. So you have your function. So this is on negative. Y is negative. So you have your function like this. And some of them are so complicated that you have to do the following argument. So you just do Taylor exam function near zero. You compute that the limit is fine. And here in the middle, you estimate the Lipschitz function and then Estimate the Lipschitz function and then you only check on some net. Because the function is bounded away from zero. And if you estimate the Lipschitz function of this function, the Lipschitz constant of this function on this interval, you can check the inequality only on some net, right? Because if I am close to the point from the net, then from the Lipschitz property, the value will not be much. Be much different. Okay? So some of these inequalities, maybe far five or six, require checking numerically some functions on 4,000 points. So the question is, why do we do such things? I think that the inequality itself is kind of nice. And you know, I was struggling with this thing for maybe one year or something like that. year or something like that so at that point once I explored all my I don't know all my set of tricks that I know at some point I decided well maybe I can talk to these guys maybe they will have some ideas and we came up with all these reductions and what do you do we realize that it's possible to do it this way Thank you.