And I really would have liked to be there with you. So, I know also that me is what is between you and Lancha, so I promise to not waste your time and to not go along. So, I will talk about conditional partial exchangeability that was already mentioned by Francesco. This is indeed an invariance condition that I studied during my postdoc. Studied during my postdoc mainly for developed Bayesian non-parametric models for longitudinal and panel data, having in mind mainly mixer models. But then indeed, after working on this, I started realizing with in particular Francesco that it could have been useful to adapt and generalize this to model network data and structured network data. So I'm still working on it. And if I have On it, and if I have time at the end, I will tell you a little bit about how we're going to move from conditional partial exchange relates to network data, even if Francesca already told you quite a bit of that. So before talking about network data, I will start with a much simpler data structure. So when we started thinking about CPE, we were thinking, and that's conditional partial exchangeability. And that's conditional partial exchangeability. We were thinking about one of either of these two kinds of data. So, here on the left, I'm thinking about panel data where we have a bunch of subjects arranged in the rows and we follow them through time. Here instead on the right, I'm thinking to you can call them multi-view data or even more simply multivariate data. Again, we have a bunch of subjects in the rows. We assume that there is no particular meaning in the order of this. Meaning in the order of these subjects. And then, when we move through columns, we observe different features that can also have different support space, but they refer to the same subjects. Those are two kind of different data sets, but I'm talking about them together because they have one important feature in common, which is particularly important when you would like to do some Bayesian non-parametric modeling of this type of data. And this feature is the fact that when I move from one column to another, That when I move from one column to another, subjects are the same. And as Francesco already anticipated, a little bit talking about network, when you have multiple information regarding the same subjects, you don't want to lose this information. You will like your model to keep in mind that your observation are the same. And most of my talk is going to be about this and formalize a little bit about this thought. Okay, so main goal in the original work. In the original work about conditional partial exchangeability was indeed to kind of create a general framework to do non-parametric modeling of this type of data. And the idea is that we're going to do non-parametric modeling using mixture or better using some partition, latent partition of the subjects, and then using some parametric model in each set of this partition, which is latent and random. So, what I'm looking at is, I will So, what I'm looking at is: I would like a framework that allows me to estimate a collection of partitions here denoted with pi, one for each of the columns of this data metrics pi t. And also, I'm not doing this partition just because, for instance, I'm interested in clustering, but because I want to learn the distribution of this data also for doing prediction. And in this setting, you would like to do prediction, for instance, for a new row, so a new subject, or also a new column. If you have panel data. New column. If you have panel data, it will be a new time point. So, um, first question: like, do we really need a new framework for doing this? Can we not just use a Dirichlet process mixer or any Bayachel non-parametric mixer, which is already out there? So, uh, if you want to kind of model non-parametrically this type of data, you can definitely use it, for instance, a Dirichlet process mixture. And the way to do it, And the way to do it is, I will claim the following. So, what you do is to consider each row, so the information for one subject is a multivariate observation. For instance, you collect y and you have this whole vector. And then you can set up any mixture model where you have a random mixing measure and you are mixing all the vectors, so the rows of this metric. And if you're interested in having some You're interested in having some partition latent structure, for instance, you can select a prior for this mixing measure, which selects almost surely discrete prior and you're done. What you will end up with is a clustering of the rows and an estimation of the distribution of these rows. So, why this may sometimes is not desirable? So, there are many reasons, and unfortunately, I do not have much time to tell you all about them. Not have much time to tell you all about them, but they are very, many of these are very interesting. Let me just say that classical parametric or non-parametric Bayesian mixture models do not struggle a little bit when the columns of your data metrics are many. One reason, probably the most interesting one, is of course of dimensionality. Very roughly speaking, what happens is that you are in a very high-dimensional space and your clustering solution at the end of the Clustering solution at the end, the posterior will concentrate on a very trivial clustering configuration, like all the observations together or each observation by itself, which is pretty useless. And there is a very interesting paper on the MLR of Norich Chandra and quoters that explain it much better than what I just did. There are also many other small problems, like you can come up with a lot of small clusters that you don't know how to interpret. There is some dependence on the dimension of each feature. The dimension of each feature, it means dimension of each column. But nonetheless, even if you do not encounter any of this problem, you're still going to estimate a single clustering structure. You are not able to describe some really dynamic, non-parametrically, no change points. And so we're going to go for a different route, which is indeed the one of not having just one partition structure, but having many partition structures, one for each column. For each column. So let's say that we are back to square one and we would like to develop some framework to do so. Of course, we all know we can define a model by likelihood and prior. And another way is also to define directly the prior predictive of your whole data. And in this case, I think looking at the prior predictive and starting from there makes sense because this data, if we are at square one and we have no If we are at square one and we have no guidance, they are quite structured, especially because we have this dependence, which is single subject dependence because we have the same subjects at different columns. And so it makes sense to ask ourselves what is the invariant structure I would like my prior predictive to have. So trivially, we said, I said at the beginning, I'm assuming that there's no specific meaning in the order of the subjects. And so I will keep asking. And so, I will keep asking row exchangeability. So, if the row changes, they permute, as should have just happened in the slide. I would like the prior predictive to remain, to return the same value, whatever permutation of this kind. And when this happens, my model, for instance, will make the same prediction for a new subject. And it makes sense because the information in the data is not changed. Then there is some thinking about. Then there is some thinking about what if we permute columns. So, this I would claim it depends and it depends on the specific application in the sense that you may want not to the prior predictive not to change if you think that this new data set contains the same information. I think that if the data are panel data and your columns are different time points, this is a very strict assumption because, for instance, if now after this permutation, I would prefer not. After this permutation, I would prefer not to predict the same new prediction for a new column as I would have done with the previous data set. But these can be, let's say, arranged within the setting of conditional partial exchangeability. It depends on what you want to do. What is the most important invariance condition that we have in mind here is the following. So we said that we can permute rows and we don't want the prior predictive to And we don't want the prior predictive to return a different value. What happens if we permute the rows, but not entirely, like it should happen now in the slides? So the whole point here, argumentatively, is that now I would definitely do not want this invariance. I want that when this type of permutation happens, in general, now my model will make a completely different prediction for a new sub. Completely different prediction for a new subject. And the reason for that is that, for instance, if I look at the first line, after this permutation, I have fabricated a completely artificial new observation. A subject now is doing a path across time that no subject before was doing. So we don't want this kind of invariance, but to convince you about that, to convince you that it's important, I would like to show you just one two. To show you just one theoretical result in which I think you can see within a Bayesian non-parametric model why these invariants is important to not have these invariants. So there are models in the BMP literature that have these invariants, and those are partially changeable models that at least Vania spoke about yesterday, but also Ricardo has talked about. So what happens when So, what happens when you want to use partial exchangeable model in this setting? So, you can consider each column as being a group. This has been done. And basically, you associate a mixing measure that depends on the specific column. And then, if you want to do borrowing across the different columns, you can choose a dependent prior. These kinds of models are indeed have these invariants, and for instance. invariance and for instance if you look at what happens is that for instance if you look at the clustering structure now the you are inducing this kind of learning mechanism where the probability of clustering together subject i and j at time t is now basically the same regardless of whether subject i and j were already clustered together or not at another time point, which clearly doesn't make much sense. Clearly, it doesn't make much sense. And the reason why this is happening is that, indeed, having this invariance, we are ignoring subjects identities. So, other things that we would like this general framework to have, I already anticipated, is to do prediction. So, if I'm looking at the prior predictive, I would like to have a Kulmogoro consistency. And also, I would love to have representation theorems. This, I think, is subjective. I like them for many reasons. So, first of all, because Um, so first of all, because if we have a representation theorem, meaning for instance that here, the rows of my uh, I know how to represent the P tilde that makes the rows of my data set IID from P tilde. I will have Kolmogorov consistency for free for new subjects, and so I don't have to go crazy about checking Kolmogorov consistency for a weird prior predictive. And also, I think is that big computational advance. I think it is a big computational advantage basically because even if you are not interested in P tilde, what happens is that is a latent parameter that you can use in your algorithm to simplify the full conditionals. For instance, if you are deriving a GIP sampler. And indeed, for instance, I'm not there yet, but this is somehow what is helping a little bit what we are doing with Francesco with network, having a representation in terms of random probabilities help us algorithmically. So, okay, this is what we want to do. How are we going to do it? The idea I think is super simple and is exactly what the name says. So, let's say that we are at time one and we have a bunch of our subjects. We are going to use some clustering model. Then, suppose to move at time two, or if you are not moving across time, but across different features of the same subjects, still, you are going to be in a new. You are going to be in a new space, and what really stays the same are the subjects that move from time one to time two. And now, here's the idea: if I look now at these smiley faces, those are colored and they have different colors. And if I look at them, I think which is quite naturally to interpret them as grouped data. And within BMP, we know pretty well how to model grouped data, and this is via partial exchangeability, and the literature is very The literature is very rich about partial exchangeable priors, and so what we can do is simply to associate a different mixing measure to each group of smiley faces, where the groups are the ones that were, let's say, corresponding to the previous time point. Okay, so just to make this a little bit more formal, what I'm saying is that if I look at Y, which is the Which is the time t. So I'm looking at the t column of the data matrix. What I'm saying, and I use the, for instance, t minus one to denote the label indicators of the partition of the previous time point. I would just say that infinite sequence is a conditional partial exchangeable with respect to the partition here denoted with the labels indicator. Labels indicator if we have these equality for each permutation sigma. This is just requiring partial exchangeability because actually it's not for any permutation sigma, but just to those permutation sigma that when applied to the clustering configuration that I'm conditioning onto, that will not change. So basically what I'm saying is that I can permute observation within the same group. Seen the same group. Actually, in the old definition of CPA, we add something more, which is this second condition. This basically what is implying is that the distribution of the, let's say, the red group should be marginally equal to the distribution of the blue group, of the yellow group, and on the green group. The reason for this is basically because. Is basically because it's twofold. First of all, we don't think that having something else would make sense since it's related with the label switching. And also because this condition is required to get this result, which is the most important result, I think, regarding conditional partial exchangeability that tells you that conditional partial exchangeability allows you to preserve subjects' identity. So, here what happens is that if we have a If we have a vector which is conditional, partial exchangeable with respect to a certain partition, what happens is that now the probability that two observations are going to be found on the same set A is higher when these two are clustered together compared to when they are not clustered together at the previous time point, which is somehow the whole point. Okay, so let me tell you. Um, okay, so let me tell you this. So, this is just the invariance condition is not a model yet of any kind. Uh, and in particular, um, we are just thinking of a partition at time point t minus one, but not giving any structure beside conditional partial exchangeability to the column at time t. So, what we did is to try to derive a general class of Bayesian non-parametric models that satisfy CPE. That satisfy CPE. And basically, what happens is indeed, following the same idea at time point one, we have a classical mixer model, and then we model time point two conditional on time point one. Another way to see it is to think that we are using the label indicators of the previous time point as they were covariate at the second time point. And one thing that is also nice is that basically Also, nice is that basically, once you have this structure, it's almost all out there because the only thing that is left to define is this prior here, conditional prior, but we have so many partial exchangeable prior that we can plug here. And in the work, we consider what happens if here conditionally we use a hierarchical Dirichlet process, so kind of a conditional hierarchical Dirichlet process. And also, we explore also another new prior based on a mixture of finite mix. New prior based on a mixture of finite mixture, but that's not the point I would like to make today. So, this is very informal because the annotation is quite heavy. So, I think it's more direct this way. But if you're interested, you can find all the details on the archive paper. So, I started asking a lot of requirements to this framework, all the invariants, the representation theorem, Kolmogorov consistency. So, is this idea actually work? And the answer is. Work and the answer is yes. We have all the kind of invariance that we would like to have, and in particular, we have not invariants to partial permutation of the rows, which was the whole point. This was what allow you in terms of partitions to keep in mind the subjects' identities. And to convince you about this, in the model we have just seen with the telescopic clustering, whatever prior, the green prior you choose. The green prior you choose, you're always going to have this inequality, which instead the partial exchangeable model cannot achieve and just have an equality. And also, I think this is the right sign, right? So now the probability that two subjects are clustered together is higher if they were already clustered together at another time point. And this is how the learning mechanism will work. Sorry, we have a representation theorem. So we can derive marginal algorithms that speed up a lot. Lot and um, and also, uh, thanks to the fact that we have an unconditional representation theorem, we are sure that we have Kolmogorov consistency for new rows. Uh, and indeed, the theorem three is not theorem three, it's just lemma of theorem two. So another, I think, interesting part of this class is that this for it's just a class. All the models basically are they need to be studied yet, but still is highly. Studied yet, but still is a highly tractable class. And two slides are to convince you about that. For instance, here you have the Winder loss of the RAND index. This is not for a specific model. This is for the whole class. They have a kind of simple expression that depends only on the number of clusters. This is a priori, of course. And also, we call it telescopic EPPF, basically, it's the joint law of all the Basically, it is the joint law of all the partitions involved. And again, we have one for each column of the matrix. And they are ugly, but you can derive them analytically and are not that much more ugly than partial exchangeable models. So another thing that I don't have that much time to talk about is that something that I like about this idea is that there are strict connections with many work in the detail. Work in the literature of BMP. And in particular, there is a strong connection with the rich prior, which are a degenerate case of conditional partial exchangeability. There are connections with the temporal partition, which we proved in the paper that are a specific case of conditional partial exchangeability. Similarly, for separate exchangeability, in the paper, Giovanni Peter and Peter and also, yeah, many others. But I wanted to say slightly more because I cheated a little bit. So I started with our metrics and then I basically just talked about two columns at most, time one and time two, given time one. So how can we actually achieve more columns? So this is basically what I've talked about so far. We have a partition. About so far, we have a partition rho at time one, and we have now a directed way. Uh, this is not the symmetric to define the partition at time point two, keeping into account subjects' identity. So, of course, if you're working with temporal data and you want to do, for instance, dynamic clustering, you can compose this in a Markovian way. And this, for instance, is what we do in the paper. In the paper, we also explore this other kind of structure. Explore this other kind of structure about which I'm still quite curious. I think there are many properties still to understand. Basically, we explore this triangular structure when X, Y, and Z are, you can think of them as three columns, but are very different features. We have an application with metabolomics and they are in different support space. And somehow, this is like we are interested in the partition of X and the clustering of X. Of X and the clustering of X, but we would like to see how this is related to Y and Z. And I think that on this line, you can start thinking about kind of regression models by a partitions. And then one of the bigger questions is like mutual dependence or how to have all the columns that are exchangeable. And the answer was actually for specific models already. For specific models already in these two works. And the idea is still you can use this triangular structure where the row again implies a conditional partial exchangeability of y given x. But now x, you're not going to associate it with any column in your data. It's just going to be latent. Actually, all the partitions are latent. This is somehow doubly latent in the sense that does not depend, it's not associated with a specific column. And what happens? Specific column. And what happens is that rho y and rho z are going to be conditional, partial exchangeable one to the other. And they also going to have, you can also make to have the same margin. So in this way, you can create a symmetric CPA between rho y and rho z and also have the exchangeability of the two. Okay, so super quick, this is a very simple simulation study. Simple simulation study to convince you again that all of this was worth it. So basically, here we have, let's say, at different time points. And here the data are very simple. Those are the simulated data. We have two clusters, very well separated. Those are two normal with unitary variants, one in zero, one in four. You can basically tell us apart the two clusters apart just looking at those. And what happens over time is that certain observations. Happens over time is that certain observations move from one cluster to the other and vice versa. So, what we did here, for instance, we can see the result for Durand index. Here is for 10 time points. So, we estimate k-means independently at each time point. The Durand index, when is equal to one, we have perfect recovery. And k-means is doing pretty well. But this is not surprising because, as I said, you can almost tell the two clusters apart looking at those. This is a very toy simulation example. Example. And what we try also to estimate is a logistic breaking that, in this case, is set up in a way that the model is partially exchangeable, so do not preserve subjects' identities, and is doing SK means actually slightly worse. And here instead is the telescopic hierarchical Dirichlet process, which is a model based on conditional partial exchangeability and is performing. And it's performing very well, I would say. Super quick, another simulation study. This is since I'm not talking about algorithms, this is just to tell you this is scalable because it seems like a quite heavy model. And the first time I was presenting this, this was the main kind of objection. Like you can never estimate something like that. Actually, you can. And in here, for instance, we are doing it for 100 time points. Time points. We are doing much better than Kimins, but still, that's not the point. It's like it's doable, it's not as heavy as many of the Bayesian non-parametric models. And finally, I would just spend a few seconds telling you, let's say, my story about how this connects to network, of which I'm not an expert, but I'm becoming maybe thanks also to Francesco. So basically, after Bernardo. Basically, after Bernardo Francesco talk, I don't need to tell you about network. Let me just say that we have this network, we can think of it. And basically, a way to encode it is to use the adhesion metrics where we have a one when there is an edge between the two nodes that correspond to row and columns, and a zero when the edge is not there. So, a popular way to cluster nodes in a network. Nodes in a network stochastic block model, which Francesca already told you about. And so I can skip a little bit. Let me just say that basically the stochastic block model is basically defining the likelihood of your network, but still you have to fix, you need a prior over the partition of the nodes. And looking at the symmetry assumption that you should have there, so if you look at the adjacent symmetrics, what Look at the adeasen symmetrics. What you would like to have is the is if this is your adhesive symmetric, you would like it to be jointly exchangeable. And this means that if we permute rows and columns of the adhesive symmetrics, the marginal law of the adhesive symmetrics in your model should not change. This is, I say, the classical assumption that substitute exchangeability when it comes to adhesion symmetrics. Basically, what happens is that if you have a stock. What happens is that if you have a stochastic block model and you have an exchangeable partition of the rows, you indeed, so the stochastic block model, let's say, is the likelihood and your prior and the partition is an exchangeable partition, you get these invariants. But as Bernardo and Francesco were saying, right now we have kind of a lot of more structured data. And right now I'm working on multiplex data, which as Francesco said, are Which, as Francesco said, are basically a repetition of the same network in the sense that we have the same nodes, but connection represents something different. For instance, at the first layer, you may have a connection if node one and two attended a certain meeting together. And instead, there's other nodes if they have a phone call, and the third node, if they have another type of contact, like for instance, a message. So, what happens here is that if you look at the adhesive metrics, super quick, what happens is that the collection of adhesive symmetrics should be jointly, jointly exchangeable. And the repetition is not a typo in the sense not only each adhesion symmetric, but the collection of the whole adhesion symmetric to be jointly exchangeable because you want the invariance when you permute rows and columns in the same way in all the adhesion C. In the old DLDSNC, but you would not like to have these invariants if you permute rows and columns in two metrics differently from the two metrics here of pi and here of sigma. And basically, it turns out that if you use a stochastic block model and CPE, you are indeed achieving this. And finally, I will conclude that instead with a dynamic network, the situation is a little bit more complex, as Francesco say said. And my take on this is that. And my take on this is that dynamic networks seem somehow very similar to multiplex networks, but they are kind of different first reasons. So, first of all, we have a time horizon and so an order of the different networks that usually do not have a multiplex network. But also, I think from a very statistical point of view in terms of application, what happens is that the edges represent usually the same type of connection and not as in the multiplex are different type of connection. In the multiplex are different types of connection, are just the same connection but at different time points. And so, also, thanks to Peter, that let's say suggested us this to take into account really this fact and the fact that we should borrow information about also the connection patterns and not only the partition we are working with, Francesco. So, I mainly talk about a work that I've done during my post-doc with Mariano Deorio, which I. My postdoc with Mariano De Yorio, which I would like to thank. I'm working on multiplex network with Valentina, Guillaume, and Danielle. Mainly, multiplex network are PhD thesis of Valentina. And also, obviously, I would like to thank Francesco. And those are a bunch of references. And this is the archive of the CPE for our mixer model, is the QR code if you're interested. Thank you. Are you there? Can you hear me? Sorry? Can you hear me? Yeah, now I can hear you. Sorry. So, thank you once again for the very nice talk. And maybe we have asked. Okay, I I have a a question. Maybe and and maybe this is naive, but And maybe this is naive, but in row, you have row exchangeability. Can you hear me? Yes, yes, yes, yes. You can think of the rows as exchangeable multivariate data points. Say, one option is to model them like that. And so, let's say, apply an exchangeable model over the multivariate space. And another option would be to do this what your model that is conditionally partially exchangeable. Partially exchangeable. And my question is: have you compared the, I don't know, the estimations of your model against the marginals of what would be the marginals of the changeable model? Oh, so no, I haven't done a comparison of that. You're thinking about something like that. No, I haven't done it. So in Yeah, no, I haven't done it. If the columns are many and the comparison we are looking in sense of clustering, usually this will not work well. Like the partition usually that generates somewhere. Typically every row will be clustered by itself. But yeah, of course it yeah. Of course, it's yeah, it's yeah, it's not always the case in the sense that this is a kind of asymptotic result. So, for moderate size, this is interesting. What I can say maybe to say this, of course, maybe this is trivial. So, what we are doing is to estimate this partition for each column. Of course, we can always define our distribution of a partition of the rows, like picking up the finer. like picking up the finer, like I have partition of, I have multiple partitions of the subjects, I can define a partition that puts together, but yeah, no, not properly compared, and we didn't do it. Okay, thank you very much, and thank you all very much for your presence. So this was the last talk of the day, and hope you enjoy the rest of the day. Thank you, Beatrice. 