Stage. Okay. So, yeah, the stage is the ones. Okay, cool. Can you hear me okay? Yep. Oh, good. Okay. Great. So, so yeah, so my lab is called the Interpretable Machine Learning Lab. And one of my most important goals is to create machine learning models that are small enough to fit on an index card, yet are as accurate as the best black box models, like deep neural networks. And you're probably thinking, is this for real? Can you actually do this? Can you actually do this? Because this would mean a lot for trustworthy and fair machine learning if you could do this. And the answer is yes, we can totally do this for a lot of important problems. And we now think we know under what circumstances you can do it. You can't do it for computer vision or language, but you can do it for many other problems. So let me give you an example. So a few years ago, there was a competition sponsored by FICO, which is the major credit scoring agency in the United States, as well as United States, as well as Google, MIT, Oxford, Berkeley, and others, called the Explainable Machine Learning Challenge. And they gave everyone a data set about loan decisions. The goal was to predict whether someone would default on a loan based on their credit history. And they told everybody to create a black box predictive model and explain it because they didn't think it was possible to create an interpretable model with the accuracy of a black box for such a complicated data set. But several years after this competition ended, my team figured out. Competition ended, my team figured out that there are accurate interpretable models. You just have to look for them carefully. So, a little bit about this data: it's got about 10,000 loan applicants from a particularly difficult subpopulation, and the goal is to use credit history to predict whether someone will default on a loan. And the data was released five years ago, but nobody released a model that was really tiny, but had black box accuracy until last year. Because last year, we created powerful. Because last year we created powerful new machine learning algorithms that could do it. And since these all, since all the models are tiny, I can show them to you. And the first model that I'm going to show you was produced by the Fast Sparse algorithm, which creates sparse generalized additive models. And this is work done with Jo Chang Lu and Chudi Zheng and Marco Selzer. All right, so this is the whole fast sparse model for the FICO data set, the whole thing right there. And you just get a score for each of the features like this. Score for each of the features, like this. You just look up the score and you just add up all the scores. And that's it. That's the whole calculation. You just look up all, you know, you calculate, okay, what's my feature value? I'm going to get a score from that. You just add up all the scores and it translates into a risk for defaulting on the loan. So you can look at each of the factors and you can see how much each of them contributes to the overall score. Okay, so but what's really shocking about this model, besides its simplicity in describing this very difficult Its simplicity in describing this very difficult benchmark data set as accurately as any black box is that we created it in under four seconds. And that's usually about how fast this algorithm runs on average for this data set. It's usually about four seconds. It used to be that finding interpretable models was so computationally expensive that we just couldn't do it at all. And now we can do it in a few seconds. Okay, so let me show you a different type of predictive model for this data set. This one's going to be a sparse decision tree. Be a sparse decision tree created by the Ghost algorithm. Ghost stands for Generalized Optimal Sparse Decision Trees, and it was developed over the course of a decade. I'm going to put up pictures of the army of people that we've had working on Ghost. Ghost is very fast. It's orders of magnitude faster than the previous approaches. All right, so this is the Ghost tree. This is a Ghost Tree for the, you know, for predicting loan default in the FICO data set. And it's really simple. You just ask up to five questions to predict loan default. Questions to predict loan default. Okay, so of course, creating optimal sparse trees is NP-hard with no polynomial time approximation. And yet, this was created by Ghost in 8.1 seconds, which is really impressive. Now, this is actually the same tree as on the previous slide, but it's just done in a more artistic way. This slide was designed by an artist friend of Margot's, so that's why it looks so cool. It's like, you know, it's like this sort of powerful predictive model just kind of. Powerful predictive model just kind of growing in nature. I actually find interpretable models much more natural than black box models. Black box models might be kind of like snarls of branches or something like that. All right, so even on competition data sets, we find interpretable models that are as accurate as black box models. Okay, so let's back up for a second. So why do we need interpretable models, right? What's wrong with using black box models? Well, black box models are Well, black box models are difficult to use in practice. This is an example where a typographical error determined whether someone determined someone's freedom, determined whether they were denied parole. In fact, they were denied parole based on a typo on their score sheet. And then they're also difficult to explain, right? Explanations, they're usually incomplete and they're not always faithful to the underlying model. If my loan is denied, like in this article, If my loan is denied, like in this article, I want to know actually why it was denied. I don't want some crappy explanation that's not the real deal. And then also, black box models are difficult to troubleshoot. And I've got an example here from healthcare. Healthcare is a huge can of worms. In healthcare, they mostly use really simple models because they're just so much easier to troubleshoot. So I have here this article about a deep learning model for detecting intracranial hemorrhage. So this is like, you know, blood. So, this is like you know, blood in your brain. And this model was FDA approved. So, it was approved to be used in practice by our Food and Drug Administration. And then it was used in practice and then it didn't work and no one knows why. All right, so if black boxes are bad, why do people keep using them? Well, first of all, people don't actually believe that interpretable models can be as accurate as black boxes. It doesn't matter how many examples you give them. When a new problem comes along, When a new problem comes along, they go, This is probably something that needs a black box. And I can understand this for domains that really benefit from neural networks, like computer vision or speech, but even if it's a domain that has never benefited from neural networks, people still think this. Also, people like black boxes. Black boxes can be someone's secret sauce. You can sell a black box. You can't sell something that fits on an index card. You've got to sell the mechanism to collect all the data from paper. Mechanism to collect all the data from patients and combine it behind the scenes and so on and so forth. And then the third reason is that interpretable models are hard to create. I'm going to try and convince you that it's were hard to create, that they were hard to create, right? So I'm not sure if I can handle the politics on the secret sauce bit, but, and I have a lot to say about the first one, but I'm going to focus today on the third one. All right. So the reason why. The reason why they are hard to create is because of the interaction bottleneck that arises when people realize that interpretable models have interpretable flaws and they need to fix them. Black box models have flaws, but it's hard to see them. Whereas with interpretable models, you see it and you go, oh no. Okay, so let's say that a doctor uses data to produce an interpretable model, okay? And the doctor looks at the model, which they can do because it's understandable. Do because it's understandable, and they find that it doesn't align well with what we know about medicine. So, what do they do? The machine learning algorithm gave them this model, and so the doctor is stuck, right? Even if there were hundreds of millions of equally accurate models, the algorithm gave them this model, right? The algorithm returned only one model, only one of them. These models, they're all equally good with respect to the data, right? With the data you've got, there is With the data you've got, there is literally no way to tell which of these models is better. The only way to tell the difference is with domain knowledge. But there's no easy way to put the domain knowledge into the algorithm. And this is a real problem that there's no feedback loop. Machine learning algorithms, they return only one model at a time. And interacting with machine learning algorithms is, you know, it's a problem that the standard machine learning framework is just not equipped to deal with. Equipped to deal with. So we need an entirely new paradigm for machine learning at this point, right? Machine learning is totally broken here. So my collaborators and I decided that the standard machine learning framework just had to go. So last year we proposed a new paradigm for machine learning, something that's more human-facing than the standard machine learning, which is to hand the user all of the good models from a given function class, not just one model, and just let Not just one model, and just let them choose which model they want so they can pick something that doesn't just agree with the data, but also agrees with their domain knowledge. So they might actually be able to use it. And we call the set of good models the Rashaman set. Now, I want to point out that doing this is extremely technically difficult because finding even one optimal sparse model is provably hard. But now I'm saying that we have to find all of them, store all of them, even if there are hundreds of millions of them, and visualize them. Of them and visualize them so the user can actually choose from them. So, this is not easy from a technical perspective, but it is extremely useful in practice. And it changes the whole notion of machine learning from just optimization to enumeration to include also enumeration and storage of the Rashaman set and visualization of it. So, I'm going to show an example of it because now we have the first algorithm that computes the whole Rashaman set. That computes the whole Roshaman set for a non-trivial function class, which is sparse decision trees. And like I said, Margo and I and our students, we've been working on variations of optimal sparse decision trees for over 10 years. And now we can solve optimal sparse tree problems very quickly. So this is an example of a ghost tree on a criminal recidivism data set. Okay, so the algorithm, so the algorithm in this new paradigm, it's called tree farms, and it finds not just one tree like ghost does. Just one tree like Ghost does, it finds all of the optimal and slightly suboptimal trees for any reasonably sized data set and it displays them in an interface called TimberTrack. So what I'm going to do is show you TimberTrack. I'm going to show you how this works. So I'm going to just click off here. I'm going to stop sharing this screen and I'm going to share another screen where I've got Timber Track up. I've just gone to a website, by the way. This website's public. Anyone can go to it. I didn't do anything special that you can't do on your computer right now. Special that you can't do on your computer right now. Anyway, so this has the recidivism data set loaded into it. And if in every one of these gray bars is a decision tree, so if you want to, let's say you want to restrict yourself to only trees where the first question you ask, like the first split, is whether the number of prior crimes is greater than three. So if you do that, you can limit yourself to only looking at trees where that top split is number of prior crimes greater than three. And you can see that the trees are color-coded. Than three, and you can see that the trees are color-coded, so you can see what the splits are. So, here, prior crimes greater than three is the top split. Um, and you can, if you want, say, the second split to be age less than 26, you can do that too if you want to. Okay, so you, and you can store any of the trees that you like. You could put a little heart on them, you can write a little note on them saying, you know, I like this tree, this is, this tree is good for this reason or whatever. And then, also, if you, let's say that you really care about something like a Really care about something like a constraint. Like, let's say you have a constraint you want to introduce, like a fairness constraint, then you can do that by just restricting yourself to only looking at trees that obey the constraint. So, for instance, let's say that I really want to not use trees that have sex equals female. So, let's say that that's like a sensitive variable. We're not allowed to use that. So, if I just click that, then I've just limited myself to only trees that don't have that feature. So, you can see that. So, you can see that it's, you know, it's a really flexible framework that allows you to kind of search through trees, incorporate constraints, and do whatever you want kind of instantaneously after you found the Rashaman set. All right, so I'm going to go back to the other screen so I can just keep going here. All right, so nice things about this setup, there's no more interaction bottleneck. It makes the feedback loop to humans essentially instantaneous. Instantaneous. It also allows you to incorporate constraints for free. Okay, so no more interaction bottleneck. You can incorporate the constraints for free. It used to be, like I said, it used to be really hard to put in constraints like fairness and monotonicity into the models because you'd have to resolve it every time you put a constraint in. But now TreeFarms has done all the hard work of finding all the good models. It's just a filter operation to incorporate the constraints. So it's really easy to do. You just, you know, you just find the ones that obey the constraints. You just find the ones that obey the constraints. And it's a lot more computation upfront, like I said, but since it completely annihilates the interaction bottleneck and constraint handling, it's just totally worth it. All right, so I'm going to tell you a little bit about how it works. I'm not going to go into detail since it's quite complicated, but in the first step, the first step in the tree farms algorithm is to run the ghost algorithm. And so, ghost finds this optimal sparse tree that minimizes this objective, which is a Minimizes this objective, which is a mix of classification error and the number of leaves in the tree. And you really do want this regularization term here, because otherwise, if you don't have this, the trees have unnecessary splits in them. So even if you have like a depth constraint, then you could have a split that leads to two leaves that vote for the same prediction, like both vote positive, right? You don't want that. So you want to have this regularization term so that it reduces the number of unnecessary splits. Unnecessary splits. So you run Ghost to get this minimizer, and then you want all the trees within epsilon of that optimal solution. And that's the Roshaman set. And Tree Farms is going to find that. And the key ingredients for Tree Farms are a dynamic programming formulation. We have also theorems that reduce the size of the search space so that the dynamic program reaches dead ends all the time. So you don't have to search that part of the space because you So, you don't have to search that part of the space because you can prove that large portions of that search space of trees don't contain any element of the Rachaman set. And these theorems involve finding lower bounds on the objective function, the one I had in the previous slide, for various subtrees. And the third element is what we call the model set representation, which is our data structure that can efficiently store hundreds of millions of trees. And it stores them in an implicit way so that you can enumerate. In an implicit way, so that you can enumerate them. But what you store is actually parts of trees that can combine with other parts of trees, other parts of other trees, so that you can sort of, you know, the storage is much more compact than storing all the trees one by one. So it's much, much, much, much more compact, like combinatorially more compact. All right. So you might be wondering if we need something as fancy as tree farms, can we get the Rasha Monset by just sampling lots? Monset by just sampling lots of trees. So we asked: do other methods produce all almost optimal models? And as it turns out, they do not. So just to show you an example, on the Monk2 data set, TreeFarms finished creating the Russian set in 46 seconds. So we tried running some sampling methods for 46 seconds. Now, in 46 seconds, Bayesian additive regression trees created 488 unique trees, and how many of them? Unique trees, and how many of them are in the Russian month? Well, only three. Random Forest created over 20,000 unique trees. Zero of them were in the Russian Manset. CART, on bootstrap, random bootstrap samples of the data, created over 20,000 trees again. Only seven of them were in the Rashaman set. And then we can compare that to tree farms, which provably provides the whole Rashaman set, and it gave us over 100. And it gave us over 105 million unique trees in the Rashaman set. So we would have no idea that the Rashaman set was quite that big if we didn't have tree farms. And we get very similar results on other data sets. All right, so far, even on competition data sets, we're finding interpretable models that are as accurate as the black boxes. And we've introduced the new Rashaman set paradigm for machine learning. Paradigm for machine learning, which is to find whole Roshaman sets and not just single models. This solves the interaction bottleneck, and constraint handling is now easy. All right, so where do I go from here? Where do we go from here? Like, where do, at least where do I go from here? All right, now that I've solved all these problems that I didn't think would be possible in my lifetime. So, what I wanted to do is I decided to spend a few minutes. To spend a few minutes kind of toward the end of my talk, just telling you about some of the projects going on in my lab. So, what I'm working on now is expanding the Rashaman set calculation to include additive models, like the fast sparse model that I showed you earlier. And we're also using the Rashamon sets for variable importance calculations. So, someone told us this was kind of like XAI on steroids. XAI on steroids. So XAI is usually like explaining variable importance for a black box model. But here, that's not what we're doing. So our variable importance measure, it's called the Rashamon importance distribution. And it estimates the true variable importance. So in other words, like variable importance for the data generation process. And it's not trying to explain just one model. Because as you know, that one model, right, there could be 105 million models that explain the data. 5 million models that explain the data equally well, and they could be different from each other. So, what this method does is it works by averaging variable importance across the Rosschaman sets and across bootstrap samples. So, you get something that's very stable and very robust and incorporates all of the models in the Rossham set. It's kind of, it's just like a more holistic view of variable importance. All right, and then we've been trying to come up with a mathematical reason, like a foundation. Mathematical reason, like a foundation for why interpretable models perform as well as black boxes, and when do they do that? And we have a couple papers now that are a really solid attempt to do this. And the papers show how knowing something simple about the generation process for the data, like whether there's noise in the outcomes, can lead to the existence of interpretable models that performs well. And a central part of all of this is Roshaman Seth's. Of all of this is Russian sets. So, this most recent paper, A Path to Simpler Model, starts with noise. It starts with noise in the data generation process and goes through a series of steps that we think actually occur without anyone knowing about them that lead to the existence of interpretable models. All right. And this slide is about our latest method for risk scores. It's called FASTARIC. It's called FASTARISK. Called faster risk. This is why there's a big asterisk here because it's faster risk. So you have to have an asterisk, right? So, anyway, so for risk scores, you just, the kind of models they are, they're sparse additive models with integer coefficients. So you just add up the points, and then that translates into, you know, the risks in this table. And I know these are full-blown machine learning models, but they're actually like, they look very simple. Like they look very simple. They're actually notoriously difficult to construct because you have both sparsity constraints as well as integer coefficients, which make it sort of painfully hard. And this is Fasterisk's result on the FICO data set that I was talking about earlier. It loses a little bit of accuracy over FastSparse and Ghost, but it's so crazy simple. And it's also pretty fast. This one took about 13 seconds to produce. And it also produces a sample from the Rashamon set. A sample from the Rashamon set. So it doesn't just produce one model, it produces like a lot of different scoring systems. And we've been using this for an application to mortality risk. So we've been trying to make mortality risk predictions from the giant MIMIC 3 data set, which is a large public ICU data set. Physicians use these types of models all the time. I'm just going to make this model bigger on the next slide so you can see it. So here you would be Here, you would basically take your patient and look up their values of these different features. So, here, for instance, if the person has a temperature in the normal range, they don't get any points. But if their temperature is too low or too high, they get points. And then same with their like bilirubin levels and all this other stuff, like heart rate, maximum heart rate within the last, you know, certain amount of time, whether they have metastatic cancer, whether, you know, what their systolic blood pressure is, and so on. And you look up all the points and then the total. And you look up all the points, and then the total number of points translates into their risk, mortality risk, in this table over here. And so, again, when you run the algorithm, it produces a whole bunch of scores like this, not just one. So you kind of get the sense of the rational onset. Okay, so I'm going to switch over very briefly to causal inference. So everything I've talked about so far is just predictive. It's not causal. But you can use these tools for causal. For you can use these tools for causal inference as well. So, let's say that we're trying to estimate KATES, which are conditional average treatment effects. So, in other words, you're trying to say, like, okay, what's the effect of this drug on this patient? So, it's a treatment effect estimate. And we can actually do this in two steps. And the first one is a matching step. So, you can try to match units on important variables. So, for instance, here, like you can find Here, like you can find units that are, it's like you're trying to find identical twins, right? You're trying to find units that are very similar on things that are important, and we don't care about matching them on variables that are not important, like what they're wearing, right? Like we really want their health care, their health records to be matched up, right? And this simulates a randomized control trial from observational data. There is a science to constructing these match groups. I'm just going to put up a paper that is our latest method for doing this. I actually co-lead a lab. Doing this. I actually co-lead a lab that its entire purpose is to do matching. And so, and then in the and then, once you have all the match groups constructed, you can actually use the tools that I, the methods that I showed you earlier to estimate KATES, because for each match group, you have an estimate of the treatment effect, and then you can smooth those out using the sparse GAMs or decision trees that I've presented. Okay, still on the theme of. Okay, still on the theme of causal inference, we've been working on developing sparse treatment regimes for patients in the ICU who are being monitored with the EEG that detects dangerous seizure-like activity in their brain. And the patients need to be given drugs, and which drugs and the amount of them depends on a lot of things, like their current seizure-like activity, their pre-admission information, like all this stuff. And actually, the relationships between all these things are really complicated. So, we've been trying to put it into an offline reinforced. Put it into an offline reinforcement learning framework so that we can get interpretable, optimal treatment regimes. This paper is a slightly different direction for us. It's a new way of defining sparsity. It measures how sparse decisions are, not sparsity of the whole model. And so the idea here is that someone looking for a loan, they don't care about the part of the model that doesn't apply to them. They just want to know about the reasoning process for themselves. About the reasoning process for themselves. So they don't care about like that guy and why his loan was denied. It has nothing to do with them. So we introduced something called the sparse explanation value, which is a new way of measuring sparsity in predictive models in the decision itself. And so for instance, let's say that your loan was denied because you have no credit history. Well, that one bit of information is all I need. So your SEV is one because it's one piece of information making that decision. That decision. My lab does a lot of interpretable deep neural networks and applications to medical imaging and medical signal processing. And this is just a sampling of some of the stuff we're doing. So this is like mammography, trying to predict breast cancer one to five years in advance. And then this one is about heart monitoring here. And then this one is on EEG patterns. So again, brain monitoring. Okay, I have a line of work on dimension reduction for data visualization. On dimension reduction for data visualization, which helps you see reliably into the structure of high-dimensional data. This is a 3D data set here. This is this data set here, and we want to project it into 2D, like we're taking the mammoth and squishing it onto the page like a leaf. And so here, like, this is what happens when you do this with Tisney. It's like it dismembered the mammoth and ran it over with a steamroller. And if you keep fiddling with the parameters, it turns it into a chicken. And this is UMAP, not much better. This is UMAP, not much better. Large Viz takes off its legs. And then this is our result from a method called PACMAP. And anyway, PACMAP has been very useful for being able to see into high-dimensional data and being able to do trustworthy analysis. And we essentially use this for all of our applied projects now. Okay, this is my last slide. I'm going to just summarize the main point of the talk because I'm really excited about this area, and I hope at least a little of that excitement rubs off on you, too. So, I introduced the Rashi Montset paradigm, which where we produce many good models instead of just one. So, the user can interact with the set of good models. It breaks the interaction bottleneck. Constraints are now easy to handle. It provides a more holistic view of variable importance. You can use it for causal inference and sparse optimal treatment regimes. And it can be used for many, many, many applications, whatever you can dream up. All right, thank you very much. Perfect. Perfect time. I think we have about three minutes left. Any outcast? Yeah. Hi. Sorry, I have an annoying question. So the rational onset is like all the good training error models, right? But the big lesson of like overparameterized models recently is that some global optimal on the training set are really, really bad, and some of them aren't good. And it's regularization or whatever that determines whether you get a good one or a bad one. One or a bad one. And in this interactive model, I guess probably what actually happens is the practitioner looks for a tree that kind of aligns with their prior beliefs about what's happening. And maybe sometimes that's a good regularization and sometimes it's not. Have you looked at like, first of all, like how on these kinds of data sets, how much does the training, like the test error vary over the Rajmaland set? And then like, do practitioners tend to pick good? Like, do practitioners tend to pick good ones? Yeah, so that's completely dependent on the problem. And you make a bunch of really good points there. I'll make a couple of points also. So, first of all, this, if you just think about the framework of regular statistical learning theory, these models are sparse. So, their function classes are really small. So, that means they generalize really well. And you can see that from generalization bounds, but it's just true and practical. If you're working with small models, they generalize very, very well. If the test data come from the same distribution as the training data, the question is: what happens when? Training data. The question is: what happens when the test data doesn't come from the same distribution as the training data? And that's where you need domain expertise because you have nothing else. There's nothing else. Regularization is not going to help you unless it's aligned with domain expertise that actually agrees with what's going on. So no matter what, you need domain expertise to help you out. And if you do it wrong and the domain expertise is wrong, you're going to get a worse model. And there's nothing else to help you because these models are all equally good with respect to the data. Equally good with respect to the data. So, I mean, I work with healthcare people, they know healthcare. They haven't steered me wrong as far as I can tell. We've never had that happen. It's definitely possible, though, that it could, but like I said. The models are sparse. Yeah, so in terms of generalization bound, I don't know. But if you have 100 million possibilities, like even, you know, it's not, clearly your VC dimension is not small if you, or too small, if you have 100. Or too small, if you have a hundred yards, be aware. Thanks. I don't know. Well, hold on. We're talking about decision trees here. So, I mean, you're taking, you're supposed to take the log of the number of models, right? It's logarithmic in the number of models, right? If you look at the, if you look at the outcomes razor bounds. So you have to think about it that way. Okay, so yeah, yeah, yeah. Petit, last question. Sure. So my question, I think this is really interesting, shifting the paradigm. So my question is for The paradigm, so my interest. My question is from like the domain experts that you've been working with. Um, you know, there is a very nice interaction plot or tool you showed us, like, but it's still like a lot of choices for them. You know, have you started to interview or chat with people using these tools? And is that too many options? Or like, are there, do we need to like ask some preceding questions to help like reduce their balance of what they can choose? It just seems a bit overwhelming. Oh, yeah, yeah, it's, I understand what you're saying. On the other hand, an encyclopedia has a lot. On the other hand, an encyclopedia has a lot more entries than maybe even 100 million. I don't know. And we still manage to look things up in it. I mean, my husband said the same thing. My husband works on HIV and I work with him sometimes on projects. And he goes, oh, yeah, I don't want 100 billion models. I want one model. And I said, well, we handed you that one model and you weren't happy with it. So, you know, it does end up being quite useful to have the choices. And then also, like I said, you could put in constraints. So if you don't want to look at those 100 million models, fine, put in a constraint. Models, fine, put in a constraint. And again, the fact that you don't have to re-solve anything makes it a lot easier because sometimes the constraints people want to put in are very difficult, you know, to even incorporate. You need a separate algorithm to do it. Whereas if you have the whole rational set, you can just look through it and find the models that obey the constraints. Thank you. Okay.  