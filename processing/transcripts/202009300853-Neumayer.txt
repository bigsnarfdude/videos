Nomir, hopefully pronouncing this correctly from TU Berlin. Thank you, Laura. Thank you. Okay, so today I will talk about curve-based approximation of images on manifolds. This is joint work with Martinella from Vienna, Manuel Greiff, and Gabrielle Steidel, who's my supervisor. Manuel Greiff was in Vienna before, but he has. In Vienna before, but he has also joined our group in Berlin now, so he's in some sense also a colleague now. I will start with some personal, some information about what I'm doing and what I'm interested in. And just because I think that most people in the audience probably won't know me, so I thought it would be a good idea to just give a broad overview, and then I will go into a bit more detail for this curve-based approximation. For this curve-based approximation of images on manifolds. Just for completeness, I have a short CV here. I think the most interesting part is I was missing in Cambridge image analysis groups with Corolla Schoenleep, which also resulted in a paper. I can point to this later. And I started my PhD actually at TU Kaislauten. Probably most people won't know Key Slauten. Most people won't know Kai Strauten, but my supervisor Gabriel Steidel moved to Berlin and then the whole group moved to Berlin. I just submitted my PhD thesis like on Monday. It's called Deformation and Transport in Image Data. And yes, I think someone next year I have to see what I will do after. So yeah, maybe I'm yeah, I don't have any job yet. So I will. Have any job yet? So I will stay probably until summer in Berlin. That's an agreement with my supervisor, but then I'm free to go wherever I want. Okay, so research interest. So we are doing working with many four-valued images. I have a few examples here of such images. On the left, I have in SAR image of the Vesouf. So what is happening here, they are somehow flying with a plane. Are somehow flying with a plane over the Vesouf, and they're just sending down a radar wave and they're measuring the phase difference between in and outcoming wave. So you have something which is naturally encoded as angle in minus pi and pi. And then like such a phase angle information is naturally on S1. So you have some manifold structure in the data. There are also some material science tasks where you have manifold where you. Where you have many value data. For example, if you have some alloy materials with cranes in there, then the crystalline structure can have certain orientations. So you, I think it's essentially the special orthogonal group, three-dimensional one. And then you have to factor out the symmetries of the materials. Or here on the right, we have a DTMRI image of the human brain, and here the tensor. Brain, and here the tensors basically describe the water diffusion in the brain. So they have some orientation information in them. So what we can see here is there are just many applications where many valued images pop up and they're getting more and more, and we need computational models for them. So, this is one big aspect of what we're working on. Then, we are interested. Then we are interested. So I have to stop the presentation here. Otherwise, I can't play the video. We are interested in deformation models for images. So I hope you should be able to see the video. So what is happening here? We have first image and then we transform it into a second image. Clearly, you could just do something like linear interpolation, or you could do it like here that you really have something diffeomorphic happening here in between. There are a lot of models. There are a lot of models how to achieve something like this. We could use optimal transport, linearized registration, registration, flow of difomorphism, metamorphosis. And one thing we did is we extended such a model to manifold value data. I can also point to the paper later. And but clearly we're also working on Euclidean versions of these models. Versions of these models. So let me go to the presentation again. Okay. And there is a lot more what we are working on. There are inverse problems. We can use regularization techniques based on such deformations. We are also interested in discrepancies, particularly as distance measures. So if we have two images, we want to judge the difference between the two images. We can just Between the two images, we can just interpret these images as measures, and then discrepancies are a natural way to measure them. There is, as we already have seen in the last talk, kind of a relation to optimal transport, or we can, it's interesting to investigate the relation because you have different approximation rates. Usually, optimal transport is known to suffer from the curse of dimensionality, and we have the discrepancies here, which are Have the discrepancies here, which are usually dimension-independent, and there is something called sinkhorn divergences, which tries to interpolate between the two models. We have also investigated this a bit more, and we also have a paper on this topic. I can point to this on the next slide. And clearly, I mean, it's like the trend topic at the moment: neural networks, but we are particularly interested in the work. Particular interested in invertible ones and which can be implied in inverse problems. So, we try to design neural network structures which are invertible by design, which has some benefits for training or if you want to model linguist problems, for example. And something interesting here are stability issues. So, there are very a lot of interesting imaging problems out there. Imaging problems out there, and I'm working on a very broad field of imaging problems. But for the next part of the talk, we're going more into detail on this curve-based approximation of measures on manifolds by discrepancy minimization. Here we have the other paper which investigates the relation with optimal transport. There we are focusing on the dual form. On the dual formulation of the discrepancy. Here we have this stabilizing issue with neural networks, and here on the bottom are more these deformation-based models for imaging, which we extended to many fold-valued images. Here, yeah, I think I'm not going here more into detail. If you have questions on anything of this, I can tell you a bit more later. Okay, now let's get Okay, now let's get to the second part where we're getting more into detail. So I have two images here on the left. So what is important here now is that our image is basically living on S2, so on the sphere. So we are not living on a flat domain, which is a bit different from the many four-valued images which I was showing in the beginning, because they were still living on the flat plane, but the data itself had many four values. Here we're still. Four values here. We're still having only gray value information, but it's like distributed over the sphere. And here on the left, we just depicted the elevation data of Earth. So you can see here the ocean, here you can see Europe, and then we have North America here on the left, South America, Africa. And the question now is can we approximate such a measure by a curve? A measure by a curve. And here on the right, I have like one such curve which we computed with our model. And there are like a lot of questions popping up. So clearly, the approximation quality will depend on the length of the curve and also a bit on the regularity requirements which we are imposing. And yeah, we investigated all these questions. All these questions in our paper. First, the general setting. So we are assuming that X is a compact Alpha Stregular length space equipped with a bounded non-negative bore measure sigma X, which has full support. So Alpha Stregular here means that essentially the volume of a ball with radius R scales as R to the power D and then. To the power d, and then d is turns out to be actually the Hausdorff dimension of the space. But for example, if you want to have examples of such spaces, you can just every smooth Riemannian manifold or just Euclidean space have fulfill this property. So, in Euclidean space, this would be just the dimension of the space. So, we have a control over the asymptotic behavior of the balls. This is just Behavior of the balls. This is just a technical requirement which we need for some of our proofs. So our approach for constructing such curves relies on an energy minimization approach. So we need some distance. So mu is our input image, which would be, if I related to the previous example, would be just the elevation data of the Earth. And then we want to approximate. Then we want to approximate it with some measure u, which here would be now has to live in some space p of suitable measures. And in our case, this would be measures supported on curves. So we just restrict to a special subset here. And then we have to judge how close the measures actually are. And for our case, we want to use discrepancies here, but clearly you could also use optimal transport or something. Optimal transport or something in between, and I think there's actually also a paper where they're working with optimal transport. What is happening if you're working with optimal transport, your approximation rates will become dimension dependent and you're dimension independent if you're using discrepancies, which is one of the reasons why we're doing so. And also the discrepancies are a bit nicer to handle in our manifold. To handle in our manifold value setting, in our manifold setting. So, the first ingredient is the space of measures supported on curves. Here we just use measures supported on curves of length L. So we just have a measure and there has to exist a continuous curve with length L such that the support is just contained in this set here. And one thing. And one thing to observe is that we can equivalently rewrite the space using a reparametrization of the curve. We can reparameterize it to constant speed L using the push forward. And then we can rewrite the whole thing like this. So we take the push forward of some probability measure omega under the curve gamma, and gamma has to have constant speed smaller than L. speed smaller than L, and you can actually show that these two spaces are the same. It's not too hard actually to prove. And for applications, we are also investigating a special case. So we're using here, I denoted the Lambda curve here for the Lebesgue measure. So we're not using any measures, so we only want to investigate measures which are the push forward of the Lebesgue measure. Forward of the Lebesgue measure under some curve with a speed smaller than L. And one thing which should be quite obvious is, so this space here is larger and it's in some sense has a simpler structure. So if we derive approximation rates, it will be usually easier to derive approximation rates for this space than for this space. And also that this restricted space usually. This restricted space usually has worse approximation rates than the larger space. Then the second ingredient is our distance measure where we want to use discrepancies. Here we assume that they are associated to some kernel. So we have some continuous symmetric positive definite function k from x times x to r. Then we know by Then we know by Mercer that there is an orthonormal basis and non-negative coefficients, such that we can rewrite this kernel. And then one way to rewrite the associated discrepancy is just to take the sum here over the alpha k's times mu k hat minus nu k hat and the hat should express the the Fourier transform of the measure, which is just given here on the right. Given here on the right. And one thing, one benefit of this formula actually is that we can evaluate it quite fast using Fourier methods on certain manifolds. So there are fast Fourier transforms on the sphere and also on the Cresmannian. So for all numerical examples which we have in our paper, we actually have fast Fourier transform methods, which make the evaluation here quite efficient. Clearly, if I think Clearly, if I think everyone should be familiar with discrepancies, you can rewrite it also in various different forms. You can write it more like in the MMD form, which shows up in machine learning, but you could also write it as a dual norm in the Hilbert space. But for our application, this was just the most handy form. I didn't want to write all of them down because it doesn't add much value to the top. Doesn't add much value to the talk. So we're just sticking with this formulation. And as I already said, there is a strong relation to optimal transport via the sinkhorn divergences. The talk time is very short, so unfortunately I cannot go more into detail here. If you want to know something, just maybe feel free to ask me later or have a look into the mentioned paper. So the first observation is. So, the first observation is some general approximation rates. So, here we have our requirement that we have an alpha stirregular length space and we take some arbitrary probability measure and we have the space p curve L. We call this the largest space. Then as we increase L, then the discrepancy and we take the optimal mu here. So, the best approximation, it scales as L to the power minus D divided by. To the power minus d divided by 2d minus 2. So if d becomes very large, then we are moving towards one half essentially. And I think this is also, shouldn't be too surprising that we get an approximation rate like this. And the proof is actually pretty much straightforward here. So you just need to bring together two tools from different. Together, two tools from different areas of mathematics. So, first, we use the approximation rates for atomic measures, which are well known. Then, we essentially have points, and we can construct the corresponding curve by just visiting the points in the order corresponding to the traveling salesman problem. And also for the traveling salesman problem, there are asymptotic bounds, and if you plug everything together, you will actually end up with this approximation rate. End up with this approximation rate. Then there is somehow the standard approach in approximation theory. So if you want to get better rates, then you just take a smoother kernel and smoother functions. And for this setting, we also have to choose X as a d-dimensional connected compact Riemann manifold without boundary, which for example is fulfilled by the sphere in the beginning. Beginning, and we have here improved approximation rates. So, we require that the s which describes the smoothness is larger than d over 2. Here we have our Sobolev space, and then we obtain a rate of L to the power minus s over d minus 1. If you just plug in here the d over 2, then you're essentially back with the old rate. And if s, so The old rate, and if S, so if the smoothness is higher, you have an improved rate. We can actually show that the bound is sharp up to a constant, so you can also establish a lower bound, worst case rate, which has the same asymptotic. And the proof relies on a result of Brandolini about a gravitature error. Then, as I said, it's harder to achieve rates for our smallest-based lambda curve. Curve there you usually have to restrict to special cases, for example, S2. I included it here because I have the numerical example for S2 also included. And here we arrive at the approximation rate L to the power minus S. But we also have approximation rates for the Taurus or the Presmannion. And but it would be interesting to actually achieve a general approximation rate. A general approximation rates in this setting, but this is open so far. Then, maybe a few words about the discretization. We can just rewrite the problem as a minimization problem over curves. This is just using the push-forward formulation of our measure set. Then we discretize curves by just making them piecewise shortest geodesics. You can show that this problem actually gamma. Show that this problem actually gamma converges to the top one. And then, if we have this discretized piecewise shortest geodesics, we can equivalently rewrite it as a minimization problem over points. And we can relax the constraint on the length of the curve just by a penalized version, which is also known as penalizer versus constraint. The resulting problem is non-convex and tricky to solve. We use special. We use special, we use adapted CG method, multi-level, good initialization. So basically, the whole machinery. And here is the numerical result. And here you can see as we increase the length, the whole thing starts to look more and more like the actual elevation data of Earth. And here I plotted also the approximation error, and we can observe that our numeric. And we can observe that our numerical results actually nicely fit the theoretically predicted approximation rate for the sphere here. So then I have a second example for the torus where we have that we're working on a surface. Here we use the spock hat and just have a uniform measure on the spock hat and we try to approximate it. And what you see Approximate it, and what you see here is as the curved length increases, we are pretty much looking like the spock hat. And what you could do, actually, in the end, is you could use this whole thing as a plan for creating a wire sculpture, which is actually quite cool. But for such curve approximations, there are also more applications, for example, in MRI. So, let me conclude. We introduced the model for approximation. Introduced a model for approximation with measures supported on curves. We investigated the theoretical approximation rates. I just have shown a few in my talk, but there are more in the paper. We developed the numerical algorithm for solving the problem. The theoretical results are actually confirmed by our numerical experiments. So we match the predicted approximation rates. And in this other mentioned paper, And in this other mentioned paper, we also have established some kind of relation to optimal transport. But numerical examples are only for discrepancies as they are usually more efficient to evaluate and for our purpose was also sufficient. And there is some related concept of principal curves for data approximation. And one thing to do in the future is, for example, investigate. Future is, for example, investigate the relation to this concept. But clearly, other different interesting questions, which I already mentioned, is to get general approximation rates also for this more restricted space of push forward of the Lebanese measure. I think we are already done with time. I would have had another image, but I think I will stop here. Stop my screen share for a moment so you can see me again. Sorry, sorry for not taking awkward because uh something happening in my apartment. Uh as these things go, I guess. Uh As these things go, I guess. Sorry. Okay, great. Thanks. Thanks for the talk. As usual, the floor is open for questions. Feel free to raise your hand or ask a question in the chat. Yes, I think we have like a few questions here in the chat already. Oh, do we? Sorry, this may be still waste distance, yeah. While people were thinking, I was wondering if, so I found myself wondering, I guess as a computer scientist, how computationally difficult is it to actually construct these curves? So I guess you didn't get to talk much about the numerical methods. Yes. More about that. As I mentioned already, like a bit here, I mean, it's quite a I mean it's quite efficient um to actually doesn't take too long to solve the problem because we can uh use the fast Fourier transform methods to to actually compute this discrepancy here the the second part here you can differentiate it it's not too tricky you can also efficiently evaluate it the problem is more like that uh that the problem is very non convex. Problem is very non-convex, so you have to come up with really clever approaches for solving it. So, this is why we need this multi-level procedure and good initialization. So, what we're usually doing is, let me go to experiments, so you compute a short curve and then you start to refine it successively, because otherwise you will usually get stuck in an unpleasant local. Stuck in an unpleasant local minima. So I think computation-wise, it's not too heavy, but you really have to think about what you're doing. Does anyone else have a question? Okay. Okay, in that case, we can go on to the next talk. Thanks, Sebastian. Thanks, Sebastian. Let me just stop the recording.