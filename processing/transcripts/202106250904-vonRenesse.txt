My screen sharing was interrupted. Is it now visible? I see. Now your screen is gone. Let me try to do it again. Now is it visible for everybody? Yep. Visible, yes. Okay, thank you. Thank you very much, Aaron, Sumik, and Brendan, for putting this workshop together and for giving me the opportunity to present this talk. I really apologize that I could not participate personally in a more active role. The circumstances are. A more active role, the circumstances are complicated for all of us, and so I apologize. But nevertheless, I'll be very happy to meet you all in person on the next occasion and interact more than we were able to do this time. Okay, so this is a note and a little remark only on something that actually deals with the theme of the workshop, namely entropic regularization and the computational part of it. I'm certainly by a degree. I'm certainly by education not a computational guy, but I had a PhD student who desperately wanted to do some computational work, so we started and embarked on this project. And we have some comment here to make on the famous Syncon algorithm. But I should make it very clear from the start that our work is a comment to the work of four French colleagues, who are the gentlemen Thibault, Chizard, Dossal and Papa Dakis. Chiza, Dossal, and Papadakis, who proposed a certain idea of a modification of the Syncot algorithm. And we were quite intrigued by this simple modification. And what I shall present is just our take on what they proposed. And I'll comment as we go along on their idea a little bit, which was apparently very nice and beautiful, but for us not fully comprehensible. So just we try to reproduce a little bit. So, just we tried to reproduce a little bit of what they did, and this is what we came up with. So, the actual work that I shall present is work with these two PhD students, Tobias Lehmann and Alexander Zambale, and together with Andrei Ushmayev, who is a computational colleague from the Max Planck Institute in Leipzig. Okay, what's the idea of the algorithm that was proposed by? That was proposed by Thibault, Cisard, Dosa, and Papadakis. It is a modification of the Synchon algorithm, which we all more or less know. Let me recall to you just for a second what the Synchon algorithm does. So it deals with the entropic regularization of optimal transport. We shall do all this, of course, in a finite-dimensional framework after we discretize and put on a mesh or something our optimal. Our optimal transport problems. So, in the end, we are dealing with finite-dimensional vectors, probability vectors A and B of probably different dimension. And if we have a cost matrix, then the actual matrix which governs the algorithm is this exponential transformation of the cost matrix where epsilon is the regularization parameter as we. Parameter, as we all probably remember. This epsilon parameter is exactly this entropic regularization parameter, if you pass to the corresponding entropic regularization of the optimal transport problem. And then the game is to find a pair of potentials or vectors of the corresponding dimensions such that these two equalities are satisfied, namely applying this K kernel. This K kernel to the vector V star, and performing a component-wise multiplication with the U star vector, which is the other leg of the solution, then this is supposed or is required to be A. And likewise, you have a similar equality with the roles of U and V interchanged. And on the right-hand side, you have the B, which is the target distribution which you want to match. match and a u star and v star are then your entropic entropic potentials okay and the synchron algorithm the synchron algorithm as we all might probably recall is a progressive scaling algorithm synchron algorithm of course was not only applied in the in this optimal transport business in fact i looked it up some while because we had a referee report on this paper which was very interesting Port on this paper, which was very interesting. The person seemed very knowledgeable. So, there isn't, I can look it up for you if you want. There is a Dutch mathematician. I provide it later. There was a Dutch mathematician in the 1930s or something who was the first to come up with this algorithm. So, this algorithm should effectively bear the name of this gentleman. So, it was applied and rediscovered in a million of different contexts. But anyway, we know it under the name of Synchon algorithm, and it does the And it does the following. It starts with a certain initial pair u and v, and then it goes by induction. You apply the v vector of level L, you apply the K matrix to the V vector of level L, and then you take the component-wise division or the ratio, the component-wise ratio. This here is supposed to denote a component-wise ratio of the entries of these vectors, and that is then. These vectors. And that is then your definition of the U vector at the next level. And then you do the same thing or the similar thing on the V part. That's the synchron algorithm. And we have different proofs, qualitative and quantitative proofs, that show that this algorithm actually does converge to a fixed point, which then will be a solution to this sink on or this entropic regularization problem. On all this entropic regularization problem, and it's easy to implement, typically, yes. And the idea that was now put forward by these four colleagues whom I haven't yet known or come to know in person, but whose idea here is very admirable and wonderful, is to apply a simple transformation of this algorithm by doing again. By doing again this product or this pointwise ratios, but before we do a pointwise multiplication of the corresponding entries of these vectors, we exponentiate. So if you like, this thing here, if you want this step, is an interpolation step that updates the UL by using the VL information and doing a certain transformation on the VL before you do an update on the UL. And here you can read this law here. Can read this law here, or this rule, which is the modification, as doing basically the same kind of update step. You take the same update, but you do an interpolation between the two, between the previous information that you had at level L for the U and the new update. So this expression here is in fact some sort of an interpolation between the previous UL and the next one using this next step information. And the interpolation here is in this exponential or logarithmic, as we shall see in a second. Exponential or logarithmic, as we shall see in a second, in this logarithmic coordinates. But it's component-wise? This is again a component-wise exponentiation. It's a component-wise exponentiation, and it's a component-wise multiplication after. And you will see in a second why, if you come from a certain numerical background, you will see with a little bit of transformation in a few minutes why this is actually natural to do. Do and on the VL level, we do pretty much the same, but just you know, it's the same prescription of what we should do and the same recipe. Okay, and the interesting thing will be that's the interesting upshot of all this thing or the remarkable result, which we can provide an explanation for, is the fact that this is beneficial to do for interpolation numbers for omega, even strictly big. Omega, even strictly bigger than one. That's the interesting thing. So, probably again for someone from the American linear algebra, that might not be so surprising. But this here looks in some proper coordinates like an honest convex combination of an iterator. So, if you forget for a moment that this has this exponentiation stuff, which you can, of course, convert into an honest multiplication. Convert into an honest multiplication and so forth by applying the log. Then this looks like just a convex combination of keeping your old information and doing an update with a certain convex weight omega. But the interesting thing here will be that it would be beneficial to choose a convex weight omega, which is strictly bigger than one. Which is, of course, so not fully standard convex combination, but. Okay, and the work of the colleagues just put forward this modification, and we shall see in a second that it is important to make a good choice. Sorry. Of course, it's important to ask what is a good choice for Omega. That's the core issue then. So, what is the capital lambda? The capital lambda, where is it appearing? Ah, it's in the next slide. Yes, yes. Sorry. In the next slide, yes, yes. Sorry, yes, I was. So, the lambda is the bulk of contraction ratio that we already define, the conventional bilk of contraction ratio. I shall, here is it again. So, here is, so our, so let me just try to make some more advertisement for the work of the four colleagues. These gentlemen okay, so as you will see in a second, it is important if you want to make take advantage of this, it is important to make a proper choice of Omega. Important to make a proper choice of omega. We shall argue, or it is known, that the omega, the optimal choice of omega, will depend on a linearization of this algorithm near the solution, which therefore says that omega, the optimal choice of omega, is affected with an a priori information, which you don't have when you start the algorithm. And so, therefore, in the paper by these French colleagues, it was argued or they proposed an adaptive. Adaptive algorithm, which says that at each step of the iteration, you make a certain choice omega, which is an omega L. And if you choose the omega L's properly, then you will have a convergent algorithm because you will have, as you go along, you have more and more information or more, you can guess better and better what the actual solution will be. And that will enhance your possibilities to make a good choice for this omega. But probably I'm going too far ahead. But probably I'm going too far ahead. First, I'd like to provide one of our first notes and remarks on this, namely, in fact, if you look at this algorithm, then we can positively state that, in fact, for a certain range of parameters omega, this algorithm will converge to a solution or A solution or up-to-scaling unique solution of this entropic regularization problem exponentially fast, where this upper bound for the admissible ranges of parameters is strictly above one in general. It is defined as two over one plus lambda, where lambda is the conventional Birkhoff contraction ratio, which is. Birkhoff contraction ratio, which is the quantity which also appears in the analysis of the standard Birkov, of the standard Synchron algorithm. So I probably will not repeat or read out to you the definition of this Birkov contraction ratio, but this is a quantity which, whenever you look into a contemporary proof of the convergence of the standard synchron algorithm, you will Synchron algorithm, you will encounter this number. And here we can just, with a very simple argument that I shall give you, we can state that this modified synchron algorithm will actually converge for weight values omega, which could be even chosen bigger than one. That's the first comment that we have on this algorithm. And there will be a second comment. Them and there will be a second comment by which we can show that it is actually beneficial. So, this first proposition is saying that okay, it is admissible to choose omega strictly above one. And the second proposition that I shall present after this will say that it is actually beneficial to choose omega strictly bigger than one. So when you uh you haven't said why you call it over-relaxed, but by over-relaxed you mean bigger than one? Yes, that's that's what it's meant. Yes. So relaxed would be less than one or? So, relaxed would be less than one, or yes, when you do really a convex combination of the two steps, so to say. And the point is that over-relaxation is a good thing, it's admissible and it's a good thing. Okay, we first admissibilities proof. Let me sketch the admissibility proof. Okay, there is this scaling invariance, obviously, in the problem. When you look at the problem you want to solve, then there. Look at the problem you want to solve, then there is a combined scaling invariance. So, if u star is a solution, you scale component-wise by a certain lambda, and you scale by one over lambda inde, then you have another solution, pair of solutions. So, there's a certain scaling invariance, which is just basically saying that it makes sense to look at the problem in, if you like, projective space. And I give you now, so what I'm trying to do now in the next two slides is just to give you a full. Two slides is just to give you a full proof of this statement. And this one of my PhD students, he has an inclination for non-standard geometries, and he proposed to formulate the stuff in this non-standard geometry that I shall tell you now about. And it turns out that it is nice to write things in this form. Okay, so we pass to this projective cone, because that's effectively the Because that's effectively the state on which we do our algorithm. And this is our equivalence relation on this projective cone. And then we introduce a certain addition definition. So when we say that the addition of two of such cone elements is defined by doing a pointwise multiplication and then passing to the equivalence class. And we also combine, we define what is meant to be in multiplication by a scalar. This is just taking the This is just taking the point-wise exponentiation with this scalar. Since x is non-negative component-wise, this is well defined. And we also define a certain norm, as it will turn out. And then we will find out that with this definition of addition and scalar multiplication, this projective cone actually becomes a finite-dimensional Bano space. So, in essence, this is just So, in essence, this is just passing to the logarithmic coordinates, but you just don't, if you like, you don't write it in logarithmic coordinates, you write it in the, you don't change the coordinates, you just change your operations, so to say, and then you find this, this defines your Banach space, which is nice because you can then write down things in a linearized fashion. And then, if you write down your over-relaxed syncome in this, by means of this new addition. Addition and multiplication, or scalar multiplication operation, you then see it's just you know, it's just a relabeling of things, it's nothing big, but in this way, to write it down like this, it becomes nicer. And you, for a person from numerical analysis, that person would immediately recognize that this is not really this convex combination. This is, if you like, the iteration step, and this is the previous step. So, in this notation, you really see that this is this convex, has this convex. This is this convex, has this convex combination structure. But the difference or the change that, or the fact that we have now to acknowledge is that this k operator in this Bana space is no longer a linear operator. It's non-linear. We have changed our concept of addition and multiplication by a scalar, and then it's no wonder that this k operator is no longer to be understood as a linear. And what we also And what we also find is that this Hilbert metric, which is the key element for the classical proof of the synchron algorithm, this Hilbert metric is just the Banach distance between these two projective representatives. So in other words, it's just another way. Well, okay, so I would probably say that this way of writing down in this Banner space, this Banner space is called compositional data analysis. Space is called compositional data analysis. Some people use it in statistics, and my students, his students, he looked into that literature and was fond of it, and so they proposed to write it down in this time. Okay, and then we could, and so what it emphasizes is basically, so this Banach space perspective emphasizes that this Hilbert metric is not only a metric, it has this linear scaling property, which we shall use now. It's not only a metric, it's a Banner space metric, so to say. It's a Banach space metric, so to say. That's what we learned from that. But so the Banach space with the Hilbert metric is not a Hilbert space. No. Yes. That's very, yes, very sharp remark. You have to be careful with your notation there. Yes. Okay, and then this Birkhoff-Hopp theorem tells us that this non-linear Theorem tells us that this non-linear map is just rephrasing the Bilerkov-Hop theorem that is typically formulated in this metric. It's just saying that these non-linear operators, this k operator as a non-linear operator in this Banach space is now a Lipschitz, this is a Lipschitz map whose Lipschitz, whose Lipschitz norm is bounded by this Dirkov contraction ratio. This is just a rephrasing of the classical result here. result here. This alone tells you that the that the classical the classical algorithm, this classical synchron converges. Okay, here's the proof of the remark that the over-relaxation is also good. Namely, we just analyze, if you like, the iteration step now according to this modified version. And so this modified version is this convex combination of different things. Convex combination of different things. But since we are working in a Banov space situation now, we can, of course, work with triangle inequality, okay, that you can do also in Hilbert's in a metric space. But we have this linear scaling relation, so we can pull out scalars out of the norms. That's very simple. This is the one step. The two steps are not completely symmetric with the roles of the iteration counter. So the V step, we've just put the, we start with, so that's that's. So that's if you like the estimate for the U step. The estimate for the V step is in the first level exactly the same. But now you realize that, of course, there is in the second part of the V step, there's the second step of the UL. So you expand one term further by plugging just in the previous inequality. So I apologize, this is very easy, but why not? Okay, so you just combine the two estimates for the second step and you get an estimate which An estimate which looks like this. So you have these two numbers in front. Probably I should not write that. Let me probably erase this, what I've just written. So in the end, you get an estimate for the increment or for the error propagation in the UL, which is a simple term or a simple estimate involving two terms on the right-hand side. Terms on the right-hand side. And for the propagation of error, if you like, in the V part, you get by this nested integration, by this nested application of the Birkhoff contraction, you get a combined estimate on the right-hand side, which involves now slightly more complicated wave factors. Okay, and now you introduce this vector of error terms just for the fun of easier notation, and then you can write this. Easier notation, and then you can write this what you have just found just in this, if you like, pseudo-vector notation by saying that the vector that the error in the UL part can be represented as an application of the matrix T omega, which I write down below here, applied to the error in the UL and in the V part. And the error in the V vector is applied, is obtained by having the error on the previous step again inserted into this matrix. Inserted into this matrix. And this, let me say again, is the matrix that shows up by just writing down what you have just done before. And this inequality here is an inequality of vectors in a component-wise sense. So this is an inequality of vectors of non-negative vectors component-wise. But since the matrix entries are all non-negative and the vectors which appear have all non-negative entries, you can iterate this point-wise. entries you can iterate this pointwise inequality of matrix and vector application and so therefore you find that that this whole that these these vectors converge in fact to a zero vector as long as the spectral radius of this matrix as long as the spectral radius of this matrix is strictly smaller than one because it's just an iteration of a matrix Because it's just an iteration of a matrix, and you write down, you can compute the spectral radius of this matrix, of course, explicitly. And so you find that the spectral radius of this matrix is strictly below one, if and only if this weight factor is less than two over one plus lambda. So it's very simple, and this shows you that the method does not at least at least gives you the solution when omega is. gives you the solution when omega is positive, basically above one. And now we come for the question to the question whether it is actually beneficial to do this. And here we draw from the previous work by the four French colleagues. What matters here is now what is called the local convergence rate. And I shall probably spare the details, although it's not so complicated. But you have a non-linear iteration, and you look for a fixed point. And what you actually need to identify is, if you like, the first interesting eigenvalue of this nonlinear iteration, which contracts or yeah, the first, which basically measures the rate of contraction of all your phase space onto your dimension on which you find your solution around the critical point. So it's effectively a spectral analysis of your nonlinear operator in the vicinity. Operator in the vicinity of a solution. So, therefore, in this spectral analysis argument, what you need to investigate in the end for the question of, if you want to investigate the question whether it's a good idea to do that, you need to analyze the linearization of your synchron iteration in the neighborhood. In the neighborhood of the solution. Because this is where the optimality of the choice comes in. Because this is, if you like, the final leg of the algorithm. And it tells you how good in the final stage the algorithm will be when it is already close to the solution. And then the contraction or the speed of the convergence of the algorithm will be effectively governed by the spectral behavior of that linearized operator. So therefore, if we assume for a moment that we have found our solution. Assume for a moment that we have found our solution u star and v star, then we can write down the linearized version of the synchron operator, or the sorry, yes, of the synchron operator. And if we so m is our hero in a sense, that's the linearization. And this linearization has a trivial eigenvalue one because that corresponds to the actual solution or the direction of the solution to which you want to contract. And then there is a second largest eigenvalue. And then there is a second largest eigenvalue, which, since it's a it's it's a it's a doubled coordinate space, so to say, we call that theta squared. And then you can find that, and as a matter of fact, it's a standard theory, in fact, in this method of over-relaxation, which was applied classically in linear systems since very long. Then you can find an explicit formula, which is very nice, that tells you what is the What is the sorry, I should probably say, you can find an explicit formula which tells you what is the corresponding spectral radius of the linearized modified method. So let me say it again. Theta squared will be a datum of the problem. It's the second largest eigenvalue of the linearized synchron operator in the neighborhood. Synchron operator in the neighborhood of the solution. And then there is a formula, once you know this theta number, there is a formula how fast your method will contract depending on the choice of the oh yeah. And the graph of this function, this is a classical graph that you find in textbooks and numerical analysis, will have this shape. Numerical analysis will have this shape. There's some sort of a square root function, and then it is extended by a linear function. So there is a clearly optimal choice of omega star, which will produce a minimal spectral radius of this localized synchron method. And this optimal choice of an omega is given by this formula. So, this is very good news theoretically because it tells you that you can really. Tells you that you can really make it the algorithm much faster if you make this over-relaxation, at least in the neighborhood of the solution. But the downside is that in order to actually choose this parameter, you need to know this second largest eigenvalue of the linearization in the neighborhood of the solution. And that, of course, raises the question: how can you get any information about Question: How can you get any information about this? Because this is something you don't know that, because it's some information which is connected to the solution which you want to identify with the algorithm. And this is now a second remark. Of course, theoretically and here explicitly, it is possible to give estimates on this theta squared number, which is by using just information on the problem. So, suppose you have. On the problem. So, suppose you have the matrix K, which is part of the problem. Suppose you have the marginal distributions A and B, which is part of the problem. And you want to know what is the second largest eigenvalue of the entropic, of the synchron solution, of the linearized sink on operator close to the solution. Then, of course, it is possible to come up with an a priori estimate for this. Come up with an a priori estimate for this for this parameter data square, which uses only the data of the problem. That's an honest a priori estimate. It might not be the best that you can come up with, but this, if you like, is just a proof of concept that it is possible to come up with such a thing. So let me say it again. Theta squared is a datum, which is very implicit in the problem because it will involve the linearized, the second largest eigenvalue of the linearized synchron iteration. Linearized sink on iteration in the neighborhood of the solution. You would wish to know that in order to tune your algorithm in an optimal fashion. But it's not available, of course, at the beginning. But at least theoretically, you can come up with certain bounds which use just the information of the problem that you have at the beginning of your argument. And so, if you combine these two statements, if you combine these. If you combine these two statements, you come up with our actual result, if you like, that if you have a certain ranking condition, okay, so the problem is full rank, then there is an explicit range, or even, yeah, there is an explicit range of over-relaxation parameters, which is strictly above one, in which the over-relaxed sinkhole is both globally. Is both globally convergent, so it is admissible, and it is asymptotically faster because the spectral radius for this choice of omega will be better or smaller than the spectral radius of the conventional synchronic. Okay, so this says this as a long, this was a long comment to the work of the French colleagues who proposed and who proposed, as I just Who proposed, as I just tried to say, who proposed an adaptive choice of the omega L's by a certain mechanism, which we found very interesting, but couldn't quite comprehend. But of course, it's a good one. Okay, we don't argue, but we were trying to come up with our own ideas. So, and what we say is, okay, if you like, with this rather theoretical result, we say, okay, you don't have to be adaptive to beat Synchor, you can do it. To beat Syncorn, you can do it straight away with one proper choice, which is a priori obtainable, and then you already will beat Syncorn. Although when we did now our practical simulations and we computed that thing and we found out that, yes, of course, it does improve the sink on in general situations, but the improvement was rather limited. So, therefore, we still Uh, we still, I mean, in practical applications, it might still be beneficial to pursue an adaptive strategy. And here is, if you like, our take of a mock adaptive strategy. We here's our numerical examples that will finish my talk. So, let me say it again. You would want to do this over the relaxation. You want to tune these parameters, and once you have the theta, And once you have a deep theta or theta square, you can choose with this rule, with this square root formula, you can choose the omega and then you can do it. The theta is not admissible. And what we can do instead, of course, it's very close to what was proposed by the French colleagues, we can let the standard synchron run for a couple of times in its standard fashion. Then we can hope that we are already, after so and so many steps, in the neighborhood of this stable. Of the stable point. And then we can try to measure this second largest eigenvalue of this matrix, which is basically exactly this contraction. This is, well, this, yeah, this measures, if you like, the contraction towards the stable direction. This is a two-step contraction, so to say, so therefore you have to take the square root of this. This is then an estimate of the theta squared, which you are looking for, and this you can use to. You are looking for, and this you can use to update your Omega. And then from that point on, you run the whole algorithm with that Omega. So, here's now our examples, just for illustration. So, these I do almost not quite remember anymore where these examples were taken from, but these believe me, these were examples that were considered elsewhere before. But don't push me too hard to recover that. Hard to recover that. Okay, so here is the blue curve in both two different examples of a concrete synchron or modified synchron computational problem. The blue curve, so you have two certain marginals and you have a certain matrix K, S K, which comes from a certain practical application. And in these two cases, we just compare the different algorithms or the two. Algorithm, so the two essentially. The blue one in both cases, in both graphics, is the plain synchron. And the error here, the error measure is here just the difference of the this here P L or P star is effectively what we call the optimal transport plan, so to say, it's just the probability distribution effectively on the product space. And we measure our Estimated coupling probability measure against the actual optimal relaxed coupling measure in total variation on L1 OR. And so this here shows you how fast or not fast the classical synchron algorithm converges in this case. And let me see. So the yellow one tells you how the It tells you how the synch on the module, the over-relax synch on performs if you knew the optimal over-relaxation parameter. So this is in a way backward engineering. So we let the blue thing run until we basically have the until we have the stable point. With this stable point, we can compute the exact theta squared, and this can be used to run the synchron again, but now with the proper over relaxation parameter, and this produces the yellow curve. So if you had known The yellow curve. So, if you had known beforehand the optimal theta squared and therefore the optimal omega, then you would have obtained the blue, sorry, the yellow curve. The red curve in this case is if you just make a totally unbased guess, you just say, Okay, I choose omega to be 1.5, why not? And the purpose. And the purple curve is the one that we, if you like, suggest you run a couple of steps. Here it's about what? Okay, here it was 20 steps or something, sink on steps, say. Then you do your estimate according to this raw, according to this rule of the theta. And then you switch on the over-relaxation with this omega, which is obtained from this formula. And then you see that the second stretch of the algorithm performs much better than the. Performs much better than the standard symbol. And here is another example where you can see the benefit of that also quite a bit. Here again, the blue line is the standard synchron for that problem. The red one would be just stupid guess of, it's not a guess, it's just putting 1.5. And yellow is the best one possible. And the purple one is the one which is obtained after. Is the one which is obtained after when you do this one guess update rule? Okay, that concludes my talk. I think I'm pretty much in time. No, kind of. And this is some references. Thank you very much for your attention. All right. Thank you, Max. Okay, so yeah, the floor is now open for questions. If you have any questions, please go ahead. You can just speak. Any questions, please go ahead. You can just speak up and ask the questions. The 1.5 that you stupidly chose, was it within the range where you're guaranteed convergence? No, no. Okay. So, Max, I have two questions. One is that So, why stop at looking at one step? Can't you just average over all the past steps of Syncon and make things better? Is that something feasible? I'm not so sure because somehow, okay. No, I just my feeling. So, okay, you want to estimate a quantity which you can observe along the trajectories, but sometimes here you see some periodic behavior. Periodic behavior, which seems to suggest, yeah, you have basically this matrix which you iterate here is some sort of a Hamiltonian, it has like a little bit of a Hamiltonian structure. It is skew symmetric in a way. The problem, this iteration thing has some, a little bit of a skew symmetric, if I'm not mistaken, a little bit of a skew symmetric structure. That's how I read it. And so it's not so, yeah, but that would basically support, by the way, your point that somehow taking average observations somehow would establish. Uh, somehow would stabilize your estimates. Uh, that is that's so, yes, I wanted to use this analogy to argue against what you propose. But yeah, so the wide open space now for all kinds of speculations. What is the best empirical algorithm to so I don't know at the moment, and it's it's it's it's something that we actually really want to look into seriously. Really, want to look into series. So, Lior is writing something in the chat. Lior, you want to speak up, or just as you want me to just read it? So, okay, so sorry, just one thing which is just, it was very nice to see how you use the kind of linear approximation near the, as kind of the iterations as a power method to compute the eigenvalue that you want. That was really nice. Yes, thank you. And about using many, just about using the full. Many just about using the full history, I think there would be a memory issue, right? If I think about usual numerical linear algebra, the matrices are very big, and we cannot store the whole history. That's one of the advantages, right? No, no, but you only have to store the running sum. You don't have to store the whole history. You can do one update on the running sum and adjust the weights, so to say. Yes. Multiply by n over n minus one or something like this. Is anybody keeping track of the Is any way keeping track of the last matrix? Yeah, but yeah, I mean, one, yeah, I mean, because I mean, one way of thinking of this overaxation is kind of trying to do a Taylor approximation to what we're doing and predict what the limit is from these two observations. So this is the other thing I was trying to understand, because if I understand this geometry correctly, that the synchron is like doing a gradient descent in the dual coordinates, right? In the dual function is doing a gradient descent. I mean, that's what I heard from, I mean, I mean, that's what I heard from. I mean, I learned. So, does it have a geometric way of understanding? I'm sorry. Does this geometry improve the understanding of that? Yes, yes. How doing a gradient descent, if you interpolate or extrapolate sometimes between two successive points, you can actually converge faster. Yeah, and I think this is roughly what this is the general idea, right? We're going to say, right, and maybe with three, I mean, maybe you can take, if you take the most recent three observations, you can try to do a better Taylor approximation to predict where you're converging to. Yes. But my way of thinking about it is that if you take a Markov chain, you know, a Markov chain, if you repeat it, sure, the last guy converges to the invariant distribution, but you can. Converges to the invariant distribution, but you can average over the past history and it actually converges much, it's much more stable. So that's why I was wondering whether it's something related to that. Yes, all this is very, very correct, what you said. I can just agree. I can just agree. Yes. It is, we haven't, I mean we haven't gone that far. We were just at this moment we are very early in our investigations to this, we were kind of happy to to have this kind of stuff. But all of the what you propose is just to All of what you proposed is just going. Any other questions for Max? All right, okay. If not, let's thank the speaker again. It's a very nice talk. And we will reconvene in four minutes. So a short bathroom break and we'll reconvene in four minutes.