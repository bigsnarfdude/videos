Now you can see him, I guess. Yes. Okay. People on Zoom can hear me okay? Yes. Okay. Okay, so improved time complexity, improved space complexity. It can improve communication complexity. So maybe you have like vectors stored on different machines and you need to pass them between each other. They're stored in different parts of memory. Now you're only communicating order login numbers when you need to communicate a vector instead of order D numbers. So the big picture here is like you do this pre-processing. Here is like you do this pre-processing and it hopefully improves many aspects of your algorithms, like runtime space, all sorts of stuff, by just getting you to work with smaller data in the first place. It tends to be a very, very flexible technique and usually is combined with other techniques. So like sketching like this is very commonly used in data structures for near-data research, for example. Okay, so that's general sketching. General sketching is used in like all corners of algorithms these days. So I think it became very popular in streaming algorithms. It became very popular in streaming algorithm design for vector data, for graphs. It's used in computational geometry. Distributed algorithms, or now as the machine learning people, it's called federated learning methods. Search algorithms, it's super widely used in database algorithms. I have a lot of work on that. And then, of course, sketching is like an important tool within linear algebra. And that's what I'm mostly talking about. It goes under different names. So in different areas, or just people using names interchangeably, if you hear terms like dimensional. Interchangeably. If you hear terms like dimensionality direction, sparsification, core sets, these are often just different names also referring to sketching options. Okay, but obviously we care about matrix sketching in this workshop. So we're going to talk about matrix sketching. And what I want to start talking about is trying to put down some like pretty general purpose definitions for what information can a matrix sketch preserve. So for vectors for JL, it was very clear. I have a vector, I compress it using JL. A compressor using JL, and we preserve the L2 line. That's the piece of information we preserve. That happens to be very useful. For matrices, it gets a little bit more complicated, so we're going to try to put down some definitions about what information is preserved. How do you compute sketches, which are good at preserving this type of information? And then what are different ways of using these sketches, which is important. Joel talked a lot about this in his talk on the first day, but like sketching isn't something that's usually used in isolation, so it's important to understand how it pieces into other parts of your controversy. Pieces into other parts of what you're controverting. Okay, so I'm going to talk about two cases dip separately because we sort of usually ask for different guarantees. So one type of case people think about is when I have a very, very, a matrix that's very asymmetric in its dimensions. So always think about it as very tall, but it could be very wide, and I want to compress that down to something that's like closer to a square. That's one type of case that we'll consider. The other case we'll talk about, second part of the talk, is. Case we'll talk about the second part of the talk is: I have a matrix that is not very asymmetric. We mentioned something square or near square. It doesn't really make it some sense to compress down just to get at the square. So, we'll be trying to compress that to a skinny matrix. Yeah, they're both tight ways of compressing by either reducing the rows or the columns in the matrix, but the sorts of guarantees we ask in these cases tend to be different. Okay, so let's start with case one. Here's a really basic guarantee in matrix sketching. In matrix sketching, that is like you know, one of the fundamental most useful guarantees. Actually, just by show of hands, this is called the subspace embedding guarantee. Who has seen the subspace embedding guarantee? Okay, so like, yeah, 80% of us, which is good, but not everyone. So we're going to go through it slowly, but that's great to know. Okay, so I don't know actually where this, I think the people who coined the term was this paper by Wolf, Liberty, Rockland, and Tiger, because they referenced Sarlos was the first. Because they referenced Sarlos was the first one, I believe, to prove a subspace embedding guarantee, and then they called it that in this paper. So that's where this goes back to. The guarantee is as follows. I have a matrix A, which is N by D, and I'm thinking of N is much bigger than D, so it's very tall. We say a matrix A tilde, which is a compression of that matrix, a sketch, is a subspace embedding for A if for all X simultaneously, so for all vectors X. x, a tilde x, that two norm is sandwiched between 1 minus epsilon of the 2 norm of Ax and 1 plus epsilon of the 2 norm of Ax. Okay. For a single vector x, so if I didn't want this for all, but I just wanted this for a single vector, this is the same as the J L guarantee. So think of y as Ax. This is just saying, like, I have a vector, and I want to compress it in such a way that it's two harmonious. Compress it in such a way that its two mark is preserved. But the trick here is that we're basically trying to get a J L type guarantee for all vectors lying in a subspace. So, vectors that can be written of AX, that's all vectors in the subspace spanned by the columns of A. And we're trying to get a sketch that can sort of preserve the norm of all of the vectors in that subspace. That's why people call this a subspace embedding. Okay, we'll see how this is useful in a bit, but let me just first talk about how we can get subspace embeddings. So, here's a Embeddings. So here's a claim from Sarlos, which is that if we, I think he got a slightly weaker bound on this, but this is what you can prove. If you set m, your sketching dimension, to be d plus log 1 over delta over epsilon squared, so m is going to be the height of my short matrix, A tilde. If you just multiply A by, again, a properly scaled random Gaussian matrix with M rows, we're just going to compress by multiplying by a Gaussian matrix, then with high probability, probably 1 minus. Then, with high probability, probability 1 minus delta, a tilde, which equals that g times a, will be a subspace embedding for the tall matrix of A. Okay, how do you prove this? It's actually a super straightforward proof. Like I do this in my master's level algorithms class. You basically prove that for any x, a tilde x approximately equals ax. And the proof of this is because a tilde x exactly equals like a Gaussian times Ax. And so you use the same like calculation. And so you use the same chi-squared concentration that you did for yeah, go ahead. So are you saying that for a Gaussian you're not getting anything better than just epsilon nets? For a Gaussian you can get the tight bound using epsilon nets. You need to be a little bit careful how you do them. So if like, I think the Sarlos paper doesn't do it quite tight. You can get the tight bound just using epsilon nets. You don't need fancy matrix concentration or anything. You get better constant type matrix concentration. Oh, do you get a, just a constant type from epsilon net? Just a constant type from epsilon? Sharp constants if you use a landing chord and one modes. Okay, yeah, I'm not saying optimal to constants. So you get the right bound to like the terms, like d plus log 1 or delta or epsilon squared. With epsilon nets, but yeah, maybe you can squeeze with more concentration, squeeze something better out of it. Yeah. I understand that n would be quite large here. N could not quite less. Epsilon is 10 minus 4. That's a great point. That's a great point. So, okay, we're going to talk about this. If epsilon is small, m is huge. So, then you're not gaining anything from this. So, when we're talking about applications, we're going to try to focus on ways to use these things where epsilon is like a constant. So, I'll talk about an application, for example, where epsilon is 1 half. So you can think of like 1 over epsilon squared as like 4. I want to write these things because I think it's important to keep around the epsilon dependence to emphasize that, yeah, as soon as you go to small epsilon, these start becoming much less useful techniques. Becoming a much less useful thing. So, any other questions about that? So, basically, to get this bound, you use the same chi-squared concentration you would for JL. And now you need to union bound over all vectors in the subspace banned by the columns of A. It's an infinite number of vectors, so you do like an epsilon net argument to get the final bound. Okay, and this is the payoff here. So, like, when I think So, like, when I think about that, if you ignore the epsilon squared, which is important to remember that it's there, you basically take a matrix that's arbitrarily tall and you compress it to something that's almost square. The dimension you compress it to has nothing to do with the original height of the matrix. It just has to do with the width of the matrix. Okay. So, that's one way to get subspace mattings, multiply by a random Gaussian. There is a whole industry, and maybe it slowed down a bit, but there was a whole industry for. Down a bit, but there was a whole industry of other methods for getting subspace embeddings. So the first thing people asked was: Can I get random matrices that I can multiply by my matrix A more quickly? And people did that by using like sparse random matrices or very structured random matrices, which can be viewed as like randomized versions, like randomized permutations and scalings of forward matrices. So the first thing people want to do is: I want to do the sketching faster, because if I'm going to actually use this, I need to be able to compress the matrix fast. I need to be able to compress the matrix fast. They replace random Gaussian matrices with sparse random matrices and structured random matrices. And that sets you compute subspace embeddings in either linear time, so like roughly order ND, or tilde ND, which means there are some extra log factors there. There's also been a lot of work on sampling methods for compressing matrices, and we'll talk a bit more about those later. But this basically gets your subspace embedding instead of by multiplying A by a random. Multiplying A by a random matrix, it subsamples and re-weights some subset of the rows in A to get A tilde, and then argues that that's a subspace of it. Okay. One thing I want to mention, because people have heard this term, if A is the edge vertex incidence matrix of an undirected graph, when you compress A, if you get a subspace embedding for A via sampling, you start with an edge vertex instance matrix, which is like every row corresponds to an edge. Like every row corresponds to an edge and has a plus minus one at the terminal nodes of that edge. When you compress A via sampling, you get some subset of the rows in that matrix. It stays in an edge vertex incidence matrix. That edge vertex incidence matrix corresponds to what we would call a spectral sparsifier for the original graph that created your edge vertex incidence matrix. It's the exact same guarantee. So when people who work in graph algorithms talk about spectral sparsification guarantees, it's literally the same definition as subspaces. Literally, the same definition as subspace embedding guarantee. The only thing is you're restricting that your subspace embedding is a subsample of the rosenite. So I know that's a term that comes up a lot. It's been super important in all the work that Rasmus has told us about and fast Laplacian system solvers. It's really the same as subspace of metal. Within these two types of sketching, people sort of classify them into two groups. They would call some things oblivious sketches and some things not oblivious sketches. Oblivious sketches. Oblivious sketch means that, okay, for all of these sketches I talked about, your compression technique is actually linear. It takes the form of multiplying A by another matrix. Even sub-sampling can be viewed that way. When you're going to sub-sample, that's the same as multiplying by a super sparse matrix that sort of plucks out the entries in A. So all this gadget techniques I'm talking about linear. And in oblivious techniques, your linear transformation G is not allowed to depend on A. In non-oblivious techniques, it might. So subsampling usually. So, sub-sampling usually needs to sub-sample with very specific probabilities that are chosen based on A. So, we call that a non-oblivious sketching technique, where oblivious, you just pick like a random Gaussian matrix. It doesn't depend on all. Okay. This stuff tends to be useful because oblivious sketches are very easy to work with in streaming distributed environments. So, the nice thing about oblivious sketching is if I have a matrix that starts, I have some rows A and then some rows B. I have some rows A and then some rows B. Maybe B is stored on a different machine. Maybe I haven't seen it yet. I'm going to see it later. You can compress A separately and then compress B later. And since all you're doing is multiplying by like an independent Gaussian or plus minus one matrix, you don't need to have C and B before you decide on how you're going to compress A, which I think is important. So you can basically compress matrices that are strewn all over the place and then merge it back together after compressing. Okay, so what can we do with subspace embedding sketches? We do with subspace embedding sketches. This is a reminder of the guarantee we have. The first thing that you might hope to do, which is very simple in your algebraic problem, is use the subspace embedding guarantee to solve very over-constrained least squares regression problems approximately. So the idea is: I have a super tall matrix A. I want to minimize over all vectors x, ax minus b, and here's the claim. X minus B. And here's the claim. If I compute a subspace embedding sketch for the matrix A with B appended onto it, so there's now a D plus one worth matrix, call that subspace embedding sketch A tilde B tilde. So it's still D plus one and now a lot shorter. If I let X tilde be the minimization of the Least Furs regression problem for my steps, so for A tilde and B tilde, then if I had an epsilon accurate subspace embedding, Epsilon accurate subspace embedding, so in other words, it took roughly d over epsilon squared rows, I can ensure that the error of x tilde on my original least squares regression problem will be no more than 1 plus epsilon times the minimum over x of ax minus b. Okay, maybe this is like super obvious to people why this guarantee is true, but I'll just like write it down just so we can see it. How do you see why this is true? Look at ax tilde minus b. Minus b. This is equal to if I took the matrix AB and I just multiply by the vector x tilde with the minus 1 on the bottom. Okay, by our subspace embedding guarantee, this should be less than or equal to, I'm looking at like this side of the inequality and dividing by 1 minus epsilon, this is less than or equal to 1 over 1 minus epsilon of a tilde. A tilde b tilde x tilde minus 1 okay which in turn is less than 1 over 1 minus epsilon of a tilde b tilde x star minus 1 because x tilde is the optimal regression solution for the sketch problem. So it certainly gives better error than if I use x star which I'm letting x star If I use x star, which I'm letting x star be our min of the original problem. And then this, in turn, is less than or equal to 1 plus epsilon over 1 minus epsilon AB x star minus 1, which is this right here. Okay, and the last inequality, I just use the right-hand side of the subspace, and I didn't guarantee. Okay, 1 plus epsilon. Okay, 1 plus epsilon over 1 minus epsilon, that's basically like 1 plus order epsilon. So as long as I like set my epsilon slightly smaller by constants, I can get this guarantee with order d over epsilon squared dimensions. Okay, very simple argument. Here's the punchline. For an n by d matrix, you can compute substitutes of endings roughly in order n d time. So if I want to solve a regression problem to epsilon accuracy, instead of taking Accuracy, instead of taking at worst case order n d squared time, if you're going to do that using a direct method, we reduce that to order d cubed over epsilon time, epsilon squared time, plus the cost to compute the sketch. So basically plus the cost to do like a linear pass over your matrix, which is great. You can improve on the epsilon dependence. So the argument I just gave is not optimal, and you can get this down to a single epsilon. But I'll still say that these aren't very super well-used techniques, just sketched regression, because that epsilon. Sketched regression because that epsilon is really costly, right? Like, ideally, you want to get the regression solution quite accurately. Or, I don't know, in machine learning, if you have a big enough data set where you're trying to, you know, you don't care about getting an accurate solution, you might be using some fancier machine learning technique like neural network or something. So, like, I think I haven't seen a ton of applications in practice where people straight up use sketch regression, but I think it's really good to know about because it's the baseline. And actually, this week I learned about some interesting. And actually, this week I learned about some interesting applications. So, like, Joel has a paper on this on using sketch regression to do approximate orthogonalization. And the Davidia, I think, you were using this as well. Say I have like a Krillov subspace and I want to orthogonalize it super fast or a block Krillov subspace. You can view this as solving a regression problem. And for those sorts of problems, you're just trying to maintain some sort of stability. You don't need to do it very accurately. So there are cases when you need very, very coarse regression solutions, and this sort of thing finds it. Solutions and this sort of thing finds approximation or finds applications. Okay. There are other ways, however, which I mentioned in answer to Yusuf's question, to use sketching where we don't need to worry about this really, really bad dependence on Hepsalon. So a classic example, which was, I think, first introduced by Rockland and Tiger in 2008, is to use sketching as a preconditioning technique. So in this case, I'm going to set epsilon to be a constant. I'm going to set epsilon to be a constant. Specifically, I'll set it to be 1/2. And I'm writing down here just my subspace embedding guarantee for epsilon equals 1/2. It says that for all x, 2-norm of AX is sandwiched between 1 half and 2 times the 2-norm of A tilde x, which A tilde x is my sketch. Equivalently, this is the same as just say the quadratic forms are close up to a constant factor. This holds for all x simultaneously. So I can move an a tilde transpose. Move an a tilde transpose a tilde negative one-half inside, like in between the x's here, and that inequality stays valid because it holds for all x. So I do that. I multiply on the left and right by an a tilde transpose a tilde negative 1 half. And what do I get? I get that x transpose, a tilde transpose a tilde, negative 1 half, a transpose a, a tilde transpose a tilde, negative 1 half, x, is sandwiched between 1 half and 2 times x transpose x. In other words, between, it's sandwiched within a constant. In other words, between its sandwiched within a constant of the two marked x. This immediately tells us that the singular values of this matrix lie between 1/2 and 2. So if I took A and I multiplied on the right by A tilde transpose A tilde negative 1 half, this is a super well-conditioned matrix as a constant conditioner. Okay. What does that mean you can do? You basically pick as a preconditioner A tilde transpose A tilde to the 1/2. Pass that into any precondition solver. Like, I think in Rock and Tigrit, they use precondition CG, but you can really use precondition whatever you want because your condition number is really a constant. And what will be the cost of solving a regression problem this way? Every iteration you need to multiply by your matrix, which is at worst case ND time, but it could be a lot faster if your matrix is sparse. So that's where that's coming from. And you need to solve a system in your preconditioner. system in your preconditioner, that's now small, right? So that can be done. You could explicitly like invert that and just keep multiplying by it. That basically takes like N D cubed time. Wait, I should have gotten, well it depends. I guess I could have a D cubed on the outside and make that a D squared. Yeah. Doesn't matter too much. So it's like some low polynomial in D. And then how many times do I need to run my iterative solver? I have a constant condition number. Solver, I have a constant condition number, so if I just run it for log one over epsilon iterations, I'll get to something that's epsilon accurate. So this is another way of solving a least squares regression problem using sketching. And it got rid of the very, very poor dependence on 1 over epsilon. We just had to set epsilon to a constant and then combine it with the narrative method. Okay, make sense? Here's another interesting way to use scheduling. Those were two different epsilons. Way to use scheduling. Those were two different epsilons, though, right? Are you fixing epsilon in the choice of the you're right, sorry. This is like the epsilon. Okay, I set epsilon to be one half. And this is the epsilon with which I want to set the solver. This is the tolerance for the CG solver. This is the tolerance for the CG solver. So this isn't a log of one over epsilon to the one. This isn't a log of two. This is a log of whatever accuracy I want to solve the system to. Yeah, thanks, Joel. Okay. Okay. I'm going to talk about one other interesting application of subspace embeddings, which is like a little non-standard in that it's not exactly linear algebra, but I love it because it's just like a cool, I think these things just tend to find applications and it's a cool like non-standard application. So this is specifically going to focus on subspace embeddings that are obtained by samples. So one thing that's been known since like Spiel and Trivistava 2008, when they first studied the case, the graph case, but everything generalizes. The graph case, but everything generalizes to matrices, is that I can get a subspace embedding with d log d over epsilon squared rows, so almost as good as I got with Gaussian, except I pay to log d. If I subsample rows of my matrix A via what are known as the statistical leverage scores of A. I'm not going to go into the details of what those are. There's some way of measuring importance of rows in A. They have like a very nice closed-form solution and they can be computed or approximately. So there are sub-sampling methods. There are also a lot of other sub-sampling methods. There are also a lot of other sub-sampling methods. So, like McCall has done a lot of work on TPP sampling, determinants of point process sampling. There's like adaptive sampling methods. There's a ton of sub-sampling methods, but this is basically what you can do. You can get near-optimal sized subspace embedding using sampling. Okay. Here's a surprising application of subspace embeddings if you compute them using subsampling, which is active machine learning. So the setup here doesn't look like linear algebra. I'm given some data points G1 through GN. So in my case here, they're points. So, in my case here, they're points in like a two-dimensional space. And I get to query the value of some function b at those data points. So, I could query b for all of those data points. And then I want to approximate that function by some simple function from a function class f to interpolate it at the places I query it. So, you can imagine the function class f being like a polynomial. And the question in active learning, or some people call this experimental design, is how few queries can I? How few queries can I use to learn the function to find the best near-best approximation to the function in the function class f? Okay, so we've been very interested in these problems related to applications in solving parametric PDEs. Usually the setting there is you want to approximate some quantity of interest surface for a differential equation. Like you solve the differential equation and then look at some parameter at the end. Every point here corresponds to solving the differential equation. To solving the differential equation with some different set of input parameters. So it's like very expensive to collect a data point, and I just want to collect as few data points as possible to try to get a mapping something like this, which tells me what the function looks like over the whole space. Okay, how can this be solved with a subspace embedding? If your function class is linear, so say it's like polynomials or like Fourier functions, lots of other function classes, you can encode it. Classes, you can encode it as a regression problem. So you take your input gi, which is say a point, two-dimensional space, and you collect vectors with all the features that you want to use in your function family. So for polynomials, this would be like all the monomials or all the genre polynomials applied to data points. And you put those as rows in a big, big matrix A, which contains all points on a super fine grid that you could have sampled. Sampled. And then you say, I'm going to get a subspace embedding for A via sampling. What did we say? We said you could approximately solve for the regression problem, minimize AX minus B using a subspace embedding. But since it's a subsample, you don't actually need to collect entries of B unless they happen to be the rows that get sampled when you produce the subspace embedding. Right? So you basically construct the subspace embedding, but you don't collect anything that you need for it until the very end. You see what rows get sampled. Until the very end, you see what rows get sampled, and then you sample those entries in B. And then you solve this small regression problem, which only required looking at a subset of entries in the vector B, which corresponds to a small number of samples in your space. Okay, so yeah, what's the punchline? If you have a function family f, which is linear in D features, then it's possible to learn f tilde, which obtains near-optimal L2 norm error. So your f tilde will be like, Two-norm error. So your f tilde will be like within a one plus epsilon factor of your best error you could have gotten from a function in the function class. And you only need it to collect d log d over epsilon function queries. Actually, the way I proved it had an epsilon squared, you can approve it to an epsilon. Yeah. So if I'm going to solve a regression problem, I need my subspace embedding to be an embedding for the augmented matrix A, B. Okay, yeah. But in this case, it's going to Okay, yeah. But in this case, it seems like if you're only looking at A? This is a really good question. So I was like pushing this on the rug and I didn't think anyone would notice, but that's sharp. So it ha Yeah, I didn't look at B when I created my subspace embedding, and then you might say that might be a problem because instead these sampling probabilities need to depend on the matrix, which is A with B appended on. There's basically a way to do the analysis where you can say that it's safe to import B. It takes a bit of extra work, but it's not super hard. Great question. Only for L2 regression, but it is a very important thing. Only for L2 regression. Only for L2 regression. Yeah, yeah, that's a great point. So, well, wait, now we do have methods for L1 regression. So, thanks to McCall, where you can avoid looking at. Yeah, he has a paper by Shui Chen, which actually just recently solved this for L1. Yeah, that's a problem. We've been really interested in what other function classes can you engage for. Yeah, but for L2, it's not actually a whole lot of extra work, and you can do it. Okay, so actually, like, okay, maybe. Okay, so actually, like, okay, matrix sketching we care about usually for computational reasons, but this was rediscovered, I mean, essentially rediscovered in the PD and uncertainty quantification community. So all these results on like approximately squares regression, you can find them written up in Hampton-Dustam, Cohen, Miguel Rati 2016. I think they had the benefit of Joel's surveys on matrix concentration already existed when they rediscovered the results. It was somehow easier when the people at NLA and TCS did it the first time. Did it the first time, but it's a cool thing. So, how these things were widely used in these active wearing systems. Okay. So, I think that shows you some of the flexibility of matrix sketching in case one when I have like a super tall matrix. I'm going to talk now about matrix sketching in case two when I have like a squarish matrix. I'm going to compress it to something skinny. Okay, in this case, when we do our compression, we reduce the rank of our matrix. So we're not going to be able to hope for something like We're not going to be able to hope for something like a subspace embedding guarantee. There would now be vectors x that I can multiply by a tilde, which will have zero norm. And they didn't have zero norm when I multiply by a. So, like subspace and betting is out the window. So, the question is, like, what information do you want to preserve? And roughly, sketching usually hopes to preserve information about the top singular vector subspaces of A. So, the hope is that A tilde contains some information about the largest singular vector subspaces of a matrix. Okay, I'm going to. Okay, I'm going to write down a definition here, which was originally introduced by Danny Feldman, Melanie Smith, and Christian Soler in 2013. And then we sort of coined the name for it in the 2015 paper. This is not like the only definition for what a low-rank sketch preserves. I'm giving it to you because it's like, I think it's an easy one to conceptually wrap your head around. But I won't even say this is subspace embedding is really a gold standard, everyone uses this. A gold standard. Everyone uses this. Projection cost-preserving sketches I really like because I've used them a lot, but they're like a less popular one in the community. So it just gives you a picture of that. Okay, anyway, here's the statement of what a projection cost-preserving sketch is. We have some matrix A. Again, it's N by D. I'm going to say that now a skinny matrix, so first dimension stays the same, but it went from D width to M width. A tilde is a rank K projection cross-preserving sketch. If for all rank K projections, P, P. If I look at the cost of projecting A onto the subspace banned by P, so A tilde minus P A tilde, this is sandwiched between 1 minus epsilon and 1 plus epsilon of the cost of the L2 cost of Frobenius norm if I projected A onto that subspace. So the whole idea is like in low-rank approximation, you're trying to find a good subspace to project your matrix onto, so you get small for meaningful. Onto, so you get small Frobenius norm error. This is saying for any rank case subspace, I can estimate the Frobenius norm error I would have gotten by projecting A onto that subspace using my sketch, which is now a much smaller size. Does that definition make sense to people? Less people will have seen them. Okay. Okay, quantifiers, this is for all P after you. P after you made the sketch? It's for all, yeah, it's for, I mean, it's universally over all ranked K projection matrices P. So, like, it's not, there's no before or after made the sketch because it really holds for all of them. So, you sketch and you say, I can guarantee it for everyone. Okay. How do you compute projection cost-preserving sketches? This will be really fast because you basically do the exact same stuff you did for subspace embedding, but you make your matrices skinnier. Make your matrices skinnier. So, like, you can multiply by a random Dauschian matrix. And if that matrix has k over epsilon squared columns, so now you're compressing from width d down to k over epsilon squared. So this would be my k tilde. This would be k over epsilon squared. Then that would be a projection cost-preserving sketch. So now the dimension I reduced down to is parameterized by this k parameter. So this basically says I'm going to preserve information about k-dimensional subspaces. The higher k goes, the bigger. The higher k goes, the bigger I need to get my sketch. So you can use random Gaussian matrices, random plus minus ones, you can use sparse sampling matrices, you can use sub-sampling. Basically, everything that worked for subspace embeddings will work for getting projection cross-projection sketches. You just sample less things. Yeah. The JL analysis with the epsilon net and all that, does that, like if you just put Gaussian, is that the extent this? Sadly, no. Like, our original analysis showing that Gaussian worked is really messy, and then Work is really messy, and then Petrus, I think you tried to simplify it. We tried to simplify it. I think we made some amount of progress, but like not that much. So, yeah, the analysis is much, much more complicated than somebody's bad. Yeah. Yeah. I mean, I have like a note on my website that says like a simplified analysis of protection costs per turn sketches is 14 pages long, so like we failed in that respect. If anyone could do it, I would love it. And I know people have tried, like, July and Nelson tried for a long time to get a bunch simpler proof. Get a much simpler proof. Okay, so what's an application of this? I mean, yeah, the first thing is you can do an approximate low-rank approximation, or what people might call randomized SVD. If I view the goal of doing a randomized SVD as finding a near-optimal low-rank approximation, which is what most people do, like that's usually how we measure the error of randomized SVDs, you can do this using a projection constitution sketch. So you set Q tilde to be the minimum over all ranked K matrices Q. It should be your orthonormal. is q, it should be orthonormal. And then look at a tilde minus qq transpose a tilde, so you want to minimize that error. If you do that, you will get a q tilde, which is near optimal in terms of Frobenius, norm, low-rank approximation error with respect to your original matrix. So you sketch, you find the best subspace, you can do that using an SVD because now you have a small matrix, and you claim this is near optimal up to a one plus epsilon factor for the original matrix. Here, even if you use the slow matrix, Here, even if you used a slow matrix like a dense random Gaussian, you've gotten this near-optimal rate approximation of n dk over epsilon squared time, which is quite good. I mean, and you're just multiplying your matrix by a block of like k over epsilon squared factors. You can improve this, so you can get this down to a one over epsilon. Still a poor dependence on epsilon, but it's like more reasonable, I think, because people are more willing to accept coarse low-rank approximations. Okay. Okay, that's one thing you can do. Here's another thing you can do with projection cost-preservium sketches. This was introduced in a paper by Jernaeus, Fries, Canon, and Palinfinet. It's like one of the first papers I read in grad school, so I love this observation. The claim I'm going to make first off is that you can also solve constrained lowering approximation problems. So, all this stuff about solving the problem approximately, if I added a constraint set, If I added a constraint set when I'm minimizing, so instead of Q just being ranked K, it lives in some constrained space S, you can also solve that constraint problem approximately using the sketch matrix instead of the original matrix. And what Petros and others realize is that some really basic problems, including k-means clustering, can be framed as constrained low-rank projection problems. So I'll show you that in one second. This constraint set doesn't have to be convex. It's an arbitrary constraint set. Arbitrary constraint set. You can, but yeah, like with the, it would be the same proof here. Like, when I was doing the JL proof, not JL proof. Maybe I won't go back, but I didn't use anything. I just like said, I just use the bounds. I didn't lose anything about the constraints. You just need to flood all the data and demonstrate. You just need a for all guarantee. Exactly. So here's a way of seeing this. I mean, this is like a transformation that I'm not going to digest a ton. The K-means clustering problem, we have a data matrix A. It says, I want to find I want to find k centers, v1 through Î¼k. And then I'm basically going to swap out every row in my matrix with its center. So I'm going to approximate it by one of k centers. And I want to look at the Frobenius norm difference between A, and now this I'm going to call C A, which is my clustered matrix A. It's not usually how people write down k-means, but this is the linear algebraic form. So you take a matrix, swap out the rows with centers, and then compute the Ferminius form. That gives you the k-means error, which is like a very, very common objective. A very, very common objective for supervised learning for clustering. Okay? You can check that this matrix, an optimal K-man solution always has the mu1 equal to the mean of the rows that were assigned to mu1. So in this case, a n minus 1 and a1 were assigned to mu1. The optimal solution will always have that mu1 equal to the mean of the places it was assigned. And you can basically, oh no, I had a picture and I lost. Oh no, I had a picture and I lost it. Okay, I have this really nice picture, but now you can't see it. You can rewrite this as a projection. So you can show that CA is a projection of A onto a low-ranked subspace, and that projection has a very specific form. It basically has like a constraint sparse structure. Okay, so punchline is. Oh. Sorry, the screen's not working properly, guys. Sorry. What should I do? Sorry. What should I do? I just asked Linda if she would come up and help us fix it. So I think. Just turn them on? Yeah. Sorry, people on the screen. So if there's some way that we can share the slides, maybe we can fix them. I should have shared them in the past. Yeah, any questions? Yeah, we have any questions up until this point. I have a question. Yeah. So for some applications like sub-suisce iteration, you actually would like to have the property where on the left-hand side and epsilon goes. On the left-hand side, when epsilon goes to one, your failure probability goes to zero. I think this embedding property doesn't capture fusion. Is there like a variant which captures this? Yeah, you mean you want to get like, you want to have, as say you went down to exactly D, your epsilon could just grow? Yeah, so I'm not really interested in having a good approximation. I want to have some overlap. Yeah. I would like to have if epsilon goes to one on the left-hand side. Yeah. But my failure probability goes to. Yeah. That my failure probability goes to zero. I think this the normal frameworks of substance embedding do not capture this. It's very variant that captures this. Um I mean yeah not that I'm aware of. I'm trying to think of how this yeah go ahead. So I think I mean that's where like random matrix theory type analysis comes in because then you can kind of get these kind of regimes where you want really small, kind of extremely small sketch size. Kind of extremely small sketch sizes. I guess we gotta be careful when we mean epsilon goes to one, because in my how, at least how I wrote things, that's not a trivial approximation. It still means you're like lower bounded by two times the if you take a Gaussian sketch, then so you roughly want to capture the low, but not you don't need to capture it too well. Yeah. But you'd like to have a very low failure for. Like up to a constant have a low failure. Yeah, whatever. But I suspect even if you But I suspect, even if you use concentration requirements, right? Even to get up to the concept. My use concept is by saying that one of the training problems becomes 10 to the minus 17 essentially for 3. It plus the train problem. Yeah, so, okay, where was I? The punchline here was you can improve this to you can you can get a k-means clustering algorithm basically using dimensionality direction. So we started with linear algebra and went to a problem that maybe doesn't immediately look To a problem that maybe doesn't immediately look like linear algebra, but because it's constrained approximation, you can solve it with a matrix sketching guarantee up to one plus epsilon accuracy. This has actually been improved. So this K-dependence can be improved to allow K. That's one result from Makarache, Makarachev, and Rajasthan for 2019 using pretty different techniques, specifically for k-means. Okay, so that's low-rank approximation, constrained low-rank approximation. A lot of people here care about unconstrained links. A lot of people here care about unconstrained learning approximation. I think it's important to talk about what is the difference between these sketching methods and, say, iterative methods. Like, we heard people talk about block prelov subspace methods. I think sketching tends to provide lower accuracy in that it requires multiplying your matrix by a sketch of size k over epsilon, and that one over epsilon is very real. Like, you really don't get around it. It gives you a one plus epsilon approximation for B. You have a one plus epsilon approximation of Frobenius norm, which is good, but people could ask for approximations to other norms like spectral norm. In contrast, like a Block-Krilov method, or as Raphael showed us, even a single-vector Krilov method with a random start, you can get away with less matrix vector multiplies, so like K over root epsilon, and you actually get the exact same guarantee and more. So you get like a Frobenius norm guarantee and you get a spectrum norm guarantee. Although, to be fair, that's an iterative method. So you're like, your multiplications have to come in sequence. One nice. Have to come in sequence. One nice thing about sketching is you can do all your multiplications at once. It's just like one matrix vector, one matrix matrix multiply. So that certainly could be a gain on some systems. So in terms of like trading these things off, I think it really depends on your application. There are some ones where sketching maybe makes sense and other ones where like integrative method will give you higher accuracy if you want to do low-rank approximation and you want to do something randomized. Okay. One place that we found that sketching methods. That we found that sketching methods tend to be really useful is again when you're in the regime when the epsilon accuracy isn't critical. So I lost a bit of time, so I might not go through this in as much detail as I was going to, but I'll go through it quickly. So here's an example. We've recently used sketching for lowering approximation in an algorithm that does variance reduction for trace estimation. So the idea here, there's a problem that Joel told us about. I have some PST matrix A. It doesn't actually be PSD, but I'm going to assume that for now because it simplifies the idea. But I'm going to assume that for now because it simplifies the guarantees. I have PSD matrix A, and I can only multiply it by vectors. And I want to approximate the trace of that matrix. How do I do that? There's a classic technique to do that, which usually is called Hutchinson's estimator, but actually it started in a paper by Girard in 1987, where you basically pick random vectors, and Hutchinson used random plus minus one vectors. You compute the quadratic form of those vectors over your matrix A. That only takes one matrix vector to multiply. And then you average to get an approximation to your trade. Average to get an approximation to your trace. And Hutchinson analyzed the variance of this. You can get high probability bounds. So, what you can show, I think this goes back to Avram Toledo, but Alicia and Daniel have like a much tighter and cleaner analysis using Hanson-Wright. What you can show is that if you repeat this process, so if you use M random vectors here, where M is log 1 over delta over epsilon squared, then with probability 1 minus delta, you'll get an estimate for your trace T tilde. You'll get an estimate for your trace T tilde, such that T tilde minus trace of A is less than epsilon times the Frobenius norm of your matrix A. And that's quite a good estimate because the Frobenius norm of your matrix for a PSD matrix is always less than the trace of the matrix. So this is less than epsilon times trace of A. So you got a relative error approximation to the trace using a number of matrix vector products that just depended on epsilon. Okay, that's a great result. Here's something that people observed, like the Here's something that people observed. Like the first paper I saw this in was a paper by Sababa, Alexandrion, and Ipsen from 2007. They observed that this inequality for menial storm being close to the trace of A is really only tight when like all of the mass of your matrix on one eigenvalue. It's like an L2, L1 inequality. So if all of your singular value, your eigenvalues, in this case PSD, are very flat, this is a very loose upper bound. So you actually get much, much better error when your matrix spectrum looks flat, something like that. Your matrix spectrum looks flat, something like this. So they said, let's just make our matrix spectrum look flat by projecting off the top eigenvectors of a matrix, computing those exactly when I need to figure out what the trace is, and then only focusing on estimating this sum of eigenvectors without eigenvalues, which is the rest of the trace. Okay, so we sort of like took this idea and ran with it. We introduced this thing we call the Hutch plus Plus algorithm, which is really the same as what other H algorithm was really the same as what other people were doing. We want to compute trace of A. To do so, we compute an approximate top singular vector subspace, q, and we project a onto it. So we break down trace of a as trace of a times i minus qq transpose and trace of a times qq transpose. This part can be computed exactly with a small number of matrix vector multiplies because it's just equal to like say trace of q transpose aq. Trace is Q transpose AQ. You just need to multiply A by your Q matrix, which is a rank K matrix. It's like a skinny matrix. This part we estimate using Hutchinson's estimator. Okay, and the main observation is that the variance in estimating that tail part when you do the trace, it only depends on the Frobenius norm error of your low rank approximation. It depends on the Frobenius norm of A times I minus T V transpose, which is the same as the Frobenius norm of A projected on the T transpose. Form of A projected on the transformers. Okay. We want to get that small, but it really is not critical to get it too small. Ideally, if we're going to use Q to be a rank K matrix, we want that Frobenius norm to be as small as the best rank K approximation to A. They are the best rank K approximation. But if we get it off by a factor of 2 or even a factor of 4, it's going to get cleaned up by Hutchinson's iteration. It's just going to mean when you do the Hutchinson's random vectors, you have a slightly higher variance algorithm, so you have to run it for more samples to get there. Run it for more samples to get down to good accuracy. So, the punchline here, and I know I'm going very fast over the details, is that you can get an algorithm, the Such algorithm, which given the ability just to compute a constant factor approximate K-rank approximation. But of course, you have a like-oh, yeah, I mean, it's like it's start related. I see, okay. Oh, good, okay. Yeah, I maybe want to, okay, good, perfect. So, the claim is if I can give you a So the claim is: if I can compute a constant factor K rank approximation using order K matrix vector multiplies, sketching lets us do that very easily. You could use our projection cost between sketches or other things. Then this algorithm will be able to return an estimate for the trace, which is relative error accurate, and it only uses one over epsilon matrix vector multiplies instead of one over epsilon squared. I didn't do the full analysis though. You have to do like a careful trade-off. Exactly what should I set the rank K to be? Exactly how many. Exactly how many multiplications that I use for Hutchinson's iteration, but that's the payoff you get. So you get like a very nice actually improvement in epsilon, like the classical method of one of epsilon squared, you get a quadratic improvement. And all we needed was a constant factor approximation. So like I think this highlights an example where a lot of error is okay because it's being used inside some inner loop that doesn't care about a lot of error. Okay, so maybe we're seeing a common theme there. Seeing like a common theme there, so poor accuracy tends to be an issue with matrix sketching methods. So, not all applications, but many of the most compelling applications I've seen use sketching methods in conjunction with some other sort of refinement technique. We saw two examples of that where you use a coarse subspace embedding as a preconditioner. Your refinement there is a precondition iterative solver. Or in the second one, we saw you use your sketch-based low-lang approximation as a coarse. Low-rank approximation as a coarse variance reduction within some like Monte Carlo refinement algorithm. Okay, there's a third example of this. So, another place where sketching has found value is actually in analyzing iterative SVD methods. So, here's a basic question. If I start like a block-Krilov method, say I want to compute a rank k approximation, and just for simplicity, I'm going to use block size K. So, block is size K. A question people we're interested in answering is to find a Q using that method, which gives us a near optimal low-rank approximation. So I want the error when I project on the Q in some norm, that could be for Benius or spectral or whatever, to be less than 1 plus epsilon times the error of the best rank K approximation, which I'm denoting by AK. How many iterations do I need to run that method for? It's an interesting question because as much as these iterative methods have been studied, it's not quite equivalent to the question. It's not quite equivalent to the question of asking how many iterations does it take Q to converge to the top subspace of the matrix. So I think like Petros and Ilse have a nice paper that sort of like crystallizes this idea. The point is that if you have a lot of singular values that are similar, you don't really care about which ones you converge to. So it's a distinct question from asking about convergence to this hops and top subspace. Okay, so how do you analyze this problem? Actually, one thing that's really interesting to me is the Really interesting to me is the current best analyses for understanding these methods they go through sketching, analysis of sketching methods. So I'll try to explain that at a very high level. A lot of the analyses to answer this question, they view your Krilov subspace method as basically applying a sketching matrix to a polynomial of your matrix A. So when you do the Krilov subspace method, you take the matrix and you repeatedly multiply. You start with a random matrix and then you repeatedly multiply by your matrix A. Repeatedly multiply by your matrix A. So at the end of the day, what you're going to output is a polynomial of your matrix times a random matrix. That looks like a sketching method. So here's an analysis approach. I'm going to call, oh, I didn't want that to be like that. I'm going to call B the polynomial of A. When I want to understand block size K, that means I'm basically choosing my sketching matrix as small as I could possibly hope to preserve rank K approximation. So I'm in rank K information. So I'm going to project down to something. K information. So I'm going to project down to something that literally only has K columns. And you can get a sketch and guarantee for that. So if everything I talked about had a big O on the K, you project down to something like order K columns and preserve information. This is going to say, what if I project down to exactly K columns? You can prove very weak sketching guarantees in that case. So you can say that from a sketch of the form PG, where B is any matrix and G is a random Gaussian matrix, just K columns. Just k columns. I can compute a low-rank projection matrix QQ transpose such that B minus Q2 transpose B in Frobenius form is no more than C is a constant NK times B minus QQ transpose B in Frobenius form. There's something missing here because wait, why? Do you mean the best? Oh, this is sorry. Sorry. This is the best. Sorry, this is the best. This is the best K-rank approximation. What is VK? VK is the. VK is going to be the optimal K-rank approximation. Yeah. Okay, so on the right-hand side, I want to have the error of the optimal rank K approximation of Fermi's norm. On the left-hand side, I have what we're going to return. This is a horrible guarantee. Like, this is n is the size of the matrix, and then there's a K in there. So it's like a polynomially loose. It's very, very weak, way weaker than any of the constant factor stuff I talked about. Factor stuff I talked about. But the thing is, when you plug it into these analyses where B is going to be a polymer one of A, this very, very weak guarantee for sketching ends up inside a model. So I'm not going to do the details like we sort of software for preconditioning, but think about it as the same way. When you preconditioned, your condition number ended up inside a log. That's going to be what happens here. So this will actually go inside a log. So this very, very weak sketching guarantee can basically yield you near-optimal boundaries. You near optimal balance for analyzing these iterative SPD algorithms for approximation. Okay, that's pretty much all I want to tell you about. So I think we were supposed to talk briefly about open questions. I'll list a couple here. So I think this is something I'm very interested in. Like, what is the right way to use sketching? And oftentimes that that comes down to what is the right way to combine it with other techniques that can do some sort of iterative refinement. Or maybe there are just problems where you can use it directly. Where you can use it directly. I'm interested in better abstractions for sketching guarantees. So I told you about two subspace embeddings and projection cost-preserving sketches. The value of these things is they are very general. They decouple analysis. So you come up with a sketching method, you prove that it can give you a subspace embedding. Now people can use that wherever they used subspace embeddings before. So like this is super, super useful. We don't have a ton of these in the literature. So I think that would be nice. More examples of concrete sketches. More examples of concrete sketching guarantees that can plug in black box to a lot of analyses. Even the ones I told you about often, they don't lead to optimal balance. I'm really interested in sketching for active learning. So we talked about like L1 regression. One thing that people have been studying a lot is applying this to much richer function classes or loss functions. Applying sketches to infinitely tall matrices. This also comes up in the active learning setting. Maybe I won't read these because you can read them on the slide. Why don't you guys just tell, if you have questions or want to talk, If you have questions or want to talk about these, you can let me know. So, in your results, sorry. Yeah, go ahead. In your results, you just presented this QQ transpose times A will not be symmetric, but A, say, is symmetric. So, you will lose that symmetric transpose. That's right. So, people have studied this, so I don't know the right references, but I know Joel has some stuff on this. You can do, I mean, we sometimes do things called Neistrom approximation, where you're basically doing, you do do a symmetric projection and prove similar guarantee. Projection improve similar guarantees. So, if you have a symmetric matrix, you probably want to project symmetrically on both sides and you can get guarantees from that. Good question. But it's interesting, I think, like most of these guarantees are in L2 or Frobenius norms. So if you look at the square, you basically end up finally with a symmetric matrix where you have a projection in the middle. Um well, wait, say that one more time? So so so this would be nice of guaranteed, right? Like you square you square it out, you Like you square it out, you could sort of translate it to a guarantee on this inventory. It actually doesn't quite work out that way. It's not quite that simple, but I agree intuitively would think that. The thing is, if you add a QQ transpose on the right-hand side here, it actually makes that this further away from A when you project on the right-hand side. And you would think maybe I can bound it by a factor of two, but it's actually sort of delicate. We can work through it, yeah. But yeah, so it does take some extra work to get through. But yeah, so it does take some extra work to get the spectrum here. That's probably very far, right? I mean, that you can do, but you don't get one plus epsilon. You get two plus two epsilon. Oh, sorry. Yeah, yeah, yeah. But you don't get one plus two epsilon. Yeah, you get two plus epsilon. Yeah, that's right. We don't know how to get it around. Yeah, we don't know how to get around that, yeah. Yeah, correctly. Yeah, I think you can actually get it to like 15 epsilon if you can do it on both sides. This should be wonderful if you could prove this to be very because when you project on both sides appearing. We can project them both sides here and our continuation of the program and get an extra product of five or six. Okay. Yeah, we tried to, I mean, we looked at this for a while because it would simplify a lot of proofs. So I'd like to see it. Yeah, that's cool. Yeah. I was just wondering in the when you do the products that touches the bugs, doesn't that may already give you some information about the product random? Yeah, okay, this is a really interesting question. So, the products Hutchinson does are just multiplying it by random vectors, which is what we use to get the top singular vectors. So, when we first coded this algorithm, we were like, well, it's probably that like there's, you should just reuse the vectors. Use the same vectors for Hutchinson's and the ones used to approximate top singular vectors. You should, the top singular subspace is like sort of going to be deterministic quantity. So, you should be fine. There's no issue with reusing a randomness. It like totally breaks. So, weirdly enough. Breaks so weirdly enough, when you reuse the randomness, stuff worked out awfully. I can't say I totally understand it, but like it's a it's one of those cases when like if you reuse the randomness, because your top subspace depends on those random vectors, it will screw up Hutchinson's estimate. But it's a great idea. It's a algorithm that does all of the stuff. Yeah, that's right. So like Ethan and Rob and Joel have this like this new X-Trace algorithm that doesn't have, it doesn't have the same form as ours, and it does sort of use all the vectors for both. Sort of use all the vectors for both Lowering approximation and Hutchinson sense. But I'll let them describe that to you because you have to be much more thoughtful to do it. Yeah. So at one point you said that it was in the context of parameter-dependent PDEs, the kind of the community like scientists played by Cohen and Mayor Lati. Yeah. And you phrase it as rediscovering it. So it's actually a question: they built up quite some machinery to They build up quite some machinery to control the L2 norm, the continuous L2 norm on the domain. And they do this via orthogonal polynomials and so on. So it's it's quite some machinery which doesn't go through to our domain. So my question is, so you have something on the discrete setting. Yeah. But would it actually go through in the continuous limit, what you have? The discrete stuff does go through in the continuous limit. I mean, if you make sure everything works out, like the function B you're trying to fit is like L2, is like an L2 function. Like an L2 function. They actually basically used, there's people who have taken all this like matrix turnoff stuff and extended it to the continuous limit. And what they're using is just those direct continuous extensions. What they sort of rediscovered was it's like pretty, it's really not obvious, this goes to Ethan's question, that you should be able to get away with selecting your samples without looking at B. Because what if there was like a big spike in trip B? Surely I need to see it. So it's like very not obvious. So like that analysis is involved and they had to be some new improvement. That's involved, and they had to do something to produce that. But they weren't aware of Sarlos' paper, which did this in 1967. Also, the stuff they do with redogmac polynomials and stuff, a lot of that work is to bound the leverage boards themselves because you can't compute them when you're in an NDMS setting. Yeah, maybe I'll mention that quickly. So these leverage words I mentioned, when you're fitting polynomials, they have like a direct analog. They're the inverse Christoffel function, which is something that people study. It's literally the exact same definition. So one thing is like if it's in the infinite set. So, one thing is like if it's in the infinite setting, I can't go and compute leverage scores because I have an infinite number to compute. So, they very much care about getting close-form bounds of what the leverage scores look like. And they've studied that for a long time. Yeah, we don't have, we don't, from our community, have tools to do that, but you can dip into those tools. There are also some earlier papers by Rachel Ward and Holga Raussie. Yeah, yeah, there's effectively doing the same thing for approximation and interpolation problems where they sample points with respect to. Sample points with respect to a measure attached to orthogonal polynomial systems and get guarantees on sparse structures of those polynomial systems. Yeah, this was, I think, probably the first paper to do this. Like Rahu Ward. I tend not to, these ones, it's easier to obviously see why they're solving the same problem, but like all the machinery is done in Rahu Ward in 2012 as well. Yeah. With your trace estimation procedure, if your account for all the function calls was If you account for all the function calls, what's the resulting complexity in total? It depends a little bit. Like we measure complexity, at least for this problem, we were thinking about in terms of matrix vector multiplies, because it sort of dominates the cost. Once you do the matrix vector multiplies, all your other cost is on matrices that have height D, where D is your original dimension, and width 1 over epsilon. So it's like D times 1 over epsilon squared. like d times like one over epsilon squared is the lower the lower term. So it's one over epsilon matrix vector multiplies, which may be for a dense matrix. The thing is, people do these in implicit settings. We like doing the matrix vector multiply. It could be solving a system. So it's hard for me to say what the cost is, but it's basically one over epsilon times time of a matrix vector with A plus D over epsilon squared would be like the overall runtime cost. So we usually think of the first term as dominating. Of the first term is dominating. Yeah. I also feel that the analysis you displayed sort of undersells the value of this idea because the errors actually depend on the quality of the Besselo rank approximation. And so for matrices that have a decaying spectrum, these methods completely destroy the Monte Carlo estimate. Yeah. I mean, there's, you know, it's interesting that you have the worst case bound, which you haven't established, but inside of your proof, of course, is a stronger claim. Right, exactly. Yeah. Which is. Right, exactly, yeah. Which is really remarkable. I mean, like, you're getting six orders of magnitude better than Hutchinson, like, right away. Yeah, so, yeah, I think, like, yeah, there's like some trade-off between precision and conciseness. But yeah, this is like a worst case bound. But Joel's right. Like, it tends to be that Hutchinson's really pays the one over epsilon squared times the Ferminius form, no getting around it. Whereas these projection limits can go way faster than the products. Thanks, Joel. Anything else? Just quickly I mean I love your uh H â  so just making sure so in if you're only restricted to quadratic forms, like you're you are using stronger information now. Just it's there's actually a lower bound. So this is where we started out in this work. There's like a lower bound from 2014 by Pang Zong and I can't remember the other authors that proved if you only can That proved if you only can do random, you can only do quadratic form queries with a matrix. So you give me a black box, and for any uv, I can do u transpose av, actually more a little more general than quadratic forms. You need to do one over epsilon squared queries. So like I had asked Ralph about this, can we extend this lower bound to major vector queries? And then that's how we actually got to this algorithm. It's like, oh, you can, you can do it. Anything else? I can't resist the follow-up on what I just said before, which is that I think a point which is not always articulated that clearly, but is extremely important in practice, is that methods that reflect the quality of the Bessel rank approximation of the matrix, so that they reflect the spectral decay of the matrix, perform far better than methods that don't. Sure. So, what's critical about this Hutch Plus algorithm is that it has that benefit. When the matrix has a decaying spectrum, it takes care of that. It's the same for the randomized SVD algorithm. If the matrix has a decaying spectrum, it can exploit that to get better results. And algorithms that can't do that tend to be far worse in practice. Yeah. I also think this is another, I mean, I don't state it that. I'm stating it as, oh, the Stated that. I'm stating it as, oh, the epsilon dependence is better. But this is like, in practice, probably more the real reason for the huge gap between sketching methods and the iterative methods, because the iterative methods really take advantage of this decaying spectrum. Okay, any more questions? No, but uh thanks for questions. Hold on, time to go like