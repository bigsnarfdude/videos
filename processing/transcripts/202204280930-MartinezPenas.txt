Berto is speaking about network coding, error correction and security. So, thank you very much, Berto. Whenever you want to start. Okay, thank you very much for the introduction and for the invitation. So, I'm going to give a short tutorial slash survey on network coding, error correction and security. So, there is going to be some overlap with Alberto's talk on Dragon. With Alberto's talk on drug metric codes from Tuesday. So I'm going to start talking about network information flow. I'm probably going to go more into the details than Alberto, and then I will go to error correction. Here I will talk a bit about the rank metric, but not much. And then I will move on to security. And finally, a more recent area in multi-shot network coding. And I will give some references and some open. And some open problems. So, I'm going to start with the basics of network coding. So, I will go very fast through the idea of flows in networks. So, a flow network is considered to be a tuple where we have a directed graph, G. We have a source node. So, here the vertices, I call them nodes, and the edges, I call them links. So, we have a source node. So we have a source node, which means it only has outgoing links, then a sink node, which only has incoming links. And then we have a capacity function. So the capacity function is basically the idea is to give a capacity to every link of the network. So here we have a network, so the directed graph. Directed graph, the source and the sink, and the sink. Usually they are called S and T, and then the capacity, which is a positive number or non-negative number, and for us it's going to be just integers, non-negative integers. So a flow is basically just another function, also on the edges, also with non-negative numbers. And we say that it's feasible if it satisfies the capacity limit. Capacity limit. So, in each link, the flow is at most the capacity. And it satisfies flow conservation, which means that in each node, the total incoming flow is the same as the total outgoing flow. So, here you have an example, and usually we write the flow first as the first number, and then comma, and then the capacity. And you can check in this example that this satisfies, so this is a feasible flow, so it's upper bounded by the capacity. By the capacity, and in every node, there is a conservation of the flow. So, the point is to maximize the total flow. So, the total flow is defined as the sum of the flows of all the outgoing links of the source. So, all of the flow that is outgoing from the source. And this, we can prove that this is equal to the incoming flow at the sink. Yes. Yes, is there a question? Yeah, so one question is the preservation, does it hold for all nodes or all nodes except the source and the sync? Maybe? It's all nodes, yeah, except the source and the sink. Yeah, because in the sync, yeah, the way the way I define it is except the source and the sync, yeah. Yeah, yeah, yeah, thanks. Yeah, that's a good point. Yeah, so yeah. Yeah, so yeah. And well, yeah, the total flow would be all the flow that is coming from the source and it's going into the sink. And the quality of the two can be proven. I'm not going to do it, but it's not very difficult to prove. And now the point is we want to maximize this total flow. So we want to find a flow that is maximum possible and it's also feasible. And for that, we need the max flow min-cut theorem. And for that, we need the concept of a... And for that, we need the concept of a cut. And a cut is basically a partition of the vertex z such that the first part has the source and the second part has the sink. And then we define the capacity of a cut as the sum of the capacities of all of the links joining nodes of nodes in these two sets of the partition. So nodes coming from So, nodes coming from, so the links going from S to T. So, the sum of the capacity of all of these links is the capacity of the cut. So, the max flow min cut theorem just says that the maximum total flow of a physical flow is equal to the minimum capacity of a cut. And yeah, so basically this is the theorem. And so, it gives the upper bound and it says that this upper bound can be reached by some feasible flow. Feasible flow. So this theorem was obtained independently in these two works from 1956. The fourth Falkerson paper is well known because they give also an algorithm to find the maximum flow. But it was also found, this theorem was also found by Elias, Feinstein and Shannon in the information theory community. So now the problem is, well, they give this. problem is well they give this theorem for flows for general flows another question is what happens with information flow so here the flow is data and the problem is in the in the traditional scenario we have the max flow min cap theorem but if we have the multicast scenario which is very typical in communications then this theorem is no longer true and the multicast scenario basically what it means is we have more than one What it means is we have more than one sink. So we have in this example T1 and T2. And we have, and each of the sinks wants all of the information that the source is broadcasting. So let's say in this case, the source S wants to send A and B. Let's say these are two bits or two messages of the same size to both T1 and T2. So in this particular case, we could send A and B like this. Send A and B like this. And then, when we arrive at the center of the network, in this node in the center, we have a problem which is we have to choose whether to send A or B. And if we send A, then at the end we have that T2 obtains A and B, but T1 only obtains A. And if instead of sending A in the link in the center of the network, we send B, then T1 obtains A and B, but T2... A and B, but T2 only obtains B. So in this example, the minimum cut is also true for both sinks, but the flow is either true for one and one for the other or vice versa. So we cannot send the two messages to both sinks at the same time. And that's because we are thinking of information in this case as a commodity, as a commodity flow. So this is the problem in classical flow networks. This is the problem in classical flow networks: is that they treat flows as commodity flows. So, it means that the thing to be sent through the network is treated like a commodity. So, like we are sending packages of meat or whatever. And the problem is that information does not behave as a commodity. We can manipulate it, we can process information, and that's what we are going to do in network coding to be able to achieve. Able to achieve the max flow mink at bound in the multicast scenario. So, this is what Jung et al. did. And here in this reference, Raymond Jung gives a historical perspective. He gives a bit of the ideas of how they came up with network coding. So, he said that in part, the idea also came from distributed storage. So, he gives this example of the RAID 4.5 system. So, here we have two. So here we have two bits or two files to store X and Y and we have three disks to store them. So we store them in X, Y. So we store in the first one we store X, in the second one we store Y. And in the third one we could store X or Y. But in order to make this resilient to one erasure, we store X plus Y. So what happens in this storage system? If the third disk fails, If the third disk fails, then we can still recover x and y. If the second one fails, we can also recover x and y because we have x and then x plus y, and so we can recover y. And if the first disk fails, then we can also recover x and y because we have y and x plus y. And the thing is, if we see this system as a network, then this is basically a network coding solution to the multicast problem where we have this network and we have one source. This network, and we have one source that wants to send X and Y to these three sinks, the three nodes in green. And this would be a network coding solution. And you can see here that if we don't process information, so instead of sending X, Y, and then X plus Y, we just send X for Y without making any additions, then there is no solution to this multicast problem. However, if we process information, so if we send X plus Y to the in the x plus y to the in the last to the to the last blue node you can see that there is a solution to the to the multicast problem meaning that every sink can obtain both x and y which which is the minimum cut in this case for every sink the minimum cut is two and here the nodes have capacity one so I didn't give the capacity but here from now on I'm going to assume the capacity of every link is going to be one just for simplicity. To be what, just for simplicity. So basically, Jean said that they got the idea in part from examples like this. So from this example, you can already see what network coding means. It basically means that in every node, we are going to send combinations of the things that we receive in that node. So yeah, that's basically the idea of network coding. The idea of network coding consists in sending at each node a combination of the received information, which is usually called packets. And one packet is usually considered a vector over a finite field. So in our case, let's say over FQ to the M and it was introduced by Al-Sfred Kai Di and Young in the year 2000. And then the notion of linear network coding, which basically means the combinations are linear combinations, cover the finite field. Linear populations covered a finite field. They were introduced by the same group in 2003 and independently also by Keter and Bedar, who gave a proof also that it works. So this is actually what I'm going to do now. I'm going to present very fast the proof of why network coding works. Well, first of all, the two examples that I gave before, the first one is called the Butterfly Network and it's very well known. And the one from distributed... And the one from distributed storage, this is just the RAID 4.5 system. And well, I took them from the original paper from the year 2000. And they give other examples too. So why does it work? So first of all, I'm going to assume that the capacity of every link is one. And we can assume this because we assume that the capacity is going to be a non-negative integer. And in that case, and in that case basically we can assume every link has capacity one because we can multiply the link by its capacity so if there's a link of capacity three we are going to consider three links of capacity one so i'm going i'm going to assume for simplicity that the capacity is only always one and in our case that means one package one packet in in fq to the m so these are the messages that we can send in every link Can send in everything. I will also assume no cycles or delays. And it's not a big assumption. I think it works in the same way or in a very similar way, but it's just for simplicity. And then I'm going to, so n is going to be the minimum cut from the source to every sink. And I'm also going to assume that this is also the number of outgoing links from the source and the number of incoming links from the And the number of incoming links from every sink. And this is also for simplicity in the notation. And the idea is that in every sync, in the sync TI, for example, we need to invert the transfer matrix. And this transfer matrix, what it is, is basically every column or every row, depends how you look at it, but every column is the. But every column is the linear coefficients of the combinations of the original n packets that the source is sending. So if you do the calculation after going through the whole network using linear network coding, at the end the sync TI will receive the original packets multiplied by this matrix and in order to recover the packets we have to invert this matrix. So essentially what we want is to see Essentially, what we want is to see that this matrix can be invertible. And here, these coefficients or the entries of this matrix are polynomials. And so, P super i sub j k will be polynomials over fq and a the coefficients where so the points where we evaluate these polynomials will be the coefficients of the linear combinations in the network. So, here I don't really give the details of So here I don't really give the details of how to calculate because it becomes very cumbersome. But you could see very easily that this matrix is basically formed by polynomials of the coefficients of the linear combinations that we form through the network. And we could actually count the number of coefficients that we have in the whole network, which I think it should be something like this if I didn't make a mistake. But it's not very important. But it's not very important. The number of variables in these polynomials is not very important. So we want this matrix Ai to be invertible for every sink, for every i. So we are going to consider the determinant and this determinant is going to be also a polynomial because the matrix, the entries are polynomials. And this polynomial Pi is not zero. And this is because there is a solution of the mean. Solution of the min card max flow theorem. So, for each sink separately, there is a solution to the linear network coding problem, actually, without using linear network coding because there is a solution to the max flow mean-cut theorem. So, that means that there must be a choice of coefficients for which the sink TI receives all the information. So, that means that for those coefficients, AI is invertible. Ai is invertible, so Pi of A is not zero. So that means that the polynomial Pi is not zero. And now what we do is we multiply all of them for all the sinks. So this is a non-zero polynomial because it's a product of non-zero polynomials over a finite field. And now we have this very simple demo. And here we have the number of variables, which is not very important, but basically we have that the number of zeros of the test of the That the number of zeros of this polynomial p is upper bounded by q to the number of variables minus one times the degree. And now, what happens is, well, the degree doesn't depend on q. So if q is bigger than the degree, then there is a solution to the linear network coding problem. So that means there is a choice of linear coefficients a, such that p of a is different than zero. And this is the product of the determinants. So it means that all of the matrices are. All of the matrices are invertible. So, if all of the matrices are invertible, it means that all of the sinks can recover the information. And actually, we can say more because we can rewrite this varsify lemma in a probabilistic way. So we can also say that if we choose the linear coefficients in the network uniformly at random, independently and uniformly at random, then we have a probability of success which goes to one as q goes to one. Goes to one as q goes to infinity because the degree of this polynomial is it doesn't depend on q. So here we have two things: one, if the field size is large enough, then there is a solution. And actually, this solution, we can find it at random if the large field with high probability as the field, the size of the field goes to infinity. So, basically, the proof that I gave is basically. That I gave is basically the one that was given in the paper by Peter and Medar in 2003. And the solution using random coefficients was given soon after by Joeta. And well, they give some improvements on the size of the field that we need and so on. So actually, I'm going to go a bit through the literature, but there are a lot of people. But there are a lot of papers here, so this is a very brief, very concise collection of references. So, well, regarding the size of the finite field, I think Ketrandmedar proved this bounds. So, if Q is at least the product of the mean cut times the number of sinks, then there is a solution to the linear network coding problem. And I think this bound was improved later to queue bigger. Improved later to Q bigger than the number of sinks in this paper from 2005. And there has been other approaches like algorithmic approaches. So there is this algorithm by Jackie et al. to find a solution to the linear network coding problem. So finding, so it's an algorithm to find the linear coefficients that would make every transfer matrix invertible. So here instead of So here instead of doing it in a probabilistic way, they would give an algorithm to find the linear network code. And then there are other works which connect also the solvability, the linear solvability of a network is connected to the representability of a Matroid. And there are some works in this direction also. And then there are some works on the size of the field that is necessary. The size of the field that is necessary for finding a linear network coding solution. So, in this paper, they show that there can be a network where a solution exists for FQ0, but not for every Q bigger than Q0. And this is because of the characteristics, because there can be a solution in one characteristic, but not in another characteristic. But they give several conjectures, so this is one of them. They say, them and they say i think i think um at this point still not solved this conjecture but i'm not i haven't really followed this path of the literature so they conjecture that um if q0 minus 1 is prime if there is a solution over fq0 and q0 minus 1 is prime then there is a solution for every q bigger than q0 and i think this includes the cases q0 equal to 2 and 3. So if there's a solution of red two, then there should be a solution. Ref2, then there should be a solution for every finite field, and then there are more pathologies. So, in so this was all for the multicast scenario, and in the non-multicast scenario, so here either we have several sources or the sinks have different demands. So, in this case, there can be networks where there is a non-linear solution, and at the same time, there are no linear solutions. So, we could only solve. So, we could only solve the network coding problem with a non-linear network code. So, this was examples like this were given in this paper. And I think they show that the network that they construct, it cannot be solved linearly even over rings. So, it's not just over finite fields, but also over rings. And well, there are also other directions. So, there is a concept of There is a concept of vector network coding, where you can find solutions over smaller fields than if you use scalar network coding, which is what I introduced. So this was shown in these papers. And then there are other variations of network coding. There is physical layer network coding, which was introduced by Tsang et al. And there is an algebraic approach given by Feng Silva and Shishan. Silva and G shar. And then there are also other variations like wireless network coding, where instead of graphs, I think you can consider hypergraphs too. And well, the number of references in general network coding is vast. I mean, it's a massive area. So regarding general network coding, I will just give these references, but there are literally thousands. So after this introduction, I will. So after this introduction, I will talk about error correction and then I will continue with security. So here we have the problem, as Alberto mentioned in his talk on Tuesday, in network coding we have the problem of error amplification if there is an error. So let's say we have the butterfly network, like the one I introduced before. And here, if there is no errors, then the sink and the two sync. The sync and the two syncs can recover all the data if we choose this linear solution. So we are sending A plus B in this node in the center of the network. So at the end, the first sync has the vector AB times this matrix 1101. And this matrix is invertible, so we can recover A and B. And the second sink can also recover A and B because we have to invert the matrix 1011. Invert the matrix 1011. Now, what happens if there is an uncorrected error? So, let's say there is an error in the first link, but in this link, we couldn't correct the error. Now, this error is going to expand throughout the network. So, in the next links, we are also going to send the packet A plus the error E, which happened in the first link. And this error is going to propagate, so it's going to expand. Propagate. So it's going to expand through the whole network. So it's going to affect the majority of the links of the network. However, if we look at what the syncs receive, then we can see that even though this error has propagated through the network, it still looks like something of size one. And this is where the rank metric is going to play a role, but also other metrics, as we will see. See, and this error we will see that these metrics will actually count this error as one error, which is the whole point. So now, before going into the metrics, so the thing is how to justify, so in what cases do we have this type of errors in network coding? So first there could be errors just from channel noise, like in traditional communication scenarios. communication scenarios. However, these are usually corrected in link just link-wise. So in every link we apply error correction. And then there can be other types of errors and erasures. For example, we could consider packets which are lost. We can consider them as erasures just to avoid resending the packets, to avoid delays. And then there is another scenario which happens in Happens in the case of security, which is that we have an adversary or a malicious node which is injecting errors in the links that cannot be corrected link wise. And this is usually the scenario that we consider in error correction in network coding. So we could, yeah, so this errors could not be treated as a ratio, so that's that's a problem. And this is called. This is called the adversarial model, which is the one that I'm going to describe, but there are also other models that I will give references for different treatments. So in this adversarial model, what we do is we send the k packets. So k is going to be the number of packets that we want to send. And we are going to add redundancy. So the minimum cut is going to be also n, but we are not going to send n packets. But we are not going to send n packets, we are going to send less, so k, for example, and k is going to be smaller than n. And instead of being vectors in fq to the m, I'm going to see them as elements of the field fq to the m because, well, thanks to this, we can also consider fqm linear codes. So we are going to use a code as in classical coding theory. So we have our information vector x with k components. With k components, and we encode it into n components, so n component or a vector of size n. So each component would be a packet. So this forms a vector of length n over fq to the m. And this is done with a code C, which is just a subset of FQ to the M to the M. And this could be FQ linear or linear over FQM also. But in principle, it's just a set, a subset. A subset. Now, if A is the transfer matrix to a given sink, so here I'm just focusing on one sink because the error correction is done separately, one sink at a time. We don't have to correct them in all sinks at the same time. So we have the transfer matrix of a given sink, A, and then this sink is going to receive this vector, Y, which is C times A plus E. times A plus E. So it's going to be the code word C multiplied by the transfer matrix plus some error. And now we are going to define the number of erasures as n minus the rank of A and the number of errors is going to be the rank of E. So for now it's just a definition and now we are going to justify why we count the number of ratios and the number of errors like this. And here what the rank of so E is a The rank of so e is a vector in FQM to the n. So the rank is going to be the rank of the associated matrix when we expand this the components of these vectors so that we obtain a matrix of size m times n over fq. So that's the rank of this matrix is going to be the rank of the vector e. So why do we define the numbers of ratios and errors like this? Well in the case of This? Well, in the case of erasures, it's a bit of a heuristic approach. So, here basically, the rank of A is the number of independent combinations of the original n packets that we have at the sink. So, kind of this means that the other n minus rank of a have been erased. So, basically, this is the number of combinations that we need to obtain the whole packets that we send. All the n packets that we sent. Now, in the case of errors, this is a bit more involved. So, here what we consider is that the rank of E, this matrix E, the error matrix, is basically the maximum number of packets that the adversary can inject in the network. And this is because of this well-known formula for the rank. So, the rank of E is basically the maximum integer. Of E is basically the maximum integer R such that E can be decomposed as E prime times B. And here E prime is of size M times R and B is of the size R times M. So here the interpretation in the network is that E prime would be the actual arrow. So the packets that the adversary is injecting, and P would be the transfer matrix from the adversary to the sink. The sink. And here we are assuming that the adversary is injecting packets in the most harmful way so that R is being maximized. So that's why the number of errors is considered to be the rank of the matrix E at the end of the, so at the sink. So it's because it's like a worst case. So basically this rank is the maximum damage that the adversary can cause. Course. So, yeah, we treat this worst-case scenario. So, there are other papers, there are references where they treat a probabilistic case instead of a worst-case scenario. And we also want to correct all of the errors always and in a deterministic way. So, basically, we want to correct, we want to obtain the code word C from the received word Y with no errors and with probability Y. So, and in this model, we also want to treat the network as a black box, which basically means that the underlying graph and the linear network code should not be relevant for the error correcting algorithm. So, and for the code. So, it means that we only see the transfer matrix at the end, but we don't see the topology of the graph, and we don't see the linear coefficients in between. So, in this case, So in this case, so this basically is the model, basically the description of the model. And we have here two options that which usually we consider, which is that the transfer matrix A is known to the receiver, and this is called coherent communication. And this is sometimes realistic, sometimes not so much. But basically, in this case, if the sync knows the transfer matrix, then what we want to know will be what we want to know what we want to do is to decode by minimizing by using a typical the typical idea of minimizing the distance between the received word and code words from the code. So our solution C will be the code word C prime of C Of C that minimizes the distance between Y and C prime times A. And here we can compute this because we know A, we know the transfer matrix. And here this can be done, this is possible if and only if two times the number of errors, two times T plus rho is less than the minimum run distance of the code. So here is where the run distance appears. But now there is another situation. But now there is another situation, which is that the transfer matrix is not known to the receivers. So this is called non-coherent communication. And in this case, what we want to minimize is this other distance. So it's the distance between, the rank distance between y and c prime times a prime. And here C prime is all the candidate code works, but A prime is also all the candidate transfer matrices, because we don't know the transfer matrix. And in this case, we can correct the errors. we can correct the errors if and only if 2t times rho is smaller than this other distance. So this would be the minimum injection distance of the code, which is defined like this. So it's the minimum injection distance. So this would be the minimum between the injection distances of the column spaces of any two code words of the code. And here call would be the column space of the matrix associated to the code word. So it would be a subspace of FQM. So it would be a subspace of FQM and di is the injection distance between any two subspaces which has this formula. Now this I will give the reference later but the proof of this I'm not going to give the proof but yeah it's a bit involved but basically what we have is that in the non-coherent case we have this other injection distance instead of the rank distance given the capacity of the correction capacity. Capacity of the correction capacity of the code. So basically, in the coherent case, this means that we can use the code itself and its rammetric properties. And well, we have the definition of rate as usual, which is basically the size, taking the logarithm of the size divided by the logarithm of the size of the total space. So divided by mn. And we can also take logarithms in base qm, which is useful, is usually useful for considering. Is usually useful for considering codes which are linear over FQM. And in the non-coherent case, instead of considering the code itself, we can actually consider the set of column spaces, or we can just forget about the code words and consider just subspaces. So in this case, we could study directly subspace codes, which basically means we take a subset of the set of all subspaces of FQM, and this would be a subspace code. via subspace code. And we can use the injection distance in the subspaces themselves instead of the vectors. Now, usually because the number of injected packets by a subspace would be the dimension, then we usually it is very difficult to consider constant dimension codes, which means that any two subspaces of the code have to have the same dimension, let's say n. And in this case, we can also give a definition of the information. Definition of the information rate. And the justification is actually very similar, and the definition is the same. So, basically, it's the logarithm in base Q of the size of C and Mn basically would be the logarithm in base Q of the total number of possibilities of the total size of the message size. So, well, the objective is to construct codes with the maximum possible minimum distance. Maximum possible minimum distance and at the same time with the maximum rate. In the coherent case, well, this is done by maximum track distance codes by definition, basically, because we have a bound on the minimum distance given the rate or of the rate given the minimum distance. So the bound is basically a random metric version of the singleton bound, which in general has this shape. Now, if Now, if we take logarithms in base QM and we consider the alphabet to be QM and this alphabet, we don't want it to play a role in the bound, which is the usual case in the singleton bound. Then we have this other bound, which is more similar to the classical singleton bound. And well, there are some considerations. So, well, MRT codes, they exist. You saw it in the talk by Alberto, so I'm not going to go into the details of how to. Not going to go into the details of how to construct them and so on. Now, there are more considerations. Well, the sizes for decoding, the complexity of decoding, also what is the complexity of least decoding, etc. But these are more questions about dragometric codes, so I'm not going to go into that. And now, what happens in the non-coherent case? So, the situation is similar. So, we also have a single tone bound, but in this case, it has a B. But in this case, it has a bit of a different shape. So, in this case, the singleton bound is like this. So, we have the size of the subspace code is upper bounded by this formula, which is the Gaussian coefficient of m minus d plus 1 over m minus n. And here, m is the packet length, di is the minimum injection distance, and n would be the dimension of the subspaces. So, here we are considering a constant. So here we are considering a constant dimension code of the dimension of n. And yeah, so C is a subspace code such that every subspace has dimension n. And now we can also, in this case, it's not clear which are the codes, if we have codes attaining this bound, but there is a construction already introduced in the first paper by Ketra and Shisha, which is very close to this bound. Close to this pattern, which are called Ketashi-San codes. But basically, they're also called lifted MRD codes because they are constructed using MRD codes. So the idea is we take an MRD code in FQM to the N or FQ to the M times N, and for each code word, we consider the column space of C, and then we append the N times N identity matrix. And this is a subspace of dimension N in FQ2. Of dimension n in FQ to the m plus n. So we have to expand the packet size, but usually this is not a problem. And we obtain subspace codes which are very close to the singleton bound. However, in this case, this is not like in the MRT case. They don't attain the singleton bound and it's not clear. There are many tables on the internet that you can see which are the best subspace codes that we know so far, but the picture is very But the picture is very, I mean, there are many papers and it's not very clear. So, yeah, there are many different families which can be record-breaking for different parameters. So, I'm not going to go into the details because the literature is also very vast in that area. And well, in the case of the Ketergishan codes, they give a bound on how good they are. And the bound is given as a difference between the rate of an optimal subspace code. Optimal subspace codes and the rate R of their construction. So they take that difference, they divide by the optimal rate, and they give an upper bound. So this upper bound is actually, it becomes very small when the parameters become large. So it means that their codes become exponentially asymptotically asymptotically optimal. So they attain a single tomb asymptotically, but for small parameters they are still far. Small parameters, they are still fine. So, well, this was the adversarial model, but the study of error correction network coding was started by Jung and Kai. And they basically also considered an adversarial model, but their model, so the way they treat it, they have to consider the underlying graph, the graph topology, and also the network code in order to give the bounds and the constructions. Whereas what we did before, we Before we don't really consider the graph topology and the network code, so it treats the network as a black box in that sense. Now, there were some papers where they give probabilistic error correcting codes, which correct all of the errors in a probabilistic with high probability. For example, in this paper by Jack Yeta. And the good thing about this one in particular is that it can also work on hypergraphs. Also, work on hypergraphs, which is good for wireless network coding. Whereas the other approaches, I think they, I'm not sure how it could be adapted to hypergraphs. Maybe it can, but I don't know. And then there are also some other deterministic approaches, but which depend on the network. So they depend on the topology of the graph and also on the network code itself. For example, in this work by Tsam from 2008. It's from 2008. So, the approach that I explained, which is independent of the graph and also is independent of the linear network code, is the approach that was given by Ketra and Shishang in their first paper in 2008 and then later also explored by Silva and the same authors, Shishang and Keter. And all the proofs about why these metrics work for error correction in these cases, you can see the last paper called On Metrics for Error Correction. Called on metrics for error correction. They explain very well the justification of using these metrics for error correction. So now, next thing I'm going to briefly talk about security. So here this is a similar thing, but now we also have an adversary, but now the adversary, instead of adding errors to the network, what the adversary is going to do is it's going to observe the information in some of the links of the network. In some of the links of the network. So, for example, here, let's say we have a web tapper which observes one of the links. So, that means that it's going to obtain a linear combination of the sent packets. So, in general, if we have a Wiretapper which observes mu links, then he's going to observe a vector of the form w, which is c times b. And here c is the code word, and b is going to be the transfer matrix to the wire w. To be the transfer matrix to the wire tapper, so that's going to be a matrix of size n times mu. And we also assume the worst case scenario, so the adversary is going to obtain as many linear combinations as possible. So mu is going to be also the rank, so this matrix is going to be a full rank. And we assume that this is less than n because otherwise the word tapper obtains all of the information. And also, we are going to assume that the word tapper has unbounded computational resources so that Bounded computational resources. So that means we want information theoretical security and not just computational security. So the approach is going to be like the classical approach for information theoretical security. So we are going to use a pair of linear nested codes. So instead of one code, we use two codes, C2 and C1, which are one containing the other. And if you know about information theoretical security, then probably you have already seen this type of encoding. Already seen this type of encodings. So the idea is we take a space D such that the direct sum between C2 and D is C1. Now we take a generator matrix G of D and then our message X we are going to encode it using G. So it's going to be X times G. And we add C prime, which is going to be a random code word inside C two. So it's going to be a code word in D plus a random code word in C2. A random code word in C2, and this is going to be a code word in C1. And the information rate is going to be the dimension of D because this is the, I mean, we have as many possibilities as the dimension of D to encode information. And the dimension of D is the dimension of C1 minus the dimension of C2, because C1 is the direct sum. So the rate is the difference of the dimensions divided by N. And here I'm assuming that these codes are linear over FQN. Assuming that these codes are linear over FQN. So these are basically linear codings. So basically, these encodings are always going to be linear. Now, C1 could also be used for error correction. So I'm not going to explain here why, but we could use C1 also for error correction. And if there are no errors, then we can take C1 as the whole space. So this would be the simplest scenario for security. And now, yeah, well. And now, yeah, well, and there is also an error correction capability of this pair, which would be given by the relative distance, in this case, the relative, the minimum relative run distance, which is given by this formula. But I'm not going to focus on the rough correction in this case. So, regarding security, what we want is perfect information theoretical security, so the word type should obtain no information about X. So, that means that. So that means that x is going to be treated as a random variable, and the information about x containing w is basically going to be the mutual information between x and w. So x is a random variable. C prime is also a random variable because we chose it as a random code word in C2. So w is also a random variable. And the information about x in w is the mutant information, which has probably you know it has this. Probably, you know, it has this formula. And we want this to be zero. So, but before that, we are going to compute it. So, there is a particular case in which it's very easy to compute. So, if x is uniform, which is usually we can assume it, and C prime, we choose it to be a uniform random variable. So, in this case, it's just by choice. In this case, it's very easy to compute the mutual information. So, the mutual information is given by this formula. Given by this formula. So it's the dimension of the difference of the dimension of these two codes. So we have the two L of C2 and C1 intersected by VL. So VL here is the set of vectors such that the row space of the corresponding matrix is inside of L and L is the column space of B. So this is a bit of a bit cumbersome. The notation might be a bit heavy, but this is actually not very difficult to prove. This equality and basically, we can do it also using linear algebra. So, this is not very involved. Using linear algebra, we can show that the mutal information is zero for every matrix B of size n times mu if and only if mu is less than the minimum relative, so the relative minimum distance of the dual pair. So, instead of c1, c2, now it will be the dual pair, c2 dual, c1 dual. And again, this is the proof is not very difficult. And again, the proof is not very difficult, it's using the definitions of multi-information and then just linear algebra. So basically, we have a description of the number of links that the wordtapper could wiretap without obtaining any information about the message. And again, from here, we could give a bound. Basically, it's derived from the singleton bounds. So the maximum mu would be n minus k, where k would be the rate that the k would be the rate the difference of the dimensions so n minus k would be the a bound on the possible on the maximum number of links that the wiretapper can observe and this bound um well it can also be proven for for general coding schemes whether they come from linear codes or not and even take in error correction into account so we could give a bound of this shape so we take a randomized encoding scheme very secure for new that is secured for new links and can also correct T errors and row erasures and then the entropy of the secret message would be at most n minus 2t minus minus mu and well this bound can be attained by choosing an estepped pair of MRT codes which is not very surprising because both the security and the error correction is given by the minimum run distance so if we choose both to be MR We choose both to be MRD codes, and then we attain this bound, both for security and for error correction at the same time. So, well, the idea of nested linear coding was introduced by Osaro and Weiner. This is a classical idea from many years ago. It's independent, it doesn't have anything to do with network coding. And it's just a high-dimensional extension of the one-time pad, which is even older. And as you may know, it was As you may know, it was proven to be information theoretically secured by Channel at the beginning. And in network coding, security was studied first by Kayan Yeo in 2002, where they give the bound, this bound n minus mu, and they also give a construction. The problem is that their construction requires a large base field, FQ, for the network. And then this bound and the field size was improved later by. Size was improved later by Fedman et al. And then there was also another scheme using probabilistic error correction and security by Jaggian Lember. And then an algorithmic construction by Salim El-Rajim Etal. And then the solution that I showed here using MRD codes was proposed by Silvan Shishan in 2011 in this paper on secure network coding. So, well, and then the results. So, well, and then there is also some works on which study the amount of leaked information using a notion of generalized weights. And this was started by Ogier and Ashbui in 2012. Then it was generalized to any FQM linear code by Kurihara et al. And we extended it to FQ linear codes in our paper in 2017. And this is basically the same, it's an analogous version of the classical general isoming. Of the classical generalized amine weights for the classical wiretap channels. So, if I have the time, I'm going to show a more recent area of network cooling, which is multi-shot network cooling. So, basically, most of what we have seen translate to this case. So, I'm not going to go too much into the details. So, basically, the thing is, well, So basically the thing is, well, the max flow mean cut theorem that I showed at the beginning basically applies to one shot of the network. So this means we want to send a given number of packets in one use of the network. However, usually the channels are used more than once. So this is the idea of multi-shot network coding that we are using the network with its network coding more than once. But now we have errors and erases. We have errors and erasures spread throughout the different shots of the network. So if we use the network L times, there could be one error in the first time, but no errors the second time we use it, or vice versa. So this is kind of like the memoryless, discrete memoryless channel, but where the channel is basically a network with linear network code. And what this scenario was considered, this scenario of error correction in multi-shot network coding was considered by T-shirt network coding was considered by Nobreka and Joafiji in these papers from 2009 and 2010. And basically, the idea is: as you see here, we have several uses of the same network. So here we have four shots of the same network. And we have a given number of errors, but we don't know in which shot the errors occurred. And actually, in one of the shots, it would be that there are so many errors that we cannot correct in that shot of the network. Shot of the network. So we want to correct the errors globally throughout the early shots of the network. So basically, the scenario is very similar as before, but here the received word is of the form y equals c times a plus e. And here, well, m is still the packet length, fq is the network alphabet. And then big n here is going to be l, the number of shots, times small n, which is the cut of the Small n which is the cut of the minimum cut of the of the network so there is number of shots n is the minimum cut or the number of incoming links also and here we are going to subdivide the code word c into l components and each component is itself a vector of length n small n and this is the vector received in the eyeshot of the network and here the thing is that this matrix A the transfer matrix has a very A, the transfer matrix has a very specific shape. So it's going to be a block diagonal matrix, where each block is going to be a matrix of size small n by small n. And the global matrix is going to be of size capital N by capital N, and it's going to be blocked diagonal. And the number of errors and erasures is just going to be the sum of the number of errors in each of the shots and the number of erasures in each of the shots. So the formula. Of the shots, so the formula is going to be like this: so the number of errors is just the sum of the ranks of each of the errors in every shot, and the same for the erasers. Umber, yes, yes, four more minutes, please. Okay, yes, can so you have four more minutes, thank you. Okay, so yes, so basically the analysis carries over to the multi-shot case. Harries over to the multi-shot case. So basically, everything works in a very similar way. Now, considering the appropriate metrics. So, in the case of errors, for the coherent case, we are going to consider the sum rank metric, which is basically given by the sums of the ranks. And in the non-coherent case, we are going to consider the sum injection distance, which is also a sum, but in this case of injection distances. Of injection distances. And here, what we consider is tuples of subspaces. So, here the code words are going to be tuples, and each component is going to be a subspace in FQ to the n. So, the metrics are very easy to describe. It's basically just the sums. Another problem is with the optimal codes. So, we also have a singleton bound, which is analogous to the rank metric case. Case. And well, if the size of the alphabet is Qm, which we consider it unconstrained, then we have a more classical singletone bound as before. And in the case of the sum injection distance, we also have a singleton bound, which is very similar to the previous one. It's just a bit more complex to describe, but the ideas are quite similar. Another problem is finding the best codes for Finding the best codes for this scenario. Now, the problem is we could take an MRD code. So, if the taking an MRD code in FQM to the N, capital N, this is also an MSRD code for every partition of capital N. So, we could partition capital N in L times N, and this also attains the singleton bound. But the problem is that the fit size is going to be exponential in capital N, which is very large and is usually complex for decoding. is usually complex for decoding so it's complex complex computationally expensive so um the challenge here in network collection in multi-shot network coding was to obtain optimal codes for smaller field sizes so basically msrd codes or maximum some transistance codes for smaller fields which are not exponential so we obtained one in this paper from 2018 which has this size which is polynomial Which has this size, which is polynomial in the number of incoming links, if that's a constant number. We have there is some other constructions which also have sub-exponential fit sizes. And then we also gave a lifted version, which would give some constant dimension codes with very close to optimal sum injection distance in this paper with Shisha. And well, in this area of codes in this area of codes in the SOM drug metric and in the SOM injection distance there is a lot to do so this is actually a very a very new area a very recent very open and well this is just an observation which is that the Somerang metric recovers the Hamming metric as a particular case but I'm not really going to go into the details but basically the Hamming metric case would be also a particular case of the some rank metric not just the rank metric. Not just the run metric. And well, there are many works in multi-shot network coding, but it's still a very open area. So there are some works on convolutional codes in the summary metric. So here are just a few, but there are more. And then there are also some works on bounds on the parameters and other properties of codes in the summary metric. This very recent paper by Bir Netal. And we also have some works on generalized weights for FQM linear codes and FQ linear codes. Linear codes and FQ linear codes. And there are also some works on faster decoding algorithms for MSRT codes, also on the hardness of decoding in general for codes in the SAMRAG metric. And well, just the last thing, we are preparing a survey tutorial on codes in the SOMRAG metric, which will appear at some point soon. But yeah, in this area, there have been some papers. In this area, there have been some papers in there recently, but there are still it's very open, so there are still plenty of open programs. And with this, I will conclude my talk. So thank you very much for your attention. Thank you. Thank you, Umberto, for all this information in network coding. So we really appreciate it. Let's thank Umberto, Umberto, please. So we have time for one very quick question. For one very quick question. Well, maybe you can join us today later in town. And so if people want to join, maybe they can ask you there. So let's thank Umberto again, please. So everyone is. So, everyone is welcome to the Gathering Towns, and we are going to put the information.