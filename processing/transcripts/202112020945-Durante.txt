Okay, so thanks a lot, Ramces, and thanks everyone for joining online and for the lucky ones who are now in Oaxaca. I hope you are enjoying the workshop. Now I'm going to the talk. As you can see from the title, today I will focus on Bayesian regression models for binary, categorical and partially discretized data. And in particular, as you will see, what I will try to do is a sort of medley of my recent results. Medley of my recent research along these lines. And let's say the final take-home message would be that, unlike what thought so far, there actually exists a class of priors which is conjugate to the likelihood induced by these type of models. And these priors are unified skewn distributions. So it's a broad class which extends multivariate Gaussian in order to include skewness within these variables. And the interesting thing is that this class actually preserves several closure properties of the original multivariate Gaussian distribution. Original multivariate Gaussian distribution, which means that if this priority is conjugate to this model, also the posterior would be from a unified skew normal. And if this class is structured, potentially even posterior inference would be much easier than was thought before. So basically, this is in a way all I wanted to say. The seminar could end up here, but I have some more time. So what I would try to do is to go much more deep into the details. So anytime I start a talk on regression models, I always like to begin with this quote by still. I always like to begin with this quote by Stevenson when he says that statisticians are engaged in an exhausting but accelerating struggle with the biggest challenge: how to translate information into knowledge. And well, if this is true, then regression when possible is definitely a great way to address this quote because it provides us with models and methods to understand how the distribution of some response variable or functionals of that change with covariance. However, as we also know, anytime we go beyond a regression model, Go beyond regression models for Gaussian response data, life is as easy. For example, in the Bayesian setting, this is because we typically lose conjugacy between the commonly assumed Gaussian priors and the likelihood induced by the non-Gaussian response data. Now, this happens also in regression models for binary data, which is arguably one of the most widely used class of regression models beyond those for Gaussian responses. And for example, in probit setting, what we do is we are Setting, what we do is we assume that our binary response data are conditionally independent realization from a Bernoulli variable whose probabilities obtained by mapping a linear combination of the covariates into the probability space using the probability link. And then what we can do at this point, as Bayesian, we need to select prior distribution for our beta parameters. A common choice here is to use multivariate Gaussian priors. And then all the goal is to direct the posterior distribution, so the conditional distribution of my beta parameter given the observed data. Make beta parameter given the observed data, and then perform posterior inference basically by studying the properties of this posterior. This is the goal, then we can just apply base rule. In the probit setting, this will tell us that my posterior density is equal to the prior density, so the Gaussian one, times the probit likelihood divided by the normalizing constant, so just the marginal likelihood. Now, what we do as Bayesian at this point is to look at this formula, in particular the numerator, cross fingers, and hope to recognize the kernel of. And hope to recognize the kernel of some well-known or at least tractable class of random variables. And this doesn't seem to be the case, for example, in probit setting. So, what we can do at this point is to, let's say, leverage all the machinery that we have in the Bayesian setting in order to perform inference on intractable posterior. So, for example, we can go with sampling-based procedure like MCMC routines, such as Metropolis ASIN, Skip Sampling, Amingonia, Monte Carlo, or we may move to optimization-based solution where we try to. Optimization-based solution where we try to approximate the posterior with some more tractable class of distribution using, for example, Laplace approximation, variational base, or expectation propagation. Now, all these methods are clearly state-of-the-art. They are nicely reviewed in an article by Nicolas Schuppin and James Riget on probit regression. However, still, also as mentioned in the article, they have some problems in high dimension, either in terms of scalability or in terms of quality of the approximation. And then at the same time, of course, they are clearly still. Same time, of course, they are clearly still suboptimal compared to situations where the posteriority is available in closed form and belong to some tractable class of random variables. So, what I proved recently is that, say, I like what's thought so far, actually the posterior distribution in probit regression with Gaussian prior is available and it's a unified skew-normal distribution. Then, more generally, as we will see later on, unified skew-normal are actually conjugate priors for probit regression. So, what are unified skew-normal distribution? Skew normal distribution, where it's a broad class of distribution that has been introduced by Areganovalen Dechi-Zellini in order to unify several extensions of the original multivariate skew normal distribution, whose density is obtained by modifying the one of a pivot Gaussian times the commodity distribution function of a univariate normal distribution evaluated a linear combination of the value of the variable. Now, the rational for why introducing this distribution was mainly to model skewed phenomena, and actually the success. And actually, the success of this class of distribution in the applied context actually led to several different extensions. For example, by including a skewness-inducing mechanism which is multivariate, no more univariate, or by including additional parameter, mainly, let's say, to recover several closure properties of the original multivariate Gaussian distribution. Now, what I want to do, so what they did basically was to unify all these different extensions within a unique class, which is called unified. Within a unique class, which is called unified skew normal distribution. As you can see, it is parameterized by several quantities. I will discuss them better, much more in detail, when we'll see those that we obtain under the posterior distribution, but these may be a parameter that control location, scale, dependence, structure, and skewness. What I want to highlight here is the density of such a random variable. As you can see here, it's a Gaussian density multiplied by the CDF of a Q-variant normal distribution divided by the normalizing constants with... Divided by the normalizing constants, which is the CDF of an added Q-variant normal. And by seeing this density, then maybe it starts becoming more clear for why this is the posterior and the probit regression with Gaussian prior, because by applying Bayes' rule, as we've seen before, this may be just my prior density. This may be just a way of rewriting the probit likelihood, and this is just the normalizing constant. So, just to be more clear, what I prove is that if we are in a classical Bayesian probit model with Gaussian. probit model with Gaussian prior on the parameter, then the posterior is a unified skew normal. Wost parameter, as you can see here, can be obtained by simple linear algebra operation applied to the prior mean, prior variance and covariance matrix and data. Data enter in the matrix D, which is nothing but your design matrix where you change or not the sign to the row or the column, depending on whether the associated y is one or zero. Okay, I will go much more into the detail on the role of the prior parameter. On the role of the prior parameter and the data in controlling the shape of the posterior. What I want to show you here is that the proof is extremely simple. Again, you just need to apply Bayes rule and recall the density of a unified skew normal. Because if we remember the Bayes rule we applied before, my posterior is the Gaussian prior. This is just a way of rewriting the probit likelihood because probit likelihood is just a product of univariate Gaussian CDF. So I can write it as a CDF of an n-variant normal distribution evaluated a linear commit. Distribution evaluated the linear combination of the parameter with a diagonal covariance matrix so that I can factorize the sprout of the marginals. And then what I need to do is just to play inside here in order to highlight the unified skew normal parametrization and I'm done. So this result is not just a curiosity in the sense that, as I was mentioning before, unified skew normal have many interesting properties similar to the original multivariate Gaussian distribution. For example, they are closed under marginalization. They are closed under marginalization. This means that the posterior of each parameter is again a unified skew normal. The avonormalizing constant, which we have seen before, is just the CDF. In our case, would be of an n-variant Gaussian distribution. So potentially, we can also compute closed-form formulas clearly depending on CDF of multivariate Gaussian for the marginal likelihood and for predictive probabilities. Another interesting thing is that you can represent a unified skew normal as linear combination between As a linear combination between a p-variate Gaussian distribution and an n-variate truncated normal distribution. So, as long as we can easily sample from these two random variables, then we can obtain IAD sample from the posterior by simply computing a linear combination of them. And then we have also a moment generating function available in closed forms, so potentially we can even compute moments without relying on sampling-based procedure. So, this is the LT representation, which also helps a bit in clarifying the role of the A bit in clarifying the role of the prior parameters and the data in controlling the shape of the posterior. So, as I was mentioning before, I can represent my posterior, which is a unified skew normal, as a linear combination between a p-variate normal distribution and an n-variant truncated normal one. So again, this is potentially useful for sampling, as long as I can easily sample from these two random variables. At the same time, it tells us something about also the role of the prior parameters and the data in controlling the shape of the posterior. Controlling the shape of the posterior. For example, as you can see here, the prior mean, as expected, plays a major role in the location of the posterior. Then the prior scale, which enter in this element W has a role also in controlling the scale of the posterior, but also as an effect, let's say, in controlling the weight, which is assigned to the multivariatum cathode component, which actually is the one which is bringing in skewness within our posterior. Again, also data plays more than a role, but a Plays more than a role, but a particular one is in controlling this weight, which is the one that is controlling how much skewed actually would be our posterior, because the skew is brought by this n-varietronicated normal component. What about posterior inference? So as I was mentioning before, potentially now we can do posterior inference leveraging all the properties of unified skew normal distribution. For example, they have expectation which is available in closed form and depends on C. form and depends on CDF of n valued normal distribution. Here you see the formula. So potentially if I'm able to easily evaluate the CDF, then I can compute the posterior mean in a very simple way without the need to rely on MCMC methods. And also we can compute, we can have expression for the posterior predictive probabilities and the marginal likelihood. So actually the marginal likelihood in this case is exactly equal to the normalizing constant of the unified skew normal distribution. So this would be the SQ normal distribution. So, this would be the CDF, as you can see here, of an end-variate Gaussian distribution evaluated a suitable set of parameters. And from that, we can easily obtain the predictive distribution. So the distribution for a new data point, given the data that I observed so far, this can be easily written as the joint marginal likelihood for the new data point, the previous data, which will be the CDF of an n plus one varied Gaussian divided by the marginal likelihood of my original data, which we already know to be the CDF of an. We already know to be the CDF of an n-barrett normal distribution. So, this is a point where again the toll could end up because, again, we have all this closed-form result, we can potentially sample IAD from the posterior. However, there is a point which I overlooked a bit up to now, because as you can see, if I want to evaluate posterior moment in closed form, if I want to sample IAD from the posterior, I need to be able to evaluate CDF of n-variant normal distribution with full covariance matrix. Distribution with full covariance matrix, or I need to be able to sample from n-bariotuncate a normal distribution with full covariance matrix. Clearly, these two are very much related tasks. And as we know, addressing this goal in either so when n is large is still an ongoing problem, which is actually always motivating lots of research throughout the years. So later on, I will try to address it. What I want to stress here is that, as you can see, an interesting thing of this result is that we are no more facing the problem of high. No more facing the problem of high-dimensional settings. So, where when P, the number of inputs is large, because here the problem is on the sample size dimension. So, with this result, we can easily address, let's say, cover a gap because we can have very good and simple posterior inference in setting where previous MCMC methods were having most of the problems. But again, we need to address the problem of large N. So far, we can do this until small to moderate using state-of-the-art results. Using state-of-the-art results, like recent, even recent contribution. But again, when it will get much, much larger, we will need to discuss this a bit better. I will go to this problem later on, because what I want to do now is spend two slides on some, say, follow-up results that were motivated and inspired by the conjugacy of the unified skill normally profit model. One interesting one is in time-series setting. So, in this case, we have binary data observed through time, and what we want to do is Through time, and what we want to do is to build up some sort of statistical model for them, and which allows us to perform meaningful inference and potentially also prediction. And if this is the goal, then the simplest model we could look at is the probit extension of classical dynamic linear models. So what I'm saying here is that my data at time t will be Bernoulli distribution, random variable whose probability is obtained by mapping to the probability space a linear combination of the predictor according to my parameter beta. According to my parameter beta, and then I allow beta to change through time through a linear state equation, which is where I write him as a linear mapping of the beta parameter the previous time plus a Gaussian noise. This is the classical extension to probit setting of dynamic linear model. Now, what I can do at this point, if we recall what are what type of inference that is done in classical dynamic linear model, I may be interested in several quantities, like for example, the predictive distribution. example the predictive distribution so the distribution of my parameter time t given data up to time t minus one i may want to study the filtering distribution so the distribution of the data time of the parameter time t given data up to that time or i may want to look at the entire smoothing distribution so the distribution of the whole sequence of beta parameter given the whole sequence of data now if you recall what is done in the dynamic linear model in particular with the kalman filter you know if i want to obtain them in a report You know, if I want to obtain them in a recursive manner and in closed form, then I need some sort of conjugacy properties between the state equation and the observation equation, and I also need that the distribution that I obtain is closed under linear combination. These are the two key properties that, again, are leveraged by the Kalman filter, which allow us to obtain this distribution in a recursive manner via simple linear algebra updating of the parameter of the previous time. So, up to this point, so without the unified skew normal conjugacy, Without the unified skew normal conjugacy result, the typical approach is to go through extended Kalman filters. Okay, so where you try to approximate the Bernoulli likelihood with some Gaussian one and then apply the classical Kalman filter occupation. At this point, our question was, well, can we now leverage the result of the unified skew normal conjugacy in probit model? Maybe yes, okay. And the hint can be seen by studying the filter distribution for the first at the first. Distribution for the first time point one. So distribution of beta one given the first data point. Then by applying Bayes' rule, this is proportional to the Gaussian density of the first data point without conditioning on the data, which can be easily computed by the state equation, assuming that the first state, so the state at time zero is Gaussian. And then we just need to update that with the likelihood of my new data point, which is Bernoulli 1. So as you can see here, now we are basically going back to a Now we are basically going back to a sort of probit regression with Gaussian prior and probit likelihood for a single data point. So we can apply again the previous result to prove that the filtering distribution at the first time is actually a unified skew normal. In this case, it's actually a multivariate skew normal. So starting from that now, we may start applying the classical recursion that are done in the Kannon filter. So what I may want to do, for example, once I have the filtering distribution at the given time point, I may want to obtain the predictive distribution. Obtain the predictive distribution. So the one of my data, my coefficient at time t given those at time t minus one. So if I want to do that, then I just need to play with the state equation. And if I know that beta t minus one is a unified skew normal, so recall what we saw for the first time point, then what I need to ask myself is how is distributed a linear combination of a unified skew normal and a Gaussian. But again, a Gaussian is a specific example of a unified skew normal and unified skew normal normal normal Unified skew normal and unified skew normal close under linear combination. Okay, so linear combination of unified skew normal are again unified skew normal. So I can leverage this result to prove that also the predictive distribution at time t is a unified skew normal with parameters that are just simple updating of those of the filter. Once I have the predictive at time t, I may want to obtain the filtering at time t, which just require me to update the predictive with the new data point at time t. So I just need to update. point time t so i just need to apply a base rule which will tell me that the filtering is proportional to the predictive okay times the likelihood of the new data point time t but if the predictive is a unified skew normal this basically acts as a prior in this probit regression for the new data point so i can leverage the closure property of the unified skew normal and prove that also the filtering is a unified skew normal and then i can iterate through this step basically having a sort of analog of the kalman filter and once i have that we can leverage And once they have done, we can leverage the result that we have seen before in order to perform inference. So, actually, here we also develop optimal particle filters, a rubber-laqualized version, and also look at procedures. Now, another interesting extension is to multinomial setting. So, so far, we have just looked at binary ones. The question is, what does it happen if we extend to J categories? So, we need to move from probit model to multinomial probit. And typically, this construction relies And typically, this construction relies on a set of Gaussian Latent utilities, one for each category, and then express the probability of a given category as a function basically of a sort of comparison among these Latent utilities. For example, in the model by Stern, which is a very popular one and probably the closest to multinomial profit, what you assume is that for each statistical unit, you have capital J latent utilities, one for each category, which are modeled through a classical. Which are modeled through a classical Gaussian linear regression, where the beta parameter clearly changes across categories in order to induce differences across the latent utilities. And then what happened is that the category that will observe is actually the one with the highest latent utility. So if I want to compute the probability of a given category, simply need to compare the different Gaussian landed utilities, which requires me to compute Gaussian CBF probabilities. Okay, so although all these methods are different, all of Although all these methods are different, all of them are characterized by the fact that the light which they induce are basically product of Gaussian CDFs. And so the question is: well, is the sun conjugate also to multinomial profit? Well, the answer again is yes, because if the likelihood associated with this model are Gaussian CDF evaluated as suitable linear combination of the parameter, I can again rewrite them as just a single multivariate Gaussian CDF. Then I just need to apply the basic. Then I just need to apply the base rule, and then I just need to follow the previous speculation to show that also in multinomial settings, the posterior is a unified skew normal if I'm using Gaussian prior or more generally unified skew normal models. So in this case, we can leverage the previous result to perform posterior inference or potentially sample IAD from the posterior. But as I was mentioning before, we still inherit the problem of large sample size. Okay, so what I will try to do from now on is try to Try to do from now on is try to address the problem of large sample size. So, again, the results up to now are useful for large, even huge P and small to moderate N, which is already interesting setting. But then our question was, what does it happen if we go to much larger sample size? And that we address this problem of evaluation of multivariate Gaussian CDF or sampling from n-variant normal CDF. And the idea was, well, let's try to see whether we can approximate our unified. Can approximate our unified skew normal, which at the moment may be intractable for large n, with another unified skew normal, which is much more tractable even in large n setting, in particular, which doesn't require us to sample from n-variant normal distribution, to truncate the normal distribution or to evaluate CDF of n-variate normal distribution. So, to understand that, as you will see, we will move to a particular variational approximation. So, to do that and to introduce To do that and to introduce it, it's quite useful to look at an augmented data representation of probit models. This has been popularized by a very nice article by Albert and Chib in JAZA. And it's a very simple data aimation because it relies on a construction of probit regression, which basically tells us that the probit regression is nothing but a Gaussian regression on a set of latent Gaussian data, Z, which, however, we do not observe, we only observe what Do not observe, we only observe whether they are above or below zero. So if I marginalize out the z variable here, I get back the probate likelihood for my response data y. Now here you can clearly see why this is a very useful data augmentation, because if I know z, okay, then I can restore the Gaussian-Gaussian conjugacy between the prior for beta and the likelihood for Z. And this actually allows me to derive a full conditional distribution for beta given Z and Y, which is actually independent from Y and is. Exactly independent from y and is a multivariate Gaussian distribution. At the other time, on the other side, of course, we don't know z, so we need to sample z as well from its full conditional, which can be also easily derived by looking that given beta, I have Gaussian quantities, and given y, I'm basically including a constraint on a region. So the full conditional distribution of each augmented data is actually conditionally independent from the other augmented data and is a truncated normal distribution. And is a truncated normal distribution centered on the linear combination of my beta parameter, and is eta truncated above or below zero, depending whether the observed y is one or not. Now, here you can clearly see why this can be extremely useful. For example, we already have the step of a gift sampling algorithm, because we can just iteratively sample from these quantities, which is tractable, in order to obtain sample from the target posterior. At the same time, it's also extremely useful, as we will see. It's also extremely useful, as we will see later on, in order to develop mean field variational approximation with global variable, my beta parameter, and local one, my z variable. So what is mean field variational base with global and local variable? Well, again, what we try to do basically here is try to find a more tractable approximation for my joint cost-zero density of my beta parameter and the augmented data. And when I'm saying tractable, I try to include this. I try to include this notion by forcing my approximating density to live in a class of tractable distribution. And the tractability in the mean field variational base is typically enforced by making some factorization assumption. For example, in classical mean field with global and local variable, at least you assume that your joint approximation density for the beta parameter and the Z augmented data factorizes the product of the two components. Now, the question is, at this point, what we want to do is to find the optimum. What we want to do is to find the optimum, so the best element within this class, and the best element is defined to be the one which is the closest in KL divergence to my exact posterior while being forced to live within such a class. This is what we do when we perform variational base. How can we obtain that? Well, typically what is done is to maximize the elbow. The solution is the same because it's minus the KL plus a constant. And this is generally done via coordinate ascent variational inference. Well, basically, what we do is we approach. Well, basically, what we do is we update the approximating density of each factor at a time, conditioned on the most recent approximating density of the other factors by iterating through these simple equations. As you can see here, they just require that the full conditional distribution are available and tractable. And then what you just need to do to compute the logarithm, compute the expectation with respect to, might generalize out the other quantities with respect to the most recent approximating density, take the exponent and try to recognize. Take the exponent and try to recognize some kernel. From this equation, it's already clear that your approximating density will have the same distribution of your full conditional in terms of the fall, but then of course you will need to update the parameter through this Kali step. So for example, in our case, what we'll add is that the full, so that the approximating density for beta, since the full conditional is Gaussian, would be also Gaussian, and the approximating density for the joint vector Z will For the joint vector z will actually factorize the squared of the marginals because they are condition independent and they will be just a univary truncator normal with suitably optimized parameters as the result of the cabin machine. Now, the question is, so this is extremely scalable as a procedure. The question is, how much accurate is this quantity, especially in high dimension? Okay, so especially when p gets larger and larger. So, what we prove is that actually as p goes to infinity, As p goes to infinity, this approximation that we get for beta does not converge to the exact posterior. So the KL divergence between the mean field approximation for beta and the exact posterior is always bounded away from zero. Now, this is not surprising as a result in the sense that here we are doing asymptotics with respect to p and not with respect to n. So it's not, I mean, it's not required that this goes to zero. However, as we will see later on, with the approximation that we will propose, actually as That we will propose actually as p goes to infinity, the KL divergence between our approximation and the exact post-zero will actually go to zero. Now, one of the reasons for this bias that we have is related to the fact that actually the mean field in I dimension tends to over shrink the location of the exact posterior much more than the actual value. So, what we prove is that as p goes to infinity, the posterior expectations are overshrinked towards zero, whereas under the expectation. Zero, whereas under the exact posterior, we can compute them because we know it's a sign, they are bounded away from zero. And actually, this has also, as you will see later on, a strong effect on predictive probabilities, in particular in overshrinking them towards zero phi. So let's say motivated by this theoretical result, the question was, can we improve this approximation while still being scalable from a computational perspective? So, I mean, if the idea is that we want to improve the approximation, then the simple To improve the approximation, then the simplest thing is: well, let's enlarge the class of approximating families because by enlarging that, maybe we get more flexibility up to the point where maybe our posterior we get actually close to this class of approximating densities. At the same time, clearly, as you start increasing this class, you get more and more complex approximating densities, so they may end up being intractable. So, all the gain here was trying to increase that while preserving tractability and computational scalability. And this is And this is why we came up with this new class of approximating distribution, which we call partially factorized. Because as you can see here, we do not assume the independence between the beta parameter and the augmented data set. We keep that dependence. The only thing that we do is that we assume that the joint approximating density for the augmented data set factorizes the prototype the marginals. The reason why we came up with this class is not magic, it's actually for two main actually for two main for two main results. The first one is that actually the mean field, so the optimum for the mean field approximation lives within this class. You can see it here because also in our case in the mean field setting we approximate that as the product of the two where the Z factorizes the product of the marginals. So it's an element of this class whereas our optimum will be the one which is the closest to the posterior in KL divergence while living within this class. While living within this class, so maybe our optimum at least is guaranteed to be better than the mean field. At the other point, from a much more constructive perspective, the idea was let's try to find a factorization which is much closer to the way in which the posterior factorizes. In particular, as you can see here, the exact posterior, joint posterior for my beta parameter and the augmented data z, I can always write it as the full conditional distribution of beta given z and y. Of beta given Z and Y, as we have seen before, the full condition. So given Z, beta doesn't depend on Y, then I can multiply that by the joint distribution of Z given Y, where I marginalize out the beta parameter with respect to their Gaussian prior. So as we've seen before, the full conditional of beta is Gaussian, whereas the joint distribution of the augmented data given y, once they marginalize out beta, include dependence, and this is will be a multivariate multivariate. Will be a multivariate truncated normal distribution with full covariance matrix. It's very easy to obtain that. We just need to marginalize out bit analytically. As you can see here, the only issue in performing posterior inference, basically in closed form, in terms of sampling IAD from the posterior, is that I need to deal with an Varetrungeter normal distribution. So our idea was: well, let's just avoid assuming independence because this full conditional is Gaussian, so it's tractable. Let's just try to apply. It's tractable. Let's just try to approximate the joint distribution of the Z given Y in a much more tractable way through the product of the marginals, okay? So that potentially may no more need to do with an very truncated normal distribution. Again, as I was mentioning before, a first hint for why this result could be actually could work is that, as I was mentioning before, the mean field solution lives within this class, whereas our solution would be the optimum within this class. Solution will be the optimum within this class. So we are guaranteed that the KL divergence between our solution and the exact posterior will be always lower than the one between the mean field and the exact posterior. But the other key point, as I was mentioning before, is that, well, now that we have enlarged this class, is it still easy to obtain the optimum just by applying simple Cavi algorithm? And at the same time, as you can see here, the optimum that we will obtain will be for the joint between beta and z. The joint between beta and z. So, can we easily marginalize out z in order to obtain the optimum just for beta, which is the one that we are interested in? The answer is yes to both points. Regarding the optimum, so the solution within this new class of approximating density, it can be obtained as easily as the mean field solution. Okay, because what we just need to do is basically apply the chain rule of the KR divergence in order to KL divergence in order to write as the sum of two components. One will be measuring the KL divergence between the full conditional and our approximating full conditional. The other will be measuring the KL divergence between our approximation, mean field approximation for the Z variable and the joint distribution of Z given Y. But now, as you can see, one component can be exactly put equal to zero by equating our approximating density for the full conditional exactly equal to the full conditional. So we are able Equal to the full conditional. So we are able to put one of these terms exactly to zero. And this is tractable because, again, it's just the p-variated Gaussian distribution. So what we just need to do now is to optimize with respect to the approximation of the joint density of Z given Y with the product of the marginals. But now here we can easily apply Cavi algorithm, leveraging the fact that multivariate truncated normals are closed under conditioning. So we can simply apply the Cavi step to prove that the approximating density Prove that the approximating density for each augmented data is a univariate truncated normal distribution whose parameter can be updated through the Caviar routine. So, it's basically solving in an iterative way a non-linear system with n unknowns. And the other interesting thing that you can see here now, so now the final goal is how do we derive the approximating density just for beta? Well, as you can see here, the conditional distribution of beta given Z, so in our optimum, is nothing. Optimum is nothing but a Gaussian component centered on a linear combination of univariate truncated normal distribution. So if I take them out, what I can see is that my distribution for beta is again nothing but a linear combination between a pivot Gaussian distribution and now n univariate truncated normal distribution. So if we recall the additive representation of the sun, this is actually a unified skew normal. And the cool thing here is that now what I am combining is no more an n-variety normal distribution with N-variate truncator normal distribution with Foucault variance matrix, but just n-univariate truncated normal distribution, which is the key point. So, what happened is that our approximating density would be still a unified skew normal, but now I no more need to evaluate CDF of n-variant normal distribution. I will just need to evaluate n CDF of univariate normal distribution. At the same time, I will no more need to sample from n-variant keton normal distribution with full covariance matrix, but I will just need to sample from n univariate. To sample from n univariate truncated normal distribution. So, this allows us to perform tractable inference also in the Cavi routine at a cost which is linear in the highest between P and N. At the same time, so maybe I can be more clear later on on the theoretical result, but what we prove is that as P goes to infinity, actually the approximation that we deliver of the posterior match the exact posterior in terms of K L divergence, and this clearly has an effect on. And this clearly has an effect on all the functionals. Like, for example, we could prove that our estimate of the predictive probabilities goes to the one that we obtain under the exact posterior. This doesn't happen, for example, under the mean field approximation. The other interesting result, which is actually related to those above, is that our Cavi algorithm probably converts in one iteration as p goes to infinity. So, in a way, the cost that I was telling you before is probably the cost of the entire routine. So, I will just skip the simulation part because most of the results here are also clear in the application, and then maybe I will just want to leave more time for questions. So, here is the application. So, here we have an application with large P and moderate N. So, in particular, what we want to do is to model presence or absence of the Alzheimer disease as a function of lots of demographic data, genotype of say results, and also interaction. And also interaction among them, which means that we end up with a sample size of n individuals. We actually held out 33 for computing out-of-sample predictive probabilities, and we have almost 10,000 predictors. So we treat this as a probit regression with these Gaussian priors, and then we applied several methods. So this gives you an idea about, say, the computational burden of some of the methods that we may apply. So, for example, here you have Line. So, for example, here you have a Miltonian Monte Carlo implemented in the R package Arstan. Here you have expectation propagation approximation using the algorithm in the article by Chopin and Lidgway. Here you have IAD sample from the unified skew normal distribution. So here we are exactly sampling from the n-variant truncated normal distribution and the p-variant one, and then we are combining. And this is at the running time of mean field variational base and our partially factorized solution. So you can clear Factorized solution. So, as you can clearly see here, these methods suffer a lot for the large P settings, as we were expecting. The unified skew normal IAD center instead suffer more from the large N, whereas mean field and partially factorized are very much scalable. So, potentially, this is the method that we would like to look at in such a high-dimensional setting, at least from a computational perspective. Now, which one of the two we want to use is clear from this result. This result. Okay, so what you have here is for some important functionals of the posterior. So you have the posterior mean of the regression parameters, the standard deviations, and the predictive probabilities for 33 LDAT units. What we do here is we compute these quantities using Monte Carlo leveraging the IID sample from the unified sphere normal posterior. So these are, say, the exact estimates, or I mean, close to them. And then we compute them, those obtained under the mean field solution. Those obtained under the mean field solution and our partially factorized one. So our partial, and then we plot them. So our partially factorized ones are the black triangles, the mean field are the gray dots. Okay. So here are some interesting results. Like for example, as you can see, a surprising one is that with our partially factorized mean field solution, we are also able to reduce the bias in the estimate of the standard deviation. So a classical criticism that some A classical criticism that sometimes is addressed to variational space is the underestimation of posterior uncertainty, whereas in our case, it seems that we're also able to remove this kind of bias, whereas this bias is apparent in the mean field solution. What is more striking in a way is what happened for the location and the predicted probabilities. So, in our case, we almost perfectly match those obtained under the exact posterior. Whereas, as you expected from theory, if you remember, mean fields tend to overstrip. mean field tend to overshrink these quantities towards zero. And this has a direct effect in overshrinking the predictive probabilities towards zero, which will yield like poor uncertainty quantification from a predictive perspective. Whereas here we can see again the theory for the partially factorized mean field solution in action because again as p goes to infinity here we're basically matching exactly the moments of the exact posterior. We also have result in terms of ascertained distance between our approximation for the posterior so the Our approximation for the posteriors of the empirical one and the exact one. So now I just want to conclude by showing you a sort of final result, which in a way unifies everything that we have seen so far. Because the question is, well, we have seen that unified skew normal are conjugate to probit regression, multinomial probit, dynamic extension. So the question is, what is the largest class of statistical models for which the unified skew normal is a Unified skew normal is a conjugate prior. And actually, the largest class of statistical models for which the unified skew normal is a conjugate prior are actually all those models was likelihood as a kernel which is equal to the one of a unified skew normal. Because in a way, unified skew normal basically are self-conjugate. And actually, this class of model is quite broad because, as you can see, so this is the likelihood. And as you can see, if I put n0 equal to 0, I go. And zero equals zero, I go back to classical Gaussian linear regression models, or even the dynamic extension, Gaussian processes, and so on, which tell us that unified skewnorma also conjugate to Gaussian linear regression. This is useful because it allows us to include skewness in our prior specification to still obtain tractable posterior inference. If I put n1 equal to zero, then I get again a probit model, multivariate probit, multinomial probit dynamic extension. If instead I let If instead I let both N1 and N0 to survive, then I also recover many other models relying on, say, this kind of partially discretized representation. A good example is Tobit regression. So in Tobit regression, we fully observe Y only if it's above a certain threshold. Otherwise, we know that it's just below it. So we set it equal to zero. This type of model induce a likelihood which is proportional to this quantity. And again, unified skew normal are quantities. And again, unified skew normal are conjugate to that. And the proof is relatively simple because, again, we just need to apply Bayes' rule, then just update the Gaussian part of the prior with the classical Gaussian-Gaussian conjugacy, and then apply the sun conjugacy when we have a likelihood that is of a CDF evaluated the linear combination of the parameter. This is kind of a unified result that potentially allows us to perform close-form Bayesian inference for a very broad class of representation. And also, I say, And also, I say, inherit the previous construction that we have seen, even in terms of approximating distribution with a partially factorized construction. So these are just some other extensions. So, I mean, one is related to a different setting, still categorical data, but for a logistic setting. And this other one instead is an attempt to better evaluate predictive probabilities when we have probit Gaussian process prior, leveraging the previous result in terms of close-frame expression of the predictive. Closed from expression of the predicted distribution. I just want to conclude. So the final take-home message is the one that I told you at the beginning. So unified skew normal distribution are actually a conjugate prior of a broad class of models, which include, again, probit regression, multinomial probit, top-bit models, dynamic extension, and so on. And this is a very interesting unified result because it brings also those form expression for the moments, which may add. Form expression for the moments, which may help also in theoretical studies. Potentially, we may be able to do IAD samples of beyond the issues of MCMC and obtain even more accurate approximation as the one that we have seen before. Of course, there is a lot to do. Like, for example, so far, I mean, I was speaking about high-dimensional settings. So, in high-dimensional setting, you may want to use shrinkage priors. In a way, we are already including shrinkage because even if I didn't mention that before, we are allowing the variance. We are allowing the variance of our Gaussian prior to decrease as p grows. So we are including a sort of increasing shrinkage, even in the theoretical part. But again, there may be better shrinkage priors, which has been developed. There actually are better shrinkage priors that have been developed in the literature, a sparsity-inducing one. So the question is, how much these results are useful when instead of using unified squinormal priors, we use shrinkage priors. Potentially, there may be something interesting in the sense that many of these. Something interesting in the sense that many of these priors can be written as scale mix or a Gaussian. So, at least condition on the scale, we inherit the unified scenery conjugacy result, but it would be interesting to see how this can be extended more. And also, the other thing is that what about using now this result in all the different models that use probit regression as a building block? Like, for example, BART for classification are typically based on a probit mapping of an additive sum of trees. Additive sum of trees. So the question is: can we use this result and a conjugacy result in order to, for example, improve inference even in part with novel close from expression? Another interesting thing, for example, is that there are some multinomial probit models which are written as sequential probit decision, which is very much related also, for example, to probit stick breaking priors. So even in like Bayesian non-parametric density regression, we may use the sun result. We may use the sound result in order, for example, to update the parameter of the mixing probabilities, which are allowed to change with covariance. So, with this, I will end up. If you're interested about these articles, you can find all the links to the archive paper in my website, and we have also quotes in the GIPTA repository in case you want to use that. And with that, I conclude, and I thank you for the attention. Thank you very much, Daniel, and for you. Okay, thank you very much for a very nice review and unification of an important class of models. Are there any questions people on Zoom? Okay, we have one question here. Okay, we have one question here. Very nice discussion, enjoyed it and learned quite a bit, Spita here. Just stupid question. How does this compare to like logistic regression and the usual variation base or polygamma, which would seem to be the obvious alternative, right? So you mean on the So, you mean on the so I mean in terms of so there are two things here. So, in terms of closed form result for logistic regression, that could be interesting as well. So, the first thing that was looking at, for example, is once I saw that the unified skill normal was conjugated, probably the question was, is there a class of distribution which is also conjugate to logistic regression? That is still unclear to me. Maybe can be obtained by looking at the broader class of skilliptical distributions. This is the, let's say, on the conjugacy part. On the conjugacy part. In terms of variational approximation, I guess that, so the interesting, so in the polya gamma data augmentation, so that would be still from the mean field class. So I guess it would be still facing the issues in I-dimension that we discussed from the from Keter Norman data augmentation for probit regression. A question could be, for example, what about now applying this partially factorized mean field class also for logistic regression? So just avoid assuming, let's say, the factor. Just avoid assuming, let's say, the factorization between the polya gamma augmented data and the beta parameter, but keep the dependence to the polyogamma augmented data and just factorize across the polya gammas. It's unclear to me how then the result, if you we will have kind of nice closed form result and nice closed form Cavi algorithm also in that case, but that could be still interesting. My guess is, however, that the classical mean field approximation with polyagama data augmentation for logistic regression may have the same. Regression may have the same, let's say, accuracy problem of the mean field for probit. And then, maybe by including this sort of partially factorized construction, we will preserve dependence of beta to the augmented data could maybe get some improvement like those that we get for the probit setting. But it's unclear to me whether the algorithm and the approximating density just for beta can be easily obtained, because in that case, we were leveraging the unified skill normal properties. Any other question or comment? Okay, apparently not. So thank you very much, Danielle, again. And it's time for a break. Thanks for joining us. And we'll be back at 11 Maxim time. Thank you. Thank you. Bye-bye. 