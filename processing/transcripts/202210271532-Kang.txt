Thank you very much. Okay, so I would first like to thank the organizer. This is a great workshop. I'm really happy. And thank you for being here. I know it's a long conference, so thank you so much for being here. So since I think everybody's tired, so I may not talk about every single slide, but I will try to give you an idea of what we're trying to do. So I think that is more important. So today what I'm going to talk about might be slightly different, but what I wanted to do is this. So what I wanted to do is this. So, and as a proposal with my witness that I had this idea a long time ago before any of this started. And then the motivation that I had was this. If you see a very interesting real phenomenon, I wanted to take a video with my camera and the camera will tell me what equation it is. This is what I wanted to do. But so I just want to have a video of one realization and see what we can get. And then, so that's the setup. So we are given a single set of time-dependent, discrete, noisy data. Discrete noisy data, and we're trying to find the underlying PDE. And I'm not going to use any machine learning, and I'm going to push as far as we can with all of the numerical techniques that we have. So that's my motivation. No machine learning, one single realization, and how far we can get. So this equation, the noise is the main thing. So there's two themes for this talk. One is how to treat noise, because I want to do a real data. A lot of the methods approaching this direction, Approaching this direction usually cannot deal with noise. Even machine learning, you have to learn noise and so on. So, noise is one aspect that I want to focus on. And another thing is how to deal with the sparsity. So, because we need to find the equation, underline the equation, I want to actually have each terms. I want to have u, ux, something like that, rather than just everything like implicitly defined. I want actual terms. So, in order to do that, it needs to have. That you need to have some dictionary. So, you have certain terms of dictionary and how to use the sparsity to find it. So, I will try to give you an outline, so just to walk you through the journey that we had to get this method. So, what I'm going to talk about is going to be very concrete and practical. I really want the data to be input in a C-wall wiki net. So, it's going to be very maybe not as elegant. Maybe not as elegant, but this is what we'll do right now. So, the first method that we tried is what is called identification of differential equation using numerical time evolution. So, moving away from that, I will try to give you the next extension, what we learned from the ident, how we improved it, and then we have some other work, but I'm going to jump to the weak ident. So, this is the most recent work. This is a joint work with. This is a joint work with Wen Jing, Wen Jing now, who's sitting right here, and then Yinje Liu, both from Georgia Tai, and also robots identity with Liu Chen He, who was my PhD student, who also worked with Hung Kai, who also gave analytical talk the other day. And then Haoliu was the postdoc, and then Wen Jing and Ying Jae as well. And this work, the week I did with my PhD student and Rachel, Rachel Kushke. Rachel, Rachel Kuski, and Rachel. Okay, so the idea is this. So, setup is that we're given a time-dependent discrete data. So, video, right? So, video, you take the video. So, you have a time-dependent discrete data, but the assumption is that all of the spatial domain, the data has to be given, and then some discrete time, the data is given. Now, we want to identify the PDE, and then the PDE is assumed to be a linear combination of linear and non-linear. Of linear and non-linear terms because I really want to get the term. So we have a rule for this because we don't want to assume anything. So the basic rule that we have is either of to second order derivative, two term multiplication, something like that. So we always have a certain rule. So of to second order, something, something, something. So since we are assuming this, given an input data, that we can make this as a matrix like this, F, U, U square, and so on and so forth. And then you consider multiplying on. And then you consider multiplying on A vector. Then you can set this equal to B, which is our UT, approximation to UT, and you can try to solve this. And naturally, the vector A is sparse. That means only certain terms are going to be activated. So this is where the sparsity comes into this. So we're going to call this to be a feature matrix. So these are feature matrix. So the identity. So the first work that we did was given the discrete data that we made. Discrete data that we made this approximation by finite difference. So we have u, given data is u, so we put it u, one, and then you have u square, given data is u, u square. But now you have ux, you have to approximate. u ux you have to approximate. So we use the finite difference to approximate this. In order to get a higher order accuracy, we did use Edo scheme. In particular, this particular case, because we are assuming of the second order derivative. Assuming of the second order derivative, so we use E05. So at least all of the featured terms should be at least second-order accurate. So after you set up this equation, we first use the L1 minimization. That was the first approach that we did. So in fact, this is just LASSO. However, when you use LASSO, the problem is you have no concrete control over which term is supposed to be zero or not. So underlying the true data may not have this term, but then the coefficient can be. But then the coefficient can be something non-zero. A lot of uncertainty. So, to improve this result, so the first one that we did is what is called time evolution error. So, from the L1 minimization that we get non-zero, any non-zero coefficients that we have, consider all the combinatorial combination of this. So, let's say you have three non-zero terms, then have one, the ut equals to this term, u t is equal to these two terms. So, two. t is equal to this two terms. So 2 times 2 times 2, so that's 8 minus 1 because all 0, we don't care. So 7, so 7 equation. Now we have a candidate of 7 equation which may be expressing this. So how do we pick one among one this 7? So we do evolution. So in fundamental idea behind the time evolution is the numerical PDE, right? The numerical PDE, when you have a convergence, that means you have a delta T. Have a convergence, that means you have a delta t. If you have a delta t, delta x gets smaller, you should converge to the original equation, right? So the fundamental idea, as I said, I want to use numerical technique as much as possible to identify it. So you evolve this seven equation with a smaller delta t. Because a given data might be the t0, t1, t2. You do it with a smaller delta t and evolve. And then see how far you get. So at the end point, if you have closest to the given data, Closest to the given data, that is the one that we're going to choose. So that's simply that. So this is what we do. So the idea behind it, as I said, is a numerical convergence, but also another reason is because it needs to be understood as a set. So it needs to be understood as a set. I'm very sorry about that. I'm not sure why. Oh, sorry, sorry. I'm very sorry about that. Okay, because you cannot say that. Are you yes or no? Are you yes or no? Yes or no, or you yes or no, or you yes or no? That would be great, but it doesn't work like that because it's a non-linear PDE. So the PDE evolved with a combination of it. So it needs to evolve as a set. So this is why we wanted to keep it as a set. So this is what we did. Now denoising. So as I said, one is sparsity and one is denoising. I'm from the variational image processing, so I have all these tricks, right? Like the total variation, like genome denoising, like you name it, like you paint it, whatever, whatever. You name it, like you pay, whatever, whatever. I should know better, but we don't know any of this. So, in this case, the denoising, we don't use any of that, but we just use a moving average leastware because we wanted to have some guarantee, the smoothness. So, the order, order of the smoothness needs to be guaranteed. So, this is the denoising that we do. Okay, so this is how it works. So, just to give you an idea, so this is a given data, this is a Berber's equation, and this has some noise, and then we denoise this by And then we denoise this by moving averagely squares. And this shows each feature matrix and the correlation with the other. Now, if you do L1, then you have 1, 2, 3 term which is non-zero. All of this, we compute 7 different equations like this. And then each activated term, you use least square fit to find the coefficient. And then you evolve. You evolve with a smaller delta t and then find the smallest time evolution error. That's going to be the correct one. Error, that's going to be the correct one. Okay, so this was the main idea. You can do this for different kinds of noise. So we increase the level of noise, and then each time 100 different realization for noise. And then the green show, this is the ratio of wrong coefficient of recovery. So as noise increase, of course, the recovery gets worse. But still, you can do, you can find something. But however, this one actually gives you a good insight on it. The recovery of this. The recovery of this is dependent on how accurate your feature matrix is. So, the feature matrix has an order of approximation that is represented here by P. But more importantly, this is dependent on underlining PDE. If the underlining PDE has higher order derivative, it is much harder to recover. So, that is represented here. So, the underlining PDE has higher order, then it is harder to recover. Is harder to recover. So, how can we do this better? So, the next thing that we did is we made three different improvements. So, one is the denoising method. We use successive denoise differential because u has u. Given data u has noise, u has noise. So, if you take the ux, the noise gets emphasized. You consider uxx, it's going to be even worse. So, it gets worse. Every time you take the derivative, it gets worse. So, that is the limitation. Worse. So that is the limitation of the finite difference. So what we did was once you get a data, we denoise it. Every time you do the differential, like the differential operator, like some sort of derivative, then we denoise it again. And then every time you have to do another approximation to derivative, we do denoising again. Surprisingly, it does work. And we do have a guarantee in a certain assumption that we can guarantee the order of recovery because you're doing differential approximation and denoising. Differential approximation and denoising, but still, there is a guarantee for that. Now, for the sparsity, the last two, because it is dependent on lambda, you have to choose different things. So, we didn't want to do that. So, we instead, we started using subspace pursue. Subspace pursue is greedy algorithm, but you have to give the number. So, let's say k is equal to 1, I want only one sparsity. Then they will give you the best result. The best result meaning lowest residual, the smallest residual. Residual, the smallest residual. Then, k is equal to 2, they will give you a lowest residual with a two-activity term. K is equal to 3, k is equal to 4, and so on. Among these choices, now we still have to pick which one because we don't know what is the underlying sparsity of the underlying data. So, from k is equal to 1, k is equal to 3, k is equal to 3, k is equal to 4, we're going to pick the one which has the smallest error. We also do the time evolution. But, time evolution, as you can imagine, is so slow, right? You have to do it with a smaller delta t and evolve. Do it with a smaller delta t and evolve all the way to the end, right? It's terrible, right? So we do better. So we do multi-shooting. So we just multi-shoot and then just compare all of the differences. But in this case that we found that subspace pursuit is more stable, that it is okay to actually do the cross-validation as well. So we both show both of them. If you do multi-shooting or if you do the cross-validation, that you can show that the result is actually comparable. And just quickly, I will show you some results. So for example, this is Some result. So, for example, this is a transport equation. The coefficient has to be negative 1. So, zero noise k cell flows. Then, 10% noise, and then you have 30% noise, and then time evolution as well as the cross-validation. Both of them, the recovery is pretty stable. So it has to be negative 1, it's a negative 0.9, and so on. The noise is compared in a different way because you can compare the residual error or you can compare the coefficient error. So, this is the number that we showed. You can do this for the higher order. You can do this for the higher order. So, like the Burger's equation with diffusion. So, you have a diffusion term, the initial identity, if you go off to the second order, it gets harder to recover. So, this was better in a way that the second order equation, but still, the recovery of the 5% noise actually can handle it. You can do it for the KDB, which has a third-order derivative. Now, we're going to go step forward on that. So, week four. So, there has been research. Weak form. So there has been recent success in using weak form for these kinds of PD identification, and we wanted to explore what we can do. So these are the methods that we're going to compare to. So this is weak Cindy. It's basically weak Cindy for PDE. Weak Cindy for OD. This is from Messenger and Word School. And we have this group of people also did the, we're just gonna call this as an RGG for this talk. RGG for this talk. They also do the weak form, and this is just the original Cindy. The original Cindy was only done for the ODE, so we're just gonna use Cindy for the ODE comparison. Weak form. So weak form. We have a feature matrix that we want, but we're going to consider the weak form. So every single term is going to be multiplied by the test function, and we're going to take an integral. This is great. Why? The one reason it is great is because The one reason it is great is because all of the derivative, which gave me trouble, right? Like, because when you have a noisy data, you take the derivative, it gets bigger, right? So now you have a test function. We can give whatever test function we want, right? Like, we can define the space. So, you, for the test function, and because of the integration by part, it moves to the test function that we already know. So, you don't have to deal with the numerical differentiation anymore, and it gets sucked into the test function. Into the test function. So, of course, one of the downsides is that you cannot do every kind of term, right? Because the ident or robust ident is free from all kinds of shape of the term, the feature term. But this has to be very particular, otherwise. So anyway, so the benefit is that you don't have to compute the derivative directly. And also, because of the numerical integration, you can use different order, higher order, different numerical approximation to get a higher order method. One thing is really interesting is this test function. Is really interesting is this test function. So, for the test function, I mean, theoretically speaking, any smooth function should do, right? So, we just thought, like, you know, something like a bell shape. So, this is most typical thing that people use. You have a center, like you have a domain that you want to take an integral, there is a center, and you sort of have like a polynomial with a bell shape. So, typical test function used in this case could be p is equal to 2 and 2. 2. Why not? Right? 2. But actually. Right, too. But actually, the group, this is the messenger Bush group who did the weak CD, they went a little bit further than that. So, what they did is that because this has to handle the denoising, so they looked at the given data's frequency response, and then from the frequency response, they decided what needs to be denoised. And then that denoising point is matched to the Gaussian of 5%. So they're not suppressing it, but it is sort of suppressing it like a Gaussian. Suppressing it like a Gaussian. So they use the similarity of the shape of this function with a Gaussian, and then that they match this with the frequency that's given to us. So it's actually quite interesting. Anyway, this works very well because we tried different things and this was quite good. So we also use that. So now our algorithm. So what do I do? So now we have an input matrix and B, like the feature matrix, as well as B, which is given as a weak form. As a weak form. Given as a weak form, and then we use sp, so subspace pursuit, for k is equal to 1, k is equal to 2, k is equal to 3, k is equal to 4. They're going to give us something. Now, every single time it is given, and then we're going to look for the minimum at the end. So all of the computed, we're going to find the minimum. During this computation, we added three different things to stabilize this process. One is normalization. Because a lot of the recovery is based on the least square fit. Based on the least square fit. So when you do a least square fit, the magnitude of the feature did matter. So we designed a normalization, which helps. And then another thing is that one other benefit of using the weak form is you're taking an integral over a domain. The domain doesn't have to be set. You can have different domain, right? So the domain can move around now. So you have a choice in domain. So we utilize that. So we try to utilize what is meaningful. To utilize what is meaningful, the more meaningful reason to do it, we have experienced different things, but there's a simple way to do it. After this is done, because we're doing the SP, we have k is equal to 1, k is equal to 2, k is equal to 3, k is equal to 4. Let's say equation, undermining equation should have only two terms. But you were asking to get 3, 4, 5. Some of the term will be very, very small, and it's contributing to UT almost negligibly. So those things. Negligibly. So, those things we just trim it. We just get rid of it. So, we have less than 5% to it, then we trim. When you consider that, you should consider more than the coefficient because coefficient might be small, but the feature could be big. So, you have to consider the combination of that. So, when we normalize it, we normalize it by the leading coefficient. This was important. So, each feature has different forms. has different forms. U, u square, ux, or something like that. But u itself has a noise. So the leading error term is going to be all different. It will be all over the place. U, u square, ux. So we normalize this so that you will give a good result for it. So this was actually crucial for that. And then this is how we choose the domain. Given all of the domain, we didn't really look for the highest variance actually, but some. Variance actually, but some sort of high dynamic is helpful. So we look, when we find this feature matrix, we look for places that, so we basically cut it in half. Your high dynamic or low dynamic. We experimented with having like a really high one, doesn't work very well. In some level, it needs to be spread out. As you can see that this yellow part, if you only take this high dynamic region, the recovery is actually not so good. The recovery is actually not so good. So, you really need to have some other region as well. So, we did take off as a half, and there's a way to do it, like just by fitting the piecewise linear fit, and then we just take, okay, whatever you look big, and then we just take those value. And it really does improve it. Because we're only using partial of the domain, not the whole domain, the matrix itself gets smaller. So, then the matrix itself gets narrower. Matrix itself gets narrower, so we do the narrow fit. And then this is the trimming that I explained already. So as sparsity increases, at some point, they realize the other terms are not important. So after about three, five, four, something like that, they all give the same result because it all gets naturally trimmed. It's if you are not important. I know you asked for 10 terms, but I don't want to give you 10 terms because other terms are not important. So they just naturally just cut it like this. So this method is. Just cut it like this. So, this method is very, very stable, and it gives a. Okay, so the result. These are the equations that we experimented with. So, this is a list of PDEs, so transport equation, KDV equation with third-order derivative, fourth-order derivative, KS equation, and then you have the isotopic force-medium equation, non-linear Schr√∂dinger equation, reaction diffusion. So, some are system or not, but we. So, some are system or not, but we can have the higher order driven. For the ODE, we also experimented with ODE. In some level, ODE doesn't have a differential term in space, so it was less sensitive, I would say, than having a higher derivative term. So, we tried with a linear system, non-linear Vanderbilt Duffling, and Malka-Volterra, and Dora system. Okay, so this is just a typical example. So, when there's no noise, for example, when there's no noise, the data is given, there are other methods who do well too. So, these three methods, all very comparable results. Now, if you start increasing the noise, then it starts to not work. They don't even find the correct term, for example. The signal-to-noise ratio that we defined was actually anchoring from the given image. So, given, I'm sorry, given data. Given data itself has a maximum. Data. Given data itself has a maximum and minimum, we take the middle, and then from that, relatively, how big it is. So, this being one is huge. It's really like a really big amount of noise, but still it is able to find the coefficient like this. So, the coefficients are always specially independent, right? Your snapshots are specially dependent, right? The coefficients are just constant, right? Yes, for now it is constant, yes. But the solution you have are special dependent. You have are special dependencies. So, so solution, but we are only fitting the whole thing, the given data, the x and t, everything we're just fitting one. So, we're only looking for the constant. In that case, you really, I mean, there's a dimension-wise, you really have over-determined data, a lot of data. That's right, you have a lot of data. But to be fair, that yes, it is true, but in our case, if the final If the final time is shorter, we can still recover it better. Because if some other method doesn't work very well, like you need to have a really long time t or you should have different initial condition, you have to do a lot of things to make it stable. But in this case, I would say even if your data is smaller amount as a total, still it was stable. So this is what I can say. But yes. That being said, the original ident paper does have. Paper does have the experiment and the model with a varying coefficient, spatially varying coefficient case, also. So we did actually consider that. Okay. Since this one, the result that I'm showing you might be just one best one, then other things fail miserably, right? So this is a statistical result for that. So this is increasing the noise level, and then we did it for a 20 experiment, and then this shows a coefficient error. And then this shows the coefficient error is lower, and then this coefficient error is high. And this is a true positive, and it's, oh, you cannot see it, but it's on top one, so one is better. And while other method has more fluctuation. This time we only compare to the weak CINDPDE because when we were doing the comparison, this was really the best one. So we compared against the best one. So this is the result that we can show. The true solution, and this is weak identity. Ident with the noise. The coefficients are very close. It's very hard to see, but all of the terms are matching up. So you can see the L2 difference between the coefficients is 0.03. So this is more statistical. So this is more general result, only comparing with the weak PD, that the red one is our weak identity. It shows that it is lower error in general, and then the true positive is higher. So this is for the ODE experiment. So, this is for the ODE example. So, the black one is the true solution. Red is what is given to us. So, the red is what is given to us. All of this red is what is given to us. From the red given data, we found the blue one. So, the green one. So, the green one is superposed to the black one, which is the true data. So, the recovery is quite simple. We also did the statistical comparison, also including Cindy. This was the time evolution in the robots ident, and then the error is much smaller. And then the error is much smaller. Even compared to the weak Cindy, this is smaller, and then the true positive rate. Of course, there are some outliers once in a while, but this is quite simple. Okay, so that's it. To summarize, we've been looking into how to identify the underlining PD from the one realization of noisy data given to us. So I would say the moral story is how to deal with the noise and then how to deal with the sparsity. And these are some of the progression in this stuff. Some of the progression in this stage. Thank you very much. Thank you very much. Yeah, two questions. One the first one is related to waste question. So for many PDs and the confusions are variable functions instead.