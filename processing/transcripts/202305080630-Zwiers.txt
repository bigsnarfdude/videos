All right, I think we should start our next session. So, our next speaker is Dr. Francis Julius, and he's going to talk about detection and attribution of human influence on Francis. Okay, thank you. Thank you very much. Thanks for coming after lunch. So, as a typo on this slide, it's the 8th of May today, not the 12th. The main reason for giving this talk, I think, is to show you some pictures. Giving this talk, I think, is to show you some pictures. There's a little bit of technical content, which I think compared to this morning, you'll find to be relatively low-tech. And I made the same mistake that Rafael has made, which is to prepare too much material for the time that's available. And so I'm going to skip very quickly over the first part and leave it to you to read the first part, which gives an explanation of what climate. Explanation of what climate scientists understand to be detection and attribution. So I won't read these definitions. They're IPCC definitions of the notion of detection, attribution, causal influences of changes on changes in the planet system. I also won't tell you much about the basic methods that are used. If I were showing you a PowerPoint, We were showing you a PowerPoint presentation rather than this PDF presentation, then I'd be able to motivate what climate scientists do by showing you a little movie. And what this movie shows on the left is the projection, the progression of changes in surface mean temperature and its spatial distribution decade by decade over a 110-year period. And on the right, what model Right, what models suggest would have happened or should have happened as a consequence of the combination of human and natural external forcing systems influences on the climate system. And then the general paradigm that we use involves what is basically a very simple regression formulation where we're asking in the observations, do we see this signal that is expected? Expected. So, this represents our understanding of the physical consequences of changes in greenhouse gas composition and so on. Climate system. We might want to scale that by some factor. There might be more than one signal in here. There might be greenhouse gas responses or anthropogenic responses as one signal, natural responses to volcanic forcing and changes in solar forcing in another vector. In another vector. And so the question is whether or not the linear combination of those vectors best matches the observations in some way that makes sense and leaving errors that are representative of the internal variability of the climate system. This is a very small sample statistical problem, at least in the way in which it's been approached over the last couple of decades in the climate sciences, because this vector on the left is considered as. The left is considered as a single observation, single realization of the evolution of change in the climate system over 110-year period. We don't impose any structure on this, on the nature of this variability, and so we need to obtain information about the nature of this variability from climate models. By running climate models many, many, many times from initial conditions under From initial conditions under control, controlled forcing scenarios, in order to learn what is the variance-covariance structure in space and time of this error vector. And it's that information that, as we saw this morning, is needed in order to estimate these coefficients and make some inferences. So, this is very elementary and very familiar to you. To you. This idea to do this first was originated by Klaus Hasselmann, whose name you might recognize as a recent Nobel Prize winner together with Suki Minabi. It's been developed over a long period of time with a couple of lines of parallel development mentioned on the right and more formally on the left. The formulation that is currently used most frequently. That is currently used most frequently looks like an errors in variables formulation, where it's recognized that these signal vectors are subject to uncertainty as well as these observations, and that the thing that, as well as these observations, the thing that is in the observations is some combination of the unknown signal vectors. And so the challenge is to simultaneously estimate these things and these things in order to obtain an estimate. In order to obtain an estimate of what is in the observations and then ask the question whether or not it's present. Okay, so I won't go over any of this, the rest of this, other than to give you some idea of the way in which people have been asking this kind of question. It's the foundation of the information that the IPCC uses to make the attribution of human. Uh, the attribution of human influence on the climate system when it's uh talking, when it's making statements such as the statement that the warming that has occurred over the past 110 years or so is unattributably in large part due to human influence on the climate system, is unequivocally due to human influence on the climate system. Okay, there's a main sense of quite early on. Quite early on in this process, starting with a paper that was led by Gabby Hegel, thinking about how to apply this paradigm or how to develop a similar paradigm for extremes. So many of these papers, most of these papers, think about how to adapt the problem so that you can then apply the standard regression type of paradigm that we've come to know and love. Simon might think that his paper doesn't. Simon might think that his paper doesn't actually belong in this classification. He's in this room here. There are a few papers, maybe Simon's included, that cast the problem more directly within the framework of extreme value theory. Okay, and it's one of these papers that we've recently published that I'm going to be talking about for this short lecture. Okay, so. Okay, so I'll start by describing how we formulate the problem using extreme value theory. And as I've indicated, it's relatively low tech compared to particularly the kind of work that Raphael has been doing on spatial extremes. So we use a technique, recently developed technique that's framed directly in extreme value theory that uses station values of the law. The station values of the log of the annual maximum one-day precipitation amount. So it's a block maximum approach, but we're taking the block maximum of the log of precipitation rather than the block maximum of precipitation itself without doing any other processing on the data. So, what I haven't had much time to explain to you is that in the process of applying this regression paradigm, people have to do People have to do a substantial amount of dimension reduction in order to be able to get the dimension of the variance-covariance matrix that is involved under control. So, here we don't do any of that. We use the station data as it's available from observing stations all over the globe. We use climate models in the method that we've developed only to estimate the expected changes in. Expected changes in the extremes of the log of one-day annual maximum precipitation amounts. We don't use the climate models to learn anything about internal variability in the model that we fit. And it's implemented in this paper by Chao Hung-sun and myself and others. So, why the log of extreme precipitation? Well, the argument goes back to thinking about thermodynamic arguments for Arguments for how extreme precipitation might intensify in a warmer climate. And so that means appealing to what's called the Clausius-Clapeyron relation, which indicates that in the absence of circulation exchange change, the moisture holding content of the atmosphere should increase by about 7%, roughly per degree C of warming. So roughly exponential relationship. And so that means that precipitation extremes, which occur in this moisture environment, should all else being equal, intensify by about 7% per degree warming. Okay, so that means there's an exponential relationship with warming. If we take the log, that makes that turns that into a linear relationship with warming. And so we might expect that the log of And so we might expect that the log of one-day annual maximum extreme precipitation should increase linearly with warming and roughly with time. And that scaling should to first order be independent of moderate spatial scale differences, so point scale versus grid box scale, for example. So we fit a GEV distribution to the log of our X1 day. RX1 day, and we do so at almost 5,100 stations spread across the globe with 65 years of using 65 years of daily data. And we fit this model using a combined score equation technique that was published in JASA in 2020 by a student of June Yang, who's a climatologist. Climatologist, a statistician who some of you know, who is located at the University of Connecticut. And so we have a time-varying, as in examples that were presented earlier today, a time-varying location parameter. It varies by time and by location. We have a spatially varying scale parameter and a spatially varying shape parameter. We think that it's okay to keep these parameters tied. Keep these parameters time invariant over the period that we're analyzing from the 1950s to about 2014 or so, because the signal in extreme precipitation is relatively small. And so we don't expect that there would actually be large changes to the scale or the shape of the distribution. We're just looking for changes in the location of the distribution. These signals. These signals are obtained from climate models. The scaling factor that adjusts these signals, there's one for each signal that is considered, and they're held constant in space and time. So inferences that we're interested in making are really about these scaling factors. Uncertainty quantification that we undertake is by a two-stage bootstrapping approach that takes into account uncertainty in the Uncertainty in these signals and then uncertainty that comes about due to noise in the observations. And there's lots of noise in the observations. So we take temporal and spatial dependence into account in the construction of the bootstrapping approach. We don't use climate models for that purpose. The parameter estimation doesn't consider spatial and temporal dependence, although this is possible with a combined This is possible with a combined score equation technique. But as I mentioned, it's accounted for in the bootstrapping scheme. So the scaling factors are estimated by minimizing the negative log likelihood function. And this is just the log of the likelihood function that's obtained by multiplying together all the Multiplying together all of the marginal distributions, so we're not taking dependence of any kind into account. We're not using any kind of fancy process model in order to represent how this location parameter varies in location and time. The notation here is just a little bit more complicated because it recognizes that there could be missing observations at any particular location over time. We do have some completeness requirements for the data required. Requirements for the data require at least 43 years of data at a particular station over the 65 years that we're considering, with a constraint on having observations available during the last five-year period. Okay, so we're fitting this model at 5,081 locations over the years for which data are available at each location, at the times of which the data are available. Times at which the data are available, and using the observed value of the annual maximum of one-day extreme precipitation each year at those locations. So, location. So, again, the little model for the location parameter is a linear combination of these signals. With this formulation, you get this rather nice-looking quantile function. This is expressed back in physical units. This is expressed back in physical units, and this lets you, assuming that you're willing to make strong inferences about these scaling factors data, and that the model fits reasonably well, this then lets you say something about how the magnitude of extreme quantiles in the upper tail of the distribution, the size of the 20-year, 50-year event, for example, have evolved over time as a consequence of including these signals in the model. Including these signals in the model. If these scaling factors all turn out to be zero, if there's no evidence that those signals are present in the data, then this number just turns out to be unity and the quantiles are constant over time, which is the typical engineering practice at the moment. So we consider one signal and two signal variants of the problem: one signal involving all forcings, two signals separating anthropogenic, the response to anthropogenic forcings from the response. Response to anthropogenic forcings from the response to natural external forcings. And for a given signal and location, we assume that this thing varies smoothly in space and in time and argue that that's reasonable for areas in which we have data. We pool data over small regions in order to be able to do a better job of estimating the scaling factor. In fact, we pull Scaling factor. In fact, we pull these RX1 Bay values from climate models from K by K grid box regions where we choose K to be three. So these are regions maybe, depending on the climate model, maybe 300 kilometers on a side, or maybe as much as 1,000 kilometers on a side. It depends on the resolution of the climate model. We pool them together for all years in this period 1950. In this period, 1950 to 2014, so in the 65-year period, and we attained the median value from all of those annual maximas, so nine times 65 annual maxima centered on a given location. That gives us kind of a climatological median one-year annual maximum precipitation amount. And then we do the same thing year by year, we calculate the ratio between the two. Calculate the ratio between the two. And so, what we would expect is if there's a signal present in this data, that the ratio should be roughly one during the middle of the period, that it should be less than one early in the period, and greater than one towards the end of the period. This is for extreme precipitation in its physical units. Okay, and so this is an example of a location in Europe. Of a location in Europe where, and we're using a climate model that had was able to produce a large ensemble of 50 members. So we had 50 realizations available in a particular location. So we not only have the 65 years of data with the nine grid points centered on each location, but we also have 50 replications of those 65 times nine sets of values of particular. Sets of values at particular location to estimate the base value. And then we have 50 replications times nine, nine locations in the grid bots each year in order to be able to estimate these ratios. And you see that during the first part of the record, those ratios are less than one, and then they become larger than one as time progresses from about the 1970s onwards, which is also when we see the very strong response. When we see the very strong response in surface mean temperature and observations, this red line is the V-spine smooth put through that data, and so that's the signal that we use in the extreme value model. The combined score equations, so scores are simply the first derivatives of the likelihood function, log likelihood function. With respect to the parameters that you're interested in. And so, if we're interested in getting a score equation for the beta parameter, then we end up with many, many scores. And we can combine those into a single equation using a weight matrix that looks like this. It's a positive definite matrix. We use the identity matrix in our study. You can use a matrix other. You can use a matrix other than the identity matrix, and that's a way of bringing spatial dependence into the problem by considering the spatial dependence amongst the scores. And so this is described in this paper in JASA that I referred to. Okay, well, what do we see in the observations? Just looking at the raw observations, this is now from a larger number of stations and a different study. We used a subset of these stations and attributes. An attribution study. All of these blue dots are locations where precipitation intensifies over time, over the 65-year period. The orange dots are where it weakens over time. The heavy dots are places where that increase is significant, just using a simple non-parametric test. The fraction of locations at which you identify significant upward increases is substantially larger than the false discovery rate. Than the false discovery rate for the test in the positive direction, which is 2.5%. The fraction of locations at which you find significant decreases is about 2%, which is entirely consistent with the false discovery rate in the opposite direction. And so, this is telling us that this very simple statistical test is operating correctly, and the upper panel is telling us that there's something going on in the data that is tending. Going on in the data that is tending to make precipitation extremes more intense over time. The figure also shows you what are positive, you know, that we have a huge paucity of data all over the globe. So some parts of the globe, the United States, for example, and Europe are very well sampled. China is very well sampled. Other parts of the globe, also South Africa, other parts of the globe may be well sampled, but meteorological services don't have the remit to share data. The remit to share data, and so we don't have access to that data. What this figure shows is for the same stations, we've simply scaled the change in extreme precipitation, expressed it relative to the change in global mean temperature that occurred during that period of time. And that number just happens to be 6.6%, which is basically the Clausius-Clapperone rate. There's no physical reason. There's no physical reason to expect that this is the right number because the number that you use for this plastic-claparone calculation is very much location and situation dependent. But it's still pleasing to see that you get this number that is suggested by theory and that it's substantially different from what you would expect by random chance. Okay, well, what do we see in models? Well, what do we see in models? So, this is from yet another paper that I've been involved in, and here we're looking at trajectory changes in 50-year one-day extreme precipitation extremes. So, Mike was involved in this paper as well. And it's showing the relative change in intensity of one-day 50-year events at a time when the climate is one degree above pre-industrial and three degrees above pre-industrial. About pre-industrial. And you see that the numbers here are more or less consistent with this 7% per degree K number over much of the globe, with much more intensification taking place, particularly in the tropical strip. And you see that more clearly when you go to greater warming. You also see some areas where extremes don't intensify and, in fact, weaken as climate warms. As climate warms, these are places where the downwelling branches of the tropical circulation strengthen, and therefore there's an invasion of additional dry air into these areas that inhibits the development of extreme precipitation events. You've seen this, so I won't show it to you again. I'll go on to some results. So, first of all, to give you some idea of this is now from the 5000. From the 5081 stations, but gridded, so you can see, give some notion of the pattern of change that is seen in the observations, the pattern of change that is seen in an ensemble of CMIP6 models using all forcings, and in an ensemble of CMIP6 models using natural forcings. And so, in an ensemble of CMIP6 models using all forcings, in these areas where we have data, we see. These areas where we have data, we see intensification. And if you smooth this out, you'll see that it corresponds pretty closely to that. So, this is a single realization. This is 46 realizations. And so we would expect the thing on the right to be a lot smoother than the thing on the left. This is what you obtain if you just take natural forcing into account. And in this case, you just see a little bit of noise fluctuating about zero. This is for Northeastern Europe or Northern Europe. And you see in red for different grid points in Northern Europe, the signals. In black, the median change in the observations in this way in which we've chosen to show fractional change in the intensity of extreme percentage. In the intensity of extreme precipitation, and you see that there's pretty good correspondence. This broadband is the range of variation that is seen at individual stations across Northern Europe. What's the difference between the left and the right? Oh, this is using the 50-member ensemble with the Canadian CAN ESM-2 model. So, this is using a CMET-5 protocol. This is using 26-member. Call. This is using 26 members from CMIP6 simulations, 46 members using a collection of climate models. So here are some inferences about these scaling factors. The solid dots are results obtained using CanECM2. The open dots are results obtained using the CMIP6 models. You see that the scaling factors for the anthropogenic forcing. For the anthropogenic forcing component, want to be a little bit less than one. The models are a little bit enthusiastic in increasing the magnitude of this ratio. The observations are telling us to reduce this a little bit. In the case of the Can ESN2 signal, we're not able to find human influence or natural influence on precipitation extremes, any evidence of it in the case of. In the case of the CMET6 simulations, we do find some evidence, but it's not particularly convincing. So in this case, the scaling factors are about a half. And so we would need to ask, well, why is there that large a discrepancy? Okay, very good. I'm just about there. So we can now make some inferences from these models. From these models, and I'll just concentrate on what we can say at the global scale. So, at the global scale, we can say that depending upon which models we're using, we would infer that about 5% of the observed change in intensity is due to human influence on the climate system. That translates into a change in a waiting time for what in the 1950s would have been a 20-year event to something like 16 or 17 years. 17 years in the last five years, the most recent five years between 2010 and 2014. So that's a substantial change in risk. You could ask what is the inferred sensitivity to warming, expressed as percent per degree Kelvin of warming, and that number turns out consistent with Clausius-Cleperman. So, these are all what a climate scientist would call attributed results. So, results that Results. So, results that are attributable to human influence on the planet system. There are certainly limitations in what we've done. So, the implementation of the statistical method is not trivial, and I'm sure that we can do much better if we represent these signals that we've been trying to estimate in a more sophisticated way. We haven't taken into account variations in spatial density, which might be an issue. We've made the assumption that change. Assumption that changes in the scale and shape parameter are insignificant, but that might not be the case. And if that's not the case, then that is affecting these signals that we're estimating and maybe providing a reason for the lack of fit that we're seeing in the sense that the scaling factor is a little bit less than the unity that we expect. We expect. Okay, and so I think that's about it. And I'll just leave you with my last photo and time for questions. Okay.