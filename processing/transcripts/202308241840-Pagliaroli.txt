And back Liar Holy. Are you here? Do you agree that we record your thought? Yeah, that's not okay. It works on diobel quantum gravity from numeric geometry. Oh, so the screen is yours. All right. Thank you very much for having me. I'll try to keep it short because it's been a long day. And I'll try to keep it to 15 minutes and five minutes for questions. So yeah, this talk is based on a paper. On a paper that I worked jointly on with Ahmed Osam and Masood Qaqali. And the idea is that we're going to consider some of the models that Masood talked about and find their critical behavior is very similar to certain models from Louisville quantum gravity. So the motivation is that it is something like as follows. So using the theory of quantum gravity, one Gravity. One wants to try to integrate or sum over manifolds and have a path integral with some matter field. Today we'll just focus on the metric portion of this sort of integral that people are still trying to define. And sort of the question is, what is DG? So usually this is interpreted as summing over degrees of freedom, geometric degrees of freedom. And in our setting, we'd like to frame that, we'd like to We'd like to sum over spectral triples. And however, if you sum over spectral triples, it's still very uni object that is hard to define. But if you consider finite versions of spectral triples, then maybe we can do something with that and recover some artifacts of a continuous theory in the limit. So just a quick reminder is that, so spectral triples consist of an algebra, a involutive concept. A involutive complex unital algebra acting by bounded operators on a Hilbert space and some self-adjoint Dirac operator D that also acts on H. And this data is required to satisfy some regularity conditions. In particular, people say that all your geometric data is stored in D because like the data of the metric is stored in D because of Kahn's distance formula and Kahn's reconstruction theorem. Construction theorem. But the idea was that, okay, well, if we want to integrate over geometric degrees of freedom in the non-continuous setting, that would be Dirac operators, but we're still dealing with very unruly integrals. So in 2015, John Baer proposed parametrizing these partition functions by the moduli space of all possible Dirac operators for a fixed algebra and Hilbert space, but where these algebra and Hilbert spaces. These algebra and Hilbert spaces were very natural choices of real finite spectral triples. In particular, they're always of a form, they're called fuzzy geometries. Our algebra is always the algebra of matrices Mn of C and M of C and our Hilbert space Mn of C tensor some Clifford module V and some matrix operator D where they still have to satisfy the actual They still have to satisfy the axioms of real finite special triples. In particular, I'll refer to type PQ fuzzy geometries, where the PQ refers to the signature of the Clifford module V. But what Barrett did was he, in the same paper, he solved these axioms to find out what possible forms these Girak operators can have. And they usually look like this. Usually look like this, or they always look like this, I should say. They are some sum of anti-commutators and commutators of skew Hermitian, the L's, and Hermitian matrices H, where tensor to some product of gamma matrices, alpha and betas, that depend on the space of spinners. But the interesting thing is that these L's and H's, the actual entries of them is free information. Is free information. They is not the axioms of the real spectral triple do not specify what they have to be. They can be anything. And so when we sum over these fuzzy spectral triples or integrate over them, we're going to integrate over all these degrees of freedom, all these entries of Fermi and skew Fermi matrices. So some quick examples are for type 1, 0. So the very simple case is we have D is just equal to the anti-commutator of H, and for type 2, Commutator of H. And for type 2, 0, we have D is equal to polyspin matrix C tensor, some Hermitian matrix H1 commutator, or anti-commutator, I mean, and some other Eisen matrices tensor, the anti-commutator of some Hermitian matrix H2. So the next step is to say, okay, well, we have these degrees of freedom. We want to integrate over them. We need to choose an appropriate probability measure. We need to choose an appropriate probability measure for d, for the entries of d. And the first starting point is to consider some polynomial function s of d and in general our dd now, which we wanted to define, where we wanted to find dg, but now we're talking about dd, is the product of Lebesgue measure on some Cartesian product of skew Hermitian and Hermitian matrices, whichever ones, however many are contained in the Dirac operator of choice. Choice and which depends on the signature of the fuzzy geometry. So, yeah, we refer to such a fuzzy geometry now that we've equipped it with a probability distribution as a Dirac ensemble since it is literally an ensemble of Dirac operators of finite ones, albeit. So in particular, for this talk, we're just going to talk about the simplest one, the type 1, 0 Dirac ensemble. So it's for these vectors triple looks like this. Fuzzy's focus triple looks like this. Our spinner space is just C. And we choose some, we choose this potential where we have a quadratic and quartic term, where T2 and T4 are some real coupling constants. And in this case, we just have D is just in terms of one Hermitian matrix H. So our Lebesgue measure is just the Lebesgue measure in the space of n by n Hermitian matrices. So we just have the The diagonals that are real and the real and imaginary parts of the off-diagonal. So, if we expand our definition of D in terms of age, we'll get this complicated looking intravil that Masoud mentioned before, where we have these multi-trace terms, which are quite difficult in general and to deal with in random matrix theory. In general, though, In general, though, when you have more matrices that D is in terms of, you'll get what are known as multi-matrix ensembles, of which almost nothing is known of these kinds at least. Very little progress is made because a lot of the techniques matrix 3 that have been developed up till now are not applicable. So, Masood talked a little bit about actually solving the model, but here I want to talk to you about a little different angle that we have. Angle that we have and how it connects to quantum gravity. So, random matrix 3 historically has been known to have connections to quantum gravity in the late 80s and early 90s. There was Witten's conjecture and Konstovich's model that was put forward. There was also around the same time connections to Louisville quantum gravity. And more recently, there's connections to JT gravity, which we'd hope to connect with our models at some point in the future. But of particular interest for this talk is just Louisville quantum gravity. For this talk is just Louisville quantum gravity. And at the time, in the 80s and 90s, physicists knew heuristically that the asymptotics of random matrix models contained certain artifacts of Louisville quantum gravity. In particular, there were certain critical exponents and critical points and asymptotic expansions that were very similar. And they had these heuristic ideas of why that should be the case. And this wasn't made more precise until much later, until the early. Until the early 2000s, I believe. But the rough idea goes something like this. So if you expand these integrals perturbatively and you look at the Feynman diagrams, in general, for Feynman diagrams for matrix integrals, what you get are either fat graphs or you get surfaces with embedded graphs, which are called maps. And they can be thought of as discretized human surfaces if you assign a unit distance to edges. Assign a unit distance to edges of these graphs. And what they found was that if the coupling constants of the model were tweaked such that the number of polygons that form maps goes to infinity, since maps can be constructed by gluing polygons edgewise in an orientation-preserving way, one would, in some sense, hopefully be counting actual continuous Riemann surfaces. And what they found was that many critical points exist in these models. Exist in these models. And in particular, we were able to show in our recent work that they exist in some Dirac ensembles. And I believe we're working on trying to prove that in general and find it in general. But we just started with a few examples. And what we were able to show is that these models have the same critical exponents and partition functions in a certain limit as models of 2D conformal field theory coupled to gravity. So, yeah, here's a sort of a graphic. So, yeah, here's a sort of a graphic of what the idea is: that you have this surface with a discrete graph on it. As you increase the number of either faces, edges, or vertices, you get sort of a denser covering. And this happens as you fine-tune the coupling constants. And hopefully, you'll get some approximation of smooth surfaces. So, here's a little more detail. So, originally proven by Tehuft is that these matrix. These matrix models have genus expansions. In particular, their partition functions have genus expansions in their log as well, where this is a sum of, like this is a formal sum of an n, g is the associated genus, and these f of g's are just functions of the coupling constant. Well, they're formal series of the coupling constant that are the generating functions of these discrete surfaces. Of these discrete surfaces, we called maps. And in particular, for these, they're called unstable maps because they're not only just regular maps that can be glued from polygons, but also maps that can be glued from more complicated one cells, such as cylinders or even more complicated objects in some cases. So yeah, this is a sum over maps, surfaces, or embedded graphs. This is the Feynman weight, which is some function of the coupling constants. Some function of the coupling constants, and automorphisms of sigma are the set or the number of ways to construct a map by gluing polygons. So the automorphisms of these maps. But in general, what's an amazing property is that these FGs are algebraic functions of the coupling constants that have either algebraic or logarithmic singularities. And they can be computed in a very recursive, streamlined way, known as topological recursion. And so, and their generating functions can be as well. And when we do this, Can be as well. And when we do this in the paper to some extent. Yeah, so in particular, each Fg is a formal series in T that is actually convergent in some disk. And you can prove this rigorously. And we're interested in the asymptotic expansion of AK as K goes to infinity. The power of T keeps track of the number of vertices in the generating function. So as K goes to infinity, the number of vertices goes to infinity. And the same would happen to the number of faces and edges. Number of faces and edges. And that is sort of the idea of this limit. I'll speed up a little bit. So for the quartic type 10 ensemble that I mentioned before, we can do the same thing. So one can show that near the, well, and we show, I should say, that near the critical points of these Fgs, which exist, they have an asymptotic expansion of the following form. So these sort of genus corrections to the partition function can be expanded near the critical. Expanded near the critical points like this. So for every g except for g is equal to one, you have some sort of algebraic singularity like this. But for g is equal to one, you have a logarithmic one. And these exponents here are known as the critical exponents, and they're very familiar to people who work with minimal models. And what you can do is you can define a formal series where you take the coefficients of this expansion, these C G's. G's and you take the coupling and you take the critical exponents. And this series, second derivative, satisfies the Payne-Lev√© equation one to all orders. This is quite remarkable. And what is interesting is that these models in the formal field theory called minimal models predict the exact same thing. They have the exact same coupling constants and they satisfy the same differential equation. And they satisfy the same differential equation. More generally, different matrix models are associated to different so-called minimal models, and their FGs also satisfy differential equations with their own critical exponents. And there's a few ways to find it. Like I mentioned, you can solve it discretely, but you can also look at the distribution of eigenvalues associated to these models. And near the edges, near the critical points, they'll start to behave with these exponents P over Q. And this P over Q is the hint to how to get to these minimum. The hint to how to get to these minimal models. So each minimal model is classified by two integers, p and q, and they're critical. And this is sort of the connection where you can check what these p and q are, and you can find the corresponding minimal model. And this was known in the 90s, but it was put a more rigorous founding in the 2000s. And we're the first to sort of apply it to these types of integrals. Those. Oh, that was my last slide. Yeah, in the future, we hope to. We also proved it for a cubic and order six model of type 1, 0, but we hope to find it in other models as well. And I believe we are in the way of, or on what the way of proving that every Dirac ensemble of at least type 1, 0, 0, 1 has such a critical point in such a correspondence. Thank you. And are there any questions? Thank you. Yes, Masutkia has a question or a comment. Do you? No. No? From Stroom? Anybody? Yes, this one. Could you just quickly comment on the techniques that you have used to prove the formulas that you have shown? Yeah, yeah. So these formulas, what you can do is you can explicitly find the FGs themselves in terms of the coupling constant. In terms of the coupling constants, using a process known as topological recursion. So you compute the loop, you write out your loop equations, you solve them to each order. But after you do the first two, what topological recursion does is it gives you a way to streamline and compute all the other higher correlation functions. And from those correlation functions, you can compute these F's. And once you have these F's explicitly, you can compute the expansion. So you actually, it's even nicer than what I've said because you actually get the whole thing. You don't just get the expansion. Thing. You don't just get the expansion. You have the actual explicit function, but we're just interested in the critical behavior of it. That's in the nutshell. Okay, thank you very much. Thank you. More questions? If not, we thank the speaker again and close our afternoon session. So thanks. Thank you. Thank you, Nathan. 