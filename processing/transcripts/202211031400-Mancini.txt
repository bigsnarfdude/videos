Thanks. So, first of all, I want to thank the organizer for giving me the opportunity to be here in Nuaca in Spanish. And yes, I'm going to talk about a work I've done with José Antonio, who everyone knows, and Stéphane Cordier, who is a professor in Orleans, too. Gustavo Daiko, who is a neurophysician. Who is a neurophysician in Barcelona? Min Benchran is now in a position in Austin, Dallas, Dallas. And Pierre is a working process also with Pierre Roux, who is a postdoc in Oxford. So we are going to talk about modeling decision-making for neuronal interaction. This is the plan of my talk. So, first of all, I consider the stochastic differential system that Stochastic differential system that Deco and his teams were considering, describing activity at the neural activity of neurons. And then we'll pass to the deterministic model, which is a two-dimensional Fokker-Planck equation, giving some results of the analysis of this equation and numerical results, which lead us to the reduction of complexity of this system. Of this system, which is based on the slow-fast behavior of the solution of this system of stochastic differential equations. We will validate it and then we'll give some results in accordance with the neurophysician experiments on a situation where we will have a three-well-like potential. And at the end, I just give you some. At the end, I'll just give you some hints on the new project we have with Pierre Rou. So, the neuroscience model has Maria-Jose Cashar has told you last Tuesday, we have some visibility problem in visual network. And this is our two examples on the Examples on the left-hand side, you have the nickel cube, which you can see has an incoming or exiting to the paper cube. And on the right-hand side, you have the ruben vase, which you can see as a blank, a white vase, or two black profile watching each other. And your vision can skip to one view to the other as you want to see one. Want to see one or the other image. But people may have problems to skip from one vision to the other. And so Nick and his team are looking to the reaction times, how many long that take to you to skip from the white vase to black profile and to performance. We want you to see the white vase and how many of you are able. Of you are able for each trial to see the white phase. This is a space-homogeneous problem, and they consider two to population of interacting neurons and just so give the differential system driving the differing rates. And they do an analysis with respect to the connectivity forces leading the interaction. Leading the interaction between these two populations, and also in terms of the bias which we can put on each neuronal population. So, the situation that we're looking for is two populations which interact in this way. So, you have W plus and W minus, which are excitatory. Excitatory interaction coefficient and Wi, which are inhibitory interaction coefficients. So you have different excitatory interaction in between the same population or in between the two population, but WI, the inhibitory connectivity, is the same for all the population, inside one population and in between the both. This leads us to a connectivity matrix, a two times two matrix. A two times two matrix, which is symmetric. As I said, you can also introduce some bias or some stimuli to the population and eventually consider a bias in one of the population. All these coefficients lead to the definition of the mean excitation of one population, x of i, which is just the sum of the stimuli we put. Of the stimuli we put on the population and a convex convincing weighted by the connectivity coefficients of the νj, which are the firing rates of each population. So, this is the stochastic differential model. Looks rather simple. Just here, the phi function is the response function to the excitation mean xi and is just a sigmoid. Just a sigmoid. And you have, they added this noise term, this white noise with standard deviation beta to get in count of the fluctuation of this many particle system. So what they do first is to consider the bifurcation diagram with respect to one of these connectivity coefficients, for instance, with respect to W plus. And it's so. And they saw that we have a pitch for supercritical type bifurcation in the BA's and unbA's case, in both cases. Here the bias delta lambda is rather big. And they decided to study the system for W plus, a critical value of W plus, which is 2.35, which enables us to have enabled, so I have always. Have enabled, so I have always three equilibrium points. One in the middle, which will be an unstable equilibrium point, and two on the side, which are two stable equilibrium points. The one in the middle, the unstable equilibrium point, they call that the spontaneous state. Your vision, when you don't ask you to see something, is an unstable point. And then it can focus. And then it can focus on vase or the profile. And they performed some moments method on this stochastic differential system. And as expected, they showed that we have a B-model solution to this system. What we have noticed is the slow-fast behavior. So here in this figure down. So, here in this figure down, you can see a trial for the stochastic differential system, a particle starting in the middle of the square and just moving randomly but fast toward this blue line, which is the slow manifold, which call that the slow manifold, and which passed through the three equilibrium points. The three equilibrium points. So we can show this first movement movement of the particle of. Yeah, I talk about particle because I'm used to talk about particle and not fibrillates, actually. But you can see this fast movement to the slow manifold and then a slow, slow, slow diffusion towards one of this equilibrium state. You can imagine so the situation as a As a system of particles moving in the double white potential, just to fix idea, because when you are not used to use fermion rates and everything, it's just easier to fix idea like particles in a double world potential. So that was the study of Deco and Martin in 2007, and we wanted to pass to the deterministic model. Model and as we can have seen during this workshop, you can pass from this stochastic differential system by mean-field limits to the Fokker-Planck equation. So here it is, our Fokker-Planck equation with some boundary condition, no flux boundary condition, which seems natural to impose, and the drift, which is given just by the deterministic part of the. Deterministic part of the stochastic differential system. What we have noticed is that this drift is not the gradient of a potential. The Schwartz conditions are not satisfied in our case. And this yields to the fact that there is no explicit steady state of this Foker-Planck equation. We have two more hypotheses. More hypothesis, and oops, what am I doing? An incoming drift f and the normalization of the initial data. So this is our starting point. And just look to the some analytical results. Before for the steady state, it is possible to show that there exists a unique positive steady state using just Freeman. Using just Freeman-Brutmann theorem. And then you get back to the evolutionary problem. And you take L2 initial data and you can show that there exists a unique solution of this Fokker-Planck equation. Just classic analysis on the bilinear form. But since the drift or the gradient of the drift is negative, we have no maximum principle. Maximum principle, and we have no direct proof of the positivity of the solution. We need to work a little bit more to get positivity. By the way, it's easy to see that we will have also the conservation of the solution, of the mass of the solution. So, in order to get positivity, we use generalized relative entropy. So, we show that We show that a certain entropy is decreasing and accurately choosing f1, f2, the function h and function g in this inequality, we can prove that if the initial data is positive, then we have a positive solution. And we also can prove the convergence in L2 norm to the steady state. Again, choosing G, F1, F2, and E in. F2 and E in a good way. But what about the rate of convergences to this equilibrium state? We have shown with Meaning Tran that this rate of convergence is exponential. So we have to prove a Francquari inequality, and this can be done by a control. By a contradiction argument, as classically do. But in this way, we have some estimation for the exponent alpha, but no information about the constancy in the exponential rate of the convergence of P towards P infinity. In this paper, we propose an alternative proof, just a sketch of the proof, some ideas. Sketch of the proof or some ideas in which we introduce an F star drift and we look to the solution psi, psi being P over P infinity of this problem. The key point in this proof is to show a conservation of C times K, K being an auxiliary function solution of an auxiliary problem. And in this way, you have also You have also this exponential decay of the convergence and information about the constant C and the exponential alpha. Okay, that was the analytical part. And then what's about numerics? Here is the plot of this two solutions for four seconds of For time equal to four seconds. And in the NBA's case, you see that the solution has converged towards these two minima of the potential and along the slow manifold I showed you before. In the BAS case, very BAS case, all the solutions are converged almost in one of the equilibrium points. But to get to this kind of picture, we have long time of computation, even if the convergence is exponential, because we have this slow movement along this low manifold. Here you have the evolution of the marginal, and so you can see that in the NBAs case, you restart from a Gaussian around the anti- One the unstable spontaneous state, and then you get to the fast diffusion, and then the two Gaussian that slowly, slowly, slowly grew up towards an equilibrium state. So we think that they have stopped to grow, but they are going, they still go on, growing, growing, and growing. So you can see that there is. Going. So you can see that there is really this slow, fast behavior for the convergence of the solution. Okay, back to this. And the same thing you can see on the other image, if I can get to this, but I can do that like that. This is in the biased case. So all the mass, all the particles just move. All the mass, all the particles just move most fastly to one of these equilibrium points and still moving, even if you don't see it. So why this is annoying to us to have this slow growth of this Gaussian. Gaussian, just because when we want to compute performance, performance is the good answer you gave when you have fixed your ideas that the vase, you really see the vase. And this is computed on the steady state. You have to reach the equilibrium, the steady state, to compute the performance. So, if you want to do that, passing or using this 2D. Or using this 2D model and waiting for our evolution problem to get to steady state or near to the steady state, we must wait a long time. And this is not used for the physician, neurophysician. So, how to reduce this complexity using this low. Using this low-fast behavior. So here is the idea. We want to use the slow-fast behavior to and we to reduce the complexity. And you have to go back to the stochastic differential system to do this. So first of all, we have to compute the Jacobian of the drift and the n values that are associated. Values that are associated to it. We just ordered them and we can introduce a small parameter epsilon, which will enable us to rescale time. With this eigenvalue, we can compute also the pass, the matrix P with its associated eigenvector, and we introduce some new variable x, thanks to this. Thanks to this change of variables. So, S0 is the coordinate are the coordinates of the spontaneous state. So, we center the new system along the manifold at the center in the unstable, the maximum, in the unstable point. And we introduce a y direction and an x direction giving this Change of variables. When you do this, you get formally a new system. This is just the deterministic part of the system. And since we introduce an epsilon in front of the derivative of x, we just think it is zero and just compute x star in such a way that h of x star and y is equal to zero and then plug it into the y. And then plug it into the y equation, you get y dot equal to a function g of x star of y and y. So just reduce it on a one-dimensional or one ODE living on the slow manifold. And we have to add noise to this equation. And since x and y are just linear combination of ν and ν. Of mu and mu2, nu1 and u2, we just rewrite the coefficient, the beta, the standard deviation, we recompute it in this new variable system. And so why we like this? So we have this ideo with noise, with the Bronyon motion, white noise if you want. You can write down the Fokker-Planck equation. Now, this Fokker-Planck equation. Equation. Now, this Faker-Planck equation is one-dimensional Faker-Planck equation. So, there is no more this problem of not having a potential to the function associated to the drift term. And so you always find this capital function g, which is which gradient is the drift term, and you always have access to the steady state. So, we know more needs. So, we no more need to wait for a long time to get to the equilibrium in order to compute performance. So, that's the idea, that's what we have done. And here is just to validate this reduction, you have in blue line the one-dimensional solution along the y-axis from zero to nine. From zero to nine and minus nine, and from zero to nine. And in black line, you have the projection of the solution we have computed before along the slow manifold along the y-axis. And in red, you have the stationary solution. And again, this is done for time equal to four seconds. And you see that we were far from far. We didn't reach the steady state in our previous computation. In our previous computation. And this is standard for Andias case on the left and for the biased case on the right. Okay. So the solution seems to be in good agreement and we can apply this method to solve our problem, to compute performance and reaction times. And only to show you that effective. Show you that effectively we still have a slow evolution of the system, even in 1D. If you just compute the solution in 1D, the evolutionary problem in 1D. Here on the left, you have every 02 seconds, a plot of the probability distribution function. And on the right, you have every 200 seconds, the same plot. You see that there is There is always a fast movement at the beginning, but the evolution is always very slow. By the way, it is a one-dimensional problem. And if you want to get rid of this time dependence, you just can use implicit time scheme. If you really want to compute the evolution of the solution and not just use the steady state. Use the steady state we obtained. So, reaction time and performance. Yeah, first we will talk about performance since I told you we needed the steady state to compute it. So, what is a performance is a good answer to what you see in the figure. And so, it's the proportion of good answers. So, it's the proportion of good answers. So, you just to compute it, you just take the steady state and integrate over, for example, the positive y. I do remember you that the two equilibrium points are one in the positive part of y and they are one in the negative part. So, you just integrate this. This red function over the y and gets your performance. And just to compare with the two-dimensional problem, we have to use an exponential regression over three points to get some notion about this density of the two-dimensional problem project. Of the two-dimensional problem projected on this or in around this equilibrium state, and we just plot the raplace, so the integral of the solution on the y, positive y, and this rho infinity we extrapolate from our data, and we see that there is a good agreement in between the two. So, still, we can use this reduction to compute this performance. What about reaction time? What about reaction time? So, reaction time, it's an exit time. As I told you, you see the base, and I told you to see the profiles. Then, how long does it take you to stabilize your vision on the other image? So, you have neurons have to exit the frequencies around one equilibrium point and get into the other. point and get into the other equilibrium point and this is just what the in probability we talk about an exit problem time and is computed just using the expectation and it's proportional to the gap to the eighth of the potential of the while of the potential in which particles evolves so no need to So, no need to have this Q infinity, this steady state to compute the reaction time, just need to know the eighth of the potential of the well in which particles are. So, we get back to DECO, and several years were passed, and it told us, yes, but my situation has changed. I'm not more looking to I'm not more looking to double well potential or unstable point in the middle and two stable points on the side, but I'm looking to a five equilibrium point situation in which you have three equilibrium stable points on the sides and one on the middle, and two unstable points in between all these equilibrium points. And also, the kind of connectivity matrix they used was rather simple, but still, you have connectivity coefficient, W plus and W i, and you write down the system in this new situation. And this is done in the MBAs and the NBA and the biased case. Now, Bias case. Now, I show you just to show you the bias 10 to minus 3. It's very, very smaller than what we had before. So we have a pitch, a super critical picture because bifurcation situation, and we study the system around the critical point in which the bifurcation change. We pass again from five equilibrium points to three. Five equilibrium points to three, and this is very sensitive to these values. You must be very careful about the values we are taking into account, but they are given by neurophysician. So, here are the potential function we get now using this reduction of the system on the left on the wall space and on the right, just a zoom in the middle point. zoom in the middle point just to see that here actually we have for some values of w plus a minima a third w also always uh consider at the stable point at the spontaneous state for the for the neurons and as w plus growth this minimum get to a flex point and eventually A flex point and eventually to a maximum. Since we are in the biased case, the potential is asymmetric and that's normal because we say particles, we say neurons that we prefer them to see the white phase, for instance. And so what we get then for the performance and reaction times. Action times, we can set this kind of behavior with respect to the variation of the bias, which were in agreement of what with these the same plot was finded by Roxy and Leidberg in some paper they in where they used and compute reaction times and performance appeared in PLAS in 208. And these are the formula they used to compute reaction time and performance. So we clearly see that they used this capital G, which is the potential we have underneath our problem. So we were fine with this. Everything was working well. And what's next? What's next? So, this is all we've done with Josie, Stefan, Tran, and myself in this kind of problem. New application. So I've told you about this problem of visual stabilization. Now, a neurophysician in Oxford has some other kind of experiments with monkeys. So they show them. So they show them for instance a blue square, and I asked Monkey to choose in between several images. Monkey, there is two possibility. Either we want monkeys to choose the shape, the good shape, so the green square, or we want the monkeys to choose the good color, so the blue triangle. So, okay, the monkey has his choice and good, say the shape, the good shape, it's the good answer, and he has his meal and he's happy. And he learns that the shape is a good answer. And eventually, he never gets an error. 80 and one after say 80% of good. Say 80% of good consecutive answers the monkey gives, and our physicians just change the rule and say, Now it's no more the shape that is the good answer, it will be the color. So monkeys are upset first and have to learn again the new rule. So, how to use this model from neuroscience bestabilization. Science destabilization problem to model this kind of situation. And we are thinking about two possibilities. The first one is to introduce or to consider four potential or firing rates for two populations, V1 and V2, and for the colour and the shapes. And write down deterministic or stochastic differential system for this. Differential system for these four equations, in which in the mean excitation term will be a little more complicated than the one we had before because we have many population of neurons interacting with each other. But then we are talking about so WS. So, W S and C will represent the connectivity action of the population of the eye of the shape population on the color population. Okay, so it seems normal to think that W S have some positive excitation in between one population, but what about the Population, what about the sign of this W, S, and C? Why it should be positive or why it should be negative? Why the action of the shape population must be enabled to see or enabled to see a color. And so, we are in in in the we are talking with this neurophysician in Oxford to try to understand. To try to understand if this kind of model will be worthful. Or another possibility is to consider the two neural populations before, and I'll just call them V C and V shape. And in order to consider this need for the monkeys to change when you change the rule to learn the new rule, just Just thinking about modify the stimuli we input into the population in time in such a way, for instance, that when monkeys began to, or neurons or particles began to get near the good, the new good equilibrium, stable equilibrium point. Equilibrium point, the well of the potential of this equilibrium point will get deeper and deeper. And the other one maybe become smaller and smaller. And while the bias received before in the asymmetry, we have seen before in the world potential change. So it's a way to So it's a way to say model the learning feature of these neurons. But still, these are just ideas and we are starting to work on it. So in conclusion, I've presented you this stochastic system and the Fokker-Planck equation associated to it. This analysis of this equation, the Fokker-Planck equation, and not this. The Fokker-Planck equation or not this stochastic system, and how to reduce your complexity in order to get to this steady state. And thank you. Good, thank you. Is there any questions? Thanks for the nice talk. You showed these reaction times or the predicted reaction times and performance compared to results from another model, I understand. Yes, I didn't show you the results of the paper of Roxin and Leg, but it just looked the same. Those experimental data? Yeah. How does this compare? The behavior of this reaction can just look the same. It agrees very well with the data. It's great. And maybe it's not a good question for you, but rather for your neuroscientists. You said that some people find it more easy or difficult to switch. Something that you can train or something that is kind of genetically something different. I think it's maybe related to some illness of your eyes. But maybe with training, some problem may be corrected. Any other questions, comments? Thank you for your talk. Can you please elaborate more on the very last slide where you say, if I'm corrected, that you are trying to reduce or at least vary the Or this varies the depth of the wells? Yeah, to change the depth of the well. I mean, when your firing rates are reached as synchronization, so things of particle, particles are inside a well, and now you want to change your point of view. You want the color, so you have these firing reads to synchronize around another frequencies, which is another well. So you have to get out. So you have to get out of this well. So maybe also browner motion, maybe at the beginning, be a higher because monkeys just are perturbed. They no more have their lunch. And so began to search the new answer. And when they get the new answer, as particles began to fall inside, they get outside again because there is branch motion, but the well becomes But the well became deeper and has the more and more particles get inside this well, the more and more particle the well will begun deeper and enable you to synchronize again on a new firing on your new frequencies. Okay, thanks. Now I'll now I go. That's the idea. After all. Thank you. Cool. Any other questions, comments? Any other questions, comments? I mean, I guess I have a question about the modeling because I'm not too used to the models. In terms of neuroscience, I mean, I imagine it would be more about patterns of neurons activations more than like population. So, here, what you're modeling is like a population of neurons are activated. Because they cannot distinguish really the population. Ah, so it's just they can see some center sort of activated. Okay, and. Okay, yeah. Any other questions about this? I think there's nothing on the chat, so I guess we'll stop for a coffee break and we'll resume in 15 minutes or yeah, quarter past three. 