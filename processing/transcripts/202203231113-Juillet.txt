About a martingale that exactly fits an infinite family of martial okay, thank you very much, Jan. Thank you for giving me the opportunity to present this work. So, Cesar's joint works with Matthias Beigelberg, so first 2016, and second or shadow marching transport paper of 2021, and another work with Matthia. Work with Matthias Martin Hussman and Martin Brukerhoff from Münster. So, in the paper with Matthias, we define shadow transports, and in the paper with Martin and Martin, we define shadow martingates. So, here is the outline of the talk. So, I will first, in this talk, I will not focus on optimal transport or martingale optimal transport. Martingale optimal transport, I let's postpone those questions to the end, but to some constructions that are based on this concept of shadow. And for that, I will maybe a little more focus on extremality. So I will just do a short reminder on extremal measures and showcase representations. And the second time, I will recall what is the peacock problem, the process cross-sample order convex. A cross-sample order convex that you may also write peacock. Then I will explain my results with and my construction with Matthias for a family of martingale transports that was starting in our first paper with the left curtain coupling and that we continued with the shadow couplings in the second paper. And I hope I will have time enough to explain you. Will have time enough to explain you how we can fit a complete family of marginals that are increasing in convex order. So I remind just of the definition of an extremal point in a vector space. So you are given some point P. If you can see P as a midpoint of two of as a midpoint of two of the points p1 and p2 where all the three points are in in a then p is not an extremal point so cisp also is not an extremal point because you can see it as a midpoint between p1 and p2 an extremal point you can find it here since if you try to see it as a midpoint of p1 and p2 p2 P1 and P2, P2 must exit the set A. So here on this picture, the set of extremal points is simply C's arc with that and also C's arc with that. So there is a famous theorem by Krain Millman that tells you that if you are finite-dimensional and say And say C is convex and compact. So here, sorry, I said A. So it is C. C is a set. So any P is a convex combination of extremal points. So I give an example here. Cis P can be written as the sum lambda 1 P1 plus Lambda 3 P3, where P1 would be. Where P1 would be here, P2 here, for instance, and P3 here. So you can draw a triangle, and you see that P with the proper coefficient will be a barycenter of the three vertices, the three edges. Okay, so in fact, So in fact, the generalization of this theorem in higher dimensional spaces, like vectorial topological spaces that are locally compact, is often called Shouquet theorem. It is part, there are other names, it is part of the so-called Shocet theory. But what I'm interested in is not to apply this Shoke theorem, but to give a name to this concept of Shoket. To this concept of Schockey representation, where in place of a convex combination, you have a generalized convex combination that is obtained as an integral. And any PA is extremal. So, during the talk, I call a Schocher representation, such a way to represent P as a barycenter where a lambda is a probability. Lambda is a probability measure. So if you are lucky, the space of parameters A is something like 0, 1, but in the showcase theory, it is an abstract space. So I give three examples. The most very most basic example is I give examples of four measures since we will be interested. Since we will be interested in spaces of probability measures, basically. So, the most basic example is to see mu as a mixture of direct masses. Since direct masses, the set of direct masses is a set of so for some X of extremal measures, you see that if You see that if you want to write delta x as a sum of two measures, you will have quickly some reflection, something to see on the support. And you will see that the support of mu0 and the support of mu1 must be x. And the only way to do that is that all the three measures are the same. The three measures are the same. So, in fact, this is very general. This is something general. Extremal measures measures have small supports. So, here we write mu as a mixture of direct masses with a probability measure that is mu itself. So, second most basic example is the Most basic example is the set of centered probability measures on air. In this case, you will also have the existence of a Schoker representation, but it will no longer be unique, as you probably know. Here you have some measure with barycenter zero, and the extremal measures are measures like that, where they are concentrated on two sets. So mu extreme. So mu extremal is some delta in P A plus some delta in one minus P minus A with weights P and one minus P in order to have the correct barycenter. And you can take this A that is non-negative, can be zero and in Non-negative can be zero, and in this case, you have the direct mass in zero. So, in this case, you can represent mu as a mixture of this type of measures, and there is no uniqueness. There is a probabilistic view on the Schoker representations. You can see, imagine you want to simulate some X with low mu. So, you first randomly pick randomly pick some parameter some parameter of the family A with respect to lambda and after you you simulate X with respect to the conditional law that will be PA okay so so now martingale measures that is closer to what it we are interested in in Martingale optimal transport interesting in Martingale optimal transport. In this case, in the space of Martingale's laws, so what I call a Martingale measure or Martingale distribution is the law of a martingale. The most basic martingale is a discrete martingale, a real valued and indexed on two times. And here, see the low, is a probability measures on R2, simply. simply and it will look like that you will have triples x y minus y plus and your probability measure will be concentrated on the two pairs x y minus x y plus so mu is extremal if mu is something like Is something like delta in xy minus with some weight plus delta xy plus. So the directed pairs are materialized by those two roots, transport roots. And here the weight is something like y plus minus x over y plus minus y minus. And you do. minus y minus and you do the same here x minus y minus y plus minus y minus so this type of transport plans appear in different works for instance in works by Obson and Klimek and Norgilas my work with Matthias Beigelberg also in a recent work by Margheriti and Jordan Margheriti and Jordan. So it's more or less a universal shape, a universal measure that you find in very much a lot of things also in the score code embedding problem. So now something interesting. It's a result of Jacques and Proter, sorry, Jacod and Yo of 1977 that is making a bridge with stochastic analysis. Making a bridge with stochastic analysis. So there is a property called PRP that I define now, and this property is equivalent to something related to being extremal. So and it is the second serum. So a martingale satisfies the so-called predictable representative property if and only if any adapted martingale with respect to the filtration, the natural filtration for XT can be represented by a stochastic. Represented by a stochastic integral. So you can write y at in c is y for any t and sigma is adapted. So this is not a property that is true for any martingale. The most famous one is the Brownian machine and it was also proved that the compensated Poisson process has this property. Okay, so now what Jacquard and your proved is that Your proved that the martingale law of X is extremal in the set of Martingale measures. Here it is a more complicated set since it is no longer a measure on R2. It is basically a measure on CAD-like spaces, sorry, CAD-like passes. So you have such a measure is extremal if and only if the martingale satisfies the predictable representation property. So if you put So if you put a showcase theory together with C's theorem, you can see that it is possible to represent any martingale and to decompose it, so to say, as a mixture of measures of processes that are martingales with the PRP. But nobody knows how to do that, except maybe in an abstract way. So what's So what we obtained in the paper with the two martins is something very close, but different and easier, of course. It is to, if you have a peacock that is a family of one-dimensional measures that are increasing in convex order, we associate a certain martingale, as we will see. And these martingales come together with a given Shoker representation. Shocker representation where all the parts of this representation have the PRP since they are extremal. So just a reminder about convex order. As you know, we will be on air here, so I write it for error. If you have two probability measures, they are called in convex order, so you must be able to integrate the first moments if and only if. Moments, if and only if, for every convex function, the integral of the first with respect to the first measure is smaller than the integral with the second measure. As you probably know, the expectation, the very centers of the two measures are the same. And so to say, the variance, the dispersion of the second is greater than the one of the first one. Just one remark in this talk, I will consider measures with the same mass. measures with the same mass and basically the mass could be will be often alpha smaller than one so um another result that i would like to present you uh uh next to um jacod and your results is a shocker representation that was uh used by by kelleraire in an abstract manner using Abstract manner using Shoket theory. He was considering the set of all measures that are greater than mu in convex order. So I show you how you can characterize them. Here, you have a certain measure mu, that is a probability measure, and you want to find measures that are greater. The nu that are greater in convex order. And in particular, you would like to see some of them with a small support, so small support that they are extremal, and so that they are extremal with respect to the exact definition of extremality. So the result of Keller is that you can do some kind of projection to a certain set. Let's just call it T. T and you are transporting the mass in the following manner. You go straight if you can, and you will reach T. Okay, so here you can, but when you can't, But when you can't, you go as close as possible to T, so you move the less you can imagine, to the closest point on the right and on the left. And here you recognize again this kind of triple that we had before with x, y minus, and y plus. So this type of measure can be called a new t. And the Shoker representations tell you. representations tell you that any nu is an integral with respect to let's say a of nu t a where you integrate the parameter a so you move the shape of your set t that is included in air and you project in this way so the name that i that we give to to this operation is a color air dilation Dilation. So this is one way to move to some measure nu t that is greater in convex order. So now since you know how to transport the mass of mu to the mass of those measures here, you are also able to transport the measure mu to nu. So you will use one of the diagrams. You will use one of the dilations indexed by A, where you pick randomly the parameter A with respect to lambda. So, doing this, in fact, Keller, we're constructing a martingale between mu and nu that is greater in convex order. So, just to remind you, Strassen theorem, that is very close to the discussion I just have, add. So, if you have a mu Add so if you have uh mu and nu in the very uh the most basic uh uh setting so you take two measures and you have equivalence between mu is smaller than nu in convex order and there exists a martingale x1 x2 that is a martingale with the law of x1 being mu and the law of x The law of x nu of two being nu, okay? So this is just what we did. We did it in a way that in our case, you can in fact write it like x1u and x2u, where u would be a random variable variable in the In the parameter set big R and for if you fix U and it equals A, so it is obtained, it is a transport obtained through a certain caloria dilation. Dilation. So, this is one way, not very explicit, to produce a martingale between mu and mu. So, now with part two, we will be going to be more explicit since we enter in the class of martingale optimal transport. And I will first recall you again for motivation, the most general martingale optimal transport where you have an infinity no, sorry, so An infinity. No, sorry, so I will. So I'm a little too quick. So I recall you what is a peacock. So a peacock is an increasing family for the convex order. If S is smaller than T, mu S is smaller than mu T in the convex order. And what is the peacock problem? You have a peacock, your family, and you want to exactly fit it in a martingale way. So you want to make your martingale flow through. Make your martingale flow through the marginals exactly. So notice that if you have a mu t with t in one and two a peacock, Strassen implies there exists a solution. So because the most basic question is, does it exist? basic question is does it exist does it exist such a martingale so i will answer by a positive manner it is just point two here if you have a general muti keler proved that there exists a way to interpolate by a martingale to exactly attach a martingale to source marginals but it is not explicit okay and he proved it uh he proved and so i mean He proved, and so maybe I write it as a remark for two. So, Keller proved there exists XT that is fitting exactly the family, the peacock, plus the fact that it is, it can be chosen. It can be chosen Markov. And it is a little spectacular since until nowadays it's not known if you can do it for marginals that are RD valued measures. So if you for it is known for real valued martingales, for real martingas, but not in general. So it is an open question that I find quite fascinating. I find quite fascinating. So, actually, my presentation for this slide was to say there are different directions for the pickup problem. So, you can find to be explicit. For instance, you have the problem of fake Bronian motion with a very recent interesting paper by Beigelberg, Palmer, Charoma, and Lothar in the other way. And also rich tradition. Also, a rich tradition. And here you can also, but it is for a very precise and given family of marginals. So, of course, here 2T, if I don't do mistake, it is that. So, it is very precise, and you try to find plenty of solutions, like in the book of Irsh Profitar and Etior for another example of the peacock. So, Keller tried to be very So Keller tried to be very general. He is not assuming anything for the peacock, but he has only an existence result. And a very excellent work is the one by Lover, where the result is at the same time explicit and for any measure mu t. So even more, in the result of Lovther, the martingale is Markov. So in the paper with Martin Hoosman, Martin Brukman. Martin Hussman, Martin Bruckerhof. We do something similar. We want to do something of the type 3, that is 1 plus 2, being at the same time similar and quite specific, say plenty of properties for the way we are interpolating our peacock. So we have not really results concerning Markov. So the Martingale is not directly Markov, but Markov, but we have something interesting with respect to extremality and Schoke representation. So now we start with a family of two indices. And so as you know, I am calling Martingel transport any distribution law of a Martingale X1, X2. So that's not new. I can call it Martingel coupling, Martingel transport, or Martingel transport. Martin L transport or martingale transport plan always it will be a measure on O2 and CD the notation for the martingales the martingale transport plans with the first marginal mu and second marginal mu so the martingale problem is a problem that came from financial mathematical financial equations with the With the robust finance in particular. But here I present it as a very crude problem of optimal transport, where you simply want to minimize a certain cost with respect to the joint laws, the transport plans, with marginal mu and nu, and the novelty with respect to the traditional Monch-Contour of Each project. The traditional Monch counter of each problem is that you are asking the transport plan to be a martingale transport plan. And you have the existence of minimizer if you measure, if your cost C is lower semi-continuous. Uniqueness depends sometimes on the assumption you have on mu, but sometimes not, like for the left-curtain coupling that I will tell you after. And after you can have results on the shape and on the combinatorics of the combinatorics of the routes that are used during the transport. And a very recent topic is the one of stability. I invite you, for instance, to listen to the talk by Benjamin Jourdain on Friday about his work with Beigelberg, Margaretti, and Pamer. So they are interesting results. So the result So the result, first result that we had with Matthias is the following. If you are in the setting, if you consider a measure, a transport plan, given two marginals, it is the same time that it is optimal with respect to a certain cost and that you have a certain condition, so this type of geometric condition. And if you and the third assumption. And the third assumption is that it is exactly the left curtain coupling that I will define. And that is the first instance of a shadow coupling. So let me explain the name of why is it called left curtain transport or left curtain coupling. So in fact, we have an analogy with light and we are closing a certain curtain from the left on the first line. On the first line, so we have a measure and we close a certain curtain. So we stop at the quantile of order alpha, for instance. This curtain is projecting a certain shadow. So the notations later will be mu alpha, and the shadow projected will be denoted by nu alpha, and it will be the shadow. Alpha and it will be the shadow of mu alpha in mu and we do the two together and so we obtain a definition for the left curtain coupling the left curtain transport. So if now you decide not to close the curtain from the left but in another way you obtain a plenty of possibility and infinitely many possibilities and for each parametrization of the curtain on the left. Of the curtain on the top, you obtain another shadow coupling, another shadow martingale transport. That is what I will explain you later. But first, let me recall you what is a shadow. We have here a measure basically of mass alpha strictly smaller than one. And you want to embed it in a martingale way through a dilation going left and right. Going left and right below the measure nu. So, one way to do that is to, for instance, to use a very bright dilation. So, I put some of the mass on the right, some of the on the left, and as you see, at the same time, this measure, let's call it eta, will be greater in convex order to nu. order to nu and it will be smaller than nu in in the which means that the density with respect to nu is smaller than one okay so you can do it in this way but the most effective way uh or let's say the most expensive way is to do something that looks like that where you move as a l as as list As least as possible. Okay, so you move very few, not so much. And this is the definition that we have for shadow. The set that we have here, the minimum for the convex order, it is called shadow. So it is called the shadow of gamma in nu and denotes. n denoted by s eta gamma okay after his work there were plenty of generalization not plenty but one work by newts stebeg and tan is defining generalized shadow and in the work with martin and martin we define so sorry they are defining obstructed shadow and we Defining obstructed shadow and we define generalized obstructed shadow. So, since here we have only time one and time two, but we are interested eventually to deal with infinitely many marginals. So, just a picture for measures that are uniform. So, I am closing the curtain, and here you recognize. And here you recognize a certain parameter U that is supposed to be to be uniform on 0, 1. So it is kind of an order of quantile parameter. So when you move alpha and you see u and x here are coupled in a way that for small u you are on the left of x and for big u you are on the right. Again, mu is a low. Again, mu is the law of x and mu will be the law of y. So you are doing that and you change your parameter alpha. Doing that, you are closing your curtain and the shadow is becoming broader and broader. You have some monotonicity for the shadow. Okay, so in fact, here you have a certain family mu alpha where alpha is in zero. where alpha is in 0, 1, and you are the corresponding family S nu of mu alpha defined through the shadow where alpha is in 01. So it's not enough to define a coupling, to define a transport plan, but in fact, as you will see, it is, if you do slices like here, you see that you have different colors. If I do, I go to the infinitime level, here I will have, for instance, the Here I will have, for instance, the quantile of order for x of order alpha, and here I will have the quantile for x for alpha plus d alpha. And the red part is going to the red part. And at the infinitesimal level, you see that the mass that is concentrated here, infiniti misally, is going here and here. And you recover the time of triple that we had. The time of triple that we had in the Shokay representation at the beginning. So, and you go further, and there is a unique way to do that, since here you have always only one point, that is the quantile, and here you have two points. You will have any measures, there will be an a unique coupling, and there is a unique way to couple the the two in a way uh that respects the sh the family of shadows. The family of shadows. So, now what about other parametrization? So, we call it sunset, we call it left curtain, we call it middle curtain. I show you how it looks like. So, here, sorry, here U is independent from X. Here you go from the middle and you go to the right just using a calibrated. In fact, here you. Cilibrated. In fact, here you are using Schoker representations for measures that are centered like this measure. And you do it in the unique way that is so that the intervals are included in each other. So you have three parametrizations, but you may consider any other parametrization. And you have again the same game. So now you have this mu. Again, so now you have this mu alpha that is going to be the law of x for u smaller than alpha. Here you have the shadow that is going to be the law of y for u smaller than alpha. And after you will differentiate, and here you have a certain measure that is going to be d mu alpha over d alpha for a certain alpha. d alpha for a certain alpha equals one, that is also the law of x provided u equals a. And you do the same here. You have another measure concentrated on two points that should be the law of y provided u equals a. So if you are able to couple x and y in a martingale way, you glue them together and you integrate over the parameter A and you get a coupling. And you get a coupling. So, this is what we did. Again, a picture now for another shape. Here, the measure is concentrated on two points, and here it is concentrated on four points. And here, we are lucky because again, there is a unique martingale way to martingale transport the measure on the top to the measure on the on the On the button, so it is not only chance. In fact, there is a theorem behind that. So that is telling you that it is related to the Kalera dilation that I presented you before. So in fact, here you have a recover, a reminder that you have a certain set T. The set T is where there is still mass, where mass should must should still be still be transported for parameters greater than alpha. So here you have a certain T alpha and you project in this way. So I give the picture. In fact, there is a here you have let's say the measure mu zero eight in In green, you have new 08 here on the on the border of the picture here you will have the the derivative so it is d mu alpha over d alpha at alpha equals zero eight and uh And here and here and so on, you will have also the derivative, but you will see that there is a unique way to transport the one on the other one, and it will be exactly the calorie dilation where you project to the set for the places where there is still mass to be transported. And so, with this observation concerning the derivative with respect to the, so to say, pseudo. The so to say pseudo order of quantile parameter alpha, we could prove that there is a unique way martingale that is transporting the probability the law of x provided u equals alpha to the law of y provided u equals alpha. And if you paste everything together, you obtained a new way to understand these Kelere. Is the colour air showcase representation by calerair so says a shadow transport have also features concerning optimal transport problems and also in interpretation with respect to the score code embedding. I can go quick okay so here in green you have the law of u and x. U and X that is the that is your parametrization of mu. As you can see, if I'm projecting this measure on the right, I obtain X, mu, sorry, that is supposed to be the law of X. And here it is the segment 0, 1, and lambda is. And lambda is a Lebesgue measure, it is the law of U. So, here in my example, X is also simply the uniform measure on 01, exactly on the animation that we did before. Now, we are considering a Brownian motion that is going only in a vertical movement, that is moving only vertical. Movement that is moving only vertically, and we proved that our shadow coupling, whatever the parametrization here is, can be obtained through a bar here. That is, if you are hitting the bar here or here, you will get a certain position and you will, with this position that is random, obtain the law of nu. So here, so sorry, again, you have ux. Again, you have ux, and since x is a certain b0 for Bronze motion, here you have a possible b toe, here you have another possible b toe, and the law of b toe will be nu. Observe also that this barrier is exactly as one wants. I mean that it is it corresponds. That it corresponds exactly to the sets T of the Klar direction of before. So before we were projecting on a set T and now the set T is here and when time is while time is Time is while time is evolu evoluting we can see set sorry since set is going to be smaller and smaller and smaller something like that and here also on the other picture it will be smaller and smaller so this is a picture corresponding to a measure mu that picture corresponding to a measure mu that is uniform 0, 1, a measure ν that is uniform on minus 1, 2, and a parametrization that is a left curtain parametrization. With the two same measures mu and nu, you can consider the middle curtain parametrization. Sorry. Here you have the middle curtain parametrization. And again, you will find a barrier that permits you to couple at the same time u, x. At the same time, u x and y, where u is the middle curtain parametrization. And again, for a measure that is uniform 0, 1 and a measure that is uniform on minus 1, 2, here you can recognize that u is independent from x, which corresponds to what I call sunset parametrization, and you have again a barrier. So we are now going to part four, how to generate. To part four, how to generalize this type of construction to an infinite family of marginals, like in this case or in this case. So the basic principle is that you are given two parameters, the parametrization parameters that we can call the pseudo-quantile order. Nicola, can I ask a question? Yes. Yes. Let's try to understand the last picture. So, if I was going to do a scorecode depending on the last picture, am I doing the sort of original construction of scorecode, i.e. I'm drawing an independent uniform and then I get two points exit from an interval? Or am I doing a root solution where I'm actually drawing a barrier? And so I don't draw an independent cube, it's just that the barrier itself couples the Barrier itself couples the network version. I'm sorry, I think I didn't understand. I can speak a little, and you tell me if part of my answer is part of your question. In fact, so it is a new solution, so to say. We proved it in two ways, in the way that I tell you just using the CIS showcase representation and the story with shadows, and we proved it in the other way based on more stochastic analytic. More stochastic analytic arguments, and um, okay, so the starting position is the here in green. You have the load. Rephrase my question: If you were to do this now, in you know, realize this as a scorecode embedding, are you using an independent uniform or you are just drawing shapes and then hitting the shapes? So, okay, here I wrote something about independence. Okay, so but here it is not, or maybe here it is not independent. Sorry, here it's not independent since u equals x. Okay, maybe I try the other way. Okay, here. Imagine that here you have the derivative, so you have alpha equals zero seven. Okay, so it corresponds on the picture to zero seven. And now here you have a certain set. I try to make it big. I try to make it big here. This is set T so I put the set T here and I start from some position here or here or here. So here or here and here. And I will hit the bar here, here and here. So this corresponds to the fact that all the To the fact that all the mass is going here and here, here and here for parameter alpha equals zero seven. So now if I change alpha, everything will change and so you will recover in fact the exact same shape of the barrier that we had before. Barrier that we had before. So the barrier is something like that. So at the beginning, when the level is, let's say, alpha equals 0, 2, you will simply embed with identity. So during the movement, the vertical movement, you stay on the same place. Okay. All right, thank you. Okay, if you want to, we can do a Skype another. Do a Skype another time. I hope I gave part of the answer. Okay, so now, how can we generalize it to infinitely many marginals? So you have this picture, as you can see, we are given two families. One family is the parametric. One family. One family is a parametrization. So here you have, for instance, mu alpha on the top. Okay, so it is a parametrization of a mu1 that is in fact the mu zero for the peacock. So this time is the time of the peacock and you are increasing for the convex order in this sense. In this sense. And in this direction, you are increasing for the usual order. So a measure or a shadow is included in another measure, and so on. So it is a parametrization with the so-called curtains that you are closing, for instance. And here you are given another family that is the peacock itself. So here you have for instance new tea. Ut okay, and you are increasing for the convex order, as I said. So, our trick is to fill the complete 2D picture with generalized shadow that we can denote by mu T alpha of mass alpha. Alpha and that are obtained as iterated shadows. So, this is a concept of obstruct shadows. So, you can do it quite easily if time is discrete, but if time is continuous, you have to optimize or to take the supremum over all partitions over the convex order. So, this is the Over the convex order. So, this is the way we define the so-called generalized obstructed shadows. So, we have this type of things that we want to be the law of, sorry, not the law, the probability that XT is in some set, provided U is smaller than alpha. So, okay, and you have to multiply by alpha in order to have a measure of mass alpha. To have a measure of mass alpha. So it is the law. So, with that, so it's not so easy to define it. After we differentiate with respect to alpha, and we obtain another peacock that is at the differentiate level with respect to the pseudo-quantile parameter alpha. So, there is a previous work by Knutz Stebeen. Stebe sorry, maybe sorry, and tan. So they do it for N, so we generalize it for L plus. They do it for the left curtain parametrization, and we do it for any parametrization. And they have existence, and we prove existence plus uniqueness. Existence plus uniqueness. And as you see, so it is a complete solution to the peacock problem, and it is even it does not depend on the data muti, and it is even a family of complete solutions since for any parametrization at the beginning, you have one solution. So, for N, what I just said is really true. For R plus, you have to plus you have to the parametrization has to be what we call convex of convex parametrization and the parametrization that we see we have seen left curtain right curtain middle curtain sunset curtain have this property so now i have a few slides i think i know i have still time i think so what uh now a statement of this paper see if you have muti the if muti is a peacock and you have a fixed parametrization Peacock, and you have a fixed parametrization. So, this parametrization is basically the law of u of n x0 that you want. There exists a unique martingale constructed through the shadow method, so it is very rough statement. A little more precise, there exists a unique martingale law, Xt, such that if you consider the probability that Xt is in something and the parameter is smaller than alpha, then it is a shadow. Then it is a shadow. So this is our generalized shadow that we denote by mu, sorry, mu zero. The notation in the paper is the following. So mu alpha. So this is the measure of mass alpha. And you kind of iterate over all possible partition shadows in order to obtain, in fact, the smallest measure for the convex order that. Measure for the convex order that could be the law of a martingale starting in distributed as mu alpha. So this has to be made precise since mu alpha is a measure of mass alpha that does have not to be the mu restricted to some sets, but thanks to the parameter mu u we can do that. So now about again about the Shocker presentation. Again, about the Schocke representation of the beginning. In one step of the proof, we have to recognize that for every R in 0, 1, the family of differentiate shadow are extremal elements of a certain set. So it is a little more complicated. Here I am speaking about the sunset parametrization where it is easy to formulate in this way. The in Ciswell. So we are not considering all peacock, but only the peacock starting in mu zero. So if you consider all peacocks starting in mu zero, this is very close to what I explained you about Keller. He was considering, remind, the set of all nu such that mu is smaller than nu in convex order. So this is very close to that. So we consider C set of peacock. And in fact, we see that all differentiated family of A family of shadows with respect to the parameter alpha is made of extremal elements. If in one case, if the index set is N, this is correct for any A, and if the index set is L plus, this is correct for almost every A. So for such an extremal peacock, there is a unique attached martingale, and moreover, this martingale is Markov. So, so to say, we have So to say, we have constructed, we can see mu t as the integral of some between zero and one of some derivatives. Also maybe it will not be correct if I write it like that. So for every t you we write it like that. And now we put a martingale that is fitting it. Moreover, it is a Moreover, it is a unique and Markov, and we can integrate over the parameter to get a marching L that is fitting mutin. So it is quite interesting with respect to Jacodier theorem and the fact that I have to write it. So the measure that we obtain, the law with respect to the martingales for alpha fixed, are extremely For alpha fixed, are extremal measures and they are Markov. So, not only the peacock are extremal, but also the martingales. So, maybe I have just so many, there are quite a lot of difficulties. So, for uniqueness in the case T positive, we have to use a certain weak transport problem. They are uncountable indices, so as I told you, so it's not so easy to define the shadows. So easy to define the shadows. I told you about the convex parametrizations that we need if the index set of our family is L. So in fact, it is a little more involved. Okay, so we need this concept of a structure generalized shadow. And here another remark. As you remember, when you differentiate shadows, you obtain calorie dilations. This will still be true with obstructed shadows, okay? Still true. Okay, still true for obstructed shadows. I mean mu t for t in n. So you can define obstructed or generalized KLR dilations that corresponds to the derivatives. But for T real. T real is not completely correct for any time. We had to find another way. So it's no longer correct. We have counter-example where it can be shown that the family, the peacock that we are creating, when you differentiate it, it's not a family of composed calendar dilations. Okay, maybe a statement about optimality. Maybe this is one. Maybe this is one. So it is a paper with the two martins. So if you have a peacock, mu0 is parametrized in a certain way. We assume that we have a cost with a cross derivative of this type that is negative. Then the shadow martingale U X T attached to the peacock and the parametrization is a unique martingale law in the set of possible solutions that minimize. That minimize the expectation of a cost that is uniquely considering the correlation structure of U and XT. And for every T. So for every T, it will be a minimizer. And if it is a minimizer for every T, in this case, it must be the shadow martingale attached to the peacock and the parametrization. So, I think I'm good in time. So, I would like to thank you for your attention. Nicola, we have time for questions. Good. Alex? So, am I right, Nicola, that all of this is in one dimension? this is in one dimension basically. Is there any hope for any of these constructions to work in higher dimensions or generalizations of that? So I hope so but the first problem is that you have to define a proper shadow. So this concept of shadow you can find it without the name in older works for instance by Rost, but you would have to prove But you would have to prove an essential feature for it. For instance, probably associativity. So until now, I'm not able to say that. I hope to be able at some time in the future. So I mean, there's no obvious reason why these shouldn't extend to higher dimension then? Yes. Yes, so it's not directly doable, but it's not excluded that it could be done. So there is the fact that in fact, if you so the basic fact is that if you look at the to the shadow of Dirac in 0, 0 and you want to embed it in the sum of the Lebesgue measure on 0, 1 time. On 0, 1 times 0 plus the Lebesgue measure on 0 times 0, 1. So I remember of this example, but why I am telling you that? Okay, so there is no minimum, there is no mean in the set, a forum, forum, the convex order of the set. Of measures that are greater than mu and smaller than nu. Since you could, for instance, consider c's measure and you can compare it with c's measure. No one of the two is smaller in convex order. You can also consider in yellow something like that. So the yellow one is not small. So the yellow one is not smaller than the red one and also not smaller than the green one in convex order. So it's not obvious. So you cannot do it with this kind of a shadow. Okay. Do you have any other questions here online, maybe? Can you repeat the definition of the generalized obstructed shadow? Okay, perfect. Good that you ask. So good that you ask, Marcel. So there are different equivalents. So I didn't write it. So I will create. So basically, so you can take the shadow of a certain measure of mass alpha in, let's say, mu1. So you can iterate. So, you can iterate. So, this is something you can always do. Let's say it's a definition. I don't remember in your paper if it is like that, but it is probably equivalent for discrete time. So, you take the shadow, so on. The shadow of the shadow of the shadow. Okay. It will be also the smallest, smallest. Let's say eta n such that so smaller smallest sorry for the convex order also also such that new uh new sorry new k 4 k equals 1 and so on is a peacock. So it's a peak. is a peacock. So it's a peacock of mass alpha. Mass alpha with sorry nu zero is mu alpha. So you start like that. And moreover, nu k is smaller than mu k. Okay, so that is something you know. And now the other way is, in fact, you have the two. In fact, you have the two same things, but you use one trick that I used also in the paper with Charles Bubel, that is a trick of partitions. And you take the supremum for the convex order over all part all n and all partitions t n of Of the shadow for the family between zero and t of mu alpha, it will be the supremum of the shadow that we defined just before. The mu t1 and so on, mu t n. Sorry, not the yes, the supremum. So if you add the new measures, you will not observe. New measures, you will obstruct more and it will increase. Okay, and it's in convex order for the convex order. Okay, and it is also the minimum for the convex order with respect to this kind of definition. Cool, thank you. You're welcome. Any other questions? Let's thank Nicola and all the speakers today.