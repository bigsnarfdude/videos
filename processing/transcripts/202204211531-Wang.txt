equation uh z d so this this following equation probably have seen probably seen this before with epsilon delta uh let's say parameter is in 0, 1 and Laplacian, let's just take the usual lattice, Laplacian nearest neighbor, and V is a family of IID random variables indexed by Zd, let's say with bounded nice distributions. So, compared to most of the talks, Compared to most of the talks in this workshop, the additional term is this term. As you see, a proper treatment of this nonlinear guide will need to make good use of deep results from spectral analysis. And actually, some of these results are provided by some of the participants in this work. So, we start from the linear equations. We start from the linear equation, so this is the familiar linear equation. And let's let H, so this is the H, be the random Schrodinger operator. Then we have two cases. First of all, you have arbitrary D. So you have arbitrary dimension D. If, for this problem, if epsilon is small, then H has underson localization. Okay? And for now, let's just. And for now, let's just state Anderson localization, and it's proven first in this generality by Friedrich Spencer and later by Isa Morchanov with those different methods. And what do I mean by Anderson localization? So let's say that includes the following three properties. I would qualitative. First of all, each eigenfunction phij is localized about some point Lj and has rapid J and has rapid exponential decay away from it. And property 2, let's say given any initial condition u0, a wave packet, a linear combination of eigenfunctions, localized about the origin, and let muj be the eigenvalues for the eigenfunction phij, then ut is localized about origin for all time. Number three, we should notice is the solutions are almost periodic in time with the frequencies, the eigenvalues, uj. So let's just record these three properties. And now in d equals to 1, we may take epsilon equal to 1, right? In fact, any finite epsilon, there is Anderson localization and property. Anderson localization at property 1, 2, 3 hold. The Lyapunov exponent is strictly positive, so this follows from the work of Würstenberg, and then there's also Gothscheid, Wochanov, Pascu in the continuous case, and Kohn-Suyad again on the lattice. And there are many other works, three dots, and then you see some of the more recent results as the talk goes on. Long time ago. Long time ago. Now we turn to the nonlinear equation. So I add this term, delta U to P U. It is natural to ask what happens to the dynamics, right? Remember property 1, 2, 3. So you have solutions which are time almost periodic. If the initial weight packet is localized about origin, it localized around origin for all time. Localize around origin for all time. Okay, now you want to add this non-linear term and you say that what will happen to its dynamics? And obviously we cannot make such statements in such generalities in the linear case. But we'll present certain solutions then. So for example, one may ask, are there still solutions which are localizing space and almost periodic or quasi-periodic? Or quasi-periodic in time. Quasi-periodic just means that you have a finite number of frequencies instead of infinite. So what we are asking is, does this non-linear equation have solutions which are localizing space and quasi-periodic in time? Okay, so this is kind of a first step in the nonlinear generalization of the dynamics. So in the linear situation, So, in the linear situation, were there situations where it was quasi-periodic instead of being almost periodic? Yeah, if you take a finite number of frequencies, finite linear combinations, then it's quasi-periodic. Okay, right? We will devote the rest of the talk to this question. So, it's this question. So, we continue the study by its dynamics, by the dynamics of the Downliny equation. The dynamics of the Downliny equation. We will devote the rest of the talk to this question, and if there is time, we will also mention more general types of solutions and related problems. Okay, we look for quasi-periodic in time and localizing state solutions to the non-linear random Schrodinger equation. And we consider two cases. A is cases. A is arbitrary d, small epsilon and small delta. Okay, so high disorder small nonlinear term. And the case v is dimension 1, epsilon equal to 1, and delta is always small. So the nonlinear term is always small. We first discuss case A, so we are in arbitrary dimensional and high. We are in arbitrary dimension and high disorder, and we consider epsilon Laplacian U and delta U to pu as perturbations of the equation where epsilon equal to delta equal to zero, right? That's a natural perturbation point. So we start from this linear equation, where the v j's are iid random variables. This is the natural starting point, arbitrary radar functions. The solutions, the above The solutions, the above equations, are of this form, right? We all know this. With appropriate conditions, we have to impose some conditions on Aj as j equals to this. So below we'll specialize to aj not equal to zero for a finite but arbitrary number of j's. So this is my initial quasi-periodic Dinner solution. And we did not. Linear solution. And we denote this number by V. Okay, just number of frequencies in the linear superposition by V. We look for solutions to the nonlinear equation near U0, right? Because we are making a small nonlinear perturbation. We are asking, are there still solutions close to my initial wave packet, U0? And we treat both epsilon delta U and delta U to P U as perturbations, right? You all agree with me, Steve. Right? You all agree with these things. Both epsilon, delta, small. So, as an answer, we look for solutions to the non-linear equation. In this form, I'll explain the notation. Bear with me. Okay, so we look for a solution of this form. Delta J is a Dirac delta functions. And the later, I should explain this. Where N, okay, we have B frequencies, right? So therefore, you have. Right? So, therefore, you have a B vector, B-dimensional vector, frequencies, and then N is the vector, integer vector in B-dimensional. Okay, so I'll explain what it is. So, note that in Ut, a priori, this N may be greater than one. Then one. I'll explain the next slide. And these are the higher harmonics of the basic frequencies. And this is the main difference with the linear solutions. The reason is because if you take u, for example, of the form exponential i x, and u cube, the cubic And u cube, the cubic u cube will give you exponential 3i n, right? So obviously n will be 3. I'm sorry, it's a stupid question. What is p? Is any p? P B P in the power. Ah, this is P is arbitrary. Yeah, you can think P equals to 1. Okay. P is arbitrary, it doesn't matter. So I'm just explaining that for the because of this term, Because of this term, it will bring higher harmonics. Because you multiply two exponentials, exponential ix times exponential ix will give you exponential 2ix. So you have to be coherent. A solution generally will have this form. So now let us explain the notation in the outsides using the linear solution. So linear solution. Using the linear solution. So, linear solution is this, right? I think all agree. And the non-linear solution, my outsides is this. Okay? Now, we can write v equals to v1 v2v as a v vector, obviously. Then, v1, for example, can be written at v1 equals to v dot e1, right? Okay, so u0 may be written in this form. Let me compare this to this you see Because of the multiplicative denominator, you have to be consistent. So this is in fact one of the main difference between linear equation and non-linear equations is the generation of high amounts. Point number one. So, from this point of view, you can see the linear solution u0 is localizing space, right? And there are only the base harmonics n such that n is 1. Putting it in words, our theorem with BOGA will say that there is an abundance of such solutions UT. Abundance of such solutions ut to the nonlinear equation as well, and moreover, they are close to usable. So, I gave an answer, I said I want to look for solutions this floor, and can you find such a solution? The answer is yes. So, more precisely, so let me explain what we can get. What we can get. So, this is the theorem. So, let me explain what this theorem says. So, you have your lattice, and I have singled out V1, V2, V lattice, V potentials. So, it's here. And outside, outside, I assume, so I have outside. So I have outside BC belongs to what I call Anderson localization space, which I'll explain. So outside, so this is kind of a Gadunkum. So you think here I have these potentials, and outside this box, I have Anderson localization. Bear with me, what I mean, really. And I fit such a good potential realization outside of this box. And you know that for the linear equation you have the superposition of these eigenfunctions is a solution. Now I turn on the nonlinear term and I fix such a good potential realization. The theorem says indeed there's a large set with these random variables v1, v2, vb such that there's a nonlinear solution close to the linear. There is a non-linear solution, a close to the linear solution. There are questions? Yeah. Yeah. So in this, considering the operator's perturbation of the operator with only a potential supply sign delta, is this. Is there any relationship between delta and epsilon? Or so so do you can can you do this uniformly in both variables at the same time? Both variables at the same time? I assume etc delta small. They're both perturbations. Yeah, yeah, but one doesn't need to be much smaller than the other. No, they're just perturbations. For this arbitrary dimension case. So you're perturbing about this I d dt u equals to du plus the perturbation epsilon delta plus delta u to pu. So this is the approximation. There's no relation between. They do not need to conspire to anything. Just perturbation. So the idea is you single out B potentials outside, you fix a good potential realization so that there's Anderson localization outside of this box. So now the answer is by tuning these V random potentials. Random potentials, you have a non-linear solution to the non-linear random Schrodinger equation to this. Outside, you fix. Fix a good potential realization. Close in the sense that the frequencies are close to the original linear frequency. And you remember the coefficients are going to zero rapidly outside of this box. So it's a way really not a major motivation. Yeah, it's more careful. Do you mean you mean from here? So what's the question? Can you repeat the question, Constanta? Do you need regularity from a variables there? Do you need regularity from a variables there? I mean I have a very nice potential distribution, yeah. Right, but the question is, sir or in that one, I'm trying to be as nice to myself as possible. As nice to myself as possible. Okay, so it's prepared. I'm trying to be as nice to myself as possible. Yeah. Hello, okay? I cannot go into the details of the proof, but I try to distill the main points of the proof. Okay? Because main points. Main point in the proof is that it's a perturbation of a diagonal matrix index by Zd, right? So it's this equation. Number two is that eigenfunctions are the Dirac delta functions on vertices of Zd which form a canonical basis for L to Z D. So we know absolutely everything. This is what we are perturbing from. Number three, the point is parameters. 3 the point is parameters of order 1, namely V1, V2, VP, D cell. All order 1 parameters. Right? V1, V2, V3. Order one. So this is like if you think of linear problems, the parameters are typically of all of order one. Because I have epsilon delta in here. This is fine. But yeah, the parameters are all of order one. Parameters of order one. And the eigenvalues of the upper term linear operator IID random variables. So you can make them do whatever you want. I harmonics, you can do whatever. So four points and and for the so you And so you for this non-linear case with this small perturbation, you have precisely the same picture as without perturbation. Yeah. Precisely the same. What? What? I mean, these three points, the localization, about the initial condition, and about... Yeah, for this type of initial data. Okay, exactly. And for the linear operator, so now I explain what I mean by the status of localization space outside of. Localization space outside of this box. It just means that we use Anderson localization at the level of Eigen estimates. For experts, people have to all know what it means is that you give yourself an E, you ask what's the probability that your spectrum is close to E. The jargon, okay, back next. It just you fix for yourself an energy E, and you ask what's the problem. And he asks what's the probability of having an eigenvalue close to it. Okay, so the estimate says that the probability of this spectrum to be very close to E, I mean E is E, is small. I don't want to write out all the estimates. Voice is small. Or more generally, for any pair of disjoint finite sets, lambda 1, lambda 2, the probability of the spectral Of the spectral distance of these two boxes as Moi Small. Okay, it's the same argument. It's a conditional probability thing, right? Because you see, you have IID random variables. So you have this and this. These are independent people. Right? Okay, so what I mean by Anderson localization outside of this box is just at this level. So to understand the last two points, remember I said there are a few point main points in the proof, to understand the last two points, let us substitute the Alzheimer's into the non-linear random Schrodinger equation. So let me write The outside is u t equal to sum a mj e i m dot omega t delta j well delta j just delta j and the equation is i d d t u equals to minus epsilon Aplacier plus V U plus U U plus U to the P U. You substitute U T into this and you get this matrix equation. And you see that for the diagonal part, right? I d d dt will bring you down n dot omega and then V j will And then Vj will just be Vj. And the others are small, right? That's not delta. So to begin with, my frequencies are by IID random variables. So to leading order, the linearized operator is diagonal. At this point, I forget about epsilon delta to begin the perturbation theory. And the diagonal entries are these people. The omegas are just The omegas are just V1, V2, VB, and then Vj are the other VJs. But these are IID rubber. To solve the non-linear equation, one should invert the linearized operator, right? Say using a Newton scheme. And this can be done by bounding zero away from zero, no. Away from zero, no, bounding D away from zero using independence, right? Just so the sorry. The previous slides, these have I and D random variables. Okay, and then I said that we need to have a Wagner estimate. Why? Because for later iterations, Because for later iterations you need to incorporate epsilon delta into the main part. So then you need i eigenvalues. This is where value x will be considered. And that's that's actually all we need. Daginary and independence. So, and proof is a multi-scale iteration using also Gata estimates and semantic algebraic geometry. These are techniques that were initiated in the work by Bogart Gothstein Schlag and Bogart's orange book. Green functions. Sorry? It's called green functions something. Right, green functions. Pressing in the words depressed. And they have become basic to the subject. Basic to the subject. So, for the random problem, is what I explained: the ingredients. Aside from that, we feed into this machinery, which is consistent primary and 700 games. This is arbitrary dimension. Here, I want to make a remark: when epsilon is small and delta is small. Is small and delta is small, specializing to d equals to 1, there is moreover linear stability. Dynamical systems and diagonalizing systems. So this is the work by Chu-Ye Zhao, who's talking very mundane. In one dimension, for extra small, delta small, you can go a bit further. Three takeaways. Let's examine what we used for the arbitrary dimension case. Three takeaways. Number one, the parameters V1, V2, Vb are order one, right? This is the usual case. Number two, this is actually crucial. Varying V1, V2, VB does not change the eigenfunction basis for H0 equals to V. h0 equals to v, which are the derived delta functions delta j. It's remarkable, isn't it? You can vary your parameter, but the eigenfunctions, they're frozen. I mean, they're just sitting there. This is what we use. Point number three is that perturbation theory begins. Is that perturbation theory begins from the basis provided by the Dirac delta functions delta J, which we know absolutely everything about? So now I want to proceed to non-linear. Non-linear random shrotting in one dimension, and we take epsilon equal to one, in fact, any finite epsilon. So it is classical following Furstenberg's theorem on positive Lyapunov exponent for products of random SL2R matrices that the 1D random Schrodinger operator, this guy, has Anderson localization for all epsilon. Thanks for the followers. Okay, so it follows the work by Kohn Suya, Goshai Mochada of Pastu, which I papers I mentioned earlier, and more recently, I think in the last two, three years, so there are Puka, Dharmanik, Fiba, Jacques, actually. Sorry? The last author in seven of her paper is Jacques. Jacques? Oh yeah, sorry, sorry. Yeah, yeah, yeah, it's Jar, yeah. Tommy yeah. Yeah. I I apologize and I think I think my feeling of the participants is Edgita Mieskaya too I think gave a talk this morning and Exo and Koreski Kanetzi sorry I believe past three four years because the the more recent results are more refined So this is the lydia situation. So actually, ever since the work with Boga, which appeared in 2008, many people have asked me the natural questions. Is epsilon small indispensable? It's a very natural question, right? And number two, can one do it for whole fixed epsilon? Want to do it for all fixed epsilon in d equals to one, right? The linear situation such that it was for epsilon, so how about the non-linear problem? What's more epsilon? Currently, I draw. People ask me many times. So below we specialize to d equals to 1 and set epsilon equal to 1, which 1 may as it is not a parameter in the problem, just. So we consider the 1d discrete number. The 1D discrete nonlinear random trending equations already. Now we consider this equation one dimension. The main difficulties. So let us look at the main difficulties when epsilon is not small. Some of them may not be purely technical. When delta equals zero, When delta equals to zero, the system is linear. By the spectral theorem, one may diagonalize the self-adjoint operator leading to eigenvalue eigenfunction pairs, right? mu i phi i i u z. The eigenfunctions phi i form a basis for L to z. Okay, so this is nice. But But this is in some sense a bit abstract. As one does not or cannot know precisely a mu i, phi i does anyone know how they look like? Okay. Even when delta is replaced by x dot delta with epsilon very small. Even for large disorder, you cannot know them very precisely. This follows from the work of Elga. Folllows from the work of L Guy Klein, eigen system approach. Don't know much. So one knows that there is a basis, but one does not know how it looks like precisely. Any more experts here than I on this problem, right? Study the Apple exposure very soon. You don't know how the actual functions. Okay, so this is a very So this is very much unlike, for example, the second order operator on the circle, which one may diagonalize using the exponentials using eigenvalues n square and eigenfunctions exponential minus i and x and in z. Right? Okay, so relative to h prime, one may therefore be tempted to say One may therefore be tempted to say that H is not integrable, okay? At least it is not integrable in the same set. These are not the direct delta functions or the exponentials in the just Okay, so novelty or difficulty number one is how can one do nonlinear perturbation about this system that one does not or cannot know precisely. Does not or cannot know precisely. The related second difficulty is that one does not or cannot know precisely how the muI phi vary at the potential speed variable. We don't know. So circumvent this, you agree, right? There's a real obstacle. You agreed, right? There's a real obstacle. If we want to try to use the random potentials and parameters, that would be a problem. To circumvent this, one is almost forced, so to speak, to use the amplitudes as parameters for the non-day problem. And this is also natural, right? If you think, okay, amplitude should also you have other parameters, but there is the small But there is the small parameter delta. I forgot delta here. This is a mixed thing. So, but there is a small parameter delta in front, right? The A's will come here. But there's a small parameter delta. Okay? So this only happens in nonlinear problems, right? For linear problems, you're not going to, well, maybe you could, but the parameters don't come with the delta vector prefect. Delta factor pre-factor. So you put all of this together, you have novelty or difficulty number two, which is how to use a small whole delta parameter to do such a non-linear perturbation theory. So we have two difficulties. Number one, we don't know very well the system we're perturbing from. Number two, we only have a very weak parameter to to tune to avoid small devices. Tune to avoid small devices, if you like. So, how do you do this? So, below is joint work with the one-tie view, which addresses dimensional one. Okay, so always takes two slides, but let me explain this result drawing pictures. But look, drawing pictures. But not drawing pictures of it. Of a black hole. Okay. So the problem here you see we have we froze B points and we work outside B points. So here what you do instead of B points we take B balls right like this We have B balls okay they're sufficiently large Sufficiently large away from the original, and sufficiently separated. If you think of Anderson localization, this is the likely generalization of these singletons. And what's important here is the separation distance only depends on the localization length and independent of delta. Then, so in this case, we say that there exists a set. We say that there exists a set in the Anderson localization space. Now we take all the potentials. There, you see, we singled out what's outside of the box. Here we take the whole potential space, and I'll tell you what I mean by Anderson Correlation space here. So now it's again the Gadunkel statement. Statement, you imagine that you fix a good potential, which is in the subspace of Anderson localization space. It's of large measure. And if you take any B eigenfunctions which have their localization centers in these boxes, so you take an eigenfunction like this, eigenfunction like this. Like this, I go much like this. Okay? And then the statement says, okay, this is the solution to the linear equation. Then you take the linear superposition that there's a large set of Fourier coefficients, the pre-factor, such that The prefactor such that there's a non-linear solution near this linear solution, exponentially localized like this, okay, and moreover localize about the base frequencies. So this picture is the yeah, same picture, except you have gotten rid of large desolate. So we have exactly the analog. So we can do it in one dimension. We do not need the small epsilon. Here, the theorem is not empty. There are quantitative lower bounds of the number of eigenfunctions. Bounds are the number of eigenfunctions localized in each box, right? Nobody objected, it could be empty. Remark two is that the method of the proof is not a priori limited to one dimension. It's just that input is only valid for one dimension. Mainly first and berry. The method is not the mistake. So, so, yeah. So, I gave you some ideas of the proof. So, you say that when you're saying Forsenberg is limited to your input to one dimension, you mean deposit couple of one? That you have such fine proportion localization for ε for that. Otherwise, the whole machinery is not that much. So, if, for example, So if if for example somebody gives you that input two is true, yes, then it's dimension piece. It's a strip. Absolutely the workout. I believe. Oh, oh, oh, oh, oh, I was pointing at you. Sorry. Sorry, sorry, sorry, sorry. Okay, yeah. Okay. Are there more questions? So, idea number one of the proof, the potential V is suitably fixed, right? And the reason is because we do not know how the eigenfunctions or eigenvalues vary if we vary the potentials. We have no choice. Don't need to do weight list. So, we may, so once we fix it, of course, we can just make a change of basis from the derived delta for. Basis from the derived delta functions of the eigenfunction basis. Yeah, that's no problem. It has to be fixed. And we seek solutions. This is like what I wrote earlier. So instead of the direct delta functions, you just have the eigenfunctions. But it's fixed, it's not the problem. And then you have the higher harmonics, which is a feature of the non-repeat coefficient. So using the above answer as a non-linear random Schrodinger equation will lead to this non-linear matrix equation, right? For me, I did not write out the non-linear term, but diagonal is like this. N dot omega. Remember, d dt will bring you down n dot omega, right? mu j is uh the Muj is coming from the random shorting quantum muj. So this is what you have. And two, all the delta, delta is the small parameter in front of nonlinear term. Omega k are just the eigenvalues of the random Schrodinger equation. We set, let me call the diagonal D. I thought this diagonal part of this. The entries of the diagonal. The entries of the diagonal matrix D are certain linear combinations of eigenvalues of the random Schrodinger H, right? To begin with. This is the main difference with when H is just V, the largest order case when epsilon is small, which consists of I ID random variables. And you can do whatever you want, but here this is much more difficult because you have Much more difficult because you have eigenvalued. The spectrum of D consists of the harmonics of the frequencies of the random Schrodinger equation, right? So it's just not IID random variable you can play with, just like this. So this is what I mean. You need some deep results on spectral paths. So we start from small scales, huh? We start from small scales. As I explained, it's a multi-scale analysis. So what do I mean by small? Small scale just means that the Fourier frequencies are less than delta minus 1. This is a battery scale. Delta is the non-precedent. Much smaller. Idea number two is that to solve the equation, it suffices one to bound D away from zero. This is like what I said earlier. Use a Newton's key U. Use a Newton speed, you want to bound the diagonal away from zero. And number two is new. It says that the spectrum of D has a clustering property at small scales. You remember, the spec D consists of the harmonics of the linear problem. We require that it has a clustering property. I'll explain what it is. This is actually a main point. This is new and it was not needed in the work with Volga. So, this is the main spectral input that we need, that the higher harmonics have a clustering property. So, the clustering property, what is it? The clustering property is small scales. What is it? Small scales. What is it? Essentially, it is the following. Let d and d prime be two diagonal entries, two harmonics. If d minus d prime is greater than delta, then d minus d prime is greater than square root of delta. So in other words, the d's have this property, spectral clustering property. Okay, so these. Okay, so D is L dot mu j well the harmonics of the decay problem. We require that it has this cluster problem. So where does the one may think of the integers which have this property, right? If two integers are not equal, then they are of distance at least one. It's a trivial statement, but it's true. Note that this property extends to algebraic numbers, for example to the square roots. So the linear combination of algebraic numbers, if it's not zero, then they're far apart. Is the root delta crucial or you get different results? It doesn't matter. I mean, you should see this figuratively, just any power less than one. Yeah, right. Figurative. Because delta is not in the perturbation, so this is the measurement. So anything bigger than delta is big. Okay. What do you mean by saying probabilistic clustering number? Yeah, yeah. No, I'm just going to get to that. Okay. No, this property extends to algebraic numbers, for example. It just, it's not only true just for integers. Integers are trivially algebraic algebraic numbers. Algebraic numbers just means the solutions to to algebraic equations. Where roots have this property. We roots up this property. Here, the clustering property is however probabilistic. Well, I have, Lambda have this infinite family of random potentials. So, probability is there. So, I want to say that this clustering property, the intuitions actually came from number theory. It's just it's not accidental. So the big reference for algebraic numbers, this geo-functing property, that much time, actually is by Wolfgang Schmidt, I think for people who work in this. So, so let me explain probabilistic Let's take uh oh probabilistic just means actually that you see the these are eigenvalues, right, the higher harmonics of the random Schrodinger equation. So there's probability there. probability there. So probabilistic just means we prove this statement. Okay, so is there firstly your model not to algebraic numbers like this? No, no, no, but the intuition came from number theory. Why do you suddenly raise this property? You know, it doesn't come from out of the blue, right? It came from that. Okay? But here we establish using the fact that the Using the fact that the spectrum of D depends on random variables. So we try to recreate algebraic number properties, essentially, using probability. So why do we need it? Because we only have small O delta parameters. Have small O delta parameters from the amplitudes A, right? I think you agree with me. We cannot use these random potentials friendly, okay? We cannot move them. So we cannot use them. And we can only use the amplitude, but unfortunately there's an O delta parameter. This is why we need it. It is just too weak to control the small divisors. Clustering property creates, you see, I drew here many gaps, right? In the gap, I have no problems. It's invertible. So it greatly reduces the number of resonances a priori. We may then localize to spectral intervals of size O delta, right? It comes to Jean's question, right? If it's greater than delta, it's greater than square root of delta. So all of these are size. Of delta. So all of these are size delta. Okay. Then you maybe you agree with me that maybe in that case the whole delta parameter does no series, they cancel out, okay? This is essentially what happened. So what you do is, you see, in the work with Zhao, we work with the whole spectrum. But here, well, these are gaps. So we can localize it. So we can localize with each such delta interval that the old delta parameter literally cancels out. Okay, it's slightly more sophisticated than that, but it's nothing that's what's going on. So we a priori we reduce the number of resonances by this spectral clustering property, by establishing it. So how do we prove it, right? Okay, eigenvalue spacing is essential. Minami is again, it's first, it's jargon, yeah, five minutes. Well, I'll tell eigenvalues. So what we need is the linear spectrum has eigenvalue spacing. Okay, so minami, so this is D, the spectrum of the The spectrum of the higher harmonics has the clustering property, but the harmonics were derived from the linear Schrodinger equation, and the Linear Schrodinger equation has eigenvalue species. Now, the point is that it is not reasonable to ask the higher harmonics of spacing. Not reasonable. What is reasonable? What is reasonable is that they have clustering properties and that's enough, believe me. This is what you could ask. So, coming from the linear equation, the linear spectral property has eigenvalue spacing and the higher harmonics has spectral cluster property. This is what we need. Diagonal also plays a role, but you know, this is A role, but you know, this is. And property C is uniform property of Anderson localization. So we have this property of the large set. In the style of the Rio Gitomeskaya last segment, and I think probably there are many other references, things, but forgive me out. But anyway, there are many doubt out. Okay. So what are the state basic Basically, you're saying that if you pick eigenvectors with separated supports with high probabilities, the corresponding eigenvalues will have cost reproduction. Yeah. Exactly. Exactly. Exactly. Certain higher harmonics, not all, can do it. Remark the probabilistic estimates here are rather delicate. In a way, we use the random potentials to recreate. The potentials to recreate, I would say, algebraic number properties. And I think it is spectral clustering is always nice, right? From geometric analysis. What I want to say is natural from numbers. Another point I want to stress: we need clustering. Stress, we need clustering only at small scales. Only at small scales, which makes the approach robust. Because at large scales, clustering is not expected. Because when you go to large scale, the whole spectrum is going to be filled. So here I'm just a small scale. Later, you know, all of these guys can be within this cannabis. Okay? Okay? It cannot be true. In any case, for life scale, it's not expected and it's not needed, so it's perfect, right? Here. Can't have it, we don't need it. Instead, at life scales, one may join the analysis in my work with Jean, with Bogart. So in a way, one could see that the work with Bogart, or more maybe generally more the, let's say, orange book, for instance. Orange book, Prince University of Press, chapter 1920 kind of addresses more than the asymptotic range of the analysis. So each time what you need is you need to tweak a bit at small-scale morphism. So it brings in these results linear analysis of the okay. Okay, so lastly the set X on which the theorem holds, it's a good set. It's the set on which that this Anderson localization, no, this is the full force of the Anderson localization, no? Minami, Wagner, and Q in the form property. It's really the full force. So it's all the deep results that the participants in this workshop approved, and we are just okay. And in additional And in addition, there is the clustering property. So this is where, this is the probability t-shirt on which we can produce non-linear dynamics, which are continuation. I think one minute I just say That in one dimension, the results are not initial when epsilon and delta are small, they are more complete. In particular, you can take more general initial conditions, like L2 or this is H1, if you like. I mean, these are much more general. And we know that in that case, you can also ask, how do they evolve in time? You see, the main results I explained are rather special, right? Okay, so you can ask, how do you evaluate evolution evolving? Okay, so you can ask how do they evolve in time? Namely, do these wave packets spread with time T? So there are results in this direction. They indicate that if they spread, the diffusion is norm anomalously slow. So I think this kind of also uh connects with what Mira was talking about. With what Vira was talking about. It's slow. So there's a kind of a in the random case, in one dimension, you have a somehow similar or related to giving out. Questions? I wonder about the nature of the non-linearity since that I think unless you have also something like focusing doesn't matter what delta is small. And so, yeah, but I wonder, that's actually my point. So I wonder if there's something where you expect, for example, this great packet dynamics that you would see that it's some sort of focusing on how to preserve the localization by the process. Yeah, I know what you mean, but for small For small delta, does it make a difference? No, I understand what you're saying, because I guess it's perturbative regime, it doesn't make a difference. Right, right. Because even you don't see the non-linearity. But do you think that is an effective thing that's not because this is discrete, it's a bit different from continuous. Yeah, so that's why I was thinking this is a factor in that setting model. For example, this is different. For example, this, it's different. It's kind of a really odd diagonal. Without epsilon, the system is integral. Right? It's really multiplication. I think it is. And also, as you were asking me, I do not know how to formulate this problem to continues. I don't know if it's true. I don't know about that. I don't know. I don't know about that. More questions? Let us ask waving again. About so when you said that that uh I didn't get uh my results for absolutely not a miss one to ask but what's possible in the continuous. the context that you you have really considered epsilon already big for any smaller problems about the market  Look, we should add that to energy level as I energy rate. It still has to say property and that I need that that would be my That that would be my my question how to find the box on the right side will be uh Fay, are you here? Here? Yes, I'm here, yes. Hi, hi. So, very good. Let us start. So, our next speaker today is Fei Su, who will tell us about spatially quasi-periodic solutions to the generalized KDV equation. Okay, thank you. Can you see you scarce for me? Yes. Yes, yes. Hi everyone, it's a great option for me to be here in our talk.