Okay, so thanks everyone for being here, actually. Thanks for the invitation. It's a great pleasure for me to be here and in person. And yeah, so I'm going to present some work with Dan Moss, who's a PhD student in Oxford, and that follows some work we had done before. And so I'll start with sort of the general idea on why you might want to do. General idea on why you might want to do cut posteriors for some imparametric inference, and then I'll go into the sort of more deeply into the application model that we have considered, which is which were the driving actually reason why we went into that, which is the semi-parametric hidden mark of models. And I'll go on. So imagine you have a semi-parametric inference problem. So the data yl will be the data from the beginning to the end. And as you might imagine, Imagine I'm going to look at assembly so something really yeah so um and so theta is going to be the parameter of interest or will be like finite dimensional typically that is and eta is going to be infinite dimensional in this slide at least uh so you have a semi-parameter that's a semi-parametric model with some likelihood you do by is an approach With some likelihood, you do Bayesian approach, which means you put a prior on theta and eta, and then you recover the poster distribution. So that's just notational things. So if you're interested in theta in particular, for instance, in a Cox model, you're interested in the parameters for the regression parameters, whatever, the mean, if you're interested in the mean for some model. If you're not careful about the prior that you put on eta. That you put on eta, you might be uh, you might have an inference on theta that's not so well-behaved, and I'll go on by later on by what I mean by well-behaved. And typically, one way to be good for theta is to be under-smoothing the distribution on eta. So, you have to be, in a sense, not so good on eta to be good for theta. It's not always the case, but it's often the case. Or at least we have some enough counter examples to show that you need to be careful. Examples to show that you need to be careful. So, that's sort of one of the reasons that started to make me interested in using cut posteriors for semi-parametric inference. So, what do I mean by being good for theta? When it's possible, is typically a semi-parametric Bernstein-Fan Mises result, which means that the marginal distribution on theta is going to concentrate at the rate one over square root of n, and more precisely, actually, is going to have the sort of the usual. The sort of the usual asymptotic distribution. In other words, square root of n theta minus theta hat for some theta hat is going to the posterior distribution of this object here is going to converge to a normal distribution with mean zero and variance V naught under a frequentist model in a sense assuming that there is a true theta naught and a true eta naught in your data, generating your data. And more importantly, what you also want is that the centering point that you have here The centering point that you have here is going to have a frequency distribution, asymptotic distribution, which is the same as the asymptotic posterior. And this sort of similarity between the two behavior implies that credible regions will be confidence regions. And so you have some kind of robustness with respect to the prior in your approach. And you can interpret frequency credible regions by the other way around. You can interpret the Bayesian measures of uncertainty as frequentist measures of uncertainty. Uncertainty as frequentist measures of uncertainty. The problem is that, so as I said before, we have quite a few examples in the literature now that shows that in order to achieve something like that for this posterior distribution on theta, the marginal posterial distribution on theta, you often have to undersmoose the estimation on eta. In other words, for instance, if you were to put a prior on eta as a function, you put a prior on eta like you. eta is a function you put a prior on eta like you expand it on the basis you would need to make sure that you have a lot of coefficients in in this basic basin coefficient so you're assuming that eta is not so smooth in your prior and so by doing that you're not so good for estimating eta so typically not typically often you can have approaches where in order to be good for theta you cannot be good for eta or it's hard to be good for both um and good being good in a frequency sense i must say Get good in a frequency sense, I must say, typically. And so, one way to sort of go around that is to use these cut posteriors. And what are cut posteriors? They were not designed at all for semi-parametric inference, they were designed to do something else. And it was to sort of join two types of data sets. But here I'm going to present them in the context of semi-parametric inference. So, imagine that you have this prior pi one, so typically a prior that would be good for theta. Typically, a prior that would be good for theta, but not good for eta. So you have a prior pi one, that's a prior on the whole parameter set. Then you recover the poster distribution, and you can marginalize out the eta. And so you recover the marginal poster distribution on theta. And you have designed pi one so that you know that it's going to be well behaved for theta, for instance, well behaved in the Bernstein-farmisis sense I was mentioning before. But the way it was done is that you know that it's not going to be not necessarily going to be good for eta. Not necessarily going to be good for eta, so you have to do something else for eta. So then what you do is you consider a conditional distribution on eta given theta in the second step, so that you can then you can from that you can define a conditional posterior distribution on eta given theta and the data, which is proportional to the likelihood times the prior, the conditional prior here. And then what you do is you construct an object which I call speedint, which is a probability distribution. Which is a probability distribution on theta and eta, which is a product of the marginal pi one of theta given y, so the blue object here that I had constructed from my prior pi one, times this orange object, which is the conditional posterior distribution on eta given theta and y n. So this is not the proper Bayesian approach in the sense that this phi t here is not a posterior distribution associated to one prior. Prior, it's a product of two, but it's a proper distribution on theta and eta. It defines a proper probability on theta and eta, but it's not a proper distribution. So you will never see what it means on the right-hand side. And I can't see it either, and I can't remember what I wrote. But essentially, what I wrote was that so I just redefined that object here. So that's the first one. And here in the second one, yeah. One, yeah, it's I just wrote the right-hand side there. And you don't, so this, yeah, so although here, sorry, although here you have a likelihood and you also have a likelihood on the right-hand side, which is hidden by my name, I'm not using twice the data. It's something else. So then what can you do? So what you might want to What you might want to study the asymptotic behavior or the posterior distribution, so this sort of cut posterior distribution. And in particular, what you might want to do is the sort of the basic sort of things that you might want to look at is the posterior contraction rates. So there is the general theory and posterior contraction rate for proper posterior distribution. So if you are using a real poster distribution, in a sense. And this theory dates from Bosal and von der Waard. And it's nice because it has simple conditions. And it's nice because it has simple conditions. So, I'm just going to remind you the conditions. So, what I mean by posterior contraction rates is making sure that the prior mass of neighborhoods of the two parameter here, which is theta naught and eta naught, is going to concentrate at the rate epsilon n. In other words, the prior mass of shrinking neighbors that shrink at the rate epsilon n is going to one as n goes to. Is going to one as n goes to infinity, or another way of saying exactly the same thing is the prior mass of the complement of this neighborhood, which is exactly what I wrote here, goes to zero as n goes to infinity in probability. So in order to be able to have that, one way to check these conditions or to find what will be epsilon n for our prior and our theta naught, you need to check two types of condition. One is the Kubebach-Labber condition, which is a condition that, for instance, Diana was mentioning this morning. So you're looking at the prior map. So, you're looking at the prior mass of cubicle neighborhood of the true theta naught, and you want to make sure that this prior mass is not too small. So, there is an epsilon n squared there and an epsilon n squared there. And the second condition is a testing condition. So, you want to prove that there exists a test, a sequence of tests that has type one error. And the test is typically for testing theta of V equals V naught versus the distance between V and V naught is greater than m epsilon n, and the type 2 error is exponential. And the type 2 error is exponentially smooth. So, these two, I mean, this theory is well established. There has been a lot of work there. And we know how to answer these questions for a lot of models. And so, the question is, can you extend this sort of theory for P tilde, the posterior, which is not quite a posterior? But it looks very much like a posterior. And so, the answer is trivial. I mean, it's really not a big deal. You can easily adapt this theory and have essentially the same conditions. And have essentially the same conditions to check for P tilde. So I'm going to write it in the context of, in the context where I'm going to be next for later on. But you can be more general than that. So for instance, if pi 1 is well behaved in the sense that you have this burn strain from this result on pi 1, then the only thing that you have to verify for pi 2 is essentially this sort of conditional kilbak label condition and the test. And the test. So, something. So, you have nothing else to do, and you're happy. In a sense, I'm happy. Whatever you are, I don't know, but I'm happy. And so now I'm going to sort of the core of the talk, which is applications to this semi-parametric hidden Markov model. And so, what do I mean by that? I mean that you have some observation yt, given some latent states xt equals j, has some distribution fj. So the Distribution Fj. So the distribution of YT given the latent states is indexed by the latent state only. And the states form a Markov chain. And Q here represents the transition matrix of the Markov chain. So in other words, the parameters that you have are the emission distribution, so the conditional distribution of y given x, fj, so you have k of them, and this transition matrix q, which are which corresponds to the Which corresponds to the transition matrix of the latent states. So it's a semi-parametric model because Q is a finite dimensional and the FJs are infinite-dimensional typically. So the first question that you might want to know is, does it make any sense? I mean, what can I do in such models? So these models, hidden Markov models, have been used a lot in the literature, but typically they are used in the parametric context. So for instance, you assume that these edges are normals. So that's J's are normals, so that's like a mixture of normals, but it's a hidden Markov models of normals with normal emissions or whatever. Okay, but the thing is that it's a bit like, so it sort of follows the discussion by Diana this morning. You might not want to make sort of normal assumptions because you have this problem of robustness if the model is misspecified, perhaps, da-da-da. But then, what does it mean to be misspecified? What does it mean to? It means to be misspecified. What does it mean to be misspecified, and what can you say? How can you relax this parametric assumption? If you didn't have a matrix cube, so if you had ID labels, then it wouldn't be identifiable unless you had some structures on the wise. What's magic is that when you have a hidden Markov model, so if you put a structure on the latent variables, then you can identify everything. And so there are two types of results. One was Two types of results. One was a result that we proved some years ago with Elisabeth Garcia, following a paper by Chris Yarrow, Chris, and Omiros, I think, as well, on location hidden Markov models. So the yt is equals to mj if xt equals j, plus some noise. And you don't want to make any assumptions on the noise, so it's not necessary Gaussian. And so you, you, it's semi-parametric, you put a pry on F, and then you have to apply on the M's. To a prior on the M's and the prior on the Q, which is a transition matrix. And what's interesting in this case is that if you look at the joint distribution of two consecutive observations, so this object is identifiable because it comes from the observables. So if you look at this, so the G for me is always the distribution on the Y's. So if I look at the distribution of the two consecutive observations, as soon as the determinant of Q is strictly positive, so my Q are. Q is strictly positive, so my Q are invertible, then I can identify everything. In other words, if I have G Qm equals G Q prime F prime, then Q equals Q prime by definition, and N equals M prime, and F equals F prime. So it's a bit magical because you can be non-parametric on the emission distribution, in a sense, or semi-parametric. But even more general than that, Elizabeth Cassia, later on, she Than that, Elizabeth Cassia later on, she proved that if you don't make such a structural assumption, but you say that the x y t y t given x t equals j is follows some fj. If you assume that the fj's are linearly independent as functions of y and the determinant of q is positive, so again q is invertible, then looking at the distribution of three consecutive observations implies that everything is. implies that everything is identifiable. In other words, g three of qf equals g three of q prime f prime implies that q equals f prime and all the fj's are always the f prime j's so you can add up to label switching of course so you can identify everything and that's super nice because that means that you can do this clustering program without having to make parametric assumptions on the emission distributions. And so that's where we are. That's what we're going to work on. We're going to work in this context. We're going to work in this context where I'm only assuming that yt given xt equals j follows fj. The fj's are linearly independent. And I'm going to assume that the true q star, I'm going to be frequentist in this respect, the true q star has a determinant that is strictly positive. Now, so following this work, we had a student with Elizabeth, Elo Di Vernet, who worked on the non-parametric posterior contraction rates. And so what she found was And so, what she found was a set of conditions that was a simplified version of Gausan and von der Waa set of assumptions to get a posterior contraction rate, but on the G's. In other words, on the marginal distribution of three consecutive observations or more consecutive observations. So that's quite nice, but it's often not the object of interest. Often you might want to recover the queue, or you might want to recover the emission distributions, or you might want to recover the small. You might want to recover the smoothing, so, in other words, the posterior polarity of x given the y's, and that's going from g to q and f is a totally non-trivial case. And so you might have good rates here for general matrix, but it doesn't mean anything about the prior on q and the marginal poster distribution on q and things like that. In the same time, since the frequencies literature, like machine learning literature, actually, using spectral methods, you can. Spectrum methods, you can find some estimator on Q that are converging to the true Q star under P star at the rate one of the square root of L. So you can estimate Q very efficiently. Something, uh, yeah, uh, and so there is there is an equivalent of this story in the mixture models. If instead, so in mixture, typically it's non-atronifiable. So in mixture, typically it's no night-and-variable, except if you assume that your y's are y1, y2, y3. They can be multivariate, y1, y2, y3. And given x, y1, y2, y3 are independent. And under this assumption, then you can again identify everything. And with Elizabeth and LOD, what we prove is that we can design a pie distribution, which is very simple, such that the posterior distribution on P in the mixture model, so not in the hidden map of model. Mixture model, so not in the hidden Markov model, satisfies the Bernstein van Mises results and it's well behaved. So the first thing that we would like to do is to find such a pi one for the hidden Markov case, not the mixture, but the hidden Markov, which are more complicated. And once we have this pi one, how can we construct a pi 2 so that we are not only good on q, but we are also good for the f's and the smoothing potentially? So that's what I'm going to talk about. If I can move on my slides. If I can move on my slides, yeah, forget it. Okay, nothing moves. Maybe it is on the slide, like that. I talk too much. I'm clicking on everything to be honest. Oh, yeah, that's okay. Oh yeah, magical touch. So, what we're going to, so I'll record the model. So, you have a you observe y and y t even x state xt equals j, follows some fj's that are unknown. And q and the x has a form a mark of chain with and such with q as a transition matrix. And yt can be in any Rd, it doesn't really matter. And so, our aim. And so, our aim is to find some priors such that we have a good estimation for Q in the frequency stance, potentially asymptotically normal, even better efficient in the sense that you can't do better in terms of the variance. And we also want to be good for estimating the FJs, and also we want to estimate correctly to have like nice results on the posterior, the smoothing probabilities, which corresponds for each observation, the probability of Xt equals J given Y. And to yeah, so these are non-trivial steps, but I start with the first one. So, interestingly, construct looking at Q is very easy if you take a very stupid prior. And what's a very stupid prior? It's a prior that will put mass on piecewise constant functions for the emission distributions. But the number of pieces is fixed, so I'm not even trying to approximate well my emission distribution. Distribution and so, um, and so, but it has to you have to have one constraints, in other words, that the partition, so the pieces, the bins that on which you have will have constants, your function will be constants in your model, has to be such that this matrix has full range. So, what I mean by this matrix is that, so, f star are the true emission distributions on each row. You have component one, one row for the row one, you have component one. row for the row one you have component one row two is component two and row k is component k here and so f star one of i1 corresponds to the mass under f star one of the bin i1 and f star of il corresponds to the mass under bin uh under f star one of the bin i l and i do that for each of the components that construct my mesh matrix and if i take l greater than q my partition is said to be admissible if this rank of this matrix is k. If this rank of this matrix is k. So if you have a full rank matrix and if you have your fj's, f starj's are linearly independent, you know that you can find partitions of the lengths of them of number of bins of at least k, such that you have a full-rank matrix like that. And then once you give yourself such a partition, which you assume to be admissible, you construct your emission distributions just as piecewise constant functions on these partitions. So this is very stupid. L is fixed. Very stupid. L is fixed, so essentially, I could take l equals k here, and so uh, and so, and then I put a prior on these objects. I'm put, I'm putting a prior, for instance, using a shared process on the y's on the w, sorry, and the q, I'm putting a usual prior on the emission distribution on the transition matrix. So that's very simple. And it looks like a parametric model. So I'm using that as a prior pi one, but I'm never thinking that this is correct for eta, for f. Correct for eta, for F. I don't think that this is a good model for F. I'm just trying to construct this prior so that for Q is going to be well behaved. And indeed, what you can prove, and I'll explain to you in a minute, you can prove that under such a stupid prior, in a sense, as soon as you have a partition which is admissible for the true distributions, and the determinant of Q star is positive, Q star being the true transition matrix, then you have a Bernstein for Mises results. Have a Bernstein for Mises results on the for the posterior distribution on Q. In other words, the posterior distribution on square root on n q minus some q hat converges to a normal with mean some kind of Fisher information matrix, the inverse of a Fischer information matrix. And this Q hat is actually the MLE in this model. Okay, so if I was assuming that F star were belonging. Assuming that F star belonged to this model, I would be in a parametric model, and then I would have sort of the usual parametric of Bernstein pharmacies, and that's exactly what happens. And then I would have this efficient formation matrix, which is sort of the projection of the whole, whatever. And so I will have this sort of asymptotic normality of the posterior distribution, and I will have the sort of the frequentist counterpart in the sense that the centering point. Centering point here have the same asymptotic distribution. And so, credible regions are confidence regions asymptotically. And I have a convergence rate, which is for the one over squared of L. So it looks a bit a miracle because in this result, the red result here, I'm never assuming that my f are piecewise constant. The f stars are not necessarily piecewise constants. They can be anything. So the reason why it works is because when you model Is because when you model, I hope you can see the green, when you model the densities by piecewise constant functions, for essentially you're not misspecified. What you're doing is that you're reducing the amount of affirmation in your data. You're replacing the y by the indicatrix that y belongs to n, to some bin. But it's still with, so with this data, the new data, which are the indicatrix values, you still well specified. And so, in particular, And so, in particular, you're well specified for Q, and this is a finite-dimensional model, so you have a symptotic normality, it's a regular model, and everything works fine, and you're happy to. And so, it's not a model misspecification, it's a data reduction. Of course, if I wanted to estimate the F stars, J's, I would be misspecified. But for whatever, for Q and for some other aspects, I'm well specified. Then another question that you want to actually see: oops, I don't know what I did. Yeah, is to see whether you can improve on the variance. Because so if I have a fixed L here, I'm having some variance that depends on the partition. And obviously, maybe there is a way to choose a partition such that the variance is optimal. So, how can I find the optimal variance in this story? And the optimal variance. In the story, and the optimal variance is called the efficient variance in the semi-parametric framework. And it's so, there is a literature on semi-parametric inference and efficient semi-parametric inference. So, in the context of, sorry, I'm going all over this place. In the context of hidden map of models, it's not a trivial story. But you can prove that if you let the number of bins go to infinity, but super slowly, as slowly as possible, your asymptotic value. Your asymptotic variance of Q goes to the efficient asymptotic variance. So, the best you could do to estimate Q, whatever estimator you would use for estimating Q. Okay. And so to do that, what you have to prove is that you have to prove that the scores, so that's the scores for the functions to estimating, these scores converges and the derivative with respect to Q of the logator converge as well. If you let L. Converge as well. If you let L as a L actually was M before, the number of bins go to infinity. Okay. So that's a nice result in a sense. And that was actually really non-trivial to prove. Because then what you can approximate efficient semi-parametric Bunchantramesis results using that. So in other words, if you choose L large enough, you're close to G0, which is the efficient. Fisher information matrix, and you have an estimator which is almost efficient and with a bunch of antrimesis results. So, in other words, the posterior looks like a normal with the correct variance asymptotically. So, all is well, and you can do credible regions that are confidence regions that are optimal confidence region as well in terms of size. It's good, but it's nothing, it's not good for f star because I've modeled f star my f functions as piece response. My f functions as piecewise constant functions with a number of pieces which is fixed and so on, almost fixed. And so I know that I'm not very precise for estimating f. And so, for instance, if I want to do smoothing, it's not so good. So now I'm going to do the cut poster story. So I'm going to use pi one as the way I described. So the pi one is going to be what I've just done. And now I'm going to pretend I haven't estimated the f and I'm going to put a prior on f12fk. So the emission distribution for each of these. So, the emission distribution for each of these components, which I call pi2, I can make it dependent on q if I want, it doesn't really matter, and then I construct my cut posterior, which is the product of the marginal distribution of q given yn, which I know is asymptotically normal, times this posterior conditional distribution on f one up to fk given q and y n. And doing that, I hope to get sort of the best of the world of the both words. Like, if I choose a good pipe. Like, if I choose a good pi 2, then potentially it can be good both for f and for q, and for something else, if possible. So, then it's up to you. You can choose whichever prior you want on the densities, because now you are estimating density, I mean, you are modeling densities. So, I personally like user process mixtures, but you can like random histograms, you can do whatever, polya trees. Um, that's for So, anybody, anyway, so you can do whatever you want. And now, if you do something, I don't know what. So, then the question is, what can you say of this behavior of the case posteriors in terms of asynthetic behavior? We know that if we use sort of the adaptive version of Gaussian and von der Vauxhaus' theory to get posterior contraction rates, Get posterior contraction rates so that it works for the pi tilde. I can get posterior contraction rates, but the posterior contraction rates I'm going to get are on the object based on the observable. So in other words, I will be able to get posterior contraction rates on the consecutive distribution densities of on the densities of three consecutive observations or two consecutive observations or whatever number of consecutive observations. So that's not good enough because what I want to get is Q and F. Well, because I Well, because I have constructed my prior as a capostera, I know that I'm going to be good for Q, but it doesn't mean that I can recover F. It's a different object. And so that's what we want to do. And it's not, again, I went in the wrong direction. Yeah. So now theta represents either Q or F or both. By using the cut posterior, I know that I have one of us quote-unquote rate of convergence for. One of us could have been the rate of convergence for the posterior on Q, and I would like to figure out how I can say something about the posterior contraction rate for the estimation of the Fj's, which are the emission distributions. So, how can I, what can I say about epsilon n here given my, like if I take a DF share process mixture on the FJs, what can I say of epsilon n here? So, the first thing to do is to get first a result on the Gs, which are the densities of the three consecutive observations, and from that, Observations, and from that, try to recover the S. So, to get a density on the G, I just use a result of LOD VRNA, so there isn't much to be done. And under like simple, fairly simple conditions, and in particular that are verified for Dirichlet process mixture of normals, I can prove that my posterior, my cut posterior concentrate on the G's, which are these emissions, the distribution of three consecutive observations, at the rate epsilon n, which is sort of the minimax rate of. n which is sort of the minimax rate of estimation under assumptions so for instance if i say that the edges are held there with regularity beta i don't know beta then the epsilon n will be of order n to the power minus beta over two beta plus one on dimension one or d in dimension d here uh if i'm if i'm taking random histogram with a number of a number of bin which is a random and i have a whole double functions f star j with beta less than one then i also get the minimum of convergence. Get the minimax rate of convergence, but it's so it's sort of minimax. But the thing is that it's minimax on an object that's not necessarily the object of interest. So now I need to go from g to k to f. And to do that, you need some kind of inversion inequality. So which is not necessarily very easy to prove. But what you can prove actually is that you can inverse. So in other words, if the g is close to g star and q is close to q star. And Q is close to Q star, then the F are going to be close to one another in L1 norm. So we have a posterior contraction rate for this part, G minus G star, in L1. We have a posterior contraction rate because pi 1 is well behaved for Q. And so now we know that we can bound these guys by the order epsilon n. And that is the same epsilon n as what we had for G. So for instance, if I take a Division process mixture of normals for estimating my S, then the rate of convergence I would get to Of convergence, I would get to estimate fj would be the same n to the power minus beta over to eta plus one. So, uh, so that's quite nice because you can recover everything without any kind of assumptions apart from the structural assumption that you had on the fact that you had a hidden Markov model behind. That's all I'm saying. Now, another question of interest is that if you want to estimate the smoothing, so the smoothings are these posterior probabilities, I mean the posterior probabilities. I mean the post operator, the probabilities of X t, so the latent state equals J, whatever J, given the whole data set. Okay, so if you want to recover the states from your observations, what you have to look at is this object. And for instance, say that you take the map, so you take the J that maximizes this quantity. And so the question that you want to know is that now you have an estimator for this guy, and is this estimator? This guy and is this estimator close to the true poster to the true smoothing distribution? True being here, the one that you would get if under Q star and F star. And another more surprising result is actually you can also recover from the G's and the F's, you can recover the same rate epsilon n for the smoothing distributions here. Distributions here. So for each observation, the L1 norm between these smoothing distributions is of order epsilon n if epsilon n was the rate that you got I got in the previous slides. So that's result is actually I think quite powerful and it comes from a first result obtained by Elizabeth Garcia and some co-authors who bounded the L1 this L1. Who bounded this L1 norm here between a P theta and a P star in terms of the Q, the difference in the terms of the Q's, and this sort of big difference here, which is the sum over all the observation, so forget R is the components, but more importantly, the sum over all the observations, so the rho to the power k should be here, of the difference between f r of y k, so y k being the observation, minus f star r of y k divided by this f star. By this L star, so some of the F stars of YK. So that's nice, but the problem is that what we know is something in terms of L1 and not a point-wise observation on the point-wise difference on the observations. And in the frequencies literature, the way they play with it is that they split the sample into two parts. And then they use one part to estimate the F's. And then for the second part of the observation, they estimate essentially this quantity by controlling these. By controlling this guy. But that's not very nice because it means you have to sample your data into two parts, or you have found a procedure where you have a controlling sub norm here, which is again for the frequency, for a Bayesian approach is a difficult thing to do. What's nice in the Bayesian approach is that because you're integrating out, you don't have to split your data into two parts, and you can prove that the posterior probability of this right-hand side here converges. Hand side here converges at the rate epsilon n as well. So it's uh, and that's one of the strengths of being Bayesian is that you don't have to split the data into two, you can have a bound on this object under the posterior distribution, which is of order of the same order as the one you get for the L1 norm. So that's very handy, and it's sort of a nice inversion inequality that you can get from that. And that's how you get this result in red here. Result in red here. Okay, so what? So we have all the theoretical results we were looking for. We have asymptotic normality for Q with the right rate and the right variance using pi one. And then I add some pi two there to construct my current posterior so that I have a good behavior for F. So I can recover the F at the sort of minimax rate, so what we imagine would be the minimax rate. And I can also recover the smoothing probabilities with a reasonable rate. I don't know whether it's minimax. Minimax. Now, the question is: how do you implement that? So, the Kirt Possier, there is a nice paper by Christian Chris Pierre-Jacob that has never been published yet. So, maybe. So, we are not inventing the powder, we say in French. So, how do you do that? Simulation from this caposteer. So, you first simulate from Pi 1, and because my prior that I have Pi 1 was so stupid, it's very easy, it's very simple. It's very easy, it's very simple because it's a parametric model in a sense. So, the first step is very easy. So, you sample, you do MCMC, a long chain, and then you do thinning, and so you recover the NQTs that are essentially distributed from Taiwan, not exactly id, but if you do a lot of thinning, then it's almost id. And then, what you have to do is now you have to sample for each Qt. For each Qt, you're going to do these steps, which is first to find you need to find a good initialization for the hidden steps. So to initialize everything, we use a map from the previous steps. And then at each T, you find a new initialization that comes from the map at the previous step. And you do an MCMC. So that would be like a usual MCMC for X, given X. Usually the MCMC for X given F and previous observations and previous iterations and F given X and previous iterations. So it's sort of Gibb sampler here. And once you have done that, so it's exactly Gibbsampler actually, then you record, what you keep at the end is only the last iteration. So for each T, you have to run a mini chain of size capital C and you recover, and you keep only the last iteration of the mini chains. And the reason why you have to do that is Reason why you have to do that is you need to sort of pretend that you have simulating under the posterior this, this, yeah, the posterior distribution. And so it has to be like under the stationary distribution of the Markov chain here. So C is supposed to be infinity, but of course we take C equals 10. So just some simulations. So for pi 1, as I said, pi 1 is super easy to simulate. So we did just some simulation studies, like very basic ones, in the context where you have k equals In the context where you have k equals 2, q star is a true transition matrix that you have used, we have used. So that's the one. And we have two normals for, so normal, I can't remember, I think it's minus two, one for the first state and normal two, one for the second or something like that. And so here for you have to read this graph two by two. So this is Q. This is Q11 and Q22, or vice versa. In each of these, and that's in the case where I take only two bins. This is for eight bins, so for some reason we didn't plot the four bins. This is 16 bins, then 64, and then a lot, 256, I think, or whatever, something like that. So, what you can see is that So, what you can see is that at the beginning, it's not too bad. It looks like normal, but it's quite white. And then here, it's centered more or less at the same values, but it's much more narrow. So it's better. And then it starts shifting, and then it starts becoming crazier. Okay. So what it shows is that if you choose the number of bins too large, you get a totally crazy result. And you have to, so somehow the way I see it is that you have to pick up The number of bins by trying to sort of find a narrow version of the first part. So you don't want to shift away from the first part. So you can sort of think about the first part being well-centered, but too wide. And then you reduce the variance and then you start shifting. When you start shifting, you stop. So you have to. So it's sort of a heuristic version of it. So the question is how to make it more formal. We haven't answered yet. Same story when you have. Same story when you have 8,000 observations. So it's still very good for two. So the two bins here, but it's wider than in the case where you have eight. And in this context, probably eight is the best. Now F2. So now if I use pi one and I use that also to estimate my f's, it will be too good. So I will have these sort of objects. So remember, we keep, let's say, eight in each cases. In each cases, and so I will be there, and my estimator will be terrible. Now, if I if I um and the same for a lot later, if I use my pi 2 now, I'm using my mixture of normals, so I'm just plotting something and I can't remember what. Yeah, um, so this is yeah, at the beginning you have n equals a thousand and c equals 10, so c is the size of my mini chains, so the number of iterations I have to use for each key. Iterations I have to use for each key. And this is C equals 100. And so what we realized is that you don't need a long mini chains. C equals 10 is enough, which is reassuring because it takes time. And so then so then we kept c equals 10 each all the time for n equals 5000. So n equals 5000 here. And here the number of beans which we chose for pi one was a four. And I'm running in my Devi Schleier process machine of normals for estimating the densities. Of normals for estimating the densities. So you don't see much, and in particular, you can't see at all the true distributions. So you have to believe me. The two distributions were in a light green, which was not the best idea I had in my life. So there it's a bit here. So it's cell tomorrow at the same value. So the blue is the posterior mean and the reds are the posterior continents, point by 12, point by point. So you see that when n equals a thousand, it's When n equals 1000, it's a bit wide, and then when equals 5,000, it's really narrow around, trust me, something which is close to the true value. For the smoothing, so we didn't try to optimize everything here, we just looked at the data brutal group data or whatever. And so, here what I have plotted, what we have plotted, actually done plotted, is for each observations, so each iteration. Observations, so each observations, I'm looking at the difference between the posterior mean of these smoothing probabilities and the true smoothing probability. Okay, so this is for n equals 1000 and this is for n equals 5000. So if you see it's more dense because you have more observations, but it's also it's not only more dense, but it's also shifted towards zero. So I don't know if you can see correctly here, but it goes sort of all over the place between. It goes sort of all over the place between 0 and 0.3 here, and here it's much more condensed around 0.0 and 0.05, really. But more importantly, if you sort of took as a decision rule Xt equals J if the posterior probability of Xt equals J is greater than a half, or the posterior mean is greater than a half. I'm not comparing this rule with the true X T because Rule with the true Xt because this is something which is much more fuzzy in a sense. I'm comparing this rule by the result I would have had if I knew the parameters. So I'm comparing the rules when I count the number of times my posterior mean of Xt equals of the probability of Xt equals one given Yn is greater than a half with the same thing, the same quantity, but computed for under P and Q under the two parameters. P and q under the two parameters. Yeah. Yeah. Can we have one more minute? Yeah, I have one more slide. That's good. And in these cases, for a thousand, for instance, the number of wrong decisions are 5.8%. And when you have n equals 5,000, it's 3.6%. So you do see that it goes to zero based on this quantity. Even though here, I mean, the errors are not so small. Are not so small, but somehow they are not large enough to put you in the wrong decision, in a sense, most of the time. So, that's so, I mean, that's still like a very first toy example that more simulations need to be done. So, to summarize, to wrap up the story, so what we can say is that, so we have a square-to-van estimation for this Q in the non-parametric hidden Markov models, and not only is it a square-to-vin convergence, but you also have a But you also have an efficient benchmark infringesis theory behind it, and efficiency was the hard part in the story to prove. Another takeaway thing that might be of interest for people is this sort of inversion inequality that to go from the G3, which was a distribution on the Y's, to the FJs, which is a conditional distribution on Y given X. And the last and not the least is this idea of cut posteriors in semi-parametric inference. Posteriors in semi-parametric inference, which I think is can be of interest because it's not often easy to construct a prior such that you can be good for both aspects. So here we are good for both for three aspects. It doesn't mean that we are good for everything, but at least we are good for three aspects, which are Q, F, and the smoothing distributions. And that's it. Thank you. I think we don't have time. Quick question. Yeah, we are a little bit behind schedule, but if there's somebody with a quick question, Peter. Do you need to know the correct positive key? The number of components of states. It's a good question. So. Because, like, in your Because, like, in your silly prior, sorry, in your silly prior, you had to assume that the FG stars were essentially different, right? Yeah, exactly. So, if you overestimate the key, I have a problem. It's a very good question. I think, so we haven't done the story. That's the next chapter, in a sense, but my intuition, so take it at it as a, yeah, is that I think you can estimate k using pi one. So, that's a nice aspect of it because pi one is so simple, but then you. Is so simple, but then you have to be clever in how you design L, and okay, and that's something to be tuned and worked out. But I think you can prove that you