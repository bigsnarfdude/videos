I will speak about our efforts in recent several years trying to machine learn kinetic energy densities for ultimately machine-learned kinetic energy functionals and how we like to use specifically features in machine learning which are based on gradient expansion. You heard Johan's talk yesterday, so the data he was describing how we He was describing how we generate the data that we feed into the machine learning. He was talking about yesterday. And I will be presenting the actual machine learning part on that data as well as on our previous efforts. This is a slide which I, every time I talked about orbital 3DFT, this was computed, I guess, maybe six years ago already. The MD curves were by Charlander. The MD curves were by Charlander Joshik, who was at the NUS at that time, and the orbital-free curves by us. And I'd like to show this slide to illustrate why it is that we really need to push for orbital-free development for routine large-scale abnitral materials modeling. There are heroic efforts, of course, with even orbital order N DFT into million. There's a paper about million atom or order M DFT. William probably knows it very well. William probably knows it very, very well. No one's gonna ever repeat or check those calculations. The word routine is critical. We need routine, large-scale ignition modeling, and routine means orbital-free because we can get both linear scaling and small, a very small pre-factor. Speaking of linear scaling, there are two ways, or by and large, or kind of limiting cases. I think there is a continuum in between, as we try to discuss with Sam on my. Try to discuss with Sam on Monday. So, there is a key DF way where you learn a semi-local approximation. You essentially learn a kinetic energy density as a function of density-dependent features, which are various derivatives and powers of the density, or you can do non-local, the non-local way, which has certain theoretical advantages associated with it. We believe there is a continuum between these two, but it is the key DF way, which is prima fasci, closer to linear scale. Closer to a linear scaling setup. You can make this neolinar scaling, where this is technically n order, a log n log n. So typically, so what will happen today, we'll talk about machine learning efforts for this part. We have been gently working on the pseudo-potential version of machine, so basically machine learning for pseudo-potential generation. This will not enter this talk in any way, shape, or form, but we've been kind of playing on the side in that space. Playing on the side in that space. Pseudo-potentials, local pseudo-potential development has kind of got a new breath with recent efforts in Jelin and elsewhere. So this is, but we keep in mind that we can also machine learn this. We will be machine learning something starting from the kinetic energy density. And so this routine idea is important. We need large scale, not just because we want to compute something very, very rapidly, but because it's But because it also will enter some sort of multi-scale approaches. So, this is again back to this picture. We see here a stress-trained curve for magnesium. Only 10,000 atoms. So, this is not enough to compare to real stress-trained curves, but this is enough to enter a plasticity regime. And therefore, this is enough to compare methods in the plastic regime. So, that's a regime in which you never benchmark your order N or force fields or with L3 DFT. You benchmark them to contra small scale. Benchmark them to concham at small scale. You almost never benchmark at 10,000 atoms. So there are three force fields here. So these are one, two, three force fields. And there are orbital free calculations, some using Cartos pseudo potentials, some using Aurora pseudo potentials, which were kind of machine learned about 10 years ago. And lo and behold, all orbital free calculations, they kind of, there is some difference. Of course, there is a difference, but they largely track each other, right? Among the three force fields, all three. Among the three force fields, all three usable, all three used in applied. You can find papers using all three in applied simulations. Only one is validated by Orbital Free and two are disvalidated. All three behave similar in the elastic regime. And it's the plastic regime when you enter the fracture where you see this difference. So this really drives to me, this kind of drives home the message that we need this, right? So we are trying to learn this functional from samples of our constant kinetic energy density as a. Crunchian kinetic energy density as a function of density-driven variables, and the number of features here, as far as database techniques are concerned, as far as machine learning is concerned, this is already highest dimensional. When you say high-dimensional feature space, we typically think 100. No, six dimensions plus, you already get hit with the curse of dimensionality. So this machine learning methods begin to suffer even if you have five, six, seven, ten features in your machine learning. So this is the kind of preliminary, some preliminary consideration. Preliminary, some preliminary considerations. What are the density-dependent variables? What they are also determines how many you need, what is the dimensionality of the feature space. And it's in the title, we like to use the gradient expansion to build our features. Of course, this is not the only way. You can find papers, I just kind of mentioned just one group, Nakai group at Waseda, who used both the density-dependent features straight up, as well as geometry-based features, which makes it sort of a hybrid. Which makes it sort of a hybrid between a force field and proper orbital 3 DFT. We tried to not do that. I understand why this would help a lot. And they were able to get potential energy curves, for example, on molecules. We tried to kind of to, I have a personal interest. Can I push machine learning to such heights that I don't need this support, right? The support of this, I just learn straight from this. Of this, I just learn straight from the density and get the energy curves, etc. Long time ago, when we were looking, can we actually converge these guys in terms of the gradient expansion, we understood that in terms of just the numerics up to fourth order is actually quite okay. The sixth order, we heard here before that people don't like six order, and we also couldn't really get this properties easily converged with the typical grades that one could use. One could use if uh or plane wave um cutoff. So, fourth order is okay. So, we use these terms as uh as features. You can uh you can work with these uh gradients and Laplacians straight up, or you can scale them. So, scaling means scaling relationship. People love scale, oops, maybe before I go there, people love scaling relationships, but there are works, including the Kai's group and others, which didn't scale them. We, in our initial works, didn't scale them. Scaling, these coordinate scaling or density scaling, however you look at it. Density scaling, however you look at it, is something that people like to use, like to claim it's necessary. I would like somebody to show me where it happens in nature. If you know, please tell me. Okay, so it's quite okay in some cases. Like I understand the scaling will help you numerically, but it never happens in nature. Okay. So some works using scaling, some don't. Our initial efforts in this direction did not use scale, gradient, and Laplacians. So what I will present, so we were ambitious. First of all, we never We were ambitious first of all. We never wanted to play with one D systems to this system, whatever synthetic potentials. If we are doing on this, we are working on real stuff. We are looking on molecules, on solids, etc. So, right off the bat, we started, let's start with some set of materials where we know there is one limiting case where it already knows to work well, and another where it's really difficult. So, we considered magnesium, lithium, aluminum, silicon. So, this is easy, light metals, existing approximations work well. WT. Work well, WT will work well. Silicon is, you know, kind of borderline where it begins to kind of work and kind of not work. And then molecules are bad, right? With orbital 3DFT. So we started to look at this and try to machine learn. And we at that time, we didn't use kernel methods. We started with neural networks. I had been using neural networks for more than a decade. At that point, I love neural networks. So we hit them with neural networks. So we would sample the So, we would sample the Kanzhikinetic energy density, we would sample all of these terms of the gradient expansion of, of course, on a grid, and we would machine learn. And as we machine learn, because we start with the four-order gradient expansion, we also monitor what the gradient expansion itself does. And this is an example from silicon. So, these arrows, which also appeared on the previous slide. So, these are, so when we plot this thing, so let's let me plot all these quantities along this direction in the crystal, or let me plot them, for example, along. Or let me plot them, for example, along this bond, et cetera. So we define some directions where we monitor this thing, right? Or we can plot them in 2D, which is kind of more difficult to read. This is very, very easy. So right off the bat, we see that the fourth formal T4, the T4 is what we call this fourth order gradient expansion, doesn't quite track the black curve, which is the Kanchan kinetic energy density. Of course, you want the integral, but it would be nice if you also track the density, right? Then what we did is that there's a thing here called T4 fitted. Called T4 fitted. Well, neural network introduces coupling and non-linearity. Before we even do that, I can simply fit the coefficients of the terms of the four-forward gradient expansion. And when I say fit the coefficients, I don't mean the coefficients between T0, T2, and T4. I mean coefficients in front rather than between, in front of every one of the summons. If I just adjust linear coefficients in front of every one of the summons, do I get something better? And of course, because it's a fit, of course, you. Better, and of course, because it's a fit, of course, you get something better. So, the green curve approaches, but you need non-linearity and coupling because it's still very far from the con-sham curve. So, that now hid it with a proper neural network. And what we observe is that with a proper neural network, you can actually fit very nicely the concham kinetic energy density almost perfectly. And you never need a big neural network. A small neural network is enough. And one reason for that is because we begin with the fourth-order gradient expansion, which is supposed to track this thing to begin with. Supposed to track this thing to begin with, so in a way, we're machine learning just adding a little bit of coupling and only extra non-linearity on top of it, right? So, uh, you never need more than 10 neurons, for example. So, that's something that we understood at that time. But, and, and, so I have in the appendix, I have other materials and including water, for example. Of course, water was worse, right? But it wasn't too bad. But, uh, one thing we realized that the data hits you in the machine learning. In the machine learning for quantum chemistry in general, not just in the orbital 3DFT field, you surprisingly rarely see the analysis of the even the basic analysis of the data on which people do machine learning. Always look at the distributions of your data. So what I'm plotting here are now the scaled version of the terms of the gradient expansion. So this is scaled Laplacian, scaled gradient, scaled Laplacian, the Fonweis, the Katoma-Sermi term, the Kancham and the positive definite version of the Kancham kinetic energy density. version of the constant kinetic energy density. These all have really bad histograms. They are really badly distributed. And this has incidence of what machine learning then does. For example, we discovered that every single one of the material, including the molecules, we can fit really well with a single hidden layer, small neural networks. There is value in that. The Kieran Berg showed recently that there is value in building a kinetic energy functional for a single system. So there is some value in that, but we would like to have portability, right? We would like to have portability, right? So, how do we get portability? We discover that no matter how hard we hit several materials at the same time, we combine the data, right? We cannot get, we get unpleasant stuff out. It is the use of multi-layer, what people like to call deep neural networks. So, the annotation case: 20 neurons in the first hidden layer, 20 in the second, et cetera, right? So, how many neurons in each hidden layer? Then we get portability. Then we can fit everything for several materials. Feed everything for several materials at the same time. So, we didn't use tons of materials here, we use several, but we saw that this is where it really shines. So, the data aspect is very, very important. Another thing that we found, we were sampling not just a uniform grid anymore, we were sampling a bit denser on some two-dimensional planes passing through some atoms, right? So then we could actually do simultaneously all four of these guys then can be fit simultaneously with that. This is still a relatively small neural network. This is still a relatively small neural network, right? We never needed to have, you know, hundreds of neurons or 20 layers and stuff like that. So we were, we then say, okay, eventually I want this thing to survive some sort of self-consistent optimization. We already knew, we wrote our own code, which is on computer physics communications, available for download in C. Never bothered about efficiency. We just wanted to have a code where we could feed our neural network functionals. Functionals, and we knew that you can actually do self-consistent optimization with these neural network functionals, even though the potential doesn't fit, doesn't enter our fitting, right? But so, eventually, we want to make sure that optimization of the structure is good enough. And we started by looking at a measure of the curvature of the energy volume curve, right? So, this is something that already kind of this, if you get this right, it means you have a chance to get the optimization right, you have a chance to get the phonon right. You have a chance to get the phonon right, etc. At that time, we liked more and more kernel methods. So we switched to A, we switched to kernel methods. So this is the equation for your generic kernel rich regression, whether it's KAR or GPR or whatnot. So we pick a kernel and you do the kernel trick. And we switch to using the gradient expansion. We scale the gradient and Laplacian, right? So the idea here being is that if I start, I'm not. I'm not adamant about scaling relationships, but if I if they help, then I might as well start with something which satisfies them. Even if I'm machine learning on top, I'm kind of almost satisfy them, if that makes any sense, right? One thing that we understood is that a big chunk of the variance of kinetic energy density can be captured by this feature, which is the product of the density and the Kansasheim effective potential. The Kanshreim effective potential can be computed without recourse to the orbital unless you Recourse to the orbital, unless you use hybrids or other orbital-based functions. So, for many practical cases, this is an available feature in your orbital-free calculation. And with that, we could get this measure of, this is like a bulk modulus-like thing. So we could get this to a relatively good accuracy. So, this is what you get for this B prime with con-Sham. This is what we get with our GPR models. So, we check directly the ability of these machine learning models to reproduce energy. Machine learning models to reproduce energy-volume dependence. Accidentally, so there's more here. There's an infinite number of hours I can talk about the machine learning aspect of the entire thing. For example, we discovered that the common radial basis function or Gaussian-like kernels are not the best. The best in this case was a Meton three-half kernel written here. So there's a lot here to discuss. So the defining pictures, just the isosurfaces of the kinetic energy density, right? Of the kinetic energy density, right? So, one thing that I would emphasize that this is learning kinetic energy density, it means we have a lot of data. So, for free materials, for you know, volume displacements, we already had a half million data points. And with some methods, it's okay. For GPR, KI, you don't want to feed many data, but we were able to get good models with a fairly small amount of data. But you can imagine that if you want portability across many, many, many, many materials, the data will quickly. Many materials, the data will quickly overwhelm you in the key D paradigm. So we want to do something about it. And we want to do something about the fact that, maybe let me talk about this a bit later, the fact that our data distribution, as I showed, is extremely spiky. So this is from another work, but basically the same kind of very spiky distributions of this G4-based features. One, of course, realizes this, I guess, the space is missing, so something looks funny here, but the idea is very simple. Funny here, but the idea is very simple. If you apply a smoothing kernel to your features and to your kinetic energy density, your kinetic energy density after this kernel is applied will still integrate to the proper kinetic energy, and therefore there is no danger doing it. So if we apply a smoothing kernel which smoothens our distributions but still retains the spatial structure, right, the partial, partial averaging, so the width of this kernel will be, for example, smaller than the unit cell, right? Then we can still come. Cell, right? Then we can still continue to work in the key D paradigm, but the distributions are much, much better. So, this is these distributions on the log scale. The blue is the original one, which were very nasty. On the linear scale, you see these tails, which are not densely sampled. And after we apply a smootel of some, maybe half of the unit cell or something like that, then this is your log-scale distribution. So, we kind of make it nicer. And because we make it nicer, we can machine. And because we make it nicer, we can machine learn them with very few training data, very few training data, right? So the example here is that when we are machine learn for the aluminum, magnesium, silicon simultaneously as a function of the number of training points, even with 2,000 training points, that's extremely few, right, out of the entire cube, right, that you sample, we can get this measure, the mean relative error on this measure of the energy volume dependence. measure of the energy volume dependence down to about a couple percent and that's good enough so what i chose to show here and it's very good that i'm following uh luis uh is that uh in in the paper where this was published we use standard gaussian process regression but what i'm showing you on this slide is when we do this with our with the our new machine learning method which we developed which is which has a similarity which was luis was showing in that what we effectively have here is a neural network with automatically Neural network with automatically optimized neuron activation functions of all neurons. The way this is done is that essentially each one of these guys is a 1D Gaussian process regression in the redundant coordinates y, which used to be the argument of these neurons. Again, I'm mentioning it extremely briefly. So, this method is introduced in this paper. It is very powerful, avoids overfeeding, spits out optimal shapes of neurons. So, Louise, if you're still listening, right? Still listening, right? Spits out optimal shapes of neuron activation functions in one linear step. No non-linear optimization is done anywhere whatsoever. And by analyzing the optimal shapes of these neuron activation functions, we see they are non-linear, we see they are not sigmoid. So nonlinearity is important. We can freely play with the size of this NN slash GPR and understand that we need maybe a few dozen terms here to converge this beast. To converge this beast, the error is ultimately limited, of course, by the density of sampling, as is always in machine learning. The lags of this method grow from the concept of additive GPR. Some of you may be familiar with additive GPR. Additive GPR would allow you to build orders of coupling expansions that I think everybody in this group are familiar with, whereby each term of these orders of coupling expansion will be. Of these orders of coupling expansion will be machine learned, in our case, with Gaussian process regression, and looking at the magnitude. So now we are in the ANOVA regime, right? Analysis of variance or what Herschel-Rabis would call HDMR, high-dimensional model representation. This ANOVA or high-dimensional model representation, when combined with machine learning, gives you a very powerful tool to machine learn from sparse data in high-dimensional spaces. We recently reviewed this. Paces. We recently reviewed this approach in artificial intelligence chemistry, the new LSEVIR journal dedicated to machine learning in chemistry. But to cut the long story short, we can then analyze the magnitudes of these terms. And if we optimize the kernels with which we do GPR of these terms, we can also see something like an ARD regime, where if the length parameter becomes very, very long, that set of variables is deemed unimportant. The kernel flattens. If length, The kernel flattens. If length parameter is short, that there is a lot of corrogation in the kernel, and therefore, that set of variables is deemed to be important. And what we observe is that, depending on how many training points we throw at this machine learning problem, different sets of variables that enter these terms of the orders of coupling expansion, they acquire different qualitatively different magnitudes and lengths parameters. For example, some of the terms acquire Some of the terms acquire very long optimal length parameters, and their magnitude also drops simultaneously. So, this is the magnitude, and this is the optimal length parameter. So, they die. And some of the terms have length parameters which are on the order of the scale of the data, which is normalized or scaled to a unit cube, and their magnitudes are substantial. So, this is then a way to see which sets of these features, of density-dependent features, are important. And once this generic machine learning technique tells you which sets of features are important, Which sets of features are important, you can go back to your notepad and start thinking of formulas which would incorporate those sets of features. So, what we are obtaining here is effectively inside elements of insight with an otherwise generic machine learning method. So, this we describe, but so if you work in this TED paradigm, then the number of data will grow. If I want portability across a thousand materials, each I have to sample. Materials, each I have to sample on the cube, so I'm in the millions of data, I'm in zillions of data. It's not really good. So, the next step we took is that we started from this idea of working with kind of partially average features and kinetic energy density, and we just fully average them. And if you fully average them, it's not the same as working with just kinetic energy, which is the integral of kinetic energy density. If you work with the average features, you don't suffer from issues having to do with kinetic energy being dependent. Risk kinetic energy being dependent on the size of the unit cell. Different materials have widely different unit cells, right? And therefore, the data aspect is made worse by that. If instead of just integrating tau, I work with the cell average tau, I kind of get rid of that problem. So I'm averaging my density-based features. I'm averaging the simulation cell are my tau. It means one material is now one data point. It means that if I were to machine learning. If I were to machine learn four materials like I did before, that would make no sense. I cannot machine learn much from four points. But if I now have data on a million materials, I have a million data points, on 400 materials, Johan yesterday, I have 400 data points, I can work with that. So when I work on many materials, this is advantageous. When I work on few materials, I just stay with the KED paradigm where I have the spatial structure and I learn that, right? So Johan introduced all this data so that saves. Introduced all this data so that saves some time for me. I'll probably still be over time, right? Yeah, we have time. Don't worry, we have time. So, Johan introduced this database of materials which contain binary, of course, unary, but also binary and ternary compounds containing all these elements because they have good local pseudo-potentials. That's why we use them. So, applying certain criteria that Johan explained yesterday, we end up with a data set of four. We end up with a data set of about 430 materials that we use for training, fully averaged features and kinetic energy density. It means that we have 430 materials at equilibrium structures. And what we do, we then strain and stretch and compress them. So we actually sample the energy volume curve. So this is like a dozen points on this curve. And then 433 times that we end up with several thousand data points, right? Several thousand data points, right? So we can machine learn that. And if I have several thousand of these strained materials, I really don't want to work in the key D paradigms. The number of data would just explode on me, right? So 18 strain structure for each material. And what do we get if we machine learning, for example, with Gaussian process regression? We get something good. First of all, because we machine learn effectively a kinetic energy, right? All quantities you are after, the bond. You are after the bond stretching, you know, energy versus bond length, energy volume curves, that is a small addition on an otherwise large kinetic energy. So that's why this orbital free problem is difficult. So of course, when you look at the correlation plot that machine learning community typically looks at, we all look, regardless what you hit it in, it's perfect. It looks great. See the number of digits for the correlation coefficient here, right? The interesting thing here is, even if you hit it with linear, Even if you hit it with linear regression, your correlation plot is perfect. The digits do differ, but you're learning something where what you are after is a small addition on an otherwise big number. So if you actually drill down, so what is the accuracy in your kinetic energy, what is the accuracy in this measure of the curvature of the energy volume curve, you see humongous differences where the GPR really gives you an accuracy. I will show you pictures in a bit, what these numbers mean. So it shows you. What these numbers mean. So it shows you good accuracy. So MRE is mean relative error, MDRE is median relative error. We look at MDRE because there are some maybe some outliers and median kind of takes care of that. So linear regression is not good, even though it looks good. Like any machine learning person would look at this, wow, this is great machine learning. No, no, no, no. In this field, you need a humongous accuracy of machine learning. Well, how about polynomial regression? You can very easily get polynomial regression by staying. Get polynomial regression by staying with your kernel regression engine and just giving it a dot product-based kernel. So the maximum value of p will determine the order of the maximum order terms in the polynomial. You have to go to very high-order polynomials to match the accuracy of GPR. So we are really in the regime where non-linear machine learning, which requires helps, which arrives on this expressive power of this non-linear MATN type kernels, the polynomial. Type kernels. The polynomial regression still cannot really compete. You get overfitting with fifth-order polynomial. Fourth-order polynomial comes close, but still more overfitting. So we have train and test, the usual, right? Another thing for those of you who are into machine learning, again, the RBF kernel was not the best. It's the Matten 5.5. If you are using Python, there is a bug in Python implementation of Matten 5.5, literally an error, which drives home the message. There is a reason why some software. The message: There is a reason why some software costs money. So, if you were to repeat this in Python, you would get rubbish because there's an error in Python implementation of this kernel. Of course, not many people use Mattem 5.5, so maybe not many people know. But this is also an application where it's Mattem 5.5, which is the best, not the RBF. So, now there are some visuals which are really beautiful. How many minutes do I still have? Five. I'll finish in five minutes. So, this is what we get. So, how to read this? This is what we get. So, how to read this plot? Maybe difficult from far away. So, these are unary compounds, right? The red curves, the blue rather curves, are reference Kant simulations, so Abinit. So, when we take the Abinit calculations, we throw away Abinit kinetic energy, we replace it with our kinetic energy, with the machine learning model, right? So, the red or green curves. So, when it's green, it's a When it's green, it's a material from a test set. When it's red, it's a material from the training set. And we, of course, ran it for many different selections of train and test sets. They all look similar, right? One thing you notice that you cannot, visually, you cannot say that your test set is any worse than your training set. So they're different accuracy depending on the material, but by and large, you get your curves right, right? Binary compounds. Oh, wait, before I go to binary compounds. So I mentioned So I mentioned that linear regression looks beautiful for any machine learner, but not so good on the actual numbers, for example, this energy volume curvature, right? And we can see this. So look, this is your linear regression. Looks beautiful on the correlation plot. Not so good. So we're really learning, we really have splitting here, and it's needed, right? So these are our binary compounds with GPR. Looks good, right? This is our 10. This is our tenner compound with GPR. Not all of them are perfect. There are some which are worse. But by and large, we get a portable model. Another thing that we discovered, again, I have slides in the appendix if there is time for discussion, is that the GPR train on a small number, we have 58 unary compounds, hundreds of binary, about 100, I think, of ternary. So if we train this model on only 58 unary compounds in our data set, we can actually extract. In our data set, we can actually extrapolate. So, these pictures for binary and tuner still look good. They are worse, the number of data is small, but they also have minimum of the curves, they all look okay. But if we train only on binary compounds, of which we have hundreds, the unary compounds don't look good. Maybe if I just go to my appendix, I want to show this. So, this is small number. This is binary trained on only 58 unary. And this is unary trained on a couple hundred. Unary trained on a couple hundred well, yeah, two, yeah, a couple few hundred binary, not so good. And that's because unary compounds actually sample a wider range. This is difficult to read, but unary compounds sample a wider range of both the density-dependent features and of the actual average kinetic energy densities and binary and ternary. This is good news because it's very easy to add unary compound in your training set and do that. It's very good. And if you train only on those. It's very good, and if you train only on those, you already get a decent model, and that's very good. Uh, so um, I think that's I'm about 30 minutes into the whole thing, right? So, I will conclude. So, we performed both neural networks and kernel regressions of the kinetic energy density. We began in the proper QED paradigm, and then we started kind of averaging this data to get rid of the data distribution. Problem: we like to use terms of the GE4, not as 024. Uh, you know, zero to four. We use all summons as separate features, so it ends up being like seven features. We like to use the product of the density and the constant effective potential as a feature that helps machine learning. The data distributions of all of these features and targets are very uneven. So please pay attention to the data aspect of this problem. It's somewhat underreported yet. With neural network, it's very easy to fit millions of data, but with GPR, not very easy because you need to inverse the covariance. We need to inverse the covariance metrics. So we can do several materials at a time, but then we are hit. If you want to do more, many materials at a time, and stay in key D paradigm means you work with a lot of data, you can help it by smoothing. Smoothing will palliate the data distribution issue. Eventually, you can get rid of the spatial variable altogether and do what I just wrote in the last slides. In principle, it allows you kind of to work between kinetic energy density and kinetic energy. Between kinetic energy density and kinetic energy paradigms. We reproduce curvatures of energy or volume curves of by now several hundreds of materials with a model which completely integrates out the spatial variables. So this looks promising. I will thank the GST Mirai program for funding as well as National Science and Technology Council of Taiwan that helped Johan. Thank you for your attention. I will try to answer any questions. Will try to answer any questions.