From it. Just for a little bit of perspective, so I'm not a mathematician, there will not be any theorems in this. I'm an engineer, I'm a fluid dynamicist. So, a lot of what I talk about will be sort of applications of many of the ideas that others have talked about here: dimension reduction, symmetry, actually very close connections in the last part of my talk with some of what Richard just talked about. Talked about. Oh, this is Madison in the summer. It's very nice. So, most of the work that I'll talk about today was done by some great grad students and postdocs. So, Alec Lineau is now postdoc at UCLA, and Daniel Florian are responsible for most of what I'll talk about. So, like I said, I'm a fluid dynamicist. Whoops. I'm interested. I'm interested in problems in a range of situations. My little movie's not playing there. Let's try it again. There it goes. So I'm interested in applications that I care about. We often have very high-dimensional data from experiments or from direct numerical simulations. I'm interested in systems with complex dynamics. Dynamics. And what, as an engineer, what we're often interested in is having very highly accurate but efficient models to do things like some kind of designs and control. I won't talk about the control today, but actually in the turbulent flow context, one of the things that we've done is to go on and apply the model that we developed for reinforcement control application. So most of what I'll talk about today is really more What I'll talk about today is really more closely connected with this. We have some very recent work looking at dynamics in complex fluids. So, things like polymer solutions, surfactant solutions, things where there's very interesting dynamics at the microstructural scale in the fluid. But I won't talk about that. All right, so the basic idea that we want to pursue, which isn't a new idea, but it's kind of there's been But it's kind of, there's a new life breathed into it with the development and availability of data-driven tools, is to develop dynamical models that are keeping only essential degrees of freedom. So the idea would be you have a reduced order model, but it's not reduced in the sense that you've thrown things away and that it's approximate. It's reduced in the sense that you're keeping only what's necessary and nothing else. And the formalization of that idea actually comes. Of that idea actually comes back to, again, to what Richard was talking about, and that's the idea of data living on manifolds. And so most people in this room will know a lot more about manifolds and topology and differential geometry than I do. But just to kind of fix ideas here, you know, we're interested in dissipative dynamical systems. We're interested in Navier-Stokes, for example. So even though it's For example, so even though it's formally infinite-dimensional, we expect that at long times the dynamics will collapse down onto some invariant manifold with a finite number of dimensions. It still might be fairly large, but nevertheless, this is the picture that we're thinking about. That we have systems where you have dissipation, viscosity diffusion, so short wavelengths get damped very rapidly. And if you wait a little while, then the dynamic If you wait a little while, then the dynamics in state space will live on some manifold that's got many fewer degrees of freedom than the original system. And this is just a little movie. So this is just an ensemble of initial conditions approaching a limit cycle. So the long time dynamics of this live in a one-dimensional manifold. Unless the data is very simple, we don't know the dimension of that manifold. The dimension of that manifold in advance. And so, actually, the last part of my talk will tie into some of Richard's work on how you might estimate the dimension of the manifold. And again, so I was pleased that you mentioned the manifold hypothesis. I mean, that really is underlying everything that I'll talk about here today. And in this case, we're really convinced that it's true because we have data from dynamical systems that are dissipated, and we really do expect that there's a finite. Really, do expect that there's a finite-dimensional manifold where the data live. That manifold could be complicated, and so I'll talk a little bit about approaches that are useful if you have a manifold that's a little bit complicated. But this is kind of the general setup. Melanie talked on Tuesday, maybe, about symmetries. And so, this is just to kind of reinforce. To kind of reinforce the idea that she had already presented, is that exploiting symmetry allows more effective use of data, and we're going to take advantage of that. So this example here, this is for engineers because we would like to work in dimensionless quantities because physical laws should be independent of the dimensions that you measure them in. And that's essentially a form of dilation symmetry. And so this is just an example. Symmetry. And so, this is just an example that will be familiar to an undergraduate engineering, a mechanical or chemical engineering student. We can represent the drag coefficient for flow around a sphere over an enormous range of Reynolds numbers. And we can only do that because you can take measurements with different systems in different laboratories and then use dimensional analysis to collapse that data. So, we won't talk about that here. What I'll talk about here is more geometric invariances. So, if we have a periodic domain, So, if we have a periodic domain, if we have translation or rotation invariance, then we can. So, one way to deal with that is to develop a neural network structure that is equivariant or invariant, depending on the application. What we've done for translation and variance is actually just factor out a phase variable and then separately evolve the dynamics of the pattern and the dynamics of a phase. And I'll show an example of that in a little bit. And then, of course, discrete symmetries. And then, of course, discrete symmetries allow you to basically think about what's going on in large regions of state space, and they're all related to one another through symmetry operations. And that again, I'll talk very briefly about an example that we've worked on here. Okay, and so the kind of manifesto for what we're going to do, this isn't unique to us. There are other people pursuing the same ideas, Yanis Kebricidis, Petrus Guinsi. Ideas, Yannis Kevrikidis, Petrus Kunsakis, George Haller, and others. So the idea is to take a data set from a complex dynamical system. We want to find the representation for the manifold, invariant manifold where the long-time dynamics live. And we're going to do that with auto-encoder architectures. And then we want to find a dynamical system for the system. For the behavior on that manifold. And we're going to use the neural ODE approach for that. And I'll talk about that more specifically. We will, so one thing that I want to emphasize is that the dynamical systems that we're going to care about are ODEs or PDEs, so they're Markovans. Are Markovian. So, if we have the initial conditions, we don't need past information to move forward with time. And we're going to stick with that. Our representations, our models will be Markovian models. Exploit symmetries, I won't have time to talk about control today, but I'll talk a little bit about treatments. And just for context, so there are lots of people looking at machine learning. At machine learning for data-driven prediction and dynamical systems. Actually, so Bethany talked a little bit yesterday about Koopman operator-based approaches. So you get linearity and trade it for infinite dimensionality in this case. And for many, many problems, particularly problems on low-dimensional manifolds, periodic, quasi-periodic dynamics, these have been quite successful. But as we had a discussion, Successful, but as we had a discussion about yesterday for chaotic systems, these have not been widely successful, and that's because they're linear. So you'll never get sensitive dependency on initial conditions, you'll never get a catalyc attractor out of a linear finite-dimensional approach. Nevertheless, they have their purposes. And actually, one thing in particular is that people like Kuben-based. Is that people like Kuben-based methods for control because there's all sorts of control theory and algorithms for linear models? So that's a big application of those approaches. There are approaches. So Cindy is kind of an example of these approaches, but there's a wide variety of approaches where you have a dynamical system. There's not necessarily a dimension reduction aspect to this, but you have a dynamical system. This, but you have a dynamical system, and you look for a highly structured form for the right-hand side of that dynamical system. You make it a polynomial, for example. Or with PDEs, you have a Laplacian term there, a bi-laplacian term, and various non-linearities, and you put coefficients in there, and you do something like sparse regression based on data to find out what those coefficients are. Those approaches generally rely on state-and-time derivative data. We don't want to have to have. Time derivative data. We don't want to have to have time derivative data. You can certainly get around the time derivative data. But you have kind of a limited pre-specified structure. And then there's not generally an explicit dimension reduction aspect to this. And then the other approach that I think is really important to mention and hasn't been a big topic here, though it has been mentioned. So there are approaches actually should. This is a slide that I built, that I made before Transformers. Made before transformers. But there are all sorts of non-Markovian approaches to time series prediction in dynamical systems. So LSTMs, long-short-term memory networks, reservoir computing, these have been very highly effective for time series prediction. But they're not Markovian. They use past, they all have a memory, they have past data. And so, in that sense, they're not. They're not reduced dimension because that history is part of the, if you think about the state space of the system here, the history is part of the state. So these aren't reduced dimensional. They're actually generally higher dimensional than the original state. I think that's actually one of the reasons they work so well is that there's a lot of redundancy actually in those representations. One thing that I would love is if we could get Markovian representations, and in particular reduced order Markovian representations. And in particular, reduced-order Markovian representations to be competitive with these, because that's people are spending a lot of time on those right now. All right, and so just to kind of remind you, so the approach that we want to do, we want an accurate reduced space state representation. We want it to be Markovian. We don't want to have to have time derivatives. We would like to have data that could be spaced some distance apart. Okay? All right. All right, so the basic framework that I'll talk about, I've already described, and many people in the audience will be quite familiar with this. So, our data lives in D dimensions, so u is the data in the original state space. We want to find a mapping to some lower dimensional representation, and then a mapping back to the original state space. And so that's an autoencoder. We have two neural networks here. We have two neural networks here: one that learns this coordinate transformation and the other that learns the inverse coordinate transformation. Okay? We haven't done anything too fancy, honestly, here, except maybe recently, and I'll talk about that at the end of my talk. Just a couple points. For a lot of data, it can be advantageous to just do a coordinate transformation work in PCA basis. And this is also. And this is also where we can do symmetry reduction, and for example, split out a spatial phase from a pattern. And then, so we have now, and then the question, an important question is, well, how many dimensions should H have? All right, and that there's for systems that aren't very low-dimensional, the answer to that isn't trivial. Generally, still, I don't think there's a robust way of answering that question. That question, except sort of experimentally. Okay, in any case, so we have our data now. We can think of our data in a reduced representation. And then we would like to find the dynamical system for the evolution of the data on this invariant manifold. And we're going to use the neural ODE approach. So we're just going to take G to be a neural network. And we're going to learn the parameters in that neural network from the data. From the data. So we have data at certain time intervals. We're going to make a prediction. And then to find the gradient with respect to this right-hand side, you can either think of this as an ODE constrained optimization problem, and the classical way to approach that would be to use an adjoint method. But also now you can just do automatic differentiation through an ODE solver. Differentiation through an ODE solver as well, if you have that capability available to you, which we do now in general. We've tried both of those, they both work. They have different advantages and disadvantages just in terms of memory usage and things like that. But we've done both of those. One issue here, and this again comes back to something Richard talked about: is if there are unimportant Unimportant dimensions, then your data doesn't tell you anything about what's going on in those unimportant dimensions. So if you keep too many degrees of freedom here, for example, or if you try this approach without doing any dimension reduction, then this neural operator approach can be unstable basically because you have small error terms in your neural networks, you integrate long enough, they just grow, and you have this bias will lead to errors at long time. To errors at long time. And so, what we do, sometimes we need this, sometimes we don't. Actually, the dimension reduction takes care of a lot of that problem because it gets rid of the dimensions that you don't care about. But we can just add an explicit dissipative term, like minus alpha h, to the right-hand side. And that works quite well. And that's this comment here. Okay? All right, so that's the basic framework we're interested in. We're interested in. So let me just show an example. And I actually put this back in. I had taken this out of the talk, but people were talking about Kermoto-Sibyshinsky. So I have some data on Kermit-Sibyshinsky. So everybody likes that problem. So I thought I'd talk about it a little bit. So we know the Kermoto-Sibyshinsky equation. It's a nice PDE. It's got chaotic dynamics, long-time solutions that are proven to live on a finite-dimensional inertial manifold. Inertial manifold for this system. And so this is just the space-time plot of the solution. And so this was our initial data set, periodic boundary conditions. And then L here, the domain size kind of sets the complexity of the results, of the dynamics. And so here we did L equals 22 because that's something that's been widely studied. Widely studied. And so the first question is: how many dimensions? And so if you just do PCA on a data set here, you get no information about that. So this is just mean squared error on test data versus the number of dimensions you keep. PCA, this is a log plot. You'll just get exponential. You get linear on a log scale. For PCA, you get no information about how many dimensions. And that's because the manifold is not flat in this case. Flat in this case. So if you use an autoencoder, then you can actually get that there's a substantial drop. You go from 7 to 8 here. Sorry, 8 to 8 to, this is with the phase, sorry, this is with the phase factored out. So that's why this drop is at 7 here. There's the additional degree of freedom as a phase variable. And so you get a drop at 8. So here we're pretty confident. So here we're pretty confident that the dimension of the manifold is 8. You can do the same thing for larger domains and make this estimate based on loss versus the number of dimensions that you keep and get results from here. This, as it turns out, is not robust. It works. We got lucky, right? We chose KSE with these parameters and it just so happened that this works. If we had chosen KSE with a bigger domain, Chosen KSC in a bigger domain, it wouldn't work as well. So, this method for estimating manifold dimension is not a generally useful approach. We got lucky this first time. So, the end of my talk is motivating how to do better on that. Okay, and then so we can now make this eight-dimensional neural ODE model. We've done this, in this case, this is data that's 10 time units apart. And a Lyapunov time, in this case, And a Lyapunov time in this case is 20. So this is data that's half a Lyapunov time apart. So not closely spaced. And so here's the data, here's the model. This is just the difference between the two. And actually, so you can see the error. Most of the error here is phase. So the phase of the pattern just drifts a little bit. And that's the initial error that you see. All right, and so we can get quantitative agreement. So here, here's a Lyapunov time. Here's a Lyapunov time. We're going to get very good results for about a Lyapunov time. You know, maybe a little bit. Okay? So that's encouraging. So that's short-time predictions. What you would also like is that your data-driven model will capture the long-time statistics, right? The shape of the attractor. And so if you look at that case, so here we've looked at the joint probability density of the first and second derivatives. First and second derivatives. The motivation for that is: if you're thinking from the point of view of things like fluid mechanics, ux in this case is the energy input to the system, and uxx is the energy dissipation to the system. Those quantities squared integrated over the domain. So that's why we chose those particular quantities to do the joint PDFs of. So here's the data for different size domains. It's got a nice pretty PDF. And this is the model. And this is the model. So we've just run, we've just run our, in this case, eight, I think this is 18, this is 28 dimensional models for tens of thousands of time units. And so the data is staying on the attractor. You get a very good representation of a long time dynamics. And then we move to a much more complicated situation, turbulent Couette flow. So Couette flow is just flow between parallel. Flow between parallel plates. It's really the simplest situation where you can see turbulent flow. The reason we care about it is that the structure that shows up in the plain Couette case is you have these streamwise vortices that are sort of wavy. And that structure turns out to be universal and turbulent flows near walls. And let's see if I can get my movies to work. There. Let's see if I can get my movies to work. There. So, this is pipe flow at very high Reynolds number, very high flow rate. And you see these are surfaces of constant streamwise velocity. So, blue is low speed, red is high speed. Oops, sorry. And then this movie, blue is low speed, red is high speed. And you see the same kind of plume structure here. So, this is kind of the simplest real turbulence. And even in this case, this is a box, this is a minimal box. This is a box, this is a minimal box, still 10 to the fifth dimensional state space. So, if we apply these same tools, we can guess what the dimension of the state space is basically by a combination of looking at the autocoder and the time prediction. Let me not dwell on this slide, but the punchline is that we can find good short-time tracking. This is just the Tracking, this is just the same initial condition from the direct simulation and the data-driven manifold dynamics model. You can kind of go, you can, it'll go through kind of some of the key dynamical transitions that you see transiently here. This is the evolution of the phase variable. So, these are periodic boundary conditions. So, this pair of vortices, it actually diffuses. So, the position of that diffuses. And so, this is the evolution of the phase variable at Z versus time. And this is the Versus time, and this is the model. So, actually, if you think about a diffusion coefficient, they're in quantitative agreement. And then for long-time statistics, we actually made a comparison with basically taking the governing equations, projecting onto PCA modes. In fluid mechanics, PCA is POD, proper orthogonal decomposition. And even with a thousand modes, you do very poorly. And it turns out that we can do, as long as we use more than about 15. Than about 15 dimensions in our latent space, we can quantitatively capture what's going on in the long time statistics. So these are the velocity fluctuations, these are the mode amplitudes, and these are the leading Lyapunov exponents as a function of the number of degrees of freedom here. Okay, so the method works quite well for this simplified system. And one thing that I find really interesting is that I find really interesting is that the dynamics of this system, which takes 10 to the 5th degrees of freedom to represent, you actually, the actual dimension where all these dynamics lives is somewhere less than 20. So high, but not enormously high. That's kind of promising for just thinking about models of what's going on near the wall and arbitrary complex flows. And actually, people in fluid mechanics care a lot about that because it's the momentum transport to the walls. The momentum transport to the walls, that's the drag. So you always have to care about what's going on in the walls at some point. All right, and then this is just movies showing the two cases and essentially their indistinguishable. All right, so there's kind of the basic framework that we've got. It works well on these systems. So let's think about other Other approaches. And in particular, I sort of glossed over the fact that, leave it here for now. So we know from topology that you can have an n-dimensional manifold, which means locally we can represent that with n Cartesian coordinates. But globally, it could be much more complicated. And in particular, you might, if you want to represent that monolith. To represent that monolithically, you can need as many as twice as many dimensions. So that's Whitney's embedding theorem. And so you can say, okay, maybe if I have a complicated manifold, it's okay I just lose a factor of two in my dimension representation. I have to use twice as many degrees of freedom as I would if I used the local intrinsic dimension of the manifold. But I think it's worth asking the question: well, what if we can represent things? Well, what if we can represent things in the intrinsic dimension of the manifold? And so, here, again, you guys are mathematicians, and so you know this stuff much more than that much better than I do, right? But here's a torus, it's a 2D manifold, but to represent it globally, you need to embed it in 3D. And so what we did was basically use this classical idea of charts analysis to find overlapping local representations of the data. Of the data. And we did this, this was very naive. We did it by k-means clustering. So we find a bunch of clusters, and then you have to expand the clusters out so that they overlap. And so you find a local, use autoencoders, you find a local representation of the data. You do that for each cluster, and then you find local representations for the dynamics. And as long as your charts overlap enough, then you can move. Overlap enough, then you can move smoothly from one to the other. And so here, you know, we just train neural networks for the representation and the dynamics in the loose space. So why complicate our lives this way? Well, one reason is that if we really want to do this in the minimal number of dimensions possible, then you need to do this. The second one is if you have systems with complex dynamics, if you have inner. Complex dynamics, if you have intermittency, you have heteroclinic behavior, I'm going to show an example of that, then these approaches may be advantageous. And then the last point is that this is a natural way of thinking about systems with symmetries. So if we have discrete symmetries, that means that everything can be mapped to one little piece of state space. So what I really only need is I need a chart for that piece of state space, and then it has to overlap. And then it has to overlap, but then I get everything as long as I've represented that one chart effectively. So we have a very new paper on archive about that, but I'm not going to talk about that today. And then we came up with a name for this approach, Charts Analysis for Nonlinear Data-Driven Dynamics on Manifolds. How long did that take? Can I? No, this was a little brainstorming late at night. So the problem with this. So, the problem with this is, I was just super, I was so pleased with myself. So, it turns out, so we submitted this first paper to Nature Machine Intelligence, and they wouldn't let us use this term. They wouldn't even let us use this term in the GitHub repo. And it's because Candyman, there's a horror movie Candyman, Candyman also means drug dealer. So physical review fluids was fine. Review fluids was fine. So we stuck with this. Okay, so here's a couple examples of this approach. So one example was just to take a reaction diffusion model that has spiral waves. So you like spiral waves, right, Jason? So this is a limit cycle. So the invariant manifold is one-dimensional. And you need two charts to cover a one-dimensional manifold, to cover a circle. A circle. We use three. I don't know why my postdoc. So there's the picture. So we have local representations. They're all one-dimensional, and then they have to overlap. And here's the ground truth. And here we just picked that point and looked at the dynamics. Oh, sorry, this isn't cycling. So there's an approach. There's an approach that had been claimed as being minimal-dimensional. And if you use 2, it's fine, because that's the embedding dimension for a circle, so no problem. But the intrinsic dimension of the circle is 1, and if you try this other approach with 1 dimension, it has to glitch because there's not a smooth one-dimensional representation of this. If you just think about polar coordinates, there's going to be a discontinuity. But in our case, because we split it up into local 1D representation, Because we split it up into local 1D representations and just piece them together, it works beautifully. All right, so that's kind of honestly kind of a trivial example, except that the ambient dimension here is quite high. It's 100 squared. So it's one-dimensional dynamics embedded in 10,000 dimensions. A more interesting example again comes back from the Kirmoto-Sivoshinsky equation. So there's a region in parameter space there where you get this very intricate, nearly heterogeneous. Nearly heteroclinic behavior. So there's a saddle point here and a saddle point here, and heteroclinic connections between them. And this is what the dynamics look like. So it'll sit there for a while, and then you go across one of these nearly heteroclinic orbits to the other saddle point. You sit there for a while, and you go back, you go back chaotically between these. So there's a very delicate state-space structure. And if we do an atlas in 3D with six charts, so that the color With six charts, so that the color coding is the charts here. So, one chart for each, and actually, we didn't choose this. K-means clustering chose this. One chart for each of the regions near the saddle points, and then one chart each for the heteroclinic connections. Then we can very nearly capture those same dynamics. And this is the time evolution in the candyman representation. We're getting the period wrong, and the reason we get the period wrong is any trajectory that approaches a Wrong is any trajectory that approaches a saddle point. If you approach a little closer, the period gets much longer. The period approaches infinity. So there's a strong sensitivity to the period. You have a lot of error in the period if you have a small error in how close you come in to the stable manifold of the silo clone. So that's the source of that error, but we get the shape of the attractor right. And if you try to do this with a one-chart method, even if Method, even if you reduce to six dimensions, it really fails quite miserably. So these delicate dynamics, and I should mention, so this is a three-dimensional manifold in 64 or 128 dimensions. I can't remember how many mesh points we used for Kermodo-Sibyshinsky. So this is just a very tiny thing, right, in the ambient space of the system. And if you try to represent that with a one. And if you try to represent that with a one-chart monolithic representation, we were not able to successfully do that. You might have an inversion instead of embedding. Well, we know, maybe we can talk about that later, but we can do it with Candyman 3D. So, Whitney says we should be able to embed that manifold in 6D. There should be a coordinate representation in 60, right? Well, I was thinking of. Well, I was thinking of, I mean, if I remember correctly, this kind of embedding theorem, ultimately, it should be something like a Nash embedding theorem to guarantee that your embedding will result in a smooth metaphor inside of an immersion. A metaphor that can cross. Yeah, maybe it's 2n plus 1 to guarantee smoothness. Something like that. I don't know, but in any case, doesn't work. And we can get it worked in three. So, yeah, I think. Yeah, I can check on that, but I'm pretty sure we're okay with six here. And so that's another motivation for using this approach where we split things up into charts and atlases. I have one more example of a system with intermittent behavior. I'm running a little low on time, so I think I'm going to skip that because I wanted to specifically get to the last bit that's connected with Richard's talk. And so, right, how many? Right, how many dimensions? This is a big question if you're trying to come up with these low-dimensional models. What's the right number of dimensions? And so I mentioned in that Kermoto-Sivishinsky example that if we just looked at auto-encoders and looked at mean squared error as a functional dimension, in that case, there was a nice drop. In general, you won't see that. Even with KS Kiramoto-Savishinsky for a larger domain, that drop, as the domain gets larger, That drop, as the domain gets larger, that drop gets smaller and smaller and becomes indistinguishable. And so, what we ran across was this very interesting idea of taking an autoencoder architecture and adding this so-called implicit regularization with linear layers, with just matrix multiply blocks in the middle. All right, and actually, Jan Le Pen's group published on this. Group published on this in 2020, and actually, they did MNIST with that. And so, we adopted that approach to see how that would work. It turns out that for robustness purposes, it's actually fairly important to add weight decay. And so that's all we've done, you know, beyond what was done in the original case here. And the basic idea was, of course, this shouldn't change, if everything's converged, this shouldn't change things at all. Things at all. But the fact of the matter is that the addition of the linear layers changes the gradient descent dynamics. And of course, we just heard about that in a related context. And then so the idea is that you find a coordinate representation where the rank of the covariance is low. And that rank, that's the number of dimensions in your manifold embedding dimension. If you do it in a monolithic. In a monolithic way. And then, if you have that, well, then you can just do PCA, and then you have an orthogonal coordinate system to do your dynamics in. So this is very, very appealing. And so we found some success with this. And so again, just coming back to Kirbato-Sivishinsky. So here, this is the covariance of the data in the latent space. I don't have the parameters here, but this is something like weight to k of 10 to the minus 6, four linear layers, or six linear layers. And what you see is: so, if you do, if you just use a standard autoencoder and look at the singular values, and this is with eight dimensions, you don't see much of the decay in the singular values. PCA is not any different, really. But this autoencoder with the linear layer. Autoencoder with the linear layers, now you see this 12-order magnitude drop when you go beyond 8. So, coming back to what's the dimension of the invariant manifold, and the mean squared error is applied to the mean squared error as well. So, we can be fairly confident that it's 8 in this case. And also, in this case, it's quite robust. If we do it again with a different piece in the attractor, different initial weights, get the same answer. And so that we can just do this with Kermoto-Sivishinsky and many more dimensions where the mean squared error test is not helpful at all once you get to large domain sizes here. But again here, so this is for the domain size of 66. This is the same plot as before, just singular values of the covariance versus position. Versus position versus index. Again, we see this very sharp decrease. And so we can make fairly precise estimates of the dimension of the invariant manifold as a function of domain size here. And what's really interesting is that in this case, it's linear. And there are physical arguments and then some very complicated computations that involve looking at the unstable manifolds of unstable periodic orbits in these systems. Orbits in these systems. So Fredrik Statanovich has done really remarkable work, really from a dynamical systems point of view. Here, there's no dynamical systems here. We collect the data set, run it through this autoencoder, look at the rank of the covariance, and it gives us these numbers. So we find that to be really promising for actually finding the dimensions of manifolds in complex systems. And then, of course, you can. And then, of course, you can do the time evolution now in these coordinates. And we've done that as well. And that's what I'm showing here. This is just the time evolution of the latent variables, eight latent variables here. And so this is the data, this is the prediction, and they're actually extremely close for multiple the up and off times. All right. Okay, so let me just wrap up. Just thinking about what's Just thinking about what's next. So, you know, I'm interested in systems with millions of degrees of freedom. I'm interested in very high-dimensional systems and making minimal dimensional representations in those cases. We would like to have longer prediction horizons. We know that state-of-the-art, if you put no constraints on the problem, these non-Markovian methods, can in some cases lead to prediction horizons that are many often off times, which is really quite remote. Times, which is really quite remarkable. We're interested in, and again, coming back to fluid mechanics, if you're thinking about turbulent flows, you're interested in systems with dynamics that are over wide ranges of time and length scales. And we're starting to look at that basically with kind of a hierarchical structure of new stored models. What else do I want to say? One thing that I've I want to say one thing that I'm really interested in is, and this is why I was asking you the question, Richard, about number of linear layers. So, for Kermo to Sivoshinski, this IRMO weight decay method, it works beautifully. But if we go to that Navier-Stokes example, the turbulent example, we'll always get dimensions that are less than about 40. But sometimes they're 20, you know, sometimes they're 30. And so we have, I think, a good estimate of an upper bound. Estimate of an upper bound, but you would like to run it with different initializations of different data sets and get the same answer each time. And for these much more complicated systems, we're not at the point of doing that. And then I'm also interested in some cases where there are other interesting symmetries. And one of those, so in complex fluids, there's a symmetry called material frame indifference, which basically means that the microstructure doesn't care about rotations. Doesn't care about rotations of the fluid. Rotations don't, if inertia is not important, there's no centrifugal forces, you take your microstructure, your oriented molecules, and if you, if you, the rotational part of the velocity gradient shouldn't change the relative positions of any of those molecules. And so that's a dynamical symmetry that we, in one example, we were able to build in basically by working in a corridor. By working in a co-rotating reference frame. Anyway, I will wrap up with that, and I'm happy to answer any questions. Are there any questions? Yes, so which P do you use in your weight decay? One? Which P, the norm? Two norm. These are all two norms. We've played with other ones because that's been suggested in the literature. That's been suggested in the literature didn't work any better. What we've done that's shown some success is we have a system with multiple branches and each branch has a different number of linear layers. So we do a forward pass and then look at what branch gave us the lowest mean squared error and then we backpropagate through that. And so basically you're sampling with kind of a Boltzmann weight. So use a Boltzmann weight on the mean squared error. Use a Boltzmann weight on the mean squared error to decide which one will backpropagate through. So you have this kind of random step. And I have no theory for that. We just kind of cooked it up one day at a group meeting. But it seems to lead to better robustness than just the straightforward approach. So you've touched a little bit on symmetry already. I mean, in general, your full-order model might have beautiful geometric structure, and then a reduced order model might have Auto model, but I've lost all of that stripe because you've just thrown it through your autoencoder that doesn't know about symmetry. And maybe to have it symmetric, you would even have a bit of redundancy there. So are you trying to address that? So the cases that we are interested in, generally, we know what the symmetries are. So these aren't, you know, these are dissipator systems. There's no Hamiltonian, no conserved Hamiltonian. And no conserved Hamiltonian for these or anything like that. And so the symmetries tend to be spatial. So we know we have contiguous translation symmetries in x and z directions, for example. And then there are a number of discrete symmetries for reflections. But there's a set of them that we know. And actually, one of the approaches. Here, this. Hear this picture here. So, this is for a Navy-Stokes example called Komogorov flow, where you just have a sinusoidal forcing. There's a continuous symmetry in one direction, and then a number of discrete symmetries. And actually, in that case, you can apply those discrete symmetries to map everything from eight different charts basically down to one. And we know those. So, if we So, if we, you know, from just kind of from the geometry of the system, there are discrete and continuous symmetries that we know. If there are hidden symmetries, those will still be reflected in the data. And the idea of this approach is to not throw anything away that's not necessary. We just want to throw away the directions that are off the invariant manifold. So, if there is symmetry that we haven't figured out, it's still there in the reduced representation. In the reduced representation. And how many are alter encoder might only preserve that symmetry approximately right? Oh, yeah, absolutely. That's right. So right now, because we know the symmetries, we preserve them exactly. I have not understood how you deal with the one for structure of the reduced problem. So you So you you make a lot of effort to to have this money for figure out this money for and then you have a method that you have to or a month. Right. Yeah, no, that's a great question. We haven't done anything special. So what we're assuming is that what is that this is giving us reasonable coordinates. And then we don't exploit a metric on the manifold or anything. You know, a metric on the manifold or anything like that. And these, you know, I'm not quite sure how to think about these. These are systems that are quite different than Hamiltonian-Lagrangian systems. There aren't any conserved quantities here. These are dissipative systems. I mean, they're dissipative even on the invariant manifold. But I mean, if people have, and then the coordinate representation is a neural network. So start thinking about derivatives of that coordinate representation. Derivatives of that coordinate representation, I'm going to get very nervous. But that's a great point. I would love to hear about how you can exploit the manifold structure. We haven't done that. Just to follow up on that, you never said this, but it's probably obvious that this chi that you have in your now is a submerged and it's going to be to submerge the manifold inside the bigger space sort of, yeah. So you know something about the derivative mappings and. You know something about the derivative mappings and so on? Well, I mean, we don't use any derivative information. So this, we're not putting smoothness conditions, for example, on this. We have, you know, if it's a ReLU network, then it's piecewise linear. But you do have to transform the vector field from the big to the small space, right? We don't worry about the vector field in the ambient space at all. We have data in the ambient space. We have data in the ambient space. It's time series data. It's a vector field in the small, right? What's that? You learn the vector field. We learn the vector field in the small. In fact, right from this, right? So we basically have data, a certain number of time units apart. And in the reduced space, our aim is to learn the ODE that'll reduce the error basically going between our data points. Basically, going between our data points. But we never, we're not assuming we know anything about a vector field. Basically, that would be knowing physics. That would be a different thing. That would be a physics informed neural networks. We have a data set. That's all we have. And then we've played with putting in like integral constraints on energy, and those help actually in robustness, but we haven't. This is not a PINS approach at all. This is really very much a data-driven approach. Any other questions? Okay, then if not, let's thank Michael again. So, you know the schedule, but we have a long break for lunch. I think we're back to 1.30 seconds.