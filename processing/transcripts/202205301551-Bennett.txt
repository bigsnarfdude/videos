Eight minutes, right? Because we have two talks. We have four talks. Got it. So for talking about health systems as opposed to outpatient, we're talking about care delivery. So generally the people are coming in when they're ill. It's not so much. There is a longitudinal record that we try to follow, but that record is often distributed among several health systems because people don't go to the same health system. The stakeholders in there are the patients who care about. Are the patients who care about their own health? How much does it cost? Can I get to the doctor? Some look at the provider reputation when they make their decisions. Healthcare providers, they earnestly do want to deliver high-quality care, and that's what motivated them to enter the field. They also, of course, have to worry about reimbursement, how many hours they spend. They also do care about the science of the thing and advancing the field. Management is just trying to, you know, again, having good motivations, just trying to keep the thing running. Of course, they have to worry about the bottom line, compliance with the government. Bottom line, compliance with the government, their reputation, which is driving a lot of crazy things nowadays, and other kinds of things like what we do research. IRB, I highlighted because of our studies, this collective group studies, the government looking after cost, effectiveness, and quality. So what we're trying to achieve in the health system is deliver the right care to the right person at the right time. And we do a couple of activities like diagnosing what the person is usually is often coming in ill. Usually, it is often coming in ill with a problem. There's some reason why they're coming to us. So, what is it they have? What's their diagnosis? What's the differential diagnosis picking that? Starting treatment. A little bit different is intensive support. We've seen a little bit about mental aid when they go to the ICU. It's a little bit different than when you just see the emergency room and deliver a therapy and they can go home. And then monitoring of maintenance, both in the hospital and when they go home, and then some amount of prevention, which we'll maybe hear more of in the other talks. Of in the other talks. So, what we're going to do is, I'm going to go through some initiatives and the kinds of projects we do within the health system related to informatics, and then Tell will give his view from a little bit more on a Colorado focus. So, learning health system is one way to look at what we're trying to achieve in this cycle of going around and gathering information, learning from those information, treating people, getting new information, trying to learn from that and continuously improve. Improve. We had something called the Informatics Intervention Research Collaboration, which was our attempt to bring informatics and give examples of projects reducing raw orders, improving documentation, question answering, and things like that. So that initiative led to an initiative that we hope would grow into what we called health practice research. This is kind of where what you'll see tomorrow about healthcare process models biases in the record kind of came out of. In the record, kind of came out of this. Let me not speak to that because we're short on time. I'd rather show you the examples. Our department actually has a service role. We get $2.4 million a year from the hospital to run systems for the organization. And some of what we do is line item stuff like menial stuff, kind of menial IT stuff that we do because we want to be in the game. And that allows us to use this as our living laboratory. And there's a list of responsibilities, but rather than read. Responsibilities, but rather than read that, I'll just go through the examples. We built what was our clinical information system, which is now replaced by a system called EPRIC, which is a vendor system. But we had built what was being used to display and capture information in the medical center. Spent a lot of time what was referred to earlier as the mediocre systems to catch silly errors. So that's kind of some of this is here. Kind of some of this is here and also supporting electronic documentation, med reconciliation. Like this is one of those disasters where what we needed to do is achieve like 100% med reconciliation so doctors double check what they put in because it was decided that this wasn't going in right. So we built the systems that achieved what we needed, which was like 99% compliance with medication reconciliation for our metric. I think reconciliation, in fact, went down with this new system, but we had checkboxes that proved that we did it 99% of the time, even though we probably dropped to 80% from 85% wherever we were, because what we needed was the metric to show that it was then got distributed everywhere. So you have to be careful what you wish for. We weren't really, you know, that's because we didn't go in and see what are people really doing, what do they really need. If what we needed was a 99% answer. Harvest, we actually know Amy already. This is No Amy's system that she put into practice. So we are putting things into actual practice. This summarizing the record. So when you're in the emergency room, someone comes in, they have a chart a mile long, this summarizes for you and lets you narrow in on different things. Instead of like one part of what Noemi had done with Kartik Nadarajan was search for stuff. But this is, if you don't know what to search for, tell me what's important with this patient. So that was a system that we put into production. This is the CONCERN project. This is the Concern project. So, Kenrick and Sarah Zetti working on figuring out who needs acute care based on, you know, the fun part of it is it senses from the notes when people are worried about the patient. They're not saying I'm worried, they're saying things that imply that they're worried, and then we use that information in addition to the other information to. Are you going to talk about this system later on? So, I don't want to go too much. Yeah, I'm going to talk about it on Thursday. Engaging patients in their care. Engaging patients in their care, I tried to stop soon. Engaging patients in their care, some of the most isolated patients are the ones that are surrounded by a team of healthcare providers. They're confused because they're not in their home, so they don't understand where they are. And the team is not their family. And so David Vaudry did an intervention where we helped them get more engaged to know who is on their team, what's going on with them, and did uncover some patient errors that the patient said, like, why are you giving me this? And they're like, oh, no. Giving me this? And they're like, oh no, we shouldn't be giving you that. Sorry about that. One in 200 notes in the patient record are on the wrong patient, believe it or not. And that also goes for ordering. So this was an intervention to improve, to reduce the amount of wrong patient ordering. Actually, I'm going to stop there because I want to hand it over to Tal so we can do all four talks. So I'm going to stop there. I can. I can unplug. Let's turn it over to Tell to go on Zoom. Are you ready there, Tell? If I can screen share. There you are. Alright, some of us are here. Good to be here. Nice to hear you. Yeah. Good to hear all y'all. Wait to hear all y'all. I've been listening most of the day. That's not very good. Okay. There we go. Yeah. So we're good? Oh, that is. Oh, yeah. It goes. Sorry, I couldn't read that. Looks okay. Looks great. You're great. Alright. Terrific. Sorry, I couldn't be there in person. I hadn't been able to shake a gold and didn't want to. Shake and cold and didn't want to get into all of you all. But I am a pediatric ICU doc. I am also an informaticist. I have a couple of campus informatics leadership roles in the section that David Melica and others are in. And we can talk more about that another day if that's desirable. I study things that make sense when you hear that I'm an ICU doc, so I put kids on mechanical ventilators and Kids on mechanical ventilators and ECMO when I'm in there. So I studied respiratory failure, sepsis and aurium dysfunction, traumatic brain injury, COVID, you know, with certainly in our wheelhouse, and then sort of the concumbers and informatics and data science tools to go along with those things. And the ICU is a crossroads, and so we touch basically all of the other clinical domains when they are at their most acute. So I'm going to just tell a couple of stories as a lead-in to thinking about implementation and deployment of tools in health system VHRs. So everybody now in the space has heard of sepsis. Its definition changed in 2016, at least for adults. And so it is now functionally defined as suspected or proving infection leading to an organ injury. This is the so-called sepsis III definition. And you see how 3D definition. And UC Health is our on-campus adult health system, University of Colorado Health. And they attempted to predict sepsis in order to improve outcomes. And so in their first attempt, you know, they had a deterioration score that was already inbuilt into the EHR, and they said, oh, this is great. The score, if we dissurface it to clinicians and maybe generate some alerts, we're going to improve outcomes. And so they. Improve outcomes. And so they showed the scores first on non-intensive care units and then they generated some alerts and they also had some color coding. I'll show you what that looks like in a second. But all in all, a sort of build it and they will come approach, which unfortunately Informatics has a habit of doing over time. This is going to be amazing if we build it. Give us a bunch of money and it'll get a ton of use. And then not infrequently. The big initiatives don't necessarily draw the attention that the people who were excited about it thought it would draw. And so, this is what this initially looked like on the unit monitor of patients. This is rank ordered from sort of the person with the highest score to the person with the lowest score. Names have been grayed out. This happened at Bern ICU in a later implementation. And you can kind of get a sense of, you know, these are the patients maybe that most need. These are the patients maybe that most need to be worried about. And so, were they affected at that site? They were not at all. And so, UC Health did the right thing, kind of looking for that, why did this work? And the reason that this didn't work, like the 17 things hanging around this charge nurse's neck, she is too busy, too many things on her plate to be able to do the deep chart dive that would need to go along with kind of why is that. Need to go along with why is that patient's number red. So the scores were displaying appropriately, but there was no change in the identified count of sepsis cases, IC transfers, or mortality, no change in the speed of placing orders for things like IV fluids and blood cultures or antibiotics. These are all things that have data associating them with better outcomes in the context of substance. Context of substance. And the summary themes when they talked to the clinicians is that they were just too busy to learn. They had chosen a really high sensitivity level so that they didn't miss any. And so it led to 61 alerts on each unit, a unit, part of a hospital, a unit, part of a floor, every day. And only one or two of those were real deteriorations. And so the challenges that this surfaced for them were that. That this surfaced for them were that this was a real low signal-to-noise approach, and also roughly 30 to 1, they got every deterioration. That's great. They didn't have any misses. Unfortunately, it led to very quickly bed site to move to tea, so they ignored it. And so they, you know, they said, okay, how can we fix this? Maybe we'll make a dedicated monitor to tea. It turns out it was also designed for deterioration in the next eight hours. For deterioration in the next eight hours. So the alerts came too late, typically, and the clinicians already knew that those patients were deteriorating and had started to do things. And so they said, okay, maybe we'll need to alter the look ahead of time, and they adjusted it back. And so I won't go into this too much in this talk, but actually, UCHelp has successfully deployed a whole virtual health center, which took the form of that remote monitoring team. So people whose job it is to watch UCH. Whose job it is to watch his type of score and others. And this has really been quite successful because then the work of monitoring the scores was offloaded, the frontline clinical teams, especially the nurses, and the virtual health center team really supports and partners with the bedside clinical teams when a patient is identified as having a words and scores and can figure out what's going on and get the right help to that patient. This is all as just sort of a This is all as just sort of a lead-in to what are generally accepted good practices and ideas. If you're going to be deploying a model and here it's labeled to be machine learning for clinical prediction, but any model really, whether it's machine learning based or not. And so, you'll hear echoes of others who have talked today. You'll hear echoes of the HCI elements that Wina talked about and echoes of some of the things that Noreeni talked about. I'll try that. Things that Naomi talked about, and I'll try to highlight those. So, is the model informative? Is it about a known clinical decisional need? Is it information that a clinician doesn't know already? The word clinician is used intentionally here, meaning the whole multidisciplinary team versus docs, RTs, because they all have kind of different antennae up for when a patient might be changing or might need more help. Is it actionable? And George mentioned these as kind of the desirable approaches in informatics are sometimes referred to as the five rights of CDS, of clinical decision support, the right information, the right person, the right format, the right channel, and the right time. Is it accurate? Is it timely? So, is there enough time for docs and others to do things to improve the outcome? Is it interpretable? And there was some discussion. And there was some discussion of explainability and what it meant to different communities earlier today. Is it generalizable to other health systems or just locally? Is it reproducible in real world conditions or just in sort of static data? Will the model inputs be available in real time? And I'll come back to that. And then is it feasible? And I'm going to spend a lot of time talking about that as well. So, one example, just to sort of Just to sort of get across how much effort it is to develop and deploy a tool in a live NHR, like many places, March 2020 came and was really tough. And so our health system was struggling. We said, you know, we don't know if we're going to have to do things like ventilator application. We don't have any predictive tools to help us decide those terrible decisions if we ever have to make them. We really need an accurate, live, dynamic. Live dynamic mortality prediction tool. And so, you know, they came to us and others, and we all partnered to build this and deploy it in six weeks. And it still runs, updating every 15 minutes across the entire 13 hospital system. But it took close partnerships between, this is us, Daniel, the data science team, with the clinical data warehouse teams and the internal UC Health Information Technology team to reach into the EHR. Reach into the EHR to find the necessary variables, some of whom were not the types of variables that come out into data warehouse use. And I think George and I are going to talk more about that tomorrow, but really were specific to individual things that we needed that were thought to be highly important for predicting mortality in this context. The clinical data warehouse team serve the purposes that they normally serve, including data integration and terminology. Data integration and terminology standardization, and then importantly, serving as an honest broker for this purpose, where they took away as much of the identifying information as they could and kind of held it and protected it so that we were operating on not a de-identified data set, but a data set that had less protective health information, there was less risk in patients for us having all this data on short notice. And then we did all the typical data science model. Typical data science model development tasks and evaluation tasks. And then, importantly, we partnered with the people who were coding it back in the live EHR and with the frontline clinicians who were validating what they were seeing as we deployed it so that we could make sure that what we were doing made sense to them and they were in some sense a double check on our work. So, you know, this is really a success story and one that we are building on going forward. So, one of the take-home points of that story and of others' work, in this case at Duke, is that model deployment and especially data engineering are really expensive. And so in this paper from Applied Group Informatics five years ago, they developed and deployed, there's actually two highly related kidney failure prediction models in the EHR. 22 really easy variables, you know, standardized. Really easy variables, standardized things, a pretty simple regression model. And they estimated that it cost them $220,000. And you might say, okay, that's all the expensive data scientists who could be off working at Microsoft or Google. But they also estimated that it would cost $90,000 to deploy that model that's already built. No more data science work at each new site. And that's the cost of the variable engineering and validation. And so these are the kinds of things. And so, you know, these are the kinds of decisions and thought processes that health system leaders have when they are thinking about should we ingest and use this new model that the APEDs at the university across the street developed. You know, how much variable engineering is going to be necessary? How much is that going to cost us? How much benefit are we going to gain or are our patients going to gain? And so, you know, pretty regularly, I must serve new papers. Regularly, I must review papers where there's some 500 variable or input neural network that they want to deploy in a health system. And it has an AUC, they're very excited because their AUC is 0.03 higher than some previously reported model. And I think the health system leaders and tied competitions like these would say it's just not worth it. It's just not worth it to have that marginal. Not worth it to have that marginal increase in accuracy given the engineering cost to do that. And in fact, it turns out that this high benefit of parsimony has shown up before in the Netflix Prize 10 years ago. They never implemented it for their production work. And the quote in the sort of online news article about this sounds exactly like what the health system leaders say. So we evaluated some of the new methods. So we evaluated some of the new methods offline, but the additional accuracy gains that we measured did not seem to justify the engineering effort needed to bring them into a production environment. And so this, I think, this echoes through a lot of our conversations with Health Systems about deployment. Thinking again about what all those model inputs are, I think, Amy mentioned something along these lines earlier today. Something along these lines earlier today. This is a paper or a table from a paper from Brett William Jones last year. And he and his colleagues separated data in the EHR and other data sets that index includes data into sort of non-clinician initiated data. So data that isn't reflecting what clinicians are doing and deciding from clinician initiated data. So really just, you know. Data. So, really, just you know, this is data that the clinicians already know because they are acting on it, and the data there are representing their actions. And so, I do think that it's worth thinking about that this, and I know there are a lot of people in the room who think about physiology quite a bit, you know, this information in some sense is independent of what the clinicians are already doing because it actually gets closer to physiology. And I think it also gets closer to what the more translational. To what the more translational scientists are excited about with biomarkers and things like that, imaging and omics, because it's closer to what's happening with the patient and not just what's going on with the healthcare process. So again, because I know there are a lot of people in the room excited about physiological signals I am as well, I have no financial interest in this company, but it is a great story how it sort of came to be. It sort of came to be. So, this is the so-called hero monitor, which is designed for use in neonatal ICUs and the ICUs where newborn babies get taken care of. And it was developed by a team at the University of Virginia led by a guy named Randall Mormon. And he is a neonatologist and data scientist. And he has been doing this longer than almost any of the other sort of clinician scientists working in this space, and really very successfully. Really varying successfully. And so, some of these principles that he outlined in an article earlier this year, I think, echo with the other sort of elements of successful deployment that I shared with you. But there are a couple that are specific to physiological signals and specific to his approach that are maybe worth discussion. So, with clinical fit, if we detect the problem early, is there anything we can do about it? Can we expect a sub-clinical Can we expect a subclinical prodrome that's detectable early on? So, those seem like they might be the same thing, but they are separable from can we detect it and do we actually have a therapy. Are we recording the right signals? Echoing what Bruce was talking about just a little bit ago. And then are we analyzing those signals in the right way? I know there are a lot of folks in the room who have incredibly sophisticated approaches to do this. And then this next one, ground truth events. This next one, Ground Truth Events, is, I think, a deviation from the norm in terms of how he thinks about establishing his training sets. He is an ardent believer in training sets that are developed by chart review and or prospective data collection, as opposed to the typical sort of have a phenotype identify all patients with diabetes and then build a model to predict the risk of diabetes, where there are two levels of abstraction. Where there are two levels of abstraction from the truth. He thinks that given the statistical properties of some of these models, you really run a lot of risk of bias there. And I think it's a valuable point and worth discussion if it's at all feasible. It's tougher to scale. And then dynamicity for a signal model, as you might imagine. And then a clinical trial. So importantly, he and his team have taken the hero model to a multi-center clinical trial where they should benefit and. Clinical trial where they should benefit, and it's spun off the company, and now is being used for thousands of babies. Another key feature of his success, it turns out that he's married to a nurse practitioner who is an expert in implementation. And her work echoes a lot of what Lino was talking about earlier today. So the key elements of successful bedside predictive analytics, this is the comic tool, a type of tool that they subsequently developed for the building. Tool that they subsequently developed for adult use. Trust, uptake, buy-in, relevance, actionability, and sustainability. So I will just, this is my last slide, as we, you know, a lot of folks in the group are thinking about computational physiology models. And, you know, that's relatively new terrain. You really don't see that a lot in terms of being deployed in EHRs. On the positive side of the On the positive side of the computational physiology models that I've seen, it's usually smaller numbers of inputs, so that's good. I have a little bit of uncertainty about just sort of, I think it depends on kind of what the system of equation is and what compute is going to be necessary in production, how easy that is to do. Probably that's a solvable problem. The tougher problem for the models that I know of is that many of them have non-standardized, hand-curated, or sort of esoteric. Curated or sort of esoteric from a DHR perspective inputs. So, for example, in the glucose insulin models, I know that David's postdoc and others have spent months curating every different kind of feed that an ICU or hospitalized patient could be getting in order to know exactly what is going into the patient in terms of food source. And that could be tough to maintain over time if all those inputs were needed. Monica is probably going to be talking about some really exciting cancer models sometime during this meeting. And we've had to think quite a bit about how to apply a proxy measure of tumor cell count. Right now it's a measure that is based on PET scans that requires radiologists to work with the image and really calculate. And so that will be tough over time. So I think I went a little bit long, I apologize, but I apologize, but we're welcome to any questions for you. That was awesome. I think that those who are very in particular, I think tell I think Tel pointed out that one of the big pieces is making sure that we have data for the models instead of expecting the models to have whatever data we want. And I think that there's a lot of space for model construction evaluation and methods based on trying to get the model to where the data are instead of the other way around. Hey, hi guys. We're gonna do what we usually do, which is we're gonna talk team and improvise a little bit. So one will talk, the other is gonna juggle and chat. Okay, so we're gonna tell you about implementation outside of the health system, and we're gonna start with examples of things that we've been implementing and deploying. So, this is one example, which is for patients specifically. Patients specifically, this is an app called Fendo. It's actually a research app where you consign through the app into the app, and there's a recruitment and everything into the app. And it's a portmanteau board for phenotyping endometriosis. The idea is actually to be a data collection tool to get direct lived experiences of disease from patients with this disease, which is actually not very well understood. We have about 15,000. We have about 15,000 users right now and it's an app that's available on all of the different app stores and it started as a data collection tool and it's moving into an intervention tool where we're also providing some self-management recommendations using reinforcement here. The second thing that we're gonna use as maybe an example is this thing called COVID Watcher. This was a Watcher. This was a departmental effort when COVID started. Where we, you know, a lot of people started working on the clinical side and we were interested in New York City specifically. You know, we realized it was going to change the city and so we kind of used principles of citizen science to get a post on what was going on with New York City residents. And so there were the app that we created in a record time through a lot of volunteering work. Work was deploying a lot of questionnaires for people, but was also doing a lot of sensing, heart rate. It was a geofence so people could get a sense of how much people are isolating or not. And also you could connect to your temperature. So that's another app that we also built and released. And that's the one that Dave and George and Matt were all working. George and Matt were all working on. So that's the computational model of blood glucose regulation that Cordesio gave with that uses a physiological model plus data assimilation. And the way we've built it is that people track their meals. We send the meals to Amazoni Clinical Turk to break it down into nutritional assessment, inclusion of different macronutrients. Then we also ask them to track their blood pollutant. Also, ask them to track their blood ripples before and after meals. And we use this data to train the model. And then what happens is that there's a smartphone app where they can take a picture of their meal. Because we want to provide a prediction instantaneously, we ask them themselves to guesstimate the different macronutrients in the meal. And then we generate a prediction for them, how their blood glucose is likely to change if they eat this meal. Change if they eat this meal. With the idea being that I'm looking at two cookies at my plate and I'm thinking, hmm, is that a good idea or not? I can take a picture of the meal, I can look at the prediction and say, hmm, maybe not. Then maybe I can take a picture of one cookie, see a different prediction, compare the two and decide which one is a better meal for me. We built it, we released it in the App Store and Google Play. Also, I'll talk about some of the experience implementing it, but it's been around for a couple of years. It but it's been around for a couple of years now and has about 4,000 downloads. So if you're interested, it's still out there. You can actually download it if you see it. But those were the experiences. So some of the things that, kind of lessons learned and what we I think opportunities both and some of the challenges. So one thing is that there is definitely an increasing adoption of mobile technologies. I do a lot of research. I do a lot of research with economically disadvantaged communities, with individuals recruited from federally qualified community health centers, and even those populations, you can pretty much assume that everybody has smartphones at this point. Whether or not they can use it and use smartphone apps, it's a different question, but at least the technology is available. And obviously, there are many interaction modalities that are available there. So you have SmartConnet. There, so you have smartphone apps. My new work is more in chatbots because it's a much easier way of communicating, but there are many opportunities to make decision support available. Also, there are new platforms to do quite intensive computation either on the phone itself or in the cloud. So we can run models, not all the models and not always in real time, but we can do a lot of intense computation. Do a lot of intense computation and make its results available to people in real time. There's also a lot of opportunities, I think we both experience this for rapid prototyping. One of the things I was talking about iterative design process, the assumption in design of interactive systems is that you're not going to get it right the first time around or even the tenth or the hundredth time around. So the idea is to iterate rapidly. And it is quite possible nowadays to build very quick prototypes. Without spending $90,000 on implementation effort. That's what you do when you are releasing a commercial quality software into production. But if you want to do quick prototyping, you can do that. There is also, thanks to a lot of the research that sorta started, I think with Apple Research Kit, there are many opportunities for different skills of dissemination. So you can do a limited release. So you can do a limited release by invitation only to a select number of people that you want to enroll in the study, or you can do what we did and throw it out in the wild and let anybody download it. Both of them have their consequences. And at this day and age, again, there is an expectation, it became a norm that apps for self-management have become available and people are just culturally attuned to the fact that you can download. Attuned to the fact that you can download an app and you can use it, and there is less hesitation about using these kinds of technologies in a daily experience. More or less, do you want to talk a little bit? So what's going to happen now is we're going to have different themes of implementation, and one of them is what is a platform that makes sense. Mina just mentioned that apps, smartphone apps, made a lot of sense. Apps, smartphone apps, made a lot of sense. There's also chatbots, which are a different way of interacting between a computer and an individual. And then there's a lot of multi-modal interactions. So Lina talked about actually combining an app with a chatbot and having different ways of getting to talk to an individual. In our Fando new version, we have a lot of combination of wearable data. A combination of wearable data and app learning. And as soon as you start having multi-ways of interacting with someone, there's a lot of interesting design questions. Again, I think we've covered some of this. There are interesting trade-offs between in thinking about how to run studies like that, and we've run into this also. So when So, when one option is to just make these technologies available to a broad set of people, which we did with Group Oracle, and we had like an insane number of downloads that broke it in the first two days, and the IRB, we went over our limit. So it was completely unexpected. And on one hand, it's an opportunity to collect a really large data set. On the other hand, we very quickly learned that we have no idea of the quality of data in this data set. So we did some This data set. So we did some experiments with the data that we've collected. And you can fake the images of meals, but you can totally fake and people do blood repose levels because you can see that there are four numbers that are repeated in a cycle, which means that somebody was taking pictures of their meals and just playing with the app, experimenting to see what you get as results, as predictions. But if we don't pay attention to things like that, then we think we're collecting all the That, then we think we're collecting all these wonderful data sets that we can then use to train our future models, but we cannot. So, the quality of data is always subject to suspicion in this kind of large-scale deployment studies. On the other hand, we did a limited study where we hand-picked participants, trained them, this is our Robert Wood Johnson Foundation project, never to be repeated, because it took us two years and Years and insane efforts to recruit people and get them to stay in the study. We have a wonderful data set that is just beautiful because we know exactly what they did, we know that it's reliable, we know that it's true, but it was just so hard to keep them in a study and to get them to do what we wanted them to do. So both of those are, like, there is no golden middle, I suppose, but both have their advantages and disadvantages. Have their advantages and disadvantages. And I think in some cases, a lot of my work is done outside of the hospital completely, so I just work with patients, but a lot of times what we really want is a combination of self-monitoring data, something that happens in the wild, and their clinical data, which often means that we need to work either with clinicians or have some kind of connection with the EHR, which presents its only challenge. Which presents its own challenge. We're talking about opportunities, not challenges. I keep going, but outreach is important both in recruiting people to use the intervention as well as engaging them. And there are actually different strategies to recruiting people and engaging them in sustained, continued use of things. Continued use of things. And so, you know, we tried in our different apps and platforms a lot of different things. And I think it's kind of like what you would expect that when you have access to a TV ad or something, or you have press or a press release even from your university, it really helps in getting a very big jolt in recruitment. It's very instantaneous, it doesn't last, but if you have. It doesn't last, but if you have many of those, you can recruit a lot of people. For us, Lina talked about the fact that the first two days were a little hectic for her tool. For us, it took over four years to get 15,000 users, so it's a very slow process. We've seen, originally, we wanted to have a lot of people who did not have under HR Z. Of people who did not have endometriosis to also be self-tracking to have controls, and outreach of controls is pretty much impossible because who wants to be self-tracking all of their non-existing symptoms? So, there's a few questions about how would you want to retain people into one of these studies. You know, we know that we want this because we're interested in longitudinal data. How do we do this? Basically, the idea is. Basically, the idea is to think of the user as a toddler who needs to be engaged at all times. And so we try different strategies through time. Newsletters, return of results, tracking challenges, and releasing a lot of new features. And all of these things have been working actually fairly well. So one of the big questions when we have amazing. Amazing. Okay, one of the big questions when we have an app is how are we going to collect the inputs. You know, I talked about when we were doing the EHR that these terminologies and ontologies were really critical. We don't have such things for app data. Apple HealthSheet has been kind of working onto it, but there's a lot to do. Interoperability across mobile platform is a real issue. We're doing some experiments with people recording their voices, and it turns out Voices and it turns out we need different models for when you record yourself on an Android phone versus an iOS. How do you prepare the data to do modeling and how would you deploy, where would the models live? You know, data warehousing. So what I'm showing here is we work with a lot of menstrual trackers. This is not our applications, but we're leveraging data from other applications. They all have different ways. Applications, they all have different ways of storing menstrual cycle information. And so we need to find a way to get a warehouse and harmonize all of this. It's also very important to have a lot of audit trails, logs, and user actions so that we can understand and study engagement. And then there are really interesting questions about if we have modeling happening in these apps, where are the models going to be deployed? Is it locally? The pros are that it could be. The pros are that it could be very private, it could ensure privacy, it could work when there's no network. But the cons is that it makes the maintenance a little bit more complicated and there's definitely some models that can be too complex to be run on someone's phone. Server is pretty much the opposite of pros and cons. Yeah, so and then the question is: what does it mean for those technologies to be useful? I mean, ultimately, we want to know whether. I mean, ultimately, we want to know whether it's working, it's doing something useful. And there are different ways to focus on this. The ultimate goal is to kind of study their impact on outcomes. In diabetes, for example, we want to know whether they actually help people lower their blood glucose levels, concentration of glucose in their blood. But for this, obviously, with the lack of control and access to clinical outcomes, Control and access to clinical outcomes in these large-scale settings, we're still bounded to buy randomized control trials. So, the two studies that I've done where I looked at outcomes, we still had RCTs funded by NADDK with a very manual kind of way of collecting and recruiting participants. So, it's a lot of money in a long time. But, and it also only allows again. Also, it only allows again to focus on a very small set of variables that we may be interested in. We can look at other aspects of the usefulness of this app in large-scale studies. For example, looking at individuals' information needs and whether they're met or not. Changes in self-management behaviors. But we need different ways to study this that are outside of the outcomes. There are interesting things that come to play in terms of engagement. That comes to play in terms of engagement. So, one thing we found with Oracle, for example, is that this is a tool where people not only can make better decisions, but they learn from it. Once you start receiving those predictions, you start kind of noticing patterns that if the meal has more carbohydrates, the prediction is higher. If the meal has less carbohydrate, more protein, the predictions are lower. The more you learn, the less useful the app becomes. Becomes. So, what we saw over time is people switching from using it every day for every meal to eventually using it for only new meals when they cook something new or when they go out or when they eat in a different place, which is a fantastic discovery, right? This is what we want. We want them to learn something and become better. But that's not the kind of engagement indicator that we typically look for. It also means that our data collection becomes more sporadic. Our data collection becomes more sporadic and it has more gaps. So we kind of are winning, but shooting ourselves in the foot while doing that. And then change of behavior again, it's hard to track it because if we don't have the good thing about clinical outcomes is that they're reliable and they are there. With behaviors, if people start tracking them less, we have less opportunity to measure them. To measure them. And again, some of the changes in engagement hurt us in our ability to assess the usefulness of the tool, even if it's actually the indicator of its success over time. So there are many different... Oh, and you were here. Yeah, so this is an example specifically about engagement where we looked at so these are people who have been using Fendo for more than seven days. More than seven days, and we're looking at their first three months of use of the data. And there's kind of what you would expect: there's some people who are highly involved in using the app, and then a little bit less, and a little bit less, and then the ones who are kind of like not really, or at least it looks like they're not really engaged. So there's a few things that were interesting in this. First was that this short-term engagement, if we now expanded the time horizon to any their whole longitudinal record, Their whole longitudinal record, we found exactly the same clusters, and people stayed in those clusters. They also were exhibiting different self-tracking behaviors in the sense that the regulars, the ones who were highly involved, were using self-management, were tracking their self-management strategies much more than the others. But interestingly, there was no difference into who are the users according to their demographics or their health status. Or their health status. And so it turns out that depending on your goal, one of our goals was to collect data to learn about the disease, all of this data was actually as useful as this data here. And in other analysis, we found that we could learn these subgroups of patients from the disease, and it was completely uncorrelated to their engagement with the L. On the other hand, if we want to build self-management, then really the guys on the top are the ones we want. Uh, the guys on the top are the ones we want to pay attention to. Uh, so engagement means different things uh for different books. I think that's it for today. Thank you, David. Do you want to stop somebody else? So we can use the first one. Eric, are you in the sky? I'm in your hopes and dreams.