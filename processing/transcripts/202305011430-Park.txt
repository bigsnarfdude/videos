I want to say that this is like a collaboration with my CE, like Rui, and Nancy, who's on faculty at the University of Toronto, and as well as three collaborators at CamH at Toronto, including Arian, Collin, and Aristotle. I want to start with some kind of motivations. And I guess many of you have heard of the term brainwashed association studies. I'm from the last two or three years. And to us, I mean, it's pretty straightforward. Like, um, to us, like, I mean, it is pretty straightforward. We kind of using like our imaging data to predict our like behavior outcomes. So, it has been like I'm termed by a polyvertex score or polyneurosports by like Wes Thompson. There have been a lot of already development in terms of the like scalar on image regression in statistical literature, right? But the nature paper published like last year kind of shows that like BWAS associations are pretty smaller than what we have previously thought. And it's going to be leading some kind of underpowered situation. Leading some kind of underpowered cities. And it'll also lead to inflated effect size and blah blah blah. Like that, like that makes us very frustrated. There are many potential reasons that this paper points out. And it may include some kind of smaller sample sizes or kind of publication bias, blah, blah, blah. There are many factors that are addressing this paper. But for me, for this fMRI-based paper, I want to point out another perspective about reliable. About reliable data of the fMRI. So, is the fMRI, especially a test fMRI, are reliable data? And there have been several references published in psychological science and neuroimage that tests test reliability in human collective project or APC studies are really poor. So, what it means to us is that if the reliability of the FMRI data, of testing female data, is not really guaranteed, then it'll probably lead to like very low. Then it'll probably lead to very low or spurious correlations on BUS. So we first need to do our best to understand those kind of reliable features of the fMRI. And that's one of the goals that I'm presenting in this talk. So our goal is to develop a novel spatial extent inference for testing on variant components problem because those kind of reliability can be translated into the variance component problem. And there are many special. And like there are many like spatial statisticians here, and there have been many, many, many spatial-related methods published in neuroimaging and statistics journal. But I want to point out that most of the methods have been focused on testing for the mean parameters in a general linear model framework. And statistical methods for variance component problem, even though it is pretty much needed everywhere, it has been pretty underlooked. It has been pretty like underlooked or underdeveloped. But let me point out what we are really talking, when you're saying spatial modeling. I did some literature review on some of the spatial modeling published in New York Image. And most of them falls into one of these two categories. First of all, we kind of model the spatial parameter of interest. And you can think of tensor, Bayesian methods, splines or whatever that you prefer and assume the noise is. That you prefer and like assume the noise is IID in space. And is it going to be a valid model? Like, from the multivariate viewpoint, you're kind of ignoring all the spatial autocorrelations of the data, right? So if we really don't model the noise structure by spatial modeling, it's kind of a misspecified model. Another aspect is that you can really try to model the spatial dependencies of the noise, but you end up conducting mass univariate analysis. It is correct model, but it's kind of a correct model but it's kind of underpowered because it's you're not you're not really considering the like spatial localizations of the um of the like parameters of interest so our idea is um can you do both uh and can you do both um in variance component testing and this kind of constructs our idea of like um clean um like family so like it was like originally like i'm like um done for like i'm testing mean parameters in general linear model and it's like mean parameters in general linear model and it's like a work without mark i'm sitting there and it has been this idea has been extended um to like testing like um intermodal correspondence for example if you have the like cortical thickness and like task fMRI um mapped on the cortical surface can you like quantify their correlations and then see how like um how like um how they're like um coupling is like a strong or not um and yeah here like we call our methods a clean B because it is like I'm we are we are sharing the same philosophy of clean but we're just applying to the like a Of clean, but we were just applying to the like a various component problem. So, like, um, why cannot it? Why what are the challenges here? Definitely, like, um, neurich data is like a big in size. So, we definitely have some kind of problems in like computations. So, fitting a model itself takes some time. It means that if your model becomes a little way like complicated, then it'll take like even more time to be fit, right? And usually, like, those kind of multivariate methods involve. Like those kind of multivariate methods involves the inversion of the matrix. And if you think about inverting like 10k times 10k matrix, it's like a nightmare. And in neuroimaging literature, people usually use permutations to control for like famous error rate. And think about repeating it like a thousand times to control for famous error rate. Like you just cannot do it by a knife modeling, knife spatial modeling. So here's our notations. So we'll be like, our notations so we'll be like i'm using our uh yi like v to denote for the neural data observed for subject i at vertex p and for notational simplicity like i'll be using the subscript to denote for the like subjects and so there should be like vector notation like y1 um that um collects all the like subject once image and at the same time we also need to define like um y parentheses like one to collect all the subjects or all the images um at vertex speed so that is like i'm So, that is the teaching that I'll be using. And whenever there's a vector, I'll be using a both one just for convenience. So, here's our setup. We have fMR data measured at vertex V, and there should be some covariate effects like age and gender. And there should be some variations that are attributed to test reliability. And there should be some noise. And if you model one by one, we are modeling the. one like we are modeling the like um the r of v to be like um a random effect with mean zero and some um like a structure like very covariance term there so k is a like um uh between image dependency like matrices and it is specific to like study designs so if you have a like um test retest um data then like um they'll they'll be having a like block diagonal diagonal matrices and this kind of means that um these two like these first two images are coming from the like same subject and if you have like two Subject. And if you have like twin studies, for example, because monozygotic tweens are like perfectly correlated genetically, and they're like half correlated. I mean, if they are like dizygotic twins, so we can like specify our K matrices like this. And our goal is to test whether or not at least one of them of the Seta term, Seta squared term is equal to zero. Because if it is not equal to zero, then like it is like. It means that there is some kind of reliability in this vertex. So, two goals here. We first want to test for this hypothesis. And if we can reject this hypothesis, we want to localize it. And we also need to model this noise, as I already mentioned. So I like exponential model, exponential structures to model spatial correlations. And that's how this BI term comes into play. term comes into play for modeling the like um spatial variations and there's a like nugget effect that describes the like known spatial variations and one question that like we are frequently getting is exponential model is a good model uh to specify the like um spatial dependencies and we can at least do some like exploratory data analysis and we here we kind of show the um empirical covariagram so it kind of shows how average um how like an average correlation How, like, our average correlation covariance depends on the distances. It looks like our exponential model for the like covariance is uh spatial covariance is okay at least for now. I'm not saying that it's going to be perfect, but like at least it seems to be okay. So the clean B like I'm consists of like three main steps. So we're going to be like, I'm using our like spatial, like I'm spatial covariance structures to construct the like multivariate statistics at each vertex. And then in step two, like we're going to be like aggregate aggregating them. Aggregating them to construct the cluster-enhanced test statistics, and then you're going to be applying permutation to control for family worse error rate. So here's our test statistics. Our like a multivariate test statistics is characterized by a UV here, which kind of takes that like quadratic form here. And like one important thing is that they are obtained under the null hypothesis. And under the null hypothesis, we only need to model. No hypothesis, we only need to like model the spatial and non-spatial variations, right? We don't really need any like static here, and um, it is kind of like equivalent to the like a scat test that is pretty popular in statistical genetics literature. And in neuroimaging literature, like there has been like some like similar methods, like a MECA that conducts a massive univariate analysis for variance components using this statistic. If there is like no like spatial modeling comes into play. No, like I'm spatial modeling comes into play. And what can be like shown here is that UV like follows a like mixture ti-scar um distribution. It looks kind of complicated. But the good thing is that we can like use like an approximation, normal approximation. So it means that if we kind of standardize U of V by its mean and variance under the null hypothesis, then you can use, you can compute the p-value pretty easily, even if like the null distribution is pretty. If, like, um, the null distribution is pretty weird in their form. But, like, one note, uh, one thing to note here is that estimating spatial covariance parameters is pretty tricky. So, we don't really use the likely best methods because it'll be very intensive. Instead, like we are using some tricks like covariance regression analysis or NNGP that, like Martin already mentioned in his slide. And these procedures, like, um, don't use, like, um, don't use uh likelihood, but they are like they provide consistent. But they are like they provide consistent estimator and they are like providing a good approximation. So I'm feeding on these steps only takes about like two minutes on your laptop. My laptop at least, but my laptop is five years old. So it should like run better on yours in more recent computers, I mean. So for cluster enhancement statistics, we kind of consider the like standardized sum of the like test statistics in a predefined like regions. So you can like think about like a vertex here. You can like on the Of like a vertex here, you can like define a neighbor. So there should be some ball, like or the circle here, and you kind of collapse all these uh like uh variance component statistics, and then you kind of send that as it. So one problem is that like we don't really know like the true areas of like signals. So we should be very careful in choosing the like optimal like radius. But without, I mean, we don't really like choose the radius, but like we just try many different radius and then try to be adaptive by technique by. Radius and then try to be adapted by taking the like maximum test statistics. So, T V, which is the cluster in S on test statistics for vertex V, would be following a maximum of dependent chi-square distributions. Yeah, kind of. So, here's how we do things in CleanV. We kind of do summing up in predefined regions, standardized, and taking the max way. Way so one of the like good things here is that we our test statistics were computed from the like a null model, which means that it's we don't really need to fit all these spatial models again like when it comes to permutation. We can just use the original model fit that we like we fit it. So even like a permutation is not like I'm too big concern because we use like a score test like a like a score based on like a testing framework. So our permutation only requires the matrix. Our permutation only requires the matrix multiplications, and which makes a dramatic boost in computational efficiency. So, how does clean work? We like discussed several times with our collaborators, and there are three main aspects that make this happen. The first one is generalized estimate equations, like GE. I mean, we are definitely not really using the likelihood-based methods to estimate the spatial covariance parameters. Spatial covariance parameters, but we are kind of using the consistent estimate. And so our within image correlations are like modeled by working exponential spatial covariance structures, which means that if we can get at a closer and closer covariance structures that is closer to the truth, then we're going to be more efficient, we're going to be more powerful. And the second aspect is score-based. Again, we don't really. We don't really sorry. We don't really like fit full model because fitting full model, especially when it comes to like various components, will be too demanding. But because our model only requires that like normal model, it's still feasible. And our permutation makes everything happen and try to control the famous error rate well, even if the working covariance is misspecified. So we did some data analysis by using the human connectum project. And in our data, they had 44 subjects with test-retest data. And they also have about 170 twin pairs, including monozygotic and dizygotic twins. And we are using five contrasts in our analysis just to show the robustness of our. um show the robustness of our um proposed methods and we use like uh i oh by the way we are not using voxels we are using the like we are using the like vertical surfaces only so we kind of rely needed to rely on the like of safety tools package that like mandy kind of um developed recently so i want to i should like point it out and here's the result um for comparison like um the the first row and like third row kind of like um uh like i shows our results by applying on clean v and like um i also And like I also computed the ICC map, like a massive universal ICC maps, to see how closely our methods kind of captures the truth. And just to compare our results to the activation maps, we also computed the Quincy D map as well. And you'll see that in most of the tasks, our maps that is detected by Clean V kind of approximated what it should be, like here. And it kind of corresponds to our. And it kind of corresponds to our activation maps as well, which kind of seems pretty supportive because if we are not really getting any reliability in activated regions, what does it really mean? It means that we just cannot really use it for our BWAS studies. And for comparisons, we consider three competitors, which are all which all of which are considered as Consider as like more like a simpler version of the clean beast. So there is a second method that does not really involve the spatial modeling. And there's a third model, a third approach that those spatial modeling, but like without constructing the cluster in test statistics. And the lastly, the mass univariate analysis that does not really do any kind of spatial thinging. And what you see here is that when you see the massive univariate for the reliability and heritability studies, I mean, Reliability and heritability studies, I mean, you really don't see anything, right? But like when you like when you come to aggregate the test statistics, then you see like definitely more like regions with significant test retest reliability or heritability. And it seems to be like I'm gaining more performance in Clean V. So for like on validations, we are kind of like leveraging a thousand. They're kind of like leveraging the huge human curriculum project database to construct the data-driven simulations. I think Martin Cortez is like downstrapping in this bioarchive paper. And I think it kind of allies to what I'm CPN is thinking about. But it can be like I'm constructed by, for example, like constructing the node data. You can also control the signal-to-noise ratio by just getting a sub-sample of the. getting a subsample of the like subjects from the like i'm twin test retest pairs and the rest of them like i'm filled with the independent like subjects that's how we kind of like um construct our like a data driven simulations to um check the robustness of our methods this is important because again i mean we are imposing a very like um strict assumptions on exponential covariance structure it may be pre-violated um and there may be some like um local like non-stationarity so we need to like valid our validate our methods using the real data That's using the real data. And that's how you see, like, when it comes to computing the proportion of times that each vertex has been identified by each of the methods, when we make the true number of pairs vary for heritability and reliability studies. And you see that on clean V still gets the most promising results compared to other competitors. And when you're comparing the first and The first and second row here, like for example, like these regions is not really detected by the cluster enhancement without the spatial correlation modeling. So we clearly see some benefits of using Clean V in both reliability and heritability studies. And it is also supported by our power. And the power curve has been most steep and most explicit when you try both spatial. Like, um, try both spatial autopilot modeling and cluster like enhancements for variance component statistics. So, here's our summary. The Clean V is a fast and powerful method for setting test, retest reliability and heritability in surface based on neuroveraging data. But based on the designs, its code can be more extended other than reliability or heritability studies, right? Right, and as I said, like PNV achieves like power from like two different aspects of the like obsession like modeling, speech modeling of the parameters, patient modeling of the like data. And familiar error is controlled very accurately by using permutation. And we are like, I'm now like I'm feeling more like pressure to move on to the multivariate methods from the univary methods because the univariate method seems to be very underpowered. And CleanV like I'm kind of providing some computationally feasible solutions. Some computationally feasible, like solutions to go to that direction. Just want to point out further, like I'm clean in general. And clean has been shown its utility in testing for the mean parameters in GLM. It's original clean methods that we first developed. And it is also useful in testing for correlations and variance components. So these kind of like our three methods, it's kind of like I'm sharing the same philosophy, but like from the Sharing the same philosophy, but like from the, from, from, but for like different purposes. And feeding these models, including all these permutations for like 5,000 times, takes less than 10 minutes on your laptop and it's available as a R package, like as you see here. I want to point out some of the like all the like co-authors that contributed to the development of the clean and clean R and Clean V. There's like a Mark here, who's my PhD supervisor in Minnesota. And for Clean R, I work with And for Clean R, I work with a Takis group. And Sarah was the first author who carried out all the Clean R project. And Rui is the microcision who just completed this CleanD project a week ago. And this work has been supported by these funding resources, like Data Science Institute, and NCERC. And our manuscript is available as a preprint. We just posted it a few days ago. It a few days ago, and there's a QR code this year. Thanks. We have time for a couple of questions. Yes. Yes. Very interesting, especially the comparison you're making between BWAS and GWAS. So in GWAS, there's something called LD or linguistic security. LD or languages of equilibrium, which I think here will correspond to spatial correlation, but using K, which is the equivalent of the kinship matrix, and you can have correlation spatially, even if you have independence between subjects. So, I'm trying to understand where the spatial correlation goes in into our calculation. When you construct the K matrix, especially, where is it? Here? Yeah, K is now between voxels. Between our images. um images within at vertex uh across all the images you can like construct like um if the first two images are coming from the like same like a subject but from the like test retest um scans so like um you can like specify like um inter subject variations by like um imposing these um covariance structures k yeah so the correlation subject two is um test retest data subject three is um test retest data yeah so where's the spatial correlation i asked spatial correlation comes into like modeling the noise like noise is modeling Like noise is modeled by this spatial and non-spatial variations. And like that's how I first needed to clarify the terminologies here because in modeling the RV that we needed to use these notations, but in modeling the spatial variations, because it is within image covariance, we need to use these notations to model the covariance this way. Yes, this way. Okay, and a small comment, just something I noticed in slide 12. I know. Yeah, you mentioned you have a mixture chi-square distribution. I wanted to approximate it by a Gaussian. Yeah. But I think a mixture of chi-square can be better approximated by a Gambler distribution. Oh, yeah. Like that's like what like a scan method is kind of doing, but there are other methods. I mean, eventually I'm scan and all the others are trying to make their best approximation to the normal to compute the p-value easier to be computed. Yeah. To be computed, yeah. Does anyone have more? Yes, so we have a big problem who was having various component estimations up to 10 years ago in the neural image. Actually, that's an interesting problem with the mixture of chi-squares is because it's sort of on the boundary of the hypothesis space, so you can't do that. But there are methods for sort of finding different approximations. Sort of finding different approximations for that. Yeah, yeah, I'll send you the page. There's a gamma approximation, like there's like a non-central, like high-scale approximation. Like, there are several approximations you can try. More questions? Armin, I think you have a question before Martin. I don't know if you want to yeah, I think we have time. Yeah. So, uh, Martin, yeah, but yes, you said that it's time to ask you about ask you now. I wasted a lot of hours of my life on this registration problem. And I think I tried several times. I gave up every time. So, I'm glad you're