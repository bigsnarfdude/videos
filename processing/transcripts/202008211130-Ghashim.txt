Hello, welcome to my presentation today. The title for today is On Bayesian Estimation for Joining the Shortest Scale Model. And this work is done under the supervision of Professor Ichin Cha. The outline for this presentation is first we I will Is first we I will introduce the motivation part. Second, we show Bayesian inference in general and how Bayesian paragism is working for cues. In the third part, fourth and fifth part are the main results of my work. Finally, I will show some challenging questions and suggested research. The figure of view The figure on view is showing a queuing network within identical servers and joint short skew policy. Briefly, we have Poisson arrivals with rate lambda coming to a system with n identical servers. Each server is distributed with exponential distribution. Distribution and each server has its own queue. And upon arrival, the job will be sent to the shortest queue among these NQs. Such queuing system has many applications, by example queuing networks, transportation systems, and many other applications. Here, the problem of interest here is to Interest here is to estimate the system parameters, that means the arrival rates and the service rates. We assume first that the system is in a steady state, that means the arrival rate will be strictly less than the sum of the service rates. Then we need to estimate the system parameters over different schemes of collecting the data. First, when full data is provided, data is provided arrivals departures and job paths second the queue length only to be provided here we can only estimate the traffic intensity we have two approaches to deal with uncertainty in queuing good models first the classical approach and second the Bayesian approach the major part of my research here is to focus on the Bayesian approach to deal with uncertain approach to deal with uncertainty in the queuing models. A quick review of Bayesian theorem, how it works. A definition here says that Bayes' theorem provides an expression for the conditional probability A given B. That is, the probability of A given B is equal to probability of B given A times the probability of A over. Given A times the probability of A over the probability of B. Actually, in general, Bayesian inference assumes that even the parameters has its, the parameters have their own distributions and they are also random. Bayesian inference is the process of fitting probability model to a set of data and summarizing the result. Data and summarizing the result by providing a probability distribution. We call it this probability distribution, we call it the posterior distribution. A basic idea behind vision inference is to update prior knowledge with new data to create posterior probability distribution. The posterior is proportionally equal to the prior times the likelihood. Times the likelihood. And due to Germany all, the three steps of Bayesian inference is first set up a full probability model, a prior knowledge, then conditioning it on observed data, new data, and then we evaluate the fate of the model and posterior distributions implications. Distributions implications. Here, the step of finding the posterior distribution. And due to Germany all as well, the central feature of Bayesian inference is the direct quantification of uncertainty. Now, moving to the Bayesian method specifically for cues, designed for cues. It has in the literature. It has in the literature it has been done a lot of work using Bayesian or classical approach for cues. By example, first paper using that used the classical approach for cues was done by Clark in 1957. And in the early 1970s, the Bayesian approach for queues were coming by Motapor. By Mudapur, by example, 1972. And most recently, a lot of work done by Armor Obiari for Bayesian inference for queuing models. And they outline a number of, Armor Obayari outlined a number of advantages of the Bayesian approach to inference for queuing systems. And due to Armor Robayari in 1999, they said that vision analysis can work nicely and easily, providing perhaps answers not easily obtained with other methodologies. And they stated four main reasons for that. First, the likelihood principle. Second, restrictions on the parameter space. Restrictions on the parameter space, third, the accuracy of estimators. And finally, the most powerful tool for using Bayesian inference in queues is the prediction. Because the Bayesian predictive densities do have into account the uncertainty of estimating the unknown parameter. An application of using Bayesian inference for queues is to have the very class. To have the very classic queuing model, the MM1. So, to apply Bayesian inference for the MM1 Kuing model, we assume first that the parameters of the model are unknown. Initially, we shall consider a simple experiment of observing for fixed NA and NS inter-arrival and service times, respectively. And because of the lack of And because of the lack of memory property of the exponential distribution, many experiments' designs lead to similar likelihood functions. A simple likelihood function designing for this experiment, designed for this experiment, is showing here. So it has a nice form here where Ts is the sum. Where Ts is the sum of the times of the inter-arrivals, and excuse me, Ts here is the sum of the whole sum of the service times, and Ta is the sum of the inter-arrival times. As mentioned before, Bayesian inference assumes that all on All unknown parameters are random, and hence we set two different priors for the arrival rate and the service rate for the MM1 model. First prior, the Jeffrey prior, and second is the natural conjugate priors. And then it can be shown that the corresponding posterior distribution for the joint lambda and u given U given given new data is following the gamma distribution. So we have two independent gamma distribution for each lambda and mu and then the inference can be done easily using Jeffrey priors or conjugate priors. So it simply is the posterior mean. And for lambda, by example, using Geoffrey prior. example using Geoffrey Prior the base estimator for lambda is Na over Ta. That means the number of inter-arrivals over the whole sum of the time taken by inter-arrivals to join the system. And similarly for mu here, to estimate the traffic intensity using Traffic intensity using Jeffrey Pryors, by example. So we define the MLE estimator for the traffic intensity and by some few manipulations here we can reach that the posterior distribution of the traffic intensity is distributed by Fischer distribution and the the the The posterior mean for the row here is equal to ns over ns minus 1 times r. R here is the MLE estimator. So comparing with the classical approach, the bias estimator here for the traffic intensity is up when the sample size is big. Then MLE and Bayes estimator are almost the same. So the powerful of using Bayes estimator, a Bayesian inference here is for small sample size. Yeah, I started my work for Bayesian inference for John Shun SQ models. Short skew models. I started with two queues in parallel with joint short skew policy. We have Poisson arrivals with raise lambda. We have a dispatcher sends the job upon arrival to the smallest queue length among two queues. And we have different service rates in each server. So I assume different settings of collecting data because in many circumstances while observing a queuing system like the two queues in parallels, the service times from its servers are not directly available. In this case, we observe the arrivals to the system, their pass, and their departure times. times so here to construct the likelihood function we need to to to know the the relationship between inter-arrivals and service times and arrivals and departure times and here i construct a set of departure equations i call it departure equations transfer equations transforming inter-arrival server server service times to arrivals and departure times as showing in the system set of equations as shown in equation 4.1 so the transformation related to the departure equations showing above Equations showing above in 4.1 is 1 to 1, so that observing all the arrival times of all jobs, E and their departure times, is equivalent to observing the related IID service and inter-arrival times. And hence, I can construct the likelihood functions as shown in equation 4.2. And we assume all parameters. And we assume all parameters lambda mu1 and mu2 are unknown and take the diff take the same settings as mm1. We assume prior knowledge of the unknown parameters and simulating data from the system showing before and assuming prior beliefs about the unknown parameters. The unknown parameters. I take into account the MLEs and different Bayesian priors and different sample size were considered here. For the choice of, by example, gamma alpha beta or gamma 2.1, gamma 20, 10, and gamma 5.5, all these 5-5. All these choices are arbitrary, and as you know, the prior beliefs depend on the experts belief about the parameters. So I take here arbitrary choice for the prior distributions and then setting the Bayesian process, we can find easily. Find easily the prior the posterior distribution, and then we can figure out the Bayesian estimators for each lambda mu1 and mu2. So, as the table in front of you shows, different estimates for lambda mu1 and mu2, as the sample size increase, all estimators are doing good. All estimators are doing good andor are approximately the same. And as I mentioned before, the small sample size is a good setting for the Bayesian inference. Now Now, for inference for two queues in parallel given only queue length data, so we have only queue length data coming from the system and we need to know the traffic intensity of the system. Then we need for sure that so we need to know the Q length distribution. The Q-length distribution of such a system. And as most former studies about the Q-length distribution in this case lead to a cumbersome infinite linear combination of product forms, I switch to use an asymptotic behavior of the mod, which is studied by Flateau in 1989. So he provided a nice A nice formula for the asymptotic behavior of the two Q's in parallel. So I use it for constructing the likelihood function as shown in equation 4.3. And then I considered two different priors for the traffic intensity. First, the improper prior, that means the prior will equal the prior distribution. Will equal the prior distribution for their traffic intensity is equal to one for its support. And I assume different prior as well to be beta distribution with hyperparameters A and B. A and B can be chosen arbitrarily. And then setting the posterior distribution will Will be as in equation 40.4. So, as the likelihood function has a complicated, a bit complicated form, I use MCMC with the Metropolis hasting algorithm to sample from the posterior distribution to get inference for the traffic intensity. Table here shows different estimates. Different estimates for the traffic intensity. First, I simulated data from the system with fixed traffic intensity to be 80%. And then I used Bayesian, three different Bayesian priors, improper, beta-AB, and beta-phi prior. So improper refers to that there is no prior information. No prior information about traffic intensity, and beta 5.5 refers to that the traffic intensity will be between 0 and 1. And beta AB is just an arbitrary choice. I have here two different Bayesian estimators: the posterior mean and the posterior mod. Here is rho B and rho m. And even for small sample size here, for by example n is equal to 25 observations of the Q length data, all estimators are doing good. And as the sample size increased, the standard error decreased as expected. Here, I switched to a more general case of the two queues in parallel. Here we have a queuing network with a large number NOTS with a dedicated input string. So, as shown in the figure on view, we have n servers. N servers distributed with the same service rates, mu with exponential distribution. Each server has its own queue and each queue has its dedicated input stream. We call it a dedicated input stream and it's also Poisson with rate lambda. And we have a smart arrivals also Poisson. Also, Poisson distributed with rate n, the number of the nodes, times lambda s. And upon arrival, each smart arrivals will be sent to the shorter queue among n cubes. So we assume that the system is in a steady state and we can State and we can only collect Q-length data from an arbitrary queue. And here the problem of interest is to estimate the system parameters. Indeed, this system was studied by Dowson et al. And they provided a nice formula for such a system with an arbitrary large. So if we assume So if we assume the traffic intensity to be strictly less than 1, that means lambda plus lambda s less than mu, then the unique stationary distribution is given by the formula in 5.1. And if we have the dedicated arrival rate is equal to zero, which is the same thing as shown in the motivation part, Motivation part, then the system, such a system with a big number of servers has only zero customer or one customer at most. So depending on 5.1 and 5.2, I constructed the likelihood function, which is approximately equal to the equation shown in The equation shown in 5.3. So, here, when taking Q-length data from an arbitrary Q with sufficient space and time, then we are able to estimate the unknown parameters in the equation 5.3, which are rho and lambda. Rho is the system traffic intensity, and sigma is the dedicated traffic intensity. Dedicated traffic intensity. So, by the same thing I did before, I said that different priors for the priors that were considered are improper, beta 5.5 and beta 5.2. Beta52 refers to the prior belief that both unknown parameters are between 0 and 1. Beta 55 refers to the information belief that both estimators are around half. And improper belie prior refers to no information provided. And the MLE as well. I consider the MLE for estimating the unknown parameters. And I simulated data from the system. Simulated data from the system with n arbitrarily large, and then or enough large, excuse me. And then I consider different sample size for even small sample size, by example MLE and improper prior are doing good for big sample size as expected, or estimators are the same. Same. The standard error for estimating decreased as the sample size increased, as expected as well. Finally, some challenging questions and suggested research. First, one can be interested in examine other complicated Markovian queuing systems and use their asymptotic behavior to derive estimates. Symptotic behavior to derive estimators for the system parameters. Second, for joint shortest Q with n big enough nodes, we are interested in constructing a joint prior for the system parameters, the smart arrival, the dedicated arrival rate, and the service rate, to derive vision estimators based on only the QREC data. So in Dublin, So, in the above, I only estimated the traffic intensity and the dedicated traffic intensity. So, here an open problem is to estimate the system parameters based on the queue-length data only. For joint short skew with two queues in parallel, we consider the case when only missing data about the queue length are to be provided. The queue lengths are to be provided. Here we are going to implement the missing data using Bayesian methods. Here are some references. And thank you very much for listening and feel free to ask any question. Thank you so much and go ahead, please, if you have. Go ahead, please, if you have any questions.