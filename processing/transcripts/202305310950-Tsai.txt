Thanks the organizer for having me here on this very special occasion. So this talk is going to be related to the logical viasions of the KB equation. I just want to start out by mentioning some results of upper telelargations in the KVD plus for a lot of a number of models. I want to take this opportunity to also mention that Timo is one of the pioneers in this direction, you see his name. In this direction, you see his name appears early, and a lot of work by Timo and his collaborators. So, the model we're going to focus on today is the KB's equation on the left. That's the K-Bees equation, one plus one dimension, eta is a space of one noise, and you can think of it as some sort of Random-Rhodes model, the high-puncture flux. And closely related to KPG is the multiple ligatives of chat-key equation. So, what you can do is Equation. So, what you can do is you formally exponentiate H here and just pretend that you were doing calculus. And that's the, you know, for PD people, that's the Half-Core transformation. It brings this equation to that equation here. This heat equation, which I wanted to deliver the noise. And the SHE also models some kind of phenomena. One of them is graphic polymer in a random environment. We've seen in the last talk, but here's one plus one dimension. The way it works is that you can try to write down a solution formula of SHE via the current cap. SHE via the Crown-Kant formula, pretending that eta is the potential, and you get expression like this. And this you can think of it as a grounding motion starting from a place that you are interested, running backward in time, and then got embedded in the potential given by a space-time lightness. So far, so good? So what we're going to focus on is the end-point large deviation principle of the K-D equation. Of the K-V equation. You probably saw this in Millennial's talk yesterday. If you run a K-VD equation for some time and do some sort of scaling from the narrow-watch initial condition, we're going to see as the hydrodynamic limit as just a problem. And I'm going to ask what is the exponentially small probability that doesn't follow this problem. Instead, we go through some pick point above the problem. What is the probability? What is the rate function? And associated to this question, I'm going to ask, what is the corresponding limit? is the corresponding limitation. Assume that the small probability event happens that the height function has in these points at a terminal height, what does the shape of the high function look like under that condition? So that's the topic of this talk. And the approach we're going to take goes through this Cas Equation. So I'm going to analyze the moments of the SHE and use that to derive information on the LDP and limit shift. And let me just start out by saying that the relation between SHE and upper tail. Between SHE and Appertail Large Deviation is kind of standard. So, one way to get the Large Deviation principle is to get moment-generating, Laplace transform on the moment-generating function. If you have the handles on this quantity here, then you can derive the Laura deviation principle by using Legend of L. So that's the thing that we're after in order to understand this. But remember from the previous slide that e to the H is Z, where Z is a solution of the. Z, where Z is a solution of the SAG G. So this moment generating function has the moment, endpoint moment of DSA G. So if we can get our handles on the moments of the SAG, we can use that to leverage into information on the loss distribution principle that helps the connection work. Does that make sense? And in order to analyze the moments of the SHE, what I'm going to do is to associate it to another system, which I call tracking browning particles, and use this to study the moments. Task to study the novel. So that's going to come up next. Alright. So the way. Okay. So here's how the attractive browning particles comes into point. We're interested in the endpoint moments of the SAG. And I'm just gonna pick some, you know, integers, k one through k n. So I have five points here. Uh interested in third moment, second, first, first, second moment. First, first, second moment. For example, I claim that this expectation, this moment, can be expressed in terms of interacting diffusion. So there's some funny factor here you don't have to worry too much about. And then an expectation over that interacting diffusion. What is that interacting diffusion? It's what I call attractive Browning particles. Each particle is driven by independent Browning motion. But on top of that, they interact through pairwise attractive pole. Attractive pull. So each pair of particles will pull each other together. So I might, this sine is the sine function. It's plus one, when it's positive, it's minus one, when it's negative. And so that's the connection. Now, the derivation of this is not very difficult, though I'm not going to show it. So one way to derive it is to use the Feynman-Cat formula, I show you on the second slide, to express this MoMA in terms of Express this moment in terms of Browning motions. And because of the white noise in the exponential, what you're going to get after you apply Feynman cats is Browning motion exponentially weighted by local times of the Browning motion. And now you can use Tanaka formula to express the local time in terms of some drift and these funny factors. And that drift, you can use Girsanov to recognize it as some sort of the drift. Recognize it as some sort of the drift of a change of magnitude. So that's how this works. I just want to point out that you can also write these pressure-brand particles as rank-state diffusion, which can be thought of as a special case of that, and that's a subject of its own interest. Many people will come at it. But for the purpose of this talk, I'm going to just focus on this particular case. All right? Alright, so the next thing I'm gonna do is to apply scale. So, you know, for the purpose of large deviations and for the purpose of limit shapes, we're always doing it under some kind of asymptotic. And I need to spec spell out what is the scale in the computer. So the first thing is I'm gonna scale the moment in Finneck, uh similar to Alford's thought. Similar to Albert's thought. The scaling parameter of the moment, or the power of the moment, is denoted by n, big n here. It goes to infinity. On top of that, you know, I just specify some fixed constant positive. So you can think of this mc as the mass of each of these points. Just later on, divide out the number of particles by scale that by 1 over m. So that's, you know, the power is going to infinity. And what about? And what about time? Well, I'm going to take time as a function of n. The natural thing to look at seems to be that you look at long-time behavior, t going to infinity. But so it turns out that for what we're going to discuss, this is not relevant. So large-time behavior of what we're going to see is always there as long as you have this condition n squared going to infinity. I'm just going to explain to you. Infinity. I'm just going to explain to you what that means in a moment. But as long as you have this, you always see the same kind of physical value. And what that tells you is that actually t can go to 0, 1, or infinity. As long as you look at high enough tau, you always see the same kind of deviation, same kind of limit shift. And there's a reason for that. And finally, the space scaling, once you have the particle number in time scaling, the natural space scaling is 1 over nt. 1 over nt. Alright, so why is that? So I've written down what the moment looks like and what the attractive browning particle looks like under the scaling I just told you. First of all, 1 over nT is natural because it makes this sum here 1 over n. So what is this? This is the total amount of drift a particle received from everybody else. There are order n particles there. So if I have 1 over n, that would normalize the thing to be unit order. So you know, it's natural. So, you know, it's natural. And once you do all the scaling here, you're going to have one over root and the post-scale system run from the post-scale system. The time is restricted to zero and then. So time also normalizes to one. After I do that, the Browning motion pickup. The Brownian motion pickup 1 over n squared. So, this condition here really says that the typical behavior of Browning motion is just not there. Typically, you don't see the action of the Brownian motion. And think about this particle system. They have holding from each other. So, under this condition, I spill out here, those particles would tend to cluster. They would like to get close to each other. And that's the physical mechanism. One way to see the physical mechanism of seeing this tracy-whedon upper tail three-half. Tracy Whedon operatal, three-half operator. Now finally, I want to address this n-to-infinity assumption. From the perspective of interacting diffusions, n to infinity just taking the number of particles to infinity. That's natural. But eventually we're going to use that to derive information on moment and the moment generating function of K B. But why do I take n to infinity here? Well, there's one constraint I didn't emphasize up until this point, because we're using particle system picture. System picture. The exponent, the power here, has to be a positive integer. You can't have fractional number of particles. So there's this implicit assumption here. Now, if I didn't take n to infinity, I would have information on this only for a positive integer power, and that's insufficient to derive the full Large deviation principle. In order to get a full Large deviation principle, I need all positive exponents, including the fractional, non-integral. If I send n to infinity, that translates into That translates into this exponent being one over n integers. And as n goes to infinity, that will become denser and denser and hence cover the entire real world. So there's a technical reason that we choose n goes going to infinity. Now, if you do all the scaling I told you on the previous slides, which I didn't write out here, but if you do that, you'll see that every quantity here goes exponential. Here goes exponential. There's some exponential nqt here and there. And this quantity that we're interested in is eventually boiled down to this. Because if everything is still exponentially, this thing is going to be controlled by the Large deviation of the attractive Brownian particles. So the natural thing I'm going to do next is to analyze, to study Large Deviation Principle of the scale particle system. So look at the empirical measure, which is you put a particle, a delta. Which is you put a particle, a delta mass, at each location of the particle, you let it evolve, you scale everything down by one over n. For each given time, this is a measure, positive measure with total mass n, and it is continuous in time. So the first thing you can do is you establish the Large deviation principle of this empirical measure. And that's the result. It satisfies all dp in the space with the speed. With the speed, and some explicit rate function, which I don't bother to write down. That's fairly explicit. Now, for experts in the audience, I just point out that for general rate-based diffusion, there's a known result by you folks that proves the Lauscheviasian principle, but it's under a different scaling. It's under a scaling where answerity is tap fixed. So the effect in this general result here, the effect of the diffusivity comes into play, whereas here, it's more like hyperbolic system. It's more like a hyperbolic system. Particle tends to cluster, and the diffusivity of the Brown emotions sort of washed away. It's a stiff. All right, so using the Large Dimension principle, you can now go back to the endpoint moments that we're interested in. So you just plot the result back to the moment. As I said, you get this pre-factor coming out of the transformation. Prefector coming out of the transformation, and the last bit, the last bit, this bit, is going to be given by the ray function, more precisely, optimizing all the strategy of the given starting condition, which are the point of our probing in the moment, and the delta initial. Oh, there's one thing I forgot to mention, which is that throughout this talk, what I'm doing. Throughout this talk, we're doing the delta initial condition for this cascade equation, and that's why you see every point has to come back to the origin. And that corresponds to narrow wedge initial condition for the KP's equation. So this bit here says you're going to look for all possible strategy of transporting mass from these given points all the way back to origin. That's the delta initial condition that I imposed, and find the best of that. and find the best effect. You minimize over it. Does that make sense so far? Now, this query wouldn't be so useful if I didn't tell you what is the best strategy. So later in this, we're going to analyze this moment 11 exponent. It turns out that you can actually explicitly characterize the minimizer, the optimizer of this variational problem, the strategy that realizes Strategy that realizes this endpoint model. So there exists a unique minimizer of this infigma here, and it's really just a sum of some delta functions. So each of them is a delta mass. They carry mass m1 through m5 here, and they move in a piecewise linear linear fashion, going backward in time. It's fairly explicit, and I'm going to tell you what it is in the next slides. Alright, here's the minimizer, which I call the optimal deviation. Go back to my time. And how do you describe them? Well, in order to describe them, let's not look at Lawschevi. Let's look at what happens without Lauschevier. If I just let this particle evolve freely under the same scale, what's going to happen? Well, I've written down the equation for the particles again. And as I said, n-square t is going to infinity. So typically, you're not seeing the effect of the Brownian motion. You are not seeing the effect of the Brownian motion. Okay? So initiate n times m1 particle here, n times m2 particle here, and so on and so forth. How are they going to evolve? Well, each of them feel the drift coming from this term here. So if I look at one of the clusters, each particle here is going to feel the pulling from this cluster and this cluster. There are M1 particle, the mass M1 particle in the first cluster. Particle in the first cluster. So there's minus that many drift going this way. And the same with this cluster. So the pulling is backward from these two clusters. And the same with these two. So initially, they're going to pick up this velocity just coming from the straight current here. And since the brown motion is not doing anything, they're just going to follow constant velocity. So that's this trajectory. And something's going to happen when they meet. Or when they meet, because Meet. But when they meet, because this colon is attractive, they're going to merge into a bigger cluster with the mass being combined. And how does the velocity evolve as they combine? Well, think about it. This drift is symmetric, right? So the polling from X3 and X3 on X2 would cancel each other once they combine into a cluster. So in terms of classical mechanics, let's just say they obey conservation of momentum. So the way they combine is following conservation of momentum. Following conservation of momentum. And evolve this. After some time, they arrive at where they want. Now that's typical behavior. But what do large deviation remember? Where does the large deviation come? It comes by requiring that all of the cluster, all the particles, have to come back to the origin at time one. So you do large deviation. The strategy is to take this and apply an affine drift that shifts the cluster back to the origin. The cluster back to the object in space-time. And the cost of this is easy to calculate, right? You apply a constant drift to these first three clusters, apply another constant drift to these two clusters. And the trajectory you get out of this is the unique minimizer of the variational problem that I showed you on the previous slide. Okay. For this, the ground emotion plays a role. That's right. It plays a role by adding a drift. So only their drift comes into play. The things are going to cluster. That's right. By velocity, you mean the slopes of those lines? The what? The velocity is the slopes of those lines? Yeah, the slope if you look this way. Any other questions? Alright, so so far we've been through this part of the chart. You see how charity growing particles can give you information on the endpoint moment of SHE. And now we're ready to go to the large deviation, operative large deviation principle. We just apply the Roundup transformation. So you have the Lyapunov exponent. Moment Lyapunov exponent. Moment, nephew, nothing's done. And you do a Legendre transformation in the power that will give you the Lach evasion principle with the rate function being the Legendre transform there. So, you know, just do it. Now, you get a rate function, and how do you interpret that rate function? This is how I interpret it. I haven't told you what a rate function is. So, this is how you see the rate function. Remember, I said a while ago, you have the hydrodynamic limit which is probable, and you want to pass through these points. Now, you take the Now, you take the convex hole of these points together with the parabola. Another way I like to describe it is: imagine that you have a rubber band that is wrapped around the parabola, and you pin it at these five points. That's the shape we give. And the rate function is given by this, the square derivative of that rubber band against that of the parabola. Now, this picture you can see from the Brownian-Gibbs long ensemble property, as Melan mentioned yesterday. Property, as Melan mentioned yesterday, and actually implement that idea to prove this Launch innovation principle. I have to emphasize that the way we got it is totally different through Logan formation. Of course, a result is a result. It has to be the same rate function. And one more thing. We only have access to positive parameters because we cannot have negative number of particles. And that will actually impose another restriction. We're not able to access the full architecturation. We're only able to access those. We're only able to access those such that this rubber band is counting. So, for example, things like this, if we do formally, it will require positive mass here and a negative mass here. That's just formal. And of course, our particles are normal particles, you can't have negative mass. But if you condition them to be bigger than all these points, then we'll just ignore this. Then you just ignore it. Well, I I'm saying condition to be a rank. Right. So let me check. What are these points? These are the points that I'm looking at the large deviation. I'm I'm saying what is the probability that a high function at x the scale high function at x one is around R one. Is around R1 and so on and so forth. But it will be the same answer in this case. It will be the same answer in this case, that's right. Any other questions? All right. So here's the architectural principle state, more precisely, and that is the scale. And that is the scaling that you should apply. So, this is a scaling you got by translating the scaling I told you a few slides ago. Okay. And so I examine, remember I told you that time doesn't have to be large. High can be zero, can be short, can be unit, can go into infinity, as long as we have the condition that n squared t going to infinity and going to infinity. So in short and unit order time, because of scale, Because the scale of the high function is n squared, in short and unit over time, what we're probing is any large deviation, any deviation bigger than big O1. And that's about the all-scale thing that you expect to see this kind of upper-tail deviation. So in short, meaning over time, as soon as you are beyond order one, you see this kind of deviation, and only if you look at that plastic. In long time, because we Time, because we require another technical assumption angle to infinity, we're only probing those deviations much larger than t. And in long time, I would imagine that the most natural scaling is to scale everything by t. In other words, n is equal to 1. That's sort of the thing I call hyperbolic scaling. In my opinion, it's the most natural scaling in any long time. We're not able to cover that because of the assumption they go to infinity, though the result that we saw and That you saw, and those I'm going to say should also hold in this scale. Here's some related literature. The one-point Large deviation Albertel is this three-half law, which is the same as the Tracy Whedon. I think it's known for a while, but the literature can pinpoint that predict this as this thesis. It was proven by me. Was proven by me and Sharon Das, and there's a bunch of other related results. And it was this result that Melan mentioned briefly yesterday, him with McGill and Gooley. They have detailed bounds, detailed tail bounds on this endpoint, upper tail deviation. And if you specialize it to the hyperbolic region, I believe you get the rate function and the shape, this rubber bin shape. Okay. So the last. So, the last part of the talk, so we've been through this informal activation, and the last part of the talk is to get our hands on the space side limit shape. So, what happens to the rest of the height function if you condition that this event happens? That I pass through these points unconditionally. So, there is a limit shape, H, somewhat explicit, such that if you condition It such that if you condition on that event, it's going to be concentrated around that. And this is a joint work with yearling, a separate joint work from what I told you earlier. So what is this lemma shape, which I'm going to explain? Well, at the terminal time, it is this rubber band thing. So you have this point that you're conditioning, and you ask the high function to pass through that discouragement. This curved, and that's going to be what you see at a terminal pi, of course. But now the question is: what do you get from pi zero to time? Before I told you what this H star is, let me remind you what is the hydrodynamic limit. So hydrodynamic limit is this analogous question where you don't have conditioning. You don't do large division. You just let the equation oneself. Typically, what's going to happen? What is the shape? What's going to happen? What is the shape without condition? And it was mentioned in the talk of Yuri Bachin yesterday that a system like this would, the hydrodynamic limit of it, would solve a Hamilton-Jacob equation, an invincible Hamilton-Jacob equation. And the special case that we're considering here is really just the integral of Burger's equation. It was a result of Chris Tiras and Field. Tiras and T-book that shows this thing. The hydrodynamic limit solved is the entropy solution of this integral of Broger's equation. And for a special case that we're considering here, the solution is very explicit. It's, you know, this problem where you replace this 2 by 2t. At one point level, it was already attained by near heightened anger. And just for experts, I want to just point out that this is analogous to the absolute angle limit of what's called strongly asymmetric particle systems. So what is the limit shape that we're interested in? Why do you see if you condition on passing through those unlikely points? Curiously enough, that conditional shape is also a solution of the same equation. Well, how could that be? Well, this is a fully nonlinear hyperbolic equation. So in general, has more than one weak solution. And whereas the hydrogen limit, you see the physical entropy solution. In the large deviation regime, you see a non-entropy solution. And we've seen this in the first talk of this conference. And here it is again. So it is a particular non-entropy weak solution. Naturally weak solution of that Burger's equation. Now, along on this plot, you see these lines here. And also here I'm plotting them in the TX plane. Or these other lines are the characteristic of the function. So these thick lines here are the shots. In Burke's equation, typically you see shots. But now if you look carefully, time goes up. And the characteristics shoot out of the shot. And the shot. This is non-entropy. For entropy solutions, the shot have to merge into, sorry, the characteristics have to merge into a shot. So this thing happens differently. That's why this is non-entropy. But how do I describe the solution, which non-entropy solution it is? And curiously enough, the entropy solution that describes, the non-entropy solution that describes the weak solution is obtained by running the equation backward in time and in the entropy fashion. And in the entropy fashion. So I think I'll show a line here. So which star as the end of the year? If you reverse time, a weak solution is still a weak solution. But in general, an entropy solution could be a non-entropy solution. So if you turn your head upside down, all the sharks merge. If you're going backward in time, this is actually an entropy solution. And you just reverse that, that's okay. Is that clear? But in the last two But in the last two slides, actually, so this is consistent with the general picture given by Jensen variant in terms of particle system. I won't have time to get into it. So in the last two slides, I'll try to explain to you how to understand things like this. So initial condition for the Mach equation is that you start at your solution. The rubber bit, right, yes. How do you know you end up in zero? Yes, um, so the rubber. So the rubber band touches with something happening here. But then when you're far out enough, it touch the product one. Now this thing, it solves both the forward and backward equation. And this thing here is all both the forward and backward equation. So once you're outside of it, you know this is. But outside of that, you know, this is going to happen. And push, since we just support everything, we push everything back to work. And which way you could do anything. You could use a remote convex. Yes, actually, this recipe should always work regardless of convex or not. Well, but only under this initial. If you are more general, it's a slightly different story. Should be. And curiously enough, And curiously enough, the solution, if you run backward in time, the solution actually blow up at this time. So this is the longest time you can run this backward in time. Beyond that, it just takes a while. It looks like your method of longness could just be narrowish. I'll briefly mention that. All right. So the physical mechanism of this, again, goes back to the conversation. To the conversation we have in Melan's talk. This is this phenomenon where, if you look at opportune deviation, most of the contribution of the noise actually concentrates around, so take one point, just one point at the origin. The best strategy to achieve that is to have anomaly of the noise within a small corridor around the origin, I think was referred to as highway yesterday. We call it noise corridor, not as the building as highway. Appealing as highway. Yeah, so that's the most appeal, that's sort of the most efficient strategy if you have one point. And this kind of phenomenon goes back very long, at least to work Timoth and Offer in the 90s, at least. That's one point. And you see, the shock, the non-entropic shock, really comes because of this highway phenomenon. All the contribution of noise concentrates around the place of the non-entropic shock. Of the non-entropic shocks. Now, you see, curtailing shooting out of the corridor for the same reason. In language-driven polymer, characteristics are the sort of the path of the polymer. If you were to follow a polymer from the origin to some place here, what it would do is stay for a while and then leave. It would stay a while because that's the preferred location. That's where all the action is going on. So the polymer would like to stay here for a while and then leave. For a while and then so that explains why current rations shoot out of the shot. And for end-point aviation, it's the same picture, right? You have these shocks, and then that's where they are. Now, that's the picture. But how do we understand these shocks, these noise curve? Does the picture somehow ring a bell? It is the sound. A phone. It is the same upon Jean transform as the optimal cluster of those particles. Think about what those particles are. They're sort of the representation of the Brownie motion of the path in the moment. And if everything is happening along the noise corridor, no wonder they like to be there. And that's how actually we have the work. All right. I think that's the end of my talk. Yeah. Yeah, there are still a few conjectures and things related to Drama's comment, but I'll probably not talk about them because since I've been giving this talk, I talked with a few people and they have very good idea. So I suspect many of them won't be open for very long, which I'm very happy about. And I think today is the last day of May. And Timo, your chair turn, I think it's end today. So Any questions? Could you say something about the interpretation of that three-house exponent from this perspective of the direct company? This is a heuristic you can get if you try to it's heuristic principle. If you try to solve the sort of hydrodynamic limit of this under, assuming there are states, there are, you know, you start from a point and you end up close to a point. You have a shape. And then you calculate the And you calculate the L to norm under that shape, you get here. So it's like these things tell you how the noise should deviate. Of course, the noise, it's not really the auto-normal. Noise is not even a function, but pretend it is. Calculate the auto-normal evaluation. And also also you can also see that from scaling. Let's say, you know, I I scale this, I put another parameter in front of n and see how this thing scales, you will also see that we have to scale. You also see the absolutely. So you rely on the fact that you have this nice representation in terms of the attractive gradient particles. If you do this for the other kind of solvable polymer models, let's say common polymer, you also have a nice delta-pose gas description and you have some clustering. Can you kind of implement your 100% sure? Sure. So I would incline not to go to the attractive brown particle picture. So in the case that we're dealing with here, another way to see how this works is to conjugate by the ground state. And a ground state for Delta both guys is simply explicit. For your all of the models. But is it nice enough that? But is it nice enough that you can work with your Okay, if that's the case then yes if you try to to make uh some representation of the expectation of e to the minus that e to the minus that yeah then make a moment if you if you go all the way negative like all the way negative like beyond All the way negative, like beyond the problem, my guess is that this picture is not valid. I mean, of course, negative moment will give you large deviation, but I don't think it's connected to a particle picture like this. But if you do something like this, I think possibly it would make sense. It doesn't mean that you can have a proof, but you know, somewhere positive and negative, but still above the probable. No, I'm just asking for combinatories or you know. I think Jeremy has an old idea. We'll have a shorter coffee and we'll be back at 10.45 if you're interested. 10:45, and if you're interested in afternoon options, then come back a couple of minutes early. Sir, Sundays.