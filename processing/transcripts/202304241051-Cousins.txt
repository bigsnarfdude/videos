Session: We have two speakers, Bob Cousins and Anthony Davidson. And the basic topic proposed is marginalizing profiling. So, Bob, over to you. Thanks. Okay, so in fact, I'm going to try to draw a big distinction between what's going on in the test statistic and which hasn't been discussed much, it seems, which is how to treat them in the simulation, which is in fact a big deal, as John Conway has emphasized 20 years ago at Flystax. Test. And that we're going to assume for this discussion that we want frequent coverage of the parameter of interest, no matter how we treat the use of the parameters. So I just have two slides. I'll go quickly over since my thesis and before. And that I put a few shots from his paper, what was called Minos. His paper, what is called Minos errors are based on delta minus log of likelihood. There's a figure from his paper. This I used to teach at UCLA for decades. Here's the table of chi-squared people and whatnot. So statisticians should recognize this, but this for us is called minui minos, because that's what Fred called it era in 1964. So at some point, over. Oh, can I say it again? Oh, I forgot up here. So it wasn't until Wolfgang gave a talk at the first or second meeting in 2000 that we learned that this was called profile language. That's when they learned about language. And it was actually an amusing story how Fred James and Wolf Klein realized they were doing the same thing another time. So at some point, a few people started integrating out nuisance parameters. Parameters. Typically, when there was a Gaussian normal contribution to the likelihood while treating the parameter of interest in a frequentist way, this was mentioned that Virgil I and I wrote this article. We didn't know what we're doing in those days, so that's a confused article. For a more enlightened discussion, you can see a more recent paper I'll talk about. Kazaslam and Tucker 15 years ago draw on this a lot in a talk. A lot of talk. When you reach my age, they get a talking off and they can pull something older. So now Luc Des Moinier actually realized that in these cases like here, that what we did actually was the same math as George Box's famous parpole to p-value in 1960, says Fisher's plot. That what Box did was he calculated tail probability in the distribution. Distribution after obtaining a Bayesian PDF. And so that was putting out a frequentist afterburner to a Bayesian calculation. What we did was take our frequentist tail probability and average it over a Bayesian PDF. So if you just reverse the order of the two integrals, it's the same equation. It's kind of unusual. Now, the profile-likelihood ratio, I would say, is by far the most common test statistic used in. The most common test statistic used at the LAC, probably in all of HEP. I didn't know my statement. One of the meetings of neutrino experiments were mentioned, marginalizing. This is because of MENO's history. There's this paper, some of you may know about with Gary Filma and I wrote, that while it was improved, we realized it was the no-nuisance special case of, here's a dusty old Candeland Stewart from many, many decades that has the program. That has the profile likelihood ratio in it. There's a current GPS invented, so-called modified frequency method that you may hear about here, CLS, but that's the ratio of two p-values that are both based on profile likelihood statistics, although in the early days at least there was some marginalization as well. So, a really key paper was by Glenn and our session chair, and Cla and Ailman. And Kwell and Aylman and Olive asymptotic formula for likelihood based in physics. And this was great, giving us all these asymptotic formulas for not only profile likelihood ratio, but various variants with extra constraints and the material. But it had a really interesting side effect, which is since these asymptotic distributions were not known in hygiene physics for other test statistics, like organizing and whatnot, then And whatnot. Then the default began to use these expressions in their paper, even for small sample size, so for consistency. So when you did your Monte Carlo and you got to large sample size, you should check that you should converge to their answer, which might not be the case if you do some other thing. So that side effect to me is as important as the original paper. Marginalization, I think, kind of ended with that paper a lot. Ending with that pain in a lot of ways. Now, but then if you look at the statistics literature, and Anthony will talk about this, there's a long history of criticism of this simple profile likelihood that we've been using for us. In fact, going back to 2005, there were various talks that had higher-order approximations, partial likelihood, adjusted likelihood, modified likelihood. When David Cox died, When David Cox died, there was a lot about his partial elective and whatnot. In my talk, I tried to understand some statistics literature. Nancy Reed gave a charitable discussion of it in her workshop and summary talk. But then also people here, Anthony and Alessandra and Igor, all talked about Harvard asymptotic thoughts in different meetings. And what's interesting is that essentially none of these. But interesting is that essentially none of these developments in development didn't have issues. So it could easily just be a matter of software tools because what we use is what we have software tools for. But it would be interesting. Instead, it's not like we blindly use profile likelihood. Instead of using these higher order asymptotics and then using the tables of how to convert what comes out to a p-value, we, be because airbind higher energy physics learns how to do High energy physics learns how to do Monte Carlo. What we do is Monte Carlo, I'm getting ahead of myself here. Let's go to this. So the marginalizing music says it's rare at the LAC for the reasons I just given. But our Bayesian friends, too bad Jamberger is not here this time, but they here's a whole paper they wrote on integrated likelihood methods for limited uses. Likely methods for limiting reasons parameters in otherwise frequent discriminations, where he, one of the commenters, Susko said, well, you can just compare. Well, let's read what Jim said about it, Jim and all. Dr. Susko finishes by suggesting that it might be useful to compute both profile likelihood and an agreement of likelihood answers in application as sort of a sensitivity study. It's probably the case that if the two answers agree, then one can feel relatively assured in the relation. And one can feel relatively assured in the related answer. It is less clear what to think in case they disagree, however. If the answers are relatively precise and quite different, we would simply suspect that it is a situation with a bad profile agreement. So you see where they're coming from. Of course, they give examples. I tend to agree with, well, no, one of the tends to agree, again, one of my standard slides on all these foxes, you should do them both. Do both the profile and the marginalization, and compare them, and you'll learn something about your problem, especially if they disagree. I think we really need more studies in real cases to see if this is true or not. Find some cases where they disagree. Okay, so now the point I started to make a moment ago: instead of using these high order asymptotics, we don't just blindly use the profile likelihood and read off the chi-square table. And read off the chi-squared table what our p-value is, we run toys. So, everybody in Iran in physics, statisticians may find this different from other sciences, is we know how to do Monte Carlo, and we'd like to do it, and we're good at it. And so we use Monte Carlo simulation, what we call Toy Monte Carlo sometimes, because it's often a simplified version, to obtain the finite sample distributions of the profile likelihood under the normal hypothesis and other alternatives as desired. Alternatives as desired. And then, as I say, as you get more and more, larger and larger sample size, you're then compared to Glenn's formulas. So for me, the real question, the hard one for this conference, is how should the nuisance parameters be treated in the simulation to get the lowest change? And because keep in mind, we want correct frequent discoverage of the parameter of interest when nature is sampling from the under. When nature is sampling from the unknown true values of the distance parameters, all we have is the light. So the usual procedure, which is based on a desire to be, quote, fully frequentist, I think that's a phrase that goes back to the people in about that era. So we throw toys using the profile values in the nuisance parameters. As we take their maximum likelihood estimates, conditional on whatever variable parameter of interest has been tested, at some point, I think it was Luke DeWont. At some point, I think it was Luke DeWontee who told me I should read the last chapter of Reference Edel's book called Bootstrap, and it's called Parametric Bootstrap. And so the hope is that just throwing one set of toys using these profile values of the nuisance parameters, that that's a good proxy for the unknown values. But it's a whole. It's true asymptotically. So I would like to see more comparisons of these results with the alternatives, particularly those from marginalized and new. Particularly those from marginalizing the nuisance parameters, which is always going to require some judiciously chosen priors that don't represent subjective complete, but are chosen explicitly to get decent coverage. I personally find it a bit comforting to average over a set of nuisance parameters in the neighborhood of the profile value than just using only that one value, especially if you have a bin with zero events in it, then you're going to start doing silly things. Start doing silly things, perhaps. Now, I'm just going to have one example I'm going to spend the rest of my time on here. I cherry-picked this example, you'll see why in a moment. But first of all, it appears often in statistical literature. It's an important prototype in higher energy physics and astronomy, and I happen to be a co-author of that 15-year-old paper about it. That's the ratio-Poisson means problem, where there's two Poisson means and only one ratio, so there's only one distance parent, and you have some choice to pick. One distance apparently you have some choice to pick what it is. And we wrote this paper on it with Jim Lennon and my student, Jordan Tucker. That has all the math. I'm just going to try to talk about it conceptually, even so I still math still. So it's a histogram with two bins, or it's an astronomer point, a gamma-ray telescope over here and over here, and that's the on-off version. And so there's in the signal bin, there's In the signal bin, there's an unknown signal mean and an unknown background mean, and in the sideband or the off, the control region, you have an unknown off mean. But we're going to assume to make it simpler that it's true in astronomy, you know exactly the ratio of the background of the two mens if you just know the ratio of the times in the two different directions you look. It's not quite as obvious in energy physics with all that hair. Here, but let's say we know the ratio of μ off to νB exactly. So then we can, our hypothesis that we want to test is that the signal mean is zero and the signal bin. We can rephrase this to say that we want saying that the mu in the off-divided by the mu in the two mu's here is equal to tau, which is what it gives us most signal. So the ratio of Passammi. Uh so the ratio of Poisson means this tau. Here's we connect to the statistics problem. Uh we let's take the mu B as a nuisance parameter. So the standard solution in for example Nancy Reed's review of conditioning, you take, you note that the total number, oh I didn't say get to the observation. So you observe in-on events in this pin and in-off events in this pin. So you observe that this total of those That this total of those amount of events has no information about the ratio. So, this is an ancillary statistic, and the statisticians say. So, you treat, you analyze the data as if this had been fixed from the beginning instead of being thrown randomly by nature. And now you consider the binomial distribution of n on and naught within a total n. So, now again, we've got another way of stating the null hypothesis, which is the binomial parameter rho is equal to is equal to mu b over mu b plus tau mu b, which is what it is if there's no signal, so 1 over 1 plus tau. Another way to say this, there's two ways to write down the likelihood function. The kind of obvious way is that there's independent Poisson likelihood for each of these two bins. So that's Poisson of n off given its mean, and then the Poisson of N on given its mean. And on given its mean, and Poisson, and off given its mean. But it's just math to rewrite this as, factorize it as the Poisson of the total number times binomial of this, and then you throw this away for further analysis because it doesn't allow you information about anything, except what we don't care about. So we do a binomial test of rho equals 1 over 1 plus tau given these data, and then you get your p-value. And then you get your p-value by inverting the confidence center along the converting the z-value as we do with that multi-dimension. So, this whole approach, which is, I would say, the standard statistics approach, gives you a z value we'll call z, sub binomial. But then you need a binomial confidence interval. Our paper used Copper-Pearson confidence intervals, which badly are badly conservative. We, in the end, did a lot of studies and came down with Lancaster-mid-P. Down with Lancaster mid-P confidence intervals for a reason we get into it if you want to. So now, alternative is to do marginalization with this nuisance parameter. And I think, in the interest of time, I'll skip over most of this slide because I think most of you all can just imagine what marginalization is. What you do as you look in this bin here to Bin here to get some information about mu off, and then that from that you can get a likelihood for mu B, and then you choose a uniform prior. Okay, that's questionable, but from the end you get a posterior probability for the gamma function, and then you take a weighted average. This is the integral here that we call the Bayesian frequent is hybrid, which is averaging Averaging weighted by the posterior for mu B over the different p-values you get for different known values of mu B. Map that to Z. So Jim Leneman was playing around with this before the Slack flashed it with a student, and they discovered numerically that this marginalization method gave exactly the right answer, the Z Var. I said I'd cherry pick this answer. Because marginalization is the known correct answer. No incorrect answer. And on the other hand, if you do parametric bootstrap, which I did by myself, and then I also checked it against this famous combined program that Nick and Frims wrote, that the math for Lee and Ma in the Gamma Ray community wrote down the profile likelihood estimate. But conceptually, you use all the data. Use all the data by, you go back to this end tote being a sample from this object, and then your profile likelihood you can throw away there. This is all. And then you do parametric bootstrap using the profile value of Mu Sophia's truth, and then proceed to calculate it as above, and you do that by simulation, direct calculation. So I have two numerical examples, and in the interest of time, I'll skip the first one. I'll skip the first one. Yeah, no, second one's the point. So let's suppose it's known that tau is two. That is, the ratio of background in the side band to the ratio of background in the signal band is two. That's known exactly. And then so you go to our exact ZPI formulas using Clapper-Pearson, that gives you, oh, sorry. Oh, sorry, and the data, the data, you see 10 events in this bin and two events in this bin. And whereas the null hypothesis, there's twice as much background here as there is here. Get excited because we see. Yeah, in fact, if you do the ZBI, which is the same answer as the marginalization, you get a 3.2 seconds. If you use the Landcastor-Mint Pete polynomial, you get 3.44 sigma. The asymptotic result using Wilkes theorem is 3.58 sigma, which looks like an old rest of that. Parametric bootstrap is kind of interesting because if the signal is zero and it's supposed to have twice as much background here as here. Background here is here, and you see 10 and 12. Well, it works out that the maximum profile estimate of mu b is 4, and mu off is 8. So when you go to throw your toys with the parametric bootstrap, you're throwing toys with μ of 4 and 8 in these two bins where you observe 10 and 2. This is where I start to worry the pick, you know. And that lines up with 3.46. So the exact, so-called exact answer is 3.27, and this gets a bigger statistical significance. So you start to say, well, it looks like this parametric bootstrap is overestimating the significance. But then you got to remember that this exact is in quotes, is what Fisher called it. In fact, because of discreetness, it overcovers. It overcovers. And so maybe we better take a look at this. So we do coverage studies following that old paper, and we pick some threshold like 3 sigma. And then we consider a pair of true values like the unknown mean background and of tau. And for each pair, we consider all possible data sets. For each one, we compute a claimed z-value. And then we Value, and then we find the probability that the claims value is bigger than the one we want, like 3 sigma. And then from that, we can map from the find out how often we're conservative or liberal or whatever. And I just do this by direct calculation. So in our paper, this is a plot in this plane of the known true value of tau, the unknown true value of mu B. Tau, the unknown true value of mu B. And this temperature map here is zero. And here, this is the difference between the true value of Z and the claimed value of Z all over this plane. And this one was for Z claimed equals 5, actually. And so in fact, you see, it's the red, which means your true value. Your true value is actually bigger than your client. You're conservative. So, all this at low end, this thing is horribly conservative because you're using this Klaus-Pearson binomial confidence curve. So, I did a couple points doing the parametric bootstrap. So, for tau equals 1 and mu b equals 10, and the z claim of 3, the true z was exactly 3. So, the lack of the The undercoverage due to the parametric bootstrap got exactly compensated by the overcoverage due to the discreetness. Same way with Z claim equals 5, Z true is 5. So what a moment ago looked like it was horribly undercover and in fact combined with this red here to be great. Okay. So I stopped there. So uh two questions really, two or three questions. First of all, how much are we losing by not examining more thoroughly this fire and Examining more thoroughly this time that's not a hearing and coding lessons. Clearly, I have to do it to English and coding up this stuff, so some students can just start using it and comparing that. And then, as I say, I cherry-picked my example problem to be one in which I knew that marginalizing the nuisance parameter yielded the standard frequency Z, while doing fully frequent parametric bootstrap, as done in the LAC, would give a higher Z with the discrete nature of problem-saves encode, which is parametric bootstrap. For Gaussian uncertainty on the mean, we would be marginalized in New South Wales can be badly anti-conservative. Kyle pointed this out at 2005. We studied a hell out of it in this paper. So my big point is that we should explore these issues in real analysis. There are references and thanks and whatnot. The backup I have a little bit about Virgil. The paper with Virgil and some. paper with Rugal and and some thanks. All right, thanks Bob I suggest if people have questions that are pressing above a certain threshold that we can have them now. But after Anthony's talk we can come back and I think address the whole question. But so questions now for Paul. So about cherry Urban about cherry-picking. The two examples you quoted on the previous slide that gave exactly the right answer for the parametric mood style. You don't have examples where it gives the wrong answer then. Or do you suspect that it's more general? Are you going to generalize from those two examples? No, I did all this work about six weeks ago thinking that I would get to that. Okay. So now. So that's a nice exercise for the audience. Good. The evenings don't have fixed drugs yet, so plenty of time for people to do that. I was just going to say the exercise is already done. The exercise is already done. So I'll talk about it in that talk. Anything else for Bob, Ebert? So there are two. So there are two Bayesian approaches, subjective and objective, which one do you prefer for this kind of integrated likelihood exercise? So, I mean, so people know, I have some sympathy for subjective Bayesian, but for what we're doing, there's one criterion, which is coverage. And so, as you know, there's a whole theory that for estimation in Jeffrey's par one parameter is That's, well, it's coverage of one we're in. For this equation information, for this, for this information. So, pardon? Well, if it's a whole thing, probably be out of school this workshop, but maybe not. If you want to start using subjective priors in your decision theory, I'd like to have some sympathy for that whole sphere. But I think in our field, But I think uh in our field at least when we say five sigma that implies a frequentness probability for the tail quote. If you were doing subjective analysis, your final result wouldn't be a p-value. And Glenn, in fact, has done a lot of interesting work on this some years ago, right? Wasn't it PDFs you were trying to use subjective priorities? That's been reincarnated in something we'll hear about in a minute. Maybe it's quite a species you want to put close. Maybe it's quite a specific one for thought, so it's worth asking. You had uh you had these two ideas whether or not you just sort of take a parametric bootstrap and you say, Okay, I'm going to assume these are the true values, and then you had you phrased it as kind of averaging over a local neighbourhood. But the third way ought to be take the barometric bootstrap, calculate your intervals, check the coverage, and then just try a different set of parameters and that again and see if the coverage is. I mean, it's not people who do those checks forever. Forever. Remember, you and I had a lunch table conversation about things once. And again, I have a slide on this in all my talks. You want to clearly want to check coverage for the parametric bootstrap values, which is more bug check if you. But I would say you want to check it for some more. And then it'd be interesting for the statisticians to read this paper by Jim Berger and friends that basically says if the answers are different, that's a parameter boost at this row. You know that, maybe I should make one other quick point. Tom Loreno likes to talk about it, I'm sure other people do too, but I first heard Tom Loreno talk about it, is that for us physicists Maximum likelihood estimate is like the temperature of an object. And the Bayesian way of doing things is like the heat content of the object. And so if you're using profile likelihood, you're going to the hottest spot of the nuisance parameter where its temperature is the hottest. But if it's some big narrow spike, Big narrow spike with a reasonable prior, there's no heat there. And so you want to be going to the region of the nuisance parameter distribution where all the heat is. And of course, that needs a prior. And so that's the big first question you ask about that. But if you can find out. So when they construct examples where profile likelihood fail, it's usually some singularity in the nuisance parameter that drags you off to some weird place. To some weird place that is irrelevant. I don't know if it's checking this now. Okay. Are there questions online for this? I should have asked maybe that the mutual. Okay. Good? Very good? Good? We're good. If there's no other urgent questions for Bob, can I suggest that we transfer over to Anthony? But we can continue then the whole discussion. Sorry, you don't want to go to the website. We'll do. Can you just read that one? And then