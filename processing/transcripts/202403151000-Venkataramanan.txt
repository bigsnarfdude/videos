Thanks very much, JF, and to all the organizers. Can you all hear me? Yes. Okay, great. Yeah, thanks for putting together such a wonderful program. I really wish I could be there, but at least I've enjoyed watching some of the talks online. So this is, again, like many of the talks in this workshop, it's a talk on multiple access with many users, and it's based on a bunch of joint work. Joint work with first part of the talk is a paper with my former PhD student Kwan Shea and Siddi. And the second part of my talk, I'll talk about more recent work with current PhD students, Shervi and Pavel. Right, so here's the setting. We have the Gaussian multiple access channel where we have L users and each of them transmits a code word over N users of the channel. And the code words are labeled C1 through CL. Through CL, and the channel receives the sum of the code words plus noise. So, now, like in many of the other talks, we're going to study the regime where the number of users is very large, but each of them has a small amount of information or data payload to transmit. So, this many user regime has been studied by several works, perhaps starting from this work by Chen, Chen, and Guo, and different scaling regimes have been studied. Um and different scaling regimes have been studied where by scaling regimes I mean where the number of users yes slides aren't moving slides aren't moving yeah we just still see the title page oh um okay now now we see the page with everybody okay so these are my co-authors sorry I was just finished um yeah okay what about this now yeah you're good No? Yeah, you're good. Gauchenbai. Okay, should I just leave it like this? Or let me try. Okay. Is this good? Good. Okay. Okay, great. So sorry about the error. So here's the multiple access channel. So as I said, C1 through CL, the output Y is the sum of the code words plus Gaussian noise. And we're going to study the regime where the number of users scales with the block length. So there's been many different scaling regimes studied, and we are going to consider the one the linear. The one, um, the linear scaling regime where L grows proportionally with n. So, l being the number of users, n being the code length, and um, l over n being user density. So, this raging was first studied by Poliansky in this 2017 paper and with by himself and a bunch of his co-authors later on. Um, so in the here, we're going to consider fixed user payload of log M bits. So, think of M. Of log m bits. So think of log m as 10 or 100 or something like that. And each user's code word has a finite energy. So EB log m is the energy, EB being the energy per bit. And the metric we're going to use, error probability metric, is the average probability of error for all the users or the PUP per user error probability. Okay, so we're going to keep mu L over N fixed and L and N. L over N fixed, and L and N are going to grow large proportionally. EB, the energy per bit, and the packet size, the payload, don't change as well. And the trade-off you can see then is what is the minimum EB over N naught required for a given user density and a target PUP. So in other words, if you want to achieve a user density of say 0.2, what is the minimum energy per bit you require? With target PUP, say, 10 to the minus 3. Okay. Okay, so we'll at the very end, I'll also talk a bit about this recent work on how you can use the same schemes for when you include random user activity, where not all the users may be transmitting information or code words, some fraction of them may be silent. And there you have trade-offs between different sorts of errors, where you have misdetection, where you decode an active user as inactive, and false alarm, which is the other way around. Which is the other way around, and when you decode a active user wrongly. Okay, so before we move on, I'd like to emphasize that this talk is about a different setting than unsourced random access. It is not unsourced in the sense that each user, we are going to assume, has a separate code book, right? And the decoder knows all these code books. So, a lot of these talks in this workshop, I understand, have been around on unsourced random access, but On unsourced random access, but the hope is that this Gmax setting where different users have different codebooks is still interesting, and a lot of the techniques can actually translate and inform schemes for unsourced random access. Okay, for our GMAC, many user GMAC setting, there's been inner and outer bounds, so converse bounds in this paper by Polyansky, and then in this series of papers, there have been a sequence of bounds. A sequence of bounds based on random Gaussian codebooks and maximum likelihood decoding. So I'll show you some plots in a few slides. But this talk is not going to be about bounds. It's about how you can get as close as possible to the outer bounds of the converse using efficient coding schemes. I think we can't really see. Yeah, you can't see previous work. Yeah, this is one. Can you see previous work? This talk? Yeah. Okay. Okay, I think I'll not maximize my screen then. Okay. Is this okay? Yeah. Okay. So, in this talk, we'll be talking about efficient coding schemes. And in the first part, I'll be concentrating on small user payloads. And here, sparse regression-based schemes do really well. Okay, and these are all probably things you've heard in the last few days, but I'll review what was needed. In the second part of the talk, you'll see that Spark-based schemes. Part of the talk, we'll see that spark-based schemes don't do so well for large payloads. And here we'll look at coded CDMA-based schemes. And for both of these, I'll use approximate message passing decoding. And we'll also consider both IID designs as well as spatially coupled designs. Okay, so just as a reminder, here's the setting. Fixed user density, fixed payload size per user, fixed EB, and for a target PUP, we want to ask what's the minimum. Want to ask what's the minimum EB over N0 for a given user density. So, since my video was sort of cutting out, are there any questions on the setting so far? There are no questions. Okay, great. So, I will move on. Okay, so here is an example of the bounds from this Zadik Poliansky-Trumpolidis paper. So, here the user payload is eight bits, and this red curve is the converse bound, which shows Bound, which shows, which tells you that anything to the left of this red curve cannot be achieved. So on the y-axis, you have user density, and on the x, you have eb over m0. So for instance, it says that if you have EB over m04, you cannot achieve a user density greater than, say, 0.2. And the black curve is the achievability bound, which means everything to the right of it can be achieved. Okay, so. Okay, so you see that for eight payload of eight bits, there's quite a bit of a noticeable gap between the inner and outer bounds. And when you go to a larger payload, 200 bits, the bounds seem a lot closer. And the y-axis, the achievable user densities are also an order of magnitude smaller, as you might expect, because each user is transmitting many more bits. Okay, so our goal today is to get as close to these bounds as possible, both for small payloads and large payloads. Both for small payloads and large payloads. Look at the efficient schemes. Okay, so the first type of schemes I'm going to talk about are random based on random linear coding. Here, the idea is that each user has a matrix, a random matrix, AI, and it's n times b. So remember, n is the code word length, and b is some small number that will be related to the payload. So they have an n times b matrix, and the code word is generated. And their code word is generated as AI times XI. They multiply this matrix by a message vector of length B to generate an n-length code word. And each of them sends that code word to the channel. So what you get at the receiver is the sum of AI XI, a plus noise. And of course, if you concatenate these matrices A1 through AL, each of them being N by B, you get a big design matrix, which is N times B L. n times b l. So you have l users, each of them have you know b columns in their matrix. And so this problem can be represented as y equals a times x plus w, where a is this big concatenated matrix and x is the concatenation of all the message vectors of the L users, where it has a very specific structure that you have L users and each user gets B columns. Okay? And those Okay, and those B columns, each the B columns of each user can be drawn according to some prior. And I'll give you some examples now. Okay, so for now, think of all these random matrices as IID Gaussian. So this big random matrix is IID Gaussian. And for random code books, where each user has a, you know, a random code word, picks a random code word from their matrix and transmits them, then you just have B equals M. Okay, you have M messages for you. Okay, you have m messages per user for log m bits, and you have b equals m, and each xi has a single non-zero entry among its b entries. Okay, and we set that entry to be square root e to satisfy the energy constraint. So this is just standard Gaussian random coding. Next, you can also consider random coding with binary modulation, which we do in a lot of our experiments. So you can have b equals m over 2. So for concreteness, let's think about sending 8 bits. Let's think about sending eight bits. So log m is eight, m is 256. So random coding would mean each AI has 256 columns and each of these XIs also. With binary modulation, you can reduce that by half. So you can reduce it to 128 columns, and then you can signal the non, you can have the non-zero to be either plus square root e or minus square root e, where you send one extra bit using that. Okay, so you have the non-zero zero. Bit using that. Okay, so you power the number of columns. And then at the other extreme, you could have just CDMA, where you have just a single signature sequence for each user. And so B is one, but then the Xi, which is now a scalar, is drawn from an MRE constellation to convey log and bits. Okay, so these are all covered by this framework. And initially, I'll talk about IND Gaussian A because it's easier to understand. These are easier to understand the algorithms for, but all of these we've also done for spatially coupled A, which just means that your overall design matrix looks something like this. Okay, it has this sort of blockwise band diagonal structure where you have IID Gaussian entries on the band diagonals of blocks, and then you have zeros elsewhere. And I'll tell you later about how this sort of structure helps improve the trade-offs you can achieve. Okay, so let's start. Okay, so let's start with an IID Gaussian matrix. So each user remember sends AI XI. So the decoding task is to recover all these XIs, or equivalently, this concatenated message vector X from an observation of the form AX plus W. Okay, so now A is a known ID-Gaussian matrix. So as you've heard in many, many talks, AMP is a good way to recover your message vector X because it can be tailored. Message vector X because it can be tailored to the prior, whether it's a random Gaussian codebook or with binary modulation or CDMA, you can design pretty easily an AMP decoder tailored to the prior. Okay, I'm not going to give you the AMP equations because you've seen it many times. So instead, what I'll just show you is the result and some plots. Okay, so what you can do with the AMP theory is that you can precisely characterize the asymptotic error rate as the number of users. rate as the number of users and the block length both go to infinity. So remember they go to infinity proportionally with L over N fixed at mu. And so by asymptotic user error rate, I mean where first this L and N go to infinity proportionally, and then you also run the AMP algorithm for a very large number of iterations. So if you do this, it turns out that there's a theoretical curve you can draw, that's this blue curve, that tells you the achievable The achievable trade-off by AMP. So everything below this blue curve is achievable. And these crosses show you simulations with 500 users. And you see that there's pretty reasonable agreement, although it's not a very large number of users. So you see that we actually improve on the ML and random Gaussian codebook achievability bound in the low SNR regime, but there's quite a gap in the There's quite a gap in the higher S and R region. Okay, so what I'm now going to do is tell you how this blue curve is obtained and then tell you a bit about how we close this gap. So any questions so far? Questions. Okay, great. Yeah, it's useful for me because I can't see you guys in case something freezes or, you know. Yeah, in case something freezes or something's not clear, please please stop me. Right, so this blue code, it's obtained from what's called a single user affected channel. So remember, each user has a message vector of length B to transmit. So I call that X. And X is drawn according to the prior of, you know, used by the code. So for instance, random Gaussian codebook will be x has b entries with a single non-zero entry equal to. non-zero entry equal to square root e. And if it has binary modulation, it'll have plus or minus square root e and so on. And the single user channel is just the channel whose output is x plus a Gaussian noise, independent Gaussian noise of variance tau. Okay, so it's a vector channel with white Gaussian noise of variance tau. Now, B is a finite number depending on your payload. And so you can do computations of things like error probabilities on the single user. Of things like error probabilities on the single user channel. This is just standard detection theory. So, for instance, you can define the map estimator on this channel, which picks the most likely code word from the codebook, given this noisy observation, and you can define the probability of error of that map estimator, and so on. So, for things like Gaussian random codebooks, you even have closed form expressions in terms of the S and E and tau. So, tau is the noise parameter here. So, in terms of the single user channel, So in terms of the single user channel, you can characterize the asymptotic user error rate of the AMP decoder for IID Gaussian A. So it says that, the theorem says that the asymptotic user error rate converges to the probability of error over the single user channel, which you can compute at this noise variance tau fp. And this tau fp is a number, it's this noise variance you can compute. Sorry, is there a question? Okay. Okay. All right. So tau fp is a number you can compute using this so-called potential function. So this potential function f of tau involves two terms. One is the mutual information of our channel with the chosen prior on the code words. And the second term is some function of mu tau. And this theorem says that tau fp is the largest stationary point of this potential function and the error probability of the single user channel at that noise variance. That noise variance will characterize precisely the asymptotic user error rate of our AMPD quore. Okay, so I'm going to mostly show you pictures with some theorems like these interspersed. So let's look at some pictures to interpret what the Star Wars P might look like. So here's a potential function I've drawn for a toy example where each user has just one bit to send. So m equals, so I'm using b equals two. So it chooses one of two columns. And this is the f of type. And this is the f of tau I plot against some linear scaling of tau. So this is, you can think of the x-axis as, it's this value, which is a function of tau, but you can think of it as a proxy for the error probability that we're interested in. So remember, we are interested in the largest stationary point. And if we fix mu as 2 and we start thinking about increasing our SNR, V over M naught, at 80. Ev over m0. At 8 dv, you just have one stationary point. So this is the largest one, and you see that our error probability is quite high. It's over 10 to the minus 1. So let me increase the SNR a bit more. So it's about 10. And now this has two stationary points, this potential function. But the theorem says, you know, AMP, user error rate is given by the largest stationary point. So you're still stuck at this large, relatively large value. It's come down a little, but not really by much. Really, by much. Okay, so let me increase it a bit more to 12. Again, there are two stationary points. One of them is really small, and the other one is relatively large over 10 to the minus one, and AMP still has a rather high user error rate. So finally, if you increase it even more to 15.8 dB or something, then you get this red curve where it has a single stationary point right very close to 10 to the minus 5 or something quite small. So here, AMP does really well, and almost all the users are. And almost all the users are decoded correctly. Okay, so for different SNRs, you can plot this potential function and find this minimum. And this will tell you what, for this mu, what minimum EV over N0 you need to guarantee certain user error rate. Okay, so this is a nice computable characterization. But looking at this, you see that after a certain point, as you increase the SNR, there are multiple minima for this potential function. For this potential function. And unfortunately, the theorem only guarantees that our error probability is going to be the largest stationary point. And it would be really nice if we could achieve this global minimum, right? Because even at 12 dB, the global minimum is like less than 10 to the minus 4. So we'd really like to achieve that. And the way to achieve that is by spatial coupling. So remember, I used an IID matrix so far. Now I'm just going to change the matrix and nothing else. And nothing else. So, this matrix is now going to have this sort of a band diagonal structure. Okay, so let me. So, there's lots of symbols here. So, the main thing you need to, you know, just notice is that this big combined matrix, A, which consists of all the users' AIs stacked next to one another, this combined matrix is chopped up into blocks. So, there are seven column blocks here. And among these seven column blocks in the bulk, only three of them are. Blocks in the bulk, only three of them are non-zero for, you know, at any given point in time. Right? So, this omega is this so-called coupling parameter, which tells you how many blocks in each row and column are non-zero. And C is the other parameter that tells you how many column blocks this combined design matrix is chopped up into. Okay, so this sort of structure was first used for compressed sensing by in this paper by Codeter and Pfister, and then some theory was developed for it by Donahoe and others in this paper. By Donovo and others in this paper, and since then it's been used for sparse repression codes and lots of other things. So now, when we use this matrix, you can write down an AMP and so on. So the machinery is sort of well developed by this point. But in this particular GMAX setting, it turns out spatial coupling has a nice physical or physical interpretation. So let's look at this toy example where you have 25 users and say you use the spatially coupled design. Use the spatially coupled design, and you have 35-channel users. And say the non-zero entry pattern looks like this for the design matrix. So, what this is saying is for the first five blocks of users, they stay active only in time slots, say, 1 through 15. And the next set of users, say, in blocks 6 to 10, are active in time slots from time 6 to 20, and so on. 20, and so on. So, at any given time, you have a certain number of blocks of users who are active, but then they become inactive after a certain point. Okay, they start becoming active at some point, and then they become inactive. So, it's sort of like time division, but there's time, there's overlap. So, it's block-wise time division with overlap, where you control the number of users who are active at any given time. And the intuition is that if you look at the very beginning and very end of this matrix, these blocks of users have less interference because. Users have less interference because the others have not become active anymore. And so AMP will find them relatively easy to decode, and that'll help decode the adjacent blocks and so on. And so there's a decoding wave that propagates. Okay, so that's the intuition. And you can write down an AMP algorithm and state evolution and all that. Okay, so happily we get what we want. This theorem basically says that the user error rate with a spatially coupled Gaussian A is essentially the given by the Essentially, the given by the error probability of the single-user channel, but now with the noise variance computed by the global minimum of the potential function. So, very same potential function, but now you can achieve the global minimum. Okay, if you want to be a bit more precise, you can achieve the error probability corresponding to tau star plus delta. Delta can be arbitrarily small, can be made arbitrarily small by taking your coupling with omega and your number of blocks to be large enough. To be large enough. Okay, so essentially, this gets you the global minimum. And now let's plot and look at what it gives you. So it gives you this green curve, which happily is very, very close to the red curve. Okay, so again, this is user payload of eight bits, and this tells you the minimum EB over N0 needed to achieve a certain user density. Okay, so it's strictly better than the ZPT achievability bound. And with 5,000 users. Bound and with 5000 users, you get these crosses here. And you see, you know, there is a gap from this theoretical potential function-based bound. And that is typical with spatial coupling because it's known that you need really large code lengths to sort of get close to the promise limits. Okay, but still, you see that it does significantly better than the IID Gaussian already. Okay, so for a user payload of eight bits, so this is pretty close to the converse curve, and you know. Close to the converse curve, and you know that was nice. But unfortunately, this sort of coding scheme runs out of steam when you want to transmit larger payloads. And the reason is to achieve this, what I used was essentially a random code books with eight bits. So you had two to the eight columns per user with one non-zero, or two to the seven columns with one non-zero, and then plus or minus square OT binary modulation. And if you want to transmit, say, 100 bits, And if you want to transmit, say, 100 bits, few hundred bits, this is you know clearly not feasible. Two to the hundred is just not possible. So you have to do something simpler. So one thing you can do, and we did in our paper, was, okay, you can take this 8-bit scheme, and if you want to transmit, say, 120 bits, you can use it 15 times. That would reduce the user density, and you could plot these curves and you get something decent, but there's quite a bit of gap then from the converse. Then, from the convos, right? What happens then is essentially the sloping part of the curve still for high SNR is still good and quite close, but then the vertical parts, there's quite a big gap. And that's the one we're interested in. So what I'm going to do is now use a very simple scheme because that's all we can do with large payloads, but now use a coding. So here's the scheme. We are just going to go back to the basics and do binary CDM. Go back to the basics and do binary CDMA. So, here, each user doesn't have a matrix, it just has a single signature sequence, A1, A2, and so on. So, there's a single signature sequence, AI, for user I, and it's of length n tilde. Okay, and I'll tell you in a moment what N tilde is. And using this signature sequence, each user is going to transmit D symbols. And D now will be related to the payload. Think of D as a few hundreds, if you like. And the idea is. And the idea is each user is going to have a D-length code word drawn from a linear code of length D and dimension K, which means it has two to the K code words, so K information bits encoded into D coded bits. And I stack these code words one on top of another horizontally as a matrix, which I call X. And you can think of this as a signal matrix that combines the code words. Matrix that combines the code words of all the users. So there are L users, each of them has a D-length code word. So it's an L by D matrix. Okay, so how does transmission happen? So you take a signature sequence AI, and then you transmit the first entry. The IP user transmits the first entry of the code word, and then using the same signature sequence, they transmit the second entry, and so on. So you can think of each user's transmitted sequence as actually now a matrix, which is AI. A matrix which is Ai times Xi, where Xi is a horizontal, is a row vector and Ai is a column vector. So this is an N tilde by D matrix. And so you vectorize it and transmit. So this, the total number of channel uses N is N tilde times D. Okay, so the reason we write it like this is because now your problem becomes a sum of these Ai Xi, which are these rank one matrices. And then when you add them all up together, your up together your y is a times x plus noise where x is now a signal matrix where each row of the signal matrix contains a user's code okay now again we can use amp and we can exploit the structure of of our signal matrix that each row um is drawn from a linear code okay so this is so i said i wouldn't show you the amp but you know here it is um this is very similar to what you would do when x is a vector except that now Except that now all your iterates are matrices. So you have a residual. So at each iteration, Xt is your latest estimate of your signal matrix. Using Xt, you form a residual, do a correction, and then you form an effective observation and then denoise it. And then the only difference being that now your denoiser, eta t now acts row wise on the input matrix. So the input matrix is an L by D matrix, and it acts row wise. And it acts row-wise, and so it takes in d-dimensional inputs and produces a d-dimensional output for each row. So eta prime refers to the d by d Jacobian of the matrix. Okay, so you have the usual state evolution guarantees that this effective observation looks like true signal matrix plus a Gaussian noise matrix whose empirical distribution of the Gaussian noise, each row of the Gaussian noise, is converges to a d-dimensional Gaussian noise. A d-dimensional Gaussian with a deterministic covariance sigma t, you know, which you can determine by state evolution. Okay, so there was a lot of words, but the point is you can precisely characterize the distribution of this ST, which looks like X plus a Gaussian noise with a given covariance. And using that, you know, we can establish the error rate of our AMP decoder. Okay, so to state the error rate, I finally do a hard decision. So E d is, you can. Decision. So e dot e is, you can think of it as a soft decision update of xt, but you can also, because remember, our code words are bipolar. Each entry is plus or minus square root e, you can just threshold your soft decision and get a hard decision. Okay, so this theorem basically says that if you use any Lipschitz denoiser, you know, a good one will take advantage of the structure of the code, then your error, asymptotic user error rate of this AMP is given by, again, Is given by again the probability of error on a single user channel, the probability of your hard decision going wrong on a single user channel, where the single user channel has a code word X bar drawn uniformly among the code words of your K comma D linear code, so two to the K code words, and GT being a D-dimensional Gaussian noise with the covariance you can track, or you know. Right, so I stated this for this algorithm for. I stated this for this algorithm for IID Gaussian A, but can be extended to spatially coupled A as well. Now, the final bit to tell you about is what is the denoiser. So the Bayes optimal denoiser will compute the conditional expectation of the code word given the code word observation and Gaussian noise. And you can write it down, but you can compute it only for small codes. So, for instance, we've done it for a Hamming code, a 4.7 Hamming code. But for large, because it requires average. But for large, because it requires averaging over two to the k core words, it's really not feasible when you have k equals a few hundreds. So that's where this very nice BPD noiser that Jameson talked about yesterday and was actually originally proposed in this joint paper by Amala Deen, Cindy, JF, and Krishna, I think. So they combined AMP with BPD noises initially for coded compressed sensing for unsourced random access, and they use non-binary. Maxis, but and they use non-binary LDPC codes. Here, actually, we do something simpler, we just use a binary LDPC code, and the denoiser is actually quite simple to write down. So, essentially, what BP denoiser does is it runs BPD coding on each row of this effective observation because it looks like X plus Gaussian noise, where X is an LDPC code word, and we know how to do BP. The nice trick there is you. The nice trick there is you shouldn't run BP for too many iterations. As they observe, this On Saga term is valid only if you run BP for a small number of iterations smaller than the girth of the graph and so on. But that's detail that I'm going to sort of gloss over. Okay, so I'll conclude with some pictures. So here is what you get for 120 bits. So these are the red and the black are as usual the converse and achievability. And if you just use uncoded CDMA, you get this dashed curve. CDMA, you get this dashed curve. And now, this blue curve is what you get if you use a rate half code. So, my K is 120, my payload, and my B is 240. And you see that, you know, and this is from state evolution. And so you see that it does quite a bit better than uncoded, but there's still a bit of gap. And it turns out that as you go to larger payloads, you know, and better longer. And better, uh, longer, it'll also give you a longer opportunity to do a longer code, and that'll kind of help you close this gap. So, this orange curve is a code of, you know, k equals 360 and rate path, so which means its d is 720. And then the right way to plot and compare all these curves, it's now spectral efficiency, because we're using different sorts of codes. And the right way and a fair way to compare them is using spectral efficiency. Right way and a fair way to compare them is using spectral efficiency. And you see that this k equals 120, which you have to use three times to get 360, you know, does quite a bit worse than this k equals 360. Okay, so one detail is that we only, so this was all, these were plots produced by Shirley only yesterday or a couple of days ago. So we've only had time to do the IID Gaussian version of this, and we expect that with spatial coupling, we can sort of convexify, if you like, this code. Right? Interpolate between the blue and orange. The blue and orange. So the point is, we can get pretty close to the bounds for large payloads, and it turns out that binary CDMA, at least coded, is pretty much all you need. So that's good news. Okay, so I think I'm running out of time. So I'm not going to talk much about random user activity, except mention that this representation where you have design matrix times signal matrix plus noise allows us to apply these things also. Allows us to apply these things also to where there's random user activity because all you need to do is say that if a user is silent, their code word is all zeros, right? And there's some prior, some probability of them being all zero, and it's easy to tailor the AMP denoises there. And we've also obtained some, you know, bounds based on random codebooks and MLD coding for the setup because they didn't exist before. And those are sort of similar in spirit to the bounds obtained in this paper by. The bounds obtained in this paper by for the unsourced case. Okay, so with that, I'll conclude. Some future directions are: we would really like to explore how close you can get to the theoretical bounds, the converse, with just CDMA, coded CDMA and LDPC codes. And there's more stuff to do in the random user activity setting because the bounds are quite finicky to compute. And finally, I think it would be really interesting to extend some. I think it would be really interesting to extend some of these ideas to the unsourced case, you know, where the users don't really have identities. And if you're interested in looking at some of this, here are the papers. The first one came out a couple of years ago, and the coded CDMA stuff is now on archive, and the random user activity one, hopefully, will be soon. So, thank you. Happy to take questions.