Observations that are conditionally IID given a random parameter. And so we all used in a vision framework that to make inference on the parameter, we first choose a prior, then we observe the data, and finally we update our prior beliefs through conditional probability via the posterior. So I guess that one of the most powerful, but also one But also, one of the most controversial properties of a Bayesian paradigm is the fact that our posteriors or our inference will depend on the choice of prior. And so basically, there is a standard foundational question, which basically asks the following. So consider now the case where we have two different priors on the parameter, and we observe the same data. Will the Will the posteriors get closer to each other? And as we see more and more data, will they finally merge? So this question was addressed in a foundational paper by Backel and Dubens, and basically they answered that yes, there will be merging if the two priors are absolutely continuous, one with respect to the other. Now, in a Bayesian paradise, Now, in a Bayesian parametric framework, this does not seem as a strong assumption, basically because we're used to considering priors that are absolutely continuous with respect to the Bag measure. And so absolute continuity doesn't seem like asking much. But in a Bayesian non-parametric framework, basically this becomes a very strong assumption. And for example, even if you consider the most common prior, so maybe two Diricher processes with different concentration parameters, well, these Parameter, well, these two are not absolutely continuous, one with respect to the other. And so, basically, in a Bayesian, at least in Abesian non-dominated setting, we do not have a general theorem answering this question. And so, what we have to do is to basically set up a general framework to be able to address this question in a case-by-case scenario. So, the first stage here is basically to reframe the question of the merging of opinion in terms of a distance between the two posteriors. So this appears as a natural thing to do. Now the merging becomes will the distance goes to zero as the number of observations increase. And moreover, we can also define a new concept, which is the merging rate of opinions in Emerging rate of opinions in the sense how fast does the distance go to zero. And moreover, we may try to attempt to also give some finite sample properties of this object. And so, for example, answer question of the type, when does the merging start? So, will the two posteriors get closer to each other immediately as we see only one observation, or do we have to wait for a while before the merging starts? Now, the nice thing, I think. Now the nice thing I think about this framework is that basically we manage to avoid any of the most common assumptions on the so-called true data generating process in the sense that we will not have to assume that the observations are actually IID from some unknown true distribution, nor that the observations are generated by one of the two models that we are considering. Models that we are considering. And here, well, the idea is that we are not looking at where the posterior converges, where it goes, but we're only looking on how similar two different posteriors are. And so we can address similarity even without knowing where the posterior goes. And so, in this framework, actually, we not only we do not have to know the true parameter, actually, this true parameter may even not exist. True parameter may even not exist. And let's say that, so first of all, of course, we need to choose a notion of distance. And well, if we were considering parametric models, we would actually have many distances at our disposal. So any distance between probabilities would be okay. For example, the Vasser Sein distance, but there are also many other choices. But now for us, we are dealing with. But now for us we are dealing with Bayesian non-parametric models. So we will need a distance between infinite dimensional quantities and much of our work will actually be devoted on defining a distance that is good for our purposes. And then let's say a brief snapshot of the kind of result that we will find is that basically it's not a coincidence that there is no general theorem stating. General theorem stating what happens to the merging because we analyze a number situation where basically we saw that every possible possible scenario actually happens. So there are certain cases where the merging occurs, certain cases where it does not. Moreover, the merging rate can depend on the choice of hyperparameters of the prior. And there are situations where it starts immediately with the first observations and situations where we actually have to wait. Situations where we actually have to wait for a while before the merging occurs. Okay, so first of all, let's try to understand what kind of distance we will be using. And to do so, we have to say a bit more on the kind of models that we, Bayesian and parametric models that we will be considering. So, we will start with the Dirich process because it's the cornerstone of Bayesian and parametrics. There are many Parametrics. There are many ways to introduce it. Let's say that for our purposes, we will now introduce the Dirsch process as a normalization of a gamma random measure. So a gamma random measure is a random measure that is basically defined by two properties. The setwise evaluations are gamma random variables and it has independent increments in the sense that if we consider disjoint powerwise distinct sets, we will have sets we will have in the setwise evaluation will be independent random variables. And the second property is the one that defines the notion of completely random measure. So there are many reasons why the Dirac process is the cornerstone of Bayesian non-parametrics and it's so popular, but let's say that for sure one of them is the fact that it leads to closed form expressions both for inference and for For inference and for predictions. So, if we look at the expression, for example, the prediction, we see that this is a convex combination between our prior guess, P0, and the empirical measure, which seems as a natural and intuitive way of choosing a prediction. However, in some sense, there is some lack of flexibility in this prediction scheme, in the sense that if you look at the weight that is given to the Weight that is given to the p-naught, so the prior guess, we can interpret this weight as the probability of having seeing an observation which is different from the past observations. And well, in that case, we see that this probability only depends on the number n of observation that we had seen in the past and not on which observation we are actually seeing. And so, in some sense, there has been a lot of work envisioned. A lot of work envisioned on parametrics to define new laws for random probability that could achieve more flexible sampling schemes and predictive and inferential sampling schemes. And so one of the proposals on which we will actually focus this talk is a notion of normalized completely random measure. So we will say more about this, but let's just say that the idea is very simple. Is very simple. So there's nothing special about the gamma, or at least not that much is that special about the gamma completely random measure. If we choose another completely random measure and we normalize it, we actually have a way, a recipe, if you want, to define a new big set of laws of random probability measure. Now, these complete random This these complete rendering measures have been shown in this nice papers to actually achieve the kind of flexibility that we might desire in terms of prediction and inference. But for sure, there is also a price to pay, let's say, for this flexibility. So, first of all, let's say both, even the computations, the computational schemes tend to be a bit more difficult. To be a bit more difficult, and let's say that there is also a price to pay in terms of the amount of notions of probability and that you need, and maybe some analytical weight. And so what we asked was basically in which situations is it actually worth to pay this extra price of using a normalized completely random measure that is not a deer process? So, in other terms, So, in other terms, when will the use of a different non-parametric prior, a different complete random measure, actually lead to a different learning outcome? And so, in this sense, we can address this question in terms on when will there be merging of opinions. And this is why what we need now is to basically define a distance between completely random measures that we can use both. We can use both a priori but also a posteriori. So, in order to define the distance, I'll say just a few more words on this class that we only briefly mentioned of completely random measures. So we have introduced them as through their characterizing property of having independent increments. And what is really remarkable about this definition is that you only need this. Is that you only need this independence of the increments, and there are so many properties that are actually entailed by this very intuitive and weak, if you want, assumption. So, first of all, one can prove that the random measure is almost surely discrete. So, in practice, we will have a random set of random jumps and a set of random atoms. And even more remarkably, And even more remarkably, with this definition alone, we can prove that the joint distribution of the jumps and the atoms follows a Poisson random measure. So in practice, when we have to assign the law of a complete random measure, it suffices to assign the joint law of this, sorry, of the James-Diatoms, so the law of this Poisson random measure, which in practice simply means to Which, in practice, simply means to specify the Levian density of the Poisson random measure. So, this Levy intensity is defined on the joint space of the jumps and the atoms. So 0 plus infinity will be the space of the jumps because they're positive, and x will be the space where usually your observations lie. And we can always disintegrate this Levy intensity in. Levi intensity in a jump and an item component. And let's say that for this talk, I will actually assume that they are in product form just for simplicity of exposition. Now, the atom component is very easy to deal with. It's an object that we know because it's simply a probability p naught on the space of observations. So nothing so strange. The jump component is a more interesting object in the sense that it's not a probability. Object, in the sense that it's not a probability, but rather a measure, and it is a measure that can have infinite mass around the origin. And basically, whenever you want to normalize a completely random measure, you actually need infinite mass around the origin. So even if this notion of measure with infinite mass maybe seems a bit cumbersome, actually, it can also come boil down to a very simple expression. So, for example, in the case of the gamma and the measure, you see. In the case of the gamma random measure, you see that this is the Levy intensity. And if you look at this jump component, it's true that it diverges near zero, but at the same time, it's also a very simple analytical expression that somehow doesn't really doesn't, I mean, it is something that you can imagine being tractable. And so this is to say that in practice, what we really know about the law of the complete random measure is. Complete random measure is its Levian density. And so, somehow, when we needed to find and define a distance between complete random measures, we basically thought it would be tempting to define the distance directly at the level of the Liby intensity, since in practice it is the object that we know explicitly. And so, this is what we did. We considered two different Libyan densities. Different Levian densities corresponding to two different complete random measures. We know that this can be the composite and jump and atom component that have very different meaning. And so our idea was quite simple. We can define a distance simply by considering first a distance between the jump components and summing up a distance between the atom component. Now the jump, the Now, the jump, the atom component is nothing strange because we said that the atom components are simply probabilities. So, any distance between standard probabilities could be okay. For example, we chose a Vaser sign distance. Now, when it comes to defining a distance between the jump component, this becomes more interesting. Why? Because the jump components, we said that they are measures with infinite mass. They are measures with infinite mass. And so, a priori, it doesn't seem simple to define a distance between measures that can have and usually have infinite mass. And so, basically, what we did with Hugo has been to actually think if there could be an extension, a natural extension of optimal transport from probabilities to measures with infinite mass. And indeed, in the end, we understood that it could be doable. That it could be doable because what you really need when you deal with optimal transport are first, finite first, or let's say moments in general. But there is not a hard constraint on having a finite mass. The problem is that basically after a while, we discovered that this distance was already defined in a paper with a slight variation, but it's really slight. So let's say that it had already been defined by Figali and Gigi. Already been defined by Figali and Gigi in 2009, and they called it the extended buser sign distance. For reason of time, I will not give you the formal definition. What I want to mention here is that what we proved is that at least when we are comparing measures on the positive line, which is exactly our case because Olivian tensities are between zero and plus infinity, this extended Vassar Sen distance has a Distance has a very natural integral representation. So, this the vast resin distance between the Levy measure will simply be the L1 distance between the tails of the Levy measures. And this, of course, makes sense because the Levy intensities will have infinite mass around the origin. So, instead of comparing the CDF, as it happens with the standard busier sign distance between random variables. Sign distance between random variables on R. Here, the characterization is in terms of the tails. Okay, so in practice, what we did now is that we defined a distance between the complete random measures in terms of the distance between its levy intensities. And then to somehow justify also the use of this distance to compare the laws of the complete random measures, we also proved the following result. Basically, that we can dominate the vast resources. Dominate the vast resentment distance between the integrals of the complete random measures in terms of our distance defined at the level of the Levy measures. Okay, so we have now our distance. We would like to study now the notion of merging of opinions in terms of this distance between complete random measures. There are just like two details. I don't have time to I don't have time to really show them, but I would like to just mention them. So, first of all, the fact of the problem of identifiability. So, basically, what can happen is that it can happen that two computer random measures actually induce the same normalization. And to see this, it suffices to really consider just the multiplication of the random measure by a constant, and you see that. Random measure by a constant, and you see that, of course, the normalization will be the same. And what we proved is that this is an if-and-only if. And so, basically, what we did is that we use this result to just choose one representative of the equivalence class of completely random measures that induces the same normalization. And this leads to a notion of scaled completely random measure. Then, there is a second problem that is that if you remember, we want to use If you remember, we want to use this distance to compare posteriors. And well, is it true that the posterior, when you use a completely random measure as a prior, will your posterior be a complete random measure? No. Basically, there is this remarkable theorem by James Leo and Krunster, where they gave a characterization of the posterior of the random measure, but this is not exactly a completely random measure, rather, a completely measure measure. Their complete random measure conditionally on a latent variable. And so, in practice, what this means is that the Levy intensity will not be a deterministic quantity, as with a complete random measure, but rather a random quantity. And for this reason, in analogy to what we do with the Poisson process, we basically call this class of random measure Cox completely random measures. And so, when we use the distance, we When we use the distance, we actually have to adapt our distance to treat this randomness as well. Okay, that being said, let's go to the results. So first question that we try to answer with our framework is simply, what about the Dirichlet process? So what happens to our posterior if we consider the two Diricher processes with different concentration parameters or different base probability? The base probability. So, to answer this question, we use the fact that the Dirichlet process is a normalized gamma. So, we considered two gamma priors and then we analyze the distance between the posteriors. Now, this is an expression that we obtained. I reported not really for you to grasp it immediately, but what I really want to say about this expression is. What I really want to say about this expression is that maybe it may not look as very nice in some sense, but at the same time, it is remarkably explicit from the point of view of its analytic expression. And so this means that this distance between posteriors, not only we can evaluate it computationally, but it's so explicit that we can actually also study it analytically. And so we used this, the fact that this This is the fact that the distance was so explicit to actually study the merging rate of opinion in, let's say, a general scenario. So, our findings. Basically, what we found is that with very mild assumptions on the data, the distance between the two postures, so the merging rate, is 1 over n, where n is the number of observations. Now, the assumption of the data are simply very simple because it's just Are very simple because it's just this integrability assumption on the data, which is connected to the fact that the Wasserstein distance needs finite moment. But this, if you think it's not saying anything about the distribution of the data, okay, we're not assuming that they are IID, they are generated from the model, nothing like that. So there is always merging of opinions, and what is interesting is that this merging does not depend on the parameters of the leadership process. Of the Diricher process. And even more interesting is that it does not even depend on the dimension of the space of the observations. And actually, the merging rate of opinion is faster than the convergence to the truth in those settings where a true probability P0 actually exists, right? Because one, like even in the best scenario, you would expect a convergence of one over square. Expect a convergence of one over square root of n, whereas the merging is actually faster. It's one over squared n. Moreover, since the expressions are very explicit, we were also able to investigate a bit the finite sample situation of the merging. And so, what we proved is that if your concentration parameters are equal, then the merging is Then the merging is decreasing. And so this means that it will start immediately. So, with the first observation, you already know and you can prove it that the two posteriors will get closer to each other. On the other hand, if the two base probabilities are equal, then the situation is completely different in the sense that before seeing the merging, which has to be at with rate one over n, there is an initial amount of observation where Observation where the distance between the posteriors is actually increasing. And so this can be interesting from the point of view of the Bayesian paradigm, because basically it tells us that even if two Bayesi have the same prior guess at the distribution of the data, so the same p-naught, and even if they see the same data, still Still, their inference may actually get far away from each other, and this really depends on how much confidence they're willing to put on their prior guess on the distribution of the data. Next, we tackled a second question, which is to see what is the impact of the discount parameters. The discount parameter sigma of the generalized gamma CRM. Generalized gamma CRMs are generalizations of the gamma CRM, as the name suggests, which basically retrieve the gamma when the parameter sigma goes to zero. So intuitively, we know that the parameter sigma has a great impact on the inference. And well, we use our framework to also study this question more analytically. Question more analytically. So, what we did is that we considered a gamma and a journalized gamma. And here we have studied the asymptotics of the distance between the posteriors. And we see that there is a completely different scenario than the one that we have seen with the Diricher process. So, with the Diricher process, the merging rate was always 1 over n. Here, the merging rate has a phase transition. rate has a phase transition that depends on k that is the number of unique values in your data set and so we can see that when k grows at the same speed as n so basically this ratio goes to one this also means that there are situations where the merging does not occur now probably these situations are not the most interesting ones because usually one uses this type of models for discrete data so you would expect Discrete data. So you would expect k to grow slower than n. But still, even in the scenario where the merging does occur, we can see that the merging rate really depends on the parameter sigma of the model, unless there is a sufficiently high number of distinct values in the data set. And so it's very different from what happened in the Diricher process, where the emerging rate was not affected by the parameters at all. Parameters at all. And so, to answer the initial question, so when does the use of a different completely random measure bring to a different learning outcome with respect to the leadership process? Here we can actually answer this question. And basically, it's either when the number of observations is small, which is what we usually expect, or even when the number of unique values k is large. And basically, we can show that how. Can show that how large and how small they have to be depends on the parameter sigma. And thanks to this result, we can express this precisely. Okay, so I had some simulations where we're just basically confirming the theoretical results about the rates that we achieved, but I guess that we don't have time to really insist on this. And so I'll jump to the conclusion. On this. And so I'll jump to the conclusions. So, what did we do in this talk? So, first of all, we defined a notion of optimal transport on completely random measures using their Libby intensities. Then, we use this distance to define a new notion of merging rate of opinions and to study the impact of the prior in a Bayesian non-parametric non-dominated setting. And we have seen that this. We have seen that this problem is interesting in the sense that there are really so many different scenarios that may occur because emerging can happen, but there are also cases where it does not happen. The emerging rate can vary according to the prior that we are using and there are situations where it starts with the first observation, but also situations where we have to wait for a while before the merging starts. So all these results can be found in this preprint that we have with HUB. This preprint that we have with Hugo Lavenon from Bokoni University. And as for the next direction, I'd say, so all this talk focused on normalization of a completely random measures, but actually in vision and parametrics, there are many different models and transformations that you can use of a completely random measures. And so it would be nice to also analyze emerging in different types of models. Different types of models for the observations. Then, another direction that we're exploring is that here we have defined a notion of a distance between completely random measures, but it could be interesting to also have a distance between random probability measures. And finally, let's say that more generally, I guess we have now a tool, which is a distance between random measures. Random measures, which has some nice property because we can evaluate it computationally and, in many cases, also analytically. And so, it could be interesting to now use this tool, this new distance, to maybe tackle a number of questions and frameworks that have already been established in a Bayesian parametric framework, and that maybe we could now export or expand also to a non-parametric one. A non-parametric one. Okay, so thanks a lot. Thank you very much. Thank you, Marta, for a very wonderful talk. Are there any questions? Hi, Marta. Thank you for the nice talk. I was wondering, when you mentioned that it might happen that Cuba Um, two Bayesians have different like confidence on their prior, but the same prior guess, and then their like posterior can diverge, like it's not gonna merge. Does that have like any, I don't know, like, what's the, is there like an applied intuition of this and like what's the impact of this on inference setting? Thank you. Yeah, it's a great question. Yes, great question. So I see it like this. So somehow the Bayesian paradigm, in some sense, can be seen as a way to define a path that goes from our prior opinion to maybe the information contained in the data. And I guess that the And I guess that the intuition behind it is that even if you start in the same point, so with the same prior, then and even if you see the same data, then according to what are your prior beliefs, you may decide to follow this path, like take a bigger step in this path or a smaller one. And so even if you start from the same path, Even if you start from the same point and see the same data, still your position, let's say, in this path can initially increase depending on how big you wanted to take a step towards the data. I mean, I think it makes totally sense. So, probably as Bayesians, we want to think that at least Want to think that at least asymptotically we are all going in the same direction, but it really makes sense that depending on how sensible your inference is, how strong information you have about your prior, maybe especially if you're using maybe a posterior coming from another model or from another analysis, then it makes sense that finite sample, we want to see big differences going from one learning. Is going from one learning scheme to another one. Thank you. Thanks for the question. Very interesting. Hi, Marta. I don't know if you can see me, but it's Trevor. Thanks for the very, very nice talk. I was really sort of maybe surprised isn't quite the right word, but something in that regime of the merging of opinion results that you have, especially in the cases where it goes sort of faster than. Especially in the cases where it goes faster than the posterior contraction itself. I was wondering, how would these results, or do you have any thoughts on how these results would generalize if you put priors on your parameters? So like if you had a prior on alpha, a prior on sigma, would these results sort of continue to hold? Would you expect maybe like more general monotonicity results? Interesting. So I haven't really thought about I haven't really thought about it, so I can give you only guesses. So, what I would tell you is that actually it is known that if you put a prior on alpha, I think that you abandon the so-called Gibbs type prior. So, I would imagine that your inference then depends not only on K and N, but Only on K and N, but also on actually the cardinalities of each unique observations. So I would imagine that, so it's super interesting and I think worth doing. I would imagine that you would have different results that probably also take into consideration more characteristics of the data set. So not only the number of observations and the number of unique values, but possibly also their cardinalities. Also, their cardinalities. Nice questions. I'll think more about it. Okay, there's another. Hello, thank you for the great talk. I'm gonna try to be quick, but I was wondering if you ever thought about using this on a vector. I've been trying to work. A vector. I've been trying to work on maybe asymptotic dependence or independence of random measures, and I think maybe using a distance like this will be a great way to see how far I am from asymptotic dependence or independence, which I haven't found a way to do. So have you ever tried something using this on dependence or nice? Yes, a great point. Yes, I think it is. point uh yes i think it it makes uh it makes sense so it it would require um an extension um because of course then uh you need to define the distance on the product space so i i had worked on dependence but always at the level of the prior so let's say that i i had used a distance between the levy measures but never the distance between this cox levy Distance between this Cox Libby measures, so with randomness. So I think it could require some more work, but it should be feasible to extend also this use of distance to measuring dependence, both a priori and a posteriori. Thanks. Okay, so let's thank again Marta. Again, Marta. Thanks a lot. Bye bye. Enjoy Oaxaka. Thank you very much. Bye.