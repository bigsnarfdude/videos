I might need it. So, thanks to the organizer for having me. This has been a wonderful workshop so far. So, I'll talk about some bit of musings, some recent ideas that I've been working on related to transformers, and particularly how this thing that we call associative memories can be seen as sort of a building block when you have like deep transformer models. So, there's not a lot of chromatic analysis, and hopefully, there's some like little tools that. Hopefully, there's some little tools that you can think about in the future, and maybe it'll motivate some new problems. That's what I'm hoping for. So, okay, first slide: we know that deep learning has been great. It's successful in practice. You have these like very deep models with a lot of parameters. The recipe is usually you just make the parameters bigger, more parameters, more data, more GPUs. And you use relatively simple algorithms, basically gradient descent, usually or some variance. That's usually working pretty well for the problems that matter. Pretty well for the problems that matter in practice, and so the question is: why does this work? So, and we have some theory for deep learning in general, for transformers, and hopefully a theory that is consistent with these facts. So, it should get better when you scale with data, compute, and it should work with simple algorithms like gradient. All right, so just a little background on like deep learning theory, so sort of what's been done so far. So, usually, the key thing is like deep learning works well, but The key thing is like deep learning works well, but on problems which have like very large dimensionality: images, text documents, genomics. So the data is hugely dimensional. There's not a lot of hope to use very basic methods because typically you're going to be cursed by dimensionality. So somehow you need to exploit the structure that's in the data using your estimator. And this is why people think that deep learning and in particular special architectures have been helpful for that. So the first message that's been studied recently, and Rebecca mentioned it yesterday as well, is And Rebecca mentioned it yesterday as well: is like maybe there's some hidden low-dimensional structure in the data. And really, instead of looking at the big vector in dimension D where D is really large, we should really be limited to some projection. And one motivation for this study, for this kind of idea, was, you know, when you look at convolutional nets, probably many of you have seen it, and you look at the very first layer, you can look at these filters that look a bit like wavelets or Geborg filters. So definitely something is being learned that is non-trivial and it kind of depends on the data. And it kind of depends on the data. So, you know, can we try to understand how a neural network does this in a way that maybe a kernel method, as in the previous talk, might not adapt to the data in this way? And so here there's been a bunch of work. So typically, you know, if you don't make any assumptions, then typically you can only hope for this very cursed rate, like n to the minus one over d, where n is the number of samples. With this kind of low-dimensional structure, neural networks, some idealized neural networks can achieve much better rates. They'd only depend on this dimension of the subspace. Than on this dimension of the subspace. And recently, there's been a lot of interest in this, in particular. So, here, actually, this would be related to harmonic analysis. Typically, you kind of consider maybe Gaussian data, decompositions of your target function, and you can study even gradient dynamics. So, does gradient descent somehow find these features? And there's been a lot of positive works, positive results in this area. But now I'll argue that, like, when you go to practical deep learning, typically you have these, so okay, here. Typically, you have these, so okay, here the architectures are usually these shallow, just a neural network, which with one layer and MLP, and then you just feed it into a, you have some neurons, and then you stick it back and predict a number. So now the second ingredient, which seems important, is that you usually have these rich architectures. And usually the first layer in a neural net kind of partitions the input into small blocks. So if it's an image, you might break it into like small patches. Typically, maybe like 16 by 16 patches. This is what people use. 16 by 16 patches. This is what people use in transformers. If it's text, you know, you have a large document, you just take word by word, and each of these words gets processed sort of independently. And then they get mixed together in different lengths. So, okay, so this is kind of locality is important, and maybe there's a multi-scale structure, and you can kind of separate the processing of individual patches from then recombining things across different patches. And so, maybe one way to think of this is that the actual target function. The actual target function, the prediction function, maybe you first can look locally at each patch and there's something reusable like wavelets. But then what happens later is like maybe you're looking for different parts of the text, some kind of interactions between different variables. So this could be maybe you see two wheels in a car and you think that's a car because there are two wheels at this distance from each other. Or, you know, maybe in tech, there's like different words that appear in different positions, et cetera. So this has been interaction models in regression. It's a kind of long, has a long history. Of long has a long history, and you can think at some basic level of architecture as capturing these sort of which interactions matter. So, we've also seen a bit of this, I guess, in the quantum world, maybe, but good timing. But okay, so architectures now beyond this, so you could think potentially of intermediate layers. So, maybe the first layer is just taking this patch and you want to find these Gabor filters. But after that, what you're really doing is maybe estimating which interaction. Really doing is maybe estimating which interactions matter in your data. So, this is kind of question mark. So, I'm postulating this. It's not really, there's no proof, but you know, hopefully, we'll get some insight on to why that's a good model. And then you can think of the different architectures as doing this differently. So, convolutional networks typically, you know, maybe they kind of each layer looks locally. So, you look at two patches that are not too far. So, maybe the first layer looks at like, here's a wheel and here's a light thing. And then a few layers later, you can look at And a few layers later, you can look at longer-range interactions, and you get to see that these are two wheels at this distance. You're like, okay, this is a car, maybe. For attention, hopefully some people are familiar with this, but in text, so convolutions haven't been very successful in these attention models, in particular transformers, have been great. And usually there's some, you know, if you're doing translation, typically you have some target sentence that you're translating and you want to attend to the correct words in the sentence. Words in the sentence in the other language. So maybe when you see the, and you're like, you find the corresponding word in the other sentence, and you find a match, and you're like, okay, this is what it should attend to. So you can be a bit more non-local in which interactions you decide to pick on. And even in vision nowadays, people use these attention models. So maybe you don't have to look locally. You can just look at content-based similarities. Like there are two wheels. I know how far they are. And you don't need to actually know their. And you don't need to actually know their position. Okay, any questions so far? So these are kind of the two ways that I'm trying to break down deep learning problems for architectures. Now just to go a little bit back on this message, what are the layers doing? So I said the first layer just takes these patches or tokens and somehow maps them somewhere. You can think of that as this feature learning aspect, essentially. And usually here, what happens is you map these patches or tokens into an embedded. These patches or tokens into an embedding space that is high-dimensional. So you have these vectors, that's an embedding. You have one for each word, one for each patch. You also usually encode the positions potentially. But you have these vectors. You choose these like directions. And what I'll argue, again, big question mark, is that these usually are some kind of, they encode maybe some discrete structure. So you can hope that, you know, if it's text, this is just like words, but they are discrete by nature. If it's images, maybe there's some clustering going on. So you can detect. There's some clustering going on, so you can detect an edge, and you call that edge. If you detect a corner, it's a corner. If it's a wheel, it's a wheel. So you can start to put names at different directions in this embedding. Okay, so this is maybe the embedding layer. What happens at the very end, so you keep this embedding layer, and usually, so what happens in neural nets, we'll see it later, is that you have residual connections. So this embedding layer sticks around and you keep adding new vectors at each layer. And at the very end, you kind of map it back to a label. So that's like the unimportant. Label. So that's like the unembedding layer. So it also goes from embedding dimension to something maybe finite for vocabulary, number of labels. And the middle layers, we don't know, but potentially, so I'll argue if these embeddings are something discrete, maybe what these middle layers are doing are kind of like some computation on these discrete elements. So discrete elements, you can think of it maybe as like a one, a sparse vector in very high dimensions. Is there a car? Yes or no? Is there a wheel? Is there a corner? Not is there a wheel? Is there a corner? Whatever. And then you have these vectors at each location of different patches and different tokens. And somehow you're communicating information around. You're saying, like, if there are two things together, then I fire. So you learn these like little rules. And there's been a little bit of evidence in the interpretability literature that these are kind of these mechanistic interpretability works that you might have heard of. This is pretty hard to see, but this is a task where, so the task is like when Mary and Joe, it's a text, it's like a language model. It's like a language model. And you ask when Mary and John went to the store, and like you can't really read it, but Mary gave the drink to, and you have to pick John. And people figured out that there's sort of a bunch of attention heads that are moving information around to decide that, you know, you first look at all the names that appeared, so Mary and John. And then if there's a name that appears twice, you're going to like filter it out and you're going to predict the other ones. So there's this little sort of circuit that kind of decides what name you should predict. And in this case, it will. Should predict, and in this case, it will predict Mary instead of John. This is called indirect object identification. So, this is sort of empirical work, and we're trying to get a sense of what's what in the layers, what makes it possible to make these little rules. And because I said something about discrete data, then you can think of kind of associative maps. So you have maybe one discrete element, you want to map it to another one, or you have two things that look alike and you want to match them. So, some sort of like discrete rules, maybe a circuit or a program that is setting. Circuit or program that is happening inside your network. Okay, let's keep going. So, what do I mean by associated memory? So, I talked about these embeddings that are just different directions and maybe have discrete or semantic meanings. So, you can think of embeddings as being maybe like an orthonormal basis or nearly orthonormal. So, maybe we have two families of embeddings, these UIs and these VJs. They're all nearly norm one and close orthogonal. norm one and close to orthogonal. And then you just define your matrices as being these outer products. So if you define a matrix to be like this, then basically when you put in on the right just UI, on the left VJ, then you get this alpha ij. And maybe what you're doing is classification. So maybe alpha ij is just like one for the correct j, which is like your label. So maybe you're trying to map something to something else. Then what you do typically, maybe for classification, is you take an argmax. But you could stack this many times. But you could stack this many times if you don't sort of collapse back the embedding and you just keep multiplying by matrices. You can do this many times and keep operating on different embeddings. This is just associated memory at the most basic level, and we'll see how this is, I should say, it's related to, there's been a lot of neural computational literature, upfield networks, et cetera. It's somewhat related. This tends to be like a one-step version of those, but usually they study some dynamics. And also here, And also, here, so usually in these models, like the u, like v is the same as u, and you're just trying to recall patterns, whereas here we're mapping things from one to another. Okay, so now I'll go on to like some first work, which tries to explore how this model is useful in transportation. But any questions? All right, so this is a first work that I'll talk about. And so, this is, I spent a year visiting at Meta. So, there's a bunch of libraries there that I've worked with. Bunch of libraries there that I've worked with, and kind of trying to get a sense of what's going on in these language models. And the one task that language models like Chat GPT, et cetera, seem to do very well is something called in-context learning. And I don't know if, has anyone heard of in-context learning? Okay, so I'll just make it sort of basic here for a language. And yeah, I apologize for this, for the vegetarians. For some reason, I picked the word bacon on the first set of slides, but I'll have to change it. So here, I'll have to change it. So, here, what I'm saying is basically: if you see, if you type this into ChatGPT, or this was an older version of GPT-3, I think, when Mr. Bacon went to the mall, it started raining, then Mr., and you query the rest. So basically, after the word Mr., probably it has to be a noun somewhere. And why wouldn't you just make up a noun like Mr. Obama or Mr. President or something? Probably makes more sense to just use the guy that showed up in the context. And so, this is what these models do. They usually just reuse. And so, this is what these models do. They usually just reuse a lot of information that shows up in the context. So, this is a basic form of in-context learning or reasoning. And so, we can kind of simplify the task further. And we'll have this toy, okay. This toy model, and we'll call this word mr. or doctor or something. We'll call it a trigger, trigger word. And then, whatever comes after it, then anytime in the same sentence you see the same trigger, you have to kind of. The same trigger, you have to kind of copy the same word that came up right after, okay. So, if I see Mr. Bacon in the beginning of a sentence, then they see Mr. It has to be Bacon again. If I see Mr. Obama, Mr. Biden, et cetera, I have to copy the same name. Does that make sense? So, this is the idea, and then we'll kind of generate a data model based on this. This is what I'll do here. So, I'll fix some of these words, so Mr., Dr., whatever you want. And then, every time we sample a new sequence, the way we'll sample. New sequence, the way we'll sample it is: first of all, for each trigger, so for each of these kind of few orange words, I'll sample a corresponding output on the specific sentence. Okay, so in this sentence, it will be bacon, another sentence, it might be raw or something. Okay, and the point is like when I generate my sentence, anytime I end up kind of randomly, so I have like a markup chain that generates my sentence. Most of the time, it's just some like general markup chain here that I just call. Mark of Chang here that I just call Pi B. So you just sample words kind of randomly. But anytime you end up on one of these trigger words, then the next word has to be this corresponding okay. Okay, so when Mr. Bacon, I just generate words, but then at some point I end up on Mr. again, I have to pick Bacon. Whereas in a different sentence, it might have been Obama or whatever. Okay. And then this model here for our toy experiments, we pick some character level vocabularies where you have characters and we generate them one after the other. Characters and we generate them one after the other following some kind of like general diagram. You just predict these probabilities given some. Okay, so this is the Tway model. How, but before I talk about how transformers do this, we just briefly give a Markov model. I'm defining a data model that somehow requires you to look at the content. That's the idea. Yeah. That's the idea. Yeah. So we want to we want to kind of understand this behavior of how a language model has to reason by looking inside the context. So here it has to like, basically, when you see the word mister, you have to like look back in the context. Yeah, so okay. I guess they didn't say that you have to do, so this is like next word prediction. You're doing like a language model. So when you get here, let me, when you get here in the sentence or there, you have the last word is mister. You have the last word is mister, and you have to predict the next word. Right. So you have to look back. Oh, here, yeah. Sorry, here. Okay, I should say this is a marker. So I generate marker fin using this like sequence dependent transition matrix where, oh, yeah. Yeah, it always depends on the just previous token. Previous token, no, exactly. So, the first time you have no idea, so you just predict a random because it's a random. But the second time, somehow the data forces you to like look back, look what was after the first time mister occurred. Yeah, you have to look for the trigger. The mister, yeah, mister has appeared twice. Yes, yeah, trigger and any other questions. All right. So, okay, just brief. So, basically, the way that these models do it is this attention mechanism. Somehow you see Mr. and you can figure out where was Mr. before, which is like kind of a pairwise match. This is how it's doing it. But let's recap a little bit this attention mechanism so you get a flavor for how it works. So, before I talk about attention, let me talk about just the actual architecture. So, I said usually you can think of like there's a first layer, a last layer. Can think of like there's a first layer, a last layer, and then in the middle, there's a bunch of these like tension layers. So, let's here I'll think about the first and last layer. So, the first layer, you have these zts, which are just like discrete tokens between one and n, so the different words, and you map them through this w function into a vector. So, this w of zt for each word is a different direction. And maybe if the dimension is large enough, these are all orthogonal. If it's not, then no, but that's the idea. And you do the same for the position, so you have to know where. Positioning. So you have to know where you are in the text. If you want to leverage the order of text, somehow you have to encode the positions that come up. So you have these two vectors and you're adding both of them. If you think of it as like a sparse position, it's like you have these two, like everything is zero, but you have one on the specific word and one on the specific position. This is how you can think of these things. Okay, so token embedding, positional embedding, and then every layer, as I said, you just keep computing potentially by looking at. Computing potentially by looking at other tokens, and then you add back the vector to this XT. So, XT, people call this a residual stream. So, you just look, because you have these residual connections, every layer, you keep computing something, then you add it back to this XT. And you have one of these XTs for each position. So you have vectors sort of at each token, and you're like copying stuff around and moving it, computing things. But at the end, at the last layer, you'll have like a bunch of information on each token that is encoded by this vector. And you keep adding. And you keep adding features to new directions. Okay, so I'll tell you later about attention in particular. In practice, people also have some MLPs that just kind of only look at the token itself without looking at the others, but we'll kind of ignore that here. And then at the very end, what you do is you have this XT vector at the layer L, where you can put some subscripts or something. And then you just check what is the prediction for the next layer. So you multiply it by these output embeddings. So for each next word, K, I'm going to multiply. Next word, K. I'm going to multiply my current embedding by this next word. Okay, so this is, and then what happens is you train this thing so that you have these logits, so these predictions for each next possible word, and you use a crest entry last to actually predict the next word in text. You do this over like many sentences and documents, like trillions of tokens. That's what people do in practice. And you get GPT, or sort of you get a base version of what these models are doing. Okay, so now just focus, let's focus on this attention layer. So, what attention does is like, let's say this is this mister word, this trigger, and somehow we want to match the previous occurrence of that. So, you have one vector here, Xt, and then Xs maybe. And what at the new XT after the attention layer will be given by a linear combination of all the other embeddings, multiplied by these like value output matrix, it's called. Like value output matrix, it's called. And the weights that you're taking are given by this attention mechanism. So, what attention does it compares the current, so the last in this case, the very last one is Xt. You're comparing it to all the XSs, everything that appeared before, modulated by this matrix, and then you just normalize. So, potentially, you know, if you have, if you want just a similarity, then you're just gonna activate when XS, when these are the same words. So, if you have Mr. and Mr., this will be like large. Mister, this will be like large for the same word, and all the other words they won't fire. So, you'll end up basically copying whatever was there, whatever was the position of this source code. Okay, so this is how attention works. So, usually you have these two matrices and potentially a different kind of bottleneck in between. For simplicity, we'll just take these to be just one matrix and one matrix, but you do have these two matrices per attention layer. Yeah, I'd be thinking of the. I think it can kind of be a catch. Yes, that's what I'll argue. Yes. Or in other cases, it'll be helpful to not frame this W O at all. And I'll say why. If you leave that to be a random matrix, that's actually also useful because it somehow rotates your vector into some random direction. So it kind of relabels what you've got. So that's also a useful way. But in a way, But in a way, I'll argue, at least in this model, you can think of W either as being the sum of outer products, kind of a spiky matrix, or just a random matrix. This is kind of two mechanisms that you can think of. Yeah. When you have to go there? Right, right. Well, Right, right. Well, so you have to, you mean if XS is not mister? If it is mister, yeah, but what do you mean by other junk in between? Oh, yeah, but it's sort of through training, maybe it'll start spiking. So initially, it's flat. Initially, all of these are like kind of zero because everything's orthogonal. Because everything's orthogonal, uh, and then except for well, actually, no, all of them are zero because this matrix is random. So, yeah, but as you train, you get these spikes, and then you start attending to relevant. Yeah, so I'll argue that sort of if you do one gradient, even if you do one gradient step or something, like if you take gradients of this and the right scalings, it gives you these outer. So, it kind of figures out the right association. Any other? Any other okay, but I've kind of given you the punchline already, but okay, so let's keep going, but yeah, so you compute these things and then you add them so remember, maybe I've copied this word mister, but then instead of like maybe I've multiplied it by this value meter, so all of a sudden it's like a new direction and I add it and you get two features instead of one. So each layer you keep adding features. And okay, I don't have much time, so I'll talk about okay, let's let's see how transformers do. So, one interesting thing here is Transformers do. So, one interesting thing here is that for the specific task, it turns, okay, one layer gives you, here, you're looking at accuracy only on these and only on the second time that it occurs. So, you should be able to predict 100% accuracy. And if you use one layer transformer, this doesn't work. Or it gives you something decent, but it's kind of exploiting correlations or something weird. If you use two layers, you're basically doing perfectly. And just for this separation, there's been some work telling you that, sort of, actually, so this. Telling you that, sort of actually, so the second paper that they have here, I played and sound for Daniel Hugh and Matthew Skilgarsky. The second paper is actually specifically on this induction head task. That's what this task is called. But so basically, you can't really do it with one layer unless you have like really huge dimensions. But with two layers, you can do it easily. And I'll just quickly tell you how you can do it. So, this has been actually, this mechanism is called an induction head, and it came up in the interpretability literature by the people at Anthropic. People are anthropic. So basically, they just played with language models and FOIA data, or also real language models, they kind of identified this mechanism. You just like copy something that in the context appeared before. And the way it works is, so remember at the first layer, you have both the word, but also its position. So what the first layer does, all it does is it tends based on position. So you're looking back. If I'm at T, I'm looking at T minus one. If I'm at T prime, I'm looking at T prime minus one. So you attempt to the previous token. attempt to the previous token and then you copy the content so the this key query matrix just looks at position and the value matrix looks at the actual content the word this is the first layer and then once you have this basically now that you have mister at the end you can look back at mister and you're able to copy word data okay so you couldn't do this with one layer because somehow maybe at the first let's let's say that you can only easily implement these sparse potential Easily implement these sparse detentions, just only look at one other guy, then it's hard to look at these two at the same time. But with two layers, you can do that nicely. Okay. The second layer is this induction head. It attends wherever this mister appeared before, the trigger word, and then copies the corresponding word. And you can look empirically if you train this. This is kind of what the attention maps look like. So these beta s coefficients, you can kind of plot them, and you see that it's basically implementing this negative. And now, to go back to constructions. So, here, basically, the way that you can think of embeddings as just being random. For this task, you don't really benefit much from like training the embeddings. Maybe you can, but let's just leave them at random. So when they're random and the dimension is large enough, they're kind of almost orthogonal. And the other thing that I mentioned is if you have a matrix and you leave it frozen at initialization, somehow you also keep norm one, and then it's orthogonal to itself. So you get a new. Thought all to itself, so you get a new token. And why is that useful? Specifically, because I said, So, here I colored this mister after you copied it. I changed the color to purple. That's helpful because now you have a new thing. You can distinguish when you, when mister wants to attend, you can distinguish this one from that one. So it's helpful to have like relabelings of the words. And then basically with just three layers. So here, this is the first layer I said you attend to the previous token. So you only look at these positions. The previous token. So you only look at these positional embeddings that encode positions. And how do you do that? Well, you create these outer products where when you put T on the left, then T minus one is what gets activated on the right. Anything else is zero. The second layer, you just look at this specific token, but the first one should be kind of something modulated by the value matrix. You get a new, you're able to copy. And then the other thing you have to do is on the output of the second layer, you take in the word, which was this. The word, which was this B token, but then you want to map it into the label space, and the label space is these unembedding. So you can construct these three matrices. This gives you something that should work. The question is, is that what you learned during training? Three? And then, you know, you basically see that, yes, that's what you learned during training on a very restricted setting where you only train these three matrices that I talked about. And you can test this kind of by picking what's the And you can test this kind of by picking what's the maximal embedding that you get on the right-hand side. So you just check that you get 100% accuracy on these associative members that we want. Sorry, the theory should be here. So basically, we look empirically that it seems that this last layer has to be learned first. Otherwise, the other ones don't learn. So we kind of get some ordering of how the layers are learned. And then we have some theory that tells you if you do one gradient step here, then here, then here, somehow you can learn these things. So that's kind of a. Somehow, you can learn these things. So, that's kind of a preliminary theory in infinite dimensions. Then, here I will not talk about these things, but basically, you can think about what happens if you have like finite dimension and finite data. People often want to figure out the scaling behavior of their neural nets. What happens if you grow number of parameters, number of data points? You want things to get better and usually in a predictable way. So, this has been studied sort of for infinite memory. So, finite data, but infinite width. And here we kind of study it with these random embeddings. With these random embeddings, finite dimension. So, the intuition is just if you have these matrices and these are random vectors, and you can kind of control the variance of these things, and you get, okay, this is when you do it right, this is when you do it wrong. It tells you how many parameters you need to score a certain number of things. You can get statistical rates if you're a statistician or if you care about these things, or if you're like a practitioner who cares about scaling laws, or you can listen to Courtney's talk on Thursday for a slightly different model. You can extend it so. You can extend it. So, what I talked about: these feedback layers, usually you have like MLPs, so you have a lot more parameters and nonlinearities. You can kind of extend these things, those settings, and see how many parameters you need to score this many associations. Even if you have like multiple inputs. So, here I talk, you only have one input and maybe one output, but you could have two inputs and not two outputs. You can do that too. And then, this is about what the link is to gradient method. So, it's pretty easy basically if the model, you know, if you have. Basically, if the model, if you have this kind of thing where on the right you multiply by an embedding on the left by another one, you take the gradient, it will basically lie on the span of this like outer product. So this kind of naturally gives you, even with one gradient step on a lot of data, you can get good associated memories. And then you can study this on the task that I talked about before, where you do a few steps one after the other. And this is just one last slide about optimization. So basically, another Basically, another thing that happens is if you have sort of a data that is very frequent and other data is very infrequent, typically, if you only do one step, your gradings are going to be proportional to their frequency. So the top things that you see a lot are going to be present, but the other ones are going to filter them out. But here we argue that sort of if you make the weights a little more balanced, that you make everything kind of equal weights, that will work better in terms of storage. You can store more things. More things, and another thing is that if the embeddings are correlated, so if the dimension is low, these embeddings are, you know, maybe you have two things that are kind of close to each other. This is either because of randomness or because you learn embeddings and they're like semantically related, but you somehow want to separate them. This actually creates co-conditioning. So you usually benefit from these like very oscillatory regimes. You get these like kind of edge of stability type behaviors that you might have heard before. And so many future directions, but Any future directions, but I'll stop here and thanks for your direction. Yeah, so this you should think of it more as like a three-layer network. Kind of a three-layer network just because so basically these matrices are like width by width, but yeah, if you know the mu p, yeah, well, so this is very like you basically you don't have the scaling with dimension of the inputs because it's just like one token, so it's a bit different, but it's it corresponds also to the when you know, I don't know if you know the MUP literature, the people who scale up neural nets, the middle layers usually have a different scaling than the first online. So this is the middle one. 