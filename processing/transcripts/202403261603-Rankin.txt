All right, well, thank you all for having me. Thank you, Silvana, for the invitation, and to the other organisers for their work. So today I'm going to talk about, well, the title is Bregman-Wascherstein Divergence, Gradient Flows and Geometry. So the plan is, first of all, to discuss the JKO scheme a bit. It's very well known, but there's a couple of points I can make. Then we're going to study some properties of what you can do when you consider optimal transport with a Gregman divergence. A great men divergence as your cost function for the optimal transport. So, we're going to study what that does to the JKO scheme, and we're also going to study what that does to some of the geometric structures that optimal transport induces on the space of probability measures. So the talk is a bit more sort of PDE optimal transport focused than some of the talks. So, I won't go into too much of the technical details. We're going to start by talking about these models. We're going to start by talking about these modified JKO schemes. So, I'm going to talk in the context of the heat equation, even though this very famous work by Jordan Kimberler and Oyo was originally in the context of the Fokker-Planck equation. So, the heat equation is obviously very well known. It's sort of the prototypical parabolic equation, the one we teach in all our sort of third-year PDE courses. It models heat flow, as you expect, it models diffusion, and it models Brownian motion. Now, for a very long time, Now, for a very long time, we sort of understood this as the gradient flow of the Dirichlet energy. I'm going to make precise exactly what I mean by gradient flow in a second. But yeah, we understood it as the gradient flow of the Dirichlet energy in the metric space L2. Now, what Jordan, Kendalera, and Otto came along and showed is that we can view it as the gradient flow of the entropy functional in the Wachenstein space. And this was a very surprising result to the PVE community. A very surprising result to the PBE community. As far as I know, the connections between the heat equation and entropy had been not so well studied in PDE. John Nash, in fact, used the entropy function in his very famous 1958 paper, but it wasn't super picked up on. So this connection was a big deal. And I mean, the key thing we need to understand this connection is exactly what we mean by a gradient flow. So I'm going to talk a bit about gradient flows first on spaces we look in a product where you On spaces with a product where you know what a gradient flow is, and then I'm going to discuss how we might extend this to general metric spaces. So, on a Riemannian manifold, and if you don't want to consider a Riemannian manifold, you can just consider Rn. It's straightforward what we mean by a gradient flow. You've got some function. We'll take it as convex for now. It's gradient flow, you take its gradient, and you define the curve whose derivative at each point is the gradient of your function. Radiant in the function. These flows are pretty well behaved. If our function is semi-convex, they're unique. If our function is, say, strictly convex, they'll converge to the minimizer of that function. Now, this relies really heavily on the inner product structure to define what we mean by pregnant. But we can give a notion of pregnant flows in more general spaces. The notion I'm going to introduce, or I'm going to talk about, this notion of minimizing movement schemes. This notion of minimizing movement schemes. It's originally due to Georgi's school. So the definition, it's not too bad. We're now going to assume we're just on some metric space. We've got some functional, we want to minimize f, and we've got a couple of technical assumptions on it. It's semi-convex, and it's lower semi-continuous. Now I'm going to assume I've got some given x0 and some small parameter tau that I'm going to set to 0. I can pretty easily consider this minimization scheme. The semi-convexity basically says that functional on the right-hand side is in fact going to be convex for some tau sufficiently small. So a minimizer is going to exist. And that's sort of how we look at that scheme. We've got our function f we want to minimize, and it would be asking a bit too much to jump straight to the minimizer. So we want to do something to enforce that the next point in our sequence stays very close, and the easiest way to do that is to add this very big constant times the distance. But we can give another motivation for this. At the minimizer, take the derivative of the right-hand side, you're going to get grad f plus in Rn 1 on tau x minus x tau k. So at the minimizer, that's going to rearrange precisely to the implicit Euler scheme. So we've got this very nice metric motivation for this, but we can also see in the case we know it's the implicit Euler scheme. Is the implicit Loyal scheme, a well-known approximation scheme for gradient flows. The key point, however, is that the only structure used here is the metric structure. So we should be able to introduce this scheme in any metric space. And the names of curves, which are a limit of a scheme like this, as tau goes to zero, are minimizing movements. So this is precisely what Jordan Kinderlira and Oli did. They performed this minimizing movement. They performed this minimizing movement scheme for the functional f of rho equals rho of rho. And when they considered the Fokker-Planck equation, they did it with a drift term. So their key result is basically the following. Given some probability measure on Rm, which we're going to assume has a finite second moment, perform the scheme we just explained. Then we're going to define a curve of probability measures. We define it as the piecewise constant curve in curve points. So that's this equation too. So that's this equation too. Their key result, as you send tau to zero, you get a solution of the heat equation. And in particular, you get it in the sense of your rho taus, your piecewise constant interpolants, converge weakly. And I've said converge weakly to be the unique solution of the heat equation. Of course, the heat equation doesn't have a unique solution on Rm, these non-physical solutions, but it will have a unique solution at finite second moment. And that's the one we're going to converge to. The one we're going to converge to. So first line would be a master no-tail. Is there no tale k? It is. Sorry about that. Thank you. Yeah. Yeah, this should absolutely be a rotal k. And I went to correct it before because I had a rotal k plus 1 and I must have got distracted halfway through correcting it. I just deleted the k plus 1. So this paper is a big deal. It's produced a lot of subsequent work. Produced a lot of subsequent work. And in fact, there's a very nice little article by Otto in Siam News where he discusses its impact. And apparently, it's the most popular paper in terms of downloads from Siam Journal of Mathematics or Analysis. They table it, not am. So presumably the table it, not ham. Yeah, well. So I'm going to list a couple of interesting applications and a couple of applications relevant to our work. So some other PDEs you can consider. So, some other PDEs you can consider. Well, you can consider this whole family of reaction diffusion equations. That's the work of Nielk in 2011. You can apply it to some interesting applications. So, there's this famous work of Ari Radna-Shibin and Santambrogio that's discussed quite extensively in his book where they applied it to crowd motion. Entropy is sort of a macroscopic quantity. You can understand the JKO still from a microscopic perspective. And that was done in this work of Adams Derradia and Simmer. Adams, Darflader, and Stimmer. Figali and Gigli, they considered the same equation, the heat equation, but with Dirichlet boundary conditions. This is quite a difficult extension because the heat equation with Dirichlet boundary conditions doesn't preserve total mass. So you're outside of all realm of probability methods. But in some work more relevant to what I want to talk about today, some other authors have considered modifying not the PDE, or not the functional you're trying to minimize, but the distance with respect. Minimize by the distance with respect to which you're minimizing. So, the first person to do this after George Pibera and Otto's paper was Agra. He considered these sort of very popular alternative transport costs, which are things of the form C of X minus Y for a convex C. And more recently, Figali, Gangbo, and Yolku, they considered costs induced by La Cranch. And then, sort of moving to a more expected extension. Expected extension, you can extend the JKO scheme to Riemannian manifolds. And I want to say a bit about that. So when we extend it to Riemannian manifolds, I don't know why my ins have turned into a dot, but when we extend the Wascherstein distance to Riemannian manifolds, we swap out Euclidean distance for a Riemannian distance. And this gives us a very good notion of distance. Good notion of distance on the space of probability measures on a Riemannian manifold. But it is difficult to work with. I mean, for most Riemannian manifolds, you've got no explicit expression in the sense of when you can actually compute pen and paper for the Riemannian distance. So as soon as you want to compute a JKO scheme, you're going to run into troubles with that non-explicitness. And I'm going to give you a specific example that's motivated us. Motivated us, and that also illustrates that difficulty that I just indicated. So, consider mega, just a subset of Rn, but equipped with the following Riemannian metric. It's in components, it's the Hessian of some locally uniformly convex function. So there's no simple expression for the Riemannian distance, which we recall is just defined as the length of any curve between two points x and y. But what I claim is that Brighman divergence for this. What I claim is that Brighman divergence for this particular form of Riemannian metrics, so for Hessian metrics, it's a pretty good standard. It's not a distance, it doesn't satisfy the triangle inequality, but it's non-negative, and you do have that important implication that if xy is 0, then x is equal to y. And I mean, the expression for the Breton divergence is here, and I think we know it fairly well, but it's just the difference between the value of the support hyperplane at y and the function at x. At y and the function at x. So there's the simple picture to understand what it's measuring. So the first sort of question we asked in this work was, well, what happens if you swap in this specific optimal transport cost, which we call the Bregman-Wachenstein divergence, and try to compute the JKO scheme with that in place of your Ring-Manner and Weischenstein distance. Why do we want to do that? Well, because as indicated, it's a lot easier. That? Well, because as indicated, it's a lot easier for these specific class manifolds than the lattice type distance. So, the answer to this question is: things work out essentially as you'd expect. So, we define this math calculus phi, this Bregman Wachenstein divergence, and then we compute the JKO scale with this in place of the Wacherstein distance. In place of the Wacherstein distance. And what we get is we converge to the exact same PDE that you would have with the remarkable distance. So that's the key part of this work, is that you're not converging to a new PDE, you're converging to the exact one you were interested in, but with a quantity that's much easier to compute. So the PDE you're converging to is the heat equation on the Riemannian manifold whose metric is that Hessian metric. So when I write partial Metric. So when I write partial t equals Laplacian rho, I'm of course talking about the Laplace-Beltran operator, which is the Laplacian one and the Barnian power model. Now, we can go a bit further. We can consider the same thing for replacing, and of course there's technicalities associated with that theorem. I'm going to present this more general one and then I'm going to outline some of those technicalities. But there's this very well-developed theory of optimal transformation. Well-developed theory of optimal transport with far more general costs. So C of X, Y, and there's some minimum set of hypotheses you need on this that I'll explain in the next slide. But we can go ahead and ask the exact same thing. What happens if we compute the JKO scheme with a general cost C of XY? And the answer is quite nice. Whenever your C is such that it's Hessian on the diagonal, defines a Riemannian metric, then you get. Metric, then you get convergence to the associated Riemannian heliophages. And there is sort of a couple of related works that consider these Riemannian and pseudo-Riemannian metrics that are induced by general costs. And this is sort of the work of Robert LeCan, Jung Hong Kim, and Mika Wara. So we're not the first to consider these metrics that are induced by the costs, but it's interesting to see that come out in the form of these JKFs. JKAS. So now let me explain some of the technicalities associated with this. So we're going to assume we've got a smooth cost function, and it needs to satisfy two well-known conditions from optimal transport. They're imaginatively called assumption one and assumption two. A1 is that these particular gradients are injective. So think about the simplest case in Rn is, of course, x minus y squared, which we know is equivalent to the. Which we know is equivalent to the cost c of xy equals minus x doty. In that case, both of these gradients are just identities, so certainly injected. This is a reasonable condition. I think it's typically called a twist condition in Delani's books, for example. And then we have a slight strengthening of that. So in fact, debt CXY is everything on the straight. We require a sort of comparability condition. So we've got our cost function. We've got our cost function, our cost function induces a Riemannian metric, and from that Riemannian metric, we get a distance. Now, one of the assumptions we need is that these things are comparable up to constants. This sort of looks like a standard uniform ellipticity condition from elliptic PDEs. And it's not too strong. In particular, whenever we're working on a compact set, it's just equivalent to your cost function being C2 up to the boundary. It's a stronger condition. It's a stronger condition in the global set. So, our work treats two cases. It takes place on a general manifold, and we consider, first of all, compact subsets, where we derive the heat equation and Blocker-Planck equation with a normal boundary condition. And then we consider non-compact manifolds. So on a non-compact manifold, we derive the same results. You converge to unique solution of the heat equation or Fokker-Planck equation with a finite extra equation. But of course, as soon as we're But of course, as soon as we're talking about heat equations on Riemannian manifolds, we need a lower Richard curvature value. So that's a technical assumption that goes into the proof. So with those technicalities, let me explain a couple of key points in the proof. So proofs of JKO schemes, they're traditionally broken into three key steps. First, you get the existence of your Rochal K and RoadCal K. Roche K and Roche count K plus 1 for each K. The details here, they're not too different. Why are they not too different? Well, we're minimizing the same functional as JKO did, so we have the same semi-convexity and lower semi-continuity properties. And optimal transport costs are always convex because they're linear in the transport plan. So changing to a new cost doesn't prevent difficulty, but present new difficulties in step one. Then the next thing you do is you derive. Then, the next thing you do is you derive the discretized equation, satisfied by each time step. This is where most of the difficulties were, because how those estimates work in the original case is you derive particular estimates by essentially differentiating a long reminder in geophysics. And if you do the same thing in this setting, the correct terms you don't drop out. So, we had to change those arguments. And what ended up showing up, which is somewhat surprising, Up, which is somewhat surprising, is these notions of C and C star segments from the optimal transport literature. So, when you want to study regularity of optimal transport maps, the conditions are kind of hard to state and kind of obscure the first time you see. One of them is you have to introduce this object called a C segment. It's precisely the curves whose image, under this gradient of the cost function, become line segments. Those turned out to be the exact right segments. out to be the exact right sets on which to perform these differentiation arguments we needed to make these estimates work. Derivation of these, sorry, proofs of the JKO scheme, third step, we analyze the convergence. This, well, it's different to JKO's original work, but it's not too different to Zhang and Erbar's work on Grinhardian Manifolds. We've got the same assumptions, so we're able to follow the language. So once we've So, once we've considered JKO schemes, a natural next step is to understand: well, a JKO scheme is sort of a way of getting a gradient flow on the Wascherstein space. You can go a bit further and you can get a whole lot of Riemannian geometry on the Wascherstein space. But a break-bund divergence is known to introduce different geometric structures. So, in related work, So, in related work, and I'm being a bit anachronistic here because this was the first of the two papers, but in related work we studied the geometric structures induced by freedom of divergence. So to explain that, I'm going to, in my remaining couple of minutes, explain two things. This Riemannian structure on the Weischerstein space, the Bregman geometry, and then I'll explain the riffing of the Bregman geometry to the Weischerstein space. So, optimal transport induces other geometry. Optimal transport induces other geometric structures in most space space. The most well-known of these is probably displacement interpolations, introduced by Robert McCann. They're geodesics in the metric sense. But you can also formally define a tangent space. This was first done by Otto. It was refined in the anniversary of Jupiter and Sabah's book. Once you've got a tangent space, you've got a Riemannian metric. And that gradient flow of the entropy, in the sense of a minimizing movement scheme, Sense of a minimizing movement scheme becomes a gradient flow in the sense of this metric as well. Once you've got a tangent space, you can in fact go all the way to lifting up your Levy Chip connection from your base space to act on the tangent space of the Wachenstein space. So you really can go almost all the way to Riemann geometry. And when you do that, you see that the geodesic equation that comes from having a connection exactly matches up with Reverend McCann's notion of dispersal interpolation. Appellation. Now, the Brigham divergence induces a number of geometric structures. So, the first is you get two coordinate systems. You get the one your functions originally convex in, and the one you get from taking the image under the gradient map. You get two notions of connections. So, these are the flat connections on your primal view space. Again, I don't know where all these dots have come from. I think there were stars on my computer, so I'm sorry about that. Computer, so I'm sorry about that. I mean, once you've got two connections, you get two notions of geo-basic. Now, I'm going to sort of, to give you an easy way into this work, explain some of the nice connections that you get from these geometric structures. So first of all, these geometric structures just induced for the Gregland divergence essentially completely describe it to third order. So I can expand my Gregland divergence in terms of the induced metric. In terms of the induced metric and these induced connections. And moreover, these induced connections have a very nice connection to the Levi-Chavita connection you get from the induced metric. So those primal and dual connections, their average is the Levy Chavita connection. But just like before, the key point is they're a lot easier to work with because they're flat connections. And they satisfy this conjugacy relation, which looks like the definition of Lebanon Javito. Looks like the definition of level to reader, but one of them is our pioneer connection and one is our dual connection. So the goal of this second work was, well, if you look at how all the Riemannian geometry is lifted, so for example, if you look at how displacement interpolations are constructed, you take your optimal transport map, you draw the curve from a point X to its along a constant speed geodesic to its image under the optimal transport. Geodesic to its image under the optimal transport map, and then you push forward your measure. But there's nothing so special about geodesics that lets us do that only with geodesics. In fact, for any family of curves which join our base space, sorry, which join our initial point x to its image under t of x, you get a new notion of curve in the Weischerstein space. So that lets us lift our primal and dual geodesics on the base space to two new notions of geodesic in the base. Two generations of geodesic in the Wachenstein space. Similarly, Lott's technique, which led him to find the Levi-Chavita connection, well, you can just swap in these new primal and dual connections and get two new connections on the Reichstein space. So a somewhat technical slide, but to explain that lifting of the connection, let me remind you of how you define the tangent space to the Lashstein space. Given an absolutely continuous curve. Given an absolutely continuous curve in the Wachenstein space, it solves a continuity equation for some vector field zt. Zt necessarily lies in the closure of the space of gradients of smooth functions with the closures taken with respect to the L2 of the measure ural. This is the tangent space to the Reichstein space. We lift our inner product on our base space, and once we do that. And once we do that, this is Lott's definition of the WebHP connection. It's a somewhat complicated expression, but what I want to convey is there's nothing to stop you from just swapping in another connection right there and defining new connections on the Weischerstein space. And that's what we did. So here's the result for the Brainwood geometry that I explained before. And here's some of the results we obtained. So in each case, sort of this. Obtained. So, in which case, sort of this double-up symbol corresponds to the one-on-one quash flat space. And what we derived in this work is most of the exact analogs still hold. So, you have this nice expression for curves in your base space. When you lift all these objects, you also have a nice expression for curves in the Wash-Stein space. That's again completely described by these connections on the Wascherstein space. And once again, you lift these connections and you get a lot of attributed connection. Debt a lot, so it should be the direction. Now, once we had these geometric objects, we sort of considered some of the standard examples. We considered a lot of the geodesic equations, these are still satisfied. We considered displacement convexity of functionals. So this is a particular example where this can be useful. If you have a functional that is not convex, but you know is primal or dual convex, then it makes more sense to minimize along these primal dual disease. And finally, we can. And finally, we considered bary sounds with respect to the Great Landmass-Stein divergence, and this we made sort of heavy use of Brandon Pass's results from 2000, however. That's everything I wanted to talk about. So, if you have any questions, thank you for listening. I don't know where to start. So can you repeat? Maybe I missed it. What's the motivation to devise this new GKLC maxion? The motivation is primarily when the Riemannian distance is too difficult to compute. Can we swap it in for something that's much easier to compute? And do you have any kind of convergence rates, any kind of these results? So to actually show that it's easier to compute the current distance? Well, what is easier to compute is the distance. Is the distance. So when you have a, let me go back to this slide with. So if you're, say, doing JKO scheme on a Riemannian manifold, where the metric is a Hessian metric, you probably can't explicitly compute the Riemannian metric. But you can easily compute the Riemann divergence. Okay, so it's more of a theoretical divergence. Well, so this is more a result. You can look at it in two ways. I'm going forward a slide to the general case. It's either a result for JKO schemes computed with respect to arbitrary costs, and the answer is what they converge to is the heat equation. Is the heat equation with respect to this metric? Or it's if you know you're on a Riemannian manifold where the structure is induced in this way. So it won't always be true, but for the class of manifolds where this is true, a particular well-known case is these Hessian manifolds, you get these convergence results. Very good question to follow up. So because there is an optimization, there is a space. using random divergence, so you can actually accelerate instead. Because if you want to do that, you have to show particular I would probably accelerate. Why don't we have some letters not here? Well the answer is I've thought about it but not in a productive way. So I don't know if this accelerates things but I wondered what would happen. I think on the property space it's hard to estimate. That's the use. Acceleration requires Acceleration requires a mixing of proper distributions. But the mixing is another standard note drawn. Like here you are working on different notes of the method. But acceleration there are then even in the JKO scheme. That's interesting. So there's no real known ways to assess the JKO scheme? There are some discussions in the Twitter about the general product how fast so it becomes upgraded on on different name. Upgraded on different mechanisms. Interesting. Thank you. I have a question. So, are there, what are current examples that the functional is not primal convex but dual convex? Not primal convex, but dual convex. Well, I I mean I'm I'm not gonna imagine that's a kind of handle. Um, but I I think these are just the fact of the convexity in different coordinate systems is different. Coordinate systems is different. So, here, because in each case we're defining convexity with respect to the coordinate system, there's no reason to assume a function that is convex in one, that's convex in the other. Okay, that's part of the day. Let's talk about Sukhong's algorithm.