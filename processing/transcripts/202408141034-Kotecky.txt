You have one minus two. Okay? And in addition to what you know from percolation, you have this q to k omega. k omega is the, where it is, is the number of connected components of the graph shown there. And you have normalization, you have partition function normalization, which is simply sum of the same things that are upstairs here to make it remote greater measure. Okay. And as we were said in some talk, partition function, all the information is in the partition function. So I will talk a lot about partition function because this is where all information is. Okay. And for q equals one, there is this q to the number of components. So for q equals one, it is just percolation and percolation on the components. And percolation on the complete graph, G skn, corresponds to attached n. You just have to translate this percolation configuration to random subgraph. Okay, so now the graph can be either finite or infinite. So I am somehow dividing into these two groups. So for infinite graphs, you know, for example, Zt, you can define this measure directly on full. Directly on the full space in any way you like. You take weak limit of the measure in finite volume with the sequence that is growing to Z D and define the limit. And for percolation, it's just a product of these measures on independent edges. So it's really useful. Okay. So, when the graph is infinite, then we are somehow talking about phase transition. So, phase transition for a random cluster model on an infinite graph, say D, with D bigger or equal than 2, is manifested by existence of a edge weight PC, PC that will depend on dimension and on Q, such that you can define You can define the event x connected to infinity, which is just the event that you have infinite component, okay? That the component zt contains a fixed vertex x is infinite. And then you compute what is the probability of this event. And up to P C it is zero, and above with P bigger than P C it is positive. Okay? So this is phase transition for infinite phase. And okay, there is, you know, there are all these papers. There are some subclatives because sometimes you can define this, you know, this critical edge weight. You can define other critical edge weight where there is something other happening, and then you are proving that these two critical edge weights are somehow exactly. Weights are somehow equivalent, or they are not equivalent, so there is no big industry. And of course, you can think about other lattices or other infinite graphs. So, for example, in this paper, 1904, there is you know big collection of of infinite graphs for which my collision or even the number graphs that was was well understood. Well understood. Okay, so let me just stress that finite and infinite graphs, this is entirely different problem. Somehow you can think about infinite graph, this is something easy. Because you are just looking at phase transition with my physical talk here. When you have a phase transition, for example, for Phase transition, for example, for easing models. So you have temperature here, or you call it beta, and you have magnetization here. So magnetization is zero at beta small, and then it jumps something. Okay, actually, for easy model, it doesn't jump, but for POS model with sufficiently large clue, it jumps if you define it correctly. Correctly. Okay? So you just need to prove this discontinuity somehow. Something non-analytic is happening there. While for the finite graph, as we will see, you have to look on finite graph, on sequence of finite graphs, GN, and then it can be smooth in some sense. We will see that it will it is not that smooth, but somehow I It's not that smooth, but somehow I I spent part of my life doing something that we called finite range no, finite size, finite size correction to phase transition. And if you do, if you compute just say this type of cosmodel in finite volume, in some volume lambda n, okay, then the curve looks like that. Okay, and then you And then you want to know where is the transition. So you take, for example, inflection point. And then you want to know how quickly inflection point is going to this transition point, where it's n going to infinity. So you have somehow similar problem that you are meeting in graph sequence. Okay? You need this sequence. So it was somehow, in my mind, it is more difficult than infinite work. Than infinite way. Okay? Okay, so finite graph, you have this one way of talking about phase transition. There is this emergence of J component. And so you take a sequence of G n graphs with moving probability Bn and say for percolation, this is a virtually random graph where you have somehow the critical point. The critical point is, so Pn is lambda over N. You choose this, no dependence on N. And critical value of lambda is actually 1. Then you know that above 1, you have, or below 1, you have, you know, maximal component is tiny, and above, above, above n, for long, above one, it takes considerable. It takes considerable size of your graph. Okay, I will talk a bit more about this situation, but let me when I will talk about random cluster. Okay? So if you try to put random cluster on Kn, surprisingly, it is not trivial. And it was, you know, I will somehow discuss the paper where it was really done. Really done. Okay, and I want to talk about percolation on M cube, so let me first introduce what is M cube, or recall rather, what is N cube. So you have, you know, you have a graph with vertices, that is, set of vertices is 0, 1 to n. So you have sequences of 0, 1 of length n and with edges that connect neighbor. Neighboring vertices that, in this definition, can be said vertices whose defining sequence is differing only on one position. But it is the standard cube you see only in n dimensions. Number of edges is one half of nn, where n is number of vertices. Number of vertices is 2 to n, of course, and number of edges is one half n, because you and every vertex. Every vertex you can go in one direction, and you have n directions, so it is nn, but you have to divide by two because you are when you are on the boundary, then you are sleeping in empty space, so you should take those. Okay, okay, and somehow even for percolation, it took some time to prove that there is this g giant component transition, okay? Component transition. So, first in the paper of Elder Spencer, this is 79, what they did was that somehow proved the subcritical situation. So, if you have the random cluster measure in this situation on this graph, and subcritical lambda, which in this case is 1, then C max is small with respect to number of. Is small with respect to the number of vertices, asymptotically almost surely. So, this was their result. And then it was actually one preceding paper by some Russian, but I now do not remember the name, Purtin, or something like that, where he suggested this problem, but didn't go anywhere, not even proving this subcritical situation. And then there is a dicon of semi-ready couple. ICONWORS Seminary a couple of years later, and they were discussing super-critical situations and were able to show that there exists a constant that depends on lambda bigger than one such that size of the maximum what I am saying such that it is smaller than one. It is positive, but It is positive, but such that the size of the maximal component is of the order 2n. And the crucial new idea in this proof was invention of this method called sprinkling. And the sprinkling is that you somehow do study your percolation with some lambda. And since your lambda is, say, above one, so then you can add a bit. A bit you somehow first consider configuration with respect to this measure, to phi nowhere here, okay, for this measure, no, yeah, it is, for this measure, and now you go, now you move lambda a bit, okay, and you do another percolation by this a bit, okay, and here it is written in detail, so somehow first you open each edge. So, somehow, first you open each edge of E independently with probability P. So, this is the standard percolation. Then, open any remaining closed edges that were still not open with probability delta 1 minus p, and then there is a easy calculation that the final thing is again random cluster model, only random calculation, only with p shifted by delta. Okay? So this was the idea that they were inventing and it's still the proof is somehow it is not easy at all and and then afterwards there was you know the burst of papers. So there are there was you know first the Bolobars, Korayakava Lilczak, then van der Rofstadt slate, Norton slate, and then there was And then there was this group of people that had papers called 1, 2, 3. And for example, in that paper, there was also Renko Sasaf that this is some review here in the year 2012. And one of the results of these papers is that somehow if you look on the transition point on critical problems, Critical probability, then it is somehow it starts with one over n, but then there is a series, and in particular, and then around this point, okay, so there is a real point where it happens, so around of this point there is a window, and in this window you are looking. So, there was all the somehow industry looking what is happening in the window, starting with Polopash in the graph, and in this industry. And in this, so somehow the window is very tiny. The window is of the order 2 to minus n over 3. So there is this really tiny window, and if you take, say, 1 over n minus 1 or 1 over n below this piece, below 1 over n, then you are away of the window. So you are somehow, if you take, for example, 1 over n minus 1 over n. Minus 1 over n squared, or something like that, then you are already in the subcritical situation. And if you take this with plus or plus 2, 1 over n squared, then you are in supercritical situation. So it is, and then you can use what is happening asymptotically with this window and simpler. So there is a lot of improvement. Okay, so concerning the message of this page is percolation. Pages percolation on N cube, you know a lot about it. Okay? What was this? Well, there is a recent paper by Lichev where he somehow collected a lot of models, and also these are like these GN models where you have some GN and you are looking for this large component. So here's some of the Component, so he somehow started a big collection of this model. So I don't know whether it is the best, the last one, but somehow I noticed there is a lot of problems of this type with different graphs, with different sequences of graphs. Surely they are products of the graphs. So in particular, Qn is belonging to the SwitchFest results. Okay, now somebody was already mentioning there is this Bolobage-Brimette-Janson paper. So this was the first paper where the emergence of Jan component was studied for random cluster model. And this is just on complete graph, so it is trivial somehow. And indeed, what's somehow important, so okay. So, okay, so there is this formula which I haven't seen before somehow, but it is, you know, this formula where you have lambda critical that depends on Q. So for Q smaller than 2, it is just Q. So in particular, for Ising model, it is 2. And for Q bigger than bigger than 2, you have this formula here. So it's kind of like that. So, it's kind of like that. Okay? And the formula stems from the connection with the mean field limit of the Potts model. I will talk about Potts model later, but somehow the thing is that, okay, I will mention it later. So, a crucial step here is, you know, they have a complete graph. This makes somehow everything doable. And one important thing is that somehow if you if you there is a way of reducing this, you know, they're reducing this random cluster with Q to percollatic model on somehow smaller letters. So a smaller set, smaller complete set. So in particular, what they are doing, they are somehow for every cluster, for every component. For every component, they choose color. Okay, you have Q colors, and among them, there is a particular color that they call from, I don't know why, red. Hated them for that. And then there are other colours. So so there are these red components and there are remaining these are green components. Okay, I would prefer it otherwise. But okay, this is how they did it. Okay, this is how they did it. And then on the red, there is, you know, number of vertices on the red component, this m tilde. And then you have somehow, if you choose, aha, I am putting this red and other components with probability R, the colors. Okay? So if I choose this R to be 1 over Q somewhere, it's Q, somewhere, it must be written. Yes, R is 1 over Q. Okay, so then what they are getting is you have indeed just percolation on slightly smaller graph, and then you have these green components with a spiel-ranger cluster model, but somehow you have this basic thing and you can you know everything about about uh alcaldation on on complete graph. This is just Erkel Schr√©nier. Graph, this is just a Pej Reni, so somehow this they are using very, and in addition, they are using the mean field, but I will come to this later. Okay, so there is Darion Myers, he wrote a paper that was part of his thesis where there was alternative proof of and it was based on you know the And it was based on, you know, there is this Marik Biskupin, Concheyus, and Miss. And they somehow, that is somehow about large deviation approach to percolation on the complete graph. So Darion is somehow extending this and helped me into working. So this is like second paper, but still on complete graph. One complete graph. And to my knowledge, there is no other finite graph sequence for which you would have random cluster transition. And from some reason, we were interested in this random class. Random class, and okay, we were interested in some other model that somehow maybe I will come in the random interchange with some weight on n cube, because n cube is the first moment where you have some geometry. Complete graph is somehow faceless, it's just everybody. So, this was somehow interesting, and I wanted to understand. And I wanted to understand what is happening there, and I didn't find, you know, the only paper was the percolation paper, Itash Komnosh Semrady, and there was nothing. So this is surprising. You can somehow you look around and it's not done, it's open for everybody. But it's a lot of other in this, how is it called, Richie or there's This guy I was showing that somehow we have a lot of finite sequences of graphs where there is percolation, but none of them has a proof for random cluster. So here we somehow came with the proof for random cluster, and there is this question mark here that comes from the fact that I somehow was. That comes from the fact that I somehow was occupied by other things and I somehow was not looking at that at all. And then somehow after after some time I decided we have to finish it and it is slightly sub-optimal. Because there is, okay, I will show you the statement, but there is the thing that it is done only for. The thing that it is done only for q integer bigger or equal than q, and the reason is that we are using Potts model. So, this is one thing. And they succeeded. Remed Bolovan Bolovanch, yeah, Bolo Blash Bolo Basha. They somehow succeeded to fill the gap between this integer. They are also using, you know, the. So I need to look whether you know. But I was looking and I somehow do not see really how you probably how to fill the gap. Okay, I will come. Can I ask a quick question? This is exactly the same balance, but a complete graph, like the exact same result. It is the same. It is the same. Is it surprising that? It is not surprising because we are using the same mean field. And this is one important point. They are using mean field and we are using mean field. Only they are using it because they are using, they are, so let me tell you now, they are living on complete graph. Okay, and complete graph is trivial. When you are a mathematical physicist, as I started to be, so somehow this was. So, somehow, this was a dirty word, mean field. Who is doing mean field? It's wrong. For example, for the easing model in dimension one, it predicts the phase transition of the first order, and there is no phase transition in one dimension. So it predicts wrong things, okay? And it's trivial. This is this Curie Weiss model, essentially. This mutual is Curry-Weis model. And what is Curie Weiss model? You let everybody interact with everybody. You have, you know, a bunch of atoms. You have a bunch of atoms, and now you somehow introduce this funny situation where everybody is interacting with everybody. So, if you take these two-point interactions, everybody with everybody, then it is simply sum of the interaction to the power end. So it's really triggering. Then you have all these random energy models and things like that. So, this is stemming from this. This is somehow better version of. Somehow, better version of that. But you're getting the same thing on the cube. You're getting the same results on the cube where things are more. So I will come to that. Okay, so here is a statement. So the subtle optimality is that we somehow need integer for the Q. And also here, somehow you would expect that you would be able to prove that in the sub- That in the subcritical situation, the maximal component is of the order n, like it is in percolation. I was showing this result by Erdes and it was and the other guy and there and there they proved only this this thing, but somehow of course But somehow, of course, it should be n, and it's possible to prove that it's n. So, here it is one non-optimality, here is second unoptimal non-optimality, but otherwise. Fine. And there are four ingredients in the proof. Okay? So there is Potts model, there is a whole so-called coupling, there is mean field, and there is random. Is mean field and there is random cluster sprinkling. So it is different sprinkling than in percolation, so we had to invent something slightly different. And then we were using it some situations in a similar way. So let me start with POTS model. So for you probably not familiar with POTS model. Probably are familiar with Potsmoder, but like, okay, I. So we have this, you have this Q values. I mean, usually people for Potsmoder, they simply say that somehow space of configuration on one side is simply this. 1, 2, 1, 3, 3. But like Imminer Copper, what he is doing, you know, you can consider the vector. You can consider the vectors in Q directions, in the space of Q. You take vector, you take coordinate directions, and what they are doing, they somehow consider this, they cut it with this plane, with this hyperplane, and then project on this. But from some reason, from some technology. But from some reason, from some technical reasons, it was somehow for us more reasonable to take just these vectors: V1, V2, and so on. So these are our spins. So the set of spins is V1 up to Vq to the power V. So you have sequences on every vertex, you have this. Vertex, you have this collection, okay? When we want to be Q, our coordinate vectors in our Q. And we are interested in the following measure. So for every X, we take lambda X, which is just counting measure on this end set. And then we define the product measure on the full for all vertices, and the Q-stable model with inverse temperature beta. model with inverse temperature beta assigns a spin convergence sigma to the vertices of Qn according to this measure here. So there is notation and it is you know E to minus beta Hn sigma. This measure I constructed here. This is just counting measure on this on this dimensional space. And you have to divide by by partition function. By partition function. Energy is this here, and somehow instead of writing like if you are using this notation, then you have to write delta sigma x sigma y or something like that into your energy. So here we are just writing scalar product. Okay, scalar product, there it is here. And scalar product, if they are somehow pointing in the same direction, it is one. If they are pointing in the different directions, If they are pointing in the different directions, it's orthogonal. So it's okay. And partition function is simply sum of those of this thing that is upstairs of the Herbus partition function. Okay? And of course you can write, you can introduce the, for every sigma you can introduce energy, that is somehow energy of those guys that have different sigma. Sigma x and sigma y and then the energy can be written as E sigma minus E. Okay? So this useful. Okay, so this was the first thing, POTS model. The second thing is Edward's local coupling. It comes under other names also, but you know, Edward's local, it's the nicest way to introduce it. And it's a link between. lose it and it's a link between pots model and cluster and cluster model around the cluster okay so with fixed q we define somehow product space so here you have the percolation configuration and here you have the pox coffin configuration so you take the product and on this product space you introduce the the measure that is somehow that is given like that That is given like that. So you have, you know, the alcali term, and then you have this condition that with given omega, omega can be, where it is written, here it is written. Omega sigma is compatible if omega E equals zero for every E that belongs to this E sigma. So when they are different. E sigma. So when they are different, there cannot be a match. Okay? And then you can simply introduce this measure, and it's easy to see that the marginal on omega is just random calculation, and marginal on sigma is just possible. Okay, so then you can use it, and you need connection between beta and p. beta is minus logarithm 1. Beta is minus logarithm 1 minus beta. Okay? And then you can simply sample if you are first sampling omega according to the percolation measure, a parameter random cluster measure, and then assign the spin from the set uniformly and independently to each component of the omega. Then you are getting distribution of omega and you can first sample sigma according to the Potts measure and then open every edge. and then open every edge from E minus E sigma independently and you are getting exactly you are getting your sampling random cluster measure okay so then using this this this CRL essentially we need this explanation then we are getting that random cluster partition function That random cluster partition function is simply e to minus beta e, partition function of positive n corresponding this beta and p should be connected by this function. And also, what is somehow important is the mean energy, sigma x times sigma y is the energy of the nearest neighbors somehow in the Potts model. So the mean energy is, oh, okay. Is given in terms of the probability of connection of that X and Y are connected by this formula. So this is a dwarf-soccer coupling. Then we had a mean field. I already said the story about why mean field is trivial somehow. And what people were already a long time ago doing, what somehow physics. Doing, or somehow physicists were doing that, somehow they were looking on the models in D dimensions, and they somehow noticed that if you go with D to infinity, then you should get mean field. Inferior is where everybody interacts with everybody, and you have some pre-factor one over n. And in in with dimension going to infinity, you are somehow approaching to this to this move, right? To this to this model. So there is a paper for random clusters. This was done for percolation and stuff like that. And also for Ising model, for example. There was proof. For a long time, people were saying that the reading somehow term in the D going to infinity is 1 over T. So this was somehow, and there was some discussion of that in physicist literature. But this is the paper. But this is the paper. This is the this is our statement. But there is a statement by there is a paper by Schumann and the guy who invented percolation, but not this age, but the K guy, Castell. And then all the time, somehow you open. I do not remember the names of fruits. So I'm saying this fruit, it's it and it has this, you know, somehow. And my wife asked me, I mean apple or I mean, age is horrible. Okay, so what they did, I am not putting their result here, but this question Shonma. But this question, Shulman, is funny. I somehow needed this paper. And I wrote to Shulman. And Shulman said, I do not have the copy and I do not know where it is published. So I found it eventually. I found it, downloaded it, and sent it to Sean Marx. So he has his paper. It was published in some journal. I don't find it. In some journals that discuss. In some journals that is called probably general of mathematical physics or something like that, but it disappeared. It's not general of mathematical, so it is slightly different there, because there is general mathematical physics there in States. But there is some other journal with similar name. And it was in issue one of the paper by Pakistan and Schulman, and then it disappeared. And it's disappearing from our view because even From our view, because even Schumann doesn't customize that, and Schoenman doesn't have it. So, okay. So, what we needed was to reprove that for our n cube. So, with n going to infinity and Potts model on the n cube, you are getting mean field. And it happens that it is. And it happens that it is the same mean field that you are getting, and it is, you know, it is the end that you are getting when you take z d and go with d to infinity. And it is the same mean field that you are getting when you have a complete graph. Complete graph is mean field by definition. And these are somehow getting in the limit. So there was somehow some proof needed. Some proof needed, but you know, actually, not so short. It's somehow, you know, it was quite cumbersome, but somehow we copied essentially this, you know, Kestanman paper, but it is long and our resource, it's 10 pages or something like that. You have to prove this one while you are somehow looking on the limit of free energy. Free energy, function, you are getting that. And if you solve for this, there is some maximum over V here, and you have some norm V squared, and you have T T sigma. So you compute that, and here is the solution of this computation, and where G is this function, and theta lambda Q. And theta lambda q is solution of what is called mean field equation. This somehow was from the very beginning in the mean field theory was this type of equation. And theta is the largest solution of this mean field equation. So sometimes now you have to look on this function and see where is the largest solution. Sometimes it grows. It's whatever it is doing. So the largest solution is the theta equal to zero. And then it starts to be somehow to have some maximum not at zero. And then the larger solution is this theta lambda q. And this determines also what is this lambda C. Because you see that for some values it is at origin and for some values you have something positive. You will have something positive. Okay, so this is the mean field solution somehow. And so it is, no, it was not immediate that you get the same mean field as this complete graph or as this is F D going to infinity, going to infinity. But anyway. But anyway, you are getting it, and then by adversarial coupling, you are getting, you know, also free energy of random cluster model. Here it is written, and you are getting also probability that X is connected with Y, and this is here, this theta is density of the spins somehow for Potts model, and here somehow it can be linked to this. And here, somehow, it can be linked with probability that x is connected with y, this is theta lambda, theta squared simply. So it is zero if lambda is below lambda c and if this is theta lambda squared. Okay. Okay, so random cluster sprinkling. I was showing you this percolation sprinkling. So this is just to show you that there exists something. Just to show you that there exists something that is this animal that we call the random cluster sprinkling. It works somehow. So here is some measure, okay, on the per xi psi omega, on two somehow, you know, on omega g times omega g. So you have two percolation configuration. So you have this, but it has, you know, this parameter p and also parameter delta. Yeah, and you have. Delta. And you have just standard random cluster measure, phi GP cubes psi, but then you multiply it by this phi delta psi omega. And this phi delta xi omega is just like percolation probability with additional condition that psi is smaller or equal than e. And you can notice that sum over Notice that sum over omega of this guy here is one. Okay, and this is somehow this shows that this, because if you sum this guy over omega, you are getting one, and then you can, this is measured, so you can sum it over xi, so it is also measured. So you have this measure, okay. And the xi marginal of this measure is given by, okay, so here I am explaining that, but somehow. I'm explaining that, but somehow you can define the omega marginal of this measure as this random sprinkling random pass energy. And then you have a claim, and this claim is saying that if P1 is smaller or equal than P2, and you choose delta that is smaller than the difference between P2 and P1. The difference between P2 and P1 divided by Q, then this Pringle measure is stochastically dominated by just random cluster measure with PP. So it is playing a role of the sprinkling that you somehow start with somehow taking random cluster measure and then you sprinkle it a bit. Then it is not random cluster measure anymore, but at least it is dominated by random cluster measure. Okay, and now I come to really most difficult part because you know both it takes some time and it's somehow so I so I am somehow trying to say some something about main ideas, but it will be somehow big formula. So I will start to write it at some moment and I decided, okay, let me. moment then I decided okay let me only do subcritical phase and su and supercritical which is more interesting I will just wave my hand so but here is is essentially subcritical case so so and it is by by contradiction so it is assumed that the limit is not zero but it's positive then if you if you Then, if you take, measure with lambda slightly bigger, with q times alpha bigger, okay, of the probability that x is connected with y, then it can be, then it is stochastically stochastically dominating this this uh this radio. Well, call it this how it is, you know, the main word here is which is which page is this oh, I totally I totally but you know to my to my uh spring so to my uh defense I somehow for example I am unable already from young age to remember the name of I'm educated in mathematics okay and not in probability too much okay so there is Okay, so there is a characteristic function, and you, probabilists, you have some different name, which is indicator. Okay, and you know, always when I want to use word indicator, I stop. In my head, it's characteristic function, and now I think, you know, how these guys hold. Okay, so this is sprinkling. So here we have sprinkling. So so here we have sprinkle geometry and then there is this step here where I write something so let me somehow try to explain it so I somehow starting with this spin measure and I do the computation and I make it smaller by introducing by introducing this this what this event A and C. So it is something, you know, it's it's small it's it's it's some event, so I It's some event, so I'm decreasing, I am inserting this event here. And this AMC is the event collection of those omegas for which cluster at X is bigger than C equal to N, and so also cluster at Y is bigger than C. At y is bigger or equal than c times 2n. And so it is somehow then you can here I am computing with this random with this sprinkling measure. So here I have this event and here I have some rest. So I am summing over xi and summing over omega. This is just probability in random cluster measure. Random cluster measure of this event. And this is more. So one has to show that this is something like 1 minus epsilon n, where with n going to infinity, this epsilon goes to 0. So it is essentially 1. Okay? And then somehow having this probability here, so there is the cx is bigger than c times 2. Is the Cx is bigger than C times 2n. So this can be written that it is again bigger or equal. I am taking under the condition, no, I am taking x belongs to C max, but C max is bigger than C times to 2x, so it is some smaller event. And now I write this as a conditional probability, and you know, this one part. And you know, this one part I put as the C to N. Now, here I have C D to N, and I have this probability of this one. So, this leads to contradiction because now, if you collect all this, we essentially prove that there is this epsilon one squared. So, if I replace the C's by epsilon, I have epsilon one squared, I have epsilon two squared, and then I have this. This error that is one microserps are n. Okay? And this is a contradiction because somehow I know at the beginning, I know that C max is bigger than zero. And here I bounded the C max by this thing that goes to zero. Okay, so this is now for lunch. So this is, and now for lambda bigger than lambda c, it's more complicated. So let me just say that we are adopting Itashkommerzen lady somehow, and their argument proceeds in two crucial steps. Given a vertex X, then they show that with high probability the size of the component CX containing given X may be approximate. X may be approximated by the size of Poisson branching because lambda is bigger than one. So you start this usual method of somehow comparing it with this probability that the branching process survives. And as a result, there is a lot of vertices that belong to components of our list of them. And then they are used. And then they are using this sprinkling. So increasing lambda slightly causes most of the components of order in Q and omega to merge, resulting finally in the single giant component 221. And we do not have possibility to use this because, of course, maybe I've wrote it somewhere here. The second step above is somehow possible. Step above is somehow possible because we have this print link also, so we can somehow try to mimic it. But the first one is more complicated because the branching process is using the fact that you have this independence. And since we have a random cluster model, so we cannot use this part of argument. So we replace it by proving that somehow that there That somehow there is a large density of positions for which the component is bigger than constant times n squared, which is stronger than they are using. They start with one. And somehow with proving, you know, belong to a component of order at least n, but we start with having a component at least of n squared. Okay? So Okay, so this is somehow really crucial point in the rules. And then somehow going, when we have already components, a lot of components of the size n squared, then we can use sprinkling by somehow connect them. And our sprinkling is easier than theirs because they have only size n, so they need to do two sprinkling. One sprinkling and then they have n square, then another sprinkling and then they have finally. Sprinkling and then they have finally to 12 when the density is correct, when you have a lot of those spots. And we are able to do it with one sprinkling somehow, which is written here, but I am not even going to explain what is, or maybe I can draw it somehow. This is RAB. So somehow if you have X, you say that X satisfies condition RAB. If you R B, if you somehow look on his neighbors, okay? Then somehow the neighbors have their neighbors. So you look on the set R and B that is such that, how to say, that set of rich vertices, which is, you know, we say that x is x is n b rich. A B rich if there exists a new subset of the neighbors, such that the set of neighbors such that the set of neighbors of this point, so there will be, for example, only this two, will be this two, and the set of their neighbors, okay, they have some neighbors, I cannot. I cannot draw it, but it is in high dimension, so I can somehow, in high dimension, I have a lot of neighbors. So I can somehow draw this U having still a lot of neighbors, namely what I need. I need that they have, you know, that there is a part of the component, we call it cell. Cell is part of the component of you. Is part of the component of u, of the point from point from here, such that the size of this vu is bigger than n squared. Bigger than b n squared. And moreover, so this is one condition, and moreover, all these somehow components, all these parts of the components, I cannot all component, but I separate them somehow, they are disjoint. And then there is some theorem saying, this is in this paper of reports and those, there are these theorems saying that if you have on Qn, if you have on the B of M cube, if you have two guys with a certain number of neighbors, there is a Of neighbors, there is a lot of passes that are somehow connecting them are and are disjoint. This at BU and BU1 and BU2 can be connected with a lot of passes that are disjoint, and this then can show that the density of those dies is already quite large. But you have to prove it. To prove it. Okay, so since I'm done, so half of my work is lost. I don't know whether I should... Oh. I put end of the document here. So in the other version, there is also the rest. But somehow, maybe to serve it only in one word, there is this. There is this random exchange of vertices. So you have a graph and you randomly exchange the vertices. You create permutation in that way. And now you are looking how long are the cycles of this permutation. And somehow to have long permutations, you have to really visit a lot of vertices. A lot of vertices. Essentially, you have to touch every vertex at least once. So this is like one-half of n logarithm n. But actually, as Odette Schram proved, something is happening already on much smaller one half of n. And if you are somehow doing it longer than one half of n, then you create this somehow quite big. Somehow quite a big component, living not on everything, but on itself, but it has long cycles of the size and then if you do it for shorter time, you do not have that. And we wanted to do the same for NQ, and we did it with Daniel Welchi and Pyotr Miankis, no, Pyotr Milos. Miloch and what happened there was that somehow we got sub-optimal result again. There is a paper, but it's published, but somehow the result is sub-optimal. You would expect that it is somehow using 2 to n vertices. The longest cycles are 2 to n, but we have square root. But we have square root of two dividends. We have two divided halves. We were unable somehow to go. But this is for percolation. And now there is this random cluster, and this has a physical motivation in POSE transition and Heisenberg model. And there is a paper by Bela Borbash. Not by. You know him, everybody. I was not lucky. Okay, so Barning Thought proved the condition that somehow you have long, that having long cycles means something reasonable in physics. And so the idea was: okay, let us, because there were proofs by Sharm of Copy Graph again, so it's trivial somehow. There are some people sitting in audience where. There are some people sitting in audios where he's over there, the one who's not listening. Yeah, so he wrote some important paper about that. But again, there was nothing for MQ. So we tried to do it and again finished in suboptimal results. And, you know, I ask And I ask Daniel, who is somehow more into this quantum stuff, so I ask him whether there is any more results in that, and he said our paper, which is like 15 or 16 or something like that. I don't know. On Z D there is it. Okay. So it is still up some. Okay, thank you. Questions? So can you say anything about the emergence, like the window size or what happens at the critical point? No. I was not looking on this server. We have this lambda C, and I don't think we can always see it's Q is not integrated. If Q is non-integer, is it easy to show there is a phase transition? We are using this, you know, this mean field. And mean field is for POTS model. And POTS model is somehow so the mean field is using POTS model. And somehow this Polobash, Grimmert and Jensen. Jensen, so they were able to. So they were able to somehow avoid it, but they had this somehow complete graph and percolations on this somehow in smaller graphs. So they had something that was helping them. But so this is, of course, it would be nice to do it. For example, Kesten wrote a paper again. For example, Kesten wrote a paper. Again, it's published in some book that nobody knows, and it's quite difficult to read. But he was proving in the interval 1 to 2 for every Q, the mean field, some sort of mean field. So there is mean field even for, but it is not going through. It was really somehow mean field for Mean field for random custom model. It should be possible, but some of us in the room have a paper which makes lots of concessions with like Q being large and everything, but we don't find the phase transition exactly, but you can sort of show that something you can show there's a window where there's a small component below the window and a large one above the window, but the window is quite wide. Yeah. We also need an engineer key for completely different reasons. Well, I proved in 1981, I had a paper that was in communications that was about limit Q going to infinity. And there was, because there was some, in physics, there was, this was not about Potts' model, it was about Potts gauge model. So this was different. And for this Potts gauge model, they were somehow claiming. This post-gauge model, they were somehow claiming there is some gauge symmetry, and you have to get rid of this gauge symmetry, and then the mean field somehow, then they were not formulating any mean field, but they somehow, this is the correct way of computing partition function. I proved it in the limit q going to infinity with some sort of mean field ideas. This was funny paper because This was a funny paper because I wrote it. The crucial idea came to me in the last night of the year, how it is called. New Year's Eve? Yeah, yeah, New Year's Eve. New Year's Eve. Yes, we call it Strategy's Eve. So so uh so it's New Yor New Year's Eve. I was somehow working through the night and I had this He had this somehow beautiful idea, so I put it on one piece of paper. They also know somehow they are compact, you know, somehow writing this. I left it on my desk, and our second son, who was about one year old, then somehow succeeded to chew it in the morning before I awoke and came to the desk and there was Came to the desk, and there was this, you know. This certainly happened, or it was on a green? You know how it happened. I took the sink, I somehow smoothed it using this, you know, how we are. How do we connect with this device? So, you were able to prove it again a year later? I was saying, I proved it for second in my first job. Did you say first January? First I was in the meeting. Okay, let's thank Brendan again.