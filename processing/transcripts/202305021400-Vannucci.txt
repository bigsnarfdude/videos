And Amla. They are both virtual. I think, unfortunately, we weren't able to bring them in person, but I'm sure their talk is going to be very interesting. So let's start with Marina Benucci, and she's going to talk about Gaussian process regression models for the analysis of event-related potentials. So please, Marina, just. All right, thank you. Thank you very much for the introduction. And sorry that I'm not able to be there with you guys. I was virtual in some of the sessions yesterday and it seems a very interesting, stimulating meeting. So congrats to the organizers for putting it together. So I'm going to give a talk on some of my recent work on the On the modeling of ERP data. And I'll spend most of the time talking about a first project which we have now completed on a Gaussian process regression model for what we call averaged ERP waveforms that accounts for derivative constraints. And then if I have time, maybe the last 10 minutes or so, I'll present an extension that I've been working on with my postdoc here, a rice dusting. My postdoc here, Rice Dust in Pluto, on analyzing, on modeling trial-level data versus instead of average data. Okay, so for this audience, probably I don't need an introduction on ERP, but so ERP represents EEG signals which are recorded in response to external stimuli. So they are time-locked according to a set of external stimulus. Of external stimulus. And typically, the ARP waveforms or signals are averaged across trials and sometimes also across subjects or across, say, electrodes for a given subject. And the purpose of the averaging is essentially to wash out the noise. So here in the figure, you see, like, you know, EEG segments, so these time-locked segments, which get averaged again across, say, multiple. Averaged again across, say, multiple trials and maybe across multiple electrodes, like for this subject here, to make up what we call an averaged ERP waveform. And then all the analysis is usually done at the level of the average ERP waveform. And the interest is in estimating the curve itself and in particular, some of the characteristics of the curve, which people refer to as ERP components, like the peaks that you see here or the The peaks that you see here, or the depths, the valleys, the points P1, N1, P2, N2. And people try to relate these characteristics of the signals to particular brain processes under study according to the type of experiment, the type of stimuli that the subject is responding to. So, when we looked into the literature now, maybe a few years ago, three or four years ago, into the statistical literature. Into the statistical literature for ERP data, there wasn't very much done. Only a few groups were working on analyzing the data or developing statistical models for this type of data. So we thought that there was an opportunity for us. And initially, this is the first project that I'm going to describe, we concentrated on trying to do a better way, a better analysis of the average ERP waveform. So essentially, coming up with a model-based Up with a model-based approach for this type of data that will allow us essentially to account for uncertainty. And then later on, and this is the second project, the ongoing project with dusting, we sort of went back and revisited this idea of, you know, this averaging and tried to model the signal at the trial level. Okay, but so my first project is about average ERP waveform. About average ERP waveform. And we are interested in estimating a curve and particular features of the curve. So statisticians have a number of tools that they can use to estimate curves. And Bayesian statisticians in particular oftentimes think about Gaussian processes. So a Gaussian process is a collection of random variables such that any finite subset has a joint Gaussian distribution. And typically, a Gaussian process is specified. And typically, a Gaussian process is specified by a mean function and a covariance function or a Kernel function that allows to model essentially correlation among different time points for us. The choice of the kernel is important and there are many kernels available. Today I will be working specifically with squared exponential kernel because they just, you know, they seem to be sufficient here for what we are trying to do. Efficient here for what we are trying to do. In the plots on the slide, you see some realizations from Gaussian processes with squared exponential kernel, and you sort of see the level of smoothness and the characteristics of the curves. They seem to resemble quite a bit certainly the average ERP waveform. So, in our exploration, we sort of read the squared exponential career were sufficient. But in the paper, there is also some exploration of. Is also some exploration of other types of kernels like the maternal. The square differential kernel is parameterized by two parameters. In particular, the smoothness parameter H allows to obtain realizations of different level of smoothness. So with this tool, the idea for us was: okay, we just modeled the data, the average ERP waveform as a signal plus. As a signal plus noise type of model. So given YI, the average ERP time series on a single subject, we assume a signal plus noise model where the signal is modeled as a GP, as a Gaussian process, and the noise is a simple white noise, which seems to work okay, at least for the modeling of average ERP, which again are rather smooth. The interest here is in estimating the function. In estimating the function f, and I will call this function the underlining structural signal. But as was mentioned before, we are also interested in specific characteristics of the signal, specific components, and in particular, peaks and valleys with the corresponding latencies. And so this means that given the function f, purpose of the estimate. The purpose of the estimate is to locate the stationary points of the signal, meaning the points where the derivative is equal to zero. So, in our model, we impose that the function f has zero derivative at a number m of points or m latencies. And we envision that these latencies, these points correspond to the unERP characteristic component. ERP characteristic components that we are interested in. And of course, M is an unknown quantity here. Now, working with GP is particularly useful when you deal with the derivatives because the derivative process of a Gaussian process is a Gaussian process itself. There is some literature on this. In particular, we found the paper by Wongenberger from 2006. Berger from 2016, where they had defined this derivative constraint Gaussian process. They essentially show how you can derive the mean function and the kernel for this derivative GP. And so we've sort of readapted here their construction by imposing specifically that the derivative, there are capital M points at which the derivative is equal to zero and deriving the corresponding derivative Gp with mean function and covariance Kerler. function and covariance calculator. So that's nice because it's a nice framework. Now we can do inference, we can easily write down the likelihood. So within the Bayesian paradigm, the next step is to complete the model by assuming priors on the unknown parameters. And here our additional unknown parameters are the variance of the error theorem and of course the points, right? The latences t0, t1, t1, t0. And here is where I think the construction became interesting for us because initially when approaching the problem, we were looking at having perhaps a prior on the unknown m number of stationary points and then priors on T1 Tm. And this, from a Bayesian point of view in particular, is computationally quite challenging because. Computationally quite challenging because we were looking at space of varying dimensionality, and therefore, if you think about MCMC, you have to think about reversible jump type of schemes, and those are very difficult to implement and they never mix that well. But by looking more closely at the construction, we sort of realized something very interesting. And it was that indeed the only thing that we needed to do a priori was to impose. Priori was to impose that there was at least one stationary point T and have a prior, so a univariate prior on a univariate parameter T, and then the posterior would be naturally multimodal. And we also were able to derive some asymptotic results and to prove that the number of modes of the posterior distribution would correspond, posterior distribution of T would correspond to B. Would correspond to big M, the number of stationary points of the unknown function f. So for this part, my young colleague here at Rice, Mang Li, was able to derive a very, very interesting theoretical framework together with a PhD student here at Rice, essentially, as I was saying, showing that the posterior distribution of T tends to a mixture of Gaussian distributions with M composed. Distributions with M components, where M is the number of stationary points of the unknown function f. And they were also able to derive asymptotic results on interval, confidence intervals, essentially. So in theory, we could have even used this sort of approximate posterior to do inference to calculate also approximate credible intervals. But we also explored the model from more an exploratory point of view by implementing. View by implementing a kind of a simple sampling algorithm from the posterior. And we decided to use a Monte Carlo EM. So Monte Carlo EM to draw sample from the actual posterior. And the results that I'm going to show you today are based on this Monte Carlo EM. And so it's easy to see that the posterior indeed turns out to be multimotal. Just to give you, just say two words about the Monte Carlo EM, just for maybe the Bayesians in the room. So, the idea here, we use the framework of Bay and Gauche, where the idea is to essentially use the M step to learn about the hyperparameters of the GP. So that basically there is no tuning here. Besides the prior on sigma squared, there is no other tuning that we do here. We learn all the That we do here, we learn all the parameters, including the hyperparameters of the GP. And so it's a stochastic EM where the stochastic E-step looks at sampling from the posterior of T via metropolis estimates algorithm and sampling from the posterior of sigma squared via its full conditional. And then at the M step, we update the I. Update the hyperparameters of the GP by margin by maximizing the marginal likelihood. It's an iterative process and it would stop when we have met a certain tolerance level. So it's relatively easy to implement and quite fast. Okay, so and it produces samples on teak, so samples from the posterior. On T, so samples from the posterior distribution on T, estimates of the hyperparameters of the DGP, estimates on sigma squared, and of course, also we can produce estimates of the unknown, the structural F by sampling from the GP itself. And then once we have the posterior distribution of T, the inference is done naturally by looking at the HPD region, which is typically high. Region, which is typically how we calculate credible intervals or confidence intervals with the multimodal distribution. So, here we're going to have again a posterior distribution which will be naturally multimodal. And if you look at the 95% HPD region, the different segments of the region will the number of segments of the region will be an estimate of the number of stationary points and then the map estimate in each. The map estimate in each one of these individual segments will be an estimate of the latency of that particular stationary point. And then, as I said, we can also provide estimates of the underlying structural F. So, I'm going to skip the simulation because I'm running a little bit too late. I'll just show you some results on the application. So, then I have a few minutes for the other process. I have a few minutes for the other project as well. Here we got data from a collaborator of mine, Rice, Simon Fischerbaum, from the Psychological Sciences Department. Simon is interested in speech recognition, so he does very complex experiments where he has people listening to words and listening and repeating words back. And I'm not going to describe the Not going to describe the experiment fully because, as I said, it's rather complicated. But the idea is that subjects listen to spoken words, and that these words are some words would make sense. They would be real words that with a meaning, and other words would be words that don't exist in the vocabulary, but that sound like familiar words. You have an example here: date and tate. Date is a true word. Date is a true word, it has a meaning. Tate, it's not, but it sounds similar to date, right? And so subjects hear these words spoken to them, and then they have to say, they have to indicate which endpoint the spoken word was closest to. There are several of these conditions. We have analyzed both all conditions and ones, and then different conditions individually. Conditions individually. But I'm only going to show you just one result of one snapshot so that you can understand what type of inference we get from our model. So we will be looking at two subgroups of subjects, 11 young adults and 11 older adults. And we have 200 trials for each subject, which we're going to average across different conditions and also across. Different conditions and also across six electrodes that our collaborator indicated as those of interest. So we're going to have within subject and also within condition ERP waveform. I think I have more. Yeah. Here on the slide, you see snapshots of the data. So where it says all trials, it actually means that the data are the subject level data that have been averaged across all trials. Cross all trials. So that's the meaning of the label. It's every nice, it's a little bit confusing. So, in this plot here, the first one, we have all subjects, young and elderly, and all the subject level data. And the black line is the grand average, right? It's by averaging all curves across all subjects. In the second plot, you see again all the data, and then you see two averages, one by young and one by elderly. You can sort of One by elderly. You can sort of see, right, that there are some differences in, like, you know, in particular, here, Simon is interested in what happens around 100 milliseconds and around 100 and around 200 milliseconds. So this depth here and this peak here. You can sort of see that there are some differences between the two groups. So what we did with our model, so first of all, we rewrote our model to handle multiple subjects because the way that I introduced Multiple subjects, because the way that I introduced it to you was for a single subject. And then we applied it to the two groups of subjects separately. So we analyzed the young adult by themselves and the elderly adults by themselves. And this is just one plot out of the results, and it's the plot of the posterior distribution of T, so the stationary of the latencies of the stationary points for all subjects, the 11 subjects. All subjects, the 11 subjects in the older group, and the 11 subjects in the young group. And you can see several things from these plots. First of all, there is a substantial subject level variability, which would be lost just by looking at these two curves here, the blue and the red. So we retain a lot of information by modeling the data at the subject level. Second, we can sort of use this information. We can sort of use this inference to maybe flag some of the subjects for our collaborators as maybe possible outliers or subjects where maybe the processing was not done properly. So certainly subjects that you may be looking at at the data a little bit more. And then at the group level, we can see things like, for example, the older group does have Um, later latencies than the young group, and this is you know, a well-known fact in the literature and something that our collaborator was expecting. So, this is just, as I said, a snapshot from the results and with few of the findings that our collaborative thought were interesting. There is much more in the paper, and the paper also has lots of simulations and comparison with other men. And comparison with other methods, and so on, and so forth. The paper is in press in biometrics. And I mentioned that we also have a paper where we studied the theoretical properties of the posterior distribution, and that has been revised for JASA. Okay, so now I have a very little time to sort of maybe tell you a little bit about what we are doing at trial, a single trial level. So, as I said, Trial level. So, as I said, when we approached this type of problems, we decided to work with the average data, but then you know, later on, it started to make sense to revisit that part of the processing as well, and maybe think about maybe translating this model or a similar model at trial level, so without doing the average across trials. And so, this is sort of like the follow-up project that I have worked on with Dustin Plu. With dust in dust in Pluto. At trial level, things are way more complicated, right? Because there is a lot of variability across trials. And something that jumps to the eye as well is that the structure of the noise is way beyond a standard normal Gaussian white noise that we had used in the previous project. So, just to give you an idea, these are simulated data. These are simulated data, simulated from some to resemble some of the characteristics that we would see in the real data. And so those are simulated single trial ERPs. And you can see things like, you know, in between across different trials, there is amplitude scaling and there is latency shift, right? And indeed, this data have been simulated from a true F signal like the Signal like the blue one up here in the figure, just by adding scaling in amplitude, scales in amplitude, and shift in latency, and then by adding a structured noise that I'm going to talk to you about a little bit more later. And you can also see what happens when you take an average, right, across these trials. The empirical here, this is the average of all data across these multiple simulated trials. Multiple simulated trials, you can see how you smooth out a lot of the interesting characteristics of the data. So, so yeah, so it seems that you know it's worth thinking about modeling the data at trial level. But again, that structure of signal plus noise has to be way more complicated. And so we revised the structure by essentially looking at a model of this type, where for each trial we introduced beta and tau, which are trial-specific amplitudes. Tryla-specific amplitude and latencies. And then modeling the data as signal plus noise, where the signal now is beta F of T minus tau. So again, taking into account scaling in amplitude and shift in latency, plus an error term that became an autoregressive model that, of course, incorporates the white noise. And this choice of noise was done based on some other. Was done based on some other literature that we have seen where people had attempted to model trial-level data by using autoregressive structures. So, this is now the model. At this stage, we have not attempted to incorporate the change points yet. We might be doing it later on, but at this point, we were only looking at a signal plus noise type of modeling and estimating essentially B times F as an estimate of the denoised data. Denoised data. I'm not going to bore you with any details also because I'm out of time. So the inferency is much more complicated choosing priors as well because these parameters have, there are some identifiability issues that we need to take into account. But so eventually we were able to do everything and implement an MCMC sampling via Gibbs sampler with Metropolis ASTINX. And again, we have now an MCMC scheme that would produce. Scheme that would produce trial-specific signals or trial-specific estimated signals as well as estimates of trial-specific amplitudes and latencies. And I'll just show you, just one example on a different application. This is now with a different collaborator, so I'm showing you different data. Here, in this experiment, subjects are Experiment subjects are looking at pictures, and those pictures have different levels of arousal: high arousal, low arousal, and neutral. Those are individuals that are involved in a clinical trial of smoking sensation at the MDN test. So, arousal is defined in kind of loose terms here. So, this is for one subject. Again, this is the trial-level data. Level data. And this on the right-hand side would be the average ERP averaged by condition, by high, low, and neutral. And the gray area are a bootstrap confidence, are a boost up confidence, 90% bootstrap confidence bands around the curve. So you can see that there are some differences between the three conditions, but it's not that easy to separate, right? Easy to separate, right? The three different conditions. So, what we did, we applied our method again, this our model, signal plus noise, to the trial-level data for this individual subject, and we derived the estimate of the, what we call the denoised signal, beta times F. And this is now the denoised data on the left side, denoised with our model, and on the right-hand side will be the inference by category. Inference by category. And I've kept the same axis here on the y-axis, so you cannot see very well, but there are now credible intervals, credible bands around the level, the category estimated means. Okay, I'll stop here because I don't want to use too much of Damla's time. There is a lot more in the paper that we have. More in the paper that we have. We just submitted the manuscript. I think eventually we will post it in the archive or make it available. But we just completed it and there is a lot more in this paper also in terms of simulations and so forth. So acknowledgements. The first project was done by Chan Han, Yu, and Mang Li, and Zejian Liu, together with Simon Fisherbaun and Colleen Nu Nu at Rice University, and this second project. University and this second project was done by myself with Dastin Pluto and Francesco Versace at the MD Anderson. Thank you very much.