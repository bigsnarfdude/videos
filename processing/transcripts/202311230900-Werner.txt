Again, thanks for the invitation. And we even got snow, right? So what I'm talking about is a random approximation of convex bodies in the host of distance. And let me first start to fix notation. So k will be a convex body in Rd always. And we want to approximate k with a random polytop. A random polytop is the convex hull, always denoted by Kn. And the square brackets mean convex hull of randomly chosen points xi, randomly chosen either in k or mostly what we will do in this talk, chosen on the boundary of k with respect to a probability measure. So then, of course. Then of course what one wants to know is how does one move forward? Like so. So one wants to know how good is this approximation. So there are several ways to quantify this question. That is to measure how good the approximation is. For instance, one example would be to measure the error of approximation in the symmetry. Of approximation in the symmetric difference metric, which is just the volume of the symmetric difference. And in our case, as this random polytope is a subset of the body, this random, this symmetric difference metric just becomes the volume of the difference because one body is containing the other. So it simplifies this way. And the other metric that we are going to look at, and that's actually the one I will be. And that's actually the one I'll be mostly concerned with in this talk is the Hausdorff distance. So I recall quickly the Hausdorff distance. You look at the maximum of these two expressions. That is how close are you from the one set to the other and from the other set to the one. And again, as the one body, this Kn is contained in the convex body K, this second expression becomes zero, and what you are left with. zero, and what you are left with is to look at the distance from actually, again, it is now enough just to look at taking the points on the boundary of k to this random polytrop. So that's what it boils down to in that situation, looking at the host-drop distance. And while there are many results on the symmetric difference metric on the approximation of convex bodies by polytropes, there is not so much available in the house drop. Much available in the house toward distance. And I'll mention the few results that are available. So, okay, what we are aiming towards is then to compute the expected, say, symmetric difference metric, that is the average of all possible choices of random polytopes measured in the symmetric difference metric, or the corresponding expression in the historic distance. That's what we wanted to deal with. Let me show on a picture how the situation looks like. So, on the left here, we look at the expected symmetric difference metric, and what we will see, okay, so here is our convex body K, here is our random polytope, and what we see in the symmetric difference metric, we pick up volume everywhere on the boundary. Whereas when we look at the Hausdorff distance, The Hausdorff distance, there is one point on the boundary which is the decisive point. So, of course, in both situations, we have that as the number of chosen points n goes to infinity, both expressions will go to zero. But what we are interested in is to find out the speed of convergence. It will turn out that there are. It will turn out that there are basically two different regimes: the regime of a smooth convex body and the regime of a polygon. So, let me illustrate that also again on a quick picture. So, if we are in a smooth situation, then the picture looks like so. That is, every point on the boundary kind of counts. Whereas, if we are in the situation of a polytop, we see, well, we bullpeak the polytop. Well, we will keep the points on the boundary. The only points on the boundary that matter for the host of or actually for both the host of distance and the symmetric difference metric is what happens at a vortex. So that's the difference between the two regimes. And here I have just zoomed in to this situation up here. Okay, let's first look at the symmetric difference metric and let's see what is known there. And let's see what is known there. So we'll have a result by Matthias Eitzner and by Karsner and myself, which says that if K satisfies mild smoothly assumptions and are not smoothness assumptions, I'm not going into details there, then the expected symmetric difference metric between a convex body k and a random polytop where the points are chosen on the boundary with respect to the normalized probability measure. This behaves. probability measure. This behaves like after, well, this is how the dependence on the number of chosen points comes in. It's n to the 2 over d minus 1. As the number of chosen points n goes to infinity, this behaves like a constant C D, depending only on the dimension. And we know exactly what this constant is. I have not written it. It is very important. But we know exactly what it is, times the affine surface area of k raised to the power d plus 1 over d minus 1. 1 over d minus 1. So affine surface error I have written here, it's the integral over the boundary of the body, the Gauss-Conega curvature, or the generalized Gauss-Conega curvature, because actually the body K need not necessarily be C2. So we have to put here a generalized notion of Gauss-Conical curvature. So this integral over the boundary of take Gauss'Conical curvature raised to the right power 1 over d. Which are raised to the right power 1 over d plus 1, and you integrate with respect to the usual surface area measure. This is the affine surface area, and it's normal that such curvature conditions come in if you talk about approximation, because this curvature kind of determines where you catch volume or where you catch the decisive point for the household distance. Okay, so that is the smooth case in the symmetric difference metric, that is the smooth regime. Now, let's go to the regime of polytops. There is a result by Matthias and Cars and myself, which says if we have a simple polytop, that is at every vertex you have the phases that meet, like for instance for the Q. So, simple polytope, for instance, the Q, that would be. That would be the situation here. So if we have such a polytope, then the expected symmetric difference metric between this simple polytope K and a random polytope Kn times n to the d over d minus 1 as the number of chosen points goes to infinity behaves like a constant that we have called c delta of k. That we have called C delta of K. And this constant depends only on K. And we didn't trace it exactly when we wrote the paper, but it supposedly that depends only on K. So here in the symmetric difference metric, with the points chosen on the boundary of K, we will see that we have this behavior. In the smooth case, the one regime, the expected symmetric difference metric behaves like difference metric behaves like n to the 1 over n to the 2 over d minus 1 and in the polytropic case behaves like 1 over d to the d minus 1 and we see that okay this difference this expression is much bigger than the other one which of course comes from the fact that in the smooth case as I already said you basically I already said you basically catch volume everywhere on the population. Okay? Now let's move to the host of distance. So as I already said, there are not so many results available as in the symmetric difference metric. In the symmetric difference metric and other like volume kind of differences have been studied extensively. There are results in Extensively. There are results in other intrinsic volumes, not just volume, measuring how good the approximation is, but not many results are available in the Hausdorff distance. In particular, we want a result of this type. Number of chosen points goes to infinity. Do we know exactly the limiting behavior of this expression here that I have written? So what is known is there are up. What is known is there are upper bounds for convergence almost surely due to Duncan and Walter, and there are results by Bunel. And I'm not going to mention them, I'll just tell you if you want to look it up. But again, what we'll be dealing with is two regimes, namely the regime of smooth bodies and the regime of polytropes. So again, these two regimes and they will be fundamentally different. So let's Different. So let's take a look at the smooth regime first. And what is available there is a theorem due to Glass-Auer and Schneider, which says that if we have a convex body in Rd with a strong smoothness assumption, so K has boundary C three plus, that is boundary is three times continuously differentiable and has strictly positive gauss curvature on every boundary point. Commiture on every boundary point. So if K is like so, and we have a probability measure P that has a density H with respect to the usual surface area measure. With respect to this probability measure, we choose the points at random or on the boundary of K to get our random point of Kn. Then, as the number of chosen points goes to infinity, the Hausdorff distance between the body k. Between the body k and this random polytroph times. Now, here comes the dependence of the number of chosen points. It is n over log n to the 2 over d minus 1. This converges in probability to that expression here on the right-hand side. So it's what we see again is, of course, that the function h comes in, but we see again how the dependence on curvature comes in, which is natural because you do. Comes in, which is natural because you do expect, as I said, that curvature should come in because that tells you where you catch volume or where you catch distance. Good. Now from that, one can actually deduce fairly quickly a result of the type that I told you we wanted to look at, namely looking at the expected Hausdorff distance of k and the random polyglot, namely Polyclub, namely using actually an upper bound on this expression by DÃ¼nke and Alter, which was proved later, and then which allows us then to use dominated convergence theorem. We can look at this expression here, the expected Hausdorff distance, and interchange the integration and limit with the dominated convergence theorem, and then use this. And then use this to get this convergence here as the number of points goes to infinity in expectation. So in the smooth situation, we have something of the nature that we wanted to find out. But let me mention again this result by Glassauer and Schneider. So I have written it, I'll just write it again. What we have been looking at is actually trying to improve on the convergence that we have here, which was established by Glaza Eichnader, which was convergence in probability. And with BATEC, we could prove that actually one can improve on the convergence and not just... Not just have convergence in probability, but have convergence of the exact same expression here to the exact same value in LP for P equals what I already showed, but for all P bigger equal than 1. So that is what we know Hausdorff distance then in the smooth regime. Now let's go to the Hausdorff distance in the other regime, which is the regime. In the other regime, which is the regime of polytops. So we have already seen that if we are looking at random polytops, that is the one that is contained in the other, then it's enough to look at what's happening with the distance of a point picked on the boundary to this random polytope. And actually, it simplifies even further when we are looking. Even further, when we are looking, or when our body case is a polytop, the only thing, and I've mentioned that several times already, also, the only random points in that context that count are the vertices. So what we'll have to look at, if we want to decide on the Hausdorff distance between a randomly chosen polytope inside or in this polytope P or K I called it, where the points are chosen on the boundary, is this expression here that we are dealing with. Expression here that we are dealing with. And from now on, I will always call V1 up to Vm the vertices of the polysub that we are looking at. Okay, before I go to the situation where we pick the points on the boundary of the polytope, I want to mention a result that is known in this context where the points are peaked inside. So Insights. So then I'll denote. So we look now at random polytope where we pick uniformly, IID, uniformly inside K random points YI. And we look at the corresponding random polytope. And if we are picking the points inside K, I'll call the random polytope Kn with a little With a little in on top to indicate where you are inside. Then there is a result by Grecker, Hessing, and Bingham, which says this, namely, well it's a result in a dimension 2. So we look at a polygon in R2 with M vertices, then they could show that as the number of chosen points inside now goes to infinity, then the host of distance. Then the Hausdorff distance between this polytop and this now random polytope that is inside times n to the one-half, so probability that this host of this is more or equal than x over n to the one-half as n goes to infinity is equal to a product where you take product, i goes from 1 to m, the number of vertices is m that you have. This m that you have. And then comes an expression 1 minus pi of x. And I don't want to say what the pi is, it's rather complicated, or it's sufficiently complicated, but it's an expression that depends on the i interior angle. And I'm not going to mention it here because we'll see a similar phenomenon happening when we go to a situation that I'll talk about next, where we pick the points of the boundary. And then I'll say more about such expression, such expression. Such expression that appears. Good. So if you look back, if you think back of the result that I mentioned, that was a result where probability was computed as the number of points goes to infinity. What we really want is a result of this nature. As I said, we want to look at the expected host of distance of k and the random particle. Now, the only thing that we could find is an insight. That we could find is an inside result, which is not giving the exact asymptotics, but that's a result by Varani, which says that: okay, if k is a polytope and we look at a random polytop where the points are chosen inside, then the expected host of difference is bounded above, behaves like 1 over n to the 1 over d Bodelo constant. So that's the only result that we could. Only result that we could find. And from this, one can easily show along the same lines as Barani showed that actually, when we pick, so here the points were picked inside the body, when we now pick the points on the boundary, one can show a similar result. The expected host of distance behaves like a modulo constant, like n to the 1 over d minus 1. But actually, what, okay. Okay, so let me take resume and show what's happening in these two different regimes. Because as I said, behavior is different in the two different regimes. So in the regime of the smooth case, the expected host of distance, looking at now the point chosen on the boundary of K, behaves like a log n over n. like log n over n to the 2 over d minus 1 and in the polytrop case it behaves like n 1 over n to the 1 over d minus 1. So we see that actually it's the other way around then in the symmetric difference situation here this number is much smaller than that number. So let me recall what we have in the second regime because we have We have in the second regime because we have seen in the first regime that is just this result that came from as a consequence of the Glazer-Auschney result. We could not only see how the behavior is in the number of chosen points, but we knew the exact asymptotics as a consequence of the class-Auschneider result. Here, so far, we have only this dependence, but we don't know the exact asymptotic constant. Exact asymptotic constant, and that's what we want to know. What is the exact asymptote? So that's what we did with Josha Povno and Carsten and Matthias von Leibniz. Again, for the moment in R2. So if k is a polygon in R2 with m vertices, then n times, so that's, we know that this is the right order. We know that this is the right order. So n times the expect right dependence on the number of chosen points. So n times the expected host of distance of k and the random polytope kn. So this quantity here on the left, as the number of chosen points goes to infinity, it behaves like, so this is just the surface area of the polytope. And then comes this double integral, so an outer integral and an inner integral. So let's look at the inner integral. So it involves again. Inner interval. So it involves again, you take a product of everything that happens at the vertices. That is the product. J goes from 1 to m. M is the number of vertices of the polygon. And then comes this kind of a bit involved expression. So this inner integral, which involves a function h that depends on the interior angles. And you integrate exponential. You integrate an exponential function involving this function h, and you integrate from Lj to infinity. So, what is Lj? First, and then I'll say something about the H. So, Lj is 1 if the interior angle is smaller than pi over 2, and it is 1 over sine alpha j if the interior angle is bigger or equal than pi over 2. That's easy now. J. That appears. This is now j that appears here in this integral. Now let me explain what this function h is. So if alpha is smaller than pi over 2 and y is in the interval 1 over cos alpha, or if alpha is bigger or equal than pi over 2 and y is bigger than 1 over sine alpha, then this function h looks like so. So it involves the sine and the cosine of the angle. Of the angle. Otherwise, if it's not in these two situations up there, this function h is equal to 1. So that doesn't explain much, or not so much. So let's see that actually this function h has a geometric meaning. So what's this geometric meaning of the function h? So we are at a vertex of our polygon at a vertex. Our colleague going at a vertex Vi with interior angle alpha. Then, what we want to do is we want to pick points on the boundary of the polygon to get our random polygon and we want to see how far are we possibly from this vertex with our random polygon. So we see if I draw this ball with radius x around that vertex or with center at this vertex vi. At this vertex di, then we are x away from this vertex if we choose all our points on the polygon below a tangent line that touches this wall. So we fix to be or we want to look or explore all these possible tangent lines. Tangent lines. So, what do we do? We'll go x times y away from the vertex and then look at the tangent line that touches this ball. This tangent line will intersect the other edge at exactly this function page. So, that's the geometric meaning of this function page. Okay? So here I have written. Here, I have written the expression again, and I have tried to explain where everything comes from. Now, it may be a bit unclear what's really, you know, the size of this expression here. And okay, I should also say I didn't give you when I mentioned this result by Frank Hessian and co-authors, I didn't say. Co-authors, I didn't say, so they had a similar product of, I say product i goes from 1 to m, 1 minus pi i of x, and their expression looks like similar to this expression. Yeah. So one wants to understand a bit better, you know, what is really the size of this expression here. So let's look at the special case when all the alpha i's are bigger equal than i over 2. Over 2. Then we have this corollary here. The left-hand side, which is this limit, n times the expected Causal difference between K and this random polytope, behaves like n times this expression, behaves like a constant, modulo, behaves like a constant. And what is this constant? It's this expression here. So incomes aside from the comes aside from the surface area of the body this sum minus 1 to the k plus 1 and then you look at all choices of kicking k indices out of your index set m and sum up these expressions involving this 1 over sine alpha i. So it is what it is. Let's see why this is so. Alright? So, this is our expression again. That's what we get in the theorem. And now we want to look at all alpha i are bigger or equal than pi over 2 for all i. And that means that the L i is 1 over sine alpha i for all i. That means that we integrate our y for the inner integral, which is the integration over y. We'll have y is bigger equal than 1 over sine alpha. is bigger equal than 1 over sine alpha i. And that then simply means, so I have written our function h again, so here is our function h, it simply means that, okay, if you look here, sine squared alpha i, y squared will be bigger equal than one, so the denominator is positive, and here I have replaced already the y by one over sine alpha i. And actually the numerator will also be bigger or equal than zero. So our function h is bigger or equal than zero. h is bigger or equal than zero. That's what we need. To now estimate the inner integral, the y integral, which is this integral. So we estimate this from above by, well, I can replace, I can, because the function h is bigger equal than 0, I can take it away and we'll get simply to look at the interval e to the minus xy, which we can integrate and we'll get this. So that is the inner integral. Now we look at the other expression that we have to look at, this product of the pi. So nothing happens. So I have written everything again. We end up here. Nothing has happened. And now we'll observe that putting what we found here in our integral. In our integral expression, we'll get that. And now, what you'll do is you simplify, simply multiply out this product. So, multiplying out this product makes you end up with that expression. And then you'll do the outer integration. That is the integration with respect to Uh now we still have to do the auto integration. Um I've got to write that already. Yeah, so we'll end up here. So which is the result? So yes. So we end up here. Again, while this looks a little bit more friendly than the original expression, Friendly than the original expression, we do not get maybe a complete idea what's the size of this right-hand side now. So let's look at a second corollary where we take actually a regular polygon in R2, a regular M gon, and then one sees that actually this expression here, this expression up here, simplifies and it simply becomes log m over m. Simply becomes log m over m. So let's quickly see how this works. If all the angles alpha i, if we have a regular polygon, then all the interior angles alpha i are the same. So this expression here becomes the sum k goes from 1 to m, 1 minus k to the k plus 1. So nothing changed here. But now all the angles are the same, so we'll get sine alpha. So we'll get sine alpha, we'll have k many, so over k, and we'll look at all the possible choices to choose k out of many. So we'll come up with this expression here. And now we observe that, okay, how does it look like? So we'll have our outer angle, so here we will have i over 2, and here we'll have, so we have our interior. And here we'll have so we have our interior angle alpha, we'll have our outer angle beta, and we'll have that 2 of pi is equal to m times beta. So we'll get that alpha which is equal to pi minus beta. So we'll get this is pi minus here we'll get beta is equal to 2 pi over n. So we'll get pi Get chi 1 minus 2 over a. So that's what I have written here. So sine alpha is the sine of this thing here when we are in the regular situation. And if m is large enough, this behaves like 2 pi over n. So our expression now simplifies, our expression which was that now simplifies to this. This up here, and now one either computes directly that this expression up here is that sum, or one looks it up. So we looked it up in this book of Katstein-Richmik table of series, integrals, and products, and that told us that this sum up here is actually that sum. Here is actually that sum, and then of course you integrate this, or you see that this brings in the log n program. And I think I stop here. Thank you. Right, yeah.