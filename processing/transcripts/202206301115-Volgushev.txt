Yeah, so first of all, this is working. I would also like to thank the organizers for inviting me here and for putting together this very nice program. I think it was I learned quite a lot already about Xtremes, but also about the other things. And so, yeah, thanks to everybody. The stuff that I'm going to present now is joint work with Sebastian Engerke, and part of it is also joint work with Mikhaila Lancet, who is Joint work with Michaela Lancet, who is a PhD student at the University of Toronto, who is finishing up now. And the general topic of the talk is graphical structure learning for extremes. So I don't think that the extremes part needs a lot of introduction, but just to briefly make sure that we are kind of on the same page. So I'll be talking about extremes in the sense of multivariate Parito distributions and in the sense of Distributions and in the sense of threshold exceedances. So, we've already seen this kind of limiting relations quite a couple of times, and we've also already heard about the notion of multivariate regular variation. But I just want to briefly recap. So, let's assume that we have some random vector x1 to xd and that we are interested in making statements about properties and conditional dependencies in observations from that random vector, but conditioning on the information. But conditioning on the information that at least one observation from that random vector was large. And large here means that we rescale that observation by its marginal distribution, and then we look at at least one component exceeding the marginal 1 minus q quantile. So you could think of this, for instance, as a vector of temperature data. And then on a given day, you know that at least in one location it was very hot. And the question is, what's the distribution of this vector given the side information? Side information. Now, under the assumption of multivariate regular variation, one can easily show that if you look at the full vector that is marginally transformed, and you condition on this event, and this sort of gives you a sequence of probability distributions, and this sequence of probability distributions stabilizes as you look at observations that are more and more extreme. And possible limits that can arise from this limiting distribution are called multivariate parity. Multivariate Pareto distributions. By the nature of this limit, they live on this kind of funny L-shaped space that's over here. And one of the basic properties that multivariate Parito distributions have is this kind of homogeneity property. And one can actually show that any distribution that lives on this L-space and is homogeneous and satisfies certain marginal standardizations will arise as a possible limit over here. Possible limit over here. So the multivariate Parity distributions are a very, very natural class of distributions to model this type of extreme events. And so what I'm interested in in this talk is essentially graphical modeling for multivariate Paris distributions. And that also actually ties back very nicely to what Yevgeny was presenting just now, because if you look at those multivariate Paris distributions on the set L. Distributions on the set L. And then you use homogeneity to kind of start to get closer and closer to zero. Then what you'll get is exactly this type of measure that is exploding at zero. So in a sense, what I'm going to present is in some ways a special case of what Evgeny was presenting just now. Okay, now if you talk about graphical models, then I want to down the roads define a notion of conditional independence. And for that, And for that, I will need certain random vectors y that are derived from this random vector ym. And essentially, it's very simple: you take the random vector y on this L-shaped space, and then you condition on the m's coordinate being greater than one. And in that case, this L-shaped space becomes just a nice rectangle. And on that rectangle, you can talk, you can define a random vector y as derived from the random vector ym. The random vector ym. Now, to simplify some of the presentation that I'm going to give, I will assume that the original random vector y does not put any mass on the faces, which means that there's no conditional independencies anywhere, and it's even stronger. So there's no kind of no degeneracies. Some of the results that I'm going to present continue to hold with all this assumption, some don't, but I don't really want to go into those technical details here. Here. And next, I would also like to introduce a new summary statistic for extremal dependence. And I will argue in this talk that this summary statistic is actually a very natural summary statistic if you're interested in graphical modeling that really has some sort of conditional independence interpretation. Okay, so the summary statistic is something that I call extremal variogram matrix, and essentially as an And essentially, as the name says, it's a matrix. So you can look at it as some sort of correlation matrix in the Gaussian world. And it depends, of course, on all the coordinates i and j. And it's also always relative to a coordinate m. Okay, so we condition on one component being greater than one. And then we can compute this matrix. So it is a summary statistic for external dependence. And as I will show, it is fairly useful for. It is fairly useful for learning graphical structures. Okay, now, one classical example of a multivariate Parito distribution that many are familiar with is, of course, the Hussler-Rice distribution. And probably the simplest way to say how a Hussler-Rice distribution arises is just to represent it as a product of an independent Pareto and a degenerate multivariate normal distribution. And it turns out that the variance covariance meets. That the variance-covariance matrix of this degenerate multivariate normal distribution can be expressed in terms of this extremal variogram matrix gamma. And what is also nice is that for those Hussularis parato distributions, it turns out that in fact gamma does not depend on this component M. So you can condition on different components m and you get the same excuse me, the same matrix gamma. So that's independent of m. Matrix gamma. So that's independent of M. Okay, so that's a brief run through multivariate Parito distributions. Now, the next thing that I would like to introduce is what exactly I mean by graphical modeling. So in general, I will talk about graphical models on undirected graphs, which is in contrast to a lot of the talks that we have seen. So in general, a graphical model on an undirected graph means that means that we have a graph where each edge, sorry, each node in the graph just corresponds to an entry of our random vector. And the presence or absence of edges in these graphs tells us something about conditional independencies or about certain probabilistic properties of this random vector. So there are a couple of properties that are popular, and probably the most basic and the most well-known one. The most well-known one is the Pairwise Markov property, which states that if there is no edge between two nodes in that graph, then those two entries in that vector are conditionally independent given all the other nodes in the graph. And then there's also a stronger version of that, which is called the global Markov property, which gives us more conditional independence relations by saying that if we have By saying that if we have three subsets A, B, and C of nodes in that vector, then the observations in the subset A are independent of the observations in B, given all the observations in C, if the observations in C separate A from B in the graph. So, for instance, in this example up here, we could say that X. We could say that x1 is independent of x3 and x4 given x2, because if you remove x2, then there's no path between x1, x3, and x3, x4. And finally, there's also a notion that is sort of related to the graph structure and to density factorization that I don't really want to go into the detail of. But there is a very classical result called the Hemmers-Slay-Clifford theorem, and essentially it's stating that if you have Essentially, it's stating that if you have a positive density of the vector x with respect to some product measure, then all of those three notions are equivalent. So we have conditional independencies and we also have density factorization. And density factorization is, for instance, very useful for simulating from models, while conditional independencies is really useful for interpreting the structure, the dependent structure of a random vector. Okay, now that. Okay, now that's for classical conditional independence notions. And now, what Sebastian and Adrian did in their 2020 paper was essentially they defined the notion of undirected graphical models for multivariate parity distributions. And they introduced a notion of conditional independence, which was already mentioned by Ergini, so I'm not going to go into the details of that. And they proved that that notion of conditional independence is very natural. Conditional independence is very natural for those multivariate Parito distributions in the sense that it leads to things like density factorization, it leads to things such as very parsimonious models for those multivariate distributions, and so on. Okay, and this is done, so you might wonder what I'm talking about here. And the main thing that I want to focus on in this talk is the following. Talk is the following problem. Assume that I give you observations x1 to xn, and those observations are in the domain of attraction of a multivariate parity distribution y. Given that, how do I actually learn this underlying graphical structure? Okay, because a paper by Sebastian and Adria is mainly on how do we construct models given a graphical structure. Now, I'm asking how do we learn the graphical structure given data? Okay, and so that is essentially what the remaining part. So, that is essentially what the remaining part of the talk will be about. Now, just a side notion: I've introduced a side remark, I've introduced a couple of notions of graphical models on the previous slide. And so, in this talk, I will understand graphical models in the sense of the global Markov property relative to the graph G of the stranding vector Ym for Ym and thus Y homogeneity for all M. 84 all n. Okay, and if there's density, then that also is equivalent to density factorization and to the other Markov property. Okay, now I'm going to talk about learning two different types of graphs. And the first type of graph that I'll mention is trees, which is essentially a very simple graph. Okay, so a tree is simply a graph that doesn't have, that is connected, but that doesn't have any cycles. Okay, so for instance, this thing here. So, for instance, this thing here would not be a tree, and this thing here would be a tree. And one thing that turns out to be useful for learning graphical structures on trees is the notion of a minimal spanning tree. Okay, so imagine that I give you a collection of nodes and I give you a collection of distances between any two possible nodes. Then the minimum spanning tree with respect to those distances is the unique tree. Unique tree that connects all the edges in the vertex, in the set of edges, and minimizes the distances on the edges in that tree. Okay, so again, looking at this example over here, here's three possible distances, two, three, and five. So the minimal spanning tree corresponding to this example would be this one here, because to minimize all the distances, we need to exclude all this node, but we still need everything. This node, but we still need everything. This edge, but we still need everything to be connected. So we get this tree here. Again, it's a theorem that if all the distances are distinct, then in fact, this minimal spanning tree is unique, and there's also very efficient algorithms for computing this minimal spanning tree. Now, the connection of minimal spanning trees to graphical models for multivariate Paris distributions is that if you take as distances As distances, those extremal variogram matrices that I introduced two slides before. And if you have a multivariate Paritio distribution that is a graphical model on a tree, then it turns out that this tree is uniquely identified as a minimal spanning tree corresponding to those distances, which are the extremal variables. Okay, so this means that this is really a summary statistic that is informative about the underlying graphical structure. Lying graphical structure of the multilayer parting to distribution. Okay, so that's our first result on population level. Okay, now to make use of this result on sample level, you of course have to estimate this variogram matrix. In principle, there's nothing too surprising. In principle, there's nothing too surprising going on here. We just use a plug-in estimator. It might at first not be completely obvious why this is a reasonable plug-in estimator. But if you look a bit closer than this part here, it's essentially saying that the M's component is large. And this 1 minus F hat of X Ti and 1 minus Fj hat of XTJ, this XTJ. This is just stating that this is an approximate sample version of the corresponding population version of 1 minus f of xi and 1 minus f of xj. Okay, and this is essentially telling us that we are only using extreme observations, and that allows us to estimate the extremal variogram from data that are only in the domain of. In the domain of attraction of our corresponding multi-year Power Interdistribution. Now, a couple of remarks on this. For people who have actually worked with theory for extreme value distributions, if you look at this closely, it's actually not entirely trivial to analyze this estimator, so it doesn't follow from sort of from a very classical analysis. But it is still possible to prove that it's complete. It is still possible to prove that it's consistent under very broad conditions, and in fact, Mikhail was able to prove much more. He proved non-asymportic concentration bounds for this matrix uniformly overall entry ψ j, even in the case where the dimension is growing with the sample size. Okay, so it's kind of it's also in a growing dimensional regime. Okay, and now we can use this information and we can plug it. And we can plug it into the minimal spending tree algorithm. We just replace the true unknown underlying gammas by their empirical version, and we compute the corresponding minimal spending tree. And then the result that we have is stating that under certain technical conditions, which I'll be happy to talk about online, but which might be a bit heavy given sort of the advanced time of the day, we are able to recover the underlying minimal spending tree. The underlying minimal spanning tree under a minimal separation assumption, meaning that the length of the path of the second shortest tree is longer than the length of the path on the shortest tree, relative to some sort of bias term that we get from the fact that we are only in the domain of attraction and relative to the stochastic error that we make in estimating those gammas. Okay, and essentially. Gammas. Okay, and essentially, this is a sort of complicated-looking result, but the main take-home message here is that, for instance, if you fix this minimal separation, so trees don't get closer to each other even as the dimension increases, then we are able to recover the true tree with probability going to one as long as the dimension grows almost exponentially in the number of extreme observations. In the number of extreme observations that we are using. So, as long as log d grows slower than some fixed power of k, we are able to consistently recover the tree. And we even have more qualitative bounds on those tail probabilities, but we can ignore those for now. Okay, so now just one brief data example as an illustration of this minimal spending tree. So, what we did here is we took exchange rates between 26. Between 26 currencies to British pound sterling. Then we filtered those daily log changes in its currency using some Armagalt filter to sort of make them more or less uncorrelated. And then we estimated the corresponding minimal spending tree based on the top 95% of the data. So, the results you see here. And it's interesting that from this, so we didn't put in any side knowledge about our data, but we do see a couple of interesting things. So, for instance, the East European countries are connected together. In European countries are all connected through the Euro. You also see, for instance, the Commonwealth countries, Canada, Australia, and New Zealand being. Canada, Australia, and New Zealand being fairly close together. And we also see that all those, most of the Far East countries are fairly close together over here. And we also see an interesting connection between the Swiss, between Switzerland and Japan, which apparently is related to the fact that both are kind of safe haven currencies. Okay, so a lot of this kind of makes a lot of intuitive sense, although the only thing that we gave our algorithm was the data without any domain knowledge. Without any domain number. Okay, so that's a good sign. Now, the one thing about this is that one might argue that trees are relatively restrictive types of grounds. So can we do something going beyond trees? And that's what the final part of my talk will be about. Now, if you're familiar with this whole graphical structure learning literature in the non-extreme case, then you will know that there's a Then you will know that there's essentially two types of results. Either you can learn very simple graphs under very general conditions on the distribution, or you can learn very general graphs, but only for very specific distributions and more precisely for multivariate normal distributions. Okay, so there's a very nice and very classical result that is saying that if you have a multivariate normal, then conditional independence relations in that are actually encoded. Relations in that are actually encoded in the zero pattern of the inverse of the covariance matrix. And it turns out that something similar is true for Hustler-Rice models. So in a sense, Hustler-Rice models are kind of the normal distributions in the extremal world, in a sense. And this continues to be true if you look at it from a structural learning perspective. Okay, so what Sebastian and Adrio proved in their paper is that if you have a Huster-Rice distribution, That if you have a Hustlerized distribution that is factorizing on a graph VE, then the edges are encoded in a slight transformation of the precision matrix corresponding to this gamma matrix. Okay, so a couple of slides back, I introduced this matrix sigma, which can be recovered from gamma through some simple linear transformations. And I also mentioned that this matrix sigma is actually dejected. That this matrix sigma is actually degenerate. But if we take out the m's row and the m's column from this matrix sigma, then it becomes invertible. And the inverse of this matrix, we can now write into a matrix theta. Then we add back around column of zeros. And now the entries in that matrix theta encode the conditional independent structure in this user rise distribution. But they do it in a slightly different way than. In a slightly different way than in the classical Gaussian setting. So if the components i and j are both not equal to m, then i and j are not in E if and only if this entry is zero. So that's exactly what we would expect from the classical setting. However, if either i or j are equal to m, then instead of the entry of the matrix, we need to look at the corresponding row or column sum. So that makes things a bit more interesting and a bit more non-standard. And a bit more non-standard than in the Gaussian mode. Okay, but this now also, of course, gives us a simple idea for an algorithm that allows us to learn this graphical structure. And on a high level, you would propose to proceed as follows. Okay, so first of all, we estimate all those matrices, gamma m from data, as I was mentioning before. And because all of those matrices are the same for each m on population level, we just average them. Sorry. Well, we just average them. Sorry, so I forget the one divided by u over here. So we define a matrix gamma hat, which is just the average of all the matrices gamma m. Here, then we transform our matrices gamma m hat to sigma hat m and now okay and again sorry there's a second typo so this k here those k's should be m apologies for that okay and now we put this matrix sigma hat into any generic algorithm Sigma hat into any generic algorithm that takes as input a matrix and returns a sparse estimator of the inverse of that matrix. And that could be something like the graphical SO, that could be something like neighborhood selection, that could be anything that you like to use essentially. Okay, now for each M, that gives us an estimate of the edges ij in the graph G, where neither i nor j is equal to m. Okay, because recall if I or j was equal to m. Okay, because recall if I or J was equal to m, then we needed to look at row sums and column sums. And then in the end, we do this for every m and we obtain a final graph by a majority vote, meaning for each edge between i and j, we look at all cases where m was not equal to i and j. There's d um d minus one of those, and then we see if the edge is present in the majority of those and we add it to the graph, and if the edge was not present in the majority of those, then we don't. Represents the majority of those NPO. And that's our final estimated graph. Okay, now, as I was mentioning, for obtaining sparse estimates of the covariance matrix, there's many different possibilities. The ones that we analyze theoretically are the most classical ones, namely the graphical SO, which essentially solves the penalized problem on the entire matrix, and neighborhood selection, which essentially links this problem of inverting a matrix. Problem of inverting a matrix to a regression problem and then use a slesser regression to enforce sparsity. Okay, but as I say, there's many other possibilities. We also do have theory for this. So again, sort of don't look at the slides too closely. It's way too late for that and it's way too many technical details. But the main story here is that again, under certain technical assumptions, we are able to recover the true. To recover the true graph as probability tending to one, as long as the dimension grows not too quickly, as in log d is small of k divided by log n to the h under some additional technical assumptions and under a certain minimal signal separation condition. So essentially, this here is saying that the non-zero entries of this matrix theta are bounded away from zero at least at a certain rate, because if there are two. At a certain rate, because if they're too close to zero, it might be too difficult to estimate them. The paper actually contains more precise results where we can quantify, for instance, how quickly those edges can go to zero and a bunch of other things. And we also have tail probability bounds, but that's not really important here. One thing that is important that I would like to mention, and that was also came as quite a bit of a surprise to us, was that although the graphical lasso in theory In theory, it should work. It did not work in a lot of our simulations. And then, once you start to dig down a bit more in what in theory works means, then soon we discovered that there's those so-called incoherence parameters, which essentially put fairly intransparent restrictions on the type of graph that we can estimate. And the theory is stating that if those incoherence assumptions are satisfied, Incoherence assumptions are satisfied, then we can recover our graph consistently. But it turns out that for most of the models and simulations we considered, those incoherence conditions for the graphical SO were not satisfied, while they were satisfied for this neighborhood selection approach of Mainshausen and Bloom. So this is just a kind of warning. Sometimes you need to read the small print in the theory to really see if it is telling you something useful about the data at hand. And this is really not a problem. 10. And this is really not about high-dimensionality. One can show that those incoherence assumptions for the graphical lesso fail even for a simple four by four graph with four nodes such as a diamond graph. Okay, so those things can really be fairly subtle. But the good news is that those four neighborhood selection are much easier to satisfy and it works well in simulations. I won't really bore you with that because, of course, I can come up with a set of simulations where things kind of As a set of simulations where things kind of work nicely. But for the last slide of this talk, what I would like to mention is to just give you a comparison between the estimated minimal spending tree over here on the data set that I mentioned before and something that we would use that we would get from this Mainshausen-Buhlmann approach for some kind of automatic tuning of the penalty parameter. Now, of course, this automatic tuning is always a bit of a fishy thing, but let's just take it at face value. That phase value. Okay, and so what we see here is that actually a tree model might have been a bit simplistic for this data set because the graph that we get under this user rise assumption is substantially more connected. Now, the good news is that almost all the edges in the tree are also in the full graph, but there are quite a couple more connections, for instance, between some of the Asian countries and also some of the European countries. Some of the European countries, then the simple tree here would suggest. So there is something more to this data than the GSIS tree structure. One thing that I didn't include here, but that I would also like to mention, is that we can do the same exercise with the river flow data. And on the river flow data, the tree essentially captures more or less all that there is to it. So even if you try something more general, then we don't get a graph that is much denser than would be suggested by the reverse flow network. The reverse flow network. Okay, so it's not that this second approach always gives a much denser graph. Okay, and that's it. I think that there's a lot of potential things to think about here, starting from data that are max stable over some kind of formal tests or confidence sets for the presence of edges in the graph. And both papers are on archive. This paper. This paper is the one that I mentioned just now about learning general graphs. So, that is actually fairly recent. We posted it just two weeks ago. But if you're interested in any more of the technical details, then please take a look. Okay, thanks.