It's my pleasure this morning to introduce Dr. John Nardini from, again, the College of New Jersey. He's going to be talking to us today about forecasting and predicting stochastic agent-based models of cell migration with biologically informed neural networks. Thank you very much, John. All right. Well, thank you so much, Kathleen. It's really an honor to be here talking today. And so, my talk today is really going to be a methods talk focused on Methods talk focused on studying and predicting the behavior of agent-based models. So, just to motivate that, I'm really interested in how in cell biology, cell-cell interactions scale into emergent behavior. And I often think about this in the form of how do physical connections between cells Physical connections between cells impact the collective migration of cell populations. And what's really fascinating to me about this is that the role these connections play on migration can change based on the context. So for example, if you think about the epidermis or outermost layer of your skin, these cells are physically connected to each other. And that makes sense because it prevents those cells from moving around so that our skin can be one cohesive barrier that prevents pathogens from entering. That prevents pathogens from entering our body. However, in response to a wound, the epidermis of our skin has now been corrupted. And so, a really important part of wound healing is these epidermal cells are going to collectively migrate under the fibroclot to re-establish the epidermal layer. And so, these cells here that are migrating, the leader cells at the front of the population are going to use these exact same physical connections to actually pull their. To actually pull their followers along with them and kind of communicate to them where they need to migrate to properly re-establish the epidermal layer. And of course, this is really focused on wound healing, not so much cancer, but this is also at play in cancer, where example, what can give cancer cells their invasive capabilities is through the epithelial to mesenchymal transition, where cancer cells lose these physical connections, and then they can invade new areas. And then they can invade new areas of the body. So, of course, as mathematical modelers, we're really interested in developing models of these processes, and agent-based models are a great way to program individual-level rules, simulate them, and observe the collective phenomena. So, this is going to be a really simple example that I'll call a cell polling AVM. It's going to be a two-dimensional cellular automata, and we're going to have what I call polling agents. Have what I call pulling agents in blue that can migrate in any of the four cardinal directions. Here, we're just assuming that they're moving rightwards, which they do with the rate Rm pull. And so we're just going to have two really simple rules in this ABM. First off, if we have an agent that wants to move rightwards and it has no neighbor to the left, then it can simply change its location. In the second rule, however, if it has a neighboring agent, then they're going to be physically connected to each other. And so when our migratory agent moves, And so, when our migratory agent moves, it's going to try and pull its follower along with it so that both agents move rightwards. That's going to be successful with probability p pull and unsuccessful with probability 1 minus p pull, in which case only the migratory agent moves, but the neighbor stays where it was. So, here we just can have a simple example where we have a rectangular lattice, and all the agents are sort of located in the middle to imitate a barrier assay. In the middle to imitate a barrier assay. And then we can look at: well, how do these age, how does the population spread based on these rules? And if we set the rate of migration to be one and half of the polled events are successful. And then down below, we're going to summarize this by just looking at the one-dimensional density of these agents over time. So here we're just simulating all these different rules, and we see how the population spreads over time. Over time. And of course, this is only going to be valid for these parameter values. And so, of course, as mathematical modelers, we want to know: well, if we change these parameter values, how will the population spread? And that's a limitation of AVMs, is that in order to really thoroughly explore parameter space, you have to simulate the AVM at every single parameter value, which is quite hard because these AVMs can be pretty time consuming. So today, we're going to be focused on methods to better predict. To better predict ABM behavior, and we're going to be focused on two tasks. Both, first, we're going to forecast ABM data by looking at a partial ABM simulation, and then using that to predict what the future ABM behavior will look like. And then, second, we're going to predict ABM behavior throughout parameter space, so like exploring new parameter values. At present, this tends to take one of two forms. We have on one end of the spectrum more white box and interpretable. Spectrum: More white box and interpretable methods that usually take the form of differential equations. On the other end of the spectrum, we have black box type models that are quite data-hungry, but not necessarily interpretable. And for that, I'm thinking about things like artificial neural networks. And so what I'm going to argue is that we can actually combine these two methods into a more gray box type model, which I'm going to be calling a biologically informed neural network or a VIM. So let's first focus on the white box end of the spectrum. So let's first focus on the white box end of the spectrum. And so what we can do is take these probabilistic and discrete rules, perform model coarse grading, and what we get is our mean field partial differential equation model. So here, capital P represents the spatial temporal density of our polling agents. And it's a nonlinear diffusion equation where the rate of diffusion just depends on our agent density here. And our diffusion here is just going to be quadratic with respect to agent density. To agent density. So here are three example diffusion rates for different volleyballies. And so this allows us to interpret how the population will spread based on both agent density and our parameter values. And it's not surprising here that these curves increase the density because with more agents we have more pulling events. So the population should spread quicker. Similarly, if pulling events are more successful, you would expect. If polling events are more successful, you would expect the population to spread more quickly. But this gives us a really precise way to quantify how our population spreading depends on these things. What's more is we can simulate the nucleophile PDE at the same initial condition as the ABM, and you see we get a really nice prediction over time on how the ABM will spread. So, with that, that's sort of the more white box approach. That's sort of the more white box approach. So let's now go to the other end of the spectrum with more black box type models. This is going to take the form of a multi-layer perceptron. So that's basically the most simple and common type of neural network. So we're going to have a neural network with multiple layers. And consecutive layers are just going to be fully connected. So we aren't going to have any convolutions or anything like that. Oops. And so there's two really key properties about neural networks that have made them so successful over the past 15 years. The past 15 years. The first is that they're universal function approximators, and the second is that they're really easy to differentiate. So, let's go through those two properties just so we know what I mean by that. And so when I say a neural network is a universal function approximator, I mean they can approximate any continuous function arbitrarily well. So let's look at this function, sine of 4x. And what we can do is we can sample a discrete number of points from our nice continuous function here. Continuous function here. And what we can do is train a neural network to input the x value of each point and then predict what the corresponding y value will be. So we're only going to train the neural network on these discrete points here, but then after training it to do that, we can then feed all x values to the neural network and see what it predicts over that range. And so here we see that from this discrete number of points, the neural network really nicely predicts what the underlying continuous Nicely predicts what the underlying continuous function is. There are some points of discrepancy here. We can improve upon those either with more data or with a larger neural network. So that's the idea of the universal function approximation. The second thing is that neural networks are really easy to differentiate. Sarah gave a really nice introduction to back propagation on Monday. And so usually we think about that in the context of getting the weight, estimating the weights and biases of the neural network. Of the neural network, but you can also take the derivative of your neural network with respect to its inputs, kind of similar to what we do in a calculus class. And so we know that the derivative of our true function is 4 cosine of 4x. And now if we take the derivative of our trained neural network, we see that it also is right on top of the true derivative of our continuous function. And this is really easy to do once you've trained a neural network to perform differentiation. To perform differentiation. So, using those ideas, we can now train an MLP to predict AVM data. And so, here I'm going to have a slight change in notation. So, capital T will correspond to the total density of agents in this AVM simulation. That's just because down the road we're going to have heterogeneous AVMs with multiple agent types. So, what we're going to do is we're going to train an MLP to input spatio-temporal points and predict the corresponding agent density. And predict the corresponding agent density at those spatial temporal points. And to assess the ability to forecast future data, what I'm going to do is train the MLP on all spatial data at the first 75% of time points. So that's going to be the training data. And then the testing data is going to be all spatial data in the final 25% of the simulation. And so here's the result of training an MLP to the training data here. And we see that it's able to. And we see that it's able to predict the training data quite nicely. The problem, however, is if we then try and forecast the MLP's prediction to future time, our prediction is quite corrupted. But anyone here who's familiar with neural networks is not too surprised by this. Neural networks are very good at doing what they're told, but they're not necessarily good at learning new things that you haven't trained them to do previously. So just to sort of summarize, there's many advantages and limitations to both of these two approaches. These two approaches. And I won't go through all of them. And so, the one thing I didn't have a chance to sort of mention is that in order to actually derive that mean field partial differential equation model, we did have to make some assumptions. And these assumptions aren't always going to be true in an agent-based model. And when that happens, the mean field model can give poor predictions for the ABM. And in fact, we'll see a scenario later on when the mean field PDE can't even make a prediction. On the other hand, ML. On the other hand, MLPs don't require any underlying assumptions, and they're universal function approximators, so they should be able to describe any ABM data set pretty well, at least. So we're going to combine these into our more gray box type model. And so that, of course, is going to take the form of a biologically informed neural network or a bin. My working group might call it a universal PIN, but I'm just going to be consistent with my previous work. And so here the idea is we're going to train this MLP. is we're going to train this MLP to also satisfy a diffusion PDE framework similar to the mean field PDE. So in order to do that, we're going to have a second MLP that we call DMLP. And so the idea here is that TMLP will approximate our dependent variable, so the agent density, and then DMLP here will represent our rate of diffusion that depends on the total agent density. Now, these are both MLPs, so they're very easy to differentiate. So we can take their derivatives during the training process and ensure that they satisfy this diffusion equation framework. And so what we're going to do is we're going to train this bin model to minimize the loss function with three separate terms. So this term ensures that TMLP matches the training ABM data pretty well. This term will ensure that both Term will ensure that both TMLP and DMLP match this PDE framework. And then this is sort of a constrained term. So if we have any sort of prior biological knowledge about the system, such as upper, lower bounds on TMLP or DMLP, or whether DMLP should be increasing or decreasing, then we can sort of incorporate that biological knowledge into the training process. So here I'm just comparing the performance of a trained BIN model to the ABM data. Model to the ABM data to the MLP that we saw previously. So these two neural networks are trained to the exact same training data, yet we see that the VIN model, because it must also satisfy the diffusion framework, is able to nicely forecast and predict the future AVM data. What's more is we can get some interpretation of what the bin model is learning by visualizing its learned diffusion rate DMLP in orange here. And here we see that it's quite similar to the mean field rate of diffusion. Similar to the mean field radio diffusion. So it's basically relearning what the mean field model already told us. So this slide is a little technical, but there's actually two ways to make predictions with bins. So in the top row, you can just plug in directly into your neural network. So we're just looking at this T here. Or down below, you can actually numerically solve the PDE model that your bin is learning. So down below, I'm just numerically solving this PDE. I'm just numerically solving this PDE, where I'm actually directly using the DMLP diffusion rates for my rate of diffusion. And so, as you'd expect, both approaches do equally well. And so, moving forward, I'm just going to use what I call here the bin-guided PDE simulation to predict ABM data, just to ensure that we're actually learning a PDE model in this process. So, with that, we have a really quick introduction to the idea of the bin model. Of the bin model. And we sort of did a standardity check that when the mean field PDE works pretty well, do we get sort of a similar result with bins? So we're now going to look into two more interesting case studies where the mean field PDE is either not, is ill-posed and can't make a prediction, and then another case study where we have a heterogeneous ABM whose mean field model is quite complex and loses its interpretability. So that's going to take us to our second ABM, the adhesion ABM. And here, red hexagons correspond to what I'll call adhesive agents. And they're going to migrate with rate RMAD. And they're going to perform adhesion events with probability TAD. So here the idea is these agents can freely move rightwards if they have no neighbor to the left. But if they have a neighbor, that neighbor is going to adhere to a migratory agent with probability PA. A migratory agent with probability P add and abort the migration event, in which case neither agent changes their location. But that adhesion event will fail with probability 1 minus p add, in which case the migratory agent moves, but the neighbor stays where it was previously. So once again, we can perform model core screening. We get our mean field PDE model. Here, capital H corresponds to the density of adhesive agents. And once again, our rigid diffusion is Diffusion is quadratic with respect to age intensity here. So, here are three examples of diffusion rates as we increase P add. And what you might notice is that if P add exceeds 0.75, our rate of diffusion can actually become negative. And so that's really a problem because diffusion equations are ill-posed when diffusion is negative. So that's why you can't even make a prediction with our PDE model. PDE model. So here's an example ABM simulation where PAT is 8, so the mean field PDE is now useless. We can't do anything with it. But despite that, I trained a bin model to this data, to the training data, and then stimulated its learned PDE model. And we see that it learned a PDE model that nicely predicts both the training data and then it can forecast to the testing data and gives us a pretty nice prediction. So you might wonder, well, what did the bin learn that the mean field model could not tell us? Bin learned that the mean field model cannot tell us. And so here are the different diffusion rates from the bin in orange and the mean field model in blue. And so we see that the bin learns a much higher rate of diffusion at low densities. And then at more intermediate densities, it has a lower rate of diffusion that then more slowly converges to zero, but is always positive. So far we've been focused on the task of forecasting future ABM data, but not necessarily exploring. Up, but not necessarily exploring parameter space. So, in order to do that, we're going to combine bin modeling with multivariate interpolation. So, here we're looking at parameter space on the left. So, we have the probability of the successful adhesion events on the x-axis and the rate of migration on the y-axis. And so, what we're going to do is I'm going to simulate amium data at each one of these orange dots, and then I'm going to train a bin model to each, to all the data. A bin model to each to all the data sets that arose from those orange dots. And then that trained bin model is going to have a rate of diffusion. So at each one of these orange dots, we have a rate of diffusion that depends on agent density. So then what I'm going to do is I'm going to perform multivariate interpolation over all of those learned diffusion rates to create an interpolant that I'll call d interp. And so then what we can do is use that interpolant. Can do is use that interpolant for new regions in parameter space in green here. And so if we plug in those new parameter space, those new parameter values into the interpolant, we get a diffusion curve for how we expect the population to diffuse at these new parameter values. And then if you want to predict ABM data at these new parameter values, then you can simply numerically solve our diffusion equation framework, yeah, diffusion equation PDE. Our diffusion equation PDE using this interpolated diffusion rate. So that's what I did at each one of these new green points. And I'm just ranking them by increasing training MSC order. So of course low MSC means good agreement between our prediction and the data. So this is where we had the best prediction, second best prediction, and the very worst prediction. So you notice that our eight best predictions all appear pretty good. Predictions all appear pretty good, and then the final two appear to sort of jump up a bit in their MSE values. Let's take a closer look at our very worst prediction that we got. And we see that it's still a pretty good prediction of how the population is spreading, but you might notice that we do have a bit of a misprediction of the elbow of the data here. If we go down, though, to our third worst simulation, we see that it gives a really nice prediction for the AB of data there. Data there. That's going to take us now into our third and final case study, ABM. This is going to have a polling and adhesive agents present. So here the idea is pretty similar to what we've seen previously. Polling agents pull their followers regardless of what type they are, and adhesive agents prevent their neighbors from migrating regardless of what type of they are. So here we're going to have five different parameter values, the rate of migration for both agent types, and then the probability of successful adhesion or. The probability of successful adhesion or pulling events, and then alpha is the proportion of adhesive agents in the simulation. So, if alpha is zero, we have no adhesive agents, if alpha is one, we have only adhesive agents. So, once again, we can perform model core screening, and here's the mean field PDE for this ABM. It's pretty simple, so I don't think I need to say any more about it. Just kidding, right? Just kidding, right? I've been staring at this thing for a year and a half, and all I can do is say it's a two-compartment TDE, and there's the fusion going on. Otherwise, it's pretty complicated. So instead, what we're going to do is with our bin approach, we're just going to look at the total population of agents. So we're going to add together P and H and learn a one compartment PDE for the total population. And hopefully, we can still match the data pretty well. We can still match the data pretty well with a simpler diffusion framework. So that's what we did. In the top row, we simulated our two-compartment mean field PDE model, added them together to get our total ABM prediction. Down below, we trained our one compartment into this data, and we trained it to the testing data, but we see that, so we trained it to the training data, but it's still able to forecast ahead of time to the testing data as well, despite being a one-compartment PDE. And then once again, And then once again, we can interpret what has the bin telling us here. And so it tells us that we have higher rates of diffusion at low and high densities. And then we have a sort of more, and then it sort of has this more constant diffusion rate at the intermediate density values. And then once again, we can also explore parameter space with this bin approach. So five parameters is a lot. So I fixed the rate of migration for all the agents, but I still vary the probability. But I still varied the probability of adhesion events, polling events, as well as that alpha parameter. And so I trained bins to the 40 orange points and then predicted, and then used multivariate interpolation to predict what happens at 20 new points with these green X's. And so these are the training MSD results. And so if you look at our very worst simulation, again, it's a pretty good prediction, though it does have a little bit of error predicting the sort of peak of the data. Error predicting the sort of peak of the data. If we go down to our fourth worst simulation, it's really nicely predicting the data there. So I just want to finish with a really quick note on the computational timing of all the things we've been talking about. So for AVM data, I was averaging over 25 simulations. So to compute those in serial takes about 40 minutes. Mean field PDEs are very fast, taking anywhere from half a second to five seconds. And at the moment, the Seconds. And at the moment, the longest process by far is training the bin model to data, currently taking on average about 11 hours. Once you've trained the bin model to data, it's pretty quick. Takes anywhere to simulate the bin-guided PDE. It can take anywhere from 16 seconds to about five minutes. So in conclusion, as we all know, agent-based models are a great way to understand how individual actions scale to emergent behavior. We've kind of focused today on cell system. We've kind of focused today on cell cell adhesions and collective migration, as well as the impacts of population heterogeneity. And I hope what I've convinced you is that VINS can be a really valuable tool to both interpret and predict AVMs. And I kind of frame today's talk as a competition between the mean field and VIN guided PDE approaches, but that's really not at all how I view these two. I really see them as complementary approaches. We know that the mean field models do well for a variety of situations. For a variety of situations, and they've been very successful for many years at this point. But they do come with limitations. Sometimes they give poor predictions, they can be ill-posed, and they can become complex for more complicated AVMs. So it's when faced with these types of challenges when I would encourage mathematical modelers to turn to bins and use them to study AVMs. And at the moment, their biggest limitation is their long training times, and that's why at the moment, And that's why, at the moment, I'm working with a student Vijay Machiraju at TCNJ to sort of speed up the bin training process. And so, what we're doing is really playing with the hyperparameters of bins, looking at the number of hidden layers, the number of neurons per hidden layer, everything stopping, et cetera. And so what happens here is each one of these points here just corresponds to a bid model that was trained by varying all these parameters. And then we're looking at, well, how long did it take to train that bid model? Well, how long did it take to train that bid model and how accurate is the prediction from that bid model? So, of course, lower MSE is a better prediction. And so, we do have some preliminary data to suggest we can trade a bid reasonably quickly while still predicting the ABM data pretty well. And I hope to have more to share with you about that in a few months. So, thank you all so much. And thank you to Kathleen, Renee, and Sarah for the invitation. And also, thank you to SMB for travel funds that allowed me. The need for travel funds that allowed me to be here this week. Thank you, and I'm happy to take any questions. Excellent course. It's really interesting seeing how we're able to integrate neural nets for ADMs. I was just wondering about the ratio of the training to the taste data. It's the ratio of like 75% training to 25% taste data. With ABMs, we're often not sure. With ABMs, we're often not sure how far in advance we might want to simulate, right? If we don't know how far we want to simulate, how do we know how much training data we need to generate in order to predict that extra step that we want to look at? Yeah, so I don't have a good answer to that question. Yeah, and in a previous paper, we did kind of more explore, you know, how much data do you have to look at. And based on the ABM, it can change quite a bit, I think. So I think, I don't have a good answer. So I think, I don't have a good answer, but I think that's definitely an interesting thing to be thinking about. Yeah. In some PDs, they're kind of built in conservation of mass or conservation of energy and have this mixture with the bins. Does that violate any of these? I mean, if you numerically solve PDEs, you would be very careful about, you know, I need this to be exactly zero or exactly seventeen all the way through seventeen point five and something like that. Is your Or it's looking like that. Is your neural network kind of catching up with this, or is it something that should be added on top to make sure that kind of physiological constraints are not violated? Yeah, so I think there are ways to incorporate conservation and mass or other types of principles into the neural network architecture. If not, one thing you could do is if we go back to If we go back to this term here. Oh, you can't see that, sorry. So if you do have sort of any sort of biological knowledge about the system, which I think would include conservation of mass, you could incorporate that into this term of the loss function. An important thing, though, is like if it's here, it's only a soft constraint, so it's not going to perfectly satisfy that, but it would roughly satisfy that. But you still have to do it. That. But you still might be able to explicitly incorporate that into the architecture itself. Yep. A couple questions on. I'm guessing you started with a simple network because it was just the first place to start. When I've looked at somewhat similar things, we looked at LSTNs. We looked at LSTMs to try to help handle the time series type stuff. What do you think about that? What considerations went into picking this architecture? When you say this architecture, are you talking about bins? Are you talking about the basic MLP that's not a bin? The basic MLP, yeah. Yeah. I do think LSTMs would be useful. I'm not 100% sure how to sort of incorporate this. To sort of incorporate this PDE model into the LSTM, which I think is the challenge. But yeah, I do think things like that, or even like residual networks kind of similar to like PDE net, might be able to do these types of things better. But yeah, it's hard to incorporate the PDE into those. We were looking more for strict prediction, not combining with gotcha, gotcha. Right, and so, but then you lose the interpretability, right? Yeah. And so I think for us, interpretability. Right? Yeah. And so I think for us, interpretability is really important. So yeah. And then just tell us what, sorry, can you just tell us what LSTM is for? Long, short-term memory. But it's a recurrent neural network that would be used for predicting, that would take in time series data and then output time series data as well. Just a different type of neural network. It's kind of more like of a recursive process. Like of a recursive process, similar to like solving a PDE numerically. Yeah. Thanks for a very clear talk. I was curious about two things. One, very short question, which is what was the kind of compute that you had for training general. Oh, yeah. So when I say it took 10 hours, that was using GPUs. Yeah, but is it like a series of industrial GPUs in Iraq, or is it like a 3070 on your personal? 3070 on your personal no. So at TC and J, we actually have an excellent supercomputer. So we have access to 64 GPUs. So I'm just using those. Yeah, so you're using 64 GPUs at the time. I'm not using 64. I think our administrator would be pretty displeased with me. But I'm usually running 10 at a time, but each job is just like using one GPU. Gotcha. Okay. Yeah, so one GPU takes 10 hours. Yeah. Okay, yeah. And so the more substantive question is: did you, so question is did you so have you considered feeding so you're feeding in x and t into to train the the the neural network have you considered feeding in the parameters to the ABM like people yeah I have yeah so I'm very interested in that I do think that's probably a better way to do parameter space exploration than combine it with multivariate interpolation it's just it's a more difficult training process but I do think Process, but I do think that's sort of the next step to do. Yeah, I'm very interested in that. I yield my time. Okay, well, let's thank John over.