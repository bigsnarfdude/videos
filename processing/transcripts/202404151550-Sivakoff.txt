Okay, so now we have David Seamkov from the Ohio State University. He's got his title wrong. It's actually supercritical: neighborhood growth with unreventional UTH. Thank you. Thank you to the organizers and to And to staff at MAMP for hosting us. This is really wonderful. Yeah, so you're correct. I forgot to put the word supercritical, which I think I put in my official title. So I'll talk about supercritical growth, neighborhood growth of one-dimensional foundation. And so here's some pictures from physical systems. I guess this audience doesn't really need motivation for physics to study these problems, but we have. To study these problems, but we have formation of ice crystals, colloidal crystals, which are just larger molecules in a body of much smaller molecules which go on together. Formation of protein crystals. And relevant to this talk is sort of a manufacturing process which forms lines in a two-dimensional way. In a two-dimensional array, in order to form a larger two-dimensional surface, which is only qualitatively resembles what I'm going to talk about today. But from Maris, nucleation means a change in a physical or chemical system that begins within a small region, that is, begins in a nucleus and with. And this early example from Huygens was water put under negative pressure, and the negative pressure causes bubbles to nucleate near the surface. This is like our spontaneous nucleation. It's like we observe in the bootstrap regulation with random functional vibration. I think, like the suction of the surface, yeah, not positive pressure. So, for us, the most interesting is homogeneous nucleation, which is occurring without any favorable nucleation sites. And normally, the nucleation process happens randomly as well as spontaneously. And traditionally, a small region, as we've seen in the previous talks, a small region. Small region from this quote, is one with a diameter which is much smaller than the time scale on which the new equilibrium is reached. And so in this sense, the nucleation is strictly local. But as we'll discuss, there's another possibility, which is that such a region could also simply have a lower dimension than the growth environment. So the two. So, the two-dimensional growth processes, which we'll consider, are essentially special cases of the U-bootstrap rules which we discussed this morning. So, we consider, we begin with a set of occupied sites, which are ones or infected sites, and empty sites, which are zeros or uninfected. And we will consider a transformation, T, which creates a new set of occupied sites. Creates a new set of occupied sites T of A, and then of course we'll iterate transformation T which satisfies certain rules. It's a solidification rule meaning if A is if a vertex is occupied it remains occupied after one step. It's monotone in the sense that if you make more sites occupied than after one step you again have more sites occupied. And then it is translation invariant. It is translation invariant, and for most of the talk, unless I have time at the end, we'll assume it's also local, meaning that the decision of whether to include x in t of a depends only on a through some finite neighborhood nx, and n x has the form x plus some fixed neighborhood m. As I said, these are essentially the Eugenstrom laws. Are essentially the Euconstructs. Still working? So then we start with an initial set A and we'd iterate, enlarge the set by applying the same map. And typically, we start with an initial set which has each site included with probability P independently. And then the event that the final configuration Event that the final configuration, omega infinity here, fills the available space is called spanning. And we'll also consider these rules, of course, on subsets of the lattice with appropriate boundary proportions. But the previous slide updated, but subject sorting. I'm going to specialize to a very special class of these rules ultimately, but. Okay, so the bootstrap percolation is such an example, right? You fix an integer threshold r and a neighborhood n, and then the neighborhood of the vertex x is just n translated by x, and then t of a enlarges a by exactly those sites whose number of occupied neighbors meets or exceeds r. Then this is the transformation which corresponds to. Is the transformation which corresponds to a threshold growth or a bootstrap percolation process? And the most studied example is the four nearest neighbor model with the threshold two. But this is my slide, which goes through the example of threshold two bootstrap recolation. If this is the initial set, then these vertices are the ones that come occupied at time one. Then again, at time two, three, we continually apply the map until nothing happens. Until nothing happens. And then this is the final configuration in my random C. And you'll look to, well, okay, so the final configuration either fills everything in C2 or it's a union of well-separated rectangles, which I think will feature in this talk tomorrow for the lower terms. Right, so here's an example of those dynamics. So, what happens is Dynamics. So, what happens is, right, you have this is the nearest neighbor on the torus with periodic bent, so periodic boundary condition, initial density 0.045. At time 20, you can see very little has happened in most regions, and you have some regions of sparse growth. And then, okay, most of the region is sort of stopped doing anything, and then you have maybe one nucleus here which is going to grow eventually, you take over. Going to grow eventually take over everything. So, this is the sort of classical influation feature that we've heard about most of the time. Colors here? The colors are just a periodic coloring based on the time at which vertices were first occupied, so you can get a sense of the order and locally the order in which things occurred. Yeah, any gray scale is occupied, and white is occupied. Occupied and white is unoccupied. Any other questions? Okay, so we've heard these results earlier today. The first result is due to VanEnter, which was extended later to higher dimensions by Schoenman. So the VanEnter's result says that for the R equal to 2 bootstrap percolation, for all p bigger than 0, everything eventually becomes occupied with the cover of the equal to 2. And so then we study these processes more closely on either n by n boxes, or if you like, we look on the infinite lattice for the first occupation time of the origin, tau, and ask, how does this scale as p goes to zero? These questions are very closely related to one another, as we saw. They're essentially like inverses to one another. It's quite a hard thing. Not working very well as this. So, to very quickly summarize the results we heard this morning again, for the R equals 2 nearest neighbor, the scaling for PC was given by Eiseman and Liebovitz. The precise asymptotic, first-order asymptotic was given later by Holroyd. And then the second-order asymptotics were sharpened subsequently by Robner. Subsequently, by Robner, Holroyd, Morris, Artars Morris, and I mean I Bello and Augusto are going to tell us about the even sharper response. And then by inverting, by solving, P is lambda over log tau, you invert and you essentially get the asymptotics for the time of the first occupation of the origin. The original brief survey of two-dimensional results. So consider another example of bootstrap dynamics with the box neighborhood, the eight nearest neighbors around each site, and the threshold three. In this case, we see, so in the parlance of Ivelo's talk, this is, or I think it was Rob's. Or I think it was Grov's talk. This is considered what's called a supercritical model. And in this case, Robner and Griffith showed that tau, the first time that the origin is occupied, scales like p to the minus 3 halves versus the exponential in the two-neighbor model near some post-prints. Very different scaling. And PC is essentially obtained by inverting this, right? Inverting this, but scales like n to the minus two-thirds. And the reason is because essentially you have small nuclei which are local, and their appearance causes unstoppable growth, which doesn't need any help from the surroundings. So you can see that the growth is much more symmetric. So, very roughly, because I didn't want to define Roughly, because I didn't want to define stable directions. There are three types of two-dimensional dynamics, which, very roughly speaking, are analogous to the threshold one, threshold two, and threshold three, or higher, nearest neighbor bootstrap locks. So, this is where I'm lying here a little bit. The sub-critical rules cannot fill, basically, cannot fill voids like the. Cannot fill voids like the threshold 3 or threshold 4 dynamics. This box is stable under the threshold 3 dynamics, even if everything else is affected. The supercritical rules are like threshold 1, in that there exists a finite set which will grow forever. Here it's very easy to very easy to compute these, right? You just need you just look for the closest point. You just look for the closest point to the origin, and that happens within distance 1 over p to the 1/2. And then there's the threshold 2 process, which is, among these rules, the most interesting to study in two dimensions, which, right, well, it's neither subcritical nor supercritical. It can fill the void, but it can't grow outwards without help. So this is summarizing the Bolovas, Smith-Guzzell, and Ballister-Bolovash, Tchikuchti, and Smith's results for the supercritical, critical, and subcritical models. And in the critical case, these exponents are known to exist, although they're hard to compute. And the case of the subcritical model. So, the subcritical models are studied more in the context of percolation, right? It's behavior more like classical percolation models. And so, today I'm going to talk a bit more about the supercritical models, where we don't know very much about the critical exponents. And in general, they may be very hard to compute, as I expect we'll hear about. But I'm going to study a But I'm going to study a smaller class of such rules where we can actually make some progress on studying these. So to tell you about what's already known for the supercritical rules, I need to tell you what is a voracious supercritical rule. And so let A be the set of finite sets which lead to unbounded growth under the dynamics. Under the dynamics. And let gamma be the size of the smallest such set, which leads to unbounded growth. Then a growth rule is supercritical if and only if this set is non-empty. Only if there's a finite set which grows forever. The eight-neighbor threshold three rule that I gave before is a supercritical. Is a supercritical rule, and it's also a voracious rule, meaning that for every set which grows, every finite set which grows forever, that has the minimal cardinality, not only grows forever, but fills the entire space. This is called voracious. If all of the smallest sets that can possibly grow forever actually fills everything. Sorry, the definition of growth, however, was just produce something infinite. Produce something infinite, exactly. The same as, because it's local, the same as the dynamics not. You can have something non-linear line. Yeah, that's the point. It could be a line or it could occupy a cone. A line, or it could occupy a cone. Was that your question? So, what is your question? Yeah, maybe it was. So, it can happen that the minimal ones occupy everything, but some non-minimal ones that grow forever don't occupy everything. Okay, grow forever. Again, it's a finite non-minimal one that grows forever and doesn't That grows forever and doesn't quite. I don't know. I would have to think for a bit. I don't know. Because it might you insisted that it's minimum. Yeah, I insisted, yeah, that it's minimal. That's true. I taught him to mention it at the insert. You should mention the problem at the insert. Yeah, so I don't know. But okay, so anyway, if it's voracious. Okay, so anyway, if it's voracious, though, it turns out, right, then essentially, all you have to do, all you have to do, is look for one of these smallest configurations as close as possible to the origin. And as soon as you find this configuration, it will form a nucleus and fill the origin at the time equal to its distance from the origin. Equal to its distance from the origin, because the growth is linear once it takes off. So this set, the smallest minimal cardinality set can be found within, with high probability, within distance n of the origin, as soon as n squared, like the area of such a set, times p to the gamma, as soon as the expected number of such sets in that distance is larger than 1. So you get tau. Tau is scales like p to the minus gamma over 2. So this is the result of Gravner and Griffin from the same thing. What's known before were supercritical rules. So here's what we did. So we looked at supercritical rules which have the neighborhood that has the shape of a cross. Of a cross. So with so just an R row by row. So your neighborhood is just the row vertices above, row to the right, to the left, and down. And each neighborhood is comprised of a horizontal component and a vertical component, which will be relevant in just a moment. Critical rules with these neighborhoods were introduced earlier by Introduced earlier by Ander Liggett and Robin Denham. And were recently used by Daniel Lenkissett to study the three-dimensional analogs with three-dimensional neighborhoods of this sort. Okay, so our rules are given by a Young diagram, which we call a zero set, Z. Set Z. And this just, this is the fact that it's a young diagram has nothing to do with algebraic combinatorics. It's just to guarantee that our rules specified in this way are monotone. So in other words, if uv is in Z, then the rectangle from zero to UV is also contained in Z. Discrete rectangle. But is this just is it just a subset of Is it a subset of the quadrant? Yeah, I'll draw a picture on the next slide. We're centering it this way. So it is a subset of the quadrant. Yeah, including the axes. Yes. The discrete. Okay, so then the rule is as follows, and it'll make more sense if I go, when I show the picture, I guess, but to determine whether x is in T of A, whether X is occupied at the next time step, you look at its horizontal. You look at its horizontal neighborhood and count how many neighbors does it have in the horizontal neighborhood. Call that U. Occupied neighbors in the horizontal neighborhood. And then V is the number of occupied neighbors in its vertical neighborhood. And then you check, is UV that pair outside of the zero set? If it's outside of the zero set, then you call that vertex a one, call it occupied at the next time step, and otherwise you don't. You know, and then throughout, we'll assume that rho, the length of the neighborhood, is large enough to make this a supercritical rule, which means it just has to be larger than either the height or the width. So Z is just a tool field definition. I just think it's a zero. It's just a tool for the definition. That was a new kind of table that's just a tool for the. Just so that we have a robust enough class of rules to study, but specific enough that we can say something. Say something. Okay, so here's an example. Over here, I have my Young diagram Z and over here I have some configuration of occupied points, and I want to know, is X occupied at the next time step? Account has two horizontal neighbors. Let's suppose that the neighborhood is long enough to encompass the entire diagram for the moment. So it has two occupied horizontal neighbors and four vertical. horizontal neighbors and four vertical neighbors and I look if two comma four is inside or outside of my diagram. So two comma four is outside so x would become occupied at the next time step. y has three horizontal and one vertical and three one is inside of z so it would not be occupied. So you said the distance that you're willing to look to find your neighbors is rho, is that? Is rho. So it's finite but large enough that it's, in this case, I would take rho. Enough that it's in this case, I would take rho to be at least 9 or 10 here, 9. And that would be sufficiently large to make this a supercritical diameter. If rho is smaller than both the height and the width, then it's a critical or subcritical diameter. Okay, so anyway, as I just said, the assumption that œÅ is larger than both the height and the width. Assumption that rho is larger than both the height and the width of z makes the rule supercritical, but not voracious, and that's the point. So unless z is just a single box, in which case this is just the threshold one rule, a long enough horizontal or vertical interval of occupied sites will occupy a line, but nothing else. So, for the remainder of the talk, I'm going to state two theorems for two special classes, subclasses of these dynamics. We don't have a general theorem that can even handle all of these Young diagram dynamics, but we can give bounds in many cases. So, the two special cases that I'll Two special cases that I'll focus on are just the bootstrap percolation rules, threshold R. So the threshold R rule is given by just this triangular young diagram. And our theorem there is, okay, well, the theorem is, okay, we had a theorem, but the more interesting part is what's happening, right? So as long as So, as long as rho is bigger than r, then these are supercritical rules, and we have tau is like 1 over p to the gamma c, where gamma c is this number where you have to plug in a is this thing. Okay. For large r, this is asymptotically r minus square root 2r to get a sense for how these gamma seats behave. seats behave. And okay, here's a below is a table for some small values of both R and A. So you can plug in. This A has a meaning, which I'll explain in just a moment. So let me show, well I'll explain this and then I'll show the simulation. Okay, yes. The exponent doesn't depend at all on rho. As long as row is exponential. It doesn't depend at all on rho. The rho probably All on rho. The rho probably affects the constants implicit here. Yeah? But not the exponent. No. So I should have said that. I don't know exactly what the effect of rho is. We're not that careful to know what it is. So here's the threshold three dynamics, which is sort of the simplest. Which is sort of the simplest non-trivial rule here. So, how do you span a box? So, first, essentially, okay, here's one way to span. I'm going to give two examples of upper bounds, and I'm not going to give boost for lower bounds. So, if I have three points together, of course, then the points next to them become occupied, and the points next to those become occupied, right? Because it's a threshold. Because it's a threshold three dynamic. So this will occupy the line. Nearby these two points, nothing happens initially. Nearby this one point, nothing happens initially. And so I fill a line. And now, of course, I filled this line, and if I have these two points nearby to that line, now they all have an extra neighbor in the line. So that will cause that next line to fill. And then finally, I fill this line, and once I have three lines, that'll fill everything. Lines that'll fill everything. So that's six points across three lines. So another way that I could spam using only five points is I could have the three points here, and then I could type these two points that are sort of diagonally adjacent to one another. And now each has three neighbors. Each of those other diagonal points has three neighbors. So they fill. Three neighbors, so they fill, and then they fill the two lines there. At the appropriate length scale for n, both this and the previous configuration have about the same probability of occurrence. So in an n-by-n box, both scenarios become likely as soon as the expected number of lines with three points that have a neighboring line with two points. Have a neighboring line with two points becomes like so this gives the correct upper bound scaling and the critical length is like n to the p. You have p to the minus 5 thirds, which gives the correct time, or at least an upper bound, the correct time. So in this case, if n is larger than p to the minus 5 thirds, n is also larger than 1 over p, so that in the first picture that I showed, First picture that I showed, right? As soon as I see these three points together and these two points together, this point here comes for free because every line is going to have one point already. And so that's what the value A in the theorem is. It's the number of lines. There's some optimization need. Fine. What should N B so that you pay the cost for the first A lines? The first A lines, and then all the lines after that become for free. So that's the proof basic upper bound. All right, so the lower bound is quite a bit more involved, and I'll hide the details, but it's essentially an optimization argument which shows that there's no more efficient way to fill the box of the appropriate size. Alright, so I'll show some simulation. So, this is what the still image looks like. Is what the still image looks like. That's just in case my simulation fails. This is my animated gear animation. So you can watch. You can see the line grows until you find some two points nearby, and then it explodes into the two-dimensional. It's a funnel loop so you can watch. It's many times to survive. So incidentally, the critical one with the cross-neighborhood it looks a little bit like this when you simulate it. Oh yeah? Depending on the private sector. Yeah. Depending on the parameters. And I feel like there's still things to be discovered about that. I've never simulated the critical one. I'll have to try it. When you look at the picture, you think there's probably still something else. Because it doesn't suck at all, but it makes these templates for ages and they sort of really slowly go, and then suddenly, boom, you get it actually to be an entrance. Cool. Oh, cool. No, no. I appreciate it. We should talk later. Okay, so now I'm going to show you another example, but now for something completely different. Here's something quite different, I think. So the line growth, we call this the line growth with a plus neighborhood. And these are the rectangular, these correspond to the rectangular zero sets. So this rule says you need either. Says you need either at least three vertical neighbors or at least four horizontal neighbors to become occupied. And there we have a theorem. The formula for gamma c, we have a, right, Pc is like n to the minus 1 over gamma c plus small of 1, and tau is 1 over p to the gamma c plus small of 1. So they're like essentially inverse of each other. But this result is a little bit weaker than. This result is a little bit weaker than the one I showed earlier for the threshold R bootstrap rule, because this small O of one here allows for the possibility of lower order corrections, like logarithmic corrections. Note that if you take R equal to S, right, just the threshold R line rule, you have gamma C is just R minus 1, which you can Is just r minus one, which you can contrast with the bootstrap percolation, which was asymptotically r minus square root two r for large r's. Very different asymptotic behavior. So as I said, are there log corrections? And yes, we can prove this in one example. Just the two by two rule. The two by two rule. So, this is the, you need at least two horizontal neighbors or at least two vertical neighbors in order to become occupied. And it's, okay, here's one open problem here is to prove or disprove log corrections for general R and S. If I had to guess, I think that the log corrections are likely in the R by R rules, the line, you know, the symmetric box rules, but I don't really. Box rules, but I don't really know confidently which way you're the other for the R by S, the asymmetric rules. Okay, so for the box neighborhood, PC scales like 1 over n for an n by n box. So the critical length is like n, but the time at which the origin first becomes occupied scales like Occupied scales like 1 over p log, 1 over. I have a blank slide here that's to remind me to show you the simulation, which will hopefully give you some guesses as to what's happening. Here's how the simulation looks for the two. It's quite a bit different. Yeah. So there's quite a bit of branching, right? Before you see Before you see a nucleus. And the reason is because it seems like there's a branching process hidden in there, two dimensions. So I'll explain the branching process. I'll explain in the picture the branching process. So the branching process here will give just the upper bound. The lower bound is a bit different. So here's a brand. We're going to give a brand, define a branching process in an n by n box where n is. By n box, where n is a large constant over p. So it's the critical length, it's like 1 over p. And so let's start with just the horizontal line through the origin. I'll have a picture after this to hopefully explain better. Each line then gives birth each time it has a point on that line, each time you see an occupied point on that line. And so it gives birth on average to n times p perpendicular lines. Lines at the initially occupied points. And so this branching process will survive. Okay, this is non-standard use with high probability, but it survives with positive probability as long as C is bigger than 1 and with probability going to 1 as I take C large as well here. So you can make this branching process survive with whatever large probability you like. And then after about log n generations of a branching process, Generations of a branching process, well it's growing exponentially, so then this branching process will have at least constant times n number of lines in it after log n generations. And once you have n lines or n initially occupied sites contained within it, because each line corresponds to an initially occupied site, at least one of those initially occupied sites will have an extra initially occupied site next to it. Initially occupied site next to it, which will start back the infection to the origin along this tree. Okay, and okay, so then this then takes the infection back to the origin in time n log n. It takes about n time to traverse each line, and you have to go about log n generations. So it takes n log n time, and n is 1 over p, so that gives the 1 over p log 1 over p upper bound lower bounds. Want to repeat upper bounds and lower bounds. So here's what I was saying in picture form, which will hopefully make sense to somebody other than me. So we start with the line through the origin. Each time I see an occupied point, I give birth to the line immediately next to it. Now that line has some occupied points in generation two, and they each give birth to lines perpendicular to them. Perpendicular to them. Watch prints repeat. Make generation 3. And then maybe in generation 4, we're so lucky, right? So log n is 4 here. We have two points next to each other. Now these two points will start the occupation of that line. And now this point has the extra point next to it, which will occupy this line, will occupy. Will occupy that line and then back to the origin. That's the argument for the one over p log 1 over p. I don't remember when I started and how much time I have. Okay, so oh, it says there. Okay, good. That's helpful to me. So I have about 10, 15 minutes. I went fast through my beginning slide. Fast that through my beginning slides, so I'll finish on quite a bit. Okay, so open problem for general, even for this special class of rules, for a general zero set Z, does there exist a gamma C such that the occupying time of the origin scales like 1 over P to the gamma C plus smaller one correction? And yeah, this this may still be very difficult even for our special class of rules. Special class of rules. We can give upper and lower bounds in general, but they can be not so close to one another. And yeah, so like we'll see later, this might also be impossible, but I don't know. However, if we let rho be infinite, So, our rule is no longer local. We just count how many neighbors horizontally or vertically in the infinite line through each point. Then we can prove the scaling for PC in some generality. So, this is a bootstrap percolation or threshold growth on a Hamming graph. So, here. So here, I think it's just the, we're just going to consider having rectangles, subsets of Z2, as a Cartesian product of two complete graphs, Kn, Km. So not necessarily square. We may scale different dimensions differently. But it's the same idea. We're given a zero set, and I decide whether a point is occupied or not, depending on. Depending on whether its horizontal or vertical counts exceed those given by the zero set to put the point or not. Except now there's no row, there's no limit to my neighborhood. So if we assume an initial configuration with density p on this rectangle, we want to say that if we scale the dimensions of the rectangle n and m as N and M as polynomials in P, then the probability of filling the entire set, the probability of spanning, coming back to the origin, scales like P to some power. And so to be precise, we let P go to 0 and N and M go to infinity such that log n is asymptotically negative alpha log p and log m is asymptotically negative beta log p. And we call the quantity i. And we call the quantity i of alpha beta the limit as p goes to zero of log the probability of spanning over log. Okay. And due to this sort of hopefully somewhat qualitative similarity to large deviation theory, we'll call this the large deviation rate or the event of spanning, provided the Of spanning, provided the limit exists. And so, of course, we can show that this exists, and we can compute it as the solution to some optimization problem, which I'll try to describe. So, for a finite set B, we let pi x of B and pi y of B be the projections of the set B onto the X and Y. B onto the x and y axes. And then for a finite set A, we define a rho of alpha, beta, and the set A to be the maximum overall subsets of A of the size of B minus alpha times the size of the X projection of B minus beta times the size of the Y projection of B. And so. Um and so, right, yeah, you can think of this as being like the size of V is the energy, and the weighted sizes of the projections is some measure of entropy of the set, which hopefully I'll try to explain here. So, where is this coming from? Well, this is very closely, if you're familiar with like the pattern containment problem for graphs, this is very similar to that. So, for any set B. For any set B contained in A, the probability that the initial configuration contains the set A is less than the probability that it contains the set B. And the probability that it contains the set B is less than the expected number of configurations which up to symmetry resemble B. So, what do I mean? Well, by up to symmetry means in our graph, we can permute. We can permute the rows and we can permute the columns, and then our dynamics will essentially do the same thing. They're unchanged. Whether or not the set will span is invariant to permutations of the rows and columns. And so the expected number of appearances of B up to symmetries is at most some constant depending on B. It's a finite set, it's a finite number. It's a finite set, it's a finite number. Then we have to choose the rows to put the points for B. We have about n to the size of the x, sorry, the x projection would be, yeah, choosing the columns. So we have n to the pi x of b, places to put the columns, m to the pi y of b, places to put the rows, and then for each point, we have to pay the cost fee, right, to put each of the actual points. To put each of the actual points for the points in V. And the CV is just the combinatorial constant taking, right? It's just all of the N choose whatever, you know, K choose whatever's that appear in a computation. Swept into here. And then plugging in our definitions for N and M, their asymptotic polynomial of t to the minus alpha of t squared theta, we get this problem. We get this is the probability. And so then it's just a matter of optimizing. The probability that it contains a set that spans is so if A is the set of finite spanning sets, then any set Any set in A gives a lower bound on the probability of spanning of about p to the rho, which is that maximum overall subsets. And then we just select the best such set. And so I of alpha beta is just the infimum of rho of alpha beta A for all A in the sets of finite sets which span. Right. And so the threshold for the event of spanning is just given by the support of this energy entropy functional, the large deviation rate, i alpha beta. So if alpha and if alpha and beta are such that i alpha That I alpha beta is 0, then the probability of spanning goes to 1. And if I alpha beta is less than is positive, then the probability of spanning goes to 0. So explicit formulas for I alpha beta are not known in general, but for certain General, but for certain examples we can compute them. For example, the threshold R models, right? Just the bootstrap percolation models with threshold R, we can compute them. And so that's this picture here is alternating colors, the threshold R model, this is the support of this phi alpha beta, which gives the threshold for spanning for the threshold R models. So this is the, I think, starting with threshold two. I think starting with threshold two, threshold three, threshold four. Just uh give it checkerboard coloring so you can see out to like the threshold, I don't know, ten or something I put there, twenty. So I'll stop five minutes early and see if you have questions. Any questions or comments? Maybe just to expand on what I brought up. So this is just, this is literally my recollection of a simulation 15 years ago, right? But here is the recollection. So you take the cross model with a critical threshold. So keep it on smaller. So So it's a critical model and and you know we have the normal theory on it. We even know what sharp line is. Let's call it an integral. But I think there's a different scaling. So the observation is this. If you take the size of the cross to be like ten or something, and then you take maybe a thousand by a thousand square, and then you tune P to be the interesting P. To be the interesting beam, just on the computer, you find the beam that's kind of critical. Then the observation is it looks just like your model. It behaves just like this. It sort of grows this network of tendrils rather slowly and then suddenly the two-dimensional thing explodes. Just like this. So we think there must be just a different scaling where you scale the size of the cost together with the other things where somehow The other thing quote, where somehow the critical model ends up behaving like the odds. What are the parameters for the critical one? What are the parameters for the critical one? Threshold, with its range rho in each direction, then it would be threshold rho plus one up to two rho or something. Yes, and I guess one wasn't it but it wasn't. But it was in the spray, it was it was just like just the barely critical one or the other end of the spray because probably the end point but you see somehow like the pictures you have this amount of noise which is sitting there which which you need for the two-dimensional growth but somehow in this scaling by the time it happens the growth matter. And all the nucleation happens with its word lines. Even by my standards, this was extremely vague. Yeah, I mean, so it's Basically, you want to show that you never get a collection of nearby lines which are able to fill, to spam. And so we use a sort of comparison process of marking lines as, say, red and blue according to whether they could potentially be. Can be, it could potentially be spanned in the next step. And it uses the observation that if in this lines process, whenever you add a new line at another step, it has to be added to a grouping of lines that you added at the previous step. And so basically, what you can show is that. So, basically, what you can show is that after a finite number of steps, you run out of places to add new lines. And there's a bit of an optimization because you have fewer large groupings of lines, but they help you more. They give you more help from the outside. So you need fewer extra points that existed initially in order to add them. But smaller groupings of lines are more numerous, but you have to add more nearby points in order to produce a new line, an orthogonal line. And yeah, so you can add new lines to each grouping of lines that you had before. And then, I'm not doing the argument justice, but basically, there's a computation where you show. Computation where you show that no matter how you do this, run out of space to put more, you run out of energy basically to put more lines. It's too expensive after some large or finite number of steps. That's in the bootstrap revelation case. In the line growth case, there's like sort of a parallels argument for path. Count paths very carefully above and log n functions. Can you do any of this in higher ed? Can you do any of this in higher dimensions? Neighborhood growth, because the whole voracious stuff that's not even well defined in higher dimensions in higher dimensions. I mean, yeah, it's well defined. You can still define voracious, but I guess there could be sort of intermediate things that create Yeah, you can get two dimensions of the creation. Any more questions or comments? I could tell you more about this. Okay. Okay. Uh present and after second. After present. I know you then. But there will be some time you make something. Well, you need something that's not anymore. They're like the analog of the. The the analog of font is going to say you're like configuration robots, which is a piece of box or clock. So that's the analysis of the part. Yeah, so there's nothing else to do. The hardest part we didn't look every day was right. I mean, I mean you want to go to the higher point. The maximum is a giant substance point. But basically the middle of the tool scale. So it's a bit maximum. Exactly. The hardest to go on. The hardest people part of it is like the rest of it. It's like in the rest of the breakout room. You kind of get the rest for free. Yeah, that's where it's like a triangle and then I add a line to the triangle. But then this is so this. Yeah, and then most of our paper is about a lot of the paper is about looking at. Since we have sleep for all you can actually get accounts on the same sort of like the shape of a diagram, and then if you have a highest. I am often also. And then we can show that there's a key sequence of the other diagrams. Horizontally vertical. It's all using it. You can show that there are IO data also for continuum defining. Right, well, we are that when cardinality will lead to everything you develop. Right, so like, say I have a set of size two, yeah.  No, it's just I think it's within con with it's the same power but different constants, but it's not sharp. It's like small O and small O. It does depend on the coverage of the version set. Your power depends just on the part now. So so that would be that would suggest That was just a powerful standard for a single gracious set as it would be for the whole family. You would get an upper bound. Right. You just find the smallest set which leads to the full application. I think the problem is with lower bound rate, you want to combine non-voracious growths from non-voracious sets like the talk. And potentially find something more efficient by forming lines. Like, I should probably produce an example. Yes, give me enough time by tonight, I think. Like, if you cook up something artificial, you could probably take the union of my room. Yeah, let me talk the cardinality of the non-gracious sex in your mind's example. Suppose that I have a rule such that two points horizontal or two points vertical will fill lines. But if I have three points maybe in this configuration somehow, it fills space. Yeah, right. So that's not actually one tradition. Well, this would be a. Oh, we want one of minimal targets. They were all of minimal tartanella. You said all of minimal tartan electronic. Yeah, you say one of the minimal cartonella. Oh, yeah, yeah. No, I think I think that should probably be steps up. Yeah, so for the lower bound, so this is the initial point. Minimum partner CL2 is not going forever. So you might be able to get it. You might be able to get a reference to that nice job. And actually, this is just used for the other houses. This is actually a nice one. So, this is exactly your situation where it's necessary and sufficient to have two points nearby each other in order to spam. It's necessary and it's really sufficient. Yeah, so of course the critical scaling is where E to be such that E squared E squared is about one main. So this gives us the right scale. I'm not sure which of what was there. Just the threshold two. R equal to two. Either if you had at least two points in your neighborhood. I'm looking at the cross or negative proxies across two neighboring thresholds to points with two diagonally neighboring points with everything, right? You can ask the more detailed question for what is the problem. Probably the box is a size like P. Exactly. And then the box will fill with positive probabilities. But what is exactly the probability that it fills? This should converge to some number strictly between 0 and 1 is probably some Poisson convergence here. You could also ask a slightly harder question here. That's probably doable by some Poisson convergence because you either have set the Because you either have a set that's just like this, or you might have like, yeah, some two of these, and then there are more complicated suggestions. Great alignment builds like this. I could have points here. Right, great. So there's a lot of configurations to you. There's a lot of configurations. Doesn't the two points with that one fill by itself? Oh, for threshold. Yeah, it's like three points with that would auto-back itself. That's right. And then you could also filter. So probably in the future. No, but you could also have sorry that you could also have points. You're expecting two points in each line. 2. So you're basically looking at this. So maybe this isn't actually targeted until you reach. But you could also ask, like, so what is exactly the time which the original is talking about. So it's basically the viable one. It's a little bit harder. Because you need to know how far these are. The scaling is the same, but it of course deputies. That's a good point. I'm never quite sure whether I'm following. We'll have to ask it as well. So, first of all, I have a global page. Well, how much time do we have now for the next software? So yeah, we're trying to think about the modern level, but I went faster than I wanted to, I think, at the beginning. But it was okay because everybody was set up. So actually that was the right thing. Basically, just in the middle of Oh, even managed to cover the hamming cost. Yeah, I thought I didn't have much time at all to cover it. Because the last time I gave the talk, I had to go all the backgrounds. And I never had time to think about it. That's right. That's typical. Yes, you don't students. If not speeds. But we hear quality more about that. Alright, so okay, so where how does the robot feature here? What are we looking at? So we roll our modified bootstrap circulation, a polluted bootstrap percolation. Oh, yeah, this log result never appeared. No, it never appeared. Did it ever? I mean, did you talk to you about it? Yeah, so I'm going to just ask you what the statement office is. And the only thing I know is an email that I received four years ago when he was looking for a job but certainly had health problems and so on. So I don't know. Where he said that heuristic computations suggest that the growth is. Computations suggest that the cross is p squared divided by log over p. And he says that he has a conditioned word that shows one direction on one side. So if q is bigger, then e squared over a lot of e, then I hope to extend this different level of control. Except this two-layer level controller code, they have two layers building levels. That's it. I spoke to Hanbat. Yeah, he's spread as Hanbat. Suchul Lee's. He's not. He was like, what monster market? The student was Mark, yeah. So I thought Marik was pretty simple. That's like what's up. I guess there will be a lot of things. Because Hombeck said that Mark is pursuing some really strong, there's some limiting process study at the same time. Yeah, that's the other. So that's not the bottom study. Okay. Normal. I just found slightly short PhD. Sure, but if you believe what I get is that the situation is quite a data sharp constant. Almost like that. Basically, all of us have that. And what's missing is family. They reduce it to a specific continuum of notion. And what's missing is sort of classical sharpenation. Yeah, that's what Hunreck told me when they were trying to apply the newer techniques to deep compiletipus. I think they were unsuccessful because the RALANS are like REN. A random or something, right? It's like you have a geometric percolation where you put like arms that have like exponential random lengths, but they're not bound. But yeah, it'd be nice if we could just understand that function should be. Yeah, yeah, so he says that blocking control made of wedges, these parts of themselves make a connection with their Excel stick relations monsters is another type of bottom structure. So a steady microscale is not 1 over P over P. So that's cryptic. That suggests that we call this over. It seems to be just very interesting. But then what's that? Resort HP, okay? So this is the process. So this is the processing path. And then you have a title is where you're