And a colleague at Zai Jinsang and the University of Michigan, very generously funded by the National Science Foundation, the Air Force, Department of Defense, and the Science Foundation. All right, so let me try to motivate what it is that we're trying to do. So the first part was motivated by the question of having, say, it's like a swarm of drones, it's like which go out into the environment, and there's an automation theoretic task which they're trying to achieve. And of course, it's like drones themselves, it's like have natural physical dynamics, and so there is an information track task layered on top of a physical optimization problem. And traditionally, of course, these things are decoupled, and what usually happens is that you do the optimization of the information gathering task, and you assume basically that the drones are capable of executing whatever it is. Executing whatever it is, it's like which comes out of your higher-level optimization. So, the problem, of course, is that in many important applications, there is no separation of time scales, which justifies the separation of a problem into the information layer and physical layer. So, what we're going to try to motivate is to use ideas of information geometry and sort of mechanics to realize the class machine learning algorithms. The class machine learning algorithms as the exact time page flow of a dynamical system on the space and property distributions, and then to unify it's like this treatment in the setting of what I call information mechanical systems, scientific physical systems, whatever you want to call it. And again, it's like the motivation is here. It's like you have these little tiny drones, it's like which the military has, it's like, and it's been deployed, and it's like you send them out, it's like in your men's survey, it's like, particularly, it's like. It's like, particularly, it's like sort of, you know, it's like high-density regions. Imagine if you're going into a building, it's like where there are insurgents. These things, it's like, are absolutely critical. And the problem, of course, is that the easiest way to disable on these strokes is just through a blanket. So, in practice, they move really, really fast. Typically, they move about 10 meters per second, about 22 miles per hour. And as a consequence of that, they're zooming down. And as a consequence of that, there's zooming down, it's like a corridor, it's like it goes past an open door. Somebody have a whole bunch of information that's like there, which you want to react to. And so you're really pushing these physical systems to the limits of their actuation capability. And then you have information which is coming in, which is again, it's like at a speed which is comparable to what they're capable of. So the idea that you can somehow assume that your drone or whatever, it's like physical system can just react, it's like on a moment's notice, to the new information which is given to it is unreal. That's given to it is unrealistic. So these basically speeds violate this notion of separation of time scales assumption. And so what that's basically telling you then is that you want to do the information gathering and coordination of swarms in a continuous way. So you want to treat both optimization props in either a fully continuous or fully discrete way so that they can be combined. So let me sort of briefly talk about some of the ingredients which come into this, one of which is this idea of information geometry. Information geometry, as the name implies, Information geometry, as the name implies, sort of makes use of information geometric tools to describe the manifold of probability densities. And this is in contrast to what people traditionally do, which is to think of these probability densities as some sort of parametric family. And you embed this, it's into the ambient space with the same support. And then the coordinates basically are these parameter values. And one thing which hasn't been said, which probably deserves to be said, is that we need. Which probably deserves to be said, is that when you do optimization, it's like on manifolds, if you're going to do gradient descent, then the geometric structures which come into play are actually important. Things like the metric, things like the connection matter, if you want to implement a gradient method or say a neuter type method. So information geometry then gives you a way to endow these basic probability distributions with some sort of intrinsic sort of differential geometric structure, and then things like the Riemannian metric, affiant connections, and curvature. Connections and curvature turn out to be related to statistical issues like asymptotic efficiency, you'd say, of maximum 100% use. Okay, so it's like we're at a machine learning conference, you probably know what this is. So at least some class machine learning algorithms can be viewed abstractly as an optimization problem. It's like what you think of trying to find some parametric property density function that minimizes the mismatch inside the data. And one way to measure those types of things are divergent. Type of things are divergent or contrast functions, which are asymmetric analogs of distances. And as it turns out, it's like these objects provide a unified way of treating things like information, energy, and entropy. And in addition to that, divergence functions induce metrics and connections, it's like on the information manifold. And then these differential geometric objects yield more natural optimization algorithms because, again, those differential geometric structures are necessary to really define sort of. Define sort of standard optimization algorithms on mathematics. And of course, what I really do is discrete mechanics, which is an attempt to use or to construct differential geometric structure preserving methods. It's like four analytical mechanical systems that regard to Hamiltonian mechanics. And one way to do this is by looking at discrete variational principles. And how all of this is connected to the previous things I've seen. All of this is connected to the previous things I've talked about is that divergence functions of information geometry have an interpretation in terms of generating functions, which are fundamental objects in discrete mechanics. So there's what's called a discrete Lagrangian. And in addition to that, it's like that analogy allows you to impart a mechanical interpretation of some machine learning algorithms, and then to allow these machine learning algorithms to be viewed as time each for those continuous systems. So let me Systems. So let me briefly now talk about what a divergence function is. It's a map. It's like from two copies of the manifold. It's like two non-negative reals. And it has some regularity. And in addition to that, it satisfies some axiomatic conditions. And again, the take-home message basically is that these act as pseudo-distance functions, but they're not symmetric. So if you interchange, it's like take d of xy, that's now should be equal to d of yx. And then from this, it's like you can take appropriate partial derivatives of this object and evaluate it along the diagonal to get the Riemannian metric and a pair of torsion-free dual connections. So if you know something about Riemannian geometry, you know that it's like associated with a Riemannian metric with the Levitic-Viter connection, which is the unique torsion-free metric-compatible connection. And so you have a pair of connections here. Connections are necessary. Of connections here. Connections are necessary for you to define the notion of parallel transport. And the pair of connections, it's like metric compatible in some sense. And what sense do I mean by that? So in the setting of the Levy-Chivita connection, what happens is that if you have a pair of vectors and you parapransport both vectors via the Levy-Tibeta connection, then the Euclidean, it's like inner product, sorry, the Riemannian inner product of that, you know, it's like a vectored pair of vectors, it's constant. It's like a pair of vectors, it's constant. Okay? So, what happens here, however, is that if you have one vector and it's paratransported by the first connection, and you paratransport the second vector by the second connection, then that reminding inner product is constant. And that's the sense in which these pair of connections are dual to each other in some sense. Okay. All right. And of course, it's like in some instances, it's like you do recover the non-treated connection for both of these. For both of these. Alright, so these dual connections can be expressed in the following way. You can think of them as sort of being perturbed, it's like from the usual Levi-Tributa connection by a skewness tensor. And the skewness tensor is going to show up. It's like later on, it's like for any physical inverse problem, which I'll talk about. So let me say a little bit about perhaps the most common divergence functions which show up. Most common divergence functions are what Most common divergence functions are what are called Regnant divergences. And they're basically constructed given a strictly convex function that essentially tells you the difference between the function value of that strictly convex function versus the sort of the linear approximation of the convex function expanded about in a point. So if you think of, say, it's like a quadratic, it's like you have one point and another, you Point and another, you take the linear approximation of the function at one point and you compare the function value with that linear approximation. That's what the divergence function is going to give you. And so perhaps the most commonly used ones, it's like the Koblik-Leibler divergence. And as I said, this is an example of Reckman divergence. So there's an associated convex function. The associated convex function is the negative Shannon entropy. And then as we discussed, And then, as we've discussed, a regiment or divergence induces a Riemannian metric. And in this case, it induces the Fischer-Raule metric. So, all these things are nicely connected in this regular. Okay, so now let me say a little bit about geometric integrators. So, these are basically numerical methods which preserve its various geometric properties. Examples of plasticity, momentum maps, or the manifold structure, it's like on which the dynamics evolves upon, say, the Evolves upon, say, the deeper homogeneous phase structure. And the usual motivation for studying these things is that they result in miracle assimilations which have long-time irreparable stability and better qualitative agreement with the exact flow. So one way to construct these type of methods in context of mechanics is to view, it's like the problem from a Lagrangian perspective, where if you have an initial point and the final point, then the trajectory which The final point, then the trajectory which connects it, like those two points, is that which extramizes the action integral, which is the integral of kinetic mines of potential energy. And then, one way to descriptize that is to imagine it's like taking that curve, and then it's like replacing it by a sequence of samples along the way. And then you introduce what is called the discrete Lagrangian, which is, you should think of, a piece of the action integral, it's like, which is evaluated connecting two nearby points. You know, connecting to nearby points. Okay? And there's an object which you can at least formally write down, which is what you want to approximate, which is the exact discrete Lagrangian. So it's the action integral which connects those two curves. It's like that's evaluated along the solution of the Oll-Lagrange boundary value problem, which basically means that you have some curve Q01 in this case, which has the appropriate boundary conditions, Q0 and Q1 at the two endpoints. Q0 and Q1 at the two endpoints, and along the curve it satisfies the Golden Grandpars. Okay, so this is related to Jacobi's solution of the Hampton-Jacobi equation. Of course, this is one of those chicken and egg things where if you could compute this, you really needed to have solved it's like all the branch functions to begin with. So, you know, it's like, yes, it exists, but it's hard to compute. But it's there. So once you have this discrete Lagrangian, you can sum it all up, and you can then take variations and set it to zero. And set it to zero when you keep the endpoints fixed. And then this gives you a discrete model Lagrange equation, so which I've given here. You can either view that equation as a two-step method in position or as a one-step method in position and momentum if you introduce inside these momentum variables. And if you look at that expression, that's really nothing more than the characterization of a symplectic map in terms of type 1 generating functions. So the discrete Lagrangian really is just a type 1 generating function of the symptom. Is just a type 1 generating function of the symbol. Okay, so what's the advantage of using this approach? One of the advantages is that it exhibits a discrete version of Nofus theorem. So Nofist theorem relates symmetries of the problem with conservation laws. And something similar happens here. Discrete Lagrangian, which is the infinitesimal group invariant under the diagonal action in the following sense. Then there's the discrete momentum that which is preserved by the discrete model of discrete model Lagrangian functions. Gravity functions. And the other thing is that there's a nice connection that's like to the error analysis. So instead of analyzing the numerical method, it's like towards order accuracy. All you have to do then is to analyze the ability of your computable discrete Lagrangian to approximate this exact discrete Lagrangian, which I talked about before. And so if you can show that the computable discrete Lagrangian approximates the exact discrete Lagrangian plus in order of accuracy, then the corresponding numerical method you get out. Corresponding numerical method you get out of this discrete Grandian as the same order variables. So, so in both cases, it's like if you want to analyze inside the conservation properties, you check, you verify properties, it's like a discrete Lagrangian, which is a scalar value function on two variables. And if you want to check the order accuracy, you're doing your validating properties of this scalar value function. So, typically, that's much easier. All right, so this stuff has been applied to multi-body systems as well as a Systems as well as continuum systems. So, this is stuff due to Tian Lee, who was a former student at George Washington. And this is applications to biomechanical systems. He's like taught RPA Northwestern. And then Ethan Greenspan, I think, now is in Canada somewhere. And he's applied this insight to various container mechanics problems. All right, so that's sort of a quick introduction insight to geometric integration. So, one of the things which was discussed yesterday was this idea. Discussed yesterday was this idea of neuro differential equations. And so, one of the things which is important to realize is that if you have any differential equation, even one without a variational or Hamiltonian structure, if you take the adjoint of it, then that system, the adjoint system, is Hamiltonian. And so, one of the things in particular is that if you look at sort of deep neural networks which come from discretizations in time of a neurodifferential equation, then you Of a neurodifferential equation, then you can think of it as starting with the neuro OE, discretizing in time, and then it's like you do back propagation in that, which is basically solving the adjoint system. So you can ask the opposite question, which is if instead of that, I first take the neural ODE, write down the continuous adjoint equations, and then discretize, under what circumstances it's like, does that diagram commit? Okay, so Sincer, it's like observed in this really beautiful paper, it's like in Scien Review from 2016, that this That this commutes, it's like if the discretization at the bottom of the adjoint system is hematomic, is symplectic, and it covers, it's like the discretization of just the original differential equation. So amongst other people, Braniov and Elena, it's like then applied this to think of deep learning. It's like it's an optional control problem. And so you can use this to Use it's like this to you can use this idea, it's like to basically use essentially symplectic integration of the engine system, it's like to train, it's like deep neural networks which are derived from hyposcretizations of neuro ODEs. And then one of the things, it's like Brian Tran, who should have been here, it's like, but got caught up, it's like with the notice, it's like Los Alamos employees are required to give. It's like when do you travel outside the US? The US. So he and I worked on the generalization of this incentive itself. It's like two index one differential algebraic equations. So if you take an index one DE, it's like in principle, you can reduce it to an ODE. And you can then ask, it's like now in this sort of cubical commutative diagram, take an index 1DE, then you discretize it versus taking the edge joints versus reducing it to the ODE problem. It's like reducing it's like to the OD problem. How do you get that cube? It's like diffuse. Okay, so in this paper, which is now in the Journal of Nonlinear Science, we've filled in every single diagram, narrow in that diagram, so that everything works together. So one opportunity potentially then is to have neural sort of DAEDs, right? So maybe that's the right way to think of various sorts of physical. Various sorts of physical form neural networks. It's like, but opportunities arise for that. Okay, so that was an aside. It's like about some applications to agile systems. So let me say a little bit about the problem of how do you actually go about constructing one of these sort of variational entries. Excuse me. So, one way to do this is to think of this exact discrete Lagrangian, not as the action evaluated on the Action evaluated on the other Lagrange boundary value problem, but to think of it, it's like in this variational way, which is that it is the action, it's like the arc extramizer, if you will, of, you know, it's like the action, subjected condition that you satisfy the boundary conditions and you have a curve which is at least twice differentiable. Okay, and it's sort of fairly clear that the stationarity conditions for this are just Bill Lagrange equations. So that reduces to the previously Reduces to the previously sort of discussed formulation for the exact discrete Lagrangian. But looking at it this way, then it naturally leads to a class of geometric discretizations where you replace, it's like this family of twice differentiable curves with some finite dimensional function space, and where you replace the integral with some sort of numerical quadrature function. And then it's like we have certain additional results which say that from that perspective, if you have Perspective, right? If you have, it's like this function space and it's parametrized in such a way that it's group equivariant. What I mean by that is imagine you have a bunch of data, right, and you have a way to construct it's like an element of the approximation space out of that parametric representation. And you take that data and you transform it by some sort of group action. And the new element of the function space which is induced by your parametrization is related to the old one by the same group action, right? By the same group action, right? Then it's like, and you apply this to construct a discrete Lagrangian, and that continuous Lagrangian, which it's built upon, has the group invariance property. And all those things together will give you a discrete Lagrangian, which is group-invariant, and as a consequence, it will conserve the associated conjugate momentum. Okay, so doing this, it's like a finite sort of dimensional setting is not so hard. It's a little bit more complicated if they try to. Hard. It's a little bit more complicated if they try to do this. But it can be. Alright. So the other thing is that you can do error analysis. And so one of the things is that you can show sort of quasi-optimality of methods. And so essentially what happens is that the numerical error, it's like which you get is related to, you can show it's bounded by some multiple of the best approximation error. So that reduces the problem of constructing good variational queries. Uses the problem of constructing good variational operators, the problem of doing sort of a priori error analysis, it's like for your problem, and then appealing to approximation theory to say that in this regularity class of solutions, you know, that's well approximated by some appropriate choice of function species. So this involves refining a proof of gamma convergence, which was due to Michael Ortiz and Stefan Mueller, in order to show that these methods are auto-optimal. And then you can construct spectral versions of this, which are. You can construct spectral versions of this, which are geometric versions. Okay, so now let me connect all this stuff back to information geometry first. So the claim is the following, right? You have a divergence function, and the divergence function induces a Riemannian metric. So one natural question to ask is, well, what is the geeseic flow associated with that Riemannian metric? And is there a sense in which the divergence function, viewed as a generating function, is going to give you a good approximation of the To give you a good approximation of that geodesic flow. So, as it turns out, okay, so the statement is like then more precisely is that you have this exact discrete Lagrangian associated with the geodesic flow, where the geodesic flow is induced by, which, what, the geodesic flow is related to the Riemannian metric induced by the divergence function. Okay? And this turns out to be well approximated by 1 over h multiplied by the divergence function. Multiplied by the divergence function. And so you end up with an error estimate like this, which in the setting of this variational error analysis says that if you take 1 over h times the divergence function and view it as a generating function, then that's going to be a quadratic approximation of the geodesic flow. If the divergence function is a Breckman divergence, and then if it's not a Breckman divergence, If it's not a breakman divergence, you still get some sort of approximation. It's only going to be first-order accurate if it's not a breakman divergence. But in particular, for KL divergence, for example, you get a second-order accurate approximation of, it's like the fischer-brow, it's like geodesic flow, if you use this construction. Okay, so that's one direction, right? And then the other direction is if you give me a Riemannian metric and a pair of sort of dual affine connections. Dual affine connections, it's like in the sense which I talked about. Can I construct a divergence function which induces those things? And of course, that's not unique because the construction of going from a divergence function to those objects only requires some information about basically the first and second derivatives of the function. And so there's sort of an infinite family of them. But one way to do this is to look at the habitat-Coby theory for the folding of the Brownian. Folding the Lagrangian, where the Lagrangian is defined in terms of the metric. So we construct a kinetic energy term, it's like a metric, which is what you might expect. And then the skewness tensor, which is the difference, if you recall, which is related to the difference between the sort of, you know, each of those dual FI connections versus the Leviticus grid connection of the reminder metric. Okay, so this gives you sort of a potential-like term, and if you And if you solve for the generating function associated with this, then you will get a divergence function which will induce the prescribed metric in this pair of dual connections. And of course you can construct a reasonable approximation of this using the kind of variation integration technology which I talked about before. And it suffices to construct something which is second-order accurate in order for it to be. In order for it to be one of these canonical divergences. Okay, so now let me sort of talk about. So the Tensor, is it? It's tensorial, that the T-shirt. Yeah, right. It's a tensor. Yeah. So it's... Point-wise that's a Christoffel symbol? It's not quite a Christoffel symbol because you'll see that it doesn't have the right. Yeah, I mean, it's like it's not the right type of tensor, right? Yeah. Textbook, right? Yeah. Okay. So, all right, so where was I? Yeah, okay. So, so the question is, how do you incorporate measurements and data? So, one of the things which Molle talked about was this idea of looking at dynamics and then relating it to optimization, right? And obviously, it's like another way to think about various machine learning algorithms is in context optimization. Okay, so if you're So if you recall, it's like if you're in this framework where you're trying to fit some property distribution to the data, then what you want to do, obviously, is you want to minimize the loss function of the mismatch between the property distribution and the data. And you want to regularize that dynamics by introducing some sort of momentum term. So the claim is that you can do this by writing down a discrete Lagrangian, which looks like this thing, where the x's are the model distribution. Where the X's are the model distributions. These are sort of your guess as to what the dynamics or sort of what the distribution is. And then the Y, it's like some sort of observation of this unknown distribution. Okay, so the second term here, it's like it's like a potential function, which measures the mismatch between your estimator and reality. And then this term here looks like basically a momentum term. It's like sorry, actually a kinetic energy-like term here. Sorry, actually, a kinetic energy-like term here. Okay, and then there's some sort of time scaling which you have to include in order to get worked up. But the claim is that if you, you know, it's like if you look at dynamics along this line, it's like then you look at something which tries to, you know, it's like minimized mismatch, it's like between, it's like your estimator, it's like, and reality as measured by the divergence function, while regularizing it, it's like by having a kinetic energy-like term. So it's a momentum-like version of this type of problem. Of this type of problem. So you can either work directly, it's like with the discrete Lagrangian and then use sort of discrete mechanics, it's like to model the various optimal control parts of the problem. Or you can, in principle, do something like backward error analysis to then view this as some sort of continuous time problem. In either case, it's like that allows you to put both the physical dynamics and the sort of this information sharing or optimization problem on the same footing. Problem on the same footing, either both in discrete time or both in the continuous footing. All right, so in the remaining few minutes, it's like let me then switch gears and then talk about discrete mechanics and simulator optimization. Bully has really done a fantastic job saying all this, so I will just quickly speed through this. So you have an optimization problem. It's like auto-convex domain of a convex function, and it has a unique minimizer. And then Nestoroff, it's like back in the 80s, introduced what is called the accelerated gradient method, which looks like the following. And the main point. Looks like the following. And the main point is that if you look at the rate at which it's like the cost function sort of converges, it's like to the minimum value, it's like along the trajectory, then it's like converges to big old 1 over k squared, where k is the number of iterations. And the main reason why this was significant was because if you look at vanilla gradient descent methods, they only converge at big O1 over k. And then Nestraff proved that this big O1 over T is. That this big O1 of the k squared rate is optimal for methods which only use information about the gradient and consecutive iterates. And again, I want to point out that one should be careful about this variant theorem, because this variant theorem is not a statement about the asymptotic gradient for dimensions, but it's about what happens in the first large number of iterations where, which is roughly comparable to the dimension. So this doesn't preclude the possibility that once you get past that initial That once you get past it's like that initial transient, that you can converge much faster, big old 1 over q squared. Okay. So, anyway, so one way it's like, you know, it's like, so there was a lot of interest, it's like constructing accelerated methods, and then one of the things which was done is like to try to do this. Again, something which Wolle alluded to, is the idea of looking at Nesborov scheme as a continuous time dynamical system or as a discretization of continuous time dynamical system. Discretization of continuous time dynamical system. And so Sue Boyden-Kindas, it's like introduced this continuous time limit. And then it's like Michael Jordan, it's like his collaborators, it's like introduced a Lagrangian-Hamiltonian formulation, which is time-dependent. So that's an important thing. And the time dependence allows you to have Lagrangian Hamiltonian dynamics, which is not energy preserved, because energy is noble quantity associated with time. Energy is the novicity associated with time transformational symmetries. And if you look at this expression, it's like you'll see that the divergence function here plays an analogous role to the one I have talked about before, because this is the divergence function that's like between sort of a slightly perturbed but nearby value of x and x itself. So, as I sort of alluded to before, this is basically a kinetic energy line term. And then it's like the second term here is obviously just some appropriately rescaled. Appropriately rescale function value inside. So that's playing the role of the momentum. Okay? All right. And then this is defined in terms of the fragment divergence function. Okay, so as I said, divergence function provides an approximation of kinetic energy, and then the function which is minimized serves as the potential energy. So there's sort of an analogy, if you will, it's like between what you see here and what had sort of suggested before. Okay, so under certain growth conditions, you know, the flows and the associated oil growth. The flows of the associated Euler Lagrange equations, the continuous time flow of the Euler-Grange equation, converges at this rate. And then, if you choose, it's like alpha, beta, and gamma, it's like in this way, then you can show that the Olagrange flow of the associated Brandon congratulates Hamiltonian converge to Delta value, that big old 1 over t to the p for any p. Okay? All right, so that's where we're starting. So, we're going to, you know, it's like, so you hear Hamiltonian and Lagrangian, it's like any geometric integrator, it's like working in some collective integration. First thing you do. First thing you do is, well, can I, you know, it's like use my big hammer and stuff on this problem. So that's what we tried to do. But the problem, of course, is that the fragment blockchain Hamiltonian is time-dependent. And if you naively apply this applied overation equation, such time-dependent problems, you can times get very, very important results. Okay, so what do we do instead? Okay, so it's like you teach a differential equations class and you have something which is non-autonomous. The first thing you do is you go into extended phase space and make it autonomous. So that's what we're going to do. Make it autonomous. So that's what we're going to do. Okay, so given Hamiltonian, and I'm going to add an additional possibility to rescale time. And so you have a time rescaling in terms of a monitor function, which is the structure GP. Then the new Hamiltonian system is given by what's called a Poincar√© transform. This was something which was used by, amongst other people, Ernst Tyrrhen back in 1997, I guess. It's like to look at time-adaptive simple. Have time-adaptive subtracted integrators. So you get the following object, and then this is an extended phase space, and you have Qt and Pt, which are the extended phase space variables. So Qt is basically a copy of time, and then Pt is the image thing. All right. So the problem with this is that this Hamiltonian is degenerate. So there's no Lagrangian analog which exists for this problem. So if you're a variational degrader person, you might think, well, it's like a block of left. You might think, well, that's like a round up left, but as it turns out, it's like a couple of many years ago, it's like he developed a Hamiltonian analog of variational integrators, which works purely in the Hamiltonian side, and so that's what he applied to this. Okay, and the basic idea behind it is that you start, the way you construct these Hamiltonian variation integrators is the same way that you take the exact formulation of the type 1 generating function, right, and then you massage it to get an exact type 2 generating function written in terms of the habitat. Okay, so what you do is that Hamiltonian. Okay, so what you do is that write the Lagrangian in terms of the Hamiltonian. L is pq dot minus h, that's fine. And then you do the Lagrangian transform to go from type 1 to type 2. And then this is what you end up here. So this is the exact type 2 generator function for Hamiltonian system. And it turns out that this is perfectly valid, even if the Hamiltonian is in general. And so that's basically what we do. And then the only thing which is perhaps interesting is that now, in principle, you have a curve, it's like from principle you have a curve it's like from you know some time interval to t star q cotangent bundle but as it turns out that all you really need to construct something which makes sense for this uh is an approximation space for q and not t star q okay so so you can prove that you get the same method it's like using this kind of riddle construction by choosing a finite dimensional function space that's like for curves in q and a quadrature rule where it's like those two And a quadrature rule where it's like those two methods you get on the Hamiltonian side and the Lagrangian side are equivalent if you have hyperregularity, which allows you to go back and forth between the Lagrangian and Hamiltonian side. So that's kind of interesting. All right, so anyway, so now let's apply that idea to this discretization of the Breckman-Hamiltonian. And one thing which is interesting is that for this family of P Breckman-Grantens and Hamiltonians, it's like there is that. That family of curves represents the same solution curve up to a time-grade parametrization. So the actual curve itself is the same, and the only difference is how fast you go along that curve. So what you can do is to take advantage of the fact that we had this ability to rescale time. It's like in the Concrete transform Hamiltonian. And so I want to imagine going from some, you know, it's like small p to a larger p. It's like small p to a larger p, actually the other way around. And so that allows me to integrate its higher order, it's like p regular dynamics with the computational sort of efficiency of actually just implementing a lower order sort of regular dynamics. And so with that rescaling, it's like you get the Holly monitor function, okay? And then you can substitute that into this concrete transformer, you get this explicit expression for what happens. Okay, so you can. Okay, so you can apply this as a discretized sort of optimization algorithm, and you can sort of see that some of the time-adaptive methods are much more efficient and converge faster. So you can compare this to a non-symplectic adaptive RK method using the direct versus indirect approach. And then it's like the upshot basically is that adaptivity doesn't really buy you anything, it's like you can be sort of. Really buy you anything. It's like sort of archae formulation because, in some sense, it's like you're just sort of spending all your time trying to take this optimal step when you could have just, you know, it's like just as easy, chosen the optimal step from this known V scale problem. And so final sort of proof of the prediction eating right is compared to the Nastross accelerated gradient method. And so this red line here, or orange line, is the Nestoff external. Or orange line is the SNF accelerator gradient method. And then you'll see this is our Hamiltonian sort of Taylor variation of the grader. It sort of takes a little bit of time, it's like to get up to speed, but then it's like it converges faster. So it does converge at a rate which is faster than Lesterov. So it's it's faster than one over k squared, but this is the SR dot of average. So what what kind of function are you? Uh I don't quite remember. I don't quite remember what was the problem. It was convinced, I don't, yeah, but I don't remember what the problem was. But I think this was the tolerances, it's like, which was necessary. So this was the number of iterations to get to the 10 to the minus 9, 10 to the minus 10. It's like of the actual optimal value for the various methods, and the number of iterations into it. So that's drawn to 1.6 million. Like 1.6 million iterations to get there. And our adaptive method took 4,000. So admittedly, it's like lots of. Sorry, but if I do this Newton method, it's a complex problem. How many iterations would I need for a Newton method maybe with an ice match? I don't know. Five? Probably so so we're comparing ten thousand iterations or a five iteration to a Newton's method or something? Sure, but you can't do Newton, it's like on a high-dimensional problem, right? Sure. Yep. Yep. I don't know. But you need uh a good starting point for that. Yeah, if I do in its argumental and business, they could do similar things. Globalization and trust regions, all the stuff that we know. I mean anyway, it's like, yeah, I don't still answer your question in fedness. So we didn't try that. That's what we're not getting. But, you know, definitely a reasonable thing to bring up. Reasonable like to bring up. Okay, so you can extend this to Riemannian manifolds. It's like that's what Wolley alluded to. So in the convex and we believe quasi-convex case, it's like you have these, you know, it's like sort of Bregman-motivated Gradians and Hamiltonians. And then this is the rate of convergence. It's like estimated what you get in continuous time, and then in stronger complexities, you get these functions, regardless of time we'll get, and then again with this kind of radioconvergence result. And then you can apply. And then you can apply this, it's like, to the generalized eigenvalue problem and the sort of unbalanced or talking of Sprudis problem. It's like this was on the Stiefel manifold. And this was stuff which was actually with Mulley, it's like, and Team Li, it's like you applied this to B groups. And this was a camera pose estimation problem. Okay, so in the remaining one or two minutes, let me sort of just In the remaining one or two minutes, let me sort of just tease you a little bit by saying one of the things which came out of this way of thinking about optimization algorithms as digitizations of their systems is to look at a different way of dealing with constraints. And this is motivated by sort of a collaboration that's like Qualcomm, where we're applying this method to try to do constraint optimization of VLSI chip designs and trying to replace its components, it's like a chip. It's like a chip. And the basic idea is that it allows you to deal with collisions. It's like sort of non-smooth, convex optical sets. So imagine you have things with sharp corners. It's like they're bouncing up on, say, a surface. And if you wanted to formulate this, it's like traditionally, it's like what you would have to do is to do write things in terms of differential inclusions. But a problem with working with differential inclusions is that when you actually want to go and implement this, you have to choose an element. Implement this, you have to choose an element, it's like of the appropriate code, or you know, it's like and that's essentially adds a random element to it. So, so we did sort of really a very naive, it's like regularization. So, we look at the level set of the function, and instead of evaluating the level set at zero, evaluate it at epsilon. And that has the effect of curving out edges. And anyway, it's one of those, it's like where the proof is meaning. So, this is a simulation, it's like of a It's like of a, you know, it's like a cube, it's like with sharp corners, okay, which under our regularization. And what's happening, and what may be obvious after one or two more bounces, is that this is not one cube, it's two cubes with very, very slightly different initial conditions. All right? So this is a fully deterministic compilation. So everything is repeatable. There is no randomization in this. But it captures the exceptional level of sensitivity, it's like of the problem, right? Problem, right, without using any randomization. So, anyway, so we're planning on using this, you know, it's like to guarantee that there's no interpenetration, it's like of the cells, it's like what you do to the chip placement. And so that's our way of trying to do sort of inequality constraint satisfaction. All right, so let me summarize by saying that information geometry provides a differential geometric interpretation of a certain class of machine learning algorithms. Of certain class machine learning algorithms, and the divergence function induces various differential geometric spectrums and manifolds and are related to discrete Lagrangians, and that machine learning and optimization algorithms are related to discrete mechanical systems on the space of probability distributions. And we've talked about two optimization algorithms based on Breckenridge-Hamiltonian dynamics and the geometric viscatizations using time-adaptive Hamiltonian variational degrees. And the hope basically is that we can leverage this analogy. And we can leverage this analogy between mechanics, learning, optimization, and optimal control to cross-specialize and apply various techniques of geometric mathematical integration to obtain new interesting classes of our workflows. So let me just stop here. Thank you. Um I I couldn't see well the keynote which joins. But the implication is that for if you're using branding integrators, then oh yes, yeah, that um yeah, so so there is the result uses ancerna which says that if you discretize and then you take the adjoints, right, that communes, yeah. Uh Use. Yeah. And it's related to things like, you know, it's related to the optimal control questions, like direct versus indirect method optimal control. So there was a result by Ferru and Ross, which basically says that you have to look at the cotangent lifted object in order to make that diagram new. And that's in the same spirit, right? Because one way to sort of think about the adjoint system is that it describes the cotangent lifted curve, it's like associated with the dynamics. It's like associated with the dynamics, it's like obviously. And that is infinitesimally Hamiltonian. So there's a Hamiltonian theory associated with it. But if you do index reduction, then the point is that if you do index reduction and then you take the adjoint of that, versus taking the DAE and you take the adjoint system, that the adjoint system of the index, oh, sorry, of the Of the index, I'm sorry, of the index 1DAE is not symplectic, but it's pre-symplectic. But it's essentially a Hamiltonian system, but with a slightly degenerate structure. And so you can make the whole diagram. So you need sort of a pre-symplectic integrator to discretize that object. But anyway, so there's a description of how you do everything so that that director programs. So that that director provides all of the date.