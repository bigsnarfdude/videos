Thank you, Dr. Hongdali, for the introduction. Okay, so as I said in the morning discussion, I feel like I'm an outlier here because I didn't use deep learning methods so far. But I hope to offer a different perspective by also providing a benchmark tool so everybody here can hopefully use. And also, I hope to encourage, I will receive your feedback throughout my talk so I can see where we can do better for making this more friendly and more comprehensive. So I listed three uses of. So, I listed three uses of the simulator I'm going to discuss. So, we intended to make this one comprehensive so it can cover both single-cell sequencing simulator and spatial omics simulator. And we want the simulator to be used for benchmarking many, many computational methods and statistical inference for learning parameters from the data and also for doing ancillary controlled experiments. And for the third part, I want to emphasize it more as my talk goes. And as my talk goes, so I'm from UCLA, and these are my students' work over the years. All right, so first of all, I want to just start by talking about single-cell and spatial omics data characteristics from a statistical perspective. So the process data is usually a cell by feature matrix plus some cell covariates we want to do modeling for, as we have seen from the previous talks. So for the cells, we have So for the cells, we have several heterogeneity structures to infer. First, discrete cell types. We can do this by cell clustering, or biologists can label cell clusters as cell types using their known marker genes. Or the cells may follow a continuous trajectory, as we have heard from Jean Schustalk this morning. Or we can measure the spatial locations of a cell, their spatial heterogeneity. And the data may have And this data may have come from different experimental designs. They may be measured in batches, and the batch effects are unwounded. We want to remove them. Or they may be measured from different conditions. Here, the condition differences are the biological signals we look for. And the features can be diverse. The most common ones are gene expression features, right? One gene of feature. And we can measure those from sequencing, RNA sequencing, or spatial transcriptomics by imaging or by sequencing as James said yesterday. Or by sequencing, as James said yesterday. And we may measure chromatin accessibility features by single-cell tax-seq or snare-seq, and we may measure protein abundances. So we're measuring more and more, and that's what we call multi-omics. So the motivation of our line of research, this line of research, is to first do computational benchmarking. So from this website, we can see that more than 1,000 computational tools have been developed for single-cell data. That's amazing. And as a users, right? And as a users, right, how can we choose among different methods? And that was East question in the morning discussion and very important question. And for the inference, because I'm in the statistics department, we always talk about inference. So here the inferential question I think is of interest is that conditional on a certain cell covariate, maybe a cell type, cell pseudotype, or cell spatial location, how can we infer every gene's distribution? And how can we infer every gene's pairwise correlation? Every gene pairwise correlation. So these are the parameters of the cell population we're interested in. And finally, how do we do incello control experiments? We know that in wet labs, the control experiments are must. We must set up a negative control to figure out whether our experimental procedure leads to false discoveries. We sometimes need a positive control to know the ideal situation, and our experiments will be in the middle. But in computational But in computational pipelines, we usually don't have these in place. So, to address these issues, I think we need a realistic simulator with interpretable parameters. Okay, so first I want to begin my talk by talking about teaser. That's not about single-cell omics, but it's about a recent finding we think is of great importance. So, this is about a very common analysis called differential gene expression analysis, and we know the two common. We know the two dominant methods in this field are DC2 and HR. So, why won't we use this example to voice the importance of benchmarking and in silicone negative control? So, this is my joint work with a computational biology lab led by Dr. Wei Li at UC Irvine and with his postdoc, Dr. Yumei Li. So, actually, I want to say that our work was online in March, but we can see we have a large number of accesses since the publication because people are interested in this topic. interested in this topic. So just briefly, the two methods, HR and DC2, they have a very similar statistical model. They assume every gene follows a negative binomial distribution under each condition. And because they were developed for small sample sizes, so back then, usually biologists only generate three replicates per condition, as this heat map shows. So every column is a gene and every row is a sample. So if you want to compare three values. So, if you want to compare three values to three values, we know the test will not have good power. So, therefore, they use this statistical technique called empirical bias to borrow information across genes so they can achieve good power. And they have been widely used ever since. So, we figured this out by accident. So, we'll apply these two methods to a large sample size data about immunotherapy. We have patients in two groups, pre-treatment. In two groups, pre-treatment and on treatment, and compare, we try to compare these two groups of patients' RNA sequencing data and so to see which genes are differentially expressed. So of course, we can see that DC2 and HR has several findings, hundreds of findings. But when we just did this random permutation of patients between the two conditions and run these two software again, we surprisingly found that for many times they can find more D-genes from permuting. Find more D genes from permuted data. And that shouldn't be right. So we look into the reason and found that the most important cause is the model misspecification of the negative binomial model for gene expression distribution for this population data, because we know human individuals are not replicate. So therefore, the fundamental assumption of negative binomial no longer holds. And we can see that if we divide genes into two groups, those that are rarely found from permuted data. Rarely found from permuted data, and those that are frequently found from permuted data. So, for the second part, the frequently found permuted data, which are likely false positives, for them, the negative binomial distribution has a very poorness of fit. So that's the finding. So what do we learn from this? What's the scientific consequence? So if we trust the D genes, these two methods found from the original data, and we run the gene ontology enrichment. We run the gene ontology enrichment analysis. We could see that many immune-related genes are enriched in their D genes. However, we know these D genes are also found from permuted data. So therefore, we found that if we trust the computation analysis, we may give some scientific conclusions that are not well supported by data. So we should be cautious. And if we do this same analysis using a very classical textbook method, Will Coxer ranks some tests on the right. Wilcoxon rank sum test on the right. We see that Wilcoxon finds nothing from original data, and Wilcoxon also finds nothing from permuted data. At least it's consistent. So therefore, we want to say that this is a choice, right, between popular bioinformatics tools and classical statistical methods. We look into the literature and we found that there were at least four or five large-scale benchmarking studies of the methods, but none of them included Wilcoxon. Of them, including Wilcoxon. It's not even in the consideration because it's a textbook method, not a bioinformatics tool. But we want to say that when we benchmark our new methods, we should go from the classics. That's the message. And I want to say that this is a very short introduction to this whole study. So if you are interested in this, you can look on my Twitter because we have some ongoing discussion about this issue. I think it's very informative. Anyway, so that's the teaser. And let's back to, let me get back to my theme: the SED design simulator. Theme, the SE design simulator. So, the whole story started from my former student's work, Vivian's work, and we published this in 2019. So, we intended to have a statistical simulator that has interpretable parameters, and we want to formulate the simulation problem by treating cells as observations or instances in the machine learning language. We want to treat genes as features. So, this can allow us to vary the cell number, and also we can allow the And also, we can allow the sequencing depth as a parameter into our model. So, we can change the sequencing depth and evaluate the data. So, that can allow us to use this simulator for guiding experimental design. How many cells to sequence and how deep can we sequence? Should we sequence? Okay, using the simulator, my other former student Miles just did this benchmark study of the computational doublet detection method, and we have some very conclusive results about which methods are recommended. Results about which methods are recommended. So everything seems okay, but I knew that SA design has cons. And actually, this was a question asked by Hong Kai, who's next speaker at a conference back then. So one con is that SD design treated genes as independent variables. Therefore, gene-gene correlations were missing. And also, SC design did apply its modeling log transform count data instead of count data directly. And we want to fix this. To fix this, so thankfully, I have a student Tian Yi who continued this line of research. And we did a literature review before Tian Yi continued the simulator research. This is a summary of the exemplar simulators in the field and not necessarily complete. But I want to say that they represent different properties. And we try to provide a summary here, whether they are adaptive to different protocols, which means that can they learn from real data or they are purely theoretical. Are purely theoretical from their assumptions. That's a difference. And also, do they preserve genes, actual genes, or they just simulate some synthetic genes? And third, do they preserve gene-gene correlations? As I said, our assay design didn't do it. Do they allow cell number to vary or do they allow sequencing depth to vary? Not necessarily, because some simulator treats cells as fixed instead of random observations. Are they easy to interpret, which means that are their parameters interpretable? means that are their parameters interpretable that's another part and finally are they efficient to train and use so using those properties we see that there is still a gap for a simulator that can achieve them all so motivated by this and i have to say that this sp simce and also xcode these two methods are most related to our simulator acid2 but we i will say we are kind of like simultaneous development and Simultaneous development. And one key difference between our method and these two is that SPSIMC doesn't allow varying sequencing depth. And SCO will require an input of every gene's marginal distribution from an external simulator. So this doesn't do the learning directory from real DM. So our simulator don't have these restrictions. So to put it simply, we started from a real count matrix. So here the columns are counts, rows are genes. Counts, rows are genes. So we will divide the cells into K cell types or clusters. And for each type, we will build, we will train a joint distribution for genes. So from this joint distribution, we can sample synthetic new cells. That's the big idea. And during the generation of new data, we can allow the sequencing depth to change and we can allow any user-specified number of cells. So using the synthetic data, users can run any Metadata, users can run any pipeline of their choice to see how the pipeline works by computational method, or they can change the sequencing depth and cell number to see how the performance change. And that's experimental design. So let me just go to the technical part for here, because I know most of our audience are computational. So here, let me start with this matrix P by N, P genes, N cells. Let's assume that the cells belong to K types, capital K types, and each Capital K types and each type K has this NK cells, so I have a submatrix for type K. Our goal is to fit a parametric probabilistic model of all genes expression for type K. So for the simplicity of my notation, in the following slides, I will focus on one cell type and drop the K notation. Okay, so we start with the marginal distributions of genes and want to model the counts directly. So let's say that I will refer. So let's say that I will refer to this as the vector for cell J, and we have P genes. So the cell J has P variables, and we consider the N cells as N observations. So in other words, the N cells represent a sample from this P dimensional distribution, and I refer to this observed count of gene I in cell J as the small X I J. So we first did a model selection to select a marginal count distribution for gene. A marginal count distribution for gene I based on its counts across our cells. So we consider Poisson, zero inflated Poisson, negative binomial, and zero inflated negative binomial. Okay, and for the joint distribution, the correlation among genes, this is the more difficult part. And we use the copular framework, which is a multivariate statistical technique to do this. So, what we did is that we want to find a joint CDF, a cumulative C D F, a cumulative distribution function from this p-dimensional count space to this zero to one interval. So we want to find this F based on the marginal CDF for every gene we already obtained from the previous slide. So the theoretical foundation will copulize this cosplar theorem, which means that if we have just marginal distributions for the p genes, there exists a copular function called C. So we can couple those P Couple those p-marginal distributions into a joint p-dimensional distribution. And the theorem says that we can find c uniquely if the f1 to fp are continuous, but if they're discrete, there's no unique solution. And that's our challenge. To address this challenge, we use this technique called distributional transform. Essentially, we try to convert each discriminate distribution for gene i into a continuous distribution. Into a continuous distribution. So we know that for a discrete distribution, it has several point masses, and we try to distribute the point mass in the interval between two consecutive values so it becomes continuous. That's the basic idea. So after this transformation, then we use Gaussian copula to couple the P marginal gene distributions into a joint distribution. So by Gaussian corpora, what it means is that this Uij. Is that this UIJ is basically gene i's transform value in cell j. So we do the transformation by using the marginal distribution here. You see, so this is the marginal distribution fi, Poisson or negative binomial or the zero inflated distributions. Using the observed value, we use this continuous distribution transformation to turn it into a value uij, which is between zero and one. With this, we then do this inverse Gaussian transformation. inverse Gaussian transformation so that after this inverse Gaussian CDF, every value here will become standard normal distributive. And with that, we just need to couple it with one correlation matrix. That will give me a joint t-dimensional Gaussian distribution. So then our task is to estimate this correlation matrix. That's the basic idea of Gaussian calcula. Okay, so let's assume that we already obtained the estimated marginal distribution of gene j. made a marginal distribution of gene j from for from our data we are looking at one cell type and we just randomly sample this um i would say interpolation constant between zero and one to make the distribution continuous and finally we can estimate this correlation matrix for the gaussian by using these transformed values and we will have one value per cell one vector per cell and each cell is p-dimensional with this transformed Is p-dimensional with this transformed inverse Gaussian values, then we can use these vectors and nodals to estimate the p by p standard sample correlation matrix. That's the fitting part for fitting the distribution. With this, we can use this fitting model to generate synthetic data. So what we have is the input from the previous step, a fitted joint gene distribution, one per cell type, and we have the cell type proportions, which are learned from the real data. Which are learned from the real data, and we can require users to specify the number of cells to simulate, fewer or more than the real cells, and the total sequence in depth. So, I left out some technical details for these, but with the fitted model and some parameter adjustments based on the user input, we can output a synthetic gene by cell count matrix with K cell types, and we can also output a fitted model parameters for user interpretation. So, it works very well. So, it works very well. So, let's look at the results. So, I'm just showing one case example here. So, this is the data used for fitting the model. This is the left-hand data untouched. This is the synthetic data from our simulator. And you can see copula plays a crucial role here because if we leave out copula, the data will be very different. And these are two competitors which do not explicitly model correlation, but we think they are also their model might capture the correlation. Their model might capture the correlation, so we include them here, and you can see that we are mimicking the real data the best. So, this measure called MILISI is between one and two. Closer to two means that the real data and synthetic data are more similar. So, you can see that we are most close to two among all the possibilities, among all the simulators. So, this is our published work, which was published in last year. And so, and we also And so, and we also, our work was published in parallel in the recon conference. And this is the software article, which contains step-by-step implementation. So, this is the method article. This is the software article. That's the SA Design 2. But we're not satisfied yet, because as we said, SA Design 2 can only simulate from discrete cell types. What if the cell states are continuous, right? We want to be able to do this simulation. Simulation. And also, we want to move beyond RNA sequencing. We want to do attack, protein, and also allow for spatial coordinates. And finally, we want our model, since I said it's a probabilistic model, we want to make use of that advantage so that our model can output a likelihood. And using the likelihood, we can come do a model comparison to see, yeah, this model fits better. So with this capacity, we can actually evaluate which pseudo-time inference method does better. Inference method does better. So, the idea is that using the inferred pseudo time, let's say we try to calculate the likelihood. If the pseudo-time fits better, the likelihood is supposed to be higher. That's the basic intuition. So, using this, we want to, we use the technique of wine copula so that our model can output a likelihood. We can compare different models. So, we can see that this is SC Design 2 result. So, from this real data, which is obviously. Real data, which is obviously continuous. If we divide the cells into cell types or cell clusters first, we can see the data synthetic data looks okay, but there are gaps in the middle. But now with our fix, our generalization, with SA Design 3, the data are more continuous. And so SA Design 3 is still not published yet, but we're wrapping it up. So I'm showing the preliminary results to be published. And also, we can see that SA Design 3 can allow for this bifurcation trajectory. Allow for this bifurcation trajectory. So here, the middle is the root, the top one is one trajectory called pseudo time two, the bottom is another trajectory called pseudo time one. So the cells are bifurcating. And we can provide users with the pseudo-time values we use for generation. So in other words, we have ground truth here. While the other simulators, they don't have this functionality. In particular, I want to point out SEDM is the only simulator. The only simulator, as the popular one, I would say, who used the GAM, the deep learning to do generation. But in this case, we did better. So, in other words, we think this problem doesn't require the complexity of GAM. So, you look at the L M L I f value, we are the highest. And furthermore, we can provide this multi-omics ground truth for benchmarking the method that does omics, or I would say, modality integration. So, you can see that here, this is the So, you can see that here, this is the RNA modality. This is the methylation modality. What we are able to do is to generate this cells with both RNA and methylation, which can be considered as ground truth. Then these integration methods can be applied to see how well they do. Okay, and we can also do spatial data generation. The left part are the data for fitting, the right part are the data for simulation. So, even though I didn't talk about the methodology of SDVAN3, but I can briefly Of SD93. But I can briefly say that we generalize the SA Design 2 model by allowing the marginal distribution of every gene to be conditional on a covariant. And the covariant could be spatial location, could be pseudotime, could be anything. Yeah, so it unifies this K cell type model into one regression model. And finally, my other student, Guang Ao, who helped us with this additional functionality, SC RedScene. SE read scene. So we can also simulate reads from our counts. So we are not restricted to simulating just count data, but we can provide this raw data as reads or UMIs. Then users can even benchmark those low-level processing tools for single-cell sequencing reads. Okay, so these are the functionalities our SESI Design 3, the newest version can do. So multiple heterogeneity structure, different designs, and different modalities. Different designs and different modalities. And for its usage, all the previous usage I have in mind: benchmarking, inference, and in silico experimental experiments can all be done using RSAD93. So for the rest of my talk, I will focus on this in-cyclical control experiment functionality. So why do we need in silicical control experiments? I obtained this figure from the Ken Academy, which shows the idea in biological experiments. Idea in biological experiments. So, when we want to see the effect of a treatment, right? The amount of water, we want to set up a negative control group with no water, then we assess the outcome. That's the simple idea. So, I want to see, say that we, in our single-cell analysis pipelines, like Sirat or ScanPy, they are like an experiment, a procedure. How do we evaluate their effects, their outcome? So, I would say there's a so. So, I would say there is a so-called double-dipping challenge in single-cell inference. So, double-dipping, I think the word is from that famous sitcom, Seinfeld, in the late 80s. And double-dipping means that when you eat chips, we're sharing like one sauce, but you add one piece and you dip again. Then you basically contaminate the sauce. That's the basic idea. But in double dipping here, we use the same data. We first perform pseudo-time inference or we first perform clustering. First, perform clustering. Then we treat the pseudopyne or cell clusters as given. Then we do the differentially expressed gene identification. So I want to say that these likely false positives caused by double dipping can be resolved if we have set up an incelical negative control. Okay, so I want to introduce the pseudo-time differential expression problem first because we had some previous work on this. So I don't think I need to introduce pseudo-time again because we have heard it in the morning. Because we have heard it in the morning. In short, we want to infer a latent temporal variable from single-cell sequencing data. And this temporal variable is expected to represent, say, cell differentiation or cell immune response. So with the pseudotype, right, the natural downstream question is which genes have expression values change along the pseudotype. So intuitively, we want to say the left gene should be called DG, the left right gene shouldn't be called DG. The left right gene shouldn't be called DG. That's our intuition. But how can we solve this statistically? So I would say that one voice we want to point out, or want to, when we start this project in 2020, 2019, we realized that the pseudo time should be regarded as random instead of given in the regression model. The reason is because the pseudo time is inferred from the data. And of course, we know the cells are a random sample from the cell population. So there's From the cell population. So there's sampling uncertainty. And there may be additional uncertainty added by the pseudotime inference algorithm. Okay, however, before our research, existing methods treat CL pseudotime as an observed covariate. So motivated by this, we published work on pseudo-time DE to consider uncertainty of pseudo-time. So what we did exactly is this diagram. So we start from the cells, single-cell RNA-seq data. Single cell RNA-seq data, we apply a pseudotime inference method. It could be a monocle, could be slingshot, or it could be Jin Shu's method, right? So these are all possible choices. So we can sort the cells and obtain each cell's estimated pseudo time. Then we use the statistical model called generalized additive model, which is a non-linear regression model, and it can output a test statistic. By generalize, we can mean that we can specify the gene distribution to be negative. Gene distribution to be negative binomial or even zero-inflated negative binomial. So, obtaining a test statistic for gene J in previous method, which I will talk about trace-seq, they just used a theoretical distribution from gamma to calculate a p-value for genej. But we think because of the randomness of pseudo-type, that theoretical distribution may no longer hold. To resolve that issue, what we did is that we did sub-sampling. Is that we did subsampling of the cells. I want to make a note that here we didn't do bootstrap because some pseudotype inference method do not allow for repetitive cells. So we can only do subsampling. And then we do pseudotime inference on the subsample. And then we permuted the inferred pseudo-time so that we can disrupt any relationship between pseudo-time and gene J. This becomes negative controls. And then for each permuted subsample, we apply the same. Sample, we apply the same GAM model, we obtain a test statistic value, and these values are from the null, so they can form an empirical null distribution, which from which we can calculate an empirical p-value. So it's a very simple idea. We just include a pseudo-time inference as part of our procedure. And we're not just looking at the test testing for GAM alone because we want to use the pseudo-time inference consider its uncertainty as well. Okay, so with that. Okay, so with this, how do we perform? So Traceeq is the method that uses GAM, but didn't consider pseudotime uncertainty. Monocle 3 also didn't consider pseudotime uncertainty and uses the generalized linear model, which is more restrictive than the GAM. And these two methods are for bulk data, not for single-cell data. So putting them together, we can see that under the null, right, for the non-D genes, we should expect that the p-values are uniform between zero and one. P-values are uniform between 0 and 1, but that was not the case for these methods. But we can satisfy the uniformity because of the way we did this empirical null distribution. What's the scientific consequence? So using our method, we can see that it's not just we can control the false discovery rate better, but we can actually lead to more discoveries. So if you look at the D genes that are only found by our method pseudotime DE, they are enriched with many informative goal terms. So in other words, So, in other words, this shows that p-value calibration is necessary and can be useful for finding the genes. But I want to say our method has limitations. First, our null, which is about the permutation on sub-samples, is still assuming that the cells follow a trajectory. We just disrupt the relationship between pseudotype and gene. What if the cells are not from a trajectory, right? That's a complete null we didn't consider. And I want to say that in the And I want to say that in this case, we need to generate the incelical negative control using a simulator. The permutation alone is not enough, and we'll show that very soon. And the second is that computational time. You can see that we need many rounds of sub-sampling, single-time inference permutation to achieve a high resolution p-value. And that can be costly computationally. So, how can we reduce the number around still achieving false discovery rate FDR? False discovery rate FDR control. So, for this, I want to mention another work in my lab called Clipper, which allows us to just do a simple contrast and control the FDR without requiring many null data generation. Okay, that's pseudo-time DE. How about cluster DE? Similar problem. We first divide cells into clusters and then we find DE genes between two clusters, double dipping. So there are two existing methods for this problem, but they're both assuming the gene. But they're both assuming the genes are following Gaussian distribution. We want to relax this assumption by making the scenario more realistic to real data. So our proposal is to use SA Design 3 to generate in silicone negative control, use Clipper to develop or to construct a contrast threshold for FDI. And these are inspired by some statistical papers, which is gap statistics for determining the number of clusters. Determining the number of clusters and knock-offs for controlling the FDR. So, these are the theoretical foundations or motivations for our work. So, let's see this. So, let's say that our cells have two labels, two types. If we simply premio the labels, we wouldn't change how the cells are, right? So, the cells still have a gap in the middle, but if we use SA Design 3 to generate the complete null, which means the cells don't form clusters, we can fill in the gap while still mimicking the data. While still mimicking the data. And we can achieve this simply because we're using a very easy-to-interpret probabilistic model. We can just change the model parameters to achieve this very easily. And what is Clipper? Clipper allows us to do FDR control without relying on high resolution p-values. Therefore, we are relieved of parametric assumptions or large sample sizes, which are necessary for obtaining p-value, high-level, high-resolution p-values. The foundation is the knockoff. The foundation is the knockoff. And for all the applications I showed in my talk, Clipper is applicable. It only requires a contrast score, one per feature. So for D features, we have D contrast scores and the cutoff. So the cutoff is determined by the knockoff theory, which means that we hope by our construction, the features, D features, will have the contrast score distribution like this. So largely symmetrical about zero. Largely symmetrical about zero, but with a heavy right tail. The heavy right tail would indicate the features that are likely interesting, our discoveries. For the uninteresting ones, their contrast scores should be randomly positive, negative. That's the foundation. So therefore, using the simple idea, we could allow us to do, say, if we have chip CP calling, which I didn't talk about, but in that case, you have just one experimental condition sample, one background condition sample, they serve. Background condition sample, they serve as a natural contrast. We could do the contrast score construction. Or for the RNA-seq differential gene expression, I showed that we can use permutation to construct a null control, negative control. So we can have this contrast. And for the single cell pipeline problem that's more complicated, we can use SC Design 3 to simulate the null control from the beginning. Then we apply the same pipeline to both the real data and the synthetic control data, and we do the contrast. Contrast. So the very flexible framework. And that's our Clipper paper published late last year. So we hope to advocate this to make this into many pipelines. I think on the one hand, we can save you a lot of computational time. So you don't need to worry about the high resolution p-values. And also, because this procedure is very transparent, we can inspect the contrast store distribution to see whether the theoretical condition holds. So finally, So, finally, I want to wrap up by showing some preliminary results of using SA Design 3 to generate in-cyclical negative control and using Clipper for FDR control in this single-cell cluster DE problem. So, let's say we use this data as a complete null case, sorry, which has only one cell type. So, no cluster should be there and no D genes should be found. But if they do CIRAC clustering or k-means clustering, we can form. means clustering we can force the there are two clusters right if we do that we can always find the e genes between the two clusters but if we set up this in silicon null control using sdon3 you can see in this case because the real data has one cluster then the synthetic null control looks like real data and as it should be so using this case we can see that using SURET no matter which statistical test we use the FDR will be one because you can FDR will be one because you can always find the e-genes from the so-called clusters, right, found by Sirat. But using our approach, setting up the synthetic in silico null control by SDN3 using Clipper to do the FDR, we can control the FDR very well in this complete null case. So finally, my take-home messages include, we think it's important to do sanity check because popular methods do not always work. So benchmarking is necessary. Benchmarking is necessary, especially benchmarking against classical methods should be considered. And second, our simulator up to this point, version 3, can allow method benchmarking, parameter inference, and also in silical control data generation. I didn't have time to talk about parameter inference, but it will be in our upcoming preprint. And double dipping is ubiquitous in genomic data science. So statistical inference is usually not a map, usually not a first step. So we need to consider the whole pipeline. So, we need to consider the pipe, the whole pipeline. And finally, our proposal for single-cell inference, including the simulator S Design 3, for generating data from the user-specified null. So, users can tell the simulator which two cell clusters you are interested in and what is the null control you have in mind. So, you can design the synthetic null based on your question. And Clipper offers a p-value-free FDR control, which means that you can save a lot of time from the null. You can save a lot of time from the null data generation because if you use the traditional approach, say for p-value 0.001, you need to generate 1000 null data that can be very computational intensive. But this time, you only need one generation. Okay, so final note. I wrote this perspective article on a different topic, but it's about formulation. Which cases should be formulated as a testing problem? Which cases should be formulated as a binary classification problem? So if you're interested. Classification problem. So, if you are interested, you can also read this paper. But it's related to our discussion about the statistical rigor or the formulation. So, finally, I want to thank my students for working on this line of research, starting from Vivian SD Lang and Tianyi Slan 2, and now Dongyuan is SLAN3. And also Dongyuan is the author for Sudo Tang DE. And Xinjiou, work who developed Clipper, and Kaxing, who's doing this work, I'm saying, to be wrapped up about using SDN3 and Clipper. About using SDN3 and Clipper to make our pseudo-time DE faster and also to be able to account for complete null and also cluster DE and also funding agencies. Thank you.