But what I thought I would talk about today is pole removal methods. So that's in problems, in Wienerhof problems, where we have singularities which are simple, and we know that we can do lots which can help us solve the problem either approximately or explicitly in a very simple fashion, which maybe we can't if we have more complicated singularities. Complicated singularities. So, the nature of this talk really is in two parts. First of all, I want to introduce you to problems where the poles arise naturally from the physical system. And then, second, how we can substitute more complicated singularities by simple poles and zeros and then affect an explicit factorization to enable the problem to be solved. So, I'm not going to go through the details. So, I'm not going to go through the details of deriving Wienerhof problems. I will just quote them and hopefully you understand them. But please shout if there's anything you don't understand. So, as I say, the talk will be in two halves, not of equal length necessarily. And I should thank all of those people there. And the first part of my talk is newer, relatively recent, and the other stuff is older because I'm now old enough that probably many of you weren't around. Probably many of you weren't around in academia when I gave that. So, one of the pleasures of getting old is that you can recycle some of your older work, which I don't think had enough airing in the original time. So, this should reiterate what I've said. Look at problems of Wienerhof type, where pole singularities are present, makes life a lot easier than if we have branch cuts or other types. Easier than if we have branch cuts or other types of singularities. The first part will be on physical problems where the poles arise naturally. And for this talk, I'm going to stick to acoustic problems because most of us are familiar with those problems and the algebra is simpler than in other problems. But the methods really do follow through to much more complicated problems. And I'll allude to one or two of those later on. Those later on. And we have systems where sometimes there's poles that arise, but they're not natural physical ones, but they still are there and we can exploit those. But I won't have time to talk about those. And the second half is about replacing functions, which are more complicated with rational function approximations. And I have to apologize here that many of you in the audience probably know Bob. Audience probably knows far more about these substitutions and replacements than I do in different areas. I'm going to stick to what I like, and you later on can talk about what you like. And I know that there are advantages in other methods, but the benefits for me of my approach is that they're very physically intuitive. Okay, so let's start, as I say here, with a very simple waveguide problem. And we won't even do it by using Wi-Binahov. Do it by using Wi-Binahoff. So let's take this problem, which I hopefully you're all familiar with. So this is a velocity potential for the perturbation, for the acoustic perturbations. And so if you substitute this in, so a time harmonic problem, your wave equation is reduced to the Helmholtz equation. And we're just dealing with two dimensions for simplicity. So we introduce X and Y, and we have a wave guide which has the class. Guide which has the classic problem: half of it to the right has rigid boundaries, so they're hard, acoustically hard boundaries at x equals zero, as y zero and h, and then it's hard and soft. So, this is a very simple problem, a textbook type problem, which you can solve explicitly analytically using the Wiener Hoff technique. But I'm going to do it in a simpler way to show you the ideas that I want to. Ideas that I want to get over in the talk. Okay, and let's force it by sending in a plane wave. This satisfies the wave equation from infinity. And because of my choice of minus i omega t, then we know that e to the minus i x means the waving coming from infinity from the right down. And then if plus would be out going to the um in in the right towards the right hand side. Okay. Right-hand side. Okay. So one naive approach would to say, let's just look in each region for x is positive, then let's pose an expansion in terms of the duct modes. So these are solutions, each one of the wave equation, which satisfies the boundary conditions. So lambda n are these quantities, and if you substitute this into Helmholtz equation, it satisfies it. Holtz equation, it satisfies it. And these are all outgoing solutions or decaying solutions, because if n is small enough, there's a finite number of propagating modes. And if n is sufficiently large, then these are decaying to the right. Similarly, to the left, we have waves which satisfy the boundary conditions at y0 and h, and then they have a slightly different y dependence in order to do that. And then the mu n's. To do that, and then the Î¼n's satisfy this relationship here. Okay, so this is very straightforward. We have an incoming forcing, and then these are outgoing waves to the right and to the left. So that's the solution, and it's complete. And then, how do we solve the problem? Well, the naive approach is to say on x equals zero, these have to be equal to each. These have to be equal to each other, and their normal derivative phi x has to be continuous so they're equal. So, if you apply continuity of potential and the normal derivative, the x derivative, then you have this system of equations. Okay, so this system of equations in principle can be solved. We don't need Winerhoff or any other fancy technique, or do we? So, we can use an orthogonality relation. We can use an orthogonality relation or collocation, et cetera, on y on x equals naught for different y values to get a linear system of equations to solve for the a n's and b n's. But we know from experience that the convergence is very slow. It's really not very nice. And this is symptomatic of many, many problems where you have a representation of your solution purely in terms of modes. Of modes and the convergence often is poor, and then there's been many, many attempts in many areas of physics to speed this up. Many of them are numerical, some of them are physically inclined, and I want to introduce one which is driven by the physics of the problem. So, that's really the point of looking at this simple example. Okay, just to show you the convergence problems, so if we look at So if we look at looking on the real part of the potential on x equals zero for y between 0 and h. This is the exact solution as it should be in black. And of course, for x is negative less than 0 and greater than 0 as you tend in. They should be equal. And you can see for large numbers of modes that you get the sort of Gibbs phenomenon. Sort of Gibbs phenomenon. So you get this oscillation in the solution, and same with the imaginary part. Thanks to Raphael for producing these a few years ago now, but it demonstrate the point. And we know why the solution is bad, because there's a singularity of y equals h, x equals zero, which is somehow infecting the whole solution. So one point, the behavior is not regular, and that's Is not regular and that mucks up everything. So, how can we deal with that in a simple, intuitive, and reasonable way to get something useful? So, let's start trying to build in the sort of ideas of Bien Hoff into here. So, one way of rewriting the system of equations I had before is to say, well, that sum could be represented as some integral. Okay, and the integral. Okay, the sum had each one had a particular value of x and y behavior. And we can build that in by taking this form of solution. So you really don't need to know anything about Wienerhof or anything. You're just guessing a solution which, by residue calculus, gives you the sum of poles desired. And the coefficients which we did. And the coefficients which we didn't know at the beginning have been replaced by a function which we don't know. And I'll call it b plus of alpha because we know that there's some analyticity properties in particular that that function has no singularities in the upper half plane, complex alpha plane. So its singularities lie in the lower half plane. Because when x is negative, if we want to pick up the poles and We want to pick up the poles and get a representation of a sum of modes, then the only singularity should be from this cosh gamma h is zero and not from this term. So for x negative, we have to deform in the upper half plane. Therefore, there's no singularities from that. We just pick them up. And this representation formally is exactly the same as the sum of modes. Okay, so again, I've replaced. So, again, I've replaced this sum of coefficients, unknown coefficients, with two functions which are unknown as well. So, have we done anything? Well, in principle, not really. We've just rewritten the expression for two modes, and we have to equate them in the same way to get some system out for B plus and A. But excuse me. Excuse me. Okay. But the one thing we do know is that these functions are related in some way to the data in the problem. And in particular, this looks like, if you look at it, a waveguide problem, where you've got rigid boundaries from x is minus infinity to plus infinity. There's nothing about the information for having a sort of different boundary condition. Sort of different boundary conditions. So for this problem, but for here we have the different problems. So let's see the relationship between A plus, B plus, and boundary value problems. So if we look here, that A satisfies this boundary value problem where you're saying the extension of the plate to the left has the same boundary condition at the top. Uh, the same boundary condition at the top, except there's some unknown function applied from x is zero to minus infinity. So, another way of saying it is, what is the boundary data, the Neumann data here, which gives me the Dirichelet boundary condition that was imposed in the original problem. I don't know that, but there is some A of minus X, as I've called it, which does satisfy. As I've called it, which does satisfy that. Okay, and I'm working here with psi rather than phi, and it's just I've substituted off that incoming wave. And this A plus is related to this boundary data in this way. It's a half range transform, which we know has got these analyticity properties. Similarly, for x is negative, then we have this boundary value problem here, and then we have an unknown forcing at this top boundary. Unknown forcing at this top boundary, so we have Neumann and Dirichlet boundary conditions, and then the B plus that I wrote down in my guess of the solution is ridded in terms of this function b of x, which is also an unknown function. Okay, so all of these functions have some relation to the physics, and for more complicated geometries, we're allowed to have unknown data specified anywhere in domain. Anywhere in the domain, or even outside the physical domain, you don't have to construct problems which somehow satisfy the physics of the problem, only in the region that you're interested in. So here, this is where the solution is valid, but we're extending the problem outside and saying this is our boundary value problem. Okay, so how does that help us? The data A of X and B of X are unknown. B of X are unknown. However, we do know something about their behavior close to the singularity, close to y equals h, x equals 0, from looking at a local analysis. So we know how they behave. It's irregular, but we know how it scales. And from that, we know because of the relationship between little A and B and A plus and B plus what the large alpha behavior is. Large alpha behavior is for these functions. So we do know something a little bit more than our naive sum representation that we had initially. Okay, so that's what we're going to do soon, but let's just take a little diversion just to sort of put it in the framework of Viner Hoff. Okay, so just rewriting this representation of our function in terms of integrals and replacing five. And replacing phi by this psi, which has just substituted off the incoming wave, then we have this relationship. I still don't know a plus and a plus of minus alpha, but I've swapped alpha to minus alpha in this representation. And if you think about it for a little while, you see that these two representations actually don't just hold in their domains x is positive and negative, but all x, in fact. So all you need to do is set y equals. So, all you need to do is set y equals h, equate these terms, and then the integrands have to be equal, and then you get this equation. So, this is a different way of approaching the Wiener Hoff problem. But this, to all of you here, will recognize this as a classic scalar Wiener Hoff problem. You've got a function analytic in the upper half plane. A plus of minus alpha is a plus function for minus alpha, so it's a minus function, it's analytic in the lower half plane. Minus functions analytic in the lower half plane times something which relates to the physics of the problem. It's the transform of the kernel of your integral equation. And for simplicity, I'll call this the kernel of the function. And then we have some forcing from that incident wave. So this is getting us from that simple representation into this form. Okay. And just writing it in this way. Just writing it in this way with replacing the Cosh over gamma shine as K, then we know that the factorization procedure is to write down K as a product of two functions, one analytic in the upper half plane and one in the lower half plane. Bracket missing apologies. And from this form here, we can see what the factors behave like for large alpha. This function for large. This function for large values of alpha, and gamma is alpha squared minus one to the half, is like alpha or mod alpha. So cosh over shine goes to one. And so this is one over mod alpha. And each one is symmetric. So each one looks like one over mod alpha to the half. Okay, as mod alpha goes to the finite infinity. So we can go through the normal mean half procedure. Need and a half procedure separate into plus factors on one side, minus on the other, equate to a polynomial by Louvall's theorem. And then it's easy to show from this information and something about the physics of our B plus and A plus that, in fact, that polynomial is zero. And that gives me that solution. So this is the exact solution for the problem. But in general, we're interested in problems where we don't know the solution and we can't. Know the solution, and we can't find that information. So, this step of being able to write down K and work out its bar field behavior in the alpha plane is not available to us. But we can do a local analysis in X and do that. Okay, and this square root behavior is why the convergence is poor in the original problem we started with. Okay, so that's what we want to deal with. We want to deal with, okay, but let me just backtrack one second. So, this equation, if you remember, has to hold in a strip in the complex alpha plane. So, there's a region in the alpha plane which this is valid. And in particular, I put a minus sign here to indicate that the alpha minus one is a singular, it's a minus function, so the singularity. As function, so this singularity must lie in the upper plane, so our strip must be indented below alpha equals one. And in particular, everyone who works on these wave problems, we have a convention where singularities on the real line, we pass under them if we're on the right-hand side, and we pass above them if they're on the left-hand real side. So our strip is sort of indented. Okay. Okay. So, where does poll removal come into this? I haven't said anything yet. So, poll removal comes in is to say, well, if we didn't, we weren't able to factorize, do this product factorization. Another way of tackling this problem is to say, let's write this equation down. And I've written it in this form. And now we have a left-hand side, we've got a plus function, analytic in the upper half-plane. In the upper half plane, a plus of minus alpha is a minus function. This is a minus function. So, this is a minus function, no singularities below, except for the singularities from that, simple poles in the lower half plane. And they lie at, I defined it at the beginning as minus lambda n. Okay, so there's this infinite set of singularities, simple poles lying in the lower half plane. All we have to do on the All we have to do on the right-hand side is remove them. So we remove them by just taking them off here. So evaluating the sort of residue of this term, each of the alpha is minus lambda m terms. And we add those on. And so this gives me a plus of minus lambda m, which is a plus of lambda n. And then I get a minus sign. So this is a plus here. If I add them on to both sides, what I've done then is remove the singularity. Is remove the singularities from this side because they're simple. I can remove them from that side and put them on the left-hand side. So now this is equivalent. We've done the Wienerhof factorization of splitting all the terms, which are plus bits, singularities in the lower half plane. All these have got singularities in the upper half plane. And then this is okay. Now I can estimate their behavior and see what both sides by analytic continuation. By analytic continuation, equal a polynomial. And let's assume that's zero again, but we can show that. And that allows us to write down a plus of minus alpha in terms of this expression here. So that's an example of pole removal without doing the factorization because we may not know it in more complicated problems. Okay, so we're in the rearrangements. We're in the rearrangement. I've had to bring this from this side and bring it up. These are unknowns. A plus of lambda n are a whole set of unknowns because we were trying to solve for a plus. And now we've replaced them by a plus a load of lambda m's, which is exactly what we had at the beginning of our problem when we were doing it in the naive manner. So we don't look like we've done very much, except what it's saying here is that this now. This, now I've really arranged things, must also be a minus function because this by definition is, but I've introduced now a load of poles of cosh gamma h in the wrong half plane. Okay, so these have now got poles at minus mu m, unless we choose the A pluses such that this term vanishes when cosh gamma h vanishes in the lower half plane. So we have another. Half plane. So we have another condition, and this is what happens in many of these problems. You have some condition, you then write it down, you rearrange it, and then you get another condition, which then gives you a system of equations. And that's what you get here. You've got some forcing terms and an infinite sum of our unknowns. That should be equal to zero. I'm sorry. Okay. So this is analogous to mode matching. To mode matching, but it's written in a different form. So it's sort of, it's essentially the same, but it looks different. But it's also slowly converging. So it has the same intrinsic difficulties than the original problem. But we've approached it from quite a different point of view. So I just mentioned that because the same problem can be rewritten in a number of guises and have the same intrinsic difficulty. Have the same intrinsic difficulty. So it's a linear system of equations, and you can just solve it, but it will be poorly convergent. So let's go back to study the corner singularity and then see how we can build that into our problem. So near the y equals h, x equals 0, we can do a local expansion, and we find that the phi must be a representation. Must be a representation of terms which are like Bessel function of m plus a half order and have this theta dependence cosine of m plus a half theta. When theta is zero, the derivative of that is sine, so it satisfies the Neumann boundary condition, and then the Dirichlet boundary condition on the other side comes from differentiating this and setting theta as a pi. Okay. Okay, so and we know we know that J m plus a half for lot small values of r looks like r to the m plus a half. So this square root behavior comes out by a local expansion of physics. So a of r, and when theta is equal to pi, this is a of minus x, is given by this thing, which is that, and b of r, theta equals zero is that. So we can see that the leading term looks like r to the term looks like r to the um uh r to the half cos n pi so it's cos of zero and then this one is similarly has that behavior okay so we know by a physical analysis locally what our behavior is of a of r and b of r then we can take half range transforms of that data and that tells me a plus and b plus in terms of an asymptotic Terms of an asymptotic expression for large values of alpha as alpha goes to infinity in the strip. But of course, because they're plus functions, that's true also in the whole region of analyticity, which is in the upper half plane. So this is the form. We don't know these coefficients R0, R1, Q0, Q1. It's a local analysis, but we know it must take this form. Okay. So we know. So we know these functions up to their asymptotic form, and we want to build that in to that integral expression that we had for these quantities. Okay, so now this is so this is where the differences occur. I had an integral on the right-hand side, and now what I'm going to do is take my integral. I have a load of singularities. I have a load of singularities, and I'm going to start pulling my contour up in the alpha plane and picking up the residues. And I'll go as far as the light up to t terms. Okay. And then I will pick those residues up and I'm left with a contour integral. So if I can just sketch it. So we have some contour. Some contour, my integration contour. I've got my functions and I have some singularities. And then I push my contour up and I start picking up the contributions from each of those. And I go to a finite distance up, which corresponds to taking p of these or p plus one of these terms. P plus one of these terms, and then I have my contour that I've left behind, which I've called it's gothic C sub P. Okay, is an integral I've left behind. And this thing can be evaluated, and it's just these duct modes. And then this integral here looks like it's hard, but because I'm now away from the origin, I'm sufficiently far up. I'm sufficiently hot, far up. I can put in the asymptotic form that I just showed on this previous page here. So I can substitute that into this integral here and collect the terms. And then I'm left with an expression, which is a sum of duct modes plus a sum of these integrals, which are explicit integrals, which I can evaluate every one of them, multiplied by some unknown cock. Multiplied by some unknown coefficients. Essentially, what I'm doing is I'm replacing the tail all of those infinite terms which you normally neglect when you're truncating an infinite system. I'm collecting them by some different representation in terms of we call them corner singularity terms. So each one of these is a corner singularity type term. We have a parameter m in here. And these terms get smaller and smaller. And these terms get smaller and smaller. We can neglect them after a sufficient number of terms. And this captures the singular behavior and leaving this system to deal with the sort of regular behavior. And that's the nature of this exercise. It's to really find a way of representing the solution by taking the tail or the infinite terms and re-representation. The tail or the infinite terms and rewriting them in terms of a tail integral or a sum of corner singularity modes. So, again, for x is positive, I redo it. I do it again for this side. I get a sum of these dot modes plus this thing, which is going to put down, improve the convergence rate. And then we have sum of terms plus this integral, and then I can write this integral. And then I can write this integral term by term, and each of those is evaluatable. Okay. So if we do that and just look at the representation, I showed these diagrams before. And now just taking a few duct modes, just one or two of them, and one or two of those corner modes regularizes the system. And so we lose the oscillations and we get very good convergence. And here it says here, I'm taking one. It says here, I'm taking one duck mode and I think one corner mode in this problem, and we get fantastic agreement. So the methodology is good. And then we can apply the same thing to more complicated problems to show it's not just simple acoustic problems. So if we take an elastodynamic bar, we have some incoming wave, scattered wave, and this is a stress-free bound. And this is a stress-free boundary. Again, this is a classical problem in the lustre dynamics. And if you plot the tractions at the end, you'll find that we have the same problem. We have singularities which are highly oscillatory. And if you look at the numbers, if you just take in these duct modes or the modes, you need 60, and we're still not getting very good convergence. Okay, and this is the shear traction on the boundary as well. We get these oscillations, and you need roughly 250 terms to get something which is reasonable accuracy in these problems. And if you study the behavior in the corner of the elastic problems, the corner singularity behaves like r to the 2.5. Behaves like r to the 2.73. It's not a half or something as simple as that. It solves a transcendental equation as r tends to zero. But that 2.73 looks rather weak. So you'd think it wouldn't infect these modes of vibration, but in fact, it does infect them and quite seriously hinders the summation by just some. By just sums of mode. So again, we can build this corner solutions into our results. And then you find very good agreement for here we have M is eight modes and four corner modes and we're comparing it with 200 modes and we get very good agreement for the tractions at the end. Okay. And then, just to show you before I move on to the other half of my talk, I've just had a student recently who looked at harder problems, and he was looking at particular classes of kernels which have that form. So, I've shown this before in talks, and I know other people have worked on it. But the interesting thing is if you use square k, then it becomes the identity. So, they have a special, special behavior. A special behavior, but it's hard to say what extra information you can gain from that information. But we can pose all these problems that he's looked at in terms of a system of equations, which look very similar to the ones I showed you earlier. They have this poor convergence. And in particular, I can show you this one, which is the This one, which is the Jones thick plate problem. Douglas Jones looked at this problem back many years ago in the 50s. But we can recast the problem in terms of a three by three Wiener Hot problem where this function L is given by that parameter there. And you can solve all of this by pole removal, except you get down to a system of equations which satisfy this relation. Uh, we satisfy this relationship here, and C plus is related to the physical boundary data in my problem. And the Jones problem was find was a semi-infinite rigid plate, you send in waves, and then you're interested in it's the generalization of the summer fold problem. And C of X is it's C plus of X. X is it's C plus of X is just this boundary data on the extension of the top line. Okay, that's phi. So we have to solve this equation. And again, we have slow convergence. The singularity is different because it's a right angle wedge rather than a discontinuity. But nevertheless, you have the same problems. Okay, so just some conclusions for this first section. Some conclusions for this first section. The method moves works very well, it captures the physics of the problem. And once you have the physical nature of the singularity, you can sort of remove it in a very elegant fashion, but the algebra is rather messy. So I can see its lack of popularity because it is a lot more work, but you nevertheless can use it. One added advantage is Rafael and I, with the student, try. Um, with a student, tried to look at the trap mode or the localized mode problem for this um elastic semi-infinite strip. And this value of Poisson's ratio occurred, and we could get very high accuracy for the Poisson's ratio by this method. And it's very generalizable. So let's move on to poll seven. Pole segregation or approximation. So, this is really more important, I think. And I've been interested in this for many years. It's really saying if we have a factorization, k is k minus k plus, how do we affect that factorization for scalars? We don't really need to do very much because we can use Cauchy's integral. Much because we can use Cauchy's integral formula. We have to do some analysis. Basically, it has to go to one the behavior. So you fiddle around with the kernel and then find the behavior that you want and then use the formulae. So there's no intrinsic difficulty. So mostly we want to do it for matrix kernels where there's no theory. But also you may. But also, you may want to use it for simple problems if you have to compute this very, very rapidly. And then an approximation might be useful. So, what do we mean by approximation? So, basically, if we're given a strip in which our Wiener Hoff equation holds, then we know the function k. And therefore, if we take an approximation k0 to that function, whether it's a scalar or a matrix, then we want every element of that to be. Every element of that to be close, so we can bound the error of k in the strip. If that's true and k0 goes to k become goes asymptotically close, this mod alpha goes to infinity in the strip, then you can show under some circumstances that the factorization, the factors of the approximation and the original kernel tend to each other. Attend to each other, okay. And also, you get something more for your money, but in the region where they're analytic, the error gets smaller. So, whatever the error is in the strip, it's better in the half-plane of analyticity. So, that's great. So, in some sense, approximations are naturally suited for Wienerhof problems because we want these factors, and we can always arrange it, but we want to find the factors in their We want to find the factors in their region of analyticity. If we have singularities we want, we can always use this relationship here. If we found this in its region of analyticity, the singularities can easily be found by the original function k. So we don't ever need to look at the approximations in the region where they're going to be very bad. Because if we're approximating a function with a branch cut, with a pole, it's not. With a branch cut, with a pole, it's not going to be right very close to those singularities. But we never need to do that. Okay, so that's the idea. And how do we do that? Well, I like to do it using POD approximants because you have a sequence of better and better approximations, hopefully, and we can systematize it and get a relationship which will give us a result which is to the specified accuracy that we wish. Okay. So we'll just do a couple of examples in a second, but for those people unfamiliar with Pardet approximants, so if we have a function f of alpha, think of it as a scalar, and then we write it as n slash m and you write it as a polynomial top and a polynomial bottom. So it's a rational function approximation of this form. Okay, where a naught a1 up to a A naught a1 up to a big n, b1, b2, bm, or unknown coefficients. And those coefficients are found by looking at the original function f at some point in the complex plane. Often we do it at the origin, and look at its Taylor series expansion. And for those people who played around with them, it allows you to get round singularities. So a Taylor series expansion. Singularity. So, a Taylor series expansion, we know its domain is only valid up to a singularity. Pardet approximants allow you to capture singularities and get you around further. And Pardet approximants can also be generalized to have not only just evaluated a one point, but you can have multiple points. So, you can have two-point PARD approximants to get the point at infinity as accurate as you want it. As you want it as well. And the coefficients here, the A's and B's, are given uniquely in terms of the C's through this relationship. You match this to this up to this order in alpha, n plus m plus 1. That gives you a sufficient number of equations for the numbers of unknowns. And you can see here that you always set the first one, b0, to 1, because you've got no loss of generation. Gotten no loss of generality in doing that because you can always multiply through. Okay, so that system I won't write it down, but essentially you can do this with exact arithmetic if you want, or you could do it numerically. And the alpha to the m plus one up to n plus m gives you a linear system of equations for b m. And then you take these equations, the coefficients of those, and that gives. The coefficients of those, and that gives you the a n. So, very, very rapidly you can find those, and then that gives you them uniquely. But there are some values of n and m for which an approximant may not exist. Okay, but then you don't bother with those. If f goes to one at infinity, then use an n by n so that the highest exponent is equal top and bottom, and then it tends to one at infinity. Then it tends to one at infinity. And often we deal with symmetric kernels. So then your party approximate would just have powers of alpha squared in them. Okay. So this function here, you could say, let's see if we can represent this by a party approximant. So it's got, you can see it has a finite cut along the imaginary axis between i and i k because k is bigger than one and minus i to minus i k. Of minus i to minus i k. And what you find is if you just calculate a Pardee approximant, you get a sequence of poles and zeros which interlace each other, which then mimic that behavior. Then for those people who remember Koita, he worked on elasticity problems. He looked at this kernel, tanch alpha over alpha, and he wanted to represent it. In those days, it was hard to do these things. It was hard to do these things numerically. So he wanted something simpler. So he wrote down a representation like that. And so he was trying a sort of rational function approximation to improve on this as an approximation to that function. And he chose C, E, and D by hand. And he could have done it as a party approximant and actually got better values. Okay. But the behavior infinity of this thing and of this. Infinity of this thing and of this thing are not the same. Okay, there's a sort of accumulation point for this, and then there's a branch point of that. So you're never going to get a very good agreement. So you can get sufficient accuracy for most purposes. But if you wanted a system where you could get better and better accuracy, this is not great. You want to get the point at infinity right. Okay. And so one trick you could do is like square this function. Square this function and then do the party approximant of the square. And then, when you separate your poles and zeros, you square root each term, and then you get a whole load of square root functions. Okay. And you can then get as arbitrary accuracy as you like for that. Okay, so that's all well and good, but I want this to work for some hard matrix Wienerhof problem. Matrix Wiener Hoff problems. So I really wanted to be a useful tool. And this is the simplest matrix Wiener Hoff problem, which is non-trivial. Okay, this is the acoustic problem. We have row one and C1 are the density and the sound speed in the upper region, and row two and C2 down here. We have a screen. This is the Rawlins type problem where we have mixed boundary conditions on this surface, soft and hard. Okay, and Rawlins. Okay, and Rawlins looked at this problem when we had the medium, medium one and two were the same as each other. So row one was equal to row two, C1, and C2. Then the problem simplifies. I'll say, let's take the sound speeds the same, but take a mismatch in the densities in the material. If you do that, then this kernel function, which we want to factorize, can be written in this form. One mu, which is a constant. 1 mu, which is a constant times gamma, our square root function, minus 1 over gamma, 1. Okay, so in Rawlins' case, and he didn't solve it like this, in fact, this allows a commutative factorization. It's the identity plus mu over gamma j. This matrix J is this quantity here, and this is gamma squared, so the square root's gone. So this is a roots gone so this is a polynomial entry so if you take j squared you get the identity times a polynomial this gamma squared so in fact this k lives in a sort of subspace where you can pose an ansat for the plus and minus factors which also has this same form it's a it's some function of a scalar times i plus a scalar times j same j. times j same j for the plus and minus and these are a plus minus b plus minus you can just plug this in and this problem reduces from a matrix to two scalar wiener equations for the a's and the b's and you can factorize that and that's what you get so this krapkoff danieli commutative factorization comes out when you have the the rule is problem when when you don't have dissimilar meaning When you don't have dissimilar media. Okay, so that's great. The problem I was interested in solving was to look at this problem, whoops, where k takes the form, the identity, plus this new form, mu gamma minus mu over delta. And gamma and delta have got different branch points. So one is that alpha is plus and minus one, and this is at alpha. And minus one, and this is alpha is plus and minus k. This does not permit a commutative factorization. So the methodology is inapplicable. And we don't know for non-commutative factorization in general how to do it. So it's still an open problem, although many people in the audience have managed to make progress and have their own ways of doing these things. My idea was to construct. My idea was to construct a non-commutative factorization for an approximation to this thing here. Okay, so the idea is if it's non-commutative, then if we take the plus times minus factor for k matrix, it's not equal to k minus times k plus, for these being the same as each other. Okay, so my approach is two-step. Let's achieve a commutative partial factorization, which will get something wrong. Okay, so what I do is take my k and say, let's rewrite it in this form. Let's recreate a j where I introduce a function f in this form. I pull out a function f so that the f goes downstairs in this term and I can write it in this in up. It in this in up here in this form, if I take f to be gamma over delta to the half, which is this function here raised to the power quarter. Okay, this has got two finite branch cuts in it, and they live inside j. And then I have this thing, but j squared still satisfies minus gamma squared i. So this expression here looks like a commutative factor. Like a commutative factorizable form because this J satisfies this relationship here. The trouble is that J has got branch cuts in both half planes. So this is the trick. I write it in this form. I then do a commutative factorization on this. So I can write my Q plus minus as this. I can then write my plus S plus minus and my R plus minus in this form. My r plus minus in this form. You don't need to worry about what they are, but they're explicit expressions. You can compute them. And so this form now has r plus minus s plus minus of functions which are analytic in the right regions, but j has got branch cuts in both half planes. So I've got an explicit commutative factorization of something which isn't, which has still got some branch. Which has still got some branch cuts in the wrong half plane in J. And I've called them Q minus Q plus to indicate they're not the full solution. So the trick is I have to get rid of these branch cuts from their wrong region. We've got them in the wrong plane. So the commutative factorization has to be modified to get the J to have no branch. To have no branch cuts in the offending half space. So that's what we do. What we do is we replace J by Jn, and all that means we have to do is replace that scalar function f, which is this square root of gamma over delta, by something which is fn and which is a Padet approximant. So my Jn is this form here. So Fn is the Padet approximant of the original function of S, which is. Of the original function of s, which is f is scalar, and because it's symmetric, it's just a function of alpha to the 2n. Okay. And how do I do that? So I construct an ANZATS to do that. So I write Q minus Q plus. I replace those by Qn minus Qn plus. So the J, the Fn F has been replaced by Fn. I then introduce a matrix M inside. So I mean. Inside, so I'm introducing m times m minus one, and then taking this bit and calling it km minus, and this being bit and calling it km plus. And this should have no branch cut or singularities in the upper half plane. So the choice of M gets rid of the branch cut or the poles and zeros in my replacement of the branch cut by this term. So that's the nature of the game. Term. So that's the nature of the game. And we can always do this in principle. And the trick here is that I've managed to do it for something which is then constructs an explicit, approximate, non-commutative factorization. And that's by ANZA. So in fact, it just is simple. It comes out like that. It's easy to take the inverse because the determinant of that is one. In fact, it has to be because otherwise you would get... Be because otherwise you would get spurious singularities. And these BNs and BN bars and ANs just have to satisfy some system of equations. There's a finite number of those, so I don't have to worry about convergence or anything. And the P n's and Qns are related to the poles and zeros of my POD approximant, and they interlace each other. So this is just bookkeeping. There's a simple linear system of equations. Linear system of equations for those, so that gets the job done. So that is the solution through this resolving matrix. So I haven't taken it much further in the last 15 or so years because I've got other things to do, but I really would like it to be pursued further because I think there is mileage in it. So I'm going to conclude now with this second half. With this second half, so we've shown that replacing one complex function another is attractive for Wienerhof problems. As I've said, the approximation has to hold in the strip, but it gets better and better in the regions where the functions are analytic. Hard approximants are a valuable tool for generating these approximations because you can take a sequence of them and then get to the error that you're. Error that you require. It's allowed me to construct exact approximate non-commutative matrix factors, which I think there's very few of them around in the literature. And therefore, to enable me to solve some long-standing problems, I know that other people have used other techniques and Chebyshev polynomials and other approaches, which we'll probably hear about this week. Which we'll probably hear about this week, and you can prove something about their convergence, which I probably can't hear because there's only limited knowledge about what you can say in terms of convergence of pad approximants. To me, one important factor, which I think is I never have managed to work out, is to consider for an approximate factorization, what is the actual boundary value problem I'm solving. Boundary value problem I'm solving. So there's some discrete problem. And for each of these different Pad numbers, what's the sequence of problems? And we know that if you look at discrete problems on a lattice and the continuum one, there's some sort of convergence. Is this related to that? In some sense, it is, in some sense, it isn't. Okay, and then what I would have liked in this talk was to say more about the relationship between this approach and Relationship between this approach and potential for Riemann-Hilbert problems. I apologize that I haven't been able to do that. But essentially, the idea is that for Riemann-Hilbert problems, you want to look at, well, we know that they're very, very intimately related to Wiener Hoff problems. We want to solve, especially matrix Riemann-Hilbert problems, have the same issues as we've faced for matrix Wiener Hoff. Matrix Wiener Hoff problems, and they arise naturally in lots of special function problems. Where we know special functions have some properties of connection formula between the solution in different spaces, different regions of the complex plane. And we know that you can often pose the connection formula problem in terms of a matrix Riemann-Hilbert problems. So the Airy function you can think of. Area function you can think of at infinity, you've got these rays and you've got a connection formulae across three lines. For the one of the Pan-Levi equations, you have six lines in space, and then mapping across is a Riemann-Hilbert problem with a matrix, which has entire elements, and those entire elements could be approximated by Pardet approximate because they work quite well at mimicking entire functions in the region where. Functions in the region where the solution is bounded. So I think there is some mileage in using this technique for those sorts of problems. And with that, I'll finish. Thank you.