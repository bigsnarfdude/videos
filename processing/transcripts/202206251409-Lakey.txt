Okay, can everybody see the screen okay? Yes. All right, so I'm gonna my talk will be quite a bit different from the other talks given so far today. I am not a card. So far today, I am not a card-carrying seismologist. I'm a mathematician by training. And so, what I'm going to be talking about is how to compute analogs of the Slepian tapers in other settings, in particular, other settings where there's enough symmetry to. To get concrete sort of formulas or expressions for what the tapers would look like. But since this is multi-taper, I want to start out talking a little bit about some motivation. Maybe it's fictitious motivation. It's not nice, pretty examples like Frederick Simone gave this morning in his context of tapers on the sphere, but it's tapers in different settings. But it but it tapers in different settings. So so the the fictitious motivation is dyadic processes. So I'm talking about I'm going to be I'm going to be talking about analogs of sleptian tapers on hypercubes. And so dyadic processes come into play here. I will skip over the review of the multi-taper method since that's been addressed quite satisfactorily. Quite satisfactorily. Those are traditional tapers. Okay, dyadic processes are processes that are going to look strange to people nowadays. They have the property that the statistics depend only on not the time difference because we're not talking about time, but if we Talking about time, but if we index by realizations by integers, then instead of taking the difference or the sum of the integers, we're taking the dyadic sum of the integers. So what does that mean? That means we express the integer as a sum of power of twos, powers of two with coefficients epsilon k, coefficient of two to the k. And so for two different integers and an Two different integers n and m the n plus m will be the sum of the coefficients added modulo two. Okay, so you actually get cancellation because of this. So for example, if n equals m, then n plus m and this and this addition is equal to zero. Okay, well, so Well so there's a Fourier transform in this setting and it's and the Fourier the Fourier uh transform functions are the Walsh functions and the and and sometimes called the Hadamard functions. So the Hadamard Walsh transform is given by this formula here and I'm going to use the letter H for Hadamard and you just normalize and you multiply the process by the Walsh functions and you sum. functions and you sum over over the wash functions. Now t is the parameter that's being used here, but I'm gonna I'll mention that that's maybe not really appropriate. These were studied in the late 70s, early 80s by a lot of people thinking that maybe in some context they would be a suitable alternative to the Fourier basis. To the Fourier basis. And the wash functions are typically ordered by what's called sequency order, which has to do with the number of zero crossings per unit time. And so the basic statistical properties of what's called dyadic stationary processes were developed. There's a similar formal theory. Formal theory compared to the traditional Fourier case, which is that you've got a spectrum process that has orthogonal increments, and you can represent the process in terms of its spectrum. And so this was worked out by Moritan, published in the early 1980s. And Moriton also developed spectrum estimation based on averaging Walsh periods. Based on averaging loss periodograms. But this again was everything was thought to function. Everything was with respect to what was called dyadic time, which is a time interval 0, 1, but with this funny addition. Okay, so the interest in dyadic processes eventually waned because they weren't shown to have. Because they weren't shown to have widespread application, I would say. And there was a survey paper done eventually in 1991, Jasso by Staffer. He argued that there was still some use in the analysis of categorical data, meaning data about a system that is in either some finite number of states in which the states could be described by. Described by small integers, and then these integers you could use dyadic addition to sort of quantify the state transitions. But he observed in that paper that the concept of dyadic time is fictitious, and so maybe we should get away from that. In the intervening period, I think there's been a realization that we should look more from the That we should look more from the perspective of networks and the structure, the topological or geometric structure of networks. And so we should treat data that has this dyadic addition as data that might lie on a hypercube, for example. And it's more appropriate to look for processes where we're trying to study data on a hypercube, although in this particular talk, I'm not going to do that. talk. I'm not going to do that. So my apologies. There's broader, there's there are broader contexts in which both the idea of a stationary process has been considered and also this idea of sloping tapers in particular. And that's the setting of graphs or networks. So vertices that are joined by edges. That are joined by edges. So, the idea of the concept of a stationary graph process was studied in some work of Martez-Segura at all, several IEEE. There's a signal processing paper and then some conference papers. This is really laying down the conceptual framework and looking at some toy examples for the most part. There's some work by Sisfiro et al. There's some work by Sesfero et al. in which they study. They didn't call them Sloppian tapers, but it's effectively what they were looking at was signals defined on the vertices of a graph that have optimal spatial and spectral localization. So there's something called the Graph Fourier transform. You can talk about the spectrum of a graph. It's usually Of a graph. It's usually quantified in terms of the eigenvalues of the graph Laplacian. And so you can make sense of these notions in that broader setting. The hypercube is an example of a graph that has a lot of symmetry. And for the rest of this talk, I'll be talking about how to use that symmetry to define the analogs of Slopian vectors explicitly and use the symmetry to. Explicitly and use the symmetry to compute them. All right, so the Boolean hypercube is basically Z mod 2 to the N. So the integer is mod 2, 0, 1, the n-fold Cartesian product of that as a set. So spatial localization on a hypercube just means truncation to a Hamming neighborhood of zero. zero okay Hamming distance zero so if you represent the elements as as vectors of zeros and ones then the k Hamming distance to zero is just the number of ones in that representation of the element Z mod 2 to the n is a finite abelian group it has an isomorphic Fourier dual group and that makes it very handy to talk about just like the with the with the Fourier Just like with the Fourier transform, you've got an isomorphic dual group, and you can talk about the spectrum. You can talk about localization in the spectrum in the same way that you talk about localization in the spatial domain just by conjugating by the Fourier transform. So band limiting is conjugate in this context is conjugating by a Hadamard matrix, conjugating this spatial transformation. This spatial transformation. Okay, so I'm going to call the eigenvectors of the joint projection that first truncates in space by applying QK and then truncates in the spectrum by applying this operator PK. I'm going to call the eigenvectors of that operator Boolean Selepian vectors. And that, here's a picture of some Boolean Slapian vectors. So these are the ones that go out all the way. So these are two to the eighth, so 256. And the eigenvectors, there are two eigenvectors there. And how are they created? I'm starting with sort of with a pre-eigenvector in the Fourier and the and the in the In the in the Fourier domain, and then I'm filling out this pre-eigenvector in a way that I'll try to explain a little bit. And that's the dash. So the dotted one is the starting vector. The dashed one is an eigenvector of the Fourier transform of these so-called Silapian vectors. And then the ones that go all the way across are the Selepian vectors themselves. And the reason why they don't. And the reason why they don't appear to have a lot of spatial regularity is because I computed them in MATLAB. And these starting vectors are defined on Hamming spheres. And MATLAB doesn't know how to take a particularly attractive one to look at. It just kind of arbitrarily chooses one. And so you end up getting things like that. But really, what it has to do with that the eigenvalues But really, what it has to do is that the eigenvectors of the graph Laplacian have high multiplicity. And MATLAB doesn't know how to separate them in any sensible way. Okay. So if I want to argue that these are analogs of the Slapian vectors, then I can talk about all the properties that these eigenvectors of spatiospectral limiting. Of spatiospectral limiting on the hypercube have in common with the Slepian vectors or what's called the prolate throw it away functions, if you're talking about the case of the real line. And so one is that they are eigenvectors of a truncated Fourier transform, in this case the Fourier transform on Z mod 2 to the n. The eigenvalues are concentrations. If these are normalized. If these are normalized to have norm one, then lambda would be the concentration on the Hamming sphere. They're doubly orthogonal, which means they're orthogonal on the Hamming ball, I should say, as well as over the whole Z mod 2 to the n. They're locally complete. If you take all the eigenvalues, sorry, all the eigenvectors, they're complete on the Hamming balls. And then they have the And then they have the spectral accumulation, which is if you take the sum of the squares of the Fourier transforms of the vectors and multiply by the eigenvalues, you get a constant value, which is like the ideal bandpass filter. On the other hand, they don't have all the properties in common. So the eigenvalues are not simple as they are in the Euclidean case. They have high multiplicity. high multiplicity in the setting of hypercubes. There's no, there's not this 2Ï‰ t or 2wn theorem that says that the number of eigenvalues close to one of the spatial spectral limiting or time and band limiting is comparable to the product of the concentration set in the spatial concentration domain and in the spectral domain. So we'll see a picture of that. And then importantly, there's also at least if you just take this simple, if you argue by analogy and say what, if there is a commuting differential operator, what it should look like, because this is a Fredholm operator, as in the classical setting. And the Slepian. And the Slepian Pollock observation of there being this commuting prolate differential operator, in the case of the realign d by dt, one minus t squared d by dt minus c squared t squared. So what you do is you say, okay, d by dt is conjugation of the Fourier transform by the eigenvalues. So we make the symbol d is conjugation of the 48. Of the Fourier transform on Z mod 2 to the n times the diagonal matrix that has the eigenvalues, the square roots of the eigenvalues of the Laplacian of the hypercube. So T is just a diagonal operator with the square roots of the eigenvalues of the Laplacian. So you make this analogous operator. It doesn't compute with the spatio spectral limiting operator. Spectral limiting operator, although it almost actually commutes with the spectrum limiting operator, it doesn't commute with the spatial limiting operator. But it almost commutes in the sense that the norm of the composition, or sorry, the conjugate, sorry, the commutator between those two has a relatively small norm, I should say. All right, so those are those, that's the two omega t theorem. That's what the eigenvalues look like in the classical setting. Here is a particular case where I take n equals 20, so I'm looking at z mod 2 to the 20th. And if I look at the eigenvalues of this spatial spectral limiting operator in that case, that's what they look like. They don't look like they don't look like like they didn't look look like the previous uh they don't look like that okay what would they what they would look like if if there was a two omega t theorem is that uh in this particular setting if you talk you could talk about a normalized time bandwidth product and you would have this is this has about 60 000 uh non-zero or sorry the the dimension Non-zero or sorry, the dimension of the bandw limiting space is about 60,000. If you took a normalized, if you tried to define a normalized time bandwidth product, you should have about 3,500 dimensions there. So you would have about 3,500 eigenvalues close to one, although in this case, you actually have only about 2,000 eigenvalues. Only about 2,000 eigenvalues that are 0.4 or above. This is not surprising. It has to do with the connectivity of the graph. It has to do with the fact that it's hard to concentrate when you've got a lot of connections. This is a property of the spectrum of the Laplacian in a highly concentrated graph. Graph. Okay, so here's an outline of finding what the eigenvectors are of spatial spectral limiting in this setting. So we look at the geometry of the hypercube. We look at this almost commuting analog of the prolate differential operator. We define adjacency and variance. Define adjacency invariant spaces. So, adjacency is adjacency on the hypercube on which this operator does act as a tri-diagonal matrix in a similar way that the Perlate operator or in the finite dimensional case, the analog of the Perliate differential operator, the difference operator analogous to that acts as a tridiagonal matrix. And then we find a basis of eigenvalues. And then we find a basis of eigenvectors of this operator and just use a numerical method to compute the eigenvectors of the spatial spectral limiting operator. Okay, so that's what this is the Z mod two to the fifth represented as a graph. And on the bottom, you have Z mod 32 represented as a cycle. And so the geometry is very different. Geometry is very different. We can represent the points z mod 2 to the n as subsets of the set one through n just by taking the binary representation and taking the indices where you've got a one instead of a zero. And we associate that with the subset of one through n. And so we can represent the vertices as subsets of one through n. And then two vertices are adjacent if there's. Two vertices are adjacent if they're a symmetric difference of those two sets is a singleton, which is the same way as saying that the dyadic sum in Z mod 2 to the n has a single non-zero coordinate. That's a graphical representation of the adjacency matrix for n equals eight and dyadic lexicon. So if we list the elements Elements of Z mod 2 to the 8th and lexicographic order. That's what the adjacency matrix looks like. All right, the Graph Fourier transform is the same as the group Fourier transform in Zma 2 to the n. And the vectors, if we use this subset of 1 through n way of indexing the vectors, so the columns of the Fourier matrix will be normalized, and otherwise the entries will just be. And otherwise, the entries will just be minus one to the number of points in the intersection of the row and the column. Okay, those are the eigenvectors of the Laplacian on the hypersheet. And that's what they look like. That's what the Fourier transform looks like in this ordering, dyadic lexicographic order. That's the Fourier transform on Z ma to the eighth. on Sigma to eighths. All right, so space limiting again is truncating to a ball, a Hamming ball around the origin. And then spectrum limiting is conjugation of space limiting by that matrix. So we can identify the eigenvectors in the spectral domain. Spectral domain just by conjugating PQ conjugated by the Fourier transform is QP. We look at certain invariant subspaces on which this operator acts in a particular way. So, invariant means adjacency invariant, invariant under the adjacency operator. And then that allows us to do a dimension reduction where. Dimension reduction, where instead of looking at a matrix of size 2 to the n, we can look at a matrix of size n and represent on that invariant space, we can represent the operator as a matrix, as an n by n matrix instead of a 2 to the n by 2 to the n matrix. All right, so we look at a Hamming sphere is simply the set of all vertices with an equal number of non-zero entries. Zero entries, having fixed having distance from the origin. All right, we can take the adjacency matrix and we can split it into its upper and lower triangular part. And so A plus, I will call that outer adjacency, and that maps data on a Hamming sphere of a fixed radius r to data on a Hamming sphere of radius r plus one. And the corresponding lower triangular part, I'll call energies. Part: I'll call interadjacency, and that maps data on a Hamming sphere of radius r to a lower hammer sphere of radius r minus one. And the if we look at so L2 of sigma r is those are just the finite vectors supported on a Hammond sphere of fixed radius. But I can take, I can look at the kernel of inner adjacency in L2 of sigma. In L2 of sigma r. Okay, and I can call that space, that's the space I'm calling Wr. So it's the vectors that are supported on a Hammock sphere of fixed radius that are in the kernel of this inner adjacency now. Right, so on that space, if I first apply outer adjacency some number of times and then I pull back one. Times, and then I pull back one step with inner adjacency, then what I actually get is just a fixed multiple of applying outer adjacency k times. So applying outer adjacency k plus one times and then inner adjacency is a fixed multiple of applying outer adjacency k times. It's just a common simple combinatorial argument that has to do with counting paths. And so I won't go through. So I won't go through that, but but the but the multiplier is k plus one times n minus two r minus k all right so what we can do is we can now define a space vr which is the span of so I take I start with an element of in this kernel of inner adjacency on the R Hamming sphere and I look at all sums of outer adjacency powers of that. Okay, so this space is a tensor product of Wr essentially with R n minus 2r plus 1. I don't want to go through why it's n minus 2r plus 1, but that's what it is, tensor product. And on this space, the outer adjacency essentially acts as a right shift. So it shifts the coefficients to the right of an element that's a sum of coefficients. That's the sum of coefficients times outer adjacency powers of a fixed element of the space WR. And inner adjacency acts as a multiplicative left shift, meaning that it shifts to the left, but it applies these multipliers in that theorem also, right? So, as a consequence of that, adjacency, which can be written as outer adjacency plus inner adjacency, preserves the space V. Preserves the space VR, which is sums of outer adjacencies applied to the solvent's G. And since the space is preserved under adjacency, it's also preserved under polynomials and adjacency. And we can write the spectrum limiting operator concretely as a polynomial of degree n in the adjacency. This is actually, there's a general, this is an instance of a general. This is an instance of a general fact about what people nowadays call graph filters, which are polynomials in what they call the graph shift operator. And so it's an instance of that particular thing. So because of this, we can write the outer and inner adjacency matrices as n by n. So on the space B R. So on this space BR, we can represent outer and inner adjacency as n by n matrices, not two to the n by two to the n matrices on this n minus two r plus one component of this space Vr. Okay, so I'm and so that's kind of what that's so the matrix on the left is the matrix of the adjacency operator on on a VR space. Operator on a VR space. And the matrix on the right is what the band limiting matrix looks like when represented on this VR space. Except I have to write it in log scale because the numbers, the entries get really big. Okay, so to cut to the chase, the eigenvectors of these matrices, of these coefficient matrices on these VR spaces, define the coefficients of the eigenvectors. The coefficients of the eigenvectors of the spatio-spectral limiting operators on the hypercube. And if by ranging over these different VR spaces, I get a complete family of eigenvectors of these spatial spectral limiting operators. So these eigenvectors are the analogs of the Slepian vectors on the hyperkey. And just the final step is that. If you try to compute these in MATLAB, you're going to run into problems. So you have to, you have to, there's a little trick that has to do with the fact that these matrices are self-adjoint, but with a weight that gets where the elements of the weight vector get very large. But once you observe that trick, what you do is you take this other operator corresponding to this sort of Corresponding to this sort of fictitious commuting differential operator. And you compute the eigenvectors of that thing, because that one's tridiagonal in this representation. And so it's easy to compute its eigenvectors. And then you use those eigenvectors as the inputs of a power method to actually compute the eigenvectors of the spatio-spectral limiting operator. algorithm so again that was uh that's what the eigenvectors examples of the eigenvectors look like and again that's what the eigenvalues look like and i'll stop there because i'm out of time um thanks for your patience with me i i realize that this is way off topic and it would have been nice to see some particular