Invitation and introduction. So, today I'm kind of trying to be a little bit ambitious because for the 30-minute slot, I would like to incorporate like three different papers in this talk. But I'm trying to make things simple. So, I just highlight the idea and the results, and leave all the technical details we can discuss offline if anyone is interested. Okay, so the idea of this talk. The idea of this talk is based on three papers. And these three papers are all about meanfield games. And we want to introduce a concept called occupational measure to solve for infield games. And this idea is kind of new. And we can observe that it differentiates with the current existing approach for solving infield games. So I will start with a very simple scenario when the When the Minfield games is on discrete time and finance state and finite action spaces, and we highlight the idea. And then I will move on to consider a more general case when we have continuous time settings. And in the end, I will also show how we can reformulate this problem to a special class of medium games where we can have stronger results and we can also do reinforcement for this mid-field. Reinforcement model for this mid-field game set. So I'll start with the simple one. So this is very simple. So we consider a representative agent in the midfield game who tries to solve this MDP problem. So this MDP problem has a reward, R, which is a function of the population, state, action, joint distribution, LT. So this is the mean field. And specifically, the agent is considering randomized policy or relaxed control. So here the Or relaxed control. So here the focus, so the intuition, why we use this randomized policy or relaxed control, is not like, like Shuyu said, we want to do exploration and when we want to learn something. But here, this is actually quite technical because for the idea of occupation measure, we have to deal with relaxed control. We'll see later why this is true. And for this representative agent, he also tries to solve. Induced the mean field induced MDP problem when the mean field is fixed. And for this specific mean field gain, so people will use this concept called Nash equilibrium. And the Nash equilibrium for this mean field gain is defined by these two conditions. The first one is the best response condition. So when the mean field distribution L is fixed, then this randomized control, the relaxed control pi, is optimal for this MTP problem. And on the other hand, we also have this. And on the other hand, we also have this consistency condition characterized by a Fokker-Plump equation, which means that when people are taking control like this pi, and then the LT is exactly the induced population distribution. So this is a goal. So we want to solve for this kind of natural equilibrium for this mid-field game. So the existing literature on categorizing Minkfield natural equilibrium, of course, there are a lot. So I just briefly mentioned a couple of them here. Mentioned a couple of them here. So, for discrete time problems, it's usually done by definition, just a coupled best response condition and focal plan conclusion. And for continuous time, we have more methods for categorizing the national group. So, of course, the HAB equation plus the Polk-Planck equation, and also FBSTE and master equations. And more recently, we have a series of work from Peter Tennis. From Peter Tankoff's group, they are utilizing a linear programming formulation to replace the best response condition. And we are actually following this, but we are doing more in terms of this linear programming. But we start with, we also utilize this linear program. So before we move on to the details discussion on windfield gain, so let's try to take a look at what this occu occupation measure means, okay. Occupational measure means. So, if we have an MDP problem and we want to solve for optimal policy pi for this MDP problem, what we can do is we transfer the problem of solving the optimal pi into solving an optimization problem on occupation measure. So the occupation measure is defined to be the sequence of distribution on the joint when the When the player is taking policy pi, and then what is the joint distribution of this player at time t? What is the probability distribution of the states and action of this player at time t? And for one specific policy pi, we can of course retrieve the occupation measure. And on the other hand, we can also easily do a disintegration or normalization to obtain. To obtain a feedback control from occupation measure for any occupation measure. So we can actually construct occupation measure from a policy and we can also construct a policy from any occupation measures. So the idea of introducing this occupation measure is because we can show that solving an MDP problem is equivalent to solving a linear program, and this linear program has variable d. Has variable D. This is the occupation measure. So it basically looks like something like this. So this C is associated with the reward function of this MDP. And this AL, this is a matrix, it actually is associated with the transition dynamics of the function like the P. So it's a very simple linear program here. So whenever you have an MDP problem, you can consult to this linear program. Linear problem to solve it. So, we're going to utilize this idea. So, starting from this linear program, we can do something more. So, what we do is we utilize the so-called strong duality results for a linear program, because here everything is finite, so there's no problem, there's no issue with it. So, the idea is that we start with this primal problem, we call this primal problem, it's a linear problem, and then we construct this dual problem. And then we construct as dual problems, and the dual problem is also a linear problem. And then we utilize the strong duality, meaning that the primal problem and the dual problem has the same value. And then we come up with this first order conditions. So what we do is that if d star solves this linear program problem, then we can find certain dual variables y and z, such that y, z, and d star satisfy this bigger like. Like systems of equalities and inequalities. So, here everything is explicit and everything is smooth as long as your transition and reward is smooth in L, then everything is supposed to be fine. And another observation is that to guarantee consistency for video games, this D star is the occupation measure for a single agent problem, so we only need to guarantee that this D star coefficient. Guarantee that this star coincides with the population distribution L, then everything is done. So the intuition of that is just we try to match the single agent occupation measure with the population we feel. So what we can do here is we now replace this D star, all the D stars here, to this L, and then we turn this problem, these conditions, to a sequence of conditions on L, Z, and L, Z, and Y. Okay, so this is all we have. We call this mean field occupational measure optimization because we can transfer the problem of finding Nashup Libra to solving a specific optimization problem. So we want to find L, Z, and Y satisfying these conditions. So here the Y is somehow related to the value function, and Z is related to function. Value function and Z is related to Beyond. So we'll see a clear connection with the value function and the Beylman resistance in the next part. So this is nice because now in order to solve for Nash equilibrium, what we only need to do is optimize the violation of these conditions. So I just want to minimize the gap between the left-hand side and the right-hand side. So we will have a new objective function here. This objective function is smooth. So as long as your reward function and Reward function and transitions is smooth, and this is smooth. So it's pretty nice. And we're also able to show that if we want to find some epsilon natural equilibrium, we only need to solve this optimization problem approximately. So this tells us that we actually have a nice target. So even if you cannot solve this to zero, if you can solve this until with Ypsilon error, then you are guaranteed to find Ypsilon dashboard paper. And we also show that for very simple projective gradient design algorithms can have global convergence, meaning that if you start with close enough initialization, then you're going to converge to that natural congruence. And we observed that with different initializations, the algorithm can converge to different natural groups. So, previously, after I finished this paper, and whenever I presented it anywhere, people will always ask. Anywhere. People will always ask. I think maybe some of you are already asking by yourself. So, the first thing is: how about continuous time setting? Can we do something in a continuous time setting? And the other thing is that for this optimization problem, it looks very general. And whether we can do something, for example, for some specific structure, specific structure in the mid-field games, is it possible that we have something nice that looks like a convex optimization or some other optimization? Or some other optimizations problem that can have a better convergence guarantee. Because currently we only have local convergence. And the third question is: can we do learning? So can we design new learning algorithms that utilizes this concept of occupational metric? Okay, so in the next two parts, I will answer these three questions. So I'll start with the continuous time setting and then move on to a special class of games where we can do a global optimization and also deploy. I'll start with this continuous time. So to start, we still try to fix our state space to be finite. So we assume we have a finite state space, but we can have a very general compact action space. And we also have a finite comparison T. And in this case, each player will control a continuous time mark of chain instead of MDP. And here we have a transition rate function q. Transition rate function q, which is a function of the states and actions and also the mean field. And we have a running cost function and terminal cost function. So here we do not assume any continuity of these transition rate functions and cost functions with respect to mu. We just, for now, we assume that they are measurable. So this is the setup. And so this is the, if you want to establish the corresponding MF OMO, the occupation. MF OMO, the occupational measure optimization from the previous MDP setting to the continuous time setting. This is what we have. So again, we have a couple of conditions to hold. So the first one is the strong duality condition. So this is saying that the value of the primal problem is the same as the value of the dual problems. So this is the value for the primal problem, and this is the value of the dual problems. So this everything here with the star is. Everything here with the star is a part of a solution. So we want to find, for example, here, this mu star and gamma star and this psi star. So this psi star is something related to y in the previous slides and is connected to the value functions. So apart from these strong duality conditions, we have some other constraints. So this is the first set of constraints. It's called the primal constraint because it arises from the primal linear program problem. Linear program problem. So basically, it says that this mu star satisfies the Fokker-Bach equation. And for the dual constraints, actually, this is coming from the dual problems. As we can see here, it's actually very closely related to HJB equation. But here, this conditions is saying that this is just a subsolution of HJB. So we only have inequalities here. It's a relaxation. So, a natural question to ask is whether there's any connection or any difference compared to the classical, for example, HGB FocalPlanck coupled HGB focal clan characterization for natural cable. I will give two examples in this part to demonstrate their connection and their difference. So, in order to write down the HJB Vocal Punk coupled system, we have to start. Coupled system, we have to start with some Hamiltonian. So this is the Hamiltonian. And we assume that if this Hamiltonian has a unique optimizer, so we call this pi, and then we can say that the coupled system admits a classical solution, and this classical solution will correspond to a natural equilibrium. So this is the standard coupled system if we write everything down. Down. So, okay, so this is it. So, now we want to compare this to formulation. So, we call the previous one a primal dual system, and we call this one HB Foucault-Planck system. So, things, problems come up when the Hamiltonian does not admit a unique minimizer. So, either you have to choose a specific mapping pie beforehand to plug in. Beforehand to plug in in the couple system. And if you do that, you are restricting yourself to a specific mapping, and then it will only provide a sufficient condition for at most one particular mushroom frequency. So when you have more than one optimizer, you have to specify one in order to obtain the couple system. And on the other hand, you cannot choose this mapping arbitrarily because some Arbitrarily, because some of the mappings you choose may cause a couple system to fail to admit a solution. For example, when the regularity is not good enough. Even if an action could only exist, you may fail to find a solution. So here's a very simple degenerate mean field game. So everything is zero, so all the cost functions are zero. Then you know that any control can emit a natural coincidence. So in this case, our primal dual. So in this case, our primal dual system can actually characterize all Nash equilibriums. It's easy to see by setting all the dual variables psi star to be zero, and there you can see any control satisfy our primal dual system. On the other hand, for a coupled HJP vocal plane system, of course, you can choose one specific pipe, like the minimizer, to be some A in this set. Then this will lead to only one specific nature. And on the other hand, if you choose some arbitrary map, pi, and if this pi is discontinuous in this μ, then the focal point equation will fail to admit a solution. So this is something that may happen when the Hamiltonian does not admit a unique minimizer. So on the other hand, in our case, we don't really need to pre-specify this mapping pi. On the other hand, we actually solve them. hand we actually solve them solve this solution this gamma together with this mu and psi so this is the difference we don't need to pre-specify this pi okay so another scenario is I want to emphasize is that this dual variables pi star psi star can be different from the value function. So this is another example that I want to share. Example that I want to share. So, for this example, basically, it's very simple. We only have two states and two actions, and everything is explicit. And specifically, I want to comment that here that for this example, this objective, this cost functions can be discontinuous with respect to mu field. And solving this problem, our primal dual systems actually can lead to a sequence of solutions where Solutions where this Posite 1 has this specific form. But for Posside 2, actually, we have a sequence of inequality constraints for this psi 2 star. But if you consider the value functions, we only have one specific form, and this V2 is actually satisfying this condition. But in practice, it's plastic. But in practice, this plastic psi2 actually can have many, many other choices. So, in practice, when you want to solve for this kind of problems, solving this primordial system can be easier in certain cases because the solution set is larger. So it's easier to find solutions that satisfy any conditions. And we also can do some other extensions by assuming, for example, additional ellipses. For example, additional Lipschitz continuity of our transition weight functions and cost functions on mean field μ. Then we can also categorize the Nash equilibrium. We can also categorize the sets of Ipsonas equilibrium by relaxing the conditions of primal new systems. So previously we have a bunch of equality constraints and inequality constraints. Now if we can relax this condition then we retrieve the set of epsilon mesh. And this method also suggests a new numerical method. Also, suggests new numerical methods for finding natural polymer by minimizing the violations of the primordial systems. And in practice, we also observed that starting with different initialization can converge to different natural polymers. And specifically, we showed that for one specific problem, when we know that there exists three different natural equilibriums, and we are able to find them all. And so this analysis. So, this analysis, the methods can also provide a new tool for the sensitivity analysis or stability analysis for Nash equilibriums. We can show that the set of Nash equilibria is sequentially upper hemi-continuous, and any natural equilibrium for given Mi-field games is an Ipshaw Nash equilibrium for perturbed Mi field games. And this Ipshone is proportional to the magnitude of the water configuration. This is all the things that can be done under this primal group of characterizations. Characterizations. So I have maybe five more minutes, and I want to briefly discuss another scenario when we have a very special mid-field games and what we can do. So let's go back to the previous MDP settings. This is simple because we want to try if we can do some learning on this model. So let's keep everything simple. So specifically, we also assume that. Specifically, we also assume that for this part, we also assume that the transition of the players are independent with each other, so they do not depend on the main field. And the expected rewards are Lipschitz, continuous in the main field, and they're also monotonous in the main field. So they satisfy this condition. And specifically, this lambda is positive, means that we have a strongly monotone problem. And in this case, the natural film is unique. So we'll see that this lambda. See that this lambda actually plays a very important role in the analysis for the learning components. So this is actually another formulation, another optimization formulation based on occupational measure. So unlike the previous case when we have objective functions, now we can reformulate the finding natural equilibrium solution to solving this. To solving this inclusion problem. So we want to find some D such that this C of D, this is a component with respect to the reward function, and this is the normal cone of a closed convex set. So this is the closed convex set. So we want to show that zero is inside this big convex set. So this is the inclusion problem. And for this specific inclusion problems, Specific inclusion problems, we can design very simple projected gradient descent-like algorithm for this inclusion problems. So, what we do is we first update our occupation measure D by doing a gradient descent. Okay, so this is like a gradient descent. And then we project our d tota back to this set, standard set. So it's very simple. And here, this eta is a perturbation added to the model to guarantee strong monotonicity. To guarantee strong automaticity. So, if we previously we have lambda positive, then you can just specify this eta to be zero. Otherwise, it has to be positive in order to guarantee the convergence. So for this simple algorithm, we can show that it converged globally when this reward function is multi-tone. And the number of iteration complexity is polynomial. So we can do this very efficiently. And of course, we would like to extend. Well, of course, we would like to extend whatever we have to the case of reinforced millennia settings. So, we want to apply our algorithm to very practical setting when we only have M players in the game. And then this M player interact with each other in the environment. And we want to solve for online reinforcement learning problems. So, the online reinforcement learning problem means that. So, the online reinforcement learning problem means that, first of all, we have a reinforced learning problem. We don't have access to the closed form of the reports and transition models. On the other hand, for the online problems, it means that we only have real-time interaction with the environment on the flight to collect the data. So we don't have any other simulators or we don't have any other historical data for us to do this, but to do this learning. So, for the online reinforceable. So, for the online reinforceable learning literature, a commonly used criteria is the regret. So, I assume you already mentioned this. So, the regret actually captures how well your algorithm converge. So, in this MPLA game setting, what we do is if we have in each episode, if we utilize this PyM as the strategies for M-players, then we use something called NashPon to event. Use something called NashCon to evaluate how far this policy is away from a Nash equilibrium. So it captures how large the if-to-square, if none Nash equilibrium of the M-player game. So we calculate the cumulative deviation from this M-player game. And we want to see how it behaves. So a couple of major recipes here for doing learning here. Because we have unplayed environments, but we are using Minkyo games as approximation. Here games as approximation. So we have to deal with these approximation errors specifically when we do learning because the underlying environment is unplayer, but you are treating it as a new video game. So when you do learning, this approximation errors will be there all the time. You have to take care of that thing. And now we don't have an optimization algorithm to work because we don't know what is the underlying true model. Underlying true model. So we have to apply our algorithm for a specific approximation oracles, and then we have to really be careful about the sensitivity control of the perturbed dynamic projections because we are doing the projections. And it's well known that doing the projections, when you have a perturbation on your projections, usually you will have very big error here. So we have to be careful about that. And for the exploration in this MPR game setting, In this MPR game setting, what we do is in each episode, we randomly generate some players in this game and then ask the player to do pure exploration while other players are following our algorithms. So this is some major recipes for designing these learning algorithms. And what we can show, in fact, is depending on whether you have a strong immunotone or not. Whether you have strongly monotone or not, so if you already have strongly monotone again, then you can set this eta to be zero in the algorithm, and then you can choose appropriate hyperparameters, and we can show that with high probability, the regret is actually, there are two components. The first one is due to the approximation error of the mean filgium and mtheliums. And the second component is on the total number of episodes MQ. On the other hand, if we have lambda equals zero, means that we have to also specify, we have to also choose eta appropriately. And by choosing appropriate hyperparameters, we can show that here we have a much worse regret bound compared to the previous setting because we are doing perturbation to our model by adding some positive beta. Okay, so here previously we have square root of n, and now we only have n to the power of y over n. To a power of 1 over 6. And similarly, the bound on the number of episodes increased from 3 over 4 to 11 over 12. So this is the regret analysis for this learning for m-player games. If you want to use BPL algorithm to do that. Okay, so I think that's all I want to share. So I hope I conveyed I conveyed clear enough information because there are a lot of materials here. So, if you're interested, these are the three references I mentioned. So, for the previous two, we have posted online. And the first one is on the very simple case when we have MTP settings and everything is finite and we have discrete problems. And the second one is when we want to deal with repost millennial online repository problems, we're unclear again. And the last one is we're trying. And the last one is: we're trying to finalize this, and hopefully, this will be hosted very soon. Thank you very much. Any question from Artis? So, does this occupation measure approach apply to the diffusion process? Because ultimately, here you are still dealing with microgene light. You mean the continuous line process? Yes, yes, exactly. So, what about the diffusion process? Yes, so that is a very good question. Yeah, so that is a very good question. That is our next step. So we want to generalize everything to a more general size. I think when we have the continuous space, for example, the diffusion problem. Stay space, yes, exactly. So the idea should be quite smooth, so we can generalize it to here. But the tricky thing is here, the tricky thing is whether we can still obtain the strong duality results for that situation. I suppose that's quite the fundamental thing. Yes, yes, exactly. Yes, yes, exactly. So, this is the. If this can be done, then I think the rest of the stuff can be easy. And for your last part, those require estimates is a model-based auto-practice. Yes, yes, yes. It's interesting to see if you can apply this to all the free settings as well. Okay, so let's speak uh send all the speaker and this people section. So we'll have a team break for about 30 minutes and we will resume for County Spasha East Missouri. Yeah, I'm sure you can find out anyway. The destination The destination, for example, the fact that it's big.  But if you're actually about your muse, if you do not know the thing, which you don't know, I think it's not a good idea. I think a lot of that's okay.  So Because to be starting various and then you go to the person that's just going to have to go in there. Inherent measure, they don't measure anything. There's nothing. It's finite, right? There's no invariant measure or stationary. So forward is x minus just minus x. So therefore this x t. Okay, so therefore this Xt is basically X0, right? So X0 is unknown, but then there's this caution, right? But here, okay, this case is important because this exponential negative S. Yes, yes, yes, yes, yes, yes, yes, yes, okay, but the variance here is going to be. So the variance here is going to be no, but it's a cumulative. There are many minutes here. But this is just after capital T, it's going to be E minus capital T hex 0. So T, if T is sufficiently large, this almost is 0. So that's what I'm saying. So that's why they just remove that. Because this is approximately zero in a sense. But this is trouble type with these dialogue. Trump type because this is really low. Really cool. You take a capital G if it's sufficiently large, and this is conditional and this value is also transported aspect. Okay, but this is like coming back to what I was saying, this is like considering the Schrodinger problem where you want to transport the chief action. But this is just forward. You're turning it backward. Is backward, but the key is the backward process. Yeah, you reverse, yes. So the question now is how to okay, from new which is Aussian, goes back to the original PC. No this P is approximate, because I saw it's a very practical within us, right? So the question is, but see, we're not see we're not one thing this p0z on that is very important and this is important for transport or shooting of shooting of bridge because even those problems these are not notice essentially close to this one no no no no no no it's p zero why in the end because the reverse process the x t two dias the x t two dias is x is x homogeneous t, right? Because x tilde t has this information p t there. So remember this is x x t tilde plus sigma is 1. So therefore this is the score function. So you see this contains the information p t. Yeah, so this one, where does it start? This one where this starting with fun new. So x 0 2 that is new. You know new. New is a Gaussian. No new one. New is the Gaussian. New is a Gaussian. The one that removed that. New Gaussian. Okay. So you see, it's different, but what I'm asking is maybe this, I mean, I don't know. I mean, you see, this is different from transport law. The way I understand it is, I would take P0 as the critical measure. No, that never works. That's the whole thing. That never works. Because one thing, sometimes the Because one thing, sometimes the sample is very little, maybe only 20, 30. So you cannot use this to have an empirical measure. I mean, this is straightforward thinking, but that's wrong. So people never do empirical measure. So my point is you even don't estimate this. You don't even try to estimate it. That's my input. But to estimate this feature is comparable. It's wrong thinking because you are not able to figure out. You are not able to figure out what is the distribution behind this bunch of bad pictures. There's no way you can figure that out. Okay, so that is never the right approach. You see? So this is a new problem. I'm just saying this is a new problem. You want to move to something whose distribution is unknown. But the thing is, I don't want to know the distribution. I just want to generate a simple problem. So it's a simple problem. In a transfer problem, you have to know what. Yeah, in just only so that's why I really need SD because SD naturally gives you a sampling mechanism, right? You have SDE, then you can discretize each other as in use. Yes, it's great. I mean in the transport, I mean in the transport and what other people do in the SEC. They also have this dynamic so-called dynamic transition. But they have to go to invalid measure, which is no, because here you don't know. There's no invalid measure CC. I understand. You don't know the initial measure. Yeah. But you just can just from this at the end you can know. Just from this, you can know the invariant motion. But you see, you don't know X0, right? It's independent X0. So if it's invariant, it may independently. Yeah, but if it's invariant. But that is, again, it's a wrong approach because the whole point is I want to know that. Not now. I want to generate a sample from this initial. So the initial is important. The initial but the distribution is important. On one hand, it is important. On the other hand, we don't want to estimate it. On the other hand, you don't want to estimate it. You know, that's the thing. Yes, yes, it's different. It is a different problem, but maybe that's why. So I was suggesting, you know, maybe somehow, you know, come up with something, but it's not exact, right? Yeah, I've been publishing.