 Good? Okay. So I'll start talking about what a conformal autoencoder is. Conformal mality and pogmality are kind of the same idea in this talk. For pogenality. For orthogonality, we're going to talk about. I'm going to generally talk about smooth functions that go from Rk to R. They are, in general, we define them to be functionally independent if the rank of the dimension of the, sorry, if the dimension of the space that they span is equal to the cardinality of the set. The problem with that type of independence is that it is, they're always independent unless They're always independent unless they're collinear. Once they're collinear, you have a collapse and you get sort of you get a decrease in rank. If instead we look at inner products of their gradients, assuming that they're smooth, these are going to be zero if the angle between them is 90 degrees. So they're independent if I have a sort of large set of functions. Sort of large set of functions, all of whose angles pointwise are independent. Yes. So in this case, it doesn't have to. Obviously, if k is less than n, then you can't be independent. Yes? So if you have a small dimensional space, you can only have so many independent functions. This is what coordinates end up being. So. Yeah. So a general autoencoder is something people use for dimension reduction, images noising, several different tasks. What it does is it takes a high-dimensional input and tries to compress it into a latent space that has fewer dimensions. You can do this with three to two as in the example. You can have something very high-dimensional on the left. Something very high-dimensional on the left. And the idea is that the decoder is the left inverse of the encoder. So I can always invert back. If you use an invertible neural architecture, you can kind of get rid of the decoder and not worry about it. So what the idea that I'm going to talk about is basically one thing, I'm going to pretend that I don't know what the correct latent dimension is at any given time. So I'm going to expand the latent space to have. Expand the latent space to have one additional node. And I'm going to add some structure in the form of a soft constraint in the loss function that encourages the latent components to be orthogonal. Okay, an equivalent way of looking at this type of a constraint is that it encourages the metric tensor of whatever input I have to be diagonal. So these are. So these are these products are symmetric. They make sense in the space that my input lives in. And yeah, I only have this sum for j greater than k. This j over here is a typo and it should be an i. So the gradients are all evaluated at the same point. So why do I think that this is a good idea? Why do I want to do this? Okay. Okay, the first reason that there's an idea, I will show you why it kind of works. The first reason we want to do this is because we usually don't know the latent, the true latent dimension of data, and we want to infer it. And the way we would do this in a linear data set is with principal component analysis or some other related method. And then if you go to a nonlinear setting, you're going to use a spectral method. You're going to use a spectral method. People here are probably familiar with diffusion maps and maybe have contributed to making them. So you use that type of method. Instead, you can use an autoencoder. Again, the idea is that if you have some low-dimensional manifold, so here it would be a sphere in three dimensions, then the tangent space everywhere is two-dimensional. So I can go at any given tangent space and say, At any given tangent space, and say I want to find vectors that are tangent to my surface and that are orthogonal. There are topological obstructions to this, so you can't parametrize a sphere with two coordinates, even though it's two-dimensional locally. What I'm going to talk about is a local result, so think about it as working on an open set in some ambient space. Okay, the second idea that is Okay, the second idea that is again related is that orthogonality is very much related to invariance in the setting of a Lie group acting smoothly on a manifold. So a very, very simple example, which we like, which is rotations. If you rotate, the function that is invariant to rotations is one whose level set is exactly the rotations. And that function is one whose gradient varies in the direction of r. Gradient varies in the direction of R. Okay. If I have R, all of the other invariants with respect to rotations are functions of R in that space. So really, I care only about the direction in which the gradient is pointing, not about the magnitude as much. Okay. So the first example in the dimension reduction setting is: let's Setting is let's take something that is two-dimensional but embedded in 3D with this big phi on the left, and then run that algorithm. So with an over-determined autoencoder, an over-determined latent layer, I'm going to optimize so that I find components with perpendicular gradients. And what I see, if I plot the Dirichlet energy over training, Energy over training is that the network kind of sequentially starts adding dimensions. Basically, if the components are non-zero here, I have a function that varies over my submanifold. If it stays low, it means that it doesn't vary over the submanifold. So this second autoencoder component in this case ends up not being activated ever. And I get good reconstruction loss with two nonlinear components whose levels, as you can see. Your components, whose levels, as you can see on the left. Okay, so what I claim is that this regularization, having orthogonal gradients, encourages the autoencoder to find a low-dimensional representation of the truly low-dimensional COI example that we came up with. Here it's three and three. It's three and three. Yeah, and plugging, it's the norm of the gradients for each board on the map, on my training set. That's the whole point, right? I want a two-dimensional representation of the two-dimensional data set. Representation of the two-dimensional data set. The orthogonality constraint. So, this guy. So, the idea is that by minimizing this constraint, I'm encouraging, I'm not forcing, I'm encouraging the autoencoder to satisfy this constraint. And the idea is that somehow this is very easy to satisfy if one of these is entirely zero, right? Right, because if one of these doesn't vary, if one of your functions is a constant, then it just is dead everywhere, and these inner products are zero, then it never participates in the loss. So, yes. Yeah, so my understanding is that there is. If you try to go and basically develop the same thing and look at the metric through whatever you would get, as the if you look at the connection between the metric and the Laplacian, it is connected to this. I don't have the analytic formulation of that, but. Yes. So if this loss was zero and these If this loss was zero and these gradients were the gradients on the manifold, so if they were projected, then everything would be fine. I would already I would only be able to find two independent things. The idea is that I don't know the manifold, so I'm not necessarily allowed to project onto it. And so I'm going to use the ambient space gradient. And that's why this is a bias internal. Yes. Why is that incurred to do that? I understand that you're on the flyer spot, but well, okay. So if you are trying to reconstruct your data and you have a very small patch of data that's close to linear, let's say, you can only find so many orthogonal vectors. Yes. Yes. Yes. So just to repeat that, the idea is that the tense To repeat that, the idea is that the tangent space has a lower dimension than the ambient space, so we can only find so many dependent vectors. Okay. A nice thing about these, so these are two other examples where we take Fourier modes of solutions to differential equations and truncate them, and they exhibit low-dimensional manifold behavior. So, in the middle, it's the Kuramoto-Skosinski, and in the on the right, it's Staffan Fante. It's Taffa Enfante, and the dimensions are in the parenthesis. What is interesting is that you see this phenomenon where the autoencoder in the network adds dimensions as the loss decreases. So on the left, you kind of see that it plateaus with the yellow one, and then it decides to add another component, and the loss drops. And that happens again in the middle one. You can't really see it that much here. Here, it kind of guessed the correct number of dimensions. The correct number of dimensions before, you know, it didn't have to try too much. What this corresponds to, and why this is interesting from a learning perspective, is the following. So, this is the S curve, which is actually a surface. It's a classical dimension reduction data set in SK Learn. If you look at how the training evolves for this, you see that first it finds a linear approximation that is one dimension. A linear approximation that is one-dimensional, and then it transforms that to a linear approximation, sorry, a one-dimensional approximation that is non-linear, and finds the S, and then it decides to start adding components in the other direction and makes it two-dimensional. So this is kind of a hierarchy where it sort of increases complexity sequentially, and that is interesting. I think that there are other larger machine learning models that kind of exhibit. Larger machine learning models that kind of exhibit this behavior, but I don't know how well it has been studied. Okay, the second reason I said that this is worth studying, and let me just check my time, okay, is that you can impose invariance. And a very simple example of this is, again, take the S curve and define your group to be translations along the y direction. So the orbits are those red lines that are parallel to that manifold. And then you can ask the network to find an invariant to these translations by finding a function whose gradient is perpendicular to the wire translation on that sub-manifold. And you see that it recovers this complicated S that is the green shape. I say complicated because it's kind of, you know, it has higher curvature than something, something trivial. So basically, So basically, gradients are nice because you can get them through automatic differentiation. Writing constraints in gradient language is easy. And then the autoencoder can handle solving the hard part, which is actually integrate, find something that is integrable and that is invariant to a group act. Okay, yes. Yes. If you have the vector field beforehand, you can just say this is my vector, find something that isn't there. But yes, here I'm forcing it to be a coordinate. Okay, these plots in the middle just show that I found the true arc length, so to speak, that it is one-to-one with the coordinate that spans S. S. Okay, there are geometric instructions to doing this generically, and these are summarized in this paper. This is the only theorem that I'm going to really talk about. It basically says that orthogonal or orthonormal frames are easy to find, but whether or not they're integrable is basically dependent on curvature, on generalized curvature. That's given by the Weill tensor. Given by the wild tensor, that is defined in terms of the metric and the rear sheet tensor. And the problem is that basically a lot of mathematics has been developed that tells you whether or not you can find these coordinates. And in general, you cannot. If you have high-dimensional data, estimating these quantities is really hard. And so, how close we can get or how much mileage we can get out of orthogonality constraints when we're doing data. Orthogonality constraints when we're doing data science is a little bit unclear because satisfying orthogonality within epsilon maybe gives you enough power to get something that performs well, even if these ideal coordinates don't exist. This result is a, so you can always ask orthogonality at a point. You can't even do it in a neighborhood. You can't even do it in a neighborhood of the point. Yes. Yes. Yes. In general, I think the answers to that. But okay. So now this is what I was only originally going to talk about. I hope this. About. I hope this up to now makes some sense. The common uncommon problem is the following. Here, I'm gonna, many of you may have seen some version of this. I have two cameras that look at three people. One of the people is the same in what both cameras see. In this case, it's Bob. One camera sees Bob on the right, and the other one sees Bob on the left, so you don't see the same image, but it is the same person. Image, but it is the same person. These are not going to be static images in what I discussed. They're going to be sort of dynamic images. So you can assume that the cameras are videotaping. Okay, given data sets that are the observations of the cameras, we would like to be able to tell that Bob is the same person in both. And we would like to be able to tell Alice and Carol apart. So finding Bob is So, finding Bob is sort of the common variable problem, and determining who Alice and Carol are is the uncommon variable problem. And this uncommon problem is defined in terms of the common one. So, yeah. And then a couple of quantitative things is that we should be able to tell what Bob looks like in camera one from camera two. And that there's a whole set of possibilities for the other people given. For the other people, given any fixed Bob. Okay, if this feels like a rerun of something you've already seen, you might be familiar with this, which was the original cast of the problem. Yeah, so we'll talk about that. A bit of a setting for to make this a little bit more mathematical. I have three manifolds. U and V are going to be the ones. To be the uncommon ones, and C is going to stand for common. I'm going to make high-dimensional observations, and then I'm going to try to find maps that basically disentangle my observations into cross-product of map. The C for the first sensor and the second sensor, so the common manifold doesn't have to be exactly the same, it just has to be within a diffeomorphism. We'll explain, so, and that is what this. I will explain so so and that is what this equation says um so as an example if you had just had u v and c being three separate circles uh your n would be one so they're embedded in euclidean space the true dimension of each of these is just your n is two because they're embedded in euclidean space your d is going to be one because they're one dimensional the k's that determine the observations that you make the dimension of those will be high Dimension of those will be high. And then the fees are going to be approximated with the neural network. This is the same slide as before. I'm going to replace orthogonality with just calling them. If you have two sets of functions that are mutually orthogonal, they're going to be called disentangled. And what we'll do is the following. We'll take observations from each sensor on the top and the bottom. So the top represents. Top and the bottom. So the top represents sensor one, the bottom represents sensor two, or camera one and camera two. We're going to split the embedding space to have the structure that we want. And then, okay, we're going to have decoders so that they're invertible. The constraints that we care about is that one, the whole function is invertible, left invertible. The second constraint is going to say that pointwise, the common embedding for the common variable is going to be the same. To be the same. And the third constraint is going to be exactly: I'm going to pairwise disentangle these guys from these guys, and separately, these guys from these guys. This disentanglement is happening in the input space. The way we optimize this is relatively simple. We first say, find an invertible transformation such that the common embedding is the same and the reconstruction is. Same and the reconstruction is satisfied. And then, after we do this, we're going to fix the common and we're going to say make the uncommon coordinates be orthogonal to the common ones. If you do this for three circles, which you observe, you then use a linear or nonlinear transformation to jumble everything together. After step one, so after finding the common, you see that. After finding the common, you see that for each of the two embeddings, the common manifold looks the same. Those are actually not the same image, they're different. They're reconstructions for each of the common parts of the encoders. But the uncommon information is still sort of looks like projections of ORI. And that's because information that is present in the common part of the variables is also present in the uncommon embedding. And embedding, right? So after the first step, those red and green things have also information about the blue stuff. And the way we disentangle them is exactly after a custom cognitive. Now these look like circles, topological circles, not exact circles. Sorry. Okay, so I claim that this works nicely for this little example. So now I'm going to do the same thing with images. This is exactly again the same data set that appeared in the original common geometry paper. So here you just take high-dimensional images. These are, I think, 200 by 300 or around that range. You make them monochrome just to make it a little simpler. And also, we then do PCA. And we keep a few components, I think 60. And we keep a few components, I think 60. What PCA does is it makes it slightly lower dimensional and so it's easier to run. The other thing that it does is that if you look at this image, you can say that, oh, system one is just lives in this half of the image, and system two lives on the other half. If you do PCA, you're again jumbling them together because each coordinate affects both pixels on the left and the right of each image. So they're actually like entangled, right? So they're actually like entangled, right? If I give you the PCA, you can't say half the image is one and half the other. Okay, and I run the same algorithm, and I get these guys. Again, these look like topological circles. They're not very circular. And what we can do with this is that we can take an original image, embed it. The large white X's are at the embedding, and then reconstruct. Okay, it's invertible. Construct, okay, it's invertible. The size here is a square, and there it's not. That's not particularly important. But after that, we can fix one coordinate, which represents the position of one bobblehead, and vary the other one and run it through the decoder and through the PCA that we use. And you see that basically, if we fix the bulldog position, which is this one over here, and vary the other ones at the blue. And vary the other ones at the blue stars. You can generate images where the bulldog. Ah, no, I fixed, I fixed Yoda, didn't fix. Yeah, okay, so this way. So you can see that we generate images which weren't necessarily taken by the cameras where the bulldog is fixed in each one, but Yoda is evolving, but rotating. Okay. An interesting thing about this methodology, even though I presented it as a cross-country. Even though I presented it as a cross product of manifolds, is that orthogonality is again a local constraint. So it doesn't require that the level sets of one figure are the same throughout the level sets of the other ones. So this model does allow you to have some conditional dependence of distributions. The level sets are allowed to change. So if I fix, maybe the easiest example is if we think of If you think of the relationship between a professor and students, if the professor is facing the board, then the students are facing left and right and talking to each other. But then if you turn around, the distribution changes and you're, you know, everybody looks straight. Okay. So there are very different ways of disentangling data. I think this touches upon some ideas that were presented earlier today. Ideas that were presented earlier today. There's geometric disentanglement, which encompasses this informal idea and then alternating diffusion, jointly smooth functions and output informed DMAPs, which people in this room have worked on, and they're more related to harmonic analysis. And then there's statistical disentanglement, which has to do with conditional dependence. Dependence, independent component analysis, and the nonlinear versions of these things. And in general, they seem to do different things. I would like to think that they are all sort of a diagonalization of some relevant quantity, but this changes. And so what each method is achieving is different, even among each grouping. And we do sort of We do sort of, we would like to have an overarching theory of when it is right to do one versus the other. You think of some operator, the notion of diagonalization, somehow you're actually. Yes. And yeah, and so when does one imply the other and when they're actually they are not compatible with each other is an important problem. Compatible with each other is an important problem. It comes up in dynamical systems, but it also comes up in machine learning and ideas that people care about today. Okay, the ideas of what I presented in the first and second part of the talk are these two papers over here. I want to thank Mauro and Solevad who are not here. I think Soledad was not here. For my advisors at Hopkins, and also Eleni and Janice. And I also want to thank Roy and Ronand, who we have talked about these problems many times, and they also gave us the data set, which I think was nice in terms of continuity. And yeah. Yeah, okay, so that is the talk. Thank you very much. So the framework representative is very common in computer vision, specifically when you try to align language and images, where the common component is the part of a text that describes how the The type of approach is based in our recorders are also used with same style of the instructions. The main differences are one, the alignment of the component part is more often than not that we have fantastic for the structure. And second, the one that you play dialectics is the distance. The one that you make dialectics is the distant time and losses are telling you more. So, what I'm curious is what do you see is the advantage of the kinds of losses, both the fantastic part and that is the kind of um maybe you would like to actually try this with the large user division. Yeah, so so there are I so okay statistical things tend to be a type of global constraint, right? You for two things to be statistically independent, you look at marginals and you say that the marginal of one is fixed regardless of the other. This type of geometric thing does not require Of geometric thing does not require that. And that may be a good or bad thing. Let me maybe go, yeah, here. The other idea, okay, and I suspect that this is also the same with other types of geometric methods. There are geometric limitations, which in high dimension, I don't know how they behave. Another thing is that because this is more of a local problem in the sense that locally the constraint is described by analogy, it might be a lot faster. The other question was, I think it's very nice, because you said this is the terminal server, it's very nice that you can begin with a number of articles and then some of them might disappear and take the data that you generate it. Data that is generated from the PCA model and use it to train whether either you observe and get the dimension right and whether there are ways to actually prove it. So a posteriori, you can check, right? You can you can kind Right, you can you can kind of see how the reconstruction behaves as you increase the dimension, so as you increase the number of components that you're using in your reconstruction. So locally, so dimension is kind of not visible when you look at point clouds. It depends on the scale that you're looking at them. And so, in that sense, making a model that has the capacity of a That has the capacity of expressing everything doesn't give you a good way of sort of provably checking. So, what I can give you is that after training the model, I can check whether the gradients and the dimensionality is correct. During training for nonlinear dimension reduction, you cannot do that. You can also not do that for any other existing method. Because even if you compute eigenfunctions, you then need. You then need to have another post-processing step where you check which eigenfunctions are functions at the other end. That's sort of unfortunate, but I think that that limitation is kind of intrinsic to the problem and you can't really like there is a two-dimensional description of something and there is a three-dimensional description of the same thing. One thing that you might care about is exactly a type of complexity of the interpolator, which is something that was. Later, which is something that was the first talk yesterday, a similar idea. So if you look at the Kolmogorov energy, sorry, not the Kolmogorov, the Dirichlet energy or the Kolmogorov complexity of an interpolating one-dimensional curve in a two-dimensional manifold, you get something horrible. And that means that you're not doing well, is the claim. Another example where this shows up is sort of colors, you know, color is one dimensional. You know, color is one-dimensional, you have a wavelength, you can do everything there, but everybody uses three coordinates for color because it's much simpler to do it in that space. Maybe also because the implementation is with LEDs. But anyway, unfortunately, I don't think I can give you a priori guarantees that you're finding the right. 