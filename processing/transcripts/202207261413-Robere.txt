Okay, so next up we have Robert about proof circuits and communication. Hey, thanks. And yeah, thanks to the organizers for inviting me to speak. So what am I going to talk about today? So the starting point that I want to sort of embark from today is starting with lifting theorems, which everybody here in the room is probably at least familiar with or has at least heard of. Many people here have proved them all, so. But as we all know, like lifting theorems have led to a wide variety of new results in communication complexity, and also in particular led to many new results in proof and circuit complexity. So this is just a sample of some of the results that have been proved just in the past 10 years alone. But basically, there's been quite a few, I think, breakthroughs in proof complexity and circuit complexity that comes from these lifting theorems. And how do these things work? I mean, they basically. How do these things work? I mean, they basically are like some sort of system of equivalences and formal relationships between a variety of different computational models. And so in particular, today I really want to talk about the relationships between these lifting theorems and proof and circuit complexity. And really, I think a quite a surprising system of connections exists between a wide variety of models in these results. Of models in these results. And they generalize and extend sort of classical lower-bound techniques, and in particular, they place the complexity of total search problems in center stage. So what's the basic idea of a lifting theorem? Well, everybody here is probably familiar with this sort of very basic setup in communication complexity, where we have some communication model where we want to be able to prove lower bounds, but maybe it's complicated. But maybe it's complicated. Maybe we don't understand how exactly to prove strong lower balance. So, what we're going to do is we're going to start with a query model, say like a decision tree, and start with some function that's sort of hard for this decision tree model. And then we'll take f and we'll take each entry of f and we'll compose it with some gadget for which the input is naturally partitioned in the two players. Partition in the two players, and then like any query algorithm in the model naturally gives rise to a communication problem, like a communication protocol, right? Because what do you do in order to simulate, say, a decision tree? Well, if they want to query a bit, Alice and Bob can just exchange their piece of each gadget and then evaluate it and then continue on down. So, does that make sense? I'm going kind of quickly here because I think that many people are sort of familiar with this, but are there That many people are sort of familiar with this, but are there any questions with kind of this basic setup? Good. Okay, I didn't think so. So, kind of the upper bound direction is sort of obvious. Like, if I have a query algorithm, then I come up with a communication protocol. The surprising thing about lifting theorems is that these things can actually be tight. Like, if you choose G's, these gadget function G's, which are somehow complicated, then this is actually the best possible strategy that you can do. And this allows us to lift query. To lift query lower bounds to commutation lower bounds. So, what this talk today, what I'm going to talk about, is I'm going to focus on lifting theorems for search problems rather than Boolean functions. So, a lot of this workshop and most of communication complexity is kind of focused on the complexity of, you know, Boolean value functions. But today we're going to generalize a little bit and look instead of search problems. I'd like to talk about basically some of the basic ideas of lifting theorems in this. Ideas for lifting theorems in this model. And the reason for focusing on search problems is because when you're looking at the complexity of search problems, in particular total search problems, I hope to convince you that this is just the same as proof complexity and circuit complexity, depending on the query or communication model you want to look at. And this talk is based on a recent column that I wrote with Mika and Susanna. So everything that I say here, if it's a little hand-wavy, is all in there as well. If you'd like to take a look at the On in there as well, if you'd like to take a look at it. Okay, so part one, let's start off with some sort of concrete, sort of a high-level view, concrete view. So what's the basic kind of pipeline to go from a proof complexity result to a circuit complexity result using these lifting theorems? Well, you can summarize it in kind of one slide. So basically, if we start with a proof system, say if you We start with a proof system. Say, if you want to keep something concrete in mind, something like tree-like resolution. It's sort of a classical result that any, that if I, in proof complexity, we're usually interested in the complexity of like refuting unsatisfiable CNS formulas. And it's known that the it's known for like many systems, in particular for tree-like resolution, for example, that the complexity of refuting an unsatisfiable CNF is the same as the complexity of a decision tree which sells. Of a decision tree which solves an associated search problem. And the search problem is simple. It's just if you give me an assignment to the variables of the CNF, output the name of some false clause. And it's known that these two things are equivalent. And I'll go over this in a little bit more detail in a second. But this relationship between sort of perfusing things and this sort of search problem is sort of relative. But then on the other hand, communication complexity. There's also this sort of famous connection between communication models and circuit models. Communication models and circuit models, which are the Karshmore-Vigerson games, the Karshmer-Vigerson connection, right? So it's known that if I want to understand, say, the Boolean formula size, or equivalently, the circuit depth of some Boolean function, then I can equivalently study the communication complexity of a related game, the Karsmer-Viggerson game. But this Karshmer-Viggerson game is itself a total search problem, right? Like it's not a Boolean function anymore. Like it's a relation on any Anymore. Like it's a relation on any given input, I could have many outputs, and I'm always guaranteed that some output is going to exist. So, this is also like a total search problem. So, to connect sort of these first complexity results with these circuit complexity results, we're going to basically use these equivalences with query complexity and communication complexity, and then use lifting theorems in the middle to sort of translate the results in each one. Okay? So, let's first. So let's first, because there's been quite a bit of talk in communication lately, let's first talk a bit more about proof complexity, which probably people need a bit more of an introduction to. And I'll always be talking about things in the realm of total search problems, so let's even start there first. So for me, a total search problem is just going to be a relation on a bipartite domain. And we think of the first part as being kind of the input, and the goal is to find some O such that X and O is in the relation. Always in the relation. And we're also going to require that these search problems are total, so in the sense that for every single input, there's always going to be at least one output. There's always at least one O in my relation. So in other words, this set here of feasible solutions will always be not empty. And now, total search problems have been well studied in lots of things, but in particular, they've been studied in this sort of classical theory of TFMP for total function MP. TFMP for total function MP that was introduced by Papa Dimitriu, right? And of course, there it was introduced in the setting of Turing machines. So here, you know, really we have like a sequence of these total search problems. And then all that we ask is that this relation, whether or not, you know, an input and a feasible solution, we just ask for that relation to be verifiable in polynomial time, by a polynomial time driven shape. But for our setting, for communication complexity, equipment. Setting for communication complexity and query complexity, it turns out to be quite natural to introduce sort of black box or like query versions of these classes and also communication versions of these classes. So what do I mean by that? Well, I have a total search problem as before, and I just want to be able to verify this relationship using either a low-depth decision tree or a low-depth communication protocol. Does that make sense? So hopefully. Yeah, hopefully this is pretty clear. Okay. What do you mean by low depth communication protocol? Sorry? Low depth is always going to mean polylogND. What is a depth strain protocol? Like what do you request treatment? Sorry? What do you mean by depth? Depth. Like number of ranks. Like deterministic communication complexity. Yeah. Any other questions about this I have? And maybe I can also point out that here um the model's really non-uniform. The model is really non-uniform, okay? So I'm gonna kind of imagine that for every possible feasible solution, I have like a decision tree that takes an input and tests whether it's in the relation. Or similarly, for every feasible solution here, now X would be like two inputs, X, Y. You would have a communication protocol that would receive O and then test if it's in a relation. Okay? Okay. So if we focus on the query model first, On the query model first. Well, we'll always be thinking of things asymptotically, so we'll have a sequence of these total search problems, right? And we'll think of the outputs as, I like to write them as just an arbitrary set just for convenience, but just think of them as being like, you know, some reasonable size so we can, you know, we could always encode it in binary if we wanted to. And then I'll say that S is in TFMP formally. If basically for every output O, there's some decision tree of Oh, there's some decision tree of polylog depth that actually verifies that this is in the relationship. Okay? And if you want like a canonical example of one of these search problems, it's the following. If you give me a low-width CNF formula, which is unsatisfiable, well then you can define the following search problem, which is just, if you give me x, just find some clause such that x falsifies locals. Okay? So it's not hard to see that this is total, because That this is total because it's unsatisfiable. There's always going to be some false clause. And it's also in TFNP because if this is polylog n width, well then what's the decision tree? I have a decision tree for every clause and it just checks, queries the variables in the clause and checks if it's false. And now when I say it's canonical, I actually really do mean it's canonical. So in fact, every search problem in TFNP, in the black Problem in TFNP, in the black box version of TFNP, is like the search problem of a CNF in disguise. And this to me motivates like the equivalence between query complexity in the setting and proof complexity and refuting problems. So in particular, if I started with just any total search problem at all, I can write down the following unsatisfiable CNF. I just write down the CNF that says that this has no solution. I take the AND over every solution and I negate the decision. Hand over every solution, and I negate the decision tree. And since this is a low-depth decision tree, I can rewrite it as a bounded-with CNF formula. And so now here we go. We have an unsatisfiable CNF. And it's not hard to convince yourself that, okay, if I have a query algorithm for this, well, then I get a query algorithm for the search problem for this as well, right? Because, I mean, if I know the name of a false clause, then I can find the name of a solution that it came from. Does that make sense? So these are really canonical. Make sense? So these are really canonical. So, like, if I wanted to, I could just define TFMP this way. Like, I think of TFMP in the black box model as really being like this. It's just the family of search problems associated with unsatisfiable CNFs. Okay? Okay, cool. And so now, this relationship between query complexity and proof complexity just follows from this, right? So the query complexity of the search problem is very, very closely related to the complexity of refuting F and related proof systems. Right? So I already mentioned this before, but let's quickly look at the connection with decision trees. So, what does a decision tree for this search problem look like? Well, we all know what a decision tree is, but the basic idea is we want to query variables from the input and then label each leaf with a false clause once we find it. And then the complexity measures are the normal ones. So the size will just be the number of nodes or leaves in the decision tree, and the depth will be the length of the longest path. Okay. And then a resolution proof, in case people like a reminder on how this works, resolution is kind of like the simplest proof system that we study in proof complexity. So in a resolution proof, we're given an unsatisfiable CNF formula that we want to refute. The lines, we think of it as being a deductive system. So I have this sequence of lines. Each line is a clause. And I can deduce new lines from old ones using one of the two. From old ones using one of the following two rules. So either the resolution rule, which just says if I have two clauses that differ on one literal, like a positive and a negative, then I can cut that literal out and deduce their or. And a weakening rule, it just says, you know, I can always add whatever literals I want to the clause. Both of these things are clearly sound. Okay? So just like in analogy with a decision tree, you mean the length would be like the number of lines, the depth is the length of the longest path. Of lines, the depth is the length of the longest path, and a proof is going to be tree-like if the natural proof gag that's associated with it is a tree. Are there any questions about this definition, this slide? This is probably what people are less familiar with, so. Okay, cool. So, how does tree-like resolution relate to decision trees? Well, in fact, it's basically the same object. In fact, it's basically the same object. So here's an unsatisfiable CNF formula. It's basically like an induction, kind of an induction principle, right? I'm saying that x1 is true, therefore x2 must be true, therefore x3 must be true, but x3 must be false. So how do we refute it? Well, here's a proof in tree-like resolution. We just resolve on the, first we resolve on x1, then we resolve on x3, and then we deduce all. But what we can see if we look at this decision tree, the decision tree that solves the search problem is just the resolution proof put like from the top down. So what do we do? The main invariant that you want to sort of satisfy is that the set of falsifying assignments that make it here should be the same as the set of assignments that make it down to this part in the decision tree. And querying this variable corresponds to resolving on it. Corresponds to resolving one. So the isomorphism basically is that the variable resolved corresponds to the variable being queried. Partial assignments correspond to the assignments which falsify this clause, which is like a subcube, right? And then when we get to the bottom, well, the assignment that falsifies this clause should correspond to one of the input clauses in the formula that we started with. And this yields a very tight connection between these two objects, right? Connection between these two objects, right? If I have a size s and depth d tree-like resolution reputation of f, well, this is true if and only if I have a size order s, depth order d, this is your tree for the search problem. And really, by the sort of the correspondence we just saw, you should really be convinced that really, moreover, these are kind of the same thing. Like, I'm just writing them down in different languages. I haven't really, there's nothing really, really mysterious going on here. Okay. Does that make sense? Does any questions about that? Okay. Okay, cool. So, focusing on the second side, I can tell the exact same story that I just told, but now in the setting of communication complexity. So, now we have a communication total search problem. So the only thing that's different is that the input domains bipartite, like I get an Alice piece and a bot piece. And now I'm going to say that S is in TFNP. I could have defined it using. I could have defined it using, you know, low-depth decision trees verifying it, but I'll use the standard NP definition of communication complexity and just say that S is in TFNP if we always have some small monochromatic rectangle cover of the relation. So what does this mean? I have some collection of rectangles such that the union of the rectangles covers everything. And for every rectangle, there's some fixed solution O that it's Fixed solution O that it's consistent with, right? So this is like an NP protocol for verifying it, because how do I verify that I'm in a relation? I guess a rectangle and then I just check to see if my input's actually in the relation. Does that make sense? Okay. And now here we also have a canonical example of a total search problem. And that's the Karshma-Wenderson game. So if you start with some Boolean function, say even a partial Boolean function, you know, Karshma-Wenderson introduces game, which calculates. Carsher Vegas then introduces game, which captures the circuit complexity of f. And it's just the following. So Alice gets a 1 of the function, Bob gets a 0 of the function, and they want to find some input data in which they differ. Right? And moreover, as is well known, this connection also holds for the monotone, if we make all the objects involved a monotone. So if I assume that this function that I begin with, Assume that this function to begin with is monotone, which just means if I flip a bit from 0 to 1, the function can only increase. Well, then we can define the monotone parts for bigger symmetry that's associated with it. And this is just to find some input i where actually Alice's one input is bigger than Bob's 0 input. And we know that this always has to exist just because the function's monotone. Otherwise, Bob's 0 would be greater than Alice's one, and so it would have to be 1. One, and so it would have to be one. Okay, so I expect this is review for a lot of people, but just to sort of put us all on the same page. Now, the next slide might not be review, which is that just like this search problem associated with a CNF formula is like canonical for TFMP in the query world, monotone Karshmir-Figgerson games are canonical for TFMP in the communication world. Okay? So by that I mean if I take any total search problem in TFMP, it's actually a monotone Karshmer-Viggerson game of some partial monotone function in discuss. And I mean, you can actually just explicitly write the function down from the function. It's a little bit technical to write it down, but I mean, the basic idea is I say that f of x, like, I'm going to have an input that I'm going to have an input bit for every rectangle in the cover. And the idea is that basically the Karshma-Figgerson game is going to tell me: okay, am I in this rectangle and not in this rectangle? That's going to be like the correspondence with the input bits. Is this input bit one or is this input bit zero? Okay, so the x inputs will basically write down the ones, write down one side of the rectangle cover, and the zero inputs will write down the other side. Inputs, we'll write down the other side of the rectangle cover, and then we'll just encode it so that if there's a difference, then we can find a pair that's in the corresponding rectangle. So, okay, so I think that this is a really important thing, and maybe somewhat underappreciated, because maybe one thing that's kind of like a common criticism of complexity theory maybe is that we're very, very good at proving monotone circuit lower bounds and very, very bad at proving circuit lower bounds. It is very, very bad at proving circuit lower bounds. And why is that? And, like, this is the reason why, right? Like, what this is telling us is that I could start, in order to prove a monotone circuit lower bound, I just have to prove a lower bound for any total search problem whatsoever. I can pick the hardest one I want, and I'll get a circuit lower bound from it automatically, right, just by running this transformation. So, like, this is a very important thing. Like, this is telling us somehow that the non-monotone Carsburg Biggerson games have way more structure. Bigger synd games have way more structure. Like, this is somehow why this is easy, I think, to prove monos are handled bounds. Okay, cool. And now, I mean, Karshmore and Figgerson, of course, famously showed that deterministic communication complexity is the same as circuit depth. But of course, you know, this correspondence goes even tighter, right? I mean, really, if you go into the depths of the proof, I mean, the protocols are really the same. A protocol for the Karshma-Fingerson game is. A protocol for the Karshmar-Fingersen game is really the same object as a Boolean formula computing the function. And all of these things correspond, and all of these things still hold in the monitoring setting. And there's actually, and this is going to come up later, Rezborov proved in a paper in the 90s that actually there's a way to sort of generalize this connection to capture circuit size using so-called the PLS communication complexity. And I'll talk about this a little bit more in depth, but this is kind of a This is a little bit more in depth, but this is kind of a foreshadowing of what's to come. But, anyways, this is just saying formally what I said a second ago. I mean, if I have any partial Boolean function, non-monotone or monotone, well, I have a size SDFD Boolean formula, if and only if I have a size SDFD communication protocol, and I can plug in monotone everywhere and I get the same equivalence. Okay, cool. So I say here that the correspondence. So, I say here that the correspondence is stronger, like these are essentially the same object. So, here's like an sort of an alternative perspective, which I think is useful when you're trying to think about these things this way. So, if I start with a total search problem, we can define what I'll call a rectangle DAG, okay? Which is just really a communication protocol, but now it could be DAG-like. So it's just going to be a DAG with a unique root node, and every node in the And every node in the DAG is going to be some combinatorial rectangle inside the input domain. So every vertex is a rectangle. The root node is everything. The leaves are going to be monochromatic in the sense that for every single rectangle at the leaf, there's exactly one, there's not exactly one, there's one solution, at least one solution that all the inputs are consistent with. And finally, it satisfies this kind of internal. It satisfies this kind of internal covering property, which just corresponds to Alice and Bob sending a single bit of communication and splitting the rectangle in two. So if R has children R1 and R2, then R better be contained in a union of them, right? So like if Alice sends this bit, well then she partitions it into these two rectangles, which cover the original rectangle, right? Okay. Is it equal or just what's it? Subset. Yeah, subset. Because there could be a couple ways to do the breakdown. You can make it a subset without lots of generality. Let's put it that way. And the subset is important for when you make it dang-like. So now, what's nice about this definition, actually, is that, I mean, I've shown you it in a tree, and it's kind of naturally a communication protocol described in a different way. But this definition generalizes naturally to a data, right? Naturally to a dag, right? Like I could fan out. You know, there's no reason to take a rectangle and fan it out multiple times. And there the subset kind of becomes important, right? Because maybe I want to use the rectangle in multiple different ways to cover multiple different things. And it turns out that, I mean, this, if I look at the rectangle tree size, well, this is just a communication protocol, so it's just the same as the monotone formula size of the QBF. But if you look at DAGs instead, well, this turns out to capture circuit complexity. To capture circuit complexity. Like these things are actually just the communication analog of circuits. Does that make sense? Should we set a high level? Any questions about that? Okay, cool. So what's the main kind of lesson of this first part that I want to communicate? It's that these bottom-up models of proofs and algorithms, like proofs and circuits, they're captured by these. Circuits. They're captured by these top-down algorithms for search problems, right? Say, decision trees in the case of proofs or query algorithms more generally, and communication protocols in the case of circuits. And moreover, the total search problems really are canonical. The search problem associated with an unsatisfiable CNF really captures proof complexity in sentence, and the monotone Karshma-Biggerson game really captures total search problems. Game really captures total search problems in communication complexity. Any questions about this first part before we move on? Hold that one slide. Yep. Oh, okay. Okay. Any other questions? Okay, cool. So let's move on to actually relating to the question. Actually, relating these two things to each other. So, what can we actually say for these models? So, now we're in this center part of this diagram that I wrote down a second ago. But first, I want to talk about the easy direction in some sense. So, turning a query algorithm into a communication problem. This is the easy direction, but in total search problems, there's actually like a nice, there's some things which are actually nice and kind of were observed too late. Were observed too late, in my opinion. So, this translation from query problems to communication problems in the total search sense is really, if you're familiar with proof complexity, feasible interpolation. So what is that? Well, let's think about it. If I have a query problem, say a total query, like a total search problem, it's like a that's in the query world, how can I turn it into a query problem in the communication world? Problem in the communication world? Well, one obvious way is to do the lifting thing, like do a composition, right? But there's an even stupider way to do it, right? What's the most obvious thing you could do? Well, you could just take the input variables and just partition them into two pieces. Give one side of the partition to Alice and one side to Bob. So if you give me any variable partition of my query problem, well, then we could define this partitioned problem as, you know, Problem as you know, Alice gets one side, Bob gets the other side, and they just want to solve the original search problem, right? Cool, okay. And I mean, it's obvious that if I have a query algorithm, then I get a communication protocol for this partition problem, right? When you just, when they query a bit, whoever owns that bit just sends it to the other person, right? So it's obviously the depth is upper bounded. But when you think about the equivalences with circuits and proofs, this is a feasible interpolation requirement. This is a feasible interpolation result. This is saying that if I have an upper bound for a particular formula, unsatisfiable C in F formula, then I get a upper bound for a circuit computing a related Boolean function. This is this. And actually, this partition, this simple partitioning trick, this was somehow not observed until 2016. And this was just the key trick that enabled us to prove these lower bounds for cutting plates. These lower bounds for cutting plates for random CNS. So, feasible interpolation, people who are more familiar with it are probably used to this setup where you have a partition of three variables and you shared variables and one side is unsatisfiable. That's equivalent to this setup here. It's just taking a query search problem and partitioning it in two pieces. There's a way to go back and forth between these two things. So, if you'd like to see it in a picture, well, here's what it looks like as a picture. If here is an unsatisfiable CNF, so like I've laid out the key. C and F, so like I've laid out the cube kind of in a line, right? Since it's unsatisfiable, every single input is covered by one of these clauses, right? Some clause is going to falsify everything. So when I take a partition into two pieces, well, all of these clauses just turn into combinatorial rectangles, right? Because since they're clauses, I mean, in order to falsify it, I have to falsify this side and also falsify this side, right? So the set. So the set of clauses turns into a monochromatic rectangle card, which is a total search problem in TFNP. It's canonical. All total search problems are kind of achieved, right? Okay, cool. So this is just saying what I said a second ago formally. So, you know, the basic point is just this. If C is a clause, then this combinatorial rectangle, this thing, this bent clause where you partition in two pieces is itself a combinatorial rectangle. Is itself a combinatory rectangle? You can split it up. And if you take any clauses of f, then you just recover a rectangle covering of your entire input space. And now, so you can also ask, well, okay, we know that this thing is equivalent to some Boolean function. I showed you this definition a couple slides ago, but it was kind of technical. Is there an actual nice function that it corresponds to? Or is it just something random we can't? To? Or is it just something random we can't really hope to explicitly write it down? And it turns out that there is a nice function. In fact, there's several ways of expressing it. So in this paper that I wrote with Noah, Dennis, and Tony, we gave one definition that uses this Monotone CSPSAT function, which is kind of the common use in rifting theorems. Sort of at the same time, Hrubesch and Pudlock gave another definition called the unsatisfiable certificate. Called the unsatisfiability certificate. So I'll define, I'll show you the unsatisfiability certificate because that's the one that's a bit further away from the domain. Many people have seen the existence that one before because it also appears in communication clause group. So what's this unsatisfiability certificate? Well, if f is an unsatisfiable C and F formula and I take a partition of its variables, well, I'm going to take every clause and partition it into its two pieces, according to this variable partition. And I'm going to write down the following function. And I'm going to write down the following function, which depends on this partition, but I'll suppress it. Okay? And it's a little bit weird, but so here's what it is, just flat out. If you give the number of variables in the function as the number of clauses, so I have one variable, think of having one input variable for every clause. And then what I'll do is I'll first look at all of the clauses where the corresponding variable is zero. And I'll check. Okay? And I'll check if the x part is satisfiable. And if so, I'll output one. And then, if that's not true, I'll look at all the clauses where the ith bit is one. And I'll check if the y part is satisfiable. And if so, I'll output zero. And otherwise, if neither of these things are true, then I'll just output star. It's a partial function. So why is this? So, I mean, in a picture, we can actually see an example, say, so here's a partition with its variables. So if this was my assignment, So if this was my assignment, I'll first look at the zeros and check if that's satisfiable and output one. Otherwise I'll look at the ones and check if that's satisfiable, and if so it'll output zero. But notice they can't both be satisfiable, right? Like if both of these conditions were true at the same time, well then the whole formula would be satisfiable, which is impossible. Does that make sense? And now also you can observe: well, if I have a yes instance, Well, if I have a yes instance and a no instance of this function, well, that would mean that I would find a place where z i is 1 and z i is 0, right? Like, in the 1 input, it would be 0, so that's false. And in the 0 input, that would be 1, so that's false. So the Marmotton Carson refigured some game tells me a false clause of the underlying formula. So, I mean, when you combine this, when you So, I mean, when you combine these things together, you can just immediately deduce the classical feasible interpolation results. So, here's one of the classical feasible interpolation results. It says that if we have a resolution refutation of size s, then there's some monotone circuit that computes this unsatisfiability certificate of size 4s. And whereas the old proof, I think if you look at the old feasible interpolation proofs, they're like a little bit, at least to me, they're like a little bit magic. You kind of got to look at them and just sort of work through the gates. At them and just sort of work through the gates one at a time. Like, I start with a resolution proof, and then I have to somehow construct a circuit which computes a related function. So, I have to inductively look at the proof and do something wacky. But once you look at this communication world, it's like a one-picture proof. So, what do we do? Well, okay, so here's my resolution refutation of my formula that I started with. Okay? Well, how am I remember now if by using this? How am I? Remember now, if by using this characterization I showed you a second ago with rectangle degs, I just need to provide you a rectangle deck. So what am I going to do? I'm going to take these clauses. I'll partition them according to my partition. But now each of these things is a rectangle. And it's easy to verify that the union property is satisfied under the resolution law. So there you go, you're done. Now you just use the equivalence with this with monotone circuits. Does that make sense? That makes sense. Okay. Cool. So once you see this sort of really simple way to do these things, you can go through the literature and just deduce all the feasible interpolation theorems more generally, like in an equally transparent way. So resolution in monotone circuits we just saw, I mean, tree-like resolution in monotone formulas. Tree-like resolution and monotone formulas, you just use the fact that those things are tree-like and everything works out the same way. If you want to go from cutting planes to real monotone circuits, this was basically done by Pruvich and Hoodlock. If you want to look at more algebraic or semi-algebraic systems, you could look at, say, Noel-Stollenstat's proofs will turn into monotone span programs. It's basically just policy ideas from a thesis. And if, and very recently, like last year, I mean, we showed that actually you can. Last year, I mean, we showed that actually you can get feasible interpolation, monotone-feasible interpolation, even for proof systems for which there wasn't really monotone-feasible interpolation before. So we could take Chirali atoms, and we can show that it's basically feasibly interpolated by extension complexity, or linear separation complexity. And a similar thing holds for SOS as well. This is really a sort of a one-size-fits-all kind of technique. It's quite simple. Okay, cool. So, this just, so my whole reason for motivating that was just to sort of give a, I think that this gives, this perspective gives a nice explanation of all of these classical results in proof complexity and circuit complexity. But now, let's talk about the other direction, which is like lifting the actual sort of equivalences between these two things. So we've just seen that the query complexity of this total search problem is always at least the communication complexity of this partition. Least the communication complexity of this partition search problem. So it's natural to ask: does the converse hold? Like, are they just the same? Maybe that would be a miracle if that was true, but unfortunately, it's not true. And in fact, there's like canonical examples. If you're familiar with these so-called sighting formulas, it's pretty easy to convince yourself that the corresponding partition search problem is easy for communication complexity, no matter what partition you pick. So, how do we fix this up and get to Converse? How do we fix this up and get to converse? Well, this is where we use composition. Okay, so now what are we going to do? We're going to take this total search problem and we're going to pick some gadget function. And instead of just partitioning the variables, we'll do this substitution. So we'll substitute into every input bit this gadget entry, this g x and y. And now the variable partition comes from the inject. And as I said at the beginning of the talk, well, if this gadget that we started with was somehow Gadget that we started with was somehow, I don't know, complex, then the best strategy for Alice and Bob in the communication world is to just simulate the wording strategy. Okay? So here's a formal theorem. So it basically goes back to Ross McKenzie, but was really given a nice treatment by Mikatoni and Tom. So if we start with any total search problem at all, and you just choose the index gadget, for example, well, if we let m to be polynomial in n, then the If we let n be polynomial n, then the communication complexity of this composed problem is just the same as the decision tree complexity of the underlying search problem. And this thing has been very greatly improved over the years. Like, this gadget has been now decreased. We know that it doesn't have to be the index gadget. It can be like any gadget with low discrepancy, but it works with ArchiveDev and others. But let me just point out that by combining this together, this is how you get this relationship with. Is how you get this relationship with resolution depth and monotone formula size. So we can just interpret this last theorem using these equivalences that we saw at the beginning in terms of the proof and circuit complexity models from before. And actually, I mean, well, let's see. Maybe one thing that you could criticize here is that, well, this function here is going to be like some. Here is going to be like some weird unsatisfiability certificate, you know what I mean, associated with this f and this gadget. These unsatisfiability certificates are not only like artificial functions that are constructed for the purpose. In fact, they're totally natural functions that we deal with every day. So, for example, if you in fact, the function that you get really only depends on the formula that you started with. The formula that you started with. So if I start with the formula being like this induction principle that we saw earlier, like x1 implies x2, implies x3, and then x3 is false, well, the corresponding lifted function is just ST connectivity. In fact, the unsatisfiability certificate is literally just ST connectivity in disguise. And if you start with, say, these pebbling formulas, which I won't define, but maybe people who've seen them before in communication or proof complexity, well, then this lifted function is this general. This lifted function is this generation problem. And you can capture other things too. You can capture things like clique and other problems like that. So these actually produce very natural families of functions, which I think is also quite surprising. So you might ask, okay, how can I say that the function depends on just the formula? How can that be? Where does the gadget come in? It must change something. Because we know that picking different gadgets changes the complexity of the problem, right? Well, the answer is that. Well, the answer is that different G's are going to give me different one instances and different zero instances of the problem, right? So when I, for certain hard G's, I'll get like all the, if I take the index gadget, for instance, I will literally just get all the minterms and all the max terms of my function. But you can like pick other hard g's, and this will modify the instances into like sparse, maybe sparser families that are also still hard for a circuit complexity. Circuit conflicts in me. So, this is how you should kind of think of these two things, I think. And of course, the you know, that's just talking about circuits and formulas is just kind of the tip of the iceberg here. I mean, we now know these lifting theorems for like lots of different models and lots of different gadgets, right? So, for resolution and tree-like resolution, these correspond to circuits and formulas, right? Formulas, right? For more algebraic things, no-Solensatt's degree corresponds to span program size. And I'd also like to point out that we do have limited sort of lifting theorems for Shirali-Adams' degree and the corresponding circuit complexity measure, which is linear or semi-definite extension complexity. But something that's a little bit funny about these is that these things are somewhat incomplete because they, well, it's a technical problem, but it's something to do with like a It's a technical problem, but it's something to do with like a 1 over n and an exponent, basically. It kind of corresponds to a low coefficient version of this thing in some sense. But yeah. So improving either one of these things, I mean, improving both of these things I think are very important open problems. They're somehow right at the border of what our techniques can handle. Okay, so yeah, any questions about that second part at all? Oh, the incomplete, like the incompleteness part? Yeah, so the problem is basically that you don't get, like, what you end up getting is something like the extension complexity of whatever this, I'll just write it like this. The extension complexity of this lifted thing, whatever that means, what you would like to get is something like n to. N to the Schirali atoms degree of F, right? But you don't quite get that, actually. Like you get something like you get something slightly weaker than this. There's like some coefficient shenanigans going on here that mean this isn't actually tight like this. Yeah. Yeah, I can explain it more offline. It's kind of hard to say without writing down a lot of definitions. Down a lot of definitions. Any other questions about anything in this part? Okay, cool. All right. So let's talk about where I think this is all going. Okay, well, hopefully by now I've kind of motivated, you know, that sort of think of these things as being characterized by total search problems. So this means that, and this is the big project that I've That, and this is the big project that I've been working on lately, that we should look to the theory of TFNP in the classical Turing machine world for inspiration. Okay, so let's just very quickly run over what the theory of TFNP actually looks like here. So, here's some of the classes of, here's the sort of TFNP complexity zoo here, if you like. So, what is the theory of TFNP? Well, it was introduced by Papa Dimitriou in 94. Papa Dimitriou in 94. And so we think of TFMP in this setting as being the class of NP search problems for which a witness always exists and is verifiable. And the way that we define this class is by taking fixed, interesting problems that are usually existence problems of like finding some existence. These are often associated with theorems. And then take polynomial time reductions to them. Okay? So oftentimes these problems represent existence theorem. Oftentimes, these problems represent existence theorems, like things like the hand-shaping lemma or fixed-point theorems. And there's lots of connections between TFNP and sort of many other fields and complexity and theoretical computer science and beyond. So let's maybe see an example. So the handshaking letter. So here we go. So the hands-haking letter says every graph has an even number of odd degree nodes. So how can I turn this into a total search problem? Total search problem? Well, the answer is like this. It corresponds to the TFMP class called PPA for polynomial parity argument. And so we think of the input as being some set of nodes describing our graph. And I'm going to have a fixed node in the input, which I'm going to enforce has odd degree in the graph. And then the graph itself, and this is kind of the special thing about TFNP, is going to be implicit. About TFNP is going to be implicitly represented. So it's not like we get to look at the adjacency matrix of the graph. In fact, the graph is going to be like exponentially large, and we only get access to it via a neighborhood oracle. So we have some function which, you know, like in maybe in the classical world, it would be represented by a Turing machine. In our world, it could be represented by a decision tree or a communication protocol that receives the name of some node in the graph and then just outputs its neighbors. In the graph, and then just outputs its neighbors. Okay? So now, the feasible solutions of the problem for this case are going to be the following. Well, I'm going to have this special distinguished node, which I'm going to force to have odd degree. So in particular, V0 will be a solution if the degree is not 1. And then every other node will be a solution if the degree is odd. And we can actually assume without loss of generality that the degree of odds. Actually, we assume without loss of generality that the degree of everything is at most zero. So the problem is: we can start with v0 and we want to find some other node with odd degree in the graph. So this is like a total problem. It'll always exist by the handshake and whatnot. Okay, good. And so, you know, formally, we need to encode these things kind of succinctly. So we think about the graph as being really, really big. I've written down here in terms of the class. I've written down here in sort of the classical Turing machine world, but it's very natural to substitute these things for, say, decision trees that tell me the neighborhood or communication protocols that tell me the neighborhood. And in that way, we would get the black box or the communication versions of these classes. Is there any questions about this problem? Hopefully it's pretty simple. Okay. So here's kind of the world that we've seen so far. Here's kind of the world that we've seen so far. So, TFNP contains this class PPA. It's contained in P to TFNP because I could always just guess a solution and verify it, right? Just guess a node and just check that it's degree bun. But in fact, of course, we have, Papa Dimitri, when his original paper introduced this sort of family of classes. These are like the classic five classes. And they all correspond to different kinds of existence theorems. So PPA corresponds to the hang checking lemma. Well, we have this line of We have this line of classes PPP, PPABS, and PPAD, which themselves correspond to variants of the pigeonhole principle, like just the standard, fog standard pigeonhole principle, or the pigeonhole principle where I want it to be injective or maybe bijective. And then on the other side, we have this PLS class, which kind of corresponds to like local search. Like it's the principle that says every directed acyclic graph has a sync note. Okay, cool. So now, um, So now, what about in the communication and in the query worlds? Well, we've seen a couple of examples, right? We've seen P. These are just polylog n-depth decision trees in the decision tree world. And, you know, the TFNP is going to be the class of narrow and satisfiable CNFs. And similarly, for communication, you know, P is going to be the class of Boolean formulas, and TFNP is the class of rectangle covers. And just like in the classical setting, we can. Just like in the classical setting, we can define other classes in this world by reductions. Okay, so we fix some complete problem and we take now shallow decision tree reductions or shallow communication protocol reductions to it. And what's cool, sorry, is that this actually gives us the ability to characterize proof systems and circuit classes beyond just the classical Karshmore-Figgerson connections or the tree-like resolution. Karshmore-Figgerson connections or the tree-like resolution and decision tree connections. So, what about PPA, for example, just to keep this run an example on the go? Well, in fact, way back in the 90s, in this nice paper by Dean Clag Edmonds, Paliaso Potesi, they observed that if you take the unsatisfiable search problem, and I'm going to say it's in PPA, which just means there's like a low-depth decision tree reduction to this hands-haking lemma problem. Well, that means that this Well, that means that this original unsatisfiable formula F actually has a low-degree null-Stellensatz reputation over F2. And they didn't observe the converse, but we observed a few years ago that actually the converse holds. So if I have a low-degree null Stellensatz proof, then in fact, I have an efficient reduction to PPA in the decision tree world. So in other words, like, you should think of this as being like an algebraic analog of this. Analog of this, you know, decision trees correspond to tree-like resolution results. Now, if we look at the class PPA, we're saying it corresponds to null-stull and satisfactions. So formally, we can say this, if you take any unsatisfiable CNF formula, say if polylogarithmically, well, there's a quasi-polynomial size and polylog, sorry, degree, no stolen sets proof, if and only if this search problem is in the classic PPA. Is in the class PPA. So this is a characterization in terms of total search problems. And you can give a similar characterization for communication PPA. So now the corresponding model is monotone span programs. And this shouldn't be surprising because this is kind of like the monotone feasible interpolant for null cell insats, as we saw, as I mentioned a while ago, right? So these things are all just kind of naturally related to each other. And taking it all together, this gives us this sort of meta-feasible interpolation result. Meta-feasible interpolation result. So if I start with any total search problem at all, and I take any well-defined TF and E class, well then if I have a decision tree formulation for S, well then I can get a communication protocol for the partition problem with the same complexity. And we can actually prove converse lifting results for a couple of special cases here. But you know, I think it's a big, it's an excellent project to kind of figure out and push this and see how far. kind of figure out and push this and see how far it really goes. So just to kind of wrap up a bit, so if we look at the query TFMP classes and the proof systems, well here are the classes that existed before. These two collapses actually, the fact that sync of potential line equals PLS intersect PPADs and end of potential line is this intersection is new. This is just a paper by us that appeared at Complexity. So we have proof systems. So, we have proof systems now that correspond to these two classes here, just by basically the argument that I showed you much earlier in the talk. And, you know, a few years ago we showed that Nels-Telenstats corresponded to PPA. Well, what about all these other ones? Do they have proof systems associated with them? Well, the answer is yes. In an upcoming, in the Fox paper that's coming up, we've given proof systems for everything here, except for PPP. And what's interesting. And what's interesting is that all of these proof systems are like totally, totally natural. So if you look at PPABS, for instance, which corresponded to the injective pigeonhole principle, the corresponding proof system is Shirali atoms, albeit with low coefficients. And if you look at the bi-jective pigeonhole principle, this somehow corresponds to low coefficient null Stellan sets proofs over the integers. And what's particularly interesting in this picture is that it gives intersection. Is that it gives intersection results. So we know that PLS intersect PPADS is equal to this class sigma potential line. And we can also give a proof system that corresponds to this. We call it reversible resolution. But in fact, it's been really well studied in MaxSat solving as the sat solving version, as the right version of resolution to study MaxSat solving algorithms. And so what we can actually And so, what we can actually deduce from these characterization results is that this kind of fact, which if you told me that it was true before, I would have thought it would have been impossible to prove, that you have a short maxat resolution proof if and only if you have both an efficient resolution proof and an efficient low coefficient Chirali Adams proof. Which, like, is a very strange result. Like, I really wouldn't, I have no idea how to prove this. I have no idea how to prove this directly. The way that we prove this is by converting them to the query algorithms using our collapse result and then converting back. I really don't know how you could prove this directly. And we can also, of course, characterize the other one, the end of potential line class. It's a natural restriction of this reversible resolution class. So you can talk about communication TFNP as well. Well, here we know, just like before, we have Boolean circuits, we have Boolean formulas, and in this recent sample. Formulas, and in this recent same paper, we showed that spanned programs corresponded to PPA. But actually, we really kind of have no idea what's going on here. So it's not totally hopeless. I mean, because of the feasible interpolation results, we actually pretty much know what these two should be. But these intersection classes, I think, are very, very interesting now. Because if you could prove that a circuit class, you have small circuits in one class, if and only if you have small circuits in another class, you know, class A and class B, that's like kind of a very A and class B, that's like kind of a very, very interesting striking results we have. So, this is a like, and this is only the tip of the iceberg. I'm far from showing you all of the classes in TFNP. These are just sort of the classical ones and some of their intersections. So, the general picture, sort of the general program, research program, if you like to call it, is to look at query TFMP and communication TFMP, relate them. TFMP, relate them back and forth using feasible interpolation and lifting theorems, and try and understand what these things are in terms of proof systems and circuit classes. So let me just mention that this is, there's no hope of making this like a universal theory. Like you can't conjure up a circuit class and just hope that there is a TFNP class that associates with it. We even have kind of examples. So one of the most prominent examples comes from Cutting Blanes. Example comes from cutting planes in the group complexity literature. So we have a feasible interpolation result for cutting planes that uses so-called real monotone circuits. And there's actually a communication analog that sort of captures this, this sort of real communication complexity, if you've heard of it. And if you want to write it down in like a DAG-like model way, you have to think about it like basically it's the same thing, but the shapes at each node changes. So before we had a rectangle, it's kind of faint here, but maybe. Rectangle. It's kind of faint here, but maybe you can see it's a rectangle. When you're looking at these real models, you actually get these triangle bags, which are kind of wacky. So this isn't a universal theory. It doesn't capture literally everything, but it seems to capture a lot of things that we've studied so far in the literature. And so it's an interesting question to try and see if there's other analogs that can kind of capture these semi-algebraic models as well. So let me conclude with some open problems. Problems. So, oh, okay. I gave this talk in Edinburgh a few weeks ago, and actually, the first problem has been resolved. So, we actually do now know a TFNP problem that captures sums of squares. This is upcoming work due to Alargo Banacina and Maria Luisa. So, you can ignore that one. But that's pretty cool. Sum of squares also somehow fits into this model. And then, kind of, it's a very sort of open field right now. Like, it's very. Open field right now. It's very, very wide open. The most obvious thing is to take TFMP classes that we know and try and come up with proof systems or circuit models that characterize them. Or the converse, come up with circuit models or things that we have and come up with classes that characterize them. So in the classical proof complexity setting, we have these sort of, like I mentioned already, we have these semi-algebraic systems that for some reason don't fit in, even though other semi-algebraic systems like Shirali atoms and sum of squares do. So understanding this difference. Sum of squares do. So, understanding this difference, there are these famous boundary problems in proof complexity. Like, oops, I don't know what happened there. Oh, sorry, everybody. My laptop decided to die, apparently. I'm out of power. What? Might be out of power? It's actually on. Uh it's actually on. It's just frozen a little bit. Yeah. Well, I can just write them down. So another really natural question is to understand number on forehead communication, like are these analogs. In fact, we know some kind of limited relationships between number on forehead complexity and lobots driver proofs. So we know that there's at least some precedent there for a connection. Connection. And also, perhaps most interestingly, is like the question of non-monotone circuit complexity. So the reductions that I showed you are kind of the, you know, in hindsight, the obvious ones, right? You start with a query problem and you just convert it to a communication problem in kind of the dumbest way. You just partition the inputs, right? You just partition the inputs, right? And we know now that if you do this, you just get some monitor Karshmore-Winkerson game. But we would like to get a Karshmart-Winkerson game. So, like, is there, like, we know circuit complexity is hard, but like, how much more, like, what this shows us is that, like, monotone Karshmore-Vigrasson games somehow can have arbitrarily complicated structure, right? While Karshmore-Vigrassen games must have very, very special structure. So, is there some way of interpreting that in this theory to actually make any Interpreting that in this theory to actually make any progress on communication with Privacy, friends. But other than that, yeah, thanks for listening. I'm sorry, my computer broke, and yeah, happy to answer any questions. Hey! So, uh, questions? Okay, maybe we'll all want to touch the coffee next day. What? Yeah, well, it's like this one over n. Yes, plus one over n. Yeah, it's plus one over n here. And you can actually, I mean, I don't know, we have this paper where we give this new characterization of strategy atoms, and like that one over n is really naturally interpreted there. It's not nice people than the macro, which is the end of this thing. Yeah, exactly. But somehow, when like what it corresponds to is like the coefficients being really small so it's not like you don't need the mean equation. I think that's right. Yeah, probably when you're doing is sort of like solving for A and assuming B is B is the same.