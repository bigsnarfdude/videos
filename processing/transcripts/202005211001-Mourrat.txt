Refrain from using the chat. Related to the chat, we welcome questions during the lecture. The moderators will try to keep an eye on the questions and flag anything important to the speaker. And JC also does a remarkable job of moderating the chat himself and responding in real time. So please feel free to ask questions. Free to ask questions. If you do, it would be very nice for you to use your full name as your display name if you feel comfortable doing so, just to create a feeling of real personness to our event, let's say. So, once again, we'll plan on roughly an hour lecture with a brief pause in the middle for a breather. And I think with that, maybe I'll just also remind everyone that next week's course is being given by Gaddy Cosma on critical and near-critical percolation. And that course will run on roughly the same schedule as this one, Monday, Tuesday, Thursday, starting at 1600 hours UTC. So we hope to all see you back for that course. Okay. Okay, with that, it's a pleasure to. Minor correction next week is Monday, Wednesday, Thursday. Ah, thank you for the correction. Sorry about that. So we'll send out the link to the Zoom meeting to the mailing list as usual. Okay, with that, I'm going to hand it over to Jean-Christophe Morat for the third of his three lectures. Thanks. For the third of his three lectures. Thanks again for being here. Thank you very much. So, yeah, again, thanks a lot for making this possible. So, let me try to share my screen. All right. So, yeah, thanks. I hope you enjoyed it. For me, so far. I hope you enjoyed it. For me, so far, it's been a good experience. So, I hope it's also a good experience for you. Make sure I see the chat. Yes, so that's good. All right, so in the last lecture, what we have done is in the first lecture, we saw this Hamilton-Jacobi equation arise from this QEWIS model. And in the second lecture, we tried to We tried to understand how to justify the rigorously the convergence to the limits. So it was almost satisfying a first-order equation, and there was some one over n kind of error term. And we try to understand how to show that in the limit, indeed, this error term disappears in some sense. And that was for the Kerryvice model. It's a very simple model, but now, as promised, finally, I will discuss the problem of. The problem of estimation of a rank-worn matrix. So, before I go into this, let me mention that this has been already solved by other people. So, I think the first that gave a complete resolution of this problem is a paper of Lolarge and Miolan. And it's also a good reference for looking at like they give a lot of context into the problem. A lot of context into the problem, so it's a great read. And shortly afterwards, Barbie and MacFris proposed an alternative method to solve this problem. So I think both papers have been published in PTRF last year. And now I'm going to discuss yet another method based on my own perspective with this partial differential equations. All right, so if you So if you don't feel too confused by the first two lectures, I will start by describing the problem. So it's introduction and also I will state the main results in this part. So this is the problem. Let's say that Let's say that we have a vector of independent random variables and the lower I will write lower i will write p sub n okay so so it's really a tensor of the of one single measure okay so they are independent and identically distributed and and for for some t positive we we observe the following we observe so i'll use matrix notation so so we observe the matrix y which is some factor which okay Which okay, it's convenient to normalize it as square root of 2t over n, okay, right, times this rank one matrix, x bar transpose, plus some noise. Okay, so the noise is very simple, it's just a big matrix full of independent Gaussians. So if you don't like matrix notation, we're observing products of x bar i, x bar j for every pair i and j between 1 and n. And this is multiplied by some factor, and then there is some noise added to this. Okay. And so again, I don't assume. And so, again, I don't assume any symmetry, it's not critical. So, they are really completely independent. And you see that t, in some sense, encodes a signal-to-noise ratio. So, if t is very small, your signal is very weak. It's going to be hard to recover information about x-bar. If t is very large, on the other hand, the signal-to-noise is very good. We have a good chance, or at least we can be optimistic that there's something to be done about understanding what X-bar is doing. Okay, so we want. Okay, so we want to recover information about x-bar a given y, and we want to understand how well we can do if we, you know, I'm not going to worry about algorithms, I'm going to assume we can find the best possible estimator given the data. So for instance, we we may want to understand the so for instance we want to understand To understand the minimal mean square error, which depends on n and on this parameter t. So I normalize by 1 over n square. And this would be the best thing we can do, like the best estimator, so the infimum over all functions, which I call theta hat here, of expectation of x bar. x bar x bar transpose minus whatever function of the data of the observation y and then i square um the t and the square root and the t on top of x bar oh yes haha bad notation thanks so i should not uh yeah so i'm used to writing transpose like this but that's bad notation so maybe i should write like this yeah that's a very good Yeah, that's a very good yeah. So I yeah, I hope each time t is an exponent, it means transpose. Thanks for the question. Okay, so here, so in this in this expression, I use this absolute value symbol and it's a matrix inside. So I just mean you sum the square of the coefficients. So you can just think it's a big vector. And by the properties of the L2 norm, the best possible L estimator is. The best possible estimator is just the conditional expectation of that variable given the data. So, this minimal mean square, you can rewrite it as 1 over n square expectation of x bar x bar transpose minus the conditional expectation of that same thing, x bar x bar transpose given y. Yeah, so I already gave up on trying to. You know, I already gave up on trying to denote transpose properly, right? So, here t is only a transpose. Okay, so this is a first motivation to want to understand the conditional law of x-bar given y. So, theta was just some, you know, so someone is asking what is the meaning of theta. It's just I'm trying to do the inf over all possible algorithms, if you want, or all possible functions of the observation. Of the observation, and I'm thinking what is the best possible thing, but this is just an L2 projection, so it's the conditional expectation. Yeah, I will not justify this, but it's a classical observation. So I want to compute the conditional law of x-bar given y, and I will not do it rigorously, but let me do some informal calculation. Me do some informal calculation. So today the pace will be a bit faster than usual. Okay, so yeah, so someone is asking about the normalization. You see, you will have n squared terms inside the matrix is n by n. So at least, you know, it's bounded by one if I put one over n squared. Yeah, I'll try to maintain a bit of a faster pace than before today. I hope you will be on board with me. I mean, feel free to still ask. Me, I mean, feel free to still ask questions, but uh, please, please don't be uh, sometimes you will have to take my word for a few things, okay? But for normalization, I hope it's clear. All right, so if I try to calculate the conditional law of x-bar given y, and if I just do very shaky and non-rigorous calculations, sorry, that's not what I meant. Oops. If I try to calculate the probability that x-bar is equal to x and that this capital Y is equal to y, well, okay, so you know it's not a regular thing, but maybe you know I can think of it as being like being like dpn of x. And then, so once I have sampled x bar, if I want to know that, if I want this capital Y to be equal to the y, I have to basically prescribe, let me I have to basically prescribe, let me display to you again what this capital Y is displayed here. You see, so if I have fixed this guy, now I have to impose that this W be Y minus the difference. And W we said it's a Gaussian. So what comes here to complete this will be this kind of Gaussian term. So exponential minus one half of the square of that value. So it's y minus square root of 2t over n. 2t over n, x bar, x bar transpose, everything squared. Okay, so this is the Gaussian part. So it's not frequent, and I apologize for this, but I hope it gives some at least vague intuition that when we want to compute the conditional law of x-bar given y, this will look something a bit like this. So it will be this x-bar. Will be this exponential that is above, so maybe I will not rewrite it. dpn x and divided by oh yeah I should write it the y divided by integral of exponential of minus that same thing maybe dpn of x prime okay maybe I should okay I should have written it explicitly let me take a step back so Take a step back. So one half y minus square root of two t over n. Oh, yeah, I'm making plenty of mistakes. So here there are no bars. x prime, x prime transpose squared dpn of x prime. Okay, so I'm kind of rushing it a little bit, but it's it's a I hope you kind of get the gist of it. And I want to expand this square, which so inside here, I should have written it, but there's also a square. And there's a one half. And it starts with a y, and then, okay, I'm almost done writing it. Then there is some x inside here. So I want to just expand it and remove the absolute value of y. remove the absolute value of y squared. So let me just keep the two other terms, the cross product and this xx transpose square. So let me introduce a definition. So let's set H n zero of T N X to be. t and x to be so so the cross border between the two terms is square root of 2t over n y dot x x transpose and again these are matrices I just think of it as the entry-wise dot product minus and then there's the square which divided by two so it's t over n and T over n and then it's xx transpose everything squared so it's just the norm of x to the power of 4, the Euclidean norm of x to the power of 4. Okay, so that's that's I'm going a bit fast, but in some sense I want to rush through this because the crucial, you know, the exact definition of this is not absolutely crucial. So the point, okay, so here you want you should remember that why At y is this sum of two terms. So there's this x-bar, x-bar transpose, and there's this w. So I'm going to rewrite it explicitly in this term. So hn 0 of t and x. So inside y, there is this noise w. And if I expand this term, so I have w dot x. So I have w dot xx transpose, but I can rewrite it like this. And then I have x bar x bar transpose, which is going to be dotted against xx transpose. So plus 2t over n. And okay, up to some little manipulations, it's just the dot product of x and x bar squared minus t over n x to the 4. I don't do anything with this guy. Do anything with this guy, okay? I leave it as it is, all right. So I rushed to this, but let's take a deep breath now and see what this says. So this term is the most important term, in my opinion. And you see, it's really like the spin glass case I discussed at the very beginning, except I changed the names of the variables just to see if you're following. To see if you're following. So, if you think that w was the j I was using at the very first lecture and x is the sigma, then you see this is this sum of j i j sigma i sigma j. Except I changed the names. So, this is like the spin glass. And this, we'll just forget about them. They are just here to help us in some crucial places. We see very shortly where, but other than Very shortly, where, but other than that, we can. I mean, they are not negligible in the sense that they are smaller, but in some sense, they are just here to make our life easier. So, let's just not worry about them. Okay, so I want to try to make the connection between the spin glass model and this model. And it's really showing here, right? When I write the conditional law of x-bar given y, it has the form of a Gibbs measure with. Form of a Gibbs measure with at least the most important term, at least in my opinion, let's say, looking like the spin glass case. All right, so let me try to wrap this up. So we have defined this function. So let me remind you the context. In our problem, there is w and x-bar that are, you know, that. bar that are you know that are inside the probability space if you want. x bar is this vector of independent variables. W is this matrix full of noises. And then I define this function of x. This is just a function of x and it's random because it depends on w and x bar. And from that function I want to study the associated gift measure. And I will use the same notation as for the pre-evice model with this bracket. And I use this little And I use this little x as the random variable inside this bracket. And so I define the geek measure according to this function. So integral of. So instead of writing sum over sigma as before, I write integral. So it strikes my fancy to write integrals. f of x exponential of this function against the the Against the reference measure divided by integral of x of h n zero of tn x dp n of x. Okay, so that's our Gibbs measure. And notice that the important difference between the Curie Weiss model and this model is that now our energy function here is still random, right? Is still random, right? So, for any realization of x-bar and w, we have defined this probability measure, okay? This probability measure with the bracket. Is that clear? Are you okay? I told you I would go faster now. Nobody has questions. So, maybe I'll state it as an exercise. Which, okay, some people take this for credit. So, let's say it's an exercise for credit. So, I rushed through a kind of heuristic, not rigorous calculation, and I'm asking that you make this rigorous. So, show that what I was trying to argue is that the way I derived this heuristically is I wanted this to be. is I wanted this to be like when I sample X according to this measure, it's sampled according to the conditional law of X bar given the observation, given the Y. And so I'm asking you to justify this, which we can encode like this. So for any reasonable function, let's say measurable and and let's say bounded, we have that this identity uh is valid. This identity is valid. Okay, this bracketing indeed encodes the conditional law of x bar a given y. All right, so before I move further, I want to justify why I say that these terms are here to help us in some sense. Or why is it that this model, which I said looks like the spin glass model, is actually. Said, looks like the spin glass model is actually simpler than the spin glass model. And so, here is the important property. Sometimes people call it Nishimori property. Okay, so the first thing I will say is actually a weaker version of the Nishimori property, but then I will build upon this. So, the first thing that you can So, the first thing that you can observe from this identity above is that, so remember, this bracket measure is still a random measure. So, if I want, I can still average it over x bar and w. And what happens if I take this double average is that, well, you know, it's like taking the average on the right-hand side. And so I can, you know, it's a conditional expectation. It's a condition expectation, and then I average over that thing. So, in fact, what I get is the same as the expectation of f of x-bar. And now, I think the first time, at least the first time I saw this, I felt a bit confused. I think I was having a hard time even distinguishing what's this variable x, you know, why is it different from this variable x-bar? And what's perhaps a useful And what's perhaps a useful guide for the intuition is if you imagine the observation is empty, t is equal to zero. So when you look at y, you learn nothing from x bar, then this variable x is a resample of x bar. If you want, it's an independent copy of x bar. On the other hand, if y was revealing perfect information for x bar on x bar for you, then On x bar for you, then x would be equal to x bar. Okay, so the coupling between x and x bar is a subtle thing that's kind of interpolates between these two trivial cases. So they are not the same, but still this identity is valid. And beyond that, so so what So, what shows up in spin glasses and also in this model is that oftentimes when we do calculations, we have to make appear a new variable which is an independent copy of this x variable. So, this is an independent copy of x under this bracket measure. So, it's also sampled according to this conditional law of x bar given y. Given y and the claim I'm making is that, okay, so maybe I'll do the calculation and then we'll see what it gives. So in physics terminology, they call this independent variable a replica. So if you want, x prime is a replica of x. So anyway, so if I look at this product situation, then Well, they are independent conditionally on this observation, right? So I can alternatively rewrite this expression like this. And now I can use the, let's say on the second one, I can rewrite this as that it's the conditional expectation, you know, it's the law of x-bar given y. So if I want, I can rewrite it like this. So, if I want, I can rewrite it like this. And now, remember that this law is random, so I cannot do whatever with this law, but it's also a conditional law. No, sorry. It's measurable with respect to y. This randomness only depends on the observation, only depends on y. So, if I want, I can really, if you want, insert this thing inside the conditional expectation, and then in the end, And then, in the end, remove this conditional with respect to y. So, what I have argued is that this is the same as the expectation of f of x times g of x bar. And, you know, like I think it takes a bit of time to digest that this is not a trivial identity, but it is not. I try to argue that when you do this sample. When you do this sampling, it's not the same as saying that x is equal to x bar. Sometimes, depending on what y is doing, it may be x is equal to x bar, or it may be x is a resampling of x bar. So this relation is non-trivial. And it's only true because I also average outside. It's not true only inside this single bracket. But this is really the engine, like the this is the reason why this problem will be simpler in the end. Reason why this problem will be simpler in the end of the day. When we try to identify the partial differential equation, like we've done in three bytes, when we have these disordered models, there are these replicas that show up. And in some sense, you feel your like each time you're taking a derivative, there's a new replica that shows up, and you don't know how to control it because as you take more and more derivatives, there are more and more replicas that show up. But in this problem, it does not happen because you can kind of You can kind of catch it back. When a new replica appears, using this relation, you can just link it back to this original signal, this original X-bar that is in the problem. So, okay, that's a kind of fuzzy talk, but it's to give you an idea. And, you know, once you have, so this here I put for, you know, like something of product form, f of x, g of x prime. But once you have this for this product form, For this product form, you can extend it to more general functions. So, Partanil is asking: Is this the Nishimoi property? And the answer is yes, or maybe it's the version where you take a generic function. So, it would be that when you have these two copies, you know, the function of these two copies, then it is the same as the function of x and x bar. And in fact, you can still play around and analyze it some more. Around and generalize it some more. So, if you have three independent copies, you can replace one of them by X bar if you want. So, only one of them. So, you can only do the argument I showed once. But in general, you can take many of these replicas, and if you want, you can replace one of them by this X bar. So, that will be a crucial tool for us to make our life easy. Life is easy. All right. So is it a good time for a break or is it too early? Maybe it's a good time for a break. Or are there questions so far? It's 12.30, so maybe it's a good time for a three-minute break. All right, so let's take a short break.  Okay, so I have uh several questions in the chat, so I will answer them. Um, so Rishab is asking what if X prime is a dependent correlated copy of X. So I think then it's a bit uh unclear, but um the rationale for why I want to study this will come uh shortly. This will come shortly. Like when you compute derivatives with these disordered systems, there are these correlations that appear, and you can represent them using these independent replicas. So it's really natural to focus on these independent copies setting because that's what shows up when you compute derivatives. And Lya probability is asking in the definition of y, what motivates the parameter t. Of y, what motivates the paramet? So you would like to have a parameter that allows you to say something about the strength of the signal. So ultimately, what we would like to see is that if the signal-to-noise ratio is too small, then we will not be able to recover good information on the signal. So, for instance, if we study this minimal mean square hour, it will be maximal. It will be maximal. We will not be able to do very much on this error. And then after this t is increased sufficiently, something happens and suddenly the minimum mean square error is starting to decrease. So I want to see this transition as T is varying. So that's the main motivation for the introduction of this T. For the introduction of this t. So it's to monitor this change of behavior as t crosses a certain value. Unfortunately, we don't have time to do all the steps towards this because we will just identify the PDE and then you can recover the minimal mean square by looking at the derivative of the limits. And I will not have time to explain this, but you can really recover this transition. I have plenty of questions. That's really cool. I appreciate it. Of questions, that's really cool. I appreciate it. So, x-bar is that thing that we want to recover. Okay, so we are given x-bar. I mean, rather, it's kind of in nature somewhere, and we're trying to understand what it is. And what we observe is this x-bar, x-bar transpose, plus noise, and we're trying to recover x-bar. And instead of x-bar, we have our best guess for x-bar, which is this conditional law of x-bar given one. Conditional law of x-bar given y, which you know in formulas is convenient to represent using this variable x, which in some sense represents our best guess for x-bar, which is different from x-bar, but maybe relative. Yes, so Sudip says that he's confused with all these x and x prime and x bars. And I agree it's confusing. So really, perhaps what I just said is a helpful. Perhaps what I just said is a helpful thing to keep in mind: x bar is that thing we want to recover. And in some sense, x is representing our best guess for x bar as a random variable. It's something that has the law of x bar given what we've seen. So it's not x bar, but if our observations were really good, it would be really x bar. If our observations are not very good, then it's kind of with some randomness inside. And x prime is. The same side. And X prime is some independent copy of that same thing. Yes, so Fardab finds my notation kind of confusing with the bracket and the f of x. And it's true, it's sort of confusing, but it's so the way I use notation x is not really the same when it's here and when it's there. So here is just. Here and when it's there. So, so here is just the integration variable. And so, it's kind of a normal way to understand this. And here, I kind of understand that in this expression, I understand that x is, if you want, the canonical random variable over the probability space. So, that's, yeah, Fardal was suggesting that instead of writing x, I'll write f of dot, which, you know, I think would make a lot of sense in some relationships. In some rigorous point of view, but I find it a bit less confusing to write it this way, although it's a bit unclear. You know, it just shifts the confusion around in some sense. So Adam is asking, why don't we study this theta thing? So, what I was saying is that what you want really is to look for the best possible sorry, for the best possible sorry for the for the best possible theta hat that minimizes this quantity and so and it turns out that the best possible thing is actually this conditional expectation so if you want once you have your observation your best possible guess for for x bar x bar transpose is actually the conditional expectation of x bar x bar transpose given one it's kind of a theoretical and abstract way to state it but that's your best guess so um and And that's a property of the La conditional expectation that it's projecting in this L2 space. So I'm a bit confused about the question of Zachary. So first, yeah, I will so the next question. Indeed, the the noise W is independent of X bar. I have not sa said it, but yes, I assume they are independent. Yes, I assume they are independent. And maybe for Zachary, I will let me try to wrap up what I've done in a slightly different wording and see if it helps. So let's forget for a moment about this statistical inference setting. So we have some random variables that are given to us. There are x-bar and w. Out of these random variables, Out of these random variables, I build this function, which is called h n of, okay, h n 0 of tn x. So it's a function of x, and somewhere in this thing, there's w and x bar. Okay. And once I have done that, I built the Geeks measure out of this. Okay, so it's a Geith measure built on this function, but it's still a Geys measure that depends on the realization of x-bar and W. Of x-bar and w, and I want to study this function. Okay? Does that make sense, Daiquary and others? So really, like if you want outside of the problem, we have a source of randomness, which is w and x-bar. And then we build a Geith measure out of, for each realization of this randomness, we build a Geez measure, which is denoted here. And I want to understand it. And I want to understand it. Does that feel reasonable? Yeah, awesome. I I also felt confused when I started to think about this. So yeah, don't feel sorry. All right. So I hope now we are back. So we want to study this Geast measure. We want to understand it. And like for the Kerryvice model, really the thing I want to focus on. Really, the thing I want to focus on first is on this normalization constant, if you want, on this, maybe we could call it partition function. Because we understand that maybe later we take derivatives or something. So it's like this moment generating function. It's a good thing to study that quantity first. And so I want to study this thing, and I want to apply the same strategy as before. And so I could define, you know. And so I could define, you know, take the log, and then that would define for me a function of t. And then I would be looking for a PDE for this thing, right? For a partial differential equation. But the problem is that I only have t in this function, and it's a bit like if I was describing to you the Curryweights model and I had not thought about adding this age parameter. If I had only introduced this. If I had only introduced this t parameter in the Kweiss model, I would have been stuck. I would have been like, I don't know differentiating in T many, many times and never finding a closed equation. And here I'm a bit in the same situation. I have this parameter t, but what do I relate it to? In some sense, what we have done in Curryvice is find the connection between this T derivative and this H derivative. So here we have to play the same game. So, here we have to play the same game. We have to find something to add to this energy, like this H term in the Curievise model, that will allow us to... So that first will be simple, like in the Curievise model, this H parameter was simple, but also will be sufficient that we can relate the derivatives. And so when you look at the energy function, There is this term which is quadratic in x, like in the QEVE model, you know, this first term which has this sigma i, sigma j, is quadratic in the sigma. And so, what I would like is to is to add a linear term in x in my model, like this sum of sigma i that we've done for the Cohemist model. But I want to do this with a constraint, because I told you it's an important property, this Nishimoi property, and so I want to. property. And so I want to make sure I do not destroy it. So I don't want to look at the Hamiltonian, at this energy function here, and just write, okay, plus blah, blah, blah. I don't want to brutally add stuff inside this energy. Because if I do this, I will break the fact that this Geese measure is a conditional expectation. And then this will destroy this nice proof of the Nishimeri property I had. Nice proof of the Nichimai property I had. So still, I want to add this linear term, but I want to preserve this kind of conditional expectation structure. Okay, so maybe I write in right. So this is an important step. We need to enrich the model. And this is a subtle step. In particular, like In particular, like for currybis, it was kind of easy. You barely notice that I had added the correct term. But in particular, for spin glasses, it's really subtle and not very easy. Yeah, it's not very intuitive or easy to figure out in general how to make this addition. But okay, we need to enrich it somehow. And okay, I don't explain more, but you see that we want some of the properties we had for Kerryvise: that it's sufficiently rich that we will be able to relate the derivatives, but also sufficiently simple that we will be able to compute what happens when only this extra bit is present. So, we want to do some linear things. We don't want to break this conditional expectation structure. So, the way I will introduce it. So, the way I will introduce it is: I will pretend that we also observe some linear observation on X-bar. So, this is a two square root of two H X bar plus Z. This is a Z where Z is a Where Z is a this is a standard Gaussian vector and again the independent of these other variables in the problem so now we have X bar that thing we want to understand and we have W and Z which are noise terms and everybody Are noise terms, and everybody is independent from the others. So by doing this thing, by saying we also observe this, I will have a more, and now I recompute my conditional law of x-bar given this more, you know, this richer observation, the observation of this guy y we had already, and also this vector. Okay, so now we observe these two things. So now we observe these two things. And what will happen is that it will define for us a new energy function, if you want, a new HF, which now depends on T, H, and X. And well, the first part is as before, this H and zero. And now we have an extra term, which is An extra term which is which is linear index, and you see it's against this noise vector. And then these compensating terms, which I claim are just out there for our convenience. So let's not worry too much about this. Okay, so they are kind of similar to the other ones, except there was a power four before, and now it's a power two or something. It's a power two or something. So these are just here for our convenience, and this is the important part. So if you think in terms of the Curievise model, if you want, it's like these are our sigma variables, and we have some sort of random magnetic field with this Gaussian C. Okay. So again, I try to summarize what we've done. There are the sources of randomness, X-bar. Randomness, x-bar, w, and z. Out of these three variables, we build this function, which is here. It's a function of x. And now, out of this function, we still build the Gields measure. I'm not going to rewrite the definition of the Gields measure, but it's as before. I just say exponential of Hn of TH, da da da. Okay. Okay, and now I can define my n-rich free energy, which depends not only on t but also on h. So fn of t and h is one over n. So I take expectation log integer. Okay, so you see it's this normalizing constant, if you want, or moment generating function. And then I divide by 1 over n as before. But there's one extra thing I do here, which I take the expectation, because as we've discussed, this quantity inside here, this Hn, it still depends on this X bar, W and Z. And so then I take the. So then I take the yes, okay, sorry. So maybe I want to define the two quantities. Maybe I want to define one version which is still random and one version which is after we take the expectation. So fn without the bar is no expectation and then fn with the bar on top is the expectation of the game. Okay, so so it's you know one extra complication compared with query biceps but Compared with scrayboise, but to some extent it's analogous, right? We have these two parameters, T and H, and we have our partition function, or maybe free energy, maybe I should say, that depends on T and H. And I'm trying to understand the limit of this guy as n becomes large. Okay, and now I feel I'm in a better position to manage because I have these two parameters like into a by. These two parameters lacking to a base model. I have T and I have H, which in some sense is simpler than the original thing. So here is the main result I want to present today. It's that this function f n bar, yeah, okay. This function f n bar converges to some function f, which, well, you Which, well, you guessed it, is a solution of a certain partial differential equation. And so you've seen a lot of new things today already, but at least when I write the equation, that will sound familiar because it is the same as for the Curieweights model. So, there is a difference actually, which is that in the way we set it up, now h has to be non-negative. So, really, this equation is only posed with h non-negative, while before it was with h over r. And yes, yes, Partner is asking if the convergence is in the sense of local uniform convergence, and the answer is yes. Convergence and the answer is yes. So, in fact, all the estimates we've seen for the QEVIC model are also true here. So, the functions are uniformly leap sheets. And another thing which is also as in the QA base model is that the initial condition, in fact, this thing does not depend if you set t equal to not depend you know if you set t equal to zero it does not depend on n so you can just write this you know like in the curveise model okay is the statement clear i'm going to spend the rest of the time which is perhaps not very long actually but okay explaining the the main steps towards proving this result Towards proving this result. So I'm checking if there are questions. Good. So there is really one fundamental ingredient in this proof, which to some extent is also very To some extent, is also very similar to what we've done for the Kerryvise model. So maybe I split this main ingredient into two bits. So let's say the first part is that when we compute this quantity, it's also the variance of something. Now, like for Curie Basedures, the variance of the magnetism. For Curiebas to as the variance of the magnetization. And here it's a variance of something a bit different, but okay. But it's also a variance. So there is this renowned variable x, and I try to explain that. And I try to explain that it's really not equal to x-bar, right? If the observation is not very good in particular, it will, you know, you can think of it as almost an, like, if you have basically no observation, then x would be a resampling of x-bar. And if you have complete information, then x would be equal to x-bar. So, anyway, so what is on the right-hand side is the variance of this scalar product between x and x-bar. So, you know, again, when you notice this, this, and this is an identity, you know, it's really an equal sign. And, you know, if you notice this, as in the K-Vice model, it gives you hope that as n becomes large, there will be a stochastic concentration somehow. And so hopefully, in the limits, the equation will be satisfied with right-hand side equal to zero. Okay, and that's what the main result says. Okay, and then for the KBIS model, we had done this thing, you know, first notice that it's a variance and then find a way to express this variance in terms of other things we hopefully kind of can wrap up or understand. So, for the Curievise model, we had expressed the variance in terms of this second derivative. And here, it's not an equality, but something. Here, it's not an equality, but something very similar is also true. So x dot x bar minus its expectation. So this variance thing, I claim that it's bounded by 1 over n dh2 fn bar. So that's, you know, if I have written an equal sign here, that would be really exactly like in the QAwec model, except Like in the query vice model, except the variable here is not the same, but other than that, it would be really exactly the same. But what I wrote is not yet completely true. There's another term. And if you want, this other term comes from the fact that the measure we care about, this bracket thing. This bracket thing is still random, so you have to kind of try to control this in some way. And so, this measures the difference between Fn and Fn bar. Okay, so in the Kevice model, this term could not make sense, right? We only have one Fn. It's already deterministic. So if you want, it was there, but it was zero in the Kevice case. And so, okay, so I have to. So I have to. My goal now is to maybe justify a little bit more why this statement is true and then explain how to close the argument. Once we have this, how do we conclude for the main result? So let me first focus on this first part. On this first part, how do we see that these derivatives sum up to this variance term? So we can call this a first lemma. So here are some, oops, yes. Here are some observations. When I differentiate with respect to H, so you remember in the So, you remember in the QEVISE case, what we found was this mean magnetization. So, in this case, I claim that it's x dot x bar divided by n. And when I take the t derivative, well then you guessed it, it's going to be the square of this thing. Okay, so if I justify the lemma, then it's clear that when I look at this t-derivative minus This T derivative minus the H derivative squared, I get the variance. Okay, and I'm not going to give a full proof of this, but I just want to explain the key ingredients which we can fancy to call ito calculus without ito. Okay, that's that's a very fancy name for Okay, that's a very fancy name for what it is. But so yeah, so I want to spend a bit of time explaining one ingredient for deriving the sign entity. I'm reaching the bottom of the pane, so I will turn here. Yes, we just say so calculus. So indeed, it will just be calculus. But I want to display how to do that calculus thing. Calculus thing. So let me start by observing this. So if you know your stochastic calculus, you know that when I take the exponential of a Boehline motion, and I will write my Boehline motion in a funny way, and then I subtract t, then I get something of constant expectation. Right? So here, if I had written the Boeing motion at time 2t, I would have looked very familiar. To t, I would have looked very familiar. Or at least I hope it is. So, anyway, this is true. And so, if I differentiate with respect to t, that should give me zero. But let's see what happens if I differentiate with respect to this expression with respect to t. So, I, you know, first I differentiate this term, so I get one over square root of two t x, and then I differentiate this guy, which gives me. I differentiate this guy, which gives me a minus one, and then I have exponential of blah blah blah, and I get equals zero. Okay, so I get this by just computing the t derivative. Okay, but it does not look obvious at all. At least, you know, in phase value, when you look at it, you're like, whoa, that's how you know, we think, well, we're lucky we know eto calculus because how do we see that? Calculus, because how do we see that? So, the point I'm going to display is: how do we see that this is correct if we don't know about ito calculus? Oh, yeah, so I haven't said, sorry, in this expression, I mean that x is a standard Gaussian. Sorry, I should have, that must have been very confusing otherwise. So, so this I think of it as a Bohr motion because it's just, you know, if x is a standard Gaussian, I multiply by square of 2t, it's a Bohr motion at time 2t. A Boeing motion at time 2t. Okay, and then I differentiate and I get that. Sorry about the confusion. Is it okay? I hope I coded it back without too much damage. So if x is a Gaussian, I hope that you're convinced that the expressions I wrote down are true. So how do we see it's true? Well, it's really something about Gaussians. So I can write it explicitly. If I write x times x times this exponential of square root of 2tx minus t times the Gaussian measure. So exponential minus x squared over 2tx. And let me forget about the normalizing constant, which will be everywhere, so it doesn't matter. The important part is that, you know, this guys want to come together and be integrated up. So again, you know, the only thing So, again, you know, the only thing I know how to do is integrating by parts. I'm going to integrate by parts. And what this amounts to is to differentiate this part. Okay, so this is the same as the integral of the derivative of what was there. So you see, I can remove this x if I differentiate the other function. That's the. And when I differentiate in x, this guy gets square root of 2t exponential of blah blah blah. Exponential of blah blah blah times the Gaussian measure. Okay, and so if we go back to this, we see that the identity here is valid, right? Because we can, you know, this x times x blah blah blah, we can do this Gaussian integration by parts, and there is this quote of 2t that shows up, and then we have 1 minus 1 and it's 0, and we're happy. We've done our Ito calculus without Ito. Will have it. Okay? So just calculus as we deep product it. So as another exercise, again, I hope I'm not asking for too much, but I think this is also doable. So complete the proof of this lemma using the ingredients I gave you. Okay, so using Gaussian integration by parts and this Nishimori thing. This Nishimori thing. And also, second part is relate this MMSC with this time derivative of Fm bar. Okay, find the relation between these two things so that we are justified in believing that if we understand this T derivative, then we are. Then we are good. Then we understand the minimum mean square. Okay. So that was for explaining the first part of the proof. Like, why is it that this, okay, it's on the other page, so maybe I will not go back. But I gave you expressions for these derivatives and essentially. And essentially, I did not give you the full proof, but I gave you really all the ingredients to allow you to prove it. And then, yeah, maybe I'll display it. I mean, it's not so far. Let me go back to maybe that was a bit longer time than I thought. I still have to practice a bit my skills. Yeah, then I want to explain a little bit why this identity. Why this identity is valid. Okay, so how do we bound? I will only be very brief. Like in one minute, I'll give you just a very short idea of how we can bound this by these two terms. And finally, how we show that this is small. This one is like with the Curievax model, so we know how to deal with it in our argument. And the remaining question is: how do we show that this one contributes little? Contributes little. And you know, I know that the time is basically over, but don't worry, I won't be long. I'll just give the fundamental of the idea and it will be very short. All right, so the for this second part. Part. So I'm going to use x now as some generic name for a random variable, not necessarily Gaussian. Okay, so x is a random variable. I just want to do this variance decomposition. Yeah, maybe I should not write x, but I'm running short of letters. How could I call it? I call everything x. A. Okay. So when I when I compute the the variance of A, I have these two layers of expectation. And what I want to just observe is that I can split it into doing one expectation at a time. So first I could look at the variance conditionally on this extra. On this extra outside disorder. So you see here now there's no E. I just look at this partial average. I compare A with the partial average of A. And that's not all. There's another term which will compare this partial average of A with its full average. Okay, and I mean, maybe you're not. I mean, maybe you're not convinced that there's an equality, but if you're worried about this, you can write an inequality with some, maybe with some tools in front. So clearly, the inequality is valid. Okay, I claim the identity is also valid, but it's not crucial anyway. And yeah, what I want to, you know, this second part of the proposition is trying to control a variance like this, and the estimate is a sum of two terms. And this one is And this one is really looking like Curievice. Even if you forget about the E, in some sense, it's looking like the Curievice thing. So this one is the one that produces this second derivative term. So like for Curievice. And this one is new, but this one is the one that produces this DHFN. So this one will be kind of related to that term minus DH of Fn. Minus dh of fn bar. Okay, so I mean there's some meddling around, but it's you know, it's very faithful to the spirit of how you derive this inequality. You split into these two terms, and then you analyze these two terms by trying to compare these two things, and then these two things. Okay, and now the final ingredient I have to explain is how do we show explain is how do we show that this extra term with this comparison between fn and f n bar is small and so that's a new ingredient that was not a part of the current model but in some sense it's not a difficult ingredient it's a very classical piece which is just a concentration estimate So, for instance, you can use the Eiffelstein inequality or the Gaussian-Poincarine inequality, and that will tell you that Fn minus Fn bar, this is kind of small, so okay, maybe I write it like this. And so now. And so now I stop writing so that I promised I will stop. But in the estimate we wanted, it was about the derivatives of fn and fn bar. And here I'm saying only that the functions are closed, not the derivatives are closed. So, you know, how do we deal with that? And if you remember, the key message of the conclusion of last time was that we need to control the error in this. The error in this L1 sense in this H variable, you know, after we average over H. And because we are allowed to do this averaging as well, in fact, we can do, you guess what, an integration by parts and still use information about this fn minus fn bar to control this derivative term. All right, so I'm already uh a bit uh past time, and uh thank you very much for uh your attention and uh Your attention. And yeah, it was a pleasure to do this thing. Thank you so much, Jean-Christophe, for a wonderful sequence of videos. Let's unmute everyone and give Jean-Christophe a big round of applause as soon as I remember how to unmute everyone. I know it's around here somewhere. Any other co-hosts know how to do that? Where has it gone? Okay, there we go. Thank you very much. So maybe we can I'm going to stop the video now so that we can have further questions. And if whoever unmuted the video can remute that. Unmuted everyone can remute them. That would be awesome to be helpful. Everyone can remute them. That would be also helpful.