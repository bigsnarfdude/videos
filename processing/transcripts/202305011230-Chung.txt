Partial differential equation. And from the partial differential equation, you need to obtain this kind of three structure. It's what we call the zero-dimensional homology structure. It's a zero-dimensional because it has a non-cycle, it's actually tree. And this yellowish color is basically the the property distribution we obtain by doing a heat connect convolution on a manifold structure here. Convolution on the manifold structure here. Now we're going to use this. And somehow, I just want to show you that this is what most of the time I do. This kind of language. So this segmentation and extraction took us one year to obtain this graph data representative of the surfaced pattern. Now it took additional one year to To do the imagerization of the three to the three, actually, to one years, actually. We're going to use a wassest than distance actually. Anyway, that was the it's our MATLAB tool actually. So let's go back to our presentation and let me show the actual oh now we have a lot of people who are acting. Epinos actually. The talk mainly comes from this local data, epilepsy data from Wisconsin. And I have a bunch of projects with all of the place. The people who are here is Anasi is here. Horlando, I don't know where is Holando. And this Pog is supposed to be based. Supposed to be based on this paper, I'm actually writing with Homando, and Soyma was a used to be a student at the Causes. He came to medicine to the costa. Fortunately, he's on the review. We haven't revised it for more than a year. I don't know whether we're going to ever revise it. But most of very often, we don't simply don't revise it, actually. We just make archive paper and that was it actually, very often. Actually, very often. So, I have no idea whether we're going to devise it or not. So, motivation for this talk actually came from this study I did with Hernando. It's an epilepsy study. The data Hernando gave us actually. And you and Pomo Stun actually analyzed this. And it's a single subject data. And with that data, we actually did some modeling. Actually, we did some modeling, and the main conclusion in this paper was that we actually didn't detect any signal, and that was a signal. No signal is a signal. That was the message of this paper. And why? Our paper claimed that there is some sort of carrier signal, topology signal, that's invariant across before seizure, after seizure. Especially particularly in this, it's a temporal lobe after left is somewhere here, especially in the channel responsible for the temporal lobe epilepsy. So that was the main message. From there, we decided to extend it to the temporal lobe epsilon data set, much bigger data set. So we have a local collaborator who are PI of this. This is a part of the. This is a part of the HSP big project. It's epilepsy HSP. So, this is a paper on the review. Usually, sadly, he didn't want to review our TDA paper. So, each time we submit a TDA paper, what happened was that the outsource editor simply decided to review it. So, we managed it too mathematical. That was explaining. That was the explanation. So, we actually didn't submit, we didn't actually publish a major TDA paper in Neuroimager. We actually submitted either Anna or the TMI actually. I mean, Neuroimage is just one journal. They don't want to publish our paper. We can go to other places. Share this here? So the data set we actually analyzed came from these two different sites, one from Two different sites, one from Wisconsin, the other from the Medical College of Wisconsin. They use the same G scanner, but if you're using traditional methodology, you should detect the site differences. There's always site differences, even using the same scanner, there will be site differences. So it's the spirit of the subject. It's temporal lobe epilepsy, 39 in Wisconsin, 62 at the Medical College Wisconsin. To have the medical college concentrate noise, noise. What's noise to you might be a signal to me? What's a signal to me might be noise to you? So I'm going to treat this as a noise. Everything is noise to us. I'm not interested in the side effect. I'm not interested in sex effect. I'm only interested in the main difference between health control and temporal abstraction. And temporal abstract. Now, our method will show p-value, difference between the site differences at this level. And we will show that the sex differences, if you're using our method, there's no sex differences, no side differences. So because of this, we don't need to cooperate. You don't need to cooperate sex inside. Inside EPUS topology. That's the main message I'm going to give it to you today. So, this is just the outcome. This is just a heavy control here. And this is the over 100 temporal web lapses. Average connectivity is slightly different. So, how much is the deeper topological? How much is data per topological while ignoring the side? And I'm not actually ignoring. We it's simply that the method will not actually detect any side and sex differences actually. So we're going to use a topology analysis as a main tool. And you may not heard about this. This actually, I got into topology, not to topology data analysis, but from my teacher. But from my teacher, he was Lee. So when I was a student, I was just, he asked me, I always recall to read all these papers and work out this random build-based multiple comparison collection. And in the multiple comparison collection, it's been random built. That's an interesting expression. It's called the expectation of the Euler characteristic of the exclusion cell. Of the exclusion cell, that's actually topology. So, my advisor was working on topology. It's actually a topological theory, it's not a geometric theory. This is a topological theory. So, that's how I got exposed to topology. And you can express this as the betting numbers. And the local editor actually wrote about two papers on the topology data analysis connection to the random build DD, but he's retired. I don't think he's doing anything much here, actually. Here actually, and this TDA kind of came out after Kit Wesley passed away. If he's still alive, I bet that the TDA is something he most likely working on. I really believe it, because there's not much difference between what he has been doing and what has been doing. So our group actually wrote a first paper on top of the data line. Paper on topology data analysis in the not only in the brain vision, but actually the whole medical engineering. And since then, we have published about 50 papers, five, zero TD papers. Every few papers, I don't know. So this paper has been very influential to the development of TD theory over the years. I don't do theory, but some of these TD theory people actually like our research. Theory people actually like our research, so they keep track of our research and they try to solve the mathematical problem motivated by our research. Actually, that's what a lot of people are doing. So these days, I mainly interact with the mathematicians actually, and less so with the statistical. So we actually are organizing some thesis, third year in series. In Syria. So, the first, so we've been doing this every year. I think we will also be doing this next year. I think most likely in South Korea as well. And so I just want to show that there's a lot of activity involving TDA. So if you want to get involved in the TDA, let me know. I can be a great tour guy to the TDA. It's very easy. It's very easy. Even high school students can do it. It's not hard, actually. It's not hard. So, to make it life easy, actually, I start working out making a toolbox actually starting this year. And so, I cannot put the chat GPT as a quota. But I'm writing an actual manual, and the actual coding is set with the ChatGPT. So I found the ChatGPT to be far more useful than many of the students. I'm sure. So that's why I'm using it. I'm sure. So if I hire postdocs, I'm going to give the same question to the postdoc and ChatGPT. So if ChatGPT cannot and the postdoc cannot answer, check questions ChatGPT can answer, I'm not going to hire. I'm not going to hire him. So, the student professionals here, if you are performing far less than ChatGPT, you will be easily replaced. I guarantee you will be very soon. So, are you going to be replaced by AI or not? So, you have to work hard, I guess. So, the difference between the difference between the the the actually topo is not showing but i want to show the difference between the geometric distance and topology difference there's a serious differences here there's a little arrow in the wow i'm very slow Well, I'm very slow. You guys, what you're saying is doesn't make any sense to me. It's okay, it's enough. Yeah, yeah. I think you need to be replaced by Chat GPT. Yeah, yeah, you're right. I should be replaced with a chat GPT. So the geometric distance is like this. So you have a distance between the A and B. You may think of A and B, you may think of this as one. Topological distance between A and B, one circle to the next circle is zero. Because circle is circle, there's no topological differences. So it's zero. And for example, I cut a part of the circle and the distance between C and B would be one at the same time. There are purposes differences. So there will be differences with here. However, if Here. However, if I change the sample to the scale, there will be still no topology differences. So the topology difference would be zero, exactly the same. But geometric distance will be, if you use like the what it calls, some sort of the shortest distance, it will be two. So there will be some differences. So let me show you some concept on the Concept on the filtration. So suppose you have a weighty graph like this. I'm going to delete one edge at a time. Delete one at a time. Each time you delete it, the topology will change. So now in this case, there are two cycles, one cycle here, two cycles. This bigger cycle is a linear combination of this smaller cycle. So cycle. So, cycle actually forms a basis function. It's a Hilbert space. You have to understand cycle from a Hilbert space. So, this is one base, another base. This bigger cycle is a linear combination. Now, and that's what we call the one-dimensional homology, high-dimensional one-dimensional homology or topology. And the zero-dimensional topology is very simple. It's a connected component. Connected component is basically the counting the number of components that's connected. So, in the connected so in the the grep setting it's just uh no this is a one connected component one connected component here this connected component is one here so it's two here three here so each node can be as a basis function for building a connected compound right so that's how you do now then i'm going to each time i delete the Then I'm going to each time I delete the edges, given an edge width, I'm going to delete the edges and construct a binary graph like this. Then, this binary graph will have this kind of the nesting structure. And this concept is called graph retration. This has the same information as the original weighted graph. The same thing. We are not losing any information. And that's the starting point of our. And that's the starting point of our version of the topology transition, which differs very little from the other TDA people. So we can have this theorem. It's called, we call it the bottom-data decomposition theorem. Now, the reason is that if each time you do the graph it deleting one edge at a time, you can actually split the topology into two parts and take the form of decomposition. So in this case, Okay, so in this case, if you trace the number of connected components, which will increase each time you delete the edges, the number of connected components will increase, while the number of cycles will decrease. So they form a monotone function exactly like this way. And this monotony is the driving force of the robustness of our methodology compared to other TDA methodology. Then, if you combine all the edges, then actually the edges that actually create a connection each time you delete the edges it will create a component basically so if you combine all the edges that's responsible for the creation of the component we call it both set that's basically form of this red edges and that's maximum spinning theory okay so anything that's not maximum spinning theory will actually Actually, edges that will destroy a cycle each time you delete the edges. For example, if you delete the edges here, the cycle is disappeared, so it will be forming a death cell. And the such decomposition is unique, and the it's unique actually. So you can represent this both and that of the connected component or cycle. The connected component or cycle in terms of this kind of two-dimensional scalar point, you mention this in the usual Poisson diagram representation this way, something like this. This is traditional Poisson diagram. But if you are using graphilitation, you can simplify this to the one-dimensional structure. That's the simplicity is. Simplicity is the main driving force of our methodology. So you can actually represent the zero-dimensional homology or topology and one-dimensional topology. It's a number of cycles of the filtration values or changing position values. And there are not much signals here actually. So main signal would be a connected component in this particular case. Component in this particular case, but in the most of the recent fMRI, most of the signal differences are actually located in here, cycle differences, but not for this particular data set. I found that quite strange actually. So this basically, so I want to characterize the geosymission topology. Now, if you, so I span express I basically plotted the maximum spani, I found that actually the found that actually the Zirouchiman topology is actually characterized by the left, right, hemisphere connectivity. That was a surprising finding actually. So I thought this makes sense actually. That's what most of the connectivity is actually about, left-right hemisphere connectivity. Then using that body and that composition, it's basically autonomous composition of the topology actually. That's the only Topology actually. That's the only autonomic decomposition. And you can do the embedding. So if you want to do this embedding with traditional method, you may need some sort of something like Argent Mab or the multi-dimensional scaling. We don't need it. So that composite directly give you this embedding structure. So no optimization needed whatsoever. So it's very scalable. So it's very scalable and it exactly tells you. So green is basically the BT head control. Yellow is the about 100 temporal lobe epsilon patient. And this is a zero-dimensional topology characterized by the bot of the connected component, both of connective components. And this is the one-dimensional topology basically characterized by the data. Basically, checked out by the depth of the cycle. And it's, yeah, this is under DB actually. And it's basically what you're seeing is a centroid. It's a closer centroid, but the closed centroid is computed to the topology. So what you're seeing is a topological mean, topological mean here. So from here, you can see that the group separation is given by the zero-dimensional topology rather than the one-dimensional topology. One-dimensional topology. So it's immediately show you the topological pattern. And then we're going to use Washington distance to quantify this visualization. And so, but what's the slantist? It's basically optimal transport. I learned that someone got a Nobel Prize on the Nobel Prize on the Washington distance. Also, one guy actually got a Bilge medal in the optimal transport. So it seems to be quite important design. I got into this Washer Style distance because it's the distance people use in the topology. I thought this is also something very familiar to my training as a student. Most of the Most of the stats static stream might send this kind of the expression somewhere, so it's simply very easy to step people. So that child got into this area. So if I want to, I'm going to use this distance to match this kind of object. Now, this topologically different object, you cannot match, yeah, okay, you cannot match this with the existing depth vegetation. DPMOP vegetation. It's an impossible problem because topologically, are you going to match this part? For example, it's not here. So you have to use a topology to match it. So the way we do is that we're going to assign the Tirak delta function at each node here and form an MPA distribution. Then I'm going to try to match them. But that will give you a lot of noise. Will give you a lot of noise. So in imaging, when you start, you have to smooth images, right? That's the group done in imaging science. So we actually did a smoothing. In this case, we did the smoothing, he corner smoothing. So it's basically most generalization of the Gaussian kernel on the manipul setting. You can actually prove, actually, it just do a master should do actually. Actually, he actually proved the imbalance of the waste. Actually, proved the imbalance of the Wasserstein distance on the Gaussian smoothing. It's equivalent to proving the imbalance of Wasserstein distance on the Gaussian mixture. It's very hard. The provision is a full page long. There's page limits, so it's just put it like a three-sentence group. So anyway, so we actually use it to match this. Our algorithm is a linear algorithm actually. O algorithm is a linear algorithm actually. It's really fast. So if you go there, we dissipate the code actually. So you can play with it. Then using this, we actually match it. I'm going to skip it. Match it. And we do, since it's one-dimensional matching, you can match, you can formulate this other statistic, actually. And then the yeah, then our inference is that the Is that the distance-based, why? You guys are using feature-based inference. If you're using two-sample t-test, that's feature-based. Why? Because you have feature, you consult the test statistic and do inference. I don't do that. We don't do that. That's very time consuming. So what we want to do is you have to use Arnova type of the distance-based inference procedure. Then you can do, if you do that, You can do the permutation test. So, what you're seeing is a distance map between the two groups. How are you going to do a permutation test? You basically do permuting one entry at a time. And I'm going to skip it. I'm running out of time. So, well, you can just play with code and so we can do 100,000, 100 million. 100 million permutations in 100 seconds in laptop. This is a laptop performance. Every computing machine, every person today, I don't need a laptop actually. Our group is a scalable computation group. So if you have to use a desktop, we spend like a year optimizing, develop algorithm. We'll do it in a laptop. So we can actually even do the localized inference. We can actually even do the localized inference. And we did some, we did a lot of validation. This is a very interesting validation. So, this one, I'm going to just, so this is the this is the apology invariant signal. So, you, if you do a crossing, you should not detect the signal. But chemist crossing will detect 98, high-defect crossing 100, apology 63. Topology 63. So, mostly excellent topology method will produce some crazy number of the false positives. So now, if you increase the halt error, extreme noise, just nonsense. It's causing stickling. So that's what I say. So I'm going to start here and Yeah, and if you well, that's our score. So we have a few times, a few minutes before you might as well. So you're basically saying that in the topological space, perhaps there's what's the think about the main difference between men and women, especially investment connectivity. I don't think that's actually. I don't think there's actually a topology difference, it's just the magnitude differences. So, topology differences are very robust on the scale changes and deformation changes. So, that's why. That's why you don't need a data harmonization if you're using topology talents. That's a message. You don't need why you want a data harmonization. Thank you. I guess I'm not clear. How do you get to the top? How do you construct the topological tree? What drives the pattern of the tree? I mean, you mean the soccer tree? Yes, soccer tree. Oh, that was actually extracted. We have both have to compute the We have first have to compute the Gaussian curvature, for example, Gaussian curvature mean curvature for print surfaces, right? And from there, we actually need some bunch of image processing to identify this graph structure. That's one way. Another way would be actually solving a differential partial differential equation that like a flow, like a kind of flow equation. And the least flow, yeah, this flow, you have to solve the litch flow on the grain surface, and from there. Page and from there, you somehow do the imposition to the discount wrap three structure. So it's very involving. It takes about one year. It's not something I did myself. I have to recruit someone who can do special and pre-image process. So you can't do it on chat GPT? Oh, this is beyond the chat GPT. Thank you.