Fine-brain complexity area. So, in the three-sum problem, it's an element problem, right? And the input to this problem is, I guess, a couple of different variations, but let's say we have three sets at A, B, and C, and these are all subsets of some set of integers, at least in this formulation, from minus u to u, it's the case for all of them, and they all have size n. And the goal is to determine, so the goal of the freedom problem is to determine if they exist. So an A and A, a B and B, and a C and C, such that A plus B is equal to C. So we have to find three numbers that, for the first two, sum to the Set for the first two sum to the last one. There are other versions of it where you only have one set, you have to find three numbers and sum to zero, but they're all equivalent at least if you look at the running time to solve it. And this problem here, I guess, what it's used for in this primary complexity areas is kind of used as a hardness, a hard problem, or reduction to prove hardness of other problems. And particularly, you have this freesome conjecture that basically says that there's no into the 2 minus C algorithm for any constant C. So you cannot beat quadratic time to solve this problem. That's the conjecture. And maybe I can also emphasize that you simply assume that the universe size here is polynomial mm and indeed our reduction And indeed, reductions where you can always reduce it to n cubed, there's a reduction where you can reduce the universe size to cubic by hashing with a linear hash function, which preserves the sum problem speed up. Okay, so the speed sum problem, right, is to use a lot of reductions to prove hardness of various algorithmic problems and also for data structures. There's a very beautiful work by a trash group in By a trash group in the German version is back in 11, where he gave a reduction from freesum to what's called the multi-phase problem. And maybe you have seen that one before in the complex communities. At least it's been studied there a bit. So what is the multi-phase problem? So the multi-phase problem, I guess, is a dynamic data structure problem. Well, there are three phases. So in the first phase, Phase phase one, right? You receive this input, case sets, and it's in a subset of the universal size n. And so you have to pre-process these sets into a data structure. And let's say k is polynomial larger than n. And then in the second phase, you receive another set T. Set T which can then be thought of as kind of like an update or something that you have to store in your data structure as well. And finally, this third phase, so you receive an index I and you have to output whether the I set of ones you got initially intersects this T that you got in the second face. That you got in the second phase. So that's the freesome problem. You have these three phases. And if you believe the freesome conjecture, then there's some, then regardless of, I guess you can almost ignore the pre-processing phase, but the idea is that if the second phase here, right, your input, the update has size m, if the threesome problem, like the threesome conjecture is true, then either the time you spend in this phase has to be at least Has to be at least into the one plus a constant c or the time you spend here in the second phase has to be at least into some constant c. Okay, so basically what you get is a polynomial lower bound on either the update time is either polynomially larger than the size of this update that you receive, like the size of the update is linear and n. Either you have to spend polynomial time in the size of the update, or the query time here where you only get an index as. The query time here, where you only get an index that has to be polynomial large. And then you can do reductions from this problem to a whole bunch of other data structure problems and get polynomial low bounds for a bunch of dynamic data structure problems. And these are problems where you have updates and queries to the data structure problem. Okay. And maybe let me finally mention that Mihai Petrasku also suggested a way to prove this conjecture unconditionally without going via the threesome conjecture. That's via a Recent conjecture: let's be a communication game with advice. And I know that Tony and Aguiler have a really nice paper on trying to prove lower bounds for this communication problem. So it's basically a three-player number on forehead communication problem. But I think for the sake of time, I don't think I'll go into that, but I'll try instead to just say what our is about. Okay, so that's a threesome problem. And based on this resume conjecture, you can prove low bounds for these dynamic data sources with both updates and queries to So they're both updates and queries to the data structure both. Then in 17 there was some follow-up work where they tried to define another version of the freesum problem that's kind of meant for proving lower bounds for static data structures. So data structures where you get an input once and for all, you pre-process it, and then you only have queries, right? So you don't have these changes to the data. And this problem was called resumpt indexing. Okay, so here you get, first you get an input, so it's a data structure problem, and you can even get an input, and the input is two sets, and maybe because I'm going to use B for something else afterwards, so let's call it A1 and A2. So you get A1 and A2, and they are both subsets of say grants U2 and And they both have size n. And then you have to pre-process them into S memory cells. Basically, when you build a data structure that uses S memory, and this is typically done in this world brand model of computation, where each memory cell have cells have, let's say, WNC. say w is let's say it's log n bits so that's a standard assumption that in this workbrand that the memory cells have enough bits to store pointers and indices and stuff to other memory cells. We have to... Sorry, Casper, can I stop you for a minute? It seems like the board is not focused. Do you want to equal zoom C? Oh, right. Like this. Yeah, I cannot see. Yeah, we can't read the board now. The board now. Okay, so not focused, yeah. Is there anything else? No, you're out of focus, too. That's not true. Okay, now we're down. Okay. Okay, let's try again. Thank you. Sorry. Okay, so you get to pre-process these two sets into the instructor S-memory cells of the bits. And then afterwards, you get queries. And then afterwards you get queries. And a query is then a number C, again from minus u to u and then you have to output whether there exists A1 in A1 and A2 in A2 such that A1 plus A2 A1 plus A2 is equal to C. So it's kind of like a data structured version of the 3-sum problem. So you just have to check whether this is part of a 3-sum. Okay. So that's a 3-sum indexing problem. And together with this problem, they phrase three conjectures. So let's spread it a little bit more over here. So they're conjectures in increasing, I guess, strength of what you say. So the first conjecture. So the first conjecture says that any solution to this problem if t is constant, then this base is n to the 2 minus the rho of 1. That's the first conjecture. And there's a second conjecture that says that the time times the space has to be at least n to the 2 minus little of 1. The last conjecture. The last conjecture says that if the time is n to the one minus a constant, then the space is at least quadratic. So they clearly the last conjecture implies the second conjecture, which implies the first conjecture. Sorry, so T is just a pre-processing file or read. Okay, you have to output this reading, sorry. I'll put this reading, sorry, T memory cells. So that's kind of the query time. Ah, that's sorry. Thanks. So either the time it takes is at least, based on all these conjectures, if you have constant time to answer a query, you need credit space. The product of the time in the space is credit. And the last one is: if you have anything that's sublinear bipolar factor in the time, then the space is quadratic. Time, then this space is spread out. And we will just see a couple of trivial solutions for this problem. I guess there are two obvious solutions to the problem. One of them, I guess, is if you want to use large space, you can just store the sumset A1 plus A2 in a hash table. And then when you get your query, And then, when you get your query, you just look out in the hash table to see whether the query element is there. So, here the space is quadratic when the time is constant. So, this doesn't violate any of the conjectures. And I guess the other one is to, I guess, store A2 in a hash table. And then, when you get your query C, I guess you can look through all the elements. Look through all the elements in A1, each A1 in A1. Basically, you know what does the element have to be in A2 to sum up to C. So you look for C minus A1 in A2 using a hash table lookup. So here the storage is linear, but the time is also linear. Is linear, but the time is also linear. So again, the product is squared. These are the trivial solutions, and basically they conjecture that this is at least the strongest conjectures, kind of saying that this is all you can do. And based on these conjectures, you can then, via reduction, prove conditional lower bunch of a whole bunch of static data structure problems. Or the same ones that you can versions of the same problems that you could do for dynamic problems. Okay. Does it like im implied from strong plus uh Implied from strong uh exponential points? Uh n no, I don't know, these threesome ones, I don't think they imply each other with the threesum and the strong differential time. I don't know, that's the orthogonal vectors problem next there. That's the one where you can reduce from the strong differential time buttons. I don't believe there's a reduction to yeah, I think maybe there's something with esum needs n to the three-half if a formula x is true or something like this. I don't remember exactly, but there's something like this, yeah. Right, okay, maybe that. Right, okay, maybe let me mention that news is by Gaussian 17. Okay, right. It's a little bit more history than two years ago I was breaking some problem where so In 20, they refuted the strongest of the conjectures. Okay, so the last one is false. So basically, they gave a data structure with, for any delta, they gave a data structure with time into the 3 delta and space into the 2 minus delta. So you can maybe set delta to be 1 over 6, then you have root n query time, and you have something. Have root n query time, and you have something subquitetic in the space side. So it violates the last one of them, but the product, these two are not violated by this upper bound. So they, and this is a really interesting reduction based on some functional version from crypto that they used to get these better upper bounds for three something. So it's a really nice work. What they also did in this work was they tried to prove, they proved some unconditional low bounds for the problem. They're not as strong as the conjectures, but they did prove. The conjectures, but they did prove the first unconditional law bars for this problem. In particular, they proved that the time has to be at least logarithmic in n over log of the space usage times the word size over n. So we have a low bound of this form. What does it say? But I guess it says if you want constant time, like in the first setup here, then you cannot do it with linear space. Then you cannot do it with a linear space. You need this ratio between the space and n to be at least a polynomial large. So basically, this implies for constant time you need space as to be least n to the one plus over there. But something non-trivial, it's not all the way there, but it's something. Okay. Maybe one thing worth mentioning is also this is. One thing worth mentioning is also this is basically, or this is the state of the ad in terms of data structure logs. We don't know how to prove anything stronger for any data structure problem. Except we can get rid of the W there, so sometimes it's V log N. If the space is always linear, this is the best log N over log login. You can get a little bit better at lower bounds in this, but not much. But there's a downside to this lower bound, and that is it only holds what's called What's called non-adaptive data structures. So, what's a non-adaptive data structure? So, non-adaptive data structures: one, basically when you get the query, I already know which T memory cells I want to read. I can ask for all of them at the same time. I'm not going to read different things depending on what I see in the memory. Then it's non-adaptive various. So, in advance, I can say I want these cells, and those are going to be sufficient to answer my query. To answer my query. Lo bound only holds for such non-adaptive data structures. And I think it is interesting to see, for instance, this last solution here, that's not a non-adaptive solution, but it's a very adaptive one, right, where you look at the elements in A1 and based on that you compute some hash function or something that depends on what you saw in A1 and you break it to different memory locations. So this is not non-adaptive. Also, this result out here is also very adaptive as well. So that's adaptive. And they mentioned in the paper that the technique very heavily relies on this being non-adaptive. There's no way of tweaking it a little bit to get rid of this non-adaptive assumption. And where they ask it, there's no problem whether you can prove it also for adaptive data structures. And that's what we do in this work. So I think I'll spend a remaining bit of time on trying to show you the main ideas in this lower-bound proof. So, how can you prove for adaptive data structures? And it's a really nice. Data structures, and it's a really nice, I think it's a Q reduction from computational complexity. So I hope you find it interesting. And it doesn't use any of the approaches from the previous work at all. So this also goes back and uses some of the nice work that Mihai Trask did on data structure lower bounds. So he had a really nice paper in 2010 or something like this. So Miha Triesco proved. So, Miha Betrask who proved also a bunch of lower bounds for standing data structures. He was the first one to prove lower bounds of this form here. Before it, all lower bounds had a log S here, which is the spaces polynomial in N, like you cannot distinguish linear space from kinetic space, so you're never going to get anything non-trivial in terms of lower bounds for problems like this one before his Fourier grade. So, he had this really nice new way of doing reductions from communication complexity, and what he Complexity and what he used, one of the problems that he used to get production for is a version of set disjoints. So, this is what we're going to look at here, is what he calls blocked LSD, so blocked lockside set disjoints. So, basically you can think of it as set disjoints where, let's say this is the universe, here all the elements that could be in your sets. We have Alice and we have Bob, and they're substructed to their inputs. To their inputs. So let's say that the, you can think of the universe as being partitioned into blocks of B. Basically, the universe is basically m times E. Okay, like this. So the universe partitioned into these blocks. And Bob said it's just an arbitrary subset of the universe, right? So this could be an arbitrary subset somewhere. What Alice has said is smaller, so it's constrained to have exactly one element in each block. There's only one element here, right? So there's a single one in each of these entries. So this communication problem, there's a lower bound dating back to I think Middleson develop from the 90s, where they proved that any problem. Any protocol for this problem? Either. Problem? With a question? But what's the problem? What we're trying to do? So set this joints between these two sets. Yeah, yeah. So basically, let's call this X and this is Y. And we have to check whether X understand Y. Okay. So there's a communication logon for this one, and one version of it says that either Alice sends N log B bits all bobsets in square root units. So there's a trade-off right where you can see that Bob's communication is larger than Alice's communication. Okay, and that's going to be important for this reduction to data structures. So basically we're going to take this problem So basically, we're going to take this problem, and if we have an efficient data structure for freesome indexing, we're going to get an efficient protocol for block-loop sites and six joints, right? And from that, we get a little bit problem. For general two-party communication? Say again? Yes, yeah. You can have arbitrary interactions between. Yeah. Yes. Okay, and in our setup, we're going to just, let me just put it right now, let this be here, we're going to set it to be some. This B here, we're going to set it to be something that's related to this word size of the data structure. Let's say government to the fourth. So let's just define it like this. Okay. We want to say that now if we have a data structure through some indexing, we can solve this block lob size status challenge. And what are we going to do? So how is the production going to go? So the idea is that, okay, so we have Alice here and we have Bob. Remember, Alice's input is zy, it has only one element. Set Y. It has only one element in each of the blocks, and Bob's set is X. And what they're going to do is that Bob from his set X is going to build an input to freesome indexing. So he's going to compute two sets, A1 and A2, of size N, and then he's going to build a data structure on these two sets, like the freesome indexing data structure. So basically going to build a data structure on these two. Alice is somehow going to take her. Alice is somehow going to take her set Y and turn it into a bunch of indexing queries, right? So she's going to perform a bunch of queries, C1 and so forth. And then they're going to, by communicating, they're going to answer all of Alice's queries on the data structure that Bob has built. And the idea is that in this reduction, we would like that these answers to these queries determine whether X and Y intersect or not. And the amount So, and the amount of communication should be proportional to the efficiency of the data structure. Okay, so let's try to see how it goes. For this reduction, let me just say a little bit. So, what we're going to do is that we're going to take this universe of these blocks and we're going to partition it into superblocks, let's say. So, superblocks consist of L individual blocks. And in each of these, And in each of these superblocks, Alice is going to ask one query for each superblock, and that query is going to determine whether the two sets intersect inside the superblock. Because that's the basic idea in the reduction, right? So, which means that since there's n blocks, Alice is going to answer n over L queries on false data structure. So we'll see that the more queries we can answer, or the more blocks we can handle in one go, the stronger the log. Handle in one go, the stronger the lower bound will get out of it. Maybe let's for now just say that let's assume that you know we actually can do such a reduction so that with n-o-wail queries, Alice can figure out whether the two sets intersect. How do we get a protocol from this? The basic idea is since the query time is t, we want to have these rounds where in the first round Alice looks at her queries and she looks at what is the first cell that each of these queries wants to read. And then she's just going to ask Bob for the contents of those cells. Contents of those cells. So basically, she collects the set of N over L cells that you want to read and she asks for them, right? And that costs log of S choose N over L bits to ask for those cells, right? She just specifies as a subset of all the N cells. And Bob is going to reply just with the contents of those cells, so that's going to be something like N over L times W bits. And then they're going to go on for T rounds. Because in each round, I ask for some cells, I get back the content, and then we go on for T rounds. Now I finished simulating all the query algorithms, so I can output all the answers at the end. Alice knows what was in the data structure. So how much do they speak, right? So Alice is T rounds. Log of S choose N over L. That's basically N over L log S L over N, right? Right, and Bob's communication is going to be T rounds n over L times W here. Okay. So let's look at Bob's communication first. Even if we ignore the L, this is at most T N W. And what does this lower bound say? It says that either Bob sends n root B, so for our choice of B, this is N. So for our choice of B, this is NW squared. Let's compare TNW versus NW squared. Well, this says that basically if you divide over, right, it says that either t is at least w, which is logarithmic in n, and if t is logarithmic in n, we're already done with our low bound, right? We're only hoping for a low bound of log n over something. So basically, Bob is going to send less than the interesting case is when Bob sends less than this many bits. Now we know that Alice has to send at least n log b bits. So that's the low bound we get out. Alice has to send n log b bits. So the low bound we get is t n over l log s l over n is at least, let's see, n log w hips. So Alright, so moving things around, the lower bound we get is T is omega L log W over log SO over M. Something like this, right? So as you can see, the lower bound gets better, the more blocks we can handle with a single query. So then we get a stronger and stronger lower bound. So we want to see, in our reduction, we want to maximize the number of the size of the superblock we can handle with. The size of the superblock, we can handle with just a single query from Alice. And if we just for now say that what we'll end up getting is that we can actually answer some small constant epsilon, we can do log n over log w blocks at the same time. And if we plug that in, we get the low boundary claim basically. The log w hence the log w and we get a this log n over log. And this is basically going to be S W. And this is basically going to be SW over N. I'm not going to go too much into precise calculations, but the important part is this that we get from this L. If you can handle log N over log W blocks in one go with one query, then we get the low bound we're looking for. Okay, so now I just want to show you how can we, what is this setting one and two, and how does L translate a superblog into one query. To one query. I don't think it takes that much. Tasker. So I'm slightly confused about the parameter regime. So W, oh, W is login. Okay, so this is login of remote login. Okay, fine. Good. Okay, so let's try to see how do we make this reduction. So the basic idea is that we're going to, so Bob is. Going to so Bob is going to build these two sets, A1 and A2. And when Bob is building the set A1, he's going to build that based on his input Y. So he takes this block here, so he has one of these superblocks, and let's just call the superblocks 1, 2, 3, 4, and so forth, right up to NOL. So in the I-th superblock, he's going to basically create a number when the leading. number in the leading digits we're going to write i and then he's going to create a number for each of each of the ones in his input and so basically the these ones are going to look like they're going to have a base 2b digit l times like one for each of the little blocks inside a superblock and he's just going to take if i have the jth element in the second block i'm just Element in the second block, I'm just going to write j in the second block and zero everywhere else. So that's the first thing, just takes every single element, writes down what is the superblock that I'm inside, and then I just kind of have a digit for each of the little blocks, and I just write my, if I'm in the second little block, and I'm the jth element, and then I'm just going to write a j in the second position. So that's the set A1. It just has all these elements for all the things in box set. And the set A2 And the set A two is ju it doesn't depend on its input at all. It just has, well every number in there has a zero in the up here corresponding to the i. Then we have these L vintages at the bottom and there we're going to basically going to take every single number where you put a zero somewhere and then you put anything between one and b in the other positions. I guess I'm going to make them one index these notes. I guess I'm going to make them one index these numbers in here. So these numbers are one to be the numbers in here. The index offset into a block. So that zero is different from all the digits I could create from this individual block. So basically, Labob is just going to put every single element in there where star is of this form. Okay. So let's see now. So what is this? What do we now do? Good, so I'll quickly wrap up. So if we look at Alice's If we look at Alice's query, you're almost done. So, Alice has this blob now again where she has a single one in each of these positions inside the i superblock. So, her query is going to write i, and then it's just going to write in each of these l positions, and it's going to write what is the position of the one I have inside this block. Three, five, two, one. Okay, and we claim this is it, right? There is a threesome here if and only if they intersect. It should be, right? The only way we can create a threesum is we have to use someone in any one that says i up here, because there's no carries. So it has to come from that block. And everyone that comes from that block is going to have the pattern that says j, something non-zero in one position, and zero everywhere else. And of course, that has to align. If we're going to use it, it has to align with my digit in that block, which means there's an intersection. And if it does, the second one here, I can take the one that has a zero, it decides in that block, it starts everywhere else. And this is an if and only if. There's no way to create a freesum in any other way. And then you have to use a zero aligned with a non-zero here. Otherwise, there will be a zero left and there's no zeros in alice here. Alice is here. Okay, I think just to wrap up, I guess we just have to see what are the size of these parameters. So, I guess the main important thing is how big are these numbers that we create? There's NB numbers in total, one for every element in Bob's set. You just need to look at are these polynomially large? Or how many numbers are there down here? That's also the second. The only thing left to see that this is of linear size. So, the numbers here is basically B to the B to the L numbers. It's basically b to the l numbers here, right? Maybe b to the l minus one different numbers down here, right? Because you have maybe times another b to say which one is non-empty. So maybe b to the l. And right, so we said b to epsilon log n or log w, and b was w to the fourth. So this is some small polynomial less than m. This is n to the epsilon line to the fourth. So that's fine, it's not too large to be input. So that's fine, it's not too large to be input. And up here, I guess there's one for every element in the box set. And the magnitude of the numbers are also not so large. These numbers here just index the superblock, so they are n over l large. And here there's some b to the l again. So the numbers are only polynomial large as well. Yeah. So that's the reduction. And I guess I'm also out of time. So let me stop here. How far is it to solving this problem? Can you find some other communication problem which is like it's if and only if for data structure or I think the main issue with this reduction from communication complexity is These reductions from communication complexities, like in all the normal reductions, it's something like, at least for static data sources, Alice always holds kind of like some queries. And Alice can always just send the queries to Bob and then you can reply. So you're not really going to get your communication loadouts are going to be higher than Alice's sensor input to Bob. But you can ask non-symmetric communication questions, right? So how much Alice versus how much bots? Like we did here, right? Yeah, yeah. Here, right? Yeah, yeah. Yeah, they still tend to break down that Alice just sending her input, which is something like you have to somehow, Alice is going to ask for something for every query for every step of the query algorithm. And if there's more than the rhythmic number of rounds, she can already just send her entire query. That's usually how, that's why they break down, it seems to me. So I guess any sensible reduction, Alice has some queries and she's going to tell Bob, what do I need to answer this query. What do I need to answer this query in each of these T steps of the query element? And even if Alice managed to do it with just one bit of communication, after login round, she has already sent login bits per query. And the lower bound is never going to be higher than login per query because she can always just send the whole thing. The communication low bound is not going to be higher than login. So it's kind of doomed to prove login and lower bounds if you try to do something like this. Yeah, yeah. The dynamic problems, there's more hope. The dynamic problems, there's more hope. I think, like Tony and Agadev did this work on this communication complexity with advice with some number one for hit game, which is very interesting. And there, there's no reason to believe that it's not possible to actually prove something. So, could there be some more surprising upper bounds? So, your framework of polynomials, I don't remember if there was this work with fast polynomials. Here you don't really care about time. Does it give anything here or Does it give anything here? No, I don't think this is. So there is this. Okay, so you always have to worry that there is a surprise in upper bound, like 1.1 SMT, so one kind of approach that in other settings that gave something but only log factors was based on polynomial evaluation. But I think there it was type, not just query and uh Yeah, I think that that one's also space-time trail, but we actually did get something surprising yet for the bettering polynomials or finitely. That doesn't always have logarithmic time to answer the risk brain. Could be that the guy. The bounds here seem to be based on cryptographic kind of crypto-inspired tricks, but it's a clean, it's like function inversion. There's something like how many extra bits do you need to store to invert functions, right? And then that the users. But this approach. But the the this approach is that doesn't work or doesn't try it? It's not clear how to use polynomials, but it's yeah, I don't I don't know. I think it would be a major issue if it was actually possible to do something like that. 