Potential theory and so forth, but it always has a very warm place in my heart. So, this is very good to see old friends and new friends in the audience. And let me try to give you just a glance what I've been doing recently, right? So, this will be about homogenization and essentially in the non-linear elliptic equations. So without further ado, oops, let's see if this is working. So what is basically the basic problem or paradigm what I'm considering here? So essentially you have a heterogeneous Lagrangian. So basically you have the gradient variable as usual, but then something which is oscillating very, very widely in spatial variable. In spatial variable. Okay. And then let's say that you're asking that minimize over given boundary values, so just the problem over the Lagrangian in ellipsic domain and with some h1 values. So I'm not kind of putting too much assumptions on L. I will kind of describe them later on, but this is just basically, so my P is 2 now. So it's uniformly convex, uniformly convex Lagrange. Okay, so now. Okay, so now what we are assuming, we are assuming some kind of ergodicity assumption. And now erichodicity, I also include something like periodicity. So that this is like given on torus, essentially that L maps to that heterogeneous variable, or it can be almost periodic, or the case what I'm considering is basically that there is some stochastic law, law behind. Law, law behind the Lagrange. I will give you just kind of like a very concrete example, soon. Okay, and if you have something like this, then it turns out that basically this so-called homogenizes to an effective minimization problem. So you send epsilon to zero, and essentially you find a new minimization problem. So you still have the same boundary values, but now you have homogenized de Lagrange. Homogeneous the Lagrange. Okay, and of course, this is our favorite object. So, for example, the questions would be: when can one expect homogenization? And this is the qualitative theory by Dalmaso and Monica from the mid-80s. So they were successfully using gamma convergence to prove that the homogenization happened. So this is the most qualitative statement what you can basically state. So then the second. So then the second question, what has been basically under intensive study now for, let's say, last 10 years. So what is the rate of homogenization? So give by means of epsilon an estimate that how fast is the homogenization happen. Okay. And now maybe the question what I'm going to study, or basically what our results are about. So we were asking a question that if you happen Asking a question that if you happen to know the regularity in this gradient variable, okay, so then how regular is the homogeneous Ragranj? So kind of like our analogy was that so Hilbert was asking that if L is real analytic, so are the minimizers basically real analytic. Now we were asking the question that, okay, is this somehow a persistent Okay, is this somehow a persistent thing that if we have, oops, sorry, if we have real analyticity in this one, okay, this is still an open problem, but let's say that you have smoothness in this coetescence. Do you actually always get smooth minimizers for the homogenized Lagrange or for the homogenized minimizers? Okay, so that's basically the setup. the the setup what i'm talking about and let me have an just an example that that that's just kind of like what what could this be so the the stochastic case so i'm just having kind of like randomly put here some some so this is called checkerboard so i'm randomly uh tossing the coin and if i get heads it's black and if it's uh tails it's white and then i just go through all these small Go through all these small cubes here and proceed with the coin toss. Okay? Or then I have something like a Poisson point cloud. So I just take the Poisson point cloud and around every point I draw a ball of radius epsilon. And then I apply the same rule that in the black region, the Lagrangian is something and in the white region, it's something else. So it's kind of like very widely oscillating. Very widely oscillating, but there is ergodicity in this one. So, kind of like, for example, here, my rule or my law for the coefficients, it's IID. So, whenever I move away, kind of like epsilon away, actually the law remains the same. So basically, the law under the coefficients, that's periodic. All right. So, now, kind of like, let me kind of just give you this a very classic. Just give it this very classical example from the linear theory in 1B. So I have here a conductor, and basically I sort of apply the same rule that I divide this one into those very kind of tiny segments. And each one, I basically put there a probability that it's a good conductor or it's 100 times worse conductor, which is a resistor. Okay? And essentially, my equation, so if I minimize this one, my So, if I minimize this one, my equation looks like this. So, I have A is basically one and one over 100 in the red part. Okay. And then I put the boundary values for the potential is one here and zero in the other end. Right? So, this is the most classical thing where you see kind of harmonic means, right? So, what you are doing, you can actually So what you are doing, you can actually explicitly write the solution for this one and you find a formula for it. And here I just drew a couple of pictures of it. Okay, so you see that whenever there is a red part, there is a very high gradient, right? And very high gradient is very bad for conductor because it's always producing heat, right? So this is why kind of like if you sort of short circuit something, then it's. short circuit something then it burns basically from the place where the where the where the conductor is the uh where the kind of where the material is the most resistant okay and now if i just kind of like make it actually like epsilon smaller and smaller and smaller so this is actually giving us homogeneous edge so you see that it starts to be the direct line which is actually solving the homogenous problem okay now the question is that so this was Now the question is that, so this was just kind of like I give you some sort of idea that what is homogenization do. Okay, so you see that there is lots of randomness here, but then in the end, when I let epsilon tend to zero, it homogenizes. Right. So the other kind of enlightening example, what I like to mention in this one, is that let's just take the linear model. Okay. And again, I apply the same rules. And again, I apply the same rule. So I have the checkerboard example. And if it's a white cube, I put identity matrix. So this is in dimension two. And then if it's a black cube, I have nine times identity matrix. Okay, so again, it's oscillating rapidly, but in a very, very nice way. Now, because of my law, there is actually a probability of kind of non-zero probability for the fact that they are all black or all white cues. Or all white cubes, right? So that it's basically just a nine times Laplacian or one times Laplacian. Okay? Well, you can show that the homogenized matrix, so this is so-called Dikner's formula. So that A bar, so the homogenized matrix, it's actually square root of nine times one. So three. Okay. Oops, sorry. So now let's compare. So this is the crazy element that all my coin tosses were basically. That all my coin tosses were basically black, okay, or giving black, and this is the homogenized problem. And of course, now we can explicitly solve this, guys, right? Okay, I'm having it in a ball. So you find a solution, and now you see that this u epsilon has to be three times, sorry, one third over u bar, right? So actually, the solutions are well off. So they are like c away from each other. Away from each other. Now, very typically, when you're actually opening kind of homogenization paper or book, so you're sort of saying, for example, in the periodic set. So I would get an error estimate at how far u epsilon is from u bar, you would get epsilon in front of that. For example, in this very particular problem. Okay, so you would get that u epsilon minus u bar in L2 is epsilon, they are epsilon apart. Okay, but now this example is basically saying that Example is basically saying that there has to be kind of like a random variable which is actually measuring somehow this crazy event that I found it all black or all white. Okay. And now kind of like one of the basic paradigms of the stochastic homogenization is then to give an estimate, kind of starting from the mixing condition or sort of like what is the law of your of your Of your coefficient field. So you want to prove that basically you have kind of like a good stochastic integrability for the largestness of this random variable x. Okay. And for example, in this example, what I was describing, the probability that this x is larger than one over epsilon, it's like exponential minus epsilon minus two. Minus epsilon minus two. Right? Okay, so this is kind of like something that I want to keep you in mind when I'm going forward. Right. Okay, so what are we assuming from the Lagrangian? I will be super fast. It's basically smooth enough in P, so it's convex, like uniformly convex, and that's it. So smooth enough in P, you can really think that it's as smooth. You can really think that it's as smooth as you want in the gradient variable. We are not interested in, or okay, so we are kind of like wanting to actually push these ones, estimates up to the very end so that k in this slide will be sent to infinity, so that we have smooth Lagrangian in cradient variable, like my example here was. So you see, it's crazy in X, but totally nice, kind of pointwise in X in P. All right. All right. So here we are. And then what are you doing? So now we want to basically give a datum for a datum for our Lagrangians, right? So basically, in the example, when I have the checkerboard, you could really like describe somehow a probability or sorry, the product structure. You're just kind of collecting. Structure. You're just kind of collecting all possible outcomes. There are finitely many of the outcomes, and you assign a probability for each one of these ones. So it becomes like a huge combinatorics problem, but you can do it. So this is a data of our problem. Okay. So what you do, you collect all these Lagrangians, what you have, so smooth and uniformly convex, and you collect them into set Omica. So there are all the Lagrangians. So there are all the Lagrangians now. And now you basically give them sigma algebras. Okay. So sigma algebras so that you're integrating that Lagrangian basically against, let's say, smooth, compactly supported test functions. Okay. And there should be capital U so that you're sort of saying that you're concentrating your Lacrimentians in U. Okay. So this is kind of like just to give basically, I want to give a probabilities for I want to give a probability for some events, right? And this is the data, basically, what is happening. And then I basically say that I have all the omegas, so all the Lagrangians and all the sigma or the sigma part of these Lagrangians, right? So then I say that P is stationary with respect to Z D translations. So just think of the checkerboard example. When my law is moving like epsilon away, it's the Way it's the same coin toss, okay, or the law. So, I'm always uh tossing the coin, right? So, basically, the law is stationary, right? And this is with respect to Z D translations. So, whenever I move the law, it remains the same. And then the other one, which is kind of like, okay, this is sort of like you can generalize this much more, but of course. This is much more, but for simplicity. So, when basically I have two sets in the physical space, and if they are one part from each other, so epsilon away in the kind of like epsilon at the epsilon level, so then they are independent. So, again, the cointos is a good example. Keep that one in mind, right? So, the cointoses are independent of each other. So, the Lagrangian is whatever. Other. So the Lagrangian is what I actually show that they're independent of each other. Okay? So that's it. I don't want to talk too much about it because I don't have time otherwise to say anything about the results. Let me just basically say a couple of things that how the history has gone. So basically, this is about the linear theory and some comments basically about the non-linear theory as well. So this is So this is kind of like the linear theories, basically that was done qualitatively by Papanikola, Varadian, Koslov, and Lurensky in the 80s. And then a different method was devised by Dalmasso and Morica. So they had kind of like this sub-additive, more like energetic methods to prove things, which is very well suited with gamma convergence. Okay. So they had that one in the 80s and this work in the non-linear setting as well. And Armstrong and Smart pushed that one like And Smart pushed that one like 2014. So there is a really, really, really seminal and good paper by them, and where they proved kind of like a larger scale regularity statements. I will come back to this one later on. So, and this quantitative theory, so actually it was basically coming from more like mathematical physics by Natov and Spencer. So, where they use so-called concentration inequalities, like logarithmic so-called inequality. So, basically, Have inequality. So basically, kind of like you could actually think of that checkerboard example again as an example of something which is having like a very high dimension, right? So basically, I have like this, this I have actually kind of like a product type of probabilities there. So I could actually sort of like say that let me have a random variable for each one of the configurations, what I get for the checkerboard, and then kind of like. And then, kind of like there will be some kind of, so that dimension will be extremely large, but nevertheless, I can actually do something like kind of like measure that how sensitive it is to a change of one cube to another cube or one cube from black to white or vice versa. And I can sort of like measure that one with logarithmic Sovalot type inequalities or something like that or other type of concentration inequality. Or other type of concentration inequalities. And this is precisely what Nadoff and Spencer did. So they had kind of like: let's just have a gigantic dimension for the problem and use the functional inequalities there. So functional inequalities, which are independent of the spatial dimension or dimension in general. Okay? And actually, Gloria and Ottawa, they were pushing these ideas like much, much, much further. So they get many optimal. So, they get many optimal, super nice results, and so forth. And then, basically, so we actually came in. So, this was a collaboration with Scott Armstrong and Jean-Christoph Muram, but then there are many, many others like Antoine Gloria, Felix Sotta, Fischer, Neuicam, Guden, Ring, Scaria, Junti, Bell, Lanol. And so, yeah, this is sort of active field. And now, Fisher and Neuicam actually, they have. Fischer and Neukam actually they have some results which are really like parallel to ours. Okay, now so now let's go to actually to the real be if I will of course run out of time, so but yeah, I knew it, so whatever. Okay, so what are actually the statements then about? So first of all, I have this random variable x tests. So I kind of like try to tie this together with the epsilon. Together with the exile and what we saw before. Okay, so what do we have? We have this kind of random variable, which is essentially, I prescribe the integral of that one. So that this is like almost like OD integral. So my example, it was essentially D was 2 and I had S is 2. So this is like a borderline result. You get almost optimal result. Almost optimal results, and I think that we can nowadays do it optimally as well. So, that S can be translated all the way to D, which is then stemming perfectly the example. All right. Okay, so this is kind of like a stochastic integrability for that. So, it's nothing else but stemming to this fact that, yes, the cubes might be all black or cubes might be all white. Right? So, crazy things happen. And then, what you're saying, you're saying that let's take the heterogeneous minimization problem. So, this is the Euler-Lagrange equation of that one. And let's take the homogenized problem and the corresponding Euler-Lagrange equation. And let's measure that how far are the gradients in H minus one and how far are the fluxes in H minus one from each other. And then you see. Each other. And then you see that there is a constant here. And then we have epsilon power alpha. Alpha is a small parameter, just larger than zero. And then there is a kind of trade-off. So if I want to lose, or if I want to gain basically a bit better deterministic smallness, so if I want to take S kind of like strictly less than D, which is then meaning that I'm kind of far away from that optimal stock. Far away from that optimal stochastic integrability, what I was describing. So you get better deterministic smallness, but then you start loose in the stochastic part. So there is kind of like always a trade-off between the deterministic and stochastic control of the problem. But this is the basic quantitative homogenization result. Okay? Now, the questions which are still to some Questions which are still to some extent open that what are the optimal exponents here and so forth. So, yeah, to a certain extent. Okay. And then which is maybe more interesting to us is kind of like the regularity statement. So what you can, so think of this one. This is a harmonic approximation result, right? Okay? Or harmonic type harmonic. harmony approximation type result. So it's basically saying we're having something horrible and then you're having something nice which you're close by. So by embedding you always get from this one that you are in you're at closeness in L2. Okay so you can indeed use this idea that you have a good regularity for this homogenized Euler-Lagrange equation. Lagrange equation, and then you can borrow that good regularity and transfer it to the heterogeneous problems. And actually, this was a seminal idea of Fang Ma Lin and Marco Avellaneda. So in kind of like late 80s, they proved in the periodic setting that indeed you get kind of like a large-scale regularity out of this one. Or well, they proved full regularity. So you can kind of come up from very far. From very far. And then you have again another random variable xs, which is saying that, okay, I can zoom in, zoom in, zoom in. I can zoom in all the way up to this minimal scale Xs. And then I have to stop my regularity result. But kind of, so if you would put this Xs to zero, so then this would be just a traditional Lipschitz estimate, right? So this would be just saying that. So, this would be just saying that, yes, gradient is bounded, and that's it. So, this is why it's kind of called, oops, kind of called Z01 type estimate. Right? So, this is kind of like a large-scale regularity statement. Right. So, what do we do then? So, again, we basically find this random variable which is satisfied. find this random variable which is satisfying that that same interpretability condition and now the next step in proving that l-bar or the minimizers are smooth or that you have more regularity actually so it's to consider the linearization of these problems so let's start from the previous problems okay so we have you know same boundary values for u epsilon and u bar but then we actually take a look of the linear We actually take a look at the linearized equation. So we take that solution, napla u epsilon, and linearize the problem against that one. Okay? And we do the same for the u-bar equation. Of course, if you would be proving, for example, the regularity of the u-bar, so let's say in Schauder estimates or something like that, you would be differentiating that one, and this would be your equation. So you would be linearizing basically against that one. And here you have your. Against that one, and here you have uniform convexity, and then you would be just going, going on and on and on, and basically proving the Schauder theory and smoothness and Hillian of the solution. So this is how you prove Hilbert 19th. Okay, but what you get here is that solutions now for this linearized problem. So the previous slide is basically saying that, yes, the actual solution. That yes, the actual solutions are close by. But now, this theorem is saying that also the linearized solutions around basically the heterogeneous and homogenized solutions, they are still close by. Right? And of course, what we had in mind is that, yes, if you do this one, you can actually prove that L-bar has better regularity. So it's actually in C to 1. In C to one, so you win one derivative basically while doing this one, okay? And then even further, so you can actually, so when you're comparing two solutions, so you can kind of prove that, yes, they are actually close by when you linearize two different solutions. Okay, well, this is this is something. This is something that I know. I'm sorry, I of course spent too much time. So I will just kind of like very quickly sort of tell that what we did, what we did. So what we ultimately would like to prove, or what we proved, is that if this one is smooth, so then kind of you would prove that L-bar would be smooth or real analytic. And I said, real. Real analytic. And I said, real analyticity, I think that is open step. And okay, so kind of because why? If we want to have actually kind of good understanding of the regularity of the heterogeneous solutions, so basically we need to always borrow it from L bar, from the homogenized problem. Okay, so if we prove higher regularity for L bar, we actually get better regularity theory. Get better regularity theorem. So, without further ado, let me actually kind of like say that what you would do. What would you do? What would you do? So, this is just to remind you that how do you do it in the linear case? So, essentially, this is the Laplacian. So, if you have a solution to this equation, then you just simply say that let me take the Taylor expansion of that solution, and then you get the Rielian Luticity estimate. Okay. And moreover, you know that actually these guys, they are harmonic or a bar harmonic functions. Okay? Right. So what about the nonlinear setting? So this is kind of like just the last two slides, what I will have. So we want to basically find a suitable replacement of this aharmonic polynomials. Aharmonic polynomials. Okay. So what do you do? Well, you go and start linearizing the problem. Okay. So this is kind of like essentially what you want to do. You want to plug in the Taylor expansion of the solution and basically isolate kind of the degree of what you are taking a look of. Okay? So you want to basically say that, okay, so let me collect all these guys and essentially. These guys, and essentially, let me decide the homogeneity of the problem. So, this is kind of like the differentiability of the problem in some sense. So, in the appendix of the second paper, actually, we sort of proved kind of like a different in different way. So, this was necessary for us for some other reason. But you can very easily prove kind of the real analyticity of the solutions, kind of like a Schauder. You can bypass, in some sense, the Schauder Ethereum like this. This. Right? So, and now the idea was that you can actually find the solution to these linearized problems. So, you find polynomial solutions to this kind of, so you fix somehow that, fix the regularity you want to take a look of, or kind of the degree of the Taylor polynomial, and you will find all those polynomials which are actually solving the equation. Okay? And now you can prove. And now you can prove this is exactly what we didn't do in the appendix: is that you actually identify these polynomials, and this is kind of like the Taylor expansion you want to plug in. And this is really analogous ideas again. And now, kind of like the last step, what we do is that we so the first thing what we did is we homogenized the linearized equations. Then in the second paper, what we do, we sort of actually We sort of actually linearize all the higher order linearizations. And we show that in the similar way that homogenization and linearization commute, but now to an arbitrary order. And while doing this one, we're actually somehow inductively showing that L-bar is smoother and smoother and smoother. So this is kind of like a Schauder theory and homogenization theory in some sense. But anyway, I'm having six seconds left. Six seconds left. That's it. Thank you. Sorry, it was a bit fast, but yeah, the slot was kind of tight. Thank you for your attention. Okay, we thank Tomo very much for this extremely interesting talk on this fantastic new results. Are there comments, remarks, questions, appreciations, complaints? Complaints. Complaints. Complaints. Complaints. Complaints. Sorry, yeah. Okay. Let me for, I will, I will kind of try to justify myself. I was actually adding that slide about the conductor and also the example on the checkerboard just before the talk started. So it was kind of unexpected, but I thought that it's nice to actually have like a nice and concrete example of something, right? Of something, right? I hope that you appreciate it. Of course, we thought you were becoming an electric engineer, but no, no, no, no, yes. This is my secret dream, of course. This is your secret dream, of course. Are there further questions? Okay, let me probably make some remarks. And all this is for linear growth problems. And I guess that absolutely. I guess that I've seen something for peak growth recently. Yes, there is a paper of Antoine and Glossieux. I think that he is a student or postdoc of Antoine. Of Antoine. Yes. Do you think that there is an analog of this up to C1 alpha regularity? Because Schauder stops there, ends up there in that case. Case, I mean, as um, well, yeah, this depends on the I think that in general, this is like very, very hard, even if you sort of like put somehow kind of like a regularization. So, I think that actually they have like, you know, P Laplace, but you have like this s squared plus gradient u squared, and then parenthesis p minus two over two, and s is one for them, but okay, they do not consider the generic case, right? Right, but but. Right, right. But then basically, even in that case, I think that it's extremely hard to show that the homogenized matrix is basically having the same structure as this one. So you cannot actually prove, I think, I'm not sure. I have to check this one, but yeah, this is like one main obstruction that basically homogenization, it can kind of like pack the energy sort of like where the gradient is small. And yeah, so you might. And yeah, so you might lose it. So, in some sense, this is not kind of like very easy, it's a problem to homogenize the pilla plush. I'm definitely not. I mean, especially already in the periodic setting, it makes yeah, yeah, yeah, yeah, yeah. So, so I'm actually just taking a look of the paper. So, it's like 70 pages and a brief. And I would say that they are nevertheless having yet, they are not having basically the having basically the full full they don't have the resulting full generality so okay yeah so so this is it's not easy no no definitely not it is but yeah so so kind of like well so me and scott we sorry we me and me and rosario and scott we were at some point discussing about this so this was when rosario was starting to do this double phase functional so so one of the kind of like original motivation for shekov Motivation for Shikhov. It was actually to sort of consider the motivation. So, because Shikov is actually like he has a fundamental contribution to homogenization as well. So, there is a beautiful book by Oleny Koslov and Zhikov. By Zhikov, Oleniyov. Yeah, exactly, exactly. So, this is little. I mean, it's full of ideas. It's incredible. No, no, no, no, no. It's incredible. This is still the best book written in the homogenization. Yeah, it is full of ideas and it is very, very. And it is very remarkable how these guys created over years, as many as also other Russian mathematicians. I mean, they only provide ideas and no marketing at all. Right, right, right, right, right. Yeah. It's really full of ideas. And this checkboard, and this checkboard, I think you can see that even with two different hardening exponents. Yeah. It was looking much forward. It was looking much more. Yeah, yeah, yeah. But of course, it would be somehow nice that. Of course, it would be somehow nice that now there starts to be kind of like all these quantitative results and kind of like in some sense, much finer analysis than there used to be still like 20 years ago. So when it was kind of that if you if you want to go in this direction, I mean, along the lines that Gikov, I mean, for so many years ago, it's of course, of course, but still take 15 years. I mean, yeah, but in some sense, kind of like that, that. Kind of like that, that let's put this way: that for him, he has like many fantastic conjectures. And I think that what should be true, but then you know, they were just like distant dreams, basically, because we like the analysis was not ready for this. So there are like so many linear new techniques and sort of ideas that, okay, maybe you can sort of like prove some very qualitative statement about homogenization, but. Homogenization, but quantitative theory, so that you could really prove something like kind of C1 alpha, even or something like that. I mean, it will take at least other 10 or 15 years. I mean, that's. Or a cleverer person. I don't know. I mean, there are tools enough and clever persons enough, but you need time, of course. Yes, yes, yes, yes, yes. But I will kind of like just so there is still lots to do and lots kind of like. Still, lots to do, and lots kind of like problems that were some or are very natural and important, basically. So, this is kind of coming from that non-linear elasticity and so forth. And kind of like a very concrete models, actually, and which were originally recast in the context of homogenization. Yeah, yeah. Okay, are there more questions, remarks, comments? And uh, no, this is not the case, so we thank Tom again and uh. We thank Tom again. And I would like to, I mean, to point out that there is a group picture occurring in now.