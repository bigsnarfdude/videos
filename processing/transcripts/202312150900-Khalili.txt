So, I'm going to talk about sparse estimation in mixture of expert models. This is a joint work with my colleague Archer Yang and a former student, Shinon Da, who is currently employed at Statistic Canada. This is a work that is partially funded by NCERC. Okay, so I'm going to. Okay, so I'm going to directly jump to the setting of the problem. So we are basically in a supervised learning setting where we have a response variable y, univariate, and a list of covariates x1 up to xp that we think could be related to our response variable. Sorry, and we also have in regards to the random behavior of the response variable, we also have a parametric family of distributions that we know in advance what they are up to certain parameters. So, in a mixture of expert model with k components, the conditional distribution of y given x is basically modeled. And X is basically modeled using a finite mixture model. So, as you can see, the X's are all over the place. So, they are in the mixing probabilities, which are GKs here. Usually, we use pi's to represent these probabilities, but I have a reason that I use G here, so I will tell you later on. So, the GKs are probabilities adding up to one, and you can see they are a And you can see they are affected by X, and also the mixture components are affected by X also through this link function here. So the specific form of the link function we know, like if H is Gaussian, we would know it's a linear predictor. If it's exponential family, so it would have a specific link function. And we may also have dispersion parameters. Also, have dispersion parameters. So, before I tell you the formulation of the mixing probabilities as a function of x, maybe I should tell you why they call this mixture of expert. So it's first of all a terminology in machine learning communities. So the early paper on mixture of expert is written by Jacob, Nolan, Michael Jordan and Hinton in 1990. Jordan and Hinton in 1991, which was published in Neural Computation. So, the reason why they call them mixture of experts is as follows. So, they use these models in the context of so-called problem decomposition context, where you, for example, you could have a complex classification problem where you divide it into using a divide and concur principle, you divide the problem into Divide the problem into sub-problems, and then each sub-problem using these probabilities, the GKs, is going to be assigned to a problem solver that could be a classifier. And they call this classifier or these problem solvers as expert, which are basically the mixture components. Okay, so and then they aggregate the output. aggregate the output of the uh the the expert or the problem solvers using a mixture model and that's why they call it mixture of expert okay so in in their language and they use so in statistical uh literature we call h simply mixture component and the g's as mixing probabilities but in their language they call the h as expert the problem solvers and the g's are called gate And the G's are called gating network. It's basically the probabilities that are used to assign each problem to a problem solver. Okay, so that's the reason why they call the mixture of expert. And the G I'm using because it's called gating network. So that's the reason. Okay, and I'm going to use their terminology from now on. Okay. Just to be consistent with the literature. So the G chase now, so remember, we know the So remember, we know the form of H. Now, the GKs, after all, these are probabilities. So we can think of a multinomial regression framework and simply model the G case using a multinomial regression. And this is the most popular way of modeling the gating network in mixture of expert literature as a function of X. So if you do so, then the So then the vector of parameters would be beta one to beta k. So these are the regression function parameters in the expert or in the mixture components. You also have phi one to phi k potentially. You could have dispersion parameter, for example, if you have Gaussian or negative binomial as the components. And you also have alpha one up to alpha k minus one, which are the parameters in the gating. Which are the parameters in the gating network. Okay, so these papers studied identifiability and also estimation, including MLE and Bayesian methods for mixture of expert. So the dimension of the parameter space would be d, which is basically the total number of parameters that you have in this structure. And for now, I'm going to assume k is fixed. Hopefully, toward the end, I will also mention briefly. I will also mention briefly on what happens if K is misspecified or unknown. Okay, so this is how you could visualize a mixture of experts. So it's similar to a neural network. So this is a mixture of experts with four components. So given the input X, the gating network is going to compute these probabilities and then assign you to corresponding mixture component. Mixture component, and then each H is going to be used to model the conditional distribution of Y given X within that specific component of the mixture. Okay, so there are special cases of this family of these models. If you were to completely get rid of the excess from the modeling, like unsupervised setting, then you would have the standard final mixture models. And you can read about these in the recent work, in the recent book. Work in the recent book by Professor Chen. I'm also advertising his book here. So, in that case, the GKs are independent of X and the eta K's are also, they have nothing to do with covariance. If you were to include the regressors in the components, then you would have finite mixture of regressions. And there are also non-parametric and semi-parametric formulation of these models. Formulation of these models. The other day I noticed Vixing Yao was among the speakers. I wasn't able to attend his talk because of the time difference. So he actually wrote a series of nice papers on non-parametric formulation in mixture models. But in this talk, I'm going to focus on the formulation that I introduce at the beginning. Okay, so, and we are interested in a sparse MOE. MOE models. The reason is because even for moderate values of k, the number of components, and dimension p, you could easily have the number of parameters out of control. And when it comes to estimation and also interpretation, then it would be very difficult, for example, to come up with an approximation to the MLE and also interpretation. And also, interpretation would be simply difficult. Okay, so, like in our real data example that I'm going to demonstrate at the end as an application, P is not large, so it's 24, but when it comes to interpretation, it is actually large. So, and if you were to fit a model between two to five components, so you would have this many number of parameters. So, that's why we would like to impose certain. Impose certain sparsity structure in a mixture of expert model. And so, this is the sparsity that we think is reasonable and that we consider for a mixture of experts, but you could think of other kind of sparsity structure depending on the application. Okay, so for the mixture components, we don't really impose any specific Really, impose any specific structure. So, we allow each component to have its own active set or subset of covariates that to be used in that component to model the relationship between Y and X. So, there are these active sets assigned to each component. But for the gating network, for a moment, I'm just ignoring the mixture structure and I'm looking at the gating network as a multinomial regression. As a multinomial regression. And we believe that there should be, like, it seems to be natural to have some kind of grouping structure in the gating network. So in the sense that if covariate X is not significant, we prefer to take it out completely of the gating network. So each covariate XJ has this parameter, like chain. This parameter, like k minus one parameters assigned to it, right, across the k minus one probability. So I have this alpha dot j, which gives you the collection of parameters that are corresponding to xj, the j's covariate in the gating network. And what we are thinking is if this covariate is not significant in the gating network, we should get rid of it completely. So it should not. Completely. So it should not be like included in one of the probabilities, but excluded from the other probability. It would be a little bit like interpretation would not be easy. Okay, so that's the sparsity structure that we think for the gating network. So there is a grouping structure over there. So if you put these two sparsity structures together, then it gives you a sub-model or sub-model. model or sub-moe model sparse moe model and if we theoretically if we believe there is such a structure explaining our data or conditional distribution of y given x then the goal would be to recover such a structure okay so um let us see how we are going to do this okay so um and we have started likelihood function We have started likelihood functions. So, as I said, if the dimension of theta is manageable depending on the dimension of the covariate space, then MLE is the most popular way of estimation in mixture of expert. And indeed, over the period of around 1991 to around 2000, there were a series of nice papers in the Annals of Statistics and JASA. Analysis of statistics and JASA basically investigating, and a couple of papers in the machine learning community investigating identifiability and also MLE and Bayesian methods for estimation in mixture of expert. But what I'm interested here is a situation where MLE is almost out of the picture. So it's simply not easy to obtain a reliable approximation to MLE. Reliable approximation to M and if you were to maximize five the likelihood function simply because the dimension could be large. And you can see from here I'm heading to the penalized estimation. Okay, so if I recall the sparsity structure that I introduced on the previous slide, so I have j components and p is the number of covariates. So in total, The number of covariates. So, in total, you would have 2 to the power k plus 1 times p sub models of this form. Okay. And so if K and P are not that large, you may actually think of using all substance selection methods. And in my own experience, I examined I examined so this is a combinatorial problem. If the size is not too large, even a couple of thousand writing a code in C ⁇  could easily go through this pool and give you a pretty good performance if you use the information criteria. I examined BIC and also EBIC, which was proposed by Chen and Chen in their biometric paper. But so that's why some people still say all these goals. So if you can manage the computation, the information criteria still perform pretty good. But the situation is, I mean, it would be out of use, if almost out of use, if you cannot actually, if you don't have the prior knowledge of, for example, excluding so many of the members of Excluding so many of the members of this pool, then you got to do continuous regularization. So things like LASU type regularization. And that's what we are going to do here. So I should say that compared to most of the talks, my talk is a lot less mathematics, and my apologies if I disappoint you. Want you so it's more computation, and I have an interesting real data application at the end. But I will briefly talk about some theoretical challenges in between. Okay, so for the expert parameters, we are going to use individual regularization on each regression parameter within the components. But for the gating network, as I said, But for the gating network, as I said, we are going to have a group regularization. So, for each covariate xj, as I said, we have this associated vector of regression parameters. So, we either chip then the covariate in or we completely remove it from the gating network. Okay, but for the components, we give the flexibility of having each component. Of having each component has its own regression coefficients regularized. Okay. So that's basically how we are going to regularize this. So that's the penalized likelihood function that we have. So as I said, the regularizer would have individual regularization on each regression coefficient within the mixture component. But for the Component, but for the gating network, you would have a regularizer that has group structure for the coefficients that are corresponding to covariate xj. In addition, we also have a rich penalty that helps numerically because the likelihood function is such a complex surface. And if you add such a convex Such a convex, beautiful L2 regularizer, it actually helps for stability of the numerical implementation. So that's why we have this L2 norm here. And in a couple of recent papers that are coming out somehow on a sparsification of neural network, I also saw something similar to this. I mean, this is completely independent. I mean, this is completely independent from that literature. But what basically they do in that literature, they want to sparsify the vector of covariates at the first layer of a neural network or the other layers. But at least in the statistical community, that's some of the recent papers that I've seen. Okay, and in the Friedman 2010 paper, 2010 paper, in a non-mixture setting, they also talk about the advantage of adding a convex regularizer, like an L2 norm regularizer to objective function. It helps in terms of from algorithmic point of view. Okay, so suppose you are able to use a numerical algorithm and come up with an approximation to theta hat n. Okay, so this is a theoretical maximizer and Maximizer and by appropriate tuning of the lambda and lambda star and tau star, you would be able to obtain sparse theta at n. So in our theory, we basically mentioned that in terms of the balance between these penalties, especially the tau star, asymptotically the rate has the tau star has to be. The task star has to be chosen in such a way that it doesn't dominate the regularizer on the group selection, otherwise, you won't have sparsity. So that kind of attention is needed here. Okay, so in terms of statistical properties, if you look at the literature in high-dimensional literature, statistic mainly on linear models. Mainly on in linear models and GLM, including these, I would call them beautiful works in that context. The regularity conditions that are required to establish a statistical optimality of maximum penalized likelihood estimation means consistency in the estimation. And that's what I mean by statistical optimality, consistency in the estimation, and also variable selection. Basically, these conditions boys. Basically, this condition boils down to the behavior of the design matrix. And also, there is an important requirement for the loss function. Like in the work of Negapon et al. 2012, they need this restricted strong convexity, which we can obtain if, for example, we assume some kind of Gaussianity or sub-Gaussianity of the covariance, tail behavior of the covariance. So similarly, in the GL. So similarly in the GLM context, but when it comes to mixture models, these kind of conditions are not really achievable as far as I know. So therefore, we have to impose regular condition on somehow the joint distribution of Y and X together when we are investigating or establishing consistency in estimation and feature selection. So we have these resulting. So we have these resulting in our work, but I'm not going to go through those here because of time. So numerical implementation of the algorithm. We use EM with proximal gradient descent in the M step, and it turns out that we can have close form updates for the regression. Updates for the regression parameters in the mixture components and also the parameters in the gating network. And depending on the parametric family, if, for example, you have a dispersion parameter like Gaussian, then you may also need certain restriction on the likelihood so that you obtain a stable estimate for the dispersion parameters. Okay, so and the method seems to, the algorithm seems to. method seems to, the algorithm seems to perform reasonable, at least in the dimensions that we considered in our simulation. Okay, so this is a quick simulation that I'm going to show here. So we considered one of the models that we considered is a three-component Gaussian mixture expert model where the mean function The mean functions are basically the linear predictors, like in linear regression. And then we have the multinomial structure for the gating network. And in this simulation, I did not really consider very high dimension. Even these dimensions are challenging in a mixture of expert contexts when the goal is to deliver. To deliver a sparse model. So, here, because we are in a statistic community, so unlike machine learning people, we don't want to just close our eyes and fit an overfitted model that we may not be even able to interpret. So, interpretation is one of the key things that statisticians also care about. Okay, so the point of this simulation, so there are some interesting observations in the simulation that actually tells us. Tells us how difficult this problem is in the context of mixture models if you start introducing sparsity. Okay, so I'm going to show you three tables. So the first table shows the empirical mean square error of the parameter estimate. So this is over 200 replications, different sample sizes. So I'd have 200, 300, 400. Have 200, 300, 400, and different. This is the total dimension of the parameter theta. Okay, so the worst case scenario that we consider is when the dimension is almost matching the sample size. So 200, 200, 300, 300, and 400, around 358. So a smaller the number, the better. Okay. So we can, so I have two sets of parameters. I ignore the dispersion parameter in this table. The dispersion parameter in this table. So I have the parameters in the gating network and the alphas and the parameters in the estimators in the mixture component. So it's a mixed performance among these three penalties. So focusing on the gating network, it seems that Lasso had the worst performance. Had the worst performance if you go across these numbers, the first two columns, alpha one hat, alpha two hat compared to the other two penalizers or regularizers. So SCAD is performing better and then adapted last su and then last su. But when it comes to the betas, it's completely opposite, which is interesting for whatever reason. Whatever reason. So, LASO, if you put these numbers together, overall, LASO seems to perform better. In the mixture components, the LASU performs better than the other two penalties, but in the gating network, the story is completely opposite. So, that's one observation that he had in the simulation. In terms of the specificity and sensitivity, so again, I divided into Again, I divide it into alphas and betas. Overall, the performance is reasonable, but there's also this weird behavior that I cannot explain what's happening. Like LASU is performing pretty good, actually, compared to adaptive LAS2 and SCAD. Wherefore, when the sample size and the dimension are closed, we have this very bad performance. So I'm highlighting these numbers like in the third component. Numbers like in the third component of the mixture, the adaptive LASO and the SCAD, the performance is not that good. But amazingly enough, the LASO is performing actually very well. So there is this mixed performance in terms of mean square error and sparse recovery of this structure. Okay, so the problem becomes much harder when you don't know the number of components. So, in a joint work with Nat Ho from UTexas at Austin, and a former student, Pudor Manole, and my current PhD student, we're looking at the same problem, but we don't know K. So it's misspecified. And you could imagine the theory gets completely out of hand. But we've been able to obtain some very interesting results that I'm hoping to be able to present. To be able to present in a future conference. So, this table basically tells you the difficulty of the problem when you don't know the K. So, here I generated the data from a three-component model. And what you could do, then you could start fitting models with different number of components, like one component, two component, three, four, five. Depending your sample size, you keep going. And then, what every time for each given But every time for each given K, you fit a sparse model. And then, for example, use BIC or EBIC to select the best model. So the result that I have here are, so in my simulation, I have the result based on BIC, EBIC, which has a tuning parameter. I think it's called gamma in Java's paper. So it has 0, 0.5. Has 0, 0.5, and 1, I think. So the result here are for pure BIC. And EBIC, for whatever reason, the performance was a little bit like underperformance, at least for the dimension that I considered here. So you can see that when the dimension is not that high, BIC is actually doing a reasonable job in identifying the In identifying the so the higher the number, the better under the column three. Okay. So, for example, when the sample size is 300 and the dimension is 93, the performance is very well. So, they are able to, all the penalties are able, based on BIC, they are able to identify the number of components. But the moment that the sample size, the dimension increases, the performance of Increases the performance of, for example, the BIC-based and adaptive LAS2 and SCAD is just simply pool. But again, the LAS2 is performing very well, actually. Okay, so that's basically the simulation results for the estimation of the number of components. Now, I'm going to show you a real data example. So, in this example, the goal is to come up with a regression model where certain bodies. Where certain body measurements and also height is given, and you could want to predict weight. And the application of these kind of problems is in, for example, anthropology or forensic studies where a body is found and they can measure certain characteristic of the body and they want to estimate or predict what the weight of this person is. Or predict what the weight of this person was. Okay, so this paper analyzed this data using linear regression. But we thought maybe, okay, let's try a mixture of experts. And we thought we found some interesting results. So this is different measurements of a human body. So I have the list in this table. So there are the skeletal measurements and there are And there are also girth measurements, side of the shoulder and chest, and all those. And also, there is age, height, sex, and weight. So, the weight is the response variable, and the rest are covariant. So, we started fitting a Gaussian MOE models with different number of components. And because this time, the dimension and the sample size that we have, it actually matches with this scenario where. With this scenario, where BIC was performing very well. So, we think, so we use BIC and we got a model with three components. So, this is the sparse model that we got. So, once we fitted the model, then what we did, we go back to the sample and then we computed the posterior probability of each sample point belonging to any of the mixed. Belonging to any of the mixture components. So, this is the gating network. I'm going to come back to this gating network. As you can see, there are only two covariates that are affecting the gating network. So, once we compute, seeing that my time is even, but I'm almost done, I need a couple of more minutes, if you don't mind, Java. So, I compute the posterior probabilities of individuals belonging to any of the mixture components. Into any of the mixture components, and this is so we got this interesting box. So we assign the individuals and then we compute the average weights. So these are the box plot of the weights of individuals assigned to each mixture component. And there seems to be a significant difference in terms of the weights. So 66% of individuals were classified to group one, about 22% to group two. 22% to group 2 and 11% to group 3. And this is the average weight for individuals in group 1:64 kg. In group 2, significant difference, 81. And in group 3, even more. So 90.57. So there is a nice classification in the sample that we have. And interestingly enough, when I look at the main features, When I look at the male-female classification of these individuals, in the first group that has lower weight, there are 62% female and then 38% male. That makes sense, why the average weight is much lower. In the other two groups, there are 78% male and the third group, there are 85% male. And weight. So when I look at the average height of these individuals, the first group is. Of these individuals, the first group is about 168 centimeters, the second group is 179, and the third group is 177 centimeters. And I was looking at these online tables that they give like what is normal in terms of height and weight. So it's kind of, it seems to me that the individuals in group one, they somehow live a healthy life, I would say. I would say. But individuals in group two, with respect to their weight and height, they are slightly overweight. And then individuals in group three, they are, you can call them obese. So they are a little bit shorter and their weight is also higher than the other two groups. So going back to the gating network, so the two covariates that are selected in the gating network, so these are the probabilities. So these are the probabilities of the mixture. The X2 is by leg diameter or pelvic breadth diameter, and the other one is chest girth or circumstance of the chest. So it seems that larger the values of X2 and X3, there is less likely that individual belongs to group one. So this is the group that I call them living a healthy. I call them living a healthy life. Larger the values of X2, more likely individuals are in group 2, which were slightly overweight. And then higher the X11, which is the chest size somehow, more likely the person is going to be classified in group 3, which was called the obese group. And that's all. That's all I wanted. That's all. That's all I wanted to talk about. Thank you very much for your attention. And this is a list of references that I use in this talk. Thank you.