I'm going to be telling you about how you can apply tools from Smooth optimization like LBFGS to handle non-smooth convex problems without changing your optimization problem. I just want to remove the mic up with what's certainly okay, so let me begin with showing setting notation. So here we're going to be So here we're going to be thinking about solving a system x beta equals y and if you have a lot less rows and columns and it's an imposed problem right and so typically as we see many times this week you can set this as an optimization problem where you want to minimize some loss function right with your data delta term matching x beta to y and since it's ill-posed okay and so your loss function typically you might take it to be the quadratic loss as Might take it to be the quadratic loss as we saw this morning, or it could be other things such as the L1 loss, or say an indicator function on y's. In this case, you would simply simply enforce the constraint x beta equals y. And since that's an ill-posed problem, you have some regularizer R here. So, and typically, if you want to impose some kind of If you want to impose some kind of structure in your solution, then R is going to be a non-smooth recognizer. So, for example, L1 would enforce sparse solutions. As we saw this morning, nuclear norm would enforce low-rank solutions, or you may have, say, group L1 norm to enforce sparse thing groups. And the typical optimization approach to handling these problems would be proximal-based methods. Similar-based methods. As we saw several times this week, so examples would be forward-backward splitting or iterative soft thresholding, which is in the L1 case, primal do, ABMM, docker stretch fifth, and so on. And so let me quickly remind you what a proximal method is. So for forward-backet splitting or iterative thresholding, so So, here, if you specialize in case in quadratic loss and L1 regularization, then it can be written in these two steps, as we also saw this morning. First, you do a descent step with respect to the smooth part, right, this gradient of the smooth sky. And then you do a backward step by applying the prox, right? So, the prox here is defined over here, right? Do you want to stay close to beta hat while also decreasing your regularizer? Decreasing your regularizer, right? And for a lot of regularizers that we like, this has a closed form expression. So for L1, it's simply a soft thresholding operator. And as we saw this morning, for nuclear norms, it's going to be somehow thresholding of single values. And for ISTE, the kind of classical convergence rate that most people refer to is the one by Beck and Taboo, which tells you that the objective here. Which tells you that the objective here decays at rate order one over k, where k here is the iteration count. And so for optimization, this is converted of a slow convergence rate sublinear, and you can improve it to one over k squared if you do something like nest draw acceleration. But this is still very far from the like the realms of smooth optimization where you can have superlinear convergence rates if you apply quasi-neutral methods. If you apply quasi-neutral methods, and the other difficulty I'd like to mention here is that this is order one over k with a constant. And this constant here is the Lipschitz constant of the gradient of the smooth part. And so in this particular case, it's going to be norm of x transpose x. So the special norm of x transpose x. And something to bear in mind is that. To bear in mind is that quite often for inverse problems that we're interested this week, they come from the discretization of some continuous operator. So in actual fact, your inverse problem is formulated as some integral, maybe. And in practice, you would have discretized your grid with these points x i's. And then you approximate this by a matrix. And so this matrix x is going to be this function and values at your point, grid points x i's here. So these will be. Grid points exercise here. So these will be your columns. So an example is where maybe you want to recover some of these slice three spikes here from low Fourier frequencies. And so here, this here would be the Fourier operator. And in this case, Xi, maybe these guys here, if you would observe Fourier frequencies up to some cutoff FC. And if you want a very accurate kind of localization of spice, you may take n here, this is quantization, to be very large. And from a normalization point of view, this is problematic because your columns of the matrix become highly coherent with each other. And in fact, this constant here, the norm of x transpose x, is going to grow with your discretization. Right, so in the case if you were to say discretize this This torus, right, in dimension D with n n grid points, well, point space one over n a part, right? And this here is going to grow. And so, in a sense, for a very finely discretized problem, these kind of convergence rates, they become meaningless. And I'll come back to this later on. And there's been some interesting work recently. Work recently by people to try to understand these convergence rates for very finely discretized problems. And so what I want to do in this talk is to move away from this kind of oximal-based approach. And I'll show you a new way of doing non-smooth optimization. And there are two key steps to what I'm going to show you. The first is I'm going to show you how you can over-parameterize a problem. You, how you can over-parametrize a problem to turn it from a non-smooth but convex problem into a smooth and non-convex problem. So, this is going to be an equivalent way of writing your optimization problem. And once I've done this, I'm going to then put it into bi-level form, which will, as you will see, lead to much better problem conditioning. And the upshot of this is that in the end, I'm going to have a very easy to handle. Easy to handle, smooth optimization problem. And then I can open the toolbox of smooth optimization and use all the tools there. And in practice, you can just throw this function I'm going to create to LBFGS solver and let that do the hard work for you. So to explain this framework, I'm going to focus on the less so, where you have L1 regularization and quadratic loss. But the method itself can be applied to many. The method itself can be applied to many settings, including total variation, group L1, overlapping group L1, nuclear norm, and also handle non-smooth losses like the ones I showed you earlier with the L1 loss. So what I'm going to do now is introduce VORPro, which is the optimization framework. And then I'm going to tell you why it's great and then show you that it's great for some numerics. Some numerics. So, this is the main idea with the overprimetrialization, and it's somehow very classical. So, the idea is simply that the absolute value function has this quadratic variational form. You can see the absolute value of theta as the infimum over all the quadratic curves which lie above this graph. So, this is very well known in computer vision from H90s. In computer vision from the 1890s, and in the machine learning, sometimes people call this the ether trick. And so, if you take this guy here, right, and plug it into L1 here, then you have this equivalent optimization problem where eta here should be over all n plus, so they're positive, and so optimizing. So, optimizing these two are equivalent. And in fact, here you see that this is non-smooth, and this here is also convex, but it's non-smooth. And it's not smooth because of this one divided by eta i here. And in fact, this here is the basis of iteratively weighted these squares, where if you minimize over beta, you have a quadratic problem that you can handle easily. And over eta, you would have to smooth it a little bit, which will be the equivalent of doing some hubric regularization. Be equivalent of doing some hubberization. So, normally to handle this, you add epsilon one over e to i, right, and that's errors of reweighted least squares where you make a smooth approximation. But equivalently, you can also make a change of variable here. So, if I let v be square root of eta and u Of eta and u b beta divided by square root of eta, right, then the absolute value function has this equivalent form, right? It's in female over u squared plus v squared, where u times v is equal to beta. And so if I plug this in into here, right, then this here is also an equivalent way of writing that problem. And something that you can notice is nice right now is that this here is now smooth. Is that this here is now smooth? Right? I don't have this one divided by e to the i term. I don't have the absolute value. It's just quadratics. So it's smooth, but it's non-convex, and it's non-convex because of this product here. So none of this is really new at this point. And in practice, right now, you could either do alternating minimization, which is nice because if you look at Which is nice because if you look at minimization over u is a quadratic problem, so you can solve a linear system. Over v is again a quadratic problem, you solve a linear system, so you can alternate and that will converge, and it works in practice. Or you can directly do gradient descent on UNV simultaneously. You can even apply LBFGS right now to this problem over UNV. But this is just step one. So, step two of the recipe is: I'm going to put it into your The recipe is: I'm going to put it into a bi-level form. And this idea is very nice, I think, and it goes back to Golob and Pereira. I think their papers is as early as the late 1970s. And their idea is that when you're faced with an optimization problem of this form, a function of over u and v, you're minimizing both over u and v, then you can equivalent. Then you can equivalently move one of the minimizations inside. So you can equivalently see this as the minimization of this function to f, which is defined as capital F over this guy here and V, where this guy here is the minimizer to this, where V is fixed. Right, this is the same thing. I just move one of the minimums inside. And something to note. Something to note is that this function here actually has a very easy to compute derivative. So, in fact, its derivative, its gradient, is simply the partial derivative of this function f, evaluated at the minimize u and v. So, provided you can solve this inner problem, it's easy to compute gradients. And under some conditions, this is a, this here is. Some conditions, this is a this here is valid, provided this here has a unique solution, and it's very easy to see by the chain rule because if I just differentiate this guy here, right, you do partial v and then partial u and then graduate but this guy here is zero because you are a minimizer, right? So this here is very easy to derive, it's just a chain rule. And the reason why you would want to do this is due to conditioning. So you can So, you can show that the Hessian of this functionality f here is the short complement of the Hessian of this capital F. And the short complement is always no worse conditioned. And in practice, it can be substantially better conditioned. And so, generally speaking, if you're faced with a problem like this and you can solve the inner problem in somewhat an efficient way, this here would take. Efficient way, this here will typically converge much faster than handling that guide directly. Okay, so let's go back to our problem of less so and directly apply this. So back to our problem. We wrote it as this primaturization over u and v. And I'm going to move the minimization over u installed to write a finite functional to f as a minimum over u of this guy. To F as a minimum over U of the sky here. And as I said, it's easy to compute the gradient. I differentiate the thing with respect to V and plug in the minimizer for U when V is fixed. And so I differentiate with respect to V. I get this thing here. So U here is a minimizer of this guy here. If you look at the problem of U, it's a quadratic problem. So it's a solution to a linear system. To a linear system. So it's easy to compute gradients for this function. And this guy here is an n by n linear system where n is a number of columns of a matrix. And you can, just by matrix identity, rewrite this as an n by m linear system where m is the number of rows. So these are two equivalent ways of writing the gradient. Equivalent ways of writing the gradient just by just a simple matrix identity. And something we can take note right now is that, well, to compute the gradient, I have to solve a linear system. This linear system is symmetric positive definite. So you can do things like Jaleski factorization, or you can use conjugate gradient. You either have to handle an M by M linear system or M by M. Or an M by M linear system or N by N linear system, right? So which one you choose depends on which dimension is larger. And something which is kind of fun is that this guy here still makes sense if lambda is zero. And lambda here is a regularization parameter in front of my L1 guy. And so having zero regularization, but when lambda equals zero, that means you're taking L1 norm subject to x p t equals y. Subject to x p t equals y, right? So you've got two non-smooth guys. But this kind of hints, right, that even when you've got two non-smooth terms, right, linear constraint and L1, you can still rewrite as a smooth optimization problem, compute gradients, and in principle, throw this to LPFGS. So this is just to show you the kind of how the iterates might behave. Kind of how the iterates might behave as if you were to simply do gradient descent on this little function f that I just showed you. And so what I've done here, I've taken a very small problem, its dimension is 20, that's the number of variables I'm solving over. And each line corresponds to one of the coefficients, right, as I go step through the iterations. So this here is iterative self-thresholding. This here is doing radiant descent on my functional to f here. On my functionality f here. And this black line is a zero line. So here I've got one, maybe it's 10, 10 coefficients, right? So this guy here, you see that it starts off as negative with the wrong sign. You know, you go to positive and it's supposed to end up here, right? And so as with ISTA, you know that these coefficients, they move along these piecewise curves, piecewise paths. So this one, it hits zero, right? And it stays at zero for a while and then it moves off. Right. So these are. Off right, so these are only piecewise smooth tasks. And whereas, if you were to do gradient descent to the W2 function to the function W2F, right, then the behavior is quite different, and you kind of really just quite smoothly jump over the zero line. Okay, so now let me give you some insight as to why you would want to put into this smooth formulation. This smooth formulation. So, one thing that you may be thinking right now is that okay, I've taken your nice convex problem and made it non-convex. But something you can show in this case is that actually all the stationary points are either global minimums or strict saddles. There are no local minimums. And what I mean by strict saddle is that the Hessian has at least one negative eigenvalue. So you can always find a direction of descent. So, you can always find a direction of descent. And so, it's known that a lot of classical methods, like trust region methods, always converge to global minimums if they don't get stuck at strict saddles. And it's also known that gradient descent will almost always avoid strict saddles, right, with random initialization. So, this non-convexity in this sense is really benign. It doesn't really matter. The second thing is that. The second thing is that, as I said earlier, it's well known that when you put things into this bi-level formulation, you're not going to make the condition number worse, right? And the question is, how much better is it? So for the last though, so before I wrote capital F and now I switched to capital G, but this is the last with the UV formulae for materialization. So you can show that the rate. So, you can show that the ratio between the condition numbers is order lambda, where lambda is the regularization parameter. So, actually, as you move towards the more difficult case where lambda is zero, where you've got two non-smooth guys, actually, that's when the ratio improvement is greatest. And the third thing I'd like to mention is that earlier I mentioned that the Lipschitz constant of gradient of the smooth. Lipschitz constant of gradient of the smooth sky when you have a very finely discretized operator it kind of blows up as you refine your discretization and that will lead to much slower convergence behavior and actually something you can show is that if you were to discretize your x's discretization of some operator and you just impose the x's normalize columns right can be arbitrarily highly coherent and columns then actually the Lipschitz constant is in the Then actually, the Lipschitz constant is independent. So, in this case, actually, it's even just one. And just a comment is that, of course, here, under some conditions, which are sort of standard in analyzing the LASSO, you can actually show that locally you have superlinear convergence rates, if you were to apply quasi-Newton methods to minimize the function f here. And the other bit of insight I'd like to mention is that there is a kind of connection to what we're doing with this overparametrization to mirror descent. So just to recall, proper similar gradient descent or iterative soft thresholding, right, has this formulation. You take a descent step with respect to the smooth. Stephens with respect to the smooth part, and then you do this backward step, this similar step, right? And let me quickly recall what Mira descent is, right? So if you were to minimize this guy here and you say that beta is in some boundaries space, let's say L1, then this gradient is going to live in a dual space. That's L infinity here. Right, so the idea of mirror descent is that you have some mirror map. That you have some mirror map, so you have some strongly complex Swiss function eta. So, such that gradient of eta is going to map from x, your band space, to a dual space x star. So, with merit descent, you map into your dual space, you do descend here, right? This guy lives in the dual space. And then, so descent here happens in the dual space, and then of course, you could do grad of either star, right? Grading of the conjugate, convex conjugate, and that maps it back to the original space. Space. So this is meridescent with the prox. And some very interesting results for the LASS, where you have a very finely disk-sized grid was done by Lynette Gizette a couple of years ago. And oh yeah, and of course, the other thing to mention is that if Eti is a quadratic, then these two are the same. Then, so if you take the quadratic, Then, so if you take the quadratic, this is exactly this guy here. And what he shows is that convergence rate, if you really take into account this very fine discretization, is all the k to the power of minus 2 divided by d plus 2, where d is a dimension you're discretizing. So, for example, if you're discretizing operating in dimension one, the conversion rate of ister is actually not order one over k, it's order one over k to the power of two-thirds. And if you wanted this order one over k rate, what he shows is that you should take Is that you should take eta here to be the so-called hyperbolic entropy function. So, with probably chosen gamma, you can show that you have essentially all the one over k conversions up to some log term. And so, a natural question is if I were to apply gradient descent to my formulation, right, this over parametrizized formulation, what kind of convergence rate can we expect to see? And so, here's a plot to give you. So, here's a plot to give you some insight into this question. So, what I'm solving here is a lasso, so L1 with quadratic loss. And X here is the finally discretized Fourier operator. And the true solution is like three spikes. So, here on the graph, I'm showing you this here, axis, is iteration, and this here is objective error. And this here is objective error, right, in long log scale. And so, what I've got here is iterative soft thresholding. This is this kind of pinkish line here, ister. And you see that it's basically parallel to the k to the power of minus two-thirds line, this two line here. So it really exactly matches the theoretical result of Linux design. And if you were to do mirror descent with the If you were to do mirror descent with this hyperbolic entropy function, that's this red line here, right? You see, it's almost parallel to this one over k line here, that's this dashed line here. And if you were to do grain descent to this UV parameterization, so you do it simultaneously, so that's why I call it a Hademart parameterization because of the Hademard product. Then that is this blue line here again. You see, it's basically one of the K-conversions. And if I were And if I were to do gradient descent to this bi-level formulation, then again you also see one over k conversions. This I call it val pro is this blue line here is parallel to this one over k line. So in terms of conversions, right, it's very similar to mirror descent with this well-chosen hyperbolic entropy function. But I would say one gain here is that I don't have to do. Say one gain here is that I don't have to do gradient descent. I can use any smooth optimization tool I like. And there is, in fact, a kind of connection to mirror descent. And so recall that this is mirror descent, right? If you have a smooth, if you're doing a smooth optimization, what each is your is the potential related to your is your mirror map, right? So if I were to subtract and divide by tau. We subtract and divide by tau, right, and look at the continuous time behavior, then this is the associated ODE. And so, in fact, if I were to do, look at the gradient flow of this UV parametrization simultaneously, right, I've taken derivatives here with respect to u and v, right, and this u dot t and v dot t. And of course, the thing that I'm actually interested is. The thing that I'm actually interested is ut point-wise multiplication was vt, right? This is the actual variable, and then I look at this guy here, then actually this is precisely your merge-cent of the E, where this eta here is exactly the hyperbolic entropy function, but where the parameter is varying. And this parameter is actually going to zero. Actually, going to zero as time goes to infinity. And when this parameter, when the t is very, very large, right at the start, it looks like this. And as t gets smaller and smaller, it looks more like pointy. So you can sort of relate this to kind of doing mirror descent with this hyperbolic entropy function where this parameter is like changing with time. So, the takeaway that I wanted to explain to you is that implicitly, it kind of looks like it's being optimized in hyperbolic entropy, and that you're kind of doing descent with this Breton distance, right? But in terms of how you actually do the optimization, I mean, you're doing descent with Euclidean geometry. And so, from that point of view, it makes it very easy to exploit. Are very easy to exploit all the tools from smooth optimization. And in particular, something that we like to do are kind of like cheap versions of Newton's method. And this would be like Bozille-Bowen step size, quasi-Newton methods, BFGS, and so on. So this is like kind of Newton descent where you look at some cheap version, cheap approximation to the Hessian inverse. And BFGS is kind of like taking a low rank approximation in this guy here. And so, something that we find is extremely effective that I'll now show you is that in practice, it's really great if you simply take this formulation, this balloon formulation, and just throw it to an LBF just solver. So let me show you some numerics just to give you an idea of how it works. So here I've taken a couple of kind of standard regression data sets. And so this here is like prediction of income. And so this data set is highly overdetermined. So this M is the number of rows, N is the number of columns to this data matrix. To this data matrix. And here we kind of so this axis is time, computational time. The y-axis is objective value error. We tested for a lot of different regularization parameters going from recently large to much smaller. And this here is the parameter found by doing minimizing the mini-squared error by cross-validation. So it's kind of the optimal one. So, there's a lot of lines here. I won't go through them all just to point out a few. So, our line is this fluid-ashed one here that is WorldPro. We compared against Fista with the BB buzz-liboring step size and restarts. And here it is, there's a plus this guy here, right? So, here it goes here. Here is actually very fast. Restart the fister here. And here is probably the fastest method on the screen. And here is probably the fastest method on this graph. Where we compared against zero-man method one, that is a similar quasi-Newton method where you simply do an approximation to the Hessen-Smith R within the prox. So that's by Joel Fedilli and Stephen Becker. And so there's this black line with circles. And again, this is very efficient. And another very competitive method would be coordinate descent, where you do support pruning. Where you do support pruning. So that's cellular here, and I would say it's one of the state-of-the-art methods. So here you notice that in this case, it's very fast for large regularization parameters, right? It's down here. But for smaller regularization, it's like this one is a bit slower. And this here is a different matrix, so it's for some kind of classification of cancers. For some kind of classification of cancer, so it's a so-called leukemia data set and is a much very underdetermined problem. And again, our guy here is this blue dashed one here, right, is very competitive. And now, seller, this coordinate descent method with support pruning is actually the fastest method, right? This in this case with restarts is slower, is this guy here. Lower is this guy here. Here for here is reasonably quick for large regularization parameters, and then it's a bit slower for small regularization parameters. And this is zero members of one, which is the approximate quasi-Newton method by this blue dot, black line with circles. So, what I want you to notice here is that our method is actually very, the performance is very consistent across different regularization strengths. And I would say that this. And I would say that this here is kind of due to the improved conditioning that I mentioned earlier with this var pro formulation. And the fact that I think this is one of the few methods where it still works as you let the regularization parameter go to zero. Because typically for optimization, as you have the regularization parameter go to zero, you converge to two non-smooth guys. And at that point, you would normally have to switch to a different method. Okay, so the other comment that I wanted to make now is that you can actually do this for a lot of different regularization functionals that are very popular. So for example, you can also use, say, group L1 for using group sparse T. Group sparsity. So it's about kind of taking including normal of groups. It also works for total variation regularization. So there's L1 components with a gradient operator, which is very popular in imaging. You can do this for nuclear norm with low-rank matrices. You can also do it for overlapping group lassoes, I think. And so the key thing that I showed you earlier was this quadratic variational form, right? Quadratic variational form, right? And this also is this for all the regularizers that I've just mentioned. So for group L1, the quadratic variational form looks like this, where U of G i is a vector restricted to the group Gi and this here is a scalar. So this is scalar multiplied by a vector. LQ norm for Q between 0 and 2 also has a quadratic variational form. That looks like this. That looks like this. The nuclear norm is very well known that you have this quadratic relational form, where this is a matrix, right, equals matrix times matrix, u times v, you find this. And in fact, this is again going back to very classical computer science literature, that these two are equivalent. You can always write in this kind of quadratic variational way. So this here is conjugate gradient. No, no, no, this is hi. This is the convex conjugate, this notation. Conjugate this notation. You can always have this kind of projective variational form provided this regularizer, right? Mapping Rn to zero infinity is of this form. It's a concave function applied to beta times beta. And you can also write a similar thing for the nuclear norm. So the nuclear norm is the trace of X transpose or M transpose X, no trace of square root of M transpose X. And so, in general, if your regularizer has a quadratic variation of form, right, and we can actually even consider it composed with some linear operating t here. And now here's a loss function as before. Then, in this case, I can exactly plug this into here and then do. Do some Lagrange multiplier stuff and rewrite it in this equivalent way. So you can rewrite again as a bi-level problem with minimization over some of the function f, which is defined to be this guy here, h of v plus a maximization problem. And again, you can just simply kind of wave your hands and say, let's differentiate this guy here, right? You differentiate with respect to v, let's hope that h is differentiable. And v only if he is here. V only appears here. So formally, and formally, its gradient is simply the gradient of h, v minus v times the maximum I say to this guy here, alpha squared. And you can show that if L is differentiable with Lipschitz gradient, then this function to f is always differentiable. Its gradient is always well defined. If L here is indicating L here is the indicator function, so this here is subject to x p t equals y. Then you can show that f is differentiable at all points v such that it's differentiable if there is this, so the constraint is feasible, there's this beta such that it's because of what beta is equals y. And you're at a point which contains the true support. So you should not initialize with all zeros, for example. In fact, you see that this here would not make sense if you initialize with all zeros. You mean here? So it's differential at a point V, if there exists a point beta such that X equals Y, so this problem here should be feasible. The constraint should be feasible. And you're at the point where... If you're at the point where your support contains the true support. Okay, so here this is just an example of total variation denoising. So in this case, if you were to write down what I just showed you and compute alpha k here, you're going to be having solved this essentially weighted person problem, which in the one-time. Which in the one-dimensional case has fast solver, so it's actually linear time to solve this. And then you, this is the gradient, right? And in fact, for this particular formulation, and I think this is this morning, Tom's comment this morning about differentiating the Mero envelope, is that if you were to dualize this guy here, then this here is the dual of this complex problem and. And you see, here is a smooth performance respect alpha with constraints. So you can do projected range descent, or you can apply of VFGS with bound constraints. That's really specific to the case where this is identity. And so this is kind of like competitive, right? And in fact, the alpha here, they're both real variables and they correspond with the same thing if we want to relate it back to the true function. But I think it's due to improved conditioning. So, both iterates, both these guys here, the compile XT is the same, right, to compute each iterate. But you see that if you were to do grain descent with var Pro here, right, you see your basic conversions after 30 iterations. And this is after 9,000 iterations with PCG. I think I'm running out of time, so I'll show you one more example and then I'll stop. So, the last example I'll show is just for Riplasto. So, this here is a MEGEEG example. And so, here what you want to do is you want to locate sources in the brain. And you have time series measurements, right, at each sensor. Right, at each sensor, and you assume the sources are stationary over time. You say sources don't move, and so you have a current, you're trying to group sparsity structure. And so, for this data set that we talk from a paper from the group of Azure Gromford, you have this many source locations, right? And for each source, you have You have 181 time points. So at each sensor, you have 181 time points, and this is the number of sensors. And so this is just again, time or objective value error. And for different regularization strengths, going from large to small method is this blue dashed line here. Seller is a coordinate descent method with sport pruning. That is this orange line. You say attached. Orange line uses. It tends to be quite slow when your regularization parameter is very small. A fist with three stars here is this blue thing here. Okay, so yeah, you can also do this for computing optimal transport distances. Well, let me just stop and take questions. And then I just draw lots of examples, but you can see our paper put in numerics. For the numerics. So, what I showed you is that in practice, you can trade non-smoothness and convexity for a smooth but non-convex problem. And the upshot is that you have a very simple algorithm that you can simply throw to use very standard optimization tools like LPFGS. And it can actually handle a wide range of settings, including things like non-smooth losses, like L1 loss. And why you might want to do this is first. And why you might want to do this is first, when you put things in a bi-level form, you recondition the problem. So, in practice, this reconditioning can be quite significant. It's non-convex, but it's essentially a bit nine in that all settle points are strict. And we might show their kind of relationship to rural descent. Yeah, that's it. Thanks for listening. Any questions, please? So I haven't fully understood this hyperbolic versus Euclidean comment that you made. I understand it's like you have U times V equals beta or something like this. So maybe it's connected to that. Can you elaborate a little bit more? And why you're saying that it's better and it's fine-grid setting? What do you mean by that? thing what do you mean by that do you mean like this constant cn doesn't grow yes okay and and why hyperbolic and i mean the hyperbolic geometry and and euclidean geometry play a role so if you were another way to write on this optimization is that you write as a minimum right and you have this quadratic of x minus xk squared whereas i think you saw in the talk of tim you could replace this quadratic norm with a write me distance With a brightening distance, right? If you want to do descent, with mirror descent. And so that's what I mean. You're minimizing a different geometry. So normally with just gradient, if you wanna, in the case where eta is a quadratic, is this what you're doing descent with respect to is the Euclidean norm. And for mirror descent, you're taking descent with respect to the Ragnar distance. So that's what. So that's what I meant. And I would say, indeed, there's something unsatisfying. What I showed you is that I just showed the graph and say, oh, it has the same behavior as mirror descent, right? It's all the one okay, but that's not something that we proved. But it is, I will say, is what is nice is that if you were to look at the Lipschitz constant of this with respect to the Euclidean norm, then norm then that is basically one when your the columns of the matrix are normalized whereas this leftist constant is growing in general if you were to do if you were to consider the original function do you have a conceptual reason why this happens that in the original non-smooth formulation you have this dependence on the x transpose x or whatever then when you discretize fine in a fine way then it explodes and in this case it Way then it explodes, and in this case, it doesn't. Conceptually, you're in the wrong space, right? Conceptually, if you were to do, it doesn't make sense to do descent on this with Euclidean norm, because beta here is living in some banded space, living in L1. Your bradian is in a different space, is in L infinity, right? And so, conceptually, you should not be doing descent if you're really living the Banach space with. With in Euclidean norm, you should be taking a different distance. Thank you. And I think that is why mirror descend is very effective. Yeah, and when you have a very coarse grid, it doesn't really matter. When you have a really fine grid, the geometry matters. Thank you. Super interesting presentation. Thanks so much. I'm sure the organizers planned for this to us talking the same day. It was really relevant. Could you go? Could you go to slide the general one, page 59? Obviously, I'm interested in the overlap, the group lasso case where it's overlapping. So 59, where you kind of have your general formulation, I think. So if I understand here, so this is, let me see here. So in the earlier slides, you ended up moving the kind of Hadamard product inside the data. Product inside the data fit term, right? And I'm not, is that possible when you have overlapping groups? It is. So, in fact, a D here, you're going to take as an operator, which will be your overlapping group. Here, in your case, is going to be just quadraticals. Right? Yeah. So, this here, just here inside, is going to be a quadratic problem with linear constraints. Yeah. So, in our paper, I actually wrote down the formulation in the appendix for L1 plus nuclear norm plus quadratic clause. I didn't use this overlapping groups for nuclear norm, but I wrote it for new for groups for group L, overlapping group L1. Actually, I included an example for overlapping group LASOF, just for you. You added that. Okay, thank you. A little comparison with ADMM with this kind of standard cancer data set we'll talk about. Okay, and if you go back to slide 41, please, I think there's multiple K's. Let's see. So, you know, I normally think of K as iteration, but K is not iteration here, right? This here is iteration. This here is just a vector. I mean, this is your column thing. The your column of xi. I should have written sh8 here, but everything else is k iteration. So then I'm confused why ist is converging at k to the negative two-thirds instead of one over k, which is the usual rate of ist. Because the one over k rate doesn't make sense anymore, right? Because it is constant divided by k, but I'm letting this here is a dimension, is a discretization independent result of Linux. And so he looked at everything actually in decontuous setting. Everything actually in the continuous setting in balance space, they will hold as you let infinity. But the point is that these one over k rates don't hold when you're verifying the specializations. How are we doing on time? Can I ask more? All right, if I may, slide 62 next. So I'm curious whether you've done this bottom row. Maybe it's in slides that follow and I missed it, but this is proximal grade. Following, I missed it, but this is proximal gradient scent, which is slow. So, what about FISTA here? Like, how does VARPRO compare to FISTA for, say, this problem right here? Uh, for this problem, I didn't do it, but I have some convergence plots against FISTA for all my other examples. Yeah, okay, okay. Um, and then last, I'll say this is the last one. So, slide 20, if you don't mind. Sorry for the jumping around, but So, the middle equation on the right. So, it's you said, I think you said it's convex. I can't do this in my head. Is it jointly convex in beta and eta? It is, really, even with that ratio in there. Okay, that's uh, I'll have to do that out later on the plane. This is the standard formulation of iterative relatively swiss. I know that, but I didn't realize that was jointly convex in the two variables. Okay, and is your In the two variables, okay. And is your math programming paper that came out? Is that the best place to read about all this? The one that's uh, yeah, the neurot's paper is probably shorter. Um, but the I think the math paper, there I talk about um overlapping as well. But I'm happy to if you I'm happy to quote it for you. Yeah, okay, thank you.