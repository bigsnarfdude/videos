It's my great pleasure to introduce Jim Demmel, Professor of Mathematics and Computer Science, and also the Dr. Richard Carl Demel Distinguished Professor at Berkeley. Jim has won numerous awards and fellowships that are just too many to mention. I just mentioned a few of them. He's an AMS fellow, a SIAM fellow, an IEEE fellow. He was an invited speaker at the International Congress of Mathematicians and also at ITSIAM. Also at ITSEM. He works on all topics related to numerical linear algebra, including hardware and software aspects, algorithms, and analysis. And he's also a major force behind important projects like LATEC, eigenvalue problem templates and BLAS. And now he will talk to us or give a tutorial talk on numerical stability. Okay, please, Jim, go ahead. Hello. Can you all hear me? Hello, can you all hear me? Yes. So it's a pleasure and an honor to be here. And it's great to see so many of my co-authors in the audience so that I can defer difficult questions to them. So there's easily 70 years of work on this field. And so it was an interesting exercise to cherry pick my favorite results from the last 70 years. So not all topics will be covered. So let me give you an outline, a two-page slide outline of what I would like. Page slide outline of what I would like to talk about. The basic definitions. Now, Peter mentioned these already, so some of them should be familiar. So forward, backward, and then what we can use norms, we can use component-wise, we can use them structured. Peter mentioned all these, and I'll go into more details later. And they can be deterministic versus randomized. So there's also the question. So, there's also the question of how to model arithmetic. And the classic model is, of course, every operation gets multiplied by one plus delta, where delta is a small error. We can modify that to include the possibility of underflow. We can modify that to include black box operations. So, for example, the IEEE standard has an operation called fuse to multiply add, which computes A times B plus C with one round-off error. And so With one round-off error. And so that sort of operation goes in there. Now, that is all sort of a mathematical model without taking into account the underlying bit structure. Then there's floating point itself, which has all sorts of additional interesting properties that we can use to make algorithms more accurate. And so I'll talk a little bit about the rounding and the precisions. And there's a new IEEE floating point standard on the way. So then, once I've done all those basics, I'll talk about dot products and matrix multiply and what we can say about them in terms of stability. Then, Gaussian elimination and lots of variations. Then, this was mentioned already, algorithms that are based on orthogonal transformations. It turns out there's one kind of unifying analysis for all of them to show they're stable. And then, for the particular case of the symmetric eigenvalue problem, there's some. Of the symmetric eigenvalue problem, there's some extra properties of floating-point arithmetic that we need to take advantage of in order to make these algorithms accurate. So, I'll mention those. So, then I will move into fast arithmetic, fast linear algebra, which means Strassen-like, order n to the omega, where omega can be whatever the latest world record is. And so I'll talk about the stability of fast matrix multiply, and then the stability of fast everything else, all the rest of linear algebra. rest of linear algebra. And that's going to be based on a different notion of stability called logarithmic stability, which I'll define later. And then exploiting the problem structure, which Peter also mentioned, if your problem is symmetric or sparse or who knows what, the number of problem structures is limited only by the imaginations of the mathematicians and scientists and engineers who model various physical or mathematical systems. There's all sorts of structures that we can take advantage of. All sorts of structures that we can take advantage of and get much more accurate answers. So, here are the basic definitions just for scalar functions. I want to compute y equals f of x, but all I have is an algorithm that takes x, computes f hat, and gives me y hat. So, forward stability is going to be a bound on the difference between the true answer and the approximate answer, y minus y hat. The backward stability is going to be a bound on. Is going to be a bound on x minus x hat in the case where I can show that the output is the exact function of some perturbed input. Mixed stability is a bound on both x minus x hat and y minus y hat in the case where what I get from my algorithm is the true result on a slightly perturbed input to give me a slightly perturbed output. And this is mixed ability is a good thing because if both errors are Because if both errors are small, what I can claim is I've gotten almost the right answer, which is y hat instead of y, to almost the right problem, which is x hat instead of x. And that is usually considered success in the numerical linear algebra world. So what are our error metrics? Well, there's absolute error, that's natural, where eta is my upper bound. There's relative error, which, you know, and we've heard about this before, where I'm going to bound by epsilon. This before, where I'm going to bound by epsilon the relative output error, and then there's mixed where I combine the two, and that is, for example, used to handle underflow because in floating point numbers get too small and they flush to zero at some point. And so you can combine those two to handle that particular case. And as Peter went on in detail about, of course, once we have bounds on, how do we get bounds on epsilon and eta, we'll multiply the bound on the input error. The bound on the input error, x minus x hat, by the condition number to get a bound on the output error. So here, so now let me go on to vector and matrix functions. So I'm going to use x hat and write that as x plus delta x and a hat as a plus delta a and so forth. And so here's the first example of something that I don't think Peter mentioned. So how do I measure the backward error when I'm trying to solve something? When I'm trying to solve something, so I could use either the norm of delta A divided by the norm of A, or I could use the component-wise error. So, this is MATLAB notation. So, this is the component-wise quotient of delta Aij divided by Aij, and take the maximum entry of all of those. So, both kinds of those backward errors are of interest when you're solving AX equal b. And LAPAC has long had a routine there that measures both of those errors and returns them to you, and also does iterative refinements. And also does iterative refinement to try to make them both small. Now, it turns out that if you actually manage to solve Ax equals B with this small backward error component-wise, then the condition number changes. And the condition number is no longer the norm of A inverse times norm of A. It's the norm of this matrix, which is the component-wise absolute value of A inverse multiplied by the matrix component-wise absolute value of A. And so you could ask, does this new condition number have any interesting? This new condition number has any interesting properties that we're used to having? And the answer is yes. It turns out it's related to the distance to singularity. But now the distance to singularity is component-wise. So the question is, what's the smallest component-wise relative change you can make to a matrix to make it singular? And it's roughly one over this new condition number. And that actually, so we can also imagine doing a component-wise relative error. component-wise relative error bound, but not where the change in each entry is proportional to the entry. It could be proportional to anything. So I could have a general matrix E and ask that the change to Aij is bounded by some multiple of Eij. And so we could ask, what is the distance to singularity when I'm allowing myself to have a multiple of this matrix as the metric of distance. And that has been Of distance. And that has been analyzed. And it turns out computing that distance is NP-hard. So that, and by the way, I have at the end of this talk, in order of presentation, all of these references that you can look up. So I'm not going to do the proofs. I'm just sort of telling you what results are out there. So the next possibility is to have structured. So this is just, again, just my introduction. So if your matrix is symmetric or bidiagonal. Is symmetric or bidiagonal or van der Monde or totally positive, and you can imagine an arbitrary number of different structures, then so should be the perturbed matrix, A hat, that's your backward error. And it turns out that when we do this, the condition numbers can be arbitrarily smaller in some cases. And so if we can come up with an algorithm that respects the structure, we can sometimes get much more accurate answers. And one particular example that's been in LA PAC for many, many years. That's been in LA PAC for many, many years, is the bidiagonal singular value decomposition. So, bidiagonal, it's only non-zero on the main diagonal and right above it. And it turns out that in that case, there's a modified algorithm, which is actually faster than the standard algorithm, which guarantees high relative accuracy in all the singular values, no matter how small they are. And that's been there for a long time. And then finally, the other metric is randomized versus. Is randomized versus deterministic. And in this case, and as Peter mentioned, the guarantees are more in the flavor of the Johnson-Lindsenstraus lemma. What we like to prove is that with probability at least one minus delta, where delta is some small parameter, the error is at most epsilon, where epsilon is some other small parameter. And we're in the process of trying to put this into a new version of LAPAC called RAND LAPAC that Michael Mahoney and many other people are collaborating on. And many other people are collaborating on. And we just released a 195-page design document a couple of weeks ago that you can find here on archive, which is describing all of the algorithms from the literature, you know, and a few new ones that are trying to attain these things and how we adjust the deltas and the epsilons. Okay, so let me go back to the beginning and talk about how to model arithmetic and talk about what's well known and what things are in change as we speak. As we speak. So, what's the traditional model? I want to do a binary operation, A op B, and round it. And what do I get? I get the true answer times one plus delta, where delta is less than epsilon, where we've traditionally thought of epsilon as being much, much less than one. Now, the trouble with that assumption is that there's a new IEEE floating point standard in progress with only three or four bits of precision. That means epsilon is either going to be one eighth. Epsilon is either going to be 1/8th or 1/16th. And why are people doing this? And a large number of companies are participating in this standards committee. They all want this standard really fast because it's for machine learning. Guess what? And so it turns out you can do a lot of the machine learning applications with this low precision. And so they all are in the process of building accelerators in order to make things like matrix multiply run at this kind of precision. At this kind of precision. And this is likely to be the fastest thing that computers do in the near future. Now, they realize that sometimes you need a little bit more, you know, your operations have to run in slightly higher precision than that. So they're probably going to support mixed precision dot products where they use a whopping 16 to do the accumulation. And so that means epsilon might be as low as 1 over 256 or 2048. Anyway, these numbers are not much, much smaller than 1 anymore. In fact, in Anymore. In fact, NVIDIA has successfully tried zero Mantissa bits, no fraction bits whatsoever. And their format is all exponent bits and assigned bit, and all the numbers are powers of root two. And they've also successfully done a lot of machine learning there. And the committee is meeting, as I said, bi-weekly. You know, we're trying to make progress quickly. But the reason I mention this is that going forward, you know, it may be that the fastest operations computers perform. Operations computers perform are these by orders of magnitude because that's what they're putting all of their accelerators into. So we might want to start thinking about what to do when the precision is this low. But I'm not going to worry about that. I just wanted to give you a heads up for this talk. So the other thing with the traditional model is you can add underflow to it. That's certainly going to be an issue in this new standard as well. In that case, the output is, you know, the true result, A op B, times 1 plus W. A op B times one plus delta plus a term eta where eta is less than an underflow threshold. And we've already analyzed that a long time ago. You can extend classical error analysis for Gaussian elimination and QR and all those sorts of things to include underflow. And there's an old paper on that. So, and finally, the traditional model, I've been assuming this A op B is a real single floating point operation, but people Floating point operation, but people, of course, do complex arithmetic. That makes the epsilon a little bit larger, but it actually gets a little more complicated than that. In fact, if you look at the C standard document, it has an appendix which explains how to do complex multiplication, and their code is 30 lines long, 30 lines to multiply two complex numbers. So, why do they tell you that this is the code you need to use to be compliant with the C standard? It's so that it can propagate. It's so that it can propagate infinities and NANDs correctly. Because their rule is if you multiply two infinite complex numbers, you should get an infinite complex number. But if you just use the standard approach, you'll get a NAND and not a number symbol. So that's why all that's in there. So, you know, the real world is complicated. Okay, so the other thing that we can do to modify the traditional model to take things into, you know, to take reality into account is to add certain Into account is to add certain black boxes. So, where the operation is not just your plus minus times and divide anymore, it can include other operations like this one, fused, multiply, add, which takes three operands and computes A times B plus C with one round off error. And this is part of the IEEE standard. It's implemented in hardware on most platforms. But many others are possible because all these companies, as I mentioned, are very busy building high-performance accelerators. High-performance accelerators, for example, for matrix multiply, and at various precisions, where they guarantee certain kinds of mixed precision results for these accelerated operations. And so one thing that we could think about, what could we do if we actually, if one of these accelerators had a highly accurate dot product, how would that change what we do? So all of this is the traditional model. And then there's floating point itself, which Floating point itself, which of course is, you know, now we're getting into bit complexity if we want to pay attention to that. So there's a sign bit, a mantissa, you know, a fixed point number, and an exponent. And then there's rules about rounding. And of course, you know, we, you know, traditionally, you know, we've used this traditional model up here with deltas and epsilons to make it work. And, you know, before IEEE 754 long ago, you know, the traditional model did not apply. Fortunately, those machines are no longer built. Are no longer built. And it turns out that if you use conventional rounding, which is to say, how do you choose a floating point number to round, you may round to nearest, then there are many tricks to extend the precision. And I'll give you examples later, just so you know what the history is there. So we can actually make the delta much, much smaller. But it turns out that this new 8-bit standard that people are working on is going to add support for stochastic rounding. Support for stochastic rounding, so randomized rounding. And why are they doing that? It's because it's going to reduce some error bounds, for example, for computing a dot product, from being proportional to n times epsilon to square root of n times epsilon, just by the central limit theorem, sort of a straightforward analysis. And there's a very nice survey on stochastic rounding by Croce et al. And there's the detailed reference at the end of this talk. So, I just want to give you a short look at some very, you know, part of a long history of some floating point tricks for higher precision. So, that, you know, use, you know, floating point numbers that you can, you know, do arbitrary high precision arithmetic if you want to. So the first one is called toSum, and it fits on one line. So suppose you want to add two numbers, x and y, and you've sorted them. So x is smaller in absolute value. Then you add absolute value. Then you add them. That gives you one result. I'm going to call the head for a reason I'll explain. And then you subtract x from head, round that, and subtract that from y, and you get a tail. And it turns out you can prove that head plus tail is exactly equal to x plus y. You've lost no bits whatsoever. Head are the leading bits of the sum, and tail is the trailing bits of the sum, which you rounded off. And you can recapture that by just doing these, you know. Capture that by just doing these two extra floating point operations. And so that lets you, in principle, do double precision and quadruple and whatever you want. And there's also doing products with two operations, and that uses this new fuse multiply add. Well, it's not new anymore, but it's, you know, it's been in the IEEE standard for a while. You compute the head, which is just A times B. So those are the leading bits of the product. And you compute the tail by computing. And you compute the tail by computing a times b minus the head exactly, because that's what fuse multiply add does. And there's a very long history of extensions to compute in higher precision. And I give a few of the many references again at the end of this talk. Okay, so let me just do the details for one algorithm about how this stuff applies, which is computing sums. So I want to compute the sum from i equals 1 to n of x sub i. And what is the error bound for conventional? And what is the error bound for conventional sequential summation? You know, I set my accumulator, which is s hat, a floating point number, to x1, and then I just keep adding the next one in, you know, add x2, add x3. I round after each one, so I get this one plus delta factor every time I do an addition. And it's straightforward to show that if I write that out, then I'm going to get the true sum of each xi multiplied by a whole bunch of different one plus delta factors. So in order to get an error bound, So, in order to get an error bound, I just need to bound how far away from 1 can be this product of 1 plus deltas. And the classical bound that people use, assuming again, n times epsilon is very small, which is no longer the case in the 8-bit world, then if I multiply n of these 1 plus delta factors together, and I could multiply their reciprocals too, then it's bounded above by 1 plus n epsilon over something small, over something hopefully close to 1, and bounded below by 1. And bounded below by 1 minus n epsilon over something hopefully close to 1. And, you know, of course, you know, n epsilon is no longer guaranteed to be less than 1 in the new world, but this is the traditional air bound. And so I can claim that my computed sum is the exact sum of xi times 1 plus delta i hat, where each delta i hat is order n epsilon. So there's backward stability. And I can also now take the difference between the true sum and the computed sum, and it And the computed sum, and it is the sum of all of these x i delta i's, and in the worst case, absolute value, use the triangle inequality. And so it's order n epsilon times the sum of the absolute values. And so we can claim that we're forward stable in that sense. So if I try to get a bound for the condition number, so I'm going to take this quantity and divide it by the absolute value of the true sum, then it's going to be the sum of the absolute values of the sum n. Sum of the absolute values of the sum n's divided by the absolute value of the true sum, which are obviously can be very, very large if I get a lot of cancellation, if the sum cancels to something much smaller. So that is the standard error analysis, which is the basis of all the rest of numerical linear algebra, all the algorithms that we run, because if you're doing Gaussian elimination or QR or eigenvalue problems, this is sort of the inner loop. And so And so this, so how can we improve on this? So, this is all n times epsilon. So, as I mentioned, what happens if we now instead do randomized rounding? So, what does randomized rounding actually do? You're supposed to round up or down with some probability. How do you choose that probability? It's proportional to the distance to the other choice. So, it's pretty natural if you're, if you, you know, if the true answer lies between two floating point numbers and you're very close to the And you're very close to the upper bound, then the chances of rounding to the upper bound are very good, and it's proportional to the distance to the lower bound. So the probability changes as a linear function of where you are in the interval between two floating point numbers. And so what the committee now is arguing about at our last meeting yesterday was how do we actually compute this probability? How accurately do we need to compute it? But that's what it's going to do. And so that, how does that change the error bound? How does that change the error bound? And it's just the central limit theorem that n epsilon changes to root n epsilon with high probability. And so that makes a big difference if n is very large and epsilon is not too small. So what if we do parallel summation with a binary tree? So we're adding x1 plus x2, and then separately x3 plus x4, and then we have a reduction tree that n changes to log n, which a lot of people like. And then if you use that trick. And then, if you use that trick called two sum that I mentioned and use it appropriately, Khan did this in 1965, I think, then order n turns into two. But the number of floating point operations, so in other words, n epsilon turns into two epsilon, but that costs you four n flops instead. So, this is just a little history of what happens, you know, if you care about computing sums accurately. So, there are two other things we could ask. Two other things we could ask to compute sums. One is to actually guarantee a small relative error, that you want all the leading bits of S hat to agree with the leading bits of S, despite whatever cancellation occurs. So one approach that has been around for a very long time is to use a very, very large accumulator. It's called a super accumulator in the literature. And so basically, how big does it have to be? You know, it's two to the number of exponent bits. It has to be exponentially large. Be exponentially large in the size of your input. And in fact, if you take the largest number possible and subtract the smallest number, you're going to get a carry that propagates the entire length of that accumulator. And so the cost is really going to be exponential in the input size. Well, it turns out there's a very simple approach, which is polynomial time and much faster. All you have to do is take all your floating point inputs and sort them in order of decreasing exponent or decreasing magnitude, if you like. Or decreasing magnitude, if you like, and add them in that order. Add the largest to the second largest, you know, to the third largest and so forth. And if you do that, so you sum them in the order of decreasing exponent and you use k extra fraction bits, then as long as the number of sum and is less than about 2 to the k, your relative error is less than 1.5 times machine precision. So no matter how big the cancellation is, you could cancel all the way to exactly zero, you will get exactly zero. You will get exactly zero as long as your number of sum and is bounded by two to the number of extra mantis a bits. So that's been used in practice sometimes. The other thing that some people care about is guaranteeing bitwise reproducibility for any summation order. And so modern systems are non-deterministic, so summation orders can vary. And I can tell lots of interesting stories about legal and political reasons people care about this. So there's So there's a, it turns out it's possible to do to implement reproducible summation. And the cost, and so in other words, no matter what the order of summation is, whatever your reduction tree is, you can add the numbers in any order you like. You're guaranteed to get the same answer at the end. It costs you nine n flops instead of n, and three n bitwise operations, and you need a six-word accumulator as opposed to a single word accumulator. As opposed to a single-word accumulator. And we added a new instruction in the last IEEE standard, which, as of 2019, reduces this quite a bit by doing a special rounding operation. But again, it's a linear time thing to guarantee reproducibility. Okay, so now we have computing dot products, and all those prior approaches apply, but I'm not going to go into too much detail. So if I want to compute the dot product of So, if I want to compute the dot product of two vectors, I get the exact result of the sum of the exact products, each one times one plus delta, that means it's backward stable. And the forward error bound is bounded by a sum of xi, of the absolute values of xi times yi, and that means it's forward stable. So now, how does this apply to the rest of linear algebra? Suppose I want to multiply two matrices, then I can take the true product A times B subtract. The true product, A times B, subtract the computed product. And the component-wise, so this is a component-wise absolute value. It's bounded by order n epsilon times a component-wise absolute value of A, that matrix, times that matrix. So that means it's forward stable. And if I take the norms of both sides, I get that the norm of the difference is bound by the norm of A times the norm of B times some polynomial in N times epsilon. Now, this is not backward stable in general because I have. Backwards stable in general, because I would have n cubed constraints on n squared data, so n squared deltas, delta ijs, and I can't do that. But if one of my matrices is orthogonal, so let's suppose A is orthogonal, then how do I push all the backward error into one matrix? I can write C hat, my computed output, is C times C plus delta C, factor out an A, and so I get an A transpose there times delta C. Transpose there times delta C. And so I can say that my backward error is of the same size as the error in C. And so this means I have the exact matrix product of A and a slightly perturbed B. And so that means that multiplication by an orthogonal matrix is backward stable. And all the algorithms based on orthogonal transformations like QR, eigenvalue problems, ESVD, are all normalized backward stable. And so that's all the analysis you have to do to do most of numerical linear algebra. Is you do most of numerical linear algebra that depends on orthogonal transformations. So I mentioned that there are a couple of other facts about symmetric tri-diagonal eigensolvers that use other properties of, you know, that are a little bit more subtle. So suppose, so a lot of the algorithms take your dense symmetric matrix and reduce it to tridiagonal form. I'll call that T. And so what is a cheap way to compute a few eigenvalues of a symmetric tridional matrix? You run bisection. That means I'm going to take T. That means I'm going to take t minus a shift and compute its do Gaussian elimination with no pivoting, compute LDL transpose. And the d's, their signs, tell you how many positive, zero, and negative eigenvalues there are of t minus delta. That's called the inertia. And that tells you how many eigenvalues of t are bigger than sigma, equal to sigma, or less than sigma. So by moving sigma around, I can obviously do, you know, divide and conquer and, you know, and bind. Divide and conquer, and bisection and get a small interval containing a true eigenvalue that I like. But for that algorithm to be correct, these counts have to be monotonic functions of sigma. If I decrease sigma, then I can only have the inertia, the number of positive eigenvalues increase. And it turns out that's not true in all floating point arithmetics unless the floating point itself is monotonic. And the theorem says these counts, you know, positive. These counts, you know, positive zero and negative, you know, inertias, are monotonic if floating point is monotonic. And so, fortunately, IEEE is, because it's rounding correctly, is monotonic. The other example here is the MR cubed algorithm. MR cubed stands for multiple relatively robust representations. And this is work that goes back to Dylan and Parlette. And the goal of this work was to achieve what Barrister would like to call the Holy Grail. Like to call the Holy Grail. You want to compute m eigenpairs of this n by n matrix. You obviously can't do it any faster than linear time m times n, that's the size of the output. And so what does stability mean? You want the residuals to be small. So tv equals lambda v up to order epsilon. And you want the eigenvectors to be orthogonal. And if you run the simple, obvious algorithm, which is bisection plus inverse iteration, that can fail because there are no guarantees on orthogonality. Guarantees on orthogonality. In fact, if two lambdas are so close they round to the same floating point number, you're going to get the same eigenvector for two different eigenvalues. And so they worked on that and they came up with a fix that almost always works. It's in LAPAC, but there's still some random failures to be fixed. So I would mention that as an important open problem to actually attain this holy grail of a linear time algorithm. So let me move on to matrix factorizations and what is Matrix factorizations and what is known about this. So I'm going to take my matrix and maybe permute it or multiply it by different matrices in the side and compute an LU factorization. And if you use the basic fact that I pointed out about dot products, you can basically get the same bound for the backward error in Gaussian elimination. A hat, the computed result, well, the computed result is A hat plus delta A hat, and that's equal to the output. And that's equal to the output, L times U. And so delta A hat is the error, the backward error, and it's bounded by absolute L times absolute U. So, and this follows directly from the analysis of dot products with an order n epsilon. And if you continue to do forward substitution and back substitution to solve ax equals b, you get exactly the same style of backward error. It's delta a double prime, and it's again bounded by this. And it's again bounded by this matrix multiply of two non-negative matrices. And so, if we want to prove backward stability, it depends on how much bigger this matrix is than the input. And so what the literature does is instead of estimating the size of this matrix, it uses what's called the growth factor, which is the largest intermediate result during Gaussian elimination divided by the largest entry in the original matrix A. So, what is known? So, what is known? There's been a lot of literature on this over time, including some very, very recent. And so, let me just give you a little history of that. So, what's the classic algorithm? Partial pivoting. At each step, you're just going to do row permutations, and I'll choose the largest entry in each column. And that guarantees that Lij is bounded by one. It costs you n squared comparisons. And the growth factor is bounded by two to the n minus one. And this is unstable, but very rare. is unstable but very rare and there's been some work over the time most recently 2022 uh by wang and tikomiroff and they and and what these what these analyses show is that if you run partial pivoting on a random matrix you can expect a growth factor of like n to the two-thirds or n to the one-half you know you know much much less than two to the n. So it is stable in practice on random matrices. So the next thing that people try that's a little bit better than partial pivoting is called rook pivoting. Better than partial pivoting is called rook pivoting. So now I'm going to permute on both the left, the rows, and the columns. And what you do is you choose your particular entry. So it's the first step. How do you choose the first pivot? It better be the largest entry in the column and the largest entry in the row. And you're going to do that at later steps as well. And so, what does that mean when you do the write down your factorization as L times D times U, where L and U are both unit triangular matrices, you're guaranteeing. Triangular matrices, you're guaranteeing that all the entries of L and U are bounded by one. Now, how many comparisons does this take? It's usually order n squared. In fact, the expected number of comparisons on a random matrix is order n squared, but it can be as bad as order n cubed, because you have to look down a column, find the largest, look across the row, find the largest, and so on. And that could repeat a lot. But what does that do to the growth factor? It gets it down from two to the n to n to the three quarters log n. The three-quarters login. So it does give you a stronger guarantee. And then there's complete pivoting. And I'm sorry I didn't see John Urshel's talk yesterday, so there might be something stronger to say now. I don't know. So in that case, you pick the largest entry of the matrix at each step. That's guaranteed to cost you n cube comparisons, so people don't do it in practice. And now the growth factor is about the cube root of what it was before. It's n to the log n over 4, and here was n to the 3 log n over 4. And people long conjecture. And people long conjectured the actual growth factor was N, but then some counterexamples were found. So then, people have also been looking at randomized LU with no pivoting. And so I want to talk about two variations on that. And this first one has been around for a while. It was originally suggested by Parker in 1995. Here's an implementation from 2013. What you do is you take your input matrix, pre and post multiply by some. Pre- and post-multiply by some random matrices, and then do Gaussian lination with no pivoting, you know, NP. So, this work shows them to be what are called random butterfly matrices. They're sort of based on the FFT. So, these are D0 and D1 are diagonal matrices with random numbers that are close to one. So, this is very well conditioned. And you do it not just, you know, at this scale, but you do it recursively a few times. So, this is why it sort of looks like an FFT, but you only do it a few levels and its backward stable. Levels and it's backward stable and much faster in practice because this pivoting turns out to be quite expensive. Now, is there any theory to do this? Well, we have a paper that's about to appear. It's on archive. So suppose you take your random matrix and you pre- and post-multiply it by HAR matrices. What's a HAR matrix? A uniformly distributed orthogonal random matrix. Then we can prove that the expected value of the logarithm of the growth factor is order the logarithm. The growth factor is order the logarithm. So, I mean, that can still be, you know, n to some power, but it's, you know, it gives you some sort of bound, some sort of guarantee that randomization with no pivoting is a good idea. Now, of course, hard matrices are too expensive to use in practice, but it's a step in the right direction. So here's another open question, which is, is there a similar analysis that gives you a guarantee on the growth factor or a probabilistic guarantee for butterfly matrices or something like that? Like that. Jim, can I ask a quick question? Sure. Just on the result that you mentioned on the last slide, is A random or is this for an arbitrary A? Oh, this is for an arbitrary A. Wait, this slide or a previous slide? This slide, but the last result with the random HAR matrices? It's for an arbitrary non-singular A. Okay, thank you. Yeah, the only randomness is the HAR matrices. Okay, thank you. Yes. Yes. And finally, there's one more kind of pivoting I'd like to mention, which is called tournament pivoting, which was invented for communication avoiding reasons. And Laura will talk about that in her tutorial. And so what's the idea here? So we're going to take a group of B columns of my matrix, and I want to choose B rows from those B columns and X touch all the data just once. So that's why we're minimizing communication. So I want to choose. So, I want to choose B pivots from B columns all in one data access. And so, the way we're going to do that is by doing a reduction. I'm going to break up my B columns into groups of two B rows, run Gaussian elimination with partial pivoting on those two B rows, and pick the B best rows from each group. And then I'll just do a reduction, take the best B rows from one group, the best B rows from another group, pick the best B from them. That's why it's called a tournament. And so it turns out that the what It turns out that what you compute at each step turns out to be the same. You get the exactly the same sure complement as partial pivoting applied to a different matrix built from A. So in some sense, it's as stable as partial pivoting. But what can we prove if the height of the reduction tree of this reduction is bounded by H, in the worst case, the growth factor is 2 to the N times H. So it can be much, much worse in theory. Much worse in theory than partial pivoting, but you know, in practice, it's as good as partial pivoting, and that's you know is confirmed by numerical experiments. Okay, so my goodness, time is flying. So now let me talk about fast matrix multiply and fast linear algebra in general. So the first, so the first analysis of this was done by Beanie and Lottie. They did it for Strassen. And it turns out that the idea extends to our It turns out that the idea extends to arbitrary, what are called stationary partition algorithms. So you have some magical way to multiply k by k matrices in less than k cubed time. And how do you do that? You take some linear combination of the entries of A, linear combination of the entries of B, multiply them and take linear combinations. So this includes Strossen and many others. And all of these algorithms satisfy an error bound that looks like the norm of the error is bounded by some polynomial in You know, polynomial in n times epsilon times the norm of a times the norm of b. And what is the exponent in that polynomial? It depends on how big these coefficients are and something that depends on the sparsity structure, their sparsity structure. And so it turns out this extends to non-stationary partition algorithms where you can maybe do a different algorithm at every step. There's some fast matrix multipliers where you preprocess A to get something else, pre-process B, multiply the pre-processed matrices, and then. The pre-processed matrices and then unprocessed them at the end. And then there was this early work by Cohn and Eumans on group theoretic recursive algorithms. So all of them give you this kind of error bound. So the question is, how do you do the rest of linear algebra stably in order n to the omega time? And it comes down to this idea of logarithmic stability. So what I'm going to do is talk about algorithms where the error bound, the forward error bound, so the difference between the output of the algorithm and what you want to compute. And what you want to compute is bounded by machine epsilon times the condition number to an exponent, which is polylog in n. So we'd like this exponent to be one. That would give us the normal error, but it's not. It's to the polylog of n. And so what do we have to do in order to get the usual error bound, which is epsilon times a condition number? We need to make epsilon smaller. And we only have to make it smaller by a factor of poly log of n. And that only is going to increase. Of n, and that only is going to increase the complexity by a factor of poly log of n. So, all the usual, you know, theoretical complexities, n to the omega, is only going to get bigger by a factor of poly log of n. So, that's perfectly reasonable. And so, what are some algorithms that have this nice property? Well, the most basic one, which is a building block, is inverting triangular matrices. And we'll just use the classical divide and conquer algorithm. And so, this cost, it's easy to analyze. It costs n to the omega. It costs n to the omega if we're doing all these matrix multiplies using the fast matrix multiply and n to the omega, and it's logarithmically stable because of the number of steps. And we can use the same idea to invert matrices. So what do I do to invert M inverse? I compute this symmetric positive definite matrix, inverted, and then multiply it by M transpose. So how do I invert a symmetric positive definite matrix accurately? It's just this divide and conquer. I factor it into this product of I factor it into this product of the lower block lower times block upper, compute the short complement, and there's this formula. So it reduces n by n matrix inversion into n over 2 by n over 2 matrix inversion and matrix multiply. And all the condition numbers work out. They only get better as you do the divide and conquer. And this also gives you this logarithmically stable matrix inversion for an arbitrary matrix. So, what about Q? So, what about QR and LU? And again, we're just going to do recursion there. And these turn out to be stable in the normal sense, not logarithmic. So, and this idea has been around for a very long time. We're going to do QR in the left half of the matrix recursively, apply it with matrix multiply to update the right half, and then call QR in the right half. And that all runs in end of the omega time. And Gauss elimination with partial pivoting is identical, left-right recursion. Right recursion. So now we get to the hard part, which is eigenvalue problems. And I'll give you a little history here. So a lot of them come down to something called a matrix sine function. And in the scalar case, that's basically using Newton to solve x squared equals one, a very hard equation. And if I apply Newton to solve x squared equals one, I keep averaging x with its reciprocal. And it's easy to show that converges to the sine. Converges to the sine of the real part of x naught. And so if I apply that to a matrix, and so here it is, a n plus a n verse divided by two. So its eigenvalues are converging to plus one and minus one. So if I, for example, add that to the identity and divide by two, the eigenvalues are converging to one and zero. And I get the spectral projector either for the eigenvalues in the right half plane or the left half plane. So this is a well-known idea. So what do I do if I can compute that? So, what do I do if I can compute that spectral projector? What I want to do is I want to get an orthogonal matrix that spans, whose initial columns span the same spaces. So I can do divide and conquer. So I'm going to do something called rank revealing QR. I'm going to start by computing a HAR matrix. How do I do that? I'll compute a Gaussian matrix. Each entry is normal 0, 1. Do a QR decomposition. So V is HAR, random orthogonal. I'll take my spectral projector, multiply it by V transport. Spectral projector multiplied by V transpose and just do QR again. This is all order n to the omega. And so I factored my matrix into orthogonal times R times R. And it's possible to prove that U with very high probability, its leading columns span the range space of P plus, which is the invariant subspace for the eigenvalues in the right half plane. And so if I now form U transpose AU, I've taken one step towards the Shur decomposition. Step towards the Shur decomposition. All the eigenvalues up here are in the right half plane, the left half plane, and this should be order epsilon, which I can confirm. And this is stable if it really is order epsilon. And then I'll just keep on dividing and conquering here, you know, by doing a shift of A and maybe adding a multiple of the identity to it. I can sort of divide it and different circles. And one way to understand how this converges is that Newton's method is just basically repeated. Newton's method is just basically repeated squaring of the Cayley transform. So the Cayley transform is going to push, you know, repeated squaring pushes all the eigenvalues inside the unit circle to zero and outside the unit circle to infinity, you know, and the Cayley transform, you know, changes that into plus minus one. So now, in order to get be stable, I want to get rid of these inverses because that algorithm is not stable. So here is the idea for inverse free. The idea for inverse-free repeated squaring of A inverse times B. So I'm going to start with A and the identity from my original ones. So, what do I do? I take these two matrices, I stack them on top of one another. I do a QR decomposition, which is stable and fast, and then I just do two matrix multiplies, Q1, 2 transpose times A and Q2, 2 transpose times B, and I get the next iteration. And it's simple algebra to show that the next iteration is just the square. The next iteration is just the square of the previous one. So, all of this is, you know, I'm doing repeated squaring implicitly. I'm not actually ever computing this inverse. So there are no matrices going to infinity, or I'm not inverting anything that's singular. And so now what I need is the rank-revealing QR decomposition of the projector for all the eigenvalues inside the unit circle. And that by that previous formula, sorry, there should be another inverse in here. So this is identity plus this matrix inverted. Is identity plus this matrix inverted, the whole thing inverted. And a little algebra says I need the QR decomposition of this. So I have another darned inverse. Turns out I can get rid of that inverse. I'm going to compute the URV decomposition that I did before on AJ. That's a Harrow matrix. Then I multiply the resulting matrix here times AJ plus BJ, do another RQ decomposition. And when I put it all together, I've gotten a Together, I've gotten a orthogonal times triangular, which I don't never have to compute it, times har. So, this is a rank-revealing decomposition of this matrix that I wanted. And so now I can apply Q to do my, you know, to do my divide and conquer. And by applying this to this particular pencil, instead, I can split the spectrum on any circle that I like in the complex plane. And in fact, this works for regular matrix pencils, not just the standard eigenvalue problem for the generalized eigenvalue problem. Problem for the generalized eigenvalue problem. And it all comes down to matrix multiplies in QR. And now the finite precision analysis is a work in progress. But of course, this is sort of an idea that was taken by Nikhil and his students, Banks and Vargas, and they took a slightly different approach, but it's very similar in background. So what they noticed is that you could shatter the spectrum so that you could guarantee that this divide and conquer always worked. So the trouble with this previous. Worked. So, the trouble with this previous algorithm is: how do you choose a circle or a line in the complex plane and guarantee that when you run this algorithm, there are no eigenvalues that are so close to that boundary that they never leave the boundary? And so what they showed is that if you add a small multiple of a Gaussian matrix that with high probability, that separates all the close eigenvalues and so that you get a well-conditioned matrix with eigenvalues that are a certain distance apart. And that means you can use actually the matrix sine function. Can use actually the matrix sine function using Newton, you know, by doing binary search on a 2D grid to find a good place to split it. And their final result is that, you know, by sort of tuning this gamma, you know, how much noise you add, you can sort of trade off cost and stability. And so what they showed is that you can get a small backward error of size delta. So this is actually, you know, computing the eigen decomposition, so not a Shor decomposition. So the columns of V are the So, the columns of V are the eigenvectors, and D is the eigenvalues. So, you can get that less than delta, but at the cost of having the condition number be large, you know, because delta is in the denominator. And the cost goes up too, as delta gets smaller, but it only goes up in a polylogarithmic fashion. And it's still basically order n to the omega, where omega is your favorite matrix multiply. So, that's what I wanted to say about where things stand on the eigenvalue problem. Value problem. So now I have six slides here. Maybe I won't get through them all. But what if my problem is structured? So it could be symmetric or sparse or diagonally dominant or Vandermonde, you name it. Can I get a more accurate answer or at least a structured backward error? So there's so many, so there's a huge literature on this. I'm just going to show a few possibilities. So here's a simple one, which is Kaleski. So I know my matrix is symmetric and positive definite. In that case, there's a theorem. In that case, there's this theorem going back to 69, Vanderslus, who said that if I have a symmetric positive definite matrix, if I choose a diagonal matrix to put ones in the diagonal, then that guarantees that I'm reducing my condition number to within a factor of n of the minimum over all possible diagonal scalings. And so if your original matrix had some very large and very small entries in its diagonal, then the condition number of this scaled matrix would be much, much smaller. much much smaller and it turns out that if you just go ahead and do kalesky you know you don't do any scaling just just do kalesky and x hat is your solution then the error is actually proportional to this different condition number so that which could be arbitrarily smaller but it's the scaled error so so in other words i need to scale these uh the error and the uh true solution by d inverse and uh you know in the spirit of what does the condition number have to do Of what does the condition number have to do with distance singularity? Again, the one over this scaled condition number is the smallest component-wise relative perturbation that makes A plus delta A singular. So this is a better interpretation of an algorithm without changing it at all. So what do people do to get higher accuracy? So one thing that people do is iterative refinement, which is basically Newton on a linear equation. You solve Ax equal B, computer. Solve Ax equals B, computer residual, solve A times D to get an update, and do X equals X plus D. And so since we're running Newton on a linear system, you would hope it would converge quickly, but there's still a lot of versions out there. The classic version that goes back a long time is you do the solve using Gaussian elimination with partial pivoting. You compute the residual in double precision, and it's been well known for a long time that X is going to converge to the true solution in norm if the condition number is not too large. Condition number is not too large. The condition number times machine epsilon should be less than one or not too close to one. So, but it turns out that you can also make progress if you compute this residual in single precision, even though you'd expect it to be just round-off noise. It actually buys you something. It turns out that x converges to a small component-wise relative backward error. So, in other words, you get the right answer for a slightly different problem where you've only made small relative changes to the entries in A. Relative changes to the entries in A. So, in particular, if it's sparse, you've preserved the sparsity. And here is a closed form formula for the component-wise relative backward error. It's very cheap. And here's the condition number. It's, you know, just basically, you know, plug in, instead of taking norms, I just take the original matrices, take absolute values, and do all these matrix multiplies. And so that's been around for quite a while. It's an LA pack. And then more recently, there have been a whole bunch of analysis in this using different Of analysis in this using different solvers, you know, GM res, different convergence criteria, and not just two different precisions, three precisions, or even five precisions in the most recent work, you know, and what you can buy from that. I'm not going to try to summarize that. So now I'm going to do something which maybe, you know, start with polynomials. How much time do I have left since we started a little late? Yeah, I think five, six minutes. I think five, six minutes or so. So, so the question is: when is high relative accuracy possible in the traditional model when all I know is that my answer is bounded by, you know, is written by as one plus delta and delta is small. So let me start with polynomials and then do linear algebra. So suppose I want to evaluate x squared minus y squared. Well, obviously I can factor it, you know, and evaluate it that way. So it all works out fine. But it turns out if you want to multiply But it turns out, if you want to multiply, if you want to add three numbers, it's impossible. There's no way to guarantee that if this actually equals zero, it all cancels out completely, that you will get a zero, just because the round off works that way. So that's impossible. So now, maybe a little surprising example, for those of you familiar with Hilbert 17th problem, you may know about the Motzkin polynomial. So the question is, what about this polynomial? What about this polynomial? Do you think offhand that you could do that one? Well, the answer is yes. And it turns out there's eight different cases. Here's one case, depending on whether all of these inequalities are true, here is a formula, and it's guaranteed to compute this to high relative accuracy. And so the question is, you know, where does this come from? And, you know, how possible is this in general? So it turns out that being able to evaluate a polynomial where x is now a vector. Is now you know a vector depends on its variety. So, some varieties are okay and some varieties are not. So, we're going to call a variety allowable if it's a finite union of intersections of these basic sets, which come from your basic floating point operations that you know you can do. We can compute sums accurately, differences accurately. And so, all I'm allowed to do basically is have finite union of intersections these sets. And there's a theorem that says that if you're That says that if your variety is not buildable that way, it cannot be evaluated accurately. There's some round off errors that will give you the wrong answer. And that's true on either Rn or Cn. So why does the Motzkin polynomial work? Here's its variety. All three components have to be equal in absolute value, and you can build that out of this. And there's a theorem which says that if you're doing complex arithmetic, then being allowable. Then being allowable is also sufficient. It's necessary and sufficient for accurate evaluation. And it's basically a very simple idea. The polynomial just factors into terms that look like this. So you just compute them all and multiply them. Now, we tried to come up with an algorithm in the real case, and we made some progress toward a decision procedure to say, if somebody hands you a polynomial, can you decide if there exists an expression that looks like You know, an expression that looks like this exists to evaluate it to high relative accuracy. And, you know, but we got stuck. So please feel free to read that paper in the Act Numerica and see if you can solve it. So I should say all of these ideas extend to adding black boxes. So for example, if I not only had addition and subtraction, if I had fuse, multiply, add, if I had dot products, you can extend the notion of an allowable set to be much, much more. To be much, much more and ask the same questions. So, now what about linear algebra? So, what kind of structures of matrices permit high relative accuracy answers? So, here's a table where there's one row for each different kind of matrix that's been explored in the literature. And there's one column for each kind of linear algebra operation you want to do, like computing a determinant or the SVD or a rank-revealing decomposition. And if there's an n. Composition. And if there's an n cubed, that means there's an algorithm that will get high relative accuracy for that particular problem for that particular matrix structure. And if it says no, that means there's a proof that there isn't. And so this is a very busy table. I'll just mention a few rows. So what's an acyclic matrix? That means its graph is bipartite. And the classic example of that is a bidiagonal matrix. And so, you know, we can do the SVD of that. That's an LA. You know, we can do the SVD of that. That's an LA pack. This is diagonally scaled, totally unimodular. That includes a lot of certain simple discretizations of elliptic PDEs. And so that also lets you do these sorts of things. This is total sign compound. I won't discuss that. So it turns out for diagonally dominant and for M matrices, you need a slightly different parameterization. What you need to know is the off-diagonal entries of the matrix and the difference between the diagonal. And the difference between the diagonal and the off-diagonals. You know, AI minus the sum of the off-diagonals. If you know that to high relative accuracy, then all these other things work. So, Cauchy and Vandermond are special cases of displacement rank matrices. That's a matrix which satisfies a Sylvester equation equals rank one. And it turns out all of these are possible to high relative accuracy as well. Totally non-negative is where matrices where all the minors are. Where all the minors are positive, and there's a huge literature on these. And these are all possible, but the assumption is they're being given to you in Neville elimination format, which I can talk about in some detail. That's basically an LU decomposition where L is represented as a product of bidiagonal matrices. So, anyway, there's a large literature on this stuff. And just to give you one example of what this lets you do, if I take a If I take a Van derman matrix, and here are the entries, so it's non-symmetric, and it's 40 by 40, so it's very large and it's very ill-conditioned, and I take its 20 by 20 Schur complement, and I compute its eigenvalues, right? So I'm doing a sequence of operations, then these algorithms will guarantee that you get high relative accuracy in all the eigenvalues of the Schwarz complement. And those are shown in blue, but if you run the class. Shown in blue, but if you run the classical algorithm, you get these eigenvalues, and they're all wrong by orders of magnitude. The largest ones are wrong by orders of magnitude, and the smallest ones are, but the blue ones are all correct. So this gives you, you know, extends high relative accuracy to a much larger class of problems. And that is my last slide. And I have many references here that you can look up that are in the order of presentation. So I will stop here and take questions. Thanks. Take questions. Thanks a lot, Jim, for this fantastic overview. Yeah, so I think we start with the questions from the Zoom audience. Are there any? Please just speak up. Yes, thank you, Jim. That was a really tour de force survey. I enjoyed that. I wanted to ask a dumb question. So, in order to say I have a fused multiply add, right? And I want to avoid catastrophic cancellation in X minus. Catastrophic cancellation in x minus y. If I write one times x minus y, then do I get a much more accurate result? You get the same result because x minus y is computed. If x and y, this is a well-known theorem, are within a factor of two of one another, then x minus y is exact. No, what I did, catastrophic cancellation, right? So yeah, so they're very close together. But you said when you presented the bound for fused multiply add, I do A times B plus. add i do a times b plus c uh with a relative error one one plus dam yes yeah let me let me just go back to that so sorry here we go so what this says is that what the what the hardware is doing it's computing a times b exactly with no it's keeping all the bits Exactly, with no, it's keeping all the bits, then it's adding C, and then it's rounding. So it computes, it's equivalent to doing computing A times B plus C exactly and then rounding. Sorry, sorry. My fault. Yes. Sorry. Never mind. Okay. And that's in basically all hardware these days. Yeah, never mind. Any other questions from online? Oh, I have one. Yeah. Thank you, Jim, for a very nice survey. So, my question is: so, you mentioned that. So, my question is: So, you mentioned that there's interest in 8-bit arithmetic. So, what sort of linear algebra problems do people hope to solve in 8-bit arithmetic? Well, so far, it's just telling cats from dogs. But that all comes down to matrix multiply. So, they're all building high-performance accelerators. Let's see. Let me go. Right. So, so the interest here. Right. So the interest here is: so, what they're doing is entirely empirical. There's a big literature on this. And what they've done is just a lot of experiments with different formats and different algorithms and lots of hyperparameters that they tune to see if the accuracy of their convolutional neural network is predicting whatever they're trying to predict as accurately as it was before. But what it's doing under the cover is matrix multiply. But for these kinds of problems, all you need is maybe the correct size. You need is maybe the correct sign if you do classification or the correct exponent. So you don't really need 16 digits or maybe not even two digits. Well, I mean, there's some, you know, so this is the this is the question. There's some experiments that say you do need higher precision, you know, because it's, you know, because you do do the wrong classification. You know, and there's some experiment, you know, people have also said, yes, you only need one bit, up or down. But, you know, it, the whole it the the whole the point about this standards committee is it's trying to you know find the the fewest number of bits that you know that solves the largest number of problems and and the literature and i you know i didn't include references but i in my list at the end but i can certainly send them if you're interested you know people are sort of converging on eight bits and so what are the two formats um actually the the the committee seems to be converging on two different eight bit formats Converging on two different 8-bit formats because one is needed for training and the other is needed for inference, and they're different. So one of them is called 1 plus 4 plus 3, 1 sine bit, 4 exponent bits, and 3 mantissibits. And the other one is called 1 plus 5 plus 2. One sign bit, 5 exponent bits, and 2 mantisibits. Those seem to be the ones that folks are converging on. And what the committee is still arguing about. And what the committee is still arguing about are infinities and NANDs and stuff like that. So you're giving the training one more bit in the mantisa. Yes. Great. Well, thank you. Yeah. Basically, it's more optimization type problems because I'm guessing it's probably impossible to meaningfully compute an eigenvector of a large matrix. Right. But the reason I'm bringing this up is that if the fastest thing that future computers do by orders of That future computers do by orders of magnitude is this, then our community is going to feel some pressure to use it. But then in this case, going back to Peter's talk, first order condition numbers enough anymore because epsilon is not at all small, as you were emphasizing. Do we need more accurate condition numbers? Well, or more iterative refinement, or there's all sorts or Or, you know, there's all sorts, or yeah, or the thing is, this is going to probably do mixed precision. So when it does the matrix multiply, it's probably going to do the accumulations in 16-bit arithmetic. And so the question is, you know, how, you know, if we know that that's the basic thing underneath the covers, how do we take advantage of it? And so that might involve and we get, you know, the leading bits and the trailing bits, you know, who knows? So who knows? Okay. Any more questions from online participants or if not, have any questions from the room? There are quite a few. Okay. Go ahead. Thanks so much. I just have a clarifying question. You mentioned that there's this two-sum method in order to let you exactly compute the sum of two loads, but then these Loads, but then he said that something later on to guess. You said that x plus y plus z was not possible to compute exactly or to even relative error. Do you think what's going on here? So that was because I was using a different theoretical model. Let me get to that. So if all I'm assuming is that these are real numbers. Assuming is that these are real numbers, and the output is a real number that satisfies the true answer times one plus delta, where all I know about delta is that it's small. That's the assumption that I can't compute x plus y plus z. So this model does not capture floating point arithmetic. It is the coarse model that we've been using for generations. Okay, so two sums for like very specifically, I don't know, slopes. Okay. Floating points. Floats. Yes. And this is an abstraction of floats, which is useful for lots of analysis. We all use it. But it does mean that certain things are impossible in this model. Right. I mean, if you're just thinking about floating point numbers, it's just bit strings and we can do whatever we want. Sounds good. Thanks. Jim, I had a question. I thought it was a great talk. I learned a lot. I also had a question about the earlier part where you were talking about these super low precision standards. And you mentioned the idea of supporting randomized rounding. It's not surprising as I guess I would have thought that this central limit theorem behavior showed up, even if you didn't do things randomly, because probably about half of your rounds are going to be positive, and about half of your negative. Well, so here's an example. You know, if you, if S, if, if, um, If x1 is very large and all the x2s are very small, then you will, they'll just round away and the answer is necessarily going to be n epsilon. So it is possible to attain this bad upper bound, but if you do randomized rounding, that can't happen in the same way with high probability. There's a proof based on the central limit theorem. Yeah. For limit theorem. Yeah, okay. So I guess, okay, so there are like reasonable, reasonable hard cases that could show up. Yes. Okay, great. Thank you. Any other questions from the room? Oh, I was just going to ask, thank you, Jim, for a great talk. Will the slides be available? Oh, yeah. I will send them to the organizers and I assume they'll post them somewhere. Great. Thank you. Yeah, they're very dense, so yes. I just wanted to ask a simple question. With the randomized rounding, I assume there would be a way to set the random number seed so that you could get repeatable results. Well, the committee was arguing about that yesterday. And the question is, yeah, what do we want to do to guarantee reproducibility? Because all of these platforms. All of these platforms are going to be, you know, designed, you know, are going to be, you know, large, highly parallel machines. So you're going to be doing a thousand of these randomized ads at the same time. Does that mean you need to have a thousand random number seeds to keep track of? Right. And the thing is that we don't want to over specify the hardware to that extent. And so what they discussed, the way the discussion went was. Way the discussion went was that you know, we will point out the pros and cons of reproducibility, but leave it up to the uh designers and not try to specify it. That just because, you know, if you're trying to do a thousand or a million of these at the same time, how do you control all those random number seeds? Okay. One more quick question. Thank you very much. It was a very instructional talk for those of us not in the Michael. For those of us not any Michael in the algebra. I wanted to ask about the stability of fast matrix multiplication. So, if I understand the differences correctly, standard matrix multiplication has guarantees that are somewhat stronger than faster matrix multiplication, right? Let's just say Strassen for you. Yeah, right. So, for Strassen, it's N to, I forget the exact