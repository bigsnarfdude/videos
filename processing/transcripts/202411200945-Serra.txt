So please all the rest comment to Francesca and Leonardo, all the rest for me that I'm here. So the physical problem that we want to address. Magnetoards. We want to try to see gravitational waves emitted by the new report magnet arts. So these are isolated and on the axisymmetry spinning neutral star with a very strong With a very strong internal magnetic field that can induce a significant asymmetry as measured by the ellipticity. So, why we want to study this kind of gravitational waves? Because they offer a natural laboratory for understanding how matters behaves under very strong magnetic fields at a density that can be replicated on Earth. This could contribute in understanding the matter. In understanding how these kinds of stars evolve, and this could give us information about the equation states of the star. So, state of art and our contribution. So, standard search techniques are really computational and feasible or very, very demanding. Okay? I'm thinking about optimal semicoherence methods like the ones you use. Cohen's method, like the ones used for the post-merger search GW18718. Okay, so an initial study using an image classifier to address this kind of problem was done in 2019, but has not been replicated with a recent simulation on audit. Okay, so our goal is to develop a computational affordable data. Computational or affordable data analysis pipeline. Two steps. The first, get gravitational weight guided using machine learning, performing a blind all-sky search. The second step, the second phase, we want to perform a detailed study to extract the signal parameters, possibly using the generalized frequency after, but on a bounded parameter space. Okay, so the first step will help us in restricting the space of the parameter that we want to explore. So, the master plan, what we have done essentially to obtain this pipeline that is highlighted over there, is a fascinating study of our approach on simulation to test neural network performances. In particular, we tried a classifier and a denoiser. So, essentially, the classifier So essentially, the classifier is a neural network that says, okay, you have a signal, or you have nothing here. The denoiser is a specialized neural network that helps us in retrieving a clean image from a noisy one. I will come back on this in two minutes. We verify what is needed for the training and the computational load, and in the end, we study if it's possible to address the training with the real To address the training with the real data. Okay, so the key points, for example, for the 12 classification task, we add this the noise, but the key point is to use a neural network that is able to learn the noise from the real data. I'm not saying that we will understand the structure of the noise, we will understand all the possible sources of the noise. We just want to have enough information to help the classifier in Help the classifier in finding this kind of signals. We tested also different neural network architecture to implement the noise, and we found that using some attention method techniques, we really can improve our results. So, data handling. So, if you have a time series, you can use the fast-fourier transform to create your favorite time Freeze map. Okay, we did it. Okay, we did it. We create interlaced frequency map with a time resolution of two seconds and a frequency resolution of 0.25 Hertz. We use a squared map with a time interval of 1200 seconds and with a frequency band of around 150 Hertz. So these choices were suggested by physical properties that we will discuss in a second and In a second, and also from additional consideration. Let me say that for the training of the denoiser, for example, we will need to give to the network a bunch of 10, 20 maps. This will load the memory of the GPU. So increasing the size too much of the map will end up in a situation where the GPU is not any more functioning well, okay? Functioning well. So these are signal examples. You can see the time scale of 1200 and the frequency band. And here you have two examples with this is the initial frequency in these two situations and the epsilon for the star. So the emission frequency and the magnet are electricity. You will see that you could have signals that are almost like persistent line, but also other. Line, but also other signals that are really, really fast, changing really quickly in frequency. Now, we use this toy signal model just to generate signal for our study. Essentially, you have here the frequency variation expression. You can see how the frequency changes when the star is rotating. So, we tested the many, many different Many different initial amplitude. In this talk, I will use just the lowest one that we can detect in our simulation, that is 2 times 10 up to minus 23 with a fixed fiducial momentum of inertia that comes from this review. And also, I want to stress that we target only signals with a significant variation in frequency. Significant variation in frequency, because otherwise, if you have permanent persistent signal, the algorithms that are looking for continuous wave will detect them. Okay, so we are looking for the other category. So, if we want to have this variation, a significant variation in frequency, this defines a parameter's range. And this is the range for epsilon, and this is the range for the initial frequency. And you can see here that the And you can see here that we are targeting a signal with an initial frequency that could be between 100 kilohertz and 2000 hertz. And the ellipticity will be around 330 times 10 up to minus 4. And you can see that from this small number, for sure, I have chosen the wrong peak. I have chosen the wrong picture for the rotator because the deformation is really, really small. So, here you can see some examples. Here you see the signal plus the noise. You can see the signal. I can't see the signal, but you can see the signal. The signal is this one, and this is just the noise. In this situation, with this epsilon, with this initial frequency, and this output. These are the good signals. These are the kind of signals that we would like to detect. How? So, first of all, just to give you the feeling how much the signal emerges from the noise level, we compute this quantity, the pixel signal to noise ratio. Each point represents one of our time frequency map, okay, in this frequency epsilon. Can see epsilon domain, and you can see that how hard is the task, in particular in the top right corner of the parameter space that we are taking. Okay? So, what we have done essentially is we implemented a very simple classifier that outputs the probability of the presence or absence of the signal. It's a simple structure. It's a simple structure because we have the noise that we for the classifier. Otherwise, Before the classifier. Otherwise, the source is different. So, a simple structure can just try to correlate the pixel in the macro because the correlation of the signal pixel is different with respect to the noise. Okay? What about the noise? So, essentially, we tested the different networks. In this talk, I will speak about one that we call residual learning approach, and the other one using the UNET. Using the UNET. And this is just a sketch just to give you an idea how we train our networks. So essentially, we give a signal plus noise map and a signal map, and we train the network to learn the noise. When the network is trained, we use the skip connection that connects the input and the output of the network just to subtract the noise and to obtain the noise. The noise and obtain the green map. And I give you a good example here. Here you have again the noise and somewhere the signal that this is one. And this is the output of our noise. It's not perfect, but it's enough for the classifier to identify the signal. This is our point. We don't want to have the full signal perfect. The full signal is perfect. Because, in any case, in the second step, we will use the frequency of transformer to obstruct all the parameters. We want some candidates, some triggers for the second step. So, how the noise is going. So, if we consider essentially how much of the original signal Of the original signal we could preserve through the noiser. To give you an idea, we calculate this overlap. It's essentially diffraction of the signal that is preserved after the application of the net. You can see here again in this frequency epsilon map the result of our the noise. And you can see that this in this coordinate the situation is much more complicated. Is much more complicated, but still we have 20% of the pixel of the exigna that we could keep with this technical. So, what about the classification results? So, here you have the maps that have been cleaned with the residual learning denoiser and the other one cleaned with the unit uh denoiser. Okay. The noiser. Okay, so you can see that you can choose the threshold as you like. You can decide if you want more efficiency or you want to decrease the fast alarm probability. If you want to have 2% of fast alarm probability, keep in mind that I'm speaking about a single detector. If we combine two or three detectors, we can go down of one order of magnitude at least, maybe more with some clever things. Maybe more with some clever things. But just to give you the basics of this technique, with this approach, we have something around 90% of the efficiency. With the other denoiser, with the same falsalaric probability, we have something slightly better. And I can tell you that here, you can see the efficiency with the yellow dots and the fashion probability with the Lot probability with the green dots, but with the cross, you can see the effect of the muscle loss. It's a technique that we use in the training of the denoiser to really increase the denoiser performance. You can see that this is quite effective. Okay, now, but okay, let's now think about the distance that our That our triggered signal comes. Okay? So again, the result for the residual learning, the result for the unit denosa. Again, in the map for the frequency, the initial frequency and the epsilon. So essentially, if we estimate using this formula the distance that from where From where the signal comes, you can see that we arrived for the moment less than one megaparsec. So, for sure, we need to reach higher distances if we want to increase the probability of CN event. Maybe if we could reach something of the order of 7, 8 megaparsec, we could have the rate of 1, 2 magnet. Two magnetar events per year. So, for sure, we need to improve the noise. You can also see, for example, looking at this kind of map. Each point is again a single map that we are working on. And you can see, for example, here that this network is performing better in this corner, that is the most complicated one, but the network are complementary. But the the network are complementary because they're uh they're essentially the positive negative answer is not always the same for the same. Now, life is not easy, so essentially stars lose energy not only by gravitational waves but also emitting electromagnetic signals. Okay, so we just want to test the To test the registers of our train and the network, so we generate a new set of signals with a different preken index, not n equal to 5, but just for gravitational wave, but in this range. And here you have the range. Again, in the frequency epsilon plane, we essentially generate all these kinds of signals with their different tracked index. And you can see here, this is. And you can see here, this is the overlap of the result of the denoiser for this signal. Still, our denoiser and our classifier are able to work. They have been trained just with the perfect gravitational waves, not in the n equals five mode. But still, we have an efficiency that is not so bad again for if we want to consider the If you want to consider the threshold with the 2% of accidental. Now, what about the computational load? I told you that we really want a procedure that is not only fast, but it's also cheap, because otherwise we could have done everything with the frequency of transform. I give you all the numbers so you can have your idea. So, essentially, the training time. So essentially, the training time for the denoise, for the denoiser, is something I'm using this single GPU that is a modern one, but not the best one. Still, the training time is ours, just to work on something of the order of 2,700 maps. And the deto uh after the training, to denoise more than five thousand maps, we need just twenty minutes. We need just 20 minutes. For the classifier, the training time is nothing, 30 minutes. And after the classifier is trained, the classification time is really for 1,000 maps is a matter of minutes. Another point is important. We think that for a real search, essentially, if we have of the order of 14 days of the inverse data, there should be sufficient. Data, they should be sufficient to train our networks. Okay, so for our approach, we don't need to wait, I don't know, months and months and months of data before we could run our procedure. And this is also another result that we will think to obtain with this technique. So, yeah, two minutes, I'll go faster. So, this is just for the general discussion. I added this slide for the general discussion. Slide for the general discussion. So, from the perspective of our analysis, when is deep learning useful for gravitational wave detection? Okay, three points for you. So, one, when you can choose the neural network architecture that fits your problem, not taking the most abstractive, recent, super powerful architecture. So, think about your problem and choose the right network first. Second point: when you can control the black box, what does it mean? What does it mean? So, in our case, this translating is essential in checking that the denoiser is not making artifacts that could mimic the signal. And in any case, you have the control of the second phase with the frequency afterso. The last point, when you need to save computing power. That is really the bound, the limit that we have for this kind of analysis with the standard nice old techniques. Nice old techniques. Okay, so conclusion. So essentially, the result of our simulation says that the technique is made to deal with the real data. Actually, we already started sometimes ago to deal with the real data. Now we are training the noiser with the real data. We think that we need to improve the classifier. Looking at the real data, obviously we have the persistent noise line, so we think that we should add a category in the classifier. In the classifier to essentially identify also the noise line. Absolutely, we want to improve the noiser to explore greater distance, and we want to start to combine different interferometers. You can find all these considerations in this preprint, because the paper was accepted and we are waiting for the publication. Some general uh comments for the discussion. How we compare with the with the previous technique? The previous technique. So essentially, we run a code to estimate the optimal sensitivity for the GRI frequency after storm. In really ideal conditions, means the best integration time with the best window for the sampling, all the best. And we found that the maximum distance to the text signal is slightly less than twice our result. So we are close, essentially. Essentially, to make a realistic comparison, okay, we should redone the full analysis with the frequency after a school, but this will imply a lot of computing power. Okay. Another general point, in principle, this technique will use it also for rather long-transient signal because the architecture is not specialized just for mathematical signal. And I think, yes, that's all. I thank you for your attention. So really nice presentation. I'm just wondering, the denoising problem is treated as a masking problem. So it's like a classification, a pixel-wise classification problem. Am I right? No, no, no. It's really. We essentially teach the network to learn a noisy map. And after the training, we use the noisy map that is learned for, I mean, it depends on the bundle frequency. So the detect the noise learner, the noise map for each band. And when you give to a trainer the noise an original map, it just Map, it just subtracts the map that he learned. Yeah, yeah, yeah, I'm sure. Pixel by pixel. I don't know if you're asking me what the mascot is. Does it regress the pixel values or does it classify this is a signal or this is just noise? You know, like so, like denoising problems, like these problems, you can like either like predict a mask, which is like ones and zeros, and then you multiply the mask by your mask we use. We use for the mask we use for the training phase. Essentially, when we were saying that we improve our result using some attention mechanism methods, we essentially, during training, we multiply in our loss, we have an X, it's a very simple loss, and we add a term in which we have a matrix, and in the first step, In the first steps of the training, we multiply this matrix just to highlight just the C signal. We go ahead and we increase a little bit, and so we enlarge the area that the noise is taking. In the end, we have the full map, and so the last result of the training phase are the weights to see. Are the weights to see the full map? I hope now it's more clear at the beginning, making it easier for the data. Yeah, because the point is that we have really few pixels for the signal. Something of the order of 600 more or less pixels for the signal, and for the noise you have 600 times 600. So it's sort of a form of like curriculum learning. You know, there's like increasing the if you want, yeah. Like increasing the short, right, yeah, okay. Thank you. Yes. I was curious to know about what are the possible challenges of pending, if any at all you're expecting if you take the data from other two data projects like multi-detector training. So okay. If you what we what we could What we could do using two or three detectors, right? Okay. So, first of all, we could decrease the Pasalar probability, just to make it coincident. It's something really easy. If you want to think about some more complicated stuff, you could sum maps coming from two different detectors, because in that situation, the signal will be the same and the noise will be not the same. Noise will be about the same. So you can play games just to enhance a little bit similar. But it's just a game that we just started. Don't trust me on this. So we're 10 minutes into our coffee break, which is my fault. I'm sorry. But the remaining questions are okay, chatting over coffee. All right, can we give Marco one more minutes? 