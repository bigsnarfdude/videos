Okay, thank you for that, for your patience during that slight delay. And thank you to the organizers for the opportunity to speak. It's been a great workshop so far. And I'm happy that I can join even remotely. It turns out probably a good thing. I came down with a cold the past couple of days. My rat tests, as I've learned now from Joel, how that's, you know, how we should refer to them, they've so far come back negative. Far come back negative. But if I have to pause for some coughs, my apologies for that as well. All right. Okay, so today I want to talk about the effort we started in early 2020 to forecast COVID confirmed cases and deaths across multiple spatial scales at LANL. And in line with this workshop, I want to not only describe the model that we came up with. Described the model that we came up with, but also spent quite a bit of time talking about some of the lessons we learned once we stopped and looked back over the first 18 months or so of forecasting. And then finally, I'd like to just describe a new effort that we've started towards incorporating COVID-19 variant dynamics into our forecasting model. You've heard quite a bit about forecasting. Of about forecasting from previous presenters, so I won't spend too much time here. But why is forecasting important? And since January 2020, before even a single case of COVID-19 had been reported in the US, models were already being developed to forecast deaths, hospitalizations, and cases with an early focus on what was happening in Wuhan. And so it was clear, even from the beginning, that forecasting was going to be a public health tool during this pandemic, and we've seen how that's been the case. And we've seen how that's been the case for the past two years now. And it's been well established at this meeting that we think forecasts can be useful ultimately because they can help with informing decisions, resource allocation, and implementation of interventions. So now what are forecasts? Carrie just talked about distinction, and I probably don't need to belabor this point for this audience, but within infectious disease modeling, we have these two kind of sub-disciplines that ask slightly different. Sub-disciplines that ask slightly different questions. You have projections, which this is where a lot of traditional epi-modeling has lived in. These are attempts to describe what would happen given certain hypotheses. You can conduct these what-if scenarios. They allow you to get the kind of longer-term outlook. Whereas forecasts, they don't make any attempt to predict what will happen. They attempt to predict what will happen based on current trends. They don't make any assumptions beyond what's already built into their model. Already built into their model. And so we here are focused on forecasts. What does the task of forecasting look like? You typically have the following ingredients. Can you see my cursor? Yes? You have some sort of measure of disease burden, and that is your public health outcome data that's recognized as the gold standard. And so for COVID-19, obviously there wasn't. COVID-19, obviously there wasn't any sort of authoritative source going into the pandemic. But the Johns Hopkins University Center for System Science and Engineering, they early on started collecting this data, making it available, and that has become the gold standard in terms of the data that we receive and the data that we're trying to predict. And of course, we know that what we're trying to predict is not the true number of COVID cases, but it is something that is measurable that we can. But it is something that is measurable that we can compare our models to. Now, the second ingredient is the definition of these targets. Here, we're looking at one to four week ahead targets, providing both a point estimate and some probability and certainty around that. We've already talked a lot about the effort of the ensemble model, the aim to bring together. The aim to bring together multiple different models to create an ensemble that traditionally has outperformed any one single model if you look at it in terms of consistency over time. So I won't spend too much time here. All right, and before I dive into the model that we developed, I want to provide some key dates in terms of a timeline that can help us kind of remember what it was like back in 2020 and help put this endeavor in place. And help put this endeavor into context. So, the first two boxes here, these are talking about kind of important dates when we became aware of a potential pandemic. January 9th, the World Health Organization announces pneumonia in Wuhan. January 21st, we have our first U.S. case. And then these next three boxes touch a little bit more on what was going on in the modeling side. In early February, the Johns Hopkins University started publishing their Git repository. So this data. Publishing their Git repository, so this data became available. By early April, the COVID-19 forecast hub had been formalized, led by the Reich Lab and in collaboration with the CDC. And then in early June, this is when the first ensemble model was, the forecast was published. And below that, you can see how the progression of cases, cumulative cases, increases. And so all during this time, even though the Even though the first forecast ensemble comes out in June, since January, these discussions are: this is kind of a goal, this is kind of what's happening in the community, but it does take kind of four months to get to that point. And on top of this, these are some events that were happening from the LANL perspective. We started developing early versions of a forecasting model almost right away. We were able to come up with a We were able to come up with a version one of that model, put it into an automatic pipeline, and develop a website where we could publish those forecasts, starting with just US states. And we got that online on April 5th. And then we followed that up with a global model where we forecasted for any country that had at least 100 confirmed cases and 20 deaths by April 29. Now we use that version one model up until the middle of the fall of 2019. The middle of the fall of 2020, when we released a new version that retained the general structure, but we had to account for some things that we weren't expecting, which I'll get into during our lessons learned. So from June 2020 to September 2021, we were making forecasts that we would submit to the ensemble effort, but also we would put on this public website. Everything in blue, these are targets we were specifically. These are targets we were specifically doing for the ensemble, and then anything outside of blue was what we were doing in addition. If you go to the website now, you'll see this disclaimer that we are no longer actively making forecasts, but if you are interested, you can still access all of the historical forecasts going to the model outputs. All right, so I'm going to describe version two in detail since this was. In detail, since this was the end model that we ended up with, we call it COFI. It stands for COVID-19 forecasts using fast evaluations and estimation. The underlying structure of the model is a dynamic susceptible infective model. So pretty simple bookkeeping from that perspective. We consider that the, I'm sorry, just trying to lose my cursor there. Oh, there it is. Trying to lose my cursor there. Oh, there it is. The number of susceptible people on day T is equal to the number of susceptible people yesterday minus the number of new underlying reported cases today. And then similarly, the number of cumulative underlying reported cases today is equal to that of yesterday plus our new cases reported today. We make the assumption that the number of new reported cases today is equal to a growth rate times To a growth rate times the fraction of the population still susceptible times the number of cumulative cases yesterday. And we can get an approximation of what the growth rate has been in the past, taking this assumption that it's the ratio of the cumulative number of cases reported today minus that value yesterday minus one. So in our model, the key parameter that we actually need to forecast that's going to be dynamic and changing over time. That's going to be dynamic and changing over time is the growth rate parameter. So, for the forecast of this parameter, we settled on a formulation of the growth rate that is a weighted combination of a rate that follows the past week's trend and a growth rate that would basically keep the number of new daily cases constant over what it's been in the past week. And the assumption behind this modeling choice is that as cases are going up, people Cases are going up, people are going to take action to curb their own behavior, whether well, either voluntarily or because they're forced to through government policy. And then, as cases are going down, policies will be relaxed. People will feel more comfortable going out, engaging in potential new transmission pathways. And so, effectively, just from behavior, we believe there's going to be the seesawing effect. And so, we were trying to capture that in terms of the growth rate with this formulation. This formulation. Now we would get estimates of what Kappa trend and what Kappa constant would be by for up until a given day of observed data, we would take the past six weeks, use the first four weeks of that past six weeks as training, and then reserve the past two for testing. And so for the Kappa trend, we would fit a linear regression with possible day of the week effects included to get what that Kappa trend. To get what that Kappa trend would be during our two-week testing period. For Kappa Constant, we only focus on what's going on in the last week of data of our four weeks of training data and calculate what the growth rate would need to be in the future to keep the number of new daily cases the average of what it's been over our training period. So then the forecasted growth rate becomes a blend of these two components. Rate becomes a blend of these two components, and that blend is determined by three tuning parameters. We have a cap on how large our capital trend is going to be, so basically, trying to keep the model from having the number of cases blow up. We have a sense of how quickly the forecast is going to revert to this constant. So, how that weight over what's How that weight over what's, if the forecast should follow what it's been in the past, how that weight will start to shift towards constant or more of a plateauing effect. And then the final parameter, phi, lambda is a function of phi here, it basically controls the tilt. And so in this example, if you were to combine these two forecasts in our training or in our testing period, you would basically just want to tilt it up. And so that's what that parameter. And so that's what that parameter allows us to do. So we can pick different combinations of these three parameters, plug them into our formula, and get a sense of what the trajectory of the growth rate would be during the two weeks of our testing period, and measure how far that is from what we empirically calculated. And by doing that over thousands of different combinations, we can create a joint probability distribution. So now with the joint probability distribution. Now, with the joint probability distribution, we have all of the pieces in play to produce the forward forecast. So, for each day where t is the last day of observed data, we go back and compute the kappa trend for the past four weeks, the kappa constant for this should actually just be the past week, not four weeks. My apologies. We draw a vector of our three parameters according to the joint probability distribution, which allows us to get a Kappa forecast, a growth rate. A Kappa forecast, a growth rate forecast over the next four or six weeks, if we're going that far. And then beyond that, it's just a bookkeeping exercise to run the forward model. So here's an example for the US in terms of cases. And you can see there are some good things, there are some bad things. I'll start with the good things. One of those is that it never really bottoms out. We never expect the cases to go to zero because of that seesawing effect that we've been. Because of that seesawing effect that we've included. And similarly, as you start to see it rise there around July, the cases don't explode and the model is able to kind of make that turn fairly quickly to forecast a downward trajectory. Of course, the big limitation when we watch this video is that now, as it's getting towards July, it doesn't anticipate this upswing. And there's really no reason why the model could have. Why the model could have because none of the prior data, which the model is only takes in, saw that. And so, this is a common, or this is a limitation of a lot of statistical models that we're trying to explore how we can improve that. The deaths model has a pretty similar flavor. We consider the deaths on a given day to be an instantaneous. Given day to be an instantaneous case fatality fraction of the moving average of daily cases for a window of size new, and that window size is a parameter that we're going to tune. And so as the growth rate was the dynamic parameter we needed to forecast for cases, the case fatality fraction is the dynamic parameter that we need to forecast for deaths. So similar to the cases, we calculate what that case fatality is. We calculate what that case fatality fraction has been for the past six weeks of data. But we do this under different conditions here. We do that assuming either we're going to use a moving average window of a week to five weeks in order to get that case fatality fraction. And so you can see empirically, depending on that assumption, how you get different values. For each of these windows, we fit a regression with potential day of the week effects here. In this example, Here in this example, none of them were significant, and so they're not included. And then the tuning parameters here, in addition to the size of the moving average window, we have a theta lower and theta upper basically bounds on what that case fatality fraction can be. And so again, we can pick different combinations of these three different parameters, see what the expected case fatality fraction would be into our two weeks of testing data, and get a joint crop of. And get a joint probability distribution across these three different parameters. So, here where darker is indicating a better fit, we can see that using a moving average window of size one week or seven days, that is a better fit for this data at this time. So, the deaths forward model works much in the same way where we compute the regressions for the case fatality fraction across those five. Case fatality fraction across those five windows for the past four weeks. We draw a vector from the joint probability distribution that we established in the previous step, and that allows us to get a forecast of what the case fatality fraction is going to be over time, and we can run the forward model. So, looking at the video on the right for US deaths, you can see over this time period in summer and into the fall of 2020, there was a really strong day of the week effect in terms of the U.S. Strong day of the week effect in terms of how deaths were being reported. And the model is able to capture that pretty well. And but again, we see those, you know, it takes some time for the model to adjust when the trends change direction, but the uncertainty seems reasonably bounded by some of those parameters that we put in there. Okay, so this is the model we came up with and that we. This is the model we came up with and that we used for about a year before we stopped forecasting. And so the question is: how well did it do? Previous presenters, I have talked about the weighted interval score. So that's what WIS stands for here. And so we're looking at the weighted interval score for our model for deaths totaled over the states just for the one week horizon. But if you were to look at cases or any of the other horizons, the results would. Or any of the other horizons, the results would be pretty similar. And we have the LANL model is in purple. And then what we're comparing here is the ensemble model. So the thing we know we're not likely to beat, that's in the lime green. And then the baseline model, which is what we hope we can always beat, is more in Chartreuse color here. And so what we can see is that the LANO model is often in between. model is often in between the two um which is which is what we would expect and and which is which is good there are periods of time where it um does as good as the base as the hub and there are times when it does worse than the baseline especially here right at the end during the delta wave and to be clear we didn't stop forecasting because we saw bad performance this is just you know This is just, you know, kind of how it went. So that's pretty good. On average, the LANL model was better than the baseline, but certainly not as good as the ensemble. But that does indicate that since it was consistently better than the baseline, that it was adding utility to the ensemble model. Now, compared to the other models that were part of the ensemble, our model was pretty middle of the pack. Middle of the pack. If here on the x-axis, we have the standardized rank, where a standardized rank of one means that a model had the best weighted interval score for a given location, target, and week. And so then the models here on the left, they are ordered by models that were most often in the bottom quartiles of the weighted interval score. The weighted interval score. So this one down here is most often has most often in the lowest quartile in terms of the weighted interval score for a location targeted meek. And this is in that lowest quartile the least amount of times. So as we would expect, the ensemble is up here at the top, which is good. The baseline is right here in the middle. And LANL is right a couple above it. So again, this It so again, this model worked decently well. It was providing utility more than just a baseline model, but lots of room for improvement. Okay, so now I'd like to shift gears to some of the lessons that we learned. Once we stopped forecasting, our team, we sat down, we kind of took a breath, and we tried to think about the decisions we had made, the lessons we had learned, and what we had. And what went well, what didn't go well, how would we do things differently next time? So, one of the lessons that we learned or that was kind of reiterated to us was that ensemble modeling works, but it requires early participation in a pandemic. And as we've heard, and I've already said, the ensemble model, this is kind of the key source of modeling support during the pandemic. Source of modeling support during the pandemic. It's the most robust. And we knew that from experience in ILI forecasting or seasonal flu forecasting. And so our belief in ensemble forecasting drove us to start publishing our model in April of 2020. And this is when we had kind of it like an 80% solution. We knew the model could be better, we knew it could be more refined, but wanting But wanting to contribute to the ensemble model as soon as possible with a reasonable solution was our goal. But this was challenging, and we did have some internal turmoil over this because if you think back to that time in 2020, there was a lot of attention that was being paid to forecasts. Some of that press was negative, some of that press was positive. But the negative press, that was very discomforting. And it took kind of some. It took kind of some, I guess, courage to put yourself out there, to put yourself forecasting and putting down what you thought might happen. And so that could have been a barrier for some people from early participation and is something that we may want to think about in the future. Some of the other reasons why we were able to participate as soon as we could was that we did have prior experience in forecasting since we had participated as an institution in seasonal forecasting for. In seasonal forecasting for a couple of years at that point. So we had a general idea of the workflow, what the rules were going to be, the objectives. We were able to partake in some of those early conversations about how everything would come together. And so that definitely helped us kind of hit the ground running. In addition, we had access to on-site computing resources that allowed us to forecast at geographic scale. As the pandemic went on, As the pandemic went on, institutions like Amazon and maybe I can't think of the other one, but they started offering computing resources to researchers, which allowed them then to kind of access and do forecasting at scale. But having that, you know, right next door, metaphorically, that definitely allowed us to get in earlier. And then finally, we had team expertise in people who could. In people who could develop these automated pipelines because it takes a different skill set from generating the forecast to putting this into something that can run every night, that goes through quality control checks that gets turned into visualizations. All of that takes a kind of a special combined skill set. And so these last two points, we had never previously done that for infectious disease forecasting, but we at least were able to pull. But we at least were able to pull in kind of the right people in order to get that done. And any of these could have been barriers for people participating in the ensemble. And now, as a community, we have a lot more experience in all of these areas. And so hopefully for the next pandemic, that timeline of four months won't be quite as long. And that's, I don't mean to say that that was slow. That was by no means slow. That was an amazing effort that took a lot of people. That took a lot of people's volunteer time, and I think a lot of people to be commended for that. But we can hopefully make it shorter next time around. The second lesson we reflected on was this trade-off in forecasting of building a model that had flexibility and building a model that had explainability. And Carrie was just talking about some of this with the trade-offs between mechanistic and statistical. Trade-offs between mechanistic and statistical models. That's been a very common theme of this workshop. But here, I'm going to focus just on statistical models and building in some explainability within statistical models themselves. And so in statistical models, a lot of the explainability can come from the additional data that you're using in your model, in addition to the historical case data. And so you can use things that may help be useful. Things that may help be useful for explaining how an outbreak's growth rate changes in response to public health actions or changes in behavior. And so, a lot of new data sources were becoming available over the course of the pandemic, like mobility, public health orders were kind of being captured in a quantitative way. All of those things could go into a model to help you explain what may happen with the growth rate. Coffee just relied on historical data. We prioritized parsimony. We prioritized parsimony, we ignored all of those auxiliary data streams, and we did this for a few reasons. One, we had seen, as Michael had described, that in past challenges, kind of simpler models had outperformed. So we kind of assumed that that would be the case here. And then logistically, we didn't want to have to worry about things like the consistency and reliability of the auxiliary data. Worrying about the consistency and reliability of the case data was challenging enough and now added. Challenging enough and now added kind of more uncertainty there. The cons, though, were that we had no way to hard code changes or take advantage of conditions that were changing as we became aware of them. And so as we knew vaccines were being rolled out, we knew that would impact our forecasts, but there was no way to input that information. And so we just kind of had to sit and watch and wait for the model to catch up two, three, four weeks. Catch up two, three, four weeks later. And then, same thing with when a variant comes onto the scene. We know it's arriving in a given location, we know it's going to impact cases, we know our forecasts are off, all we can do is wait. And so that's a big problem that we would like to address. And in terms of the variants takeovers, we are trying to do that. And hopefully, I'll get a few minutes to talk about that at the end. But another con was Another con was that we have this distinction in our minds of the difference between projections and forecasts, but decision makers it's not quite as clear. And oftentimes they really want to be able to test out different scenarios to weigh different modes of action. But they want a validated model. And so kind of there was a lot of tension there. And so I think thinking of how we can bring the strengths and weaknesses of both of those. Can bring the strengths and weaknesses of both of those sub-disciplines together would really benefit the field going forward. And things like the scenario modeling hub, those are great starts. And I think there's room for also in forecasting challenges to try and incorporate counterfactual scenarios as well. And then the third lesson I want to talk about, this touches on some of the decisions we eventually reversed and considers why. And considers why we made those decisions in the first place and what can we take from that. And so, some, but not all, of our early decision making was influenced by our prior experience forecasting, a single seasonal disease that only had one weekly update. So, the first example of this is forecasting a peak. So, we spent a lot of time, I say a lot of time, the pandemic is much larger than Is much larger than just those early few months. But in those early few months, we spent a lot of time thinking about forecasting a peak. And this was in flavor of the ILI challenges. But we stopped putting out a forecast for the peak as early as June 2020. And we did this for several reasons. One, from an operational standpoint, it turned out to be really challenging to come up with a robust definition that could be automated when you have. Automated when you have really noisy, kind of chaotic data that's coming in. And so at the beginning of the pandemic, all of the systems were just learning how to collect this data. And so you were having data dumps, you were having irregular reporting. We were also seeing kind of different behavior than we expected with week-long plateaus in New York. And so it was really hard to come up with a definition of a peak that matched our intuition. Match our intuition. You would get like a really big data dump, and it was, you know, the probability assigned to that being the peak was so high, and it took a long, you know, a long time to move beyond that. So operationally, it was challenging. The second decision, or the second reason why we reversed and stopped producing a forecast, was that we realized we may be giving the wrong impression. And so this is more of a communication error, but we were giving the impression that there would be one. We were giving the impression that there would be one and only one peak, and that once it had been reached, things would get better. And while we might have known this before, now we really know it. But with a novel disease in which the population is entirely susceptible, behavior and not as much depletion of susceptibles will drive waves. And this is before you get into the messy area of variance, but just before evolution has had time to wreck havoc. Wreck havoc, it's going to be behavior. There's no such thing as kind of a peak in the seasonal sense. The second decision we reversed was going with the hierarchical architecture. This is a modeling point, but this is kind of the main difference between our version one and our version two. Our version one had this hierarchical architecture because we had done this in seasonal flu forecasting. But what happened was that as the number of geographies That as the number of geographies expanded, as the historical time series for COVID expanded, this model got slow. We couldn't run it frequently. It was really hard to do some testing on model improvements. And so ultimately, we had to abandon this architecture and go with something that was faster. And so thinking critically about the timeliness that is needed during a pandemic versus a seasonal disease, if we had thought about this, then we may not have used this architecture. We may not have used this architecture going forward, which would have saved us a lot of headache. So that's just to say that at each kind of with each disease, forecasting questions and scope are context dependent. Okay, and so for the next few minutes, I just want to talk about how we're trying to improve this forecasting model or models in general, and that's incorporating the variant dynamics into forecasting. Forecasting. And so we're both aware of these two problems. On the problem on the left is that the continuing evolution of SARS-CoV-2 has led to a series of waves where we have different variants. It looks like the letters didn't translate here, but we've got Omicron, Delta, and going back forward, back in time, gamma and alpha perhaps. And then the problem on the right is that even the ensembles of epidemiological forecasting models, Demological forecasting models, they're unable to accurately forecast at these key transition points when new variants emerge. And so, the question and the hope is that if we can define and use the relationships between variants and cases, we can capture these key inflection points. So if we categorize that relationship, or categorizing that relationship falls into two challenges. The first one is: okay, now we've given the task of forecasting something else. We need to now forecast. Else. We need to now forecast what a variant takeover is going to look like, taking into consideration the competition that's going on in the viral population. And then the second challenge is, okay, we know a variant takeover is occurring, but what does that mean in terms of cases? A takeover doesn't necessarily mean a huge spike. And if it means a huge spike in one location, it may not mean the same thing in another location. So, figuring out that relationship. Now, these downward arrows are most likely Arrow arrows are most likely also impacted by things like susceptibility, age structure, mobility. And so now this is getting into that question of: you know, is there a simple statistical model that can define this vertical relationship, or do we really need more mechanistic understanding of things like susceptibility, the immunological history of the population in order to define that relationship? And my suspicion is that you do need these mechanistic understandings here. Mechanistic understandings here. And so I was pretty excited by Jen's talk earlier as they were able to capture this in a model. As Rick mentioned yesterday, these takeovers of variants really look like logistic curves. And so we've taken that approach to fitting the proportion of a variant in a population in a given location with logistic curves, which allows us to get two parameters: the speed, which is K, and T0 here, which is. And T0 here, which is the defined midpoint of this takeover. And so, looking at all the geographies that have sufficient sequence data, we can get estimates of these two different parameters. Here on the right, we're looking now at the heterogeneity between the U.S. as a whole, if you were to consider it as the whole country, or if you broke it down to individual states. And of course, we see lots of variation there. Some states are slower, some states are faster. And heterogeneous is always challenging. So now that we have those two parameters, we can compare them across a variance and also space to see if there's any consistency. So on the top, we have that for delta, and then on the bottom, we have Omicron looking at the K parameter that we fit. And we can see that with Omicron, That with Omicron, that parameter is estimated to be faster in certain locations than Delta was, and that jies with all of our experience. But interesting, you do see a lot of this sublevel heterogeneity in certain locations. So Brazil, the US has some sub-level heterogeneity, Canada, India. And then in terms of arrival, so we're using T0 here as a proxy for arrival. Proxy for arrival. So, from when, so Delta emerging from India, it took longer to make its way around the globe, whereas Omicron emerging from South Africa, you see that those T0 time points hit much more consistently across the globe. And so, with these two parameters, we've started looking at can we explain the speed and the timing depending on country level or sub-country level. Depending on country level or sub-country level characteristics, like number of vaccines, number of cases, kind of recent past, all of these things that we think may play a role. And we've tried doing this in a statistical and statistical models and with some machine learning. And what we're finding so far is that, no, we really can't explain these yet. So there's more work to be done there. And then the last thing is tying this variant takeover. Is tying this variant takeover to case waves. And so we've found three different patterns if we consider the Omicron wave. The black line here, these are cases, and the colored lines refer to different variants. So green, delta, red is Omicron, and we have the two sublineages, BA1 and BA2 here. And so even though a complete takeover is taken in each of these three different countries, you see different case patterns. And so the question is: is there information? The question is: Is there information going on here in the variant population in other country-level characteristics or geographic characteristics that can give us an indication of what is going to happen with the cases? Because that is the real key to tying variant dynamics to better forecasts. So, hopefully, in the next few months, we will have some results. So, to conclude, we think that forecasting models, they should We think that forecasting models they should be fast, they need to be able to be run frequently, and they need to be flexible to account for any situation that you aren't prepared for because pandemics will present a whole new, will present things we really hadn't even thought of. Ensemble efforts continue to drive forecasting collaboration. We've seen that through this whole pandemic. And so, progress should be made not only in how do we make the best ensemble. Make the best ensemble, which a lot of people are looking at, but also how do we lower the barrier to participation? And then the final point is that hybrid models will probably bridge the gap between the current limitations of statistical models in terms of not just in terms of forecasting performance, but also they may better address the needs of decision makers who want that flexibility to look at longer-term horizons. And so with that, I'd like to thank the So, with that, I'd like to thank the large number of individuals behind these efforts, specifically those involved in the COVID-19 forecasting model development and program management, and then as well as the individuals who are helping to explore this idea of incorporating variant dynamics. And I will take any questions and I'll stop sharing my screen at this point.