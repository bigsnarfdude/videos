So, it's a light board talk. Julio Bakhov, this is this guy who is the chairman, as you might know. It's joint work with Matthias Begelberg and Bertram Chiederer, who is a PhD student here in Vienna. And, well, I will of course explain everything appearing here in the title during the talk, but let me first start. Let me first start in the 18th century, so we all know of course here our hero Munch, who before the French Revolution took place, was considering everybody knows the picture here. Here is Bieu and here is New and there is a huddle of sand and we transport it. Okay, what are What I want to emphasize, so the classical problem is there is a static version and there is a dynamic version. Okay, what is the static version? I can write it in probabilistic terms like this: expectation under pi of x minus. Of x minus y squared. This should be minimized. Okay, and here we have, I think everybody knows it. We have mu and mu are probabilities on R D with second moments. We suppose that mu is absolutely mu is absolutely continuous with respect to lambda and then you have pi is a joining of x and y i don't explain too much because we shall not need it the point is that since the work of gangbo mcan and benamou brunier to the static problem there is a dynamic problem and i write it again in some probabilistic terms like this like this you take the expectation of what of integral from 0 1 of dt squared dt you minimize this okay and what is the bt what we want is we have a stochastic process starting at at time zero with distribution with distribution mu at x at time one it is like mu and it should be of this form that dx is equal to b t dt okay bt's are some directions in Rd and as we all know the best thing to do is to have straight lines here whether Bt Straight lines here where the bt is constant, and this is well understood. I told you about Gangbo Meccan and Benamou Brenier. This gives the same thing. Yeah, maybe I should mention Monge already was considering one of his results was that when you transport these grains of sand, they will never cross. Sand, they will never cross. This in R2 he has already established, and this is a little bit already in this direction of dynamic thinking. Now, little, almost trivial observation. When you take here any constant C, okay, C is any constant, then the problem does. Constant, then the problem does not change. You do an easy calculation, and again, you get the optimal Pernamou Brenier transport along the straight lines for any C in fixed C in R. And so the point is that we have a lot of different things that we have. The point is, in this case, that you try to make the transport, which is as close as possible to a constant transport. And this brings me to the idea of stretch Brownian motion, which appears here in the title. And I will not need this again. I will not need this again. This was just the motivation that we try in classical transport. We try to make the transport which is closest to constant speed transports. What is the analog for Martingale transports? So you see, this is the classical way. The classical way of cleaning the blackboard. So we are passing to Martingale transport. Again, I don't have to tell a lot because this was done already in some very nice presentation, previous presentations. I have to clean better here. So, what is our setting? We have mu and mu in again in measures on R D with finite second moments. And we suppose that is smaller than μ in convex order. And now we. And now we have the martingale transports. We define what are the martingale transports from mu to nu. Well, these are all the pi's which are transports from mu to nu. Everybody knows what is meant by this. And pi is of the form integral pi x d mu x. Every transport from μ to ν has such a disintegration and such that the barycenter of pi x is equal to x for every x. So these are the martingal transports and the result of Strassen in 65 was that Was that this set is non-empty? So there are such martingale transports. Okay. And now a lot of the things has been done to understand the structure of these martingale transports. We have heard here nice talks on this. And a lot of things are known in D equal one. known in D equal one, but we shall focus on D greater or equal to two. Okay. And here I will define what is a stretched Brownian motion, the stretched Brownian motion. So this is again a paper by Julio, by Matthias, by Martin Hussman. Martin Hussmann and by Sigrid Kehlblat. I think all of them are in the audience, which appeared in 20 on the stretch Brownian motion, where they introduced this. And the idea is analogous to what I just told you about the Benamou-Brenier approach. So stretch Brownian motion. We try to find the We try to find the element pi and call it with an SBM here, which is closest to Brownian motion. Okay, so how do we define it? Again, we have a dynamic point of view and a static point of view. Let's start with the dynamic point of view. So here is dynamic. What we try, we try to look at martingales. M is a martingale. It starts as before at time zero. At time zero with law mu, at time one with law mu. It's a martingale, and it should have dmt is equal to sigma t d, say b t. B t is a Brownian motion. Sigma t is any predictable process a priori such that this makes sense and this is really. Make sense, and this is really a martingale, L2 bounded martingale, because we have second moments here. And if we have such a martingale, then what we try to minimize, we try to minimize the expectation of integral from zero to one. And we write it like this, just sigma t minus. minus i and this here squared dt so dt is a d-dimensional brownian motion so sigma t has values in the d by d non-negative definite matrices this is the identity matrix and this is very This is very intuitive that we try to compare the sigma t. It should be as close as possible to I. Well, I, of course, would be Brownian motion. And in general, of course, we will not find a Brownian motion. And what we try to do, something which is very close to what we have considered before. Here is the I. As before, it would matter very little whether we take I or whether we take 10 times I or one half times I. This is just a matter of scaling. Okay, so the nice thing is that this makes very good sense also in the high dimensional case. And there is a static problem which is attached. Which is what is the norm on the matrices? What is the norm on the matrix? Thank you very much. So, this is what they call the Hilbert-Schmidt norm. So, it's the sum of the squares of the eigenvalue. Thank you very much. Okay, now I need some space here. So, what is the static version? The static version is we try. We try to minimize the squared Wasserstein distance from pi x to gamma x. Okay, we have to integrate over d mu x and this should become minimal. Okay, and where our pi they are. where our pi layer of this form pi x, the mu x is a martingale transport from mu to nu. So this is when you translate this and play a little bit around, then that's the static version and you see there come There come this optimal transports in usual distance in Wasserstein distance, okay, from pi x to gamma x, and gamma x is just the standard normal random variable with unit variance and centered at x. A little technical remark in the paper by I have given the initials of the authors. You find the references in the abstract, which is on the program. Okay, whether you put your gamma centered at x, where pi x is the very center of pi x, or whether you put, for example, Or whether you put, for example, gamma at zero does not make a difference for the minimization problem. Okay, now let us have a look what this stretch-Brownian motion can do for us. So once again, this is Barkhoff, Bagelberg, Hussmann, and Kellblatt 2020. Okay, the results. Okay, the results are. Well, first of all, there always exists a unique optimizer. And that's remarkable, because we are, after all, or it's interesting for D greater or equal than two, where it's not always so clear that you have a dual optimizer. Another thing is. Another thing is static is the same as dynamic. What do I mean by this? When you have a solution to the dynamic problem, then you naturally attached is this thing here as the joining of M0 and M1. And also the other way around, when you have the solution here, you find the martingale. Martingale. So this thing here, another thing, the optimizer and the dynamic version, the Martingale version is strong Markov. And another nice thing is we have consistency. What do I mean by that? What do I mean by this? Well, this is also going back essentially to Mons, when you have these two huddles. You have the two huddles and you bring them continuously. You transport one into another from time zero to time one. When you look at the huddle at time, say one over four and how it looks at time three over four. Time three over four, then again, this is the optimal transport from the huddle which has the shape at time one over four and the huddle which at the time three over four. So it's consistent up to rescaling and that's the same thing here when you take here the martingale and you look at the law of the martingale at time one over four say etc then again the martingale gives you Martingale gives you the stretch Brownian motion up to some obvious rescaling. So this is very nice. And what I shall do now, we shall analyze the static version here and we will see what this can do for us.  Let me start with an easy example. So, when we have here, we have the x-axis, and we have here, say, delta at one, and here. Here we have geometric Brownian motion. So this is my mu and this is my nu, which is the law of e to the power gamma to the minus one half. We're in dimension one, d equal one for the moment, just to give you an idea where gamma is a standard Gaussian. Standard Gaussian, okay, then it is clear what the joining is. There is only one multi-gal transport from here to here. What is not so clear is what is the dynamic solution. So the dynamic solution is some diffusion, and not too surprisingly, it is just the geometric Brownian motion. And so again, this remark applies when you take intermediate. When you take intermediate times, this is a non-trivial example how such a stretched Brownian motion looks like. Now let's call this mu plus and mu plus and mirror this thing at the point zero. So call this mu minus and the mirrored thing here is mu minus. Okay, then of course. Okay, then of course you have the same solution here. But the interesting thing is when you, or relatively interesting, when you take one half of mu plus and mu minus, and you take one half of mu plus and mu minus. Okay, then of course. Okay, then of course again there is in fact there is only one martingale coupling between the measure mu, this is now my mu and this is my mu between mu and nu. And you see that these two intervals, they don't communicate, they don't speak to each other. So all the trajectories have to go here and all the trajectories and all the trajectories have to go at i minus which is the negative axis okay so i resume that there are two irreducible components and which is i minus which is r minus and i plus equals r plus so when did i show So, why did I show you this very easy example? First of all, for later use, and second, to introduce this idea of components or pavings and irreducible. I don't formally define these things yet, but it's clear that the problem decomposes into the I plus and into the I minus, and you have. Minus, and you have to solve it on each of these irreducible components separately. Now, this goes back to a paper by Matthias Bageberg and Juyet in 2016, which really started this whole business. And in the one-dimensional case, it is very clear whenever you have a general situation of a mu and a General situation of a μ and a ν, then you can find at most countably many intervals, and these intervals are irreducible, etc., etc. This is all perfectly worked out, and there are little surprises at this stage. There are many other deep results in this paper, but this is at least on an intuitive level very clear that in general you have a decomposition here. Decomposition here into irreducible components. But the problem is, this was the case d equal to one and the case d greater or equal to two. Here, the question is, how are these irreducible components, how do they look in the higher dimensional case? And there are three papers which I want to mention. Three papers which I want to mention. There is Gusoup Kim Lim, the pioneering paper, which was the first here, 2016. And then there is Obloi and Sieur Paes. And there is Desmarches and Tussi. They were put on archive 2007. And the point is, there is a lot of overlap between Overlap between Oblois and Siora Pais and Demarche Tussi, and they mutually recognize. Apparently, they were surprised that the other group had done very similar things and they acknowledged this mutually. Okay, so before I go on, I want to make a little interruption, something slightly different. Something slightly different. I was quoting here my good decayed old friend, Nassif Gusoup. And at this stage, I would ask as many people as are willing to follow me to switch on their microphones. And those who are in a presentable outfit, please also switch on your camera because I. Because I want to make clear that Nassif was the driving force behind birth. It is so obvious for us that we can be, yeah, some of the lucky ones, they are physically in this beautiful banff. But this didn't exist always. It was Nassif with infinite energy who started it from scratch, who developed it single-handed. Who developed it single-handed, who made it not only in Canada, but then there was Oxaca, which I also enjoyed already very much. There came China, there came Spain, etc. And Nassif, I really think you did such a great favor to the international mathematical community. I mean, judging myself, I've been to such a number of events and every Number of events, and everyone was really very fruitful. And please, everybody who has switched on, let's make a big applause for Nassif. So, end of the commercial and coming back to the back to the to the question here what is how do these uh irreducible components like uh look like in the uh higher dimensional case so there are as i told you already these two papers are very similar but there are two points of view the one is uh measure dependent Dependent partitions, and here are universal partitions. Now, these are two different points of view. And to tell you, what I'm aiming for is that the notion of stretch-Brownian motion can somehow make a link to Somehow, make a link tuition between these two points of view. I will explain what is meant here with the help of an example. Let me Okay. So what is the example? Take a circle in R2 of radius one. Okay. And what you do, and I tell you right away, the mu will be the Lebesgue measure on the sphere of radius one. So it's uniformly distributed on this circle here. Okay. And now you take here, make a line here, okay, such that it's not rectangular, just a fixed angle, okay, and draw it in two directions. So this has a length A, and this should also be A. Should also be A. And now make two circles. Make this circle here and this circle here. Okay, so that this has radius, say, capital R and this has radius little r and the new is equal to one half of Lebesgue of the Lebesgue of the sphere with radius big R plus Lebesgue on the radius with little r. Okay, so how do the martingale transports from mu to nu look like? Well, there is an obvious one, what is, I call it p lin from From mu from mu to new, I write it like this: p lin in the Martigal transports from mu to new, namely each point here, each point in the sphere, you You ether transport it to here to or here. So you flip a coin whether you go here or here. And this you do with the same angle at every point. And this is, of course, gives you a wonderful martingale transport from mu from the yellow circle to the two blue circles. So this is not very difficult. The point is. The point is, when you look at this, you have plenty of invariant sets, namely all of these lines. So there are continuum many such invariant sets. But they are not unique because they depend on the transport. So, for example, you could also, you could mirror this, you take the same angle, but you do it the Angle, but you do it the other way around, okay. And here, okay, then this is also a transport, but now these are different invariant sets. Or making it a little bit more complicated, you first flip a coin whether you do this or you do that. And so, in other words, with probability one-fourth, you take either of these four points by choosing. By choosing special angles, you can make periodicities, you can make it very complicated. Okay, the point is, this depends on the partition depends on the measure, on the martingale transport, and as this example shows here. Now, this here is a different one. This is we want to have the finest partition, which Partition which is invariant under every martingale transport. This is what they had in mind. There are different applications. In Gusub Kim Lim, they had special optimization problems involving the Euclidean distance in mind. And there it's very natural to apply this point of view to the question. To the question of invariant partitions. And this looks for the universal version. Okay, so what is the universal partition? Yeah, you can, for example, when you take the stretch-Brown emotion, if you have it already, then one can prove that this is some diffusion here, which is stopped when the first time it hits this. When the first time it hits this or this blue circle here. And this also gives a Martingale transport. But this Martingale transport, of course, has no non-trivial invariant sets. And therefore, the universal partition of DeMash-Tussi or Obloi and COPA's is trivial in the sense it consists only of one set. One set. Okay, now how do we relate this to the stretch-Brownian motion? Okay, now first of all, I should, yeah, before I go on, this is clearly, this phenomenon only turns up for dimension bigger, two or bigger in the one dimension. or bigger in the one-dimensional case you don't find such a uh this this phenomenon doesn't turn up and there is no difference between these two notions okay now let me work out now the excuse me louder i assume you cannot you cannot read the chat but uh nasif kusoup says that the check is in the mail for you aha aha okay Check in your mail, just as you know. Okay, thank you. Hi, Nassif, by the way. So first So first of all let's start with the definition okay so we have our mu and our nu as usual on Rd and in convex order. Okay, we say that this is irreducible. That this is irreducible if there is if there exists, yeah, like this for every A and B, say borel in R D, such that mu has positive measure and mu has positive measure b there exists. There exists a transport, Martingale transport from mu to nu such that pi of A cross B is positive, strictly positive. So intuitively, from every non-trivial set A, you can travel to the set B with positive probability. B with positive probability under such a Martingale transport. So, for example, when you think of this diffusion from the yellow circle to the blue, two blue circles, and you take A any non-trivial set in the yellow circle and B any non-trivial in the blue circles, then with positive probability, you can travel there. Okay, and now. And now, definition. What is this Demash IIsi decomposition? Now, this is the finest partition I X in. X in R D of relatively open convex sets okay such that X is in IX For mu almost surely and and support of pi x is in I of x bar again for mu almost every x. So when you think of the partition into the two geometric Brownian motion Geometric Brownian motion. I mean, in this case, you have two such i's, the i plus and the i minus. They are, we index them by x, which means for every x we associate the ix. It can be for two different x's, it can be the same ix, namely all the points which are inside. And whenever you have for every pi, which is a martingale transport. Martingale transport from nu to nu, the Ix are invariant so that it can only transport to the closure here. And what Dimash Tussi have proved, and this has a strong overlap with Obloi and Cior Paes. COC by A's, they have very similar things here. So they match 2C, okay, and also obloi and 0 base. Okay, but I quote the version of Dimash Tussi here. Okay, the first point is this IX is well defined. The second thing is that x goes into Ix is measurable. Now, what does this mean? The IX, these are the relatively open sets. You have to put the topology on it. This is the Basement topology, which was also considered in both of these papers. This gives you a measurable structure. Measurable structure and then it is analytically measurable. So, in particular, new measurable bottom line, highly non-trivial bottom line, it is measurable. And the next thing is there exists a pi hat in MT mu nu such that for mu almost all x we have that the pi hat of pi hat. Pi hat of x, the support of this and the closed convex hull is equal to the i of x bar. So in other words, on each of these ix here, you have when you start from a point x in Ix. Then it can only travel into the closure of this ix, and this holds true for μ almost all x. Okay, so this is an abstract construction, so you should imagine this like this. So you have a paving of Rd into convex sets, something like this. They can also Something like this. They can also look like this. They could look like this, for example. And this is, in fact, appearing as an instructive example in the Mashtusi paper. So this is the open triangle. And then the line, which I mean, this is just the side of this triangle, could appear as another IX, okay, without the two corner points here. Okay, these things may happen. This can be very complicated. This can be very complicated. In particular, of course, nothing has to be countable as in the one-dimensional case. And this, the construction in both of these papers is abstract. And now, my question is does the stretch process? The stretch Brownian motion does the job. So, what does this mean? Is the stretch Brownian motion, is it such an element which has exactly these properties? And do they maybe tell us something how you get this partition Ix, etc.? Etc. And here is the short answer. The short answer is yes. Okay? And in the remaining 10 minutes, I will briefly tell you what this short yes means more precisely. Okay, and So So we are back to this minimization problem, which is the stretch-Brown emotion. I call this P mu nu. So this is P for the primal problem. We try to find the Martingale transport, which optimizes this functional here. So this is. This is a generalized transport problem as was analyzed by Groslan and his co-authors. So there is a lot of work on this. Okay, and now I will tell you what we have found. So here's a theorem, and this is joint work with the authors. Work with the authors I have mentioned: Julio, Matthias, myself, and Bertram Chidera. I think they all are in the audience. Okay, so we first consider the case where we have only one component in the DeMash-Tussi paving, and then we consider the general case. Okay. Case. Okay, so mu absolutely continuous with respect to nu in PRD. No other assumptions. The following are equivalent. Okay, so first the demas to see paving. Having consists of one element only in the sense for mu almost all x, the ix is the same. So in the case with the two circles, this would be the Ix would be the interior of the outer blue circle. Okay. Okay. So two mu nu is irreducible. Okay. Mu nu is irreducible. I gave you the definition before. It means that from every set you can travel to each other set with by some martingale transport with positive probability. Positive probability three. Okay, pi x s b m so remember the optimizer of this thing here is the stretch Brownian motion, which where the pi consists of these measures pi x s b m. This is in fact equivalent to nu for mu. For mu almost all X. So think again about the example with the three circles. So when you start from any X in the middle circle, okay, then you end up either at the inner or the outer circle. And for every X in this case, you end up with some measure which is equivalent to nu, which is very intuitive. Into new, which is very intuitive in this example. But this is, of course, a special form. In particular, this implies the property which was obtained in the abstract setting by Dimash and Tussi. And four is there is a dual. Dual optimizer to P mu mu. So here I didn't tell you yet what a dual optimizer is, but the p mu nu, of course, this is a convex optimization problem over a certain set. And there is a, well. Well, the dogs were by Pavlov, I think. When you have a certain situation, then you automatically do something, you write a Lagrangian and you pass to a dual set. And the question is, yeah, it's very instructive to analyze the dual set. It was already proved in this paper by paper by Bakhov, Begeberg, Hussmann and Kellblab that there is no duality gap. And then of course you try to find a dual optimizer and this is equivalent here. Of course the idea of the dual optimizers also appear in the three papers which I have mentioned before. Mentioned before. So one morning, we found it quite hard to prove this thing. It looks very intuitive and I thought, oh, okay, okay, this should easily be possible. It was, it's true, but it was hard work and there's a lot of convex analysis involved in doing so. I have to admit, we have not yet got the paper in the form to put out, but it should be in the A form to put out, but it should be very soon, it should be on archive. And I will, yeah, time permitting, I wanted to tell you more about the dual optimizer because there you learn a lot of structure of this question of stretch Brown emotion. And when we were working on this, I fell in love with this notion of stretch-brown emotion. Of stretch Brownian motion. And I think it's a real beauty and will have still lots of application in the D-dimensional case because it always works also in the D-dimensional case. And now let me give you the second theorem. And with this, I will finish. How do we do it for a general demage to? General Demash-Tussi paving. So here is another theorem. Okay, so again, we have the mu is in convex order with mu and and let Ix. I X X in R D vid des mach 2 C paving, which is not general. And as I told you before, it can be very complicated and you cannot do anything with countability. So the first thing is that you have You have that the support of pi x spm and the convex hull of this thing here is equal to i x bar for mu almost each x. So, in other words, what has been done abstractly. has been done abstractly in Dimash Tussi, and this also has been done in obloisiopace, you get from the stretch-Brownian motion as a concrete manifestation. And the nice thing, and I will just give a somewhat cryptic sentence: any dual Dual optimizing sequence. I have not told you what the dual problem is. So we always have an optimizing sequence. As we have seen before, we have an optimizer only in the case when there is a trivial dimension. To see decomposition, okay, induces the Dimash-Tussi paving as the domains of convergence. Of appropriate air fine perturbations. So the problem is: can you pass when you have an optimizing sequence of optimizers? Of optimizers, for lack of time, I cannot tell you how they precisely look like. These are functions on Rd, and the question is, do they converge on the elements of the Dimash-2C paving to the optimizer localized on each of these components I of X? And the answer is. And the answer is yes. And it does so for mu almost each of these I-axis. And it is precisely the domain of convergence where you can start with one dual optimizing sequence which works globally. And for every Ix, you have to add different affine perturbations. This again makes contact with an idea of idea of appearing in oblois and theopace where they also consider the affine perturbations of such functions. So I just wanted to give an outlook what kind of results we have obtained here. And the bottom line is that the That the notion of stretch Brownian motion gives you a very concrete hand at the Dimash-Tussi decompositions. Thank you very much. Thank you, Walter. We have time for some questions. Anyone? I have to look down because I only see you down here. Down because I only see you down here, but I should look into the camera. Yes, Martin, please. Hi, Raja. Thanks a lot for this very nice talk. So a very vague question. So in dimension one, as you explained, there's a very intuitive way of calculating these irreducible components using potential. These irreducible components using potential functions, yeah, you read it off from the potential, yeah, yes. So now that you know that stretched Brownian motion essentially induces this paving, the mash-Tusi paving, paving, would it be possible to construct potential functions maybe using some kind of generator of the stretched point emotion. The stretched bond in motion in maybe to construct them this paving. I mean, like in one dimension, you mean the potential functions are used using the Newtonian potential, so kind of something like the green function of the Laplace and so of the Brownian motion. So, now can you do something similar here, or would you expect well, of course. Well, of course, there is a problem. There is no analog of the potential function in higher version. Now you can say, in a way, what I tried to sketch here, and of course I did not explain the details here, is that this sequence of optimizing functions somehow uh somehow gives something uh a similar information as the as the potential function maybe i come back to the first problem where i had the two geometric brownian motions okay on each one on i plus and on i minus you find uh unique uh dual optimizers these are essentially convex functions okay Convex functions, okay? But you cannot paste them together. You cannot paste them together because you lose the convexity and simply doesn't work. Okay, now the idea is that you put, when you want to have convergence on one side, then you put a big affine function here, okay? And here it converges to plus infinity or something like, yeah, necessarily to plus infinity, but here it gives the good convergence and The good convergence, and the other way around, you plug it up here. And this does not only work for these two, it works in perfect generality. This is what I wanted to say here. Now, when you consider this as a kind of analog to the potential function or where it hits the zero, that you have points where you get in the limit, you get somehow kinks in the... You get somehow kinks in the optimizing sequence, then you are most welcomed. Okay. Yeah, that's okay. That's one answer, yes. Okay, thanks. I have a question. This is Nasif. Hi, Nafif. Walter, nice talk, as usual. And thanks for the shout out. For the shout out, um, it's it's really these things are coming nicely together, but um, you know, the original problem is when the cost is just uh Euclidean distance. This is the problem you treated in your paper, yeah. Yeah, Damon, this is universal in many ways. Um, and so what does it say? What does this decomposition or Decomposition or a new insight into the decomposition gift on the original problem. And we did this because we wanted on each component the dual is attained. And because of that, you can have a characterization of the support of the automatic transport of the original problem. And is there a way you can relate? Relate. So you have a pie that's coming from a different cause. Here you don't even mention the costs, right? Well, the cost is this optimization problem here. I mean, in Obloisier Pays and in Dimash Tusi, they don't mention a cost, they analyze all the martingale transports. That's what I'm saying. I mean, it's independent on the cost. And suppose now you have a And suppose now you have a fixed cost, which is really the Euclidean distance, and you have an optimal transport, pi, you would like to characterize its, you'd like to analyze its supports. And our conjecture is whether for each fiber, you know, it's sitting on the extreme points of the fiber, right? Which is the case once you restrict to each component. And is there a way to relate the optimal? The optimal mass transport pi for any cost C to this universal decomposition? Can Brian Motion tell you how to relate pie coming from a different problem to this pie coming from this problem? You see, it's well, as I told you, you can cook up for you can you can cook up for for for for these examples with the three circles which i which i showed you can cook up artificial cost functions for for which these examples uh the one which generate huge uh a huge number of invariant sets uh for which they are optimizers so therefore even in general when you find an optimizer it it need not have any nice properties but Properties, but it's okay. So, this here was the point of view that you tried to do it in the universal way. But, Nassif, I think we should come together and work on this. No, but also, it reminds me of something else in a regular mass transport, the classical case where the Mohs original problem, there's a proof of whatever Gangbo and Caffarelli and Gangbo and Caffarelli and McCann, etc. did is that you first optimize using the classical distance function, but then you use a secondary optimization using the quadratic function to analyze really the support of. And this seems to be similar, like that this is a secondary optimization. This is a secondary optimization of the original optimization, which is uses Euclidean distance. So there is some similarity with the classical transport problem, the solution of the Mosh problem, the original problem. But that's all I can say. Thanks again. Okay. Thank you. So in the interest of time, and so everyone can enjoy their coffee, we stop here. Thank you again, Walter, for beautiful talk. Again, Walter, for a beautiful talk. Yes, we reconvene in 20 minutes or so, correct? Thanks, Mario.