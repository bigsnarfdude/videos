I'm going to be presenting some recent work done with my advisor, Mike and Angela, here today, on this problem of policy learning under multiple, when you have multiple outcomes of interest. And where policy learning, the goal is to try to learn an optimal treatment allocation for individual based on people's People's based on an outcome of interest. And in this case, the setting that we're interested in, we have multiple outcomes of interest. So we'll talk about that. And so the problem of multiple outcomes that kind of measure, that are used to measure a target of interest is everywhere. And so there are many examples both that we've talked about today, like in the healthcare domain, where clinicians are interested in measuring a person's either. Persons, either maybe if they would have an adverse reaction to a specific treatment or just in general, like overall benefit. Or another example is in maybe recommendation platforms that are trying to get some sort of measurement of people's propensity to purchase something. And so they measure a bunch of things like click rates and things like that to try to get at their target of interest. In the example, the setting that we use, The example, the setting that we use that we're interested in looking at, we look at a data set of a program run by the World Bank where they're trying to provide poverty alleviation interventions to households. And they collect a bunch of different measurements on the different households and try to then allocate different interventions based on those measurements. But there are But there are issues with doing that, I guess. So most of the current literature and the way that they try to allocate their interventions, they end up choosing one measurement to represent a person's total experience. And all of these in general are. And all of these, in general, are proxies for the true target of interest that we care about. And so, and they tend to be pretty noisy. And so, yeah, so people will either pick one measurement or use other sorts or choose one, use some subset of these and aggregate them, or just treat them as a proxy for the true target outcome of interest. And so, our motivation from this. So, our motivation from this is to try to learn, to try to remove the noise from these outcomes and use those denoised outcomes to get a better estimate of whatever your target outcome or outcomes of interest is, like multiple outcomes. And then from that, learn a policy based on a weighted average of the attendees' outcomes to then allocate. To then allocate interventions. And so, yeah, so our main contributions are denoising the outcome data using a latent variable model, a reduced framework regression model. There are other models that you could have used and that we actually tried, but yeah, run into some modeling issues there. But yeah, ultimately, I guess the main conclusion is that when you're able to denoise To denoise these outcomes, you get improvements in the final treatment policy, in which in our case, the denoise approaches reduces the variance of our estimated values and also improves the final treatment policy when compared to sorry, when compared to any of the other. Another any of the other policies. So that's it. What was the latent variable model you guys ended up exploring? So the final one that we ended up using is reduced rank or reduced rank progression model. Is there like reasons you chose that? Yeah, I guess we tried. So there's other work that has looked at. There's other work that has looked at like PCA, but with some of the assumptions that you end up having to make, in probabilistic PCA specifically, it became tricky when we tried to account for the covariance of individuals and get back to whatever target outcome of interest. Yeah, and there are some other things that people try out too. Yeah, I think reduce rates. I think reduced rank regression, it's a linear model, so there are some issues with that. But yeah. So when you say it improves the final treatment policy, what metric are you looking at there? Yeah, we looked at need square error and well, so the output. Well, so the outcome is, it ends up being like a weighted average of all the outcomes. Over the weighted outcome. Yeah, over the weighted outcome. And then we also looked at just reducing, I guess we looked at reducing the variance of the estimated values. But yeah, I think it includes B squared. With respect to the D-noise outcome the one that's like the way it happened? Yeah, yes. I was just trying to stop every single time.