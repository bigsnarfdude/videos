Okay, it's my pleasure to introduce Lauren Jay in the second talking development session, coming back a bit to Luberto just again. It's just nice that we have this opening here again today, switching back and forth a bit. Thank you, Felix, for inviting me to these next controls. Um so my talk is uh as a different flavor. It's gonna be our numerical methods for initial data problems. For initial value problem. Fixed ideas, you can think about simple ODE problems with initial value. Most of what I present here is applicable to days, applicable to time-dependent needs, all the type of deterministic time-dependent equations, applicant for example, some sorts. Some integral equations will be applicable to expression. I summarize already the main ideas of a new strategy to solve initial value problems. One thing you can do is time rescaling. So instead of using the original time t, using a rescale time tau and having a rescaling function, consider implicit methods, for example methods, but Methods, but can I consider any type of implicit methods about your choice? The main thing is that I want to consider constant step size. The main reason is that it has many advantages compared to using the standard variable step size strategy. I'll discuss that a bit later. In a sense, if you're rescaling the constant step in the tau times the rescale, Times the rescaling function acts like a variable step in the original time key. One reason also for using constant step is that I can have predictors of high order for this implicit method. You cannot really have them for variable steps. And you can use them to your advantage when you do the fixed point iterations. I'm not actually not just as the initial guess, but also for the inner iterates of fixed point or Newton type iterations. Okay, for example, you could consider interesting equipment methods. I may go fast on certain slides because of time limits. Excuse me, but I will go to the main points of my talk without spending too much time on some details. Without spending too much time on some DDA systems. So, implicit methods you have to solve a linear system of equations. Like I said, you can use any other type of implicit methods. The example of using the New Col experiment in this talk is the two-stage gauss method of all four with those coefficients. Coefficients are the Pij, Pi, CJ. And by the way, here is the equation without any time rescaling, by the way. And still with the stronger variance. I don't consider time rescaling. I will multiply that later. So, okay, so my main assumption is I want to consider constant steps. So, the main reason I hold this advantage is that we have asymptotic, a lot of nice theoretical results. We have asymptotic expansion of global error in age. Error in age, your backward error analysis results, modified equations results. You can interpret vertical solution as the exact solution of vertical problems. But only in the case of constant steps, you don't have these results generally for variable steps. Of course, you do constant steps, you don't need to control the step because you constant step and only to recompute Jacobian used to step size change. Use to step size change because there are no step size change. Of course, the major drawback is totally usually computationally deficient. So that's why it's a visit that considers seriously using constants that many years ago it's based on science. But in fact, this can be addressed by timely scaling, this coefficients. Um probably don't want to spend too much time on explaining the predictor aspect because in this uh group of people you maybe some people let us avoid the control aspect. So I don't want to spend too much time into the details of the predictor algorithm. I'll show you some uh numerical experiments and if you want to know the details about this predictor it's just that useful data. I can discuss that briefly now. The main thing is that the predictors are applicable to any type of implicit discrete dynamical system if you want. So they can come from discretion of OE or PAEs and any type of implicit method. So it's very universal. And you can put H as a small parameter. So this is applicable to all the type of problems which could be expressed that way. It doesn't have to come from OD or from the Doesn't have to come from OD or from different equations. In the case of multi-coupter methods, capital Xn is just a collection of stages of a method. Yeah, I will not discuss this algorithm in detail. The main thing: so, first, you have a sorting algorithm. You can choose many choices. So, this predictor do not replace that. So, for example. Predicator do not replace that, so for example, you can use simple interpolation from previous steps, as initial starting cases. And then you can improve on that by calculating errors you've made at previous steps, not just for that predictor, but also for the correction that you use. The correction are based on just, for example, interpolating the previous error, previous steps. So you correct your prediction before doing anything, before doing the first. Anything before doing the first Newton iteration. But you can also do prediction for the future iterate of the Newton iteration. So here is the Newton fixed point iterate number. I know there are a lot of indices here. L is the level. Like I said, I don't want to spend a lot of index. I can even add one more for the components level. The nice thing about the algorithm is The nice thing about the algorithm, it's embarrassingly parallel. It's universal, it extends different approaches. In a sense, I trade computational time for memory. So I saw all the previous errors and I use that to my advantage to improve on the current iterate. I should have said, yeah, when I write exact error here, I don't mean the exact solution, I mean this is the exact numerical solution, and this is the approximate numerical solution at level F. What you had up to before. Okay, I'll skip clustering, but basically you can get any type of order of approximation at different levels. You can go in very high order. Of course, you have got error constant, they go at the same time. I'll skip this technical details, but let's look at some graphs, some initials. So let's have a bit technical and some simple Simple numerical experiment on the simple Hoody four-dimensional predicate a lot of stuff orbit. At this point, I have done nothing, no time is getting okay. So this I use constant step very small step, it's about two period. And you can see if you use constant step, you have trouble. Basically, it's a system is about. Basically, it's a system is Earth and Moon and then satellite orbiting. And this is the difficult vision of integration here that calls for really variable steps. So doing constant step right away, I mean, it's not going to work in general unless you're ready to take very tiny steps, but it's totally useless to take a tiny step with this region. So this is illustrated. So this is illustrated by the predictors, by the predictor algorithm I showed you. So it has peaks. So that's where it's difficult to solve the problem here. And so the idea, we'd like something more uniform. Like we would like the error more uniformly distributed in time. So this is the time. Actually, this this starter is not the scaling. This is the error in the various predictor. This is the exact number of the stage. So you have two speaks. It was done very nice. Same for the select on the stage. Okay, let's look at another problem. I'll treat this for two different problems. This one is Hamiltonian. This is the Hamiltonian error. I use the Gauss to stage Gauss, so this is a symplectic method. You get a nice behavior of the error. Methods, you get a nice behavior of the R on D mite standard, but you still see some big qualities that were state. Or you can see the region where it's more difficult to solve the problem, I guess. It's also not easier. So do X3 and X4 represent the primary velocities? So that's the predictor here. So this is the predictor from interpolation, and then this is the new algorithm, the predictor algorithm that goes to almost nothing, just subtraction edition. Just interpolation basically. And then from there you do one step of the fixed point, like that, and then you can predict for just like the rockies or like this. Rocky is so lactose. That's what the signal in the north stage of the cosmetic time. Okay, all right, so it's clearly we need to do some variable steps kind of needed, but I want to do it indirectly. So instead of the usual way, I want to do it indirectly by introducing some timely scaling in the equation. So h times. So h times rho will be active in somehow like the volume of that. So I need to choose a function rho that makes sense, that would work well. And that's where maybe people doing control, they could help me improve on that or something else. Because I'm sure there's some control theory interpretation of what I'm doing here. It's like you use a control and then what do you use? Why do you use them? Okay, I don't want to keep the same differentiability of F, so I'm taking like a Q norm, which is E norm. Okay, you can add sort of things that matrix, over here, some non-positive coefficient here, because if the solution tends to a fixed point, you want to tend to that fixed point. So you need that alpacy. I thought this example I show you here. For this example I show you here, solutions of periodic or quasi-periodic. You don't you can use alpine also because you don't tend to fix them. Here is the example for the midpoint rule, that would be the way it would look like. So you can really make the interpretation of vertical step with the functional rule in that case. Not directly for the constitution. For Amitan and system, so For Hamiltonian system, so if you rescale the Hamiltonian system directly, of course, it's not Hamiltonian. So, but for Hamiltonian system, what you can do is to rescale the Hamilton. Still have a Hamiltonian variable S. So you can still keep the Hamiltonian structure rescaling at the Amitonian level, not at the before the condition. Okay, so let's see what works. Okay, so let's see how it works for Aaronsdorf orbit. Oh, by the way, I should have said, by using this type of controller of timely scaling, you know, when F is large, you want to take small steps, and when F is small, you want to take large steps. That's the main choice. So in my quiz. So Arns forbid, so it works well. I mean, before, even with this tiny step, I mean, the solution was getting completely wrong. But here it's fine. I mean, so it's the risky of time. I do much less work here. It's much less work of this is the row function. You can see what the abort is. So it calls for very So it it goes for a very you know small step when you reach this time here. It's at the beginning of that variable and the end of the variable. No time rescale input would mean rho equal one. That's the rescale time and the original time. So to hand the raw is more but it still increased but It's still increased, but more flat, of course. You spend more time in the region where it's difficult to integrate. And for the predictor, it's much nicer, but flatter curves. So this time rescaling is a way to flatten the curve. Well, different. Well, different. So, this is the predictor, then I mean the standard predictor with interpolation, one level correction, two level correction, then you do your ticks point filter and then have some tick-stopping criteria that I don't even calculate more. Yeah, um this predictor reduces a lot the number of uh iterations Iterations and then over the level you know it should go a little fast, but I want to give you the main idea for the Kepler program that's the raw function. Yes the American error is smaller. I do less work than 10 minus 10 for those 10 minus 8. Than escaped, it's more chosen, I mean less peak. And for the predictor, instead of the Rocky Mountains, I have the up there, I call them the Iowa hills. So I like this graph. So you see, you don't have any digs anymore. Anymore. The goal is to make details very uniform, very fast. Okay, so let's come to some conclusion. So I didn't go into the detail of this predictor, but this is a new strategy. And the applicant proposed is not the same like microdefinitories. Is not saying like my predictor is better than all the predictors, it's on top of them. So that unifies what people have been doing. So you can use the current existing predictors and you can improve on them. And the nice thing in the predictor, I didn't go into the details, but you can improve also not just the starting finish of gas, but also the inner iterates of fixed point or Newton process. So it's based on using exact parallel. It's based on using exact, like I said, it's not from the exact solution, the exact numerical solution compared to the actually complete message from previous steps. So this predictor is applicable to any type of recursion of this type. You can get arbitrary high-order predictors, some are semi-parallel. And of course using constant step, usually it fails miserably, but Usually it fails miserably, but it can be addressed only by tactical scale. So that was the way make this work. So what I would like to investigate in the future, first of all, make some worker experiment to compare explicit implicit meta versus explicit because both this potential parallel and prediction, I believe that I believe that we can make this method more efficient and exclusive. For this community here, it's mostly how can I choose this function, time scaling function in a better way? I was thinking in the spirit of PID control, I mean, I could also consider using values. Uh values at the past control Now we like to apply this type of methods to for non-linear optimization to minimize function f the main approach The main approach are like trust region and step-like platform and I believe only method can play a role in this kind of optimization also of what's not much considered for solving optimization problem using implicit method, but it is a cheap way just to the goal is to do just one function evaluation and Just one function evaluation and the predict. Well, you don't need to be accurate, you go to follow trajectories, we just want to minimize. All right, thank you very much.