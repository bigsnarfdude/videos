Kind of for similar reasons as in the psychometric networks case. In this case, a typical workflow is you get a bunch of people, sit them in an MRI machine or a PET scan or something like that. You identify using magic neuroscience that I don't know very much about. Science that I don't know very much about. Regions of interest in their brain that you want to measure activity at during the scan. They might be doing activities like playing Guitar Heroes apparently popular for some reason. It's not really Guitar Hero, but games like that. Or have them look at pictures of friends or alcohol or something like that. But you get a time series for each individual of the activity at each region of interest in their brain. The regions of interest then form nodes in a network based on the correlational data. Based on the correlational data, there are many details about how you can turn the correlation matrix for each person into a network that represents that person's brain activity during that period. Some people opt for just a dense weighted graph. That is, you literally just take the matrix and write it as a graph. But this is, I mean, then it's not really a graph problem, which is kind of pain. Of pain. A lot of people will threshold, they'll get rid of the smallest correlations and things like that. We'll talk extensively about whether any of this is a good idea. But this is really motivated in the brain science because, similar to psychometric, they've done so much study on what your amygdala does, on what your prefrontal, what various parts of your pre-frontal cortex do. And they kind of feel like they've milked that dry. And what they really need to do is talk about, instead of studying. Talk about instead of studying an oboe and studying a trumpet, you want to study the symphony. You want to figure out it all fits together. So they're really interested in pursuing this. There are hundreds and hundreds of thousands of papers that basically do this with different kinds of disease setups, different kinds of measurement techniques. Application three, gene co-expression networks. I know a little bit less about this. Biology is hard, as it turns out. Um, but basically, there are many, many studies where you find some way of technically measuring which of you know, tens of thousands or hundreds of thousands of genes might be expressed in a single individual. You might get this expression in many individuals. Often they like all have the same disease or something like this. There's some reason to think their genomes have something in common that's interesting. And it turns out, if you just do scatter plots of the expression of gene number four and gene number 15, sometimes they look. gene number 15. Sometimes they look super unrelated, but sometimes it's really clear that people that express one gene are also expressing other genes. And so your goal is to create a network that expresses which genes tend to go together. And again, the kind of the theme across all of these applications, there's been a lot of single gene studies, and a lot of the open problems still in genomics are thought to be caused by interactions of multiple or even many genes. So again, you want to build a web and a So, again, you want to build a web and a system and analyze it. Downstream, you can look at things like finding hub genes that seem to be connected to the activation of many other genes, differential co-expression analysis, which I'm going to interpret on this slide as for different cohorts, that is like disease cohorts versus non-disease cohorts. Is there a big difference in what their networks look like? And identifying regulatory sub-networks, which would be communities. I do know in the gene co-expression community, working with small sample sizes is a challenge. The data is super high-dimensional. It turns out that it's really expensive and maybe unethical to get lots of people with the same disease. So you're just stuck with the small sample size. For the math people in here, there's a really interesting fact that a lot of these studies like this have compositional type data. It basically means that. It basically means that geometrically you are trapped on the simplex because the exact number of little snippets that you gather is not what's important. It's the proportion relative to all the other ones that you get. And you don't have a lot of control over the dimension of the simplex that you're working on because your results have to be robust to failing to measure a bunch of genes that might be relevant. And so there's really interesting math and statistics. Really interesting math and statistics there that people are doing. And again, I'm going to constantly beat on this drum. We have correlation data, that's like a one-point linear measure. But what really people want is regulatory dynamics. That is, if I touch here, what's going to happen in the rest of the network? Or at least, you know, this is one of the considerations. Very similar is metabolite networks. Instead of genes, you look at metabolites. In this case, there's an interesting conservation of mass thing going on in the dynamics. Going on in the dynamics because it turns out chemical reactions are physical and you can't just make particles appear. Microbiome networks, again, very similar. Microbiomes are little communities of microbes. They exist in your gut, in your nose, in soils, everywhere. Don't think about it too much. But I mean, you can use these to study things like allergies or gut diseases like irritable bowel syndrome and stuff. Bowel syndrome and stuff like that. So the nodes would be microorganisms, the edges would be the co-occurrence of those microorganisms. And in this case, because it's kind of ecological, people like to interpret the edges as meaning things like, you know, competition between species and predator-prey relationships. Although, just from this kind of data, you can't really pin that down. You need a little bit of extra data there. Okay, I'm kind of flying. I think there's only one or two major applications left that I wanted to touch on. This one's kind of cool: disease networks. Like many, actually, I like this one for a few reasons. Naively, you can just make a bipartite graph of different diseases and different facts about those diseases, like which people had those diseases or which or. Which are different parts of the genome that are present in different bacteria and things like that. And then you do the usual thing with bipartite graphs. You project down to either just looking at which of these nodes can share something in common on this side or which of these nodes share something in common on this side. So you usually get two different networks from this, and you can analyze whichever one you want. Different ways of linking genes to each other. Comorbidity is really important. People are talking about which disease. You know, talking about which disease might cause you to have other diseases farther down the line. Again, genetic analysis, symptom sharing, or shared metabolic reactions. One reason I like this example is because it's not obvious that this is a correlation network. In practice, it tends to be a, you get count data of just how many people had each of these diseases. But if you view each patient, for example, as a sample from a random distribution, then the co-occurrence of these diseases is basic. Co-occurrence of these diseases is basically a correlation of two binary random variables. Okay. Oh, yeah. The last one, the elephant in the room, financial correlation networks. This is a huge, huge growing field because you need health, wealth, and stealth to get funding. And this is the wealth part. So who did, I can't remember. I can't take credit for that. Someone else came up with health, wealth, and stealth, but it's a very helpful grant writing. It's a very helpful grant writing tool. Okay, but basically, the nodes in financial correlation networks are almost always some kind of asset, that is, stocks in a company or something like that. And I think there are a lot of papers in this field that are not done with particularly great insight because people just take correlation matrices between stock indices and try to analyze them using centrality measures and stuff like that. There is a really serious. There is a really serious community that's grown up that's worth noting, which is the Econophysics. They're very controversial. The economists generally hate them, I think. That's my impression from the economists I've talked to. But these are kind of people who are applying physics-y techniques to economic models. So they tend to stress ideas like ergodicity or non-equilibrium physics models, and applying these to finance, because I guess the idea is that financial systems never really equilibrate fully. I'm not deep in. I'm not deep in that field. One of the big contributions of financial correlation networks to this, to the larger discourse that probably should be imported into other fields and hasn't been, is random matrix theories. There are a lot of good random matrix theory people who are looking at financial networks. And we can talk about some of those details later. But I would love to see some of these techniques applied in other fields, some of these random matrix ideas applied in other fields. And then bibliometric. And then bibliometric networks, that is, citation networks. I have the constant disclaimer: using any network like this in a paper constitutes cheating because it gets you cited a lot because scientists love to compare themselves to other scientists in a network and talk about who's the top dog. And its application is generally in the science of science, of science now, because there are now survey papers discussing the science of science. We're waiting for the fourth tier of science of science, the premium. Of science of science, the premiere in a couple of years when someone does a meta-analysis of the survey papers on the science of science. Kind of a niche application. We have climate networks. I don't have a strong opinion on how strong the literature is here. But generally, you're looking at regions across space, all over the globe. Nodes are regions. Samples are basically weather maps of the globe across time. Globe across time. And people tend to look at things like searching for key nodes that tend to be really heavily correlated with weather conditions in other places. It turns out, for reasons that I don't really understand, that the tropics are really well correlated with other regions in terms of their weather. The big application that people talk about with these is searching for early warning signs of major weather conditions. Yeah. It's not super clear to me from what I've read so far. To me, from what I've read so far, whether there's a serious advantage to using these over the physics models that are much more popular. But it is, you know, an area that's been developing in recent years. Okay, that's all my applications. These are all the major applications of correlation networks that I could find. Now let's talk about pulling some math out of these applications, things that people argue about, things that people have found in common. So first off, let's, and yes, I did use Google Slides and just paste pictures. I did use Google Slides and just paste pictures of equations in here because I was lazy. But I think you can read them just fine. Most of these are somewhat standard equations. So, first, let's just have one slide on constructing covariance matrices. There's one standard estimator that basically everyone uses most of the time to estimate the entries in a covariance matrix. You just sum over the samples minus the mean times the sample of the other variable minus the mean. So just a very naive textbook thing to do. A very naive textbook thing to do, but it's very, very common. So that's covariance. To get the correlation, you do the same thing, but you normalize by the total variance of each of the variables that you're working with. In summary, sample correlation between i and j over the geometric mean of the sample correlation of i and the sample correlation at j. For those who don't do correlation and covariance all day, covariance is an unbounded real number and Unbounded real number and correlation is bounded between plus and minus one. So it's just a normalization difference. Mostly I'm talking about correlation today, but covariance sneaks in there because the finance people especially really like covariance matrices a little bit more. For the math people, it turns out that if you take 2 minus 2 times the sample correlation matrix, that actually gives you a Euclidean distance on the nodes in the network. So you can convert all this to metrics. So, you can convert all this to metric space language if you want to. I mean, it's not a very stable Euclidean distance in the sense that it depends on the sample, but still, it exists. Okay, so there's a name for the distribution of this sample covariance matrix. If your data in the background is normally distributed, it's called the Wishert distribution. One big consideration statistically when you're working with these covariants and When you're working with these covariants and correlation matrices, is you usually have way more parameters that you're trying to estimate than there are samples. Right? I mean, even if you have, I don't know, 200 variables, then you're looking at 40,000 entries to estimate, which can cause various problems. Generally, you get a little nervous when you're estimating that many variables from few observations. One problem is that the range. One problem is that the rank of the estimated correlation matrix or covariance matrix is limited to the number of samples, which might be really unrealistic, right? If you only have 10 samples, that means your covariance matrix is going to be at most rank 10. And in particular, even if the original covariance matrix was invertible, which is needed if you want to fit this to a Gaussian distribution, your estimated covariance matrix is not going to be invertible. So you need to deal with this at various points. You need to deal with this at various points. We will talk later. There are various people who think that you can use sparsity priors and regularization to get better estimates. We're also going to talk about various thresholding and shrinkage techniques to also reduce the variance of the standard covariance estimator. And we talked about the Euclidean distance. Okay. The most contentious thing in the correlation network community. The correlation network community is this dichotomization issue. So the literature is not super clear on this. In fact, in some papers, it's super unclear what people actually have done. But we're going to carefully distinguish between dichotomization and thresholding here. So the data flow is like this. You get a correlation matrix or a covariance matrix. You form kind of a not yet finished matrix that is weighted. Notice there might be some negative. Weighted. Notice there might be some negative weights. The thresholding part is when you set all the small entries to be non-edges. This is really common, but you retain the weights of all the other edges. That's just the thresholding part. Notice in this part, we made the choice to say any negative correlation is a small correlation. This is really, really common and in many cases, probably a really bad idea, but it happens all the time. And then the dichotomization step is when you forget. Dichotomization step is when you forget the weights of the remaining edges. From my observations in the literature, most people do both dichotomization and thresholding, but lots and lots of people do just the threshold as well. There is sadly very, very little theory out there about when thresholding and dichotomization are good ideas, which is crazy because this is likely like the most lossy step in preparing your. Glossy step in preparing your network before the rest of your analysis. Lots of people think that this is a denoising step, right? You're getting rid of the small correlations that are presumably maybe like spurious and not real. Practically speaking, a lot of people do it even though they don't really believe that it's like that valid to do because it makes a really parsimonious model that they can tell a story about and they're willing to sacrifice a lot of accuracy for parsimony, which is understandable. Accuracy for parsimony, which is understandable in some cases. And really, one of the main reasons is just the amount of software and algorithms that have been developed for unweighted, unsigned, undirected networks is really big. And people want their data to be compatible with that software or with the algorithms or the theory or whatever is downstream. There are, and I'm going to make a case for this a little bit more. I actually think there are cases when thresholding and dichotomization matches your prior understanding of the. Matches your prior understanding of the field. In supply chain networks or in biological networks or in brain networks, there is lots of reason to think that maintaining consistent relationships and communications between entities can be expensive. And so probably your nodes don't all talk to each other. And so you should get rid of some of the links between nodes. But anyway, this is the big hairy monster that everyone argues about just in the technique. About just in the technical aspects of constructing correlation networks. Some more problems, because you can't fit them all in one slide. Lots of people have problems with this. There's no consensus on how to threshold. So you can toss the negative edges, but at least as significantly, how do you pick the thresholds at which you decide to toss edges at? Some people use really kind of naive ideas to say, you know, in p-tests, we use 5%, so maybe we should keep the top 5% of the edges. Percent of the edges. Some people use more, you know, ideas that are more involved and intricate. But generally, anytime you do this in a paper, you're opening yourself up to a criticism from someone of, yeah, but did you test all the other thresholds that you could have used? And in many cases, the results are indeed sensitive to the threshold you use, especially in brain networks or, well, in any case where you have data for multiple people. People. The question often becomes: Do we want to set the same threshold for every person? If you do that, then you don't have control of the density across the graphs. And a lot of the graphs end up being really dense and others are sparse or perhaps not even connected. And it turns out every network metric under the sun is heavily correlated with edge density. So in the end, you might just be measuring density. If you instead have an adaptive threshold for each person, then you need to find some. Then you need to find some justification for why each person has a different threshold for significance for when their entities are communicating with each other. There are lots of ways to get false positives and false negatives on the existence of an edge. There's also this issue that correlation gives you too many triangles, or it's thought to give you too many triangles. This is kind of a dumb bound on correlation. So the correlation between nodes I and J is bounded above by the product of the correlations between I and J. The product of the correlations between i and k and j and k for any k minus this adjustment. But the point is, if i and k are correlated and j and k are correlated, then i and j must be at least a little bit correlated too. So there's a lot of worries about triangle inflation, especially again, because the number of triangles in a network is super correlated with most network outcomes. Yeah. Sure. You're talking about like graphical lasso and stuff like that. Yeah, what are some of the keywords that you're thinking of? I mean, I don't know if you know high dimension. Yeah. Yeah. Yeah. So you think I so you think I'm underselling how much theory there is, basically, or underselling how much is actually possible? Yeah. Yeah. Okay. No, I think that's fair. We should talk more over tea to make sure I'm like saying this right next time. I mean, I do, I, I, and even if it, I mean, I think one of the questions here is, I think a lot of these things are more resolved on theory. Things are more resolved on theory sides than they are actually resolved in applications because a lot of people are not communicating either with their statisticians or with people in other silos. So it's also possible there's a miscommunication there. Because in the literature, there's lots of concern about this. Yeah. Yeah. Yeah. Yeah. I'd like to talk about that, I believe, maybe a lot more later. Different things that have been proposed for if you want to threshold, well, and also the thresholding issue is kind of different from sparse correlation estimation, right? Like a lot of the concerns come from going from an estimate of the matrix and then doing some kind of rounding procedure instead of trying to get directly. But again, we can talk much more about this. In the neuroscience literature, especially, there's kind of this trend of using. Sure, especially there's kind of this trend of using many thresholds in your analysis, maybe even all the thresholds from zero up to one or from negative one up to one. And then report, and then either averaging the outcome over many thresholds or using functional data analysis techniques to do your testing over all of the possible thresholds. Again, I think of the brain networks people as talking about differences between the efficiency of the proposed network and the density of the network. The idea is that. The network. The idea is that density is good because it makes things connected, efficiency is good, meaning there shouldn't be that many edges because edges can be physically expensive to maintain. In fact, one of the most common thresholding techniques is to just greedily add, oop, is just greedily add edges until you have a connected network and then stop. And this is kind of similar to a minimal spanning tree or minimal planar maximally filtered graph technique. Again, the idea is to... Graph technique. Again, the idea is to just include very few edges, but still keep it connected. If you do decide to keep some or all of the weights, that is maybe threshold, maybe not, this does restrict which downstream methods can be used because there's a lot of simple graph methods compared to weighted graph methods. Although, people in this room, we love our weighted graphs. It does retain more information. It does retain more information. It is most common to just take the absolute values of the correlations and drop the negative weights. We'll talk about that more in the next slide. Another alternative is actually split your correlation matrix into a positive matrix and a negative matrix, analyze them both separately and like average the results or something like this. But it tends to be pretty ad hoc. I did include the shrinkage function here. You can just use weight shrinkage. Here, you can just use weight shrinkage rather than thresholding away some weights. This is one of the possibilities. And there are various methods that are called adaptive that might account for node degree and stuff like this. For negative weights, one of the things that practitioners tend to say is that negative weights are hard to interpret. I've talked a lot with them, and I kind of disagree, and I can't, I have trouble grasping what they think is hard about them. In some cases, the Them. In some cases, the negative weights are really easy to interpret. In a financial network, two things being negatively correlated is like a very straightforward idea. In brain networks, you can kind of see how it might be harder, right? Two regions that positively coordinate their activity, like maybe this is due to them needing to work together for a function or something. But there are lots of regions that are negatively correlated, like when one turns on, the other one seems to turn off or decrease its activity. A lot of practitioners say that this. Say that this is harder to interpret. It is definitely true that there are fewer tools for analyzing signed networks. I think probably the most work has been done on signed community detection, at least of the things that I'm aware. It's also worth noting that there is a fairly good signed-weighted stochastic block model tool out there by Tiago Pecioto. Well, lots of people have opinions on how good it is. Well, lots of people have opinions on how good it is. I think it's pretty good. Okay. Partial correlation. I included an awful definition here. I think not everyone in this room will know it, so it's probably worth going through briefly. Partial correlation is this idea that you're trying to remove bonus correlation between nodes I and J that are caused by their joint correlation with other nodes. So the procedure goes like this: you select two nodes I and J that you want. You select two nodes, i and j, that you want to work with. Do a linear regression of node i on all the other nodes, except j. So you get x as a sum of all the other nodes with some coefficients. Do the same thing, regress node j on all the nodes. So you get xj approximately as a linear combination of the other node values with some coefficients. Compute the residuals, that is, the signal after you subtract off the part that you've decided to. Subtract off the part that you've decided to impute to the other nodes. So you get the residual for I and the residual for J. And then you correlate the residuals for nodes I and J. So under kind of simple assumptions and maybe more robust things, but certainly under simple assumptions that like the nodes actually are linearly related, you know, this is supposed to be a better kind of a correlation coefficient. I'm actually not, I think I. I think I'm not deep enough in the literature to have an opinion on whether this is always a good idea or not, or like what the exact conditions when it is. But this is widely done. So a lot of people will try to convert their correlation network into a partial correlation network. You can also compute the partial correlation coefficients in terms of the elements of the precision matrix, that is the inverse of the covariance matrix. So, anyway, a lot of people use partial correlation. There is one paper that I sadly don't remember all of the assumptions off the top of my head that says that if you have a sparse or a rank deficient estimated covariance matrix, then just using the pseudo-inverse in this definition is suboptimal. It is true that when you move to partial correlations, you get networks with smaller clustering coefficients. So, maybe this fixes some of the. Coefficient, so maybe this fixes some of the deflation problems. There is some interesting spectral work on the eigenvalues and eigenvectors of the partial correlation matrix. And there is some theory that says that the dominant eigenvalues, well, eigenvectors of this matrix can be strongly affected by noise, which is generally regarded as a pretty bad thing. Yeah. Yeah, and the standard estimator for partial correlation matrices have higher variance than the standard correlation estimators. And some papers have reported that they found the thresholding of partial correlation matrices to be a little bit less satisfactory because the partial correlations entries tend to be smaller, and then you are not sure which ones to throw away because there are like less. Because there are less clear winners. Okay. Graphical lasso is really famous. This is you're trying to directly estimate a sparse covariance matrix instead of get a full covariance matrix and then round it. So you do maximum likelihood estimation on the data, assuming a Gaussian generating process with an L1 penalty. And yeah, I mean, some papers in the literature have reported. In the literature, have reported that they think this gives you fewer false positives on edges that you retain in your network because you don't have to, they don't, they, I'm trying to report this without giving an opinion, but they feel that there's less of a risk of multiple comparison considerations giving you lots of false positive edges. I'm kind of skeptical that this is the right way to understand why graphical lasso is effective, but it is reported in the list. It is reported in the literature. Okay. The other thing that people are doing more and more when they construct these networks is to do temporal networks. So this is generally if you think that this process you're observing is non-stationary, for example, in neuroscience, learning might be taking place. This is really important while you're doing a task. You can do windowing to get a sequence of correlation networks. There are many details of how you do the windowing. Details of how you do the windowing. And there is an increasingly wide range of temporal network analysis tools available. I will say, I haven't worked with the software for these very extensively. The smart students I've worked with have reported that this is not yet super user-friendly for non-experts. So if we believe their reports, it might take a little while to actually get these things going if you're not already an expert in them. Already an expert in them. As usual, with the window size, there's a trade-off between if you do really small windows, then you might potentially be able to catch rapid time scale changes. But then you don't get as much data per window. So your correlation matrix estimator might be highly variant. Yeah, that's all I want to say about that right now. That's right now. Okay. A lot of the kind of more generalist work on correlation networks is about null models. Financial correlation networks too also do a lot on null models. So null model is these comparison models of usually random matrix models or random graph models that kind of contain no interesting effects, and you want to see if your measured data departs from them. Let's see. So, in regular network analysis, a lot of people use the correlation or the configuration model, which is the maximum entropy, depending on exactly how you define it, is the maximum entropy model given a fixed sequence of degrees or expected degrees of the nodes of the network. Most people agree that this is inappropriate for use with correlation networks data because it's not obvious that you have. Networks data because it's not obvious that you actually can generate very many correlation-like networks from the configuration model. For example, the configuration model does not create triangles. There is a way, and you can look in our paper, there's kind of a fun half page where we do derive the configuration model from some specific statistical assumptions. The most important one is that the node-level signals have to be assumed to be independent, conditioned on the sum of all the signals, which I think. Of all the signals, which I think in most applications is kind of an unnatural assumption to make. There are lots of calls in the literature for better null models for correlation networks. I'll talk about a few that are important on the next slides. But in general, if you have a null model for correlation matrices, you can then push that model forward through, say, a thresholding process to get a null model for correlation networks. Correlation networks. Okay, so simple models that are important as a baseline. Some people are using tests where you permute the data and then build a kind of a bootstrapped model of networks from that. You can add various constraints. One notable one is to just preserve the power spectrum. Preserve the power spectrum. So you take Fourier transform, perturb the phase randomly, and then convert back. Wishart null model is really important because assuming the data is Gaussian, like I said, the correlation matrix is itself Wishart distributed. Some people use the identity matrix as the mean. And there's a lot of work in the finance literature talking about there's two kinds of eigens. Talking about there's two kinds of eigenvalues. There's the bulk eigenvalues and there's the market or other significant eigenvalues. Yeah, market mode, market modes. The bulk eigenvalues are eigenvalues that are consistent with the eigenvalues you'd get if you draw from the Wishert distribution. And the others are supposed to give you like actual market information that is actionable. Generally, there's one mode that's stronger than all the others that kind of gives you the general direction of the entire market. The general direction of the entire market, depending on how you normalize your data. Okay. Another idea for null modeling is to actually measure the data, compute the whole spectral decomposition of the correlation matrix, identify these market modes, kill them, and whatever's left is your null model that supposedly reflects the actual nature of the noise in your data. I don't think there's actually a theoretical guarantee that what's left after you do that is an actual correlation matrix. After you do that, it is an actual correlation matrix. So it's kind of a weird null model. But it's, I mean, I don't find it wholly unreasonable. Okay, how are we doing on time? Do I need to have 10 minutes? Okay. Yeah, we can skip HQS. HQS is cool. It's a maximum entropy model. I think it's kind of the closest analog to a configuration model for correlation networks. You basically are making sure the expected diagonal. Making sure the expected diagonal entry and expected off-diagonal entry are equal to the sample variances and covariances, respectively. In terms of mathematical analysis that's been done kind of at a general level, a couple of papers have looked at the degrees of correlation networks. I guess a few papers have. One finding that keeps me up at night sometimes is if you keep if you is if you keep if you take a bunch of correlational data um build a correlation matrix out of it and then threshold that matrix um if the underlying data is gaussian distributed and maybe a couple other assumptions that i have forgotten you get power law degree distributions in the network which is kind of unexpected because there actually are no variables that are more important than the other variables or more central than the other variables so that keeps me up at light at night that's a paper So that keeps me up at night. That's a paper by Alice Schwartz and others, which I can share. But kind of generally, thresholding tends to fatten the tail of degrees in correlation networks. There's been a fair amount of work developing community detection methods that are native to correlation matrices. I don't think we need to go over them in detail. The ideas in them are pretty simple mostly. There's been a series of papers recently. There's been a series of papers recently on getting clustering coefficients for correlation networks. They're a little bit straightforward. They're not terribly complicated. Basically, the clustering coefficient, for those who don't know, is like one of the most important items in social network analysis and network analysis generally. And it's just counting the number of triangles that exist in the network compared to how many might exist in the network. Since correlation networks are often weighted. Networks are often weighted, fully connected networks. What you do instead of counting the number of triangles is you just multiply the absolute value of the correlations around the triangle and sum them up. There are tricks. Some people involve partial correlations at different points. Software to actually work with these things is kind of segmented between silos. There are multiple packages that are for gene correlation networks. Correlation networks. This brain connectivity toolbox that's like natively in MATLAB but also in Python is used by the brain community, but it's also used really widely outside the community because it's a pretty high quality toolbox. Okay, I want to talk about Outlook, things that I observe that seem to be important in the literature, maybe and might be open areas of research for the next couple of years. And some of you might know the answer to immediately, which would be fantastic. Which would be fantastic. So, dichotomization and thresholding continues to be a really big concern among people who use these networks. A lot of people favor using things like graphical lasso and partial correlation that are supposed to kind of natively get to this to your sparse prior on what these kind of natively get you a network without thresholding. And some people say, never threshold. Why would you do that? Never threshold. Why would you do that? It's a super discontinuous process. You should just compare to the null models directly and not even try to sparsify. In psychometric networks and neuroscience, there's huge reproducibility questions right now. It's been a year or two since I took the pulse on this, but it was really developing rapidly when I was working in this area. Problems, I mean, these are super high dimensional, super high noise regimes. High dimensional, super high noise regime. It's real regimes that there's good every reason to think that the ground truth is super complex. So it's hard. There are several papers recently that are calling for better power analysis on detecting things like whole brain effects in mental health. Yeah. One really interesting line of research. One really interesting line of research, Danny Bassett's lab in neuroscience, if anyone knows that, has kind of led some of the way on this, is looking at low correlation edges. So like I said, a lot of people think that if you remove low correlation edges, this should be some kind of denoising thing that will increase your signal-to-noise ratio and has all kinds of other benefits. But it's definitely known that low correlation edges have valuable information. I think this is one of Danny Back. I think this is one of Danny Bassett's group's papers. They actually took some brain networks, wiped all of the high correlation edges, like the top 90% of high correlation edges, and did really cool predictive work with just the 10% that remained. So you're definitely throwing away information in some of these applications if you get rid of low correlation edges. And I think there's a feeling of need in this field to understand when it's okay to toss data. When it's okay to toss data, because especially in the brain science, like there's so many steps of the computational pipeline that if you mess one up, then well, you just really don't want to mess up any of the steps and lose all the valuable data there. Generally, I think a lot of people are looking for how to incorporate domain knowledge into their pipeline, like this general technique of taking correlations and then thresholding and then using centralities or something is well established. But using domain Well established, but using domain knowledge to decide: yeah, when is it okay to threshold? When is communication really dichotomous? How costly are relationships to maintain? How parsimonious do you need your model to be? I think these are going to be my prediction is that anyone who could make substantial progress on this would be making a big contribution. Some people have suggested using random matrix. Have suggested using random matrix theory as a guide, studying how thresholding affects the spectrum and the eigenvectors of the correlation matrices. Maybe if you're keeping the most important eigenvectors relatively intact, that's a sign that you're not losing the information you care most about. Okay. I have not seen a lot of people. Oh, well, correlation matrix as a random variable is really well understood. Correlation network is a random variable. I've seen less work on. I've seen less work on this. And I don't think I've seen anyone in their actual papers put an uncertainty quantification on their estimate of the actual network, like identify the edges they're most likely to have gotten wrong, and then actually sample from some kind of posterior distribution for what the network might have looked like and make sure their analysis is robust to that kind of estimation or robust to those kind of errors. There are some general methods for this. There are some general methods for this. Mark Newman and a couple others developed one method that I think is pretty decent, but none that are really analyzed in terms of the correlation matrices. Let's skip multi-layer correlation networks because everyone is talking about multilayer networks nowadays, so I feel like I don't need to tell you about them. This is one paper by Dane Taylor, Peter Mooka, and a couple of others on thresholding for network super-resolution. This to me is like the best. Network super resolution. This to me is like the best defense of thresholding for network analysis. The idea is really easy. A lot of their geniuses, how they do the analysis, right? It's a good mathematical analysis. But the idea is you set up a model where you've got a bunch of, for simplicity, just a bunch of IID, Erd≈ës Reni, random graphs with a community of a certain size embedded in them. It might be a click community. I can't remember. It's some kind of just regular dense community. And they talk about different ways that you would try to. And they talk about different ways that you would try to detect the existence of the community in this network. They set up the size so that you can't detect it from just a single slice. It's too small. It's beyond the resolution limit of. And for them, community detection is looking at the, is spectral community detection, right? So they're just looking at an eigenvector of a graph Laplacian matrix. But kind of, at least on a cartoon level and maybe mathematically rigorously, if you're, well, not quite rigorously, but if you do a correlation matrix. Rigorously, but if you do a correlation matrix analysis, it's analogous to taking all of these snapshots of a process and then averaging the adjacency matrices and then analyzing that directly without thresholding. This gives you a single matrix and it's easy, but you do lose a bunch of detail. Alternatively, you could form a thresholded correlation network that is add up a bunch of these slices. is add up a bunch of these slices and then threshold the edges at a right value at the correct value so like if an edge hasn't appeared more than seven times you chuck you chuck it um and they show mathematically that if you're using spectral community detection which you know has certain kinds of optimality properties if you're using spectral community detection uh the thresholding really significantly and uh enhances the scale at which you can detect the implanted community both in terms of there's there's there's a In terms of, there's a factor that combines the number of edges that you're looking at with the size of the community that's planted in there. But the size of the community that you can detect by just pure averaging is exponentially larger than the size of the community that you can detect by just by averaging and then thresholding. So I think that's a really interesting piece of theory work related to the thresholding. Since we have geometry people here, I'll just talk about this briefly, and then I think I'm about done. So it turns out that people have studied the geometric structure of the family of correlation matrices. And there are a couple of papers that talk about measuring the distance between correlation matrices using a geodesic distance on this manifold structure and report increased accuracy. Increased accuracy on fingerprinting individuals from their brain data using fMRI. I think it would be really interesting if someone, if we, if we could look at the geometry of thresholding or dichotomizing these matrices to see how far you move them in space, or like whether you move them really close to the wrong part of the manifold, for example. I think that would be really cool. Okay. Let's see. I talked about ensembles of networks. I don't think they're. networks. I don't think they're, I think almost all of the work that exists, I'm trying to think if there's an exception to this. All the work that I can think of that exists on ensembles of correlation matrices and ensembles of correlation networks really mean when you study these, you draw one correlation network and correlation matrix and you do some downstream analysis. But what happens in a lot of these, a lot of the applications where Of these, a lot of the applications where people actually use correlation networks is you don't draw just one network, you draw a family of correlation matrices, and then you need to jointly analyze them together. Vladis this morning did one actually that does treat multiple samples together. But I think from the network science community, we don't see that much about it. And there are considerations, especially picking a group-level versus individual-level threshold, that I think are not fully resolved, or maybe not resolved at all. Maybe not resolved at all here. Graphons. Is the person who does graphons here, or did they go? No. Okay. We're going to skip the graph on slide, and I'll just have to talk with them over lunch. So in short. Like extensively? Oh, okay. There we go. So we can skip it. Okay. We'll skip it. No, I'll. Okay, we'll skip it and I'll blab at you over lunch. Basically, you can view correlation networks as a generalization of graphons. And so I think it would be super interesting to try and port graphon technology into correlation network analysis. Anyway, cross-fertilization. Like I said, I think that there's some potential for some cool mathematical and statistical theory around estimating these correlation networks and understanding when the different choices in assembling them are appropriate. Assembling them are appropriate. But, like, most of the practitioners are fairly siloed, I think, in their own domains, possibly with the exception of statistics people who tend to cross between areas quite a bit more, I think. Anyway, that's all for now. So that's what I've learned. Happy to chat more over lunch and stuff like that. Thank you. Okay. Yeah, the place how it's information. Yeah, I was going to say, like, you can characterize correlation matrices. I think pretty much everyone just assumes that once you threshold, then you're out of positive definite matrix territory anymore. Yeah, so no. Yeah, it's a network. Well, yeah. Yeah, I mean, I think this is the yeah, exactly. I mean, yeah. Yeah. Whether you're tracked with a particular variety what variables we are looking at, the colouring doesn't have whatever. You're saying in the precision matrix? No. In the correlation? You add a variable, put an add variable, and that's part. But after you add another variable, the feeder may not be far. There is a set of variables. Yeah, yeah. Yeah, that's interesting. Yeah. I haven't heard that argument before. I mean, it's really interesting to me. I'll think about it more. To go back to the beginning, like, are these just networks after you? Are these just networks after you threshold and dichotomize them? Yeah. But I mean, you can definitely see some information from the way they were formed leaking through, right? Like they tend to have lots of triangles, for example. And I think that's part of the core of the part of the big research questions in the area is like what is preserved when you go through this process and is it what you wanted to preserve or not, right? I mean, I think there's lots of reasons to think it's often not what you wanted to preserve, but you know. To preserve. But, you know, something like 300,000 papers trying to do this in application areas also seem to suggest maybe there's something there, or at least a strong publication bias. So, yeah. After doing all this reading, to me, the jury's still out on like how much value, like how much of the analysis in the end is really valid that people do with these things. 