Thank you, Sam. So thank you to the organizers who, yeah, unfortunately could not all be here. And also, I'm, as Jing Peng mentioned, I'm part of this. I'm part of this FRG grant at UNC that has jointly with the University of Illinois Chicago that has brought together a lot of us to think about this question of how to find low-rank structures in complicated geometries. And we've seen a lot of interesting talks on thinking about the sort of statistics side of this problem. And Jingfeng gave us a very nice overview of some of the computational aspects of finding this low-ranked structure and using this. Of finding this low-rank structure and using this low-rank structure. And so my talk's gonna be somewhat different, although I will be able to connect some to the conversation, to some of the topics that Jing Feng introduced. I'm gonna talk a lot about sort of how the geometry of a network or a domain really could impact what sort of features you should look for, how to find low-rank features, how to locate things like clusters or community structure. And the approach that I come at these problems from is. At these problems, from is that classically, I'm trained as a PDE analyst and do a sort of a mathematical physicist and think a lot about spectral theory of elliptic operators from a PDE domain point of view. And then looking for interesting ways to translate some of that intuition and some of that insight into things we can learn and say about networks or graph-related problems. So I'm going to give an overview of the motivation from a very sort of PDE and sort of And sort of continuum framework. And then at the end, I'm going to try to introduce some of the ideas that we're working on now to see how some of these extend to graph-related problems and also talk about some of the intuition along the way that I'm trying to use to learn ways to help with, let's say, setting numbers of communities when you do community detection or partitioning networks or graphs in helpful ways using the spectrum in a useful. The spectrum in a useful fashion. So, I'm going to give kind of an introduction and motivation. It's going to be very much related to, again, sort of a PDE-style approach to the question of partitioning domains or manifolds. I'm going to talk about how we think about this problem using a concept from mathematical physics known as spectral flow. I'm going to give an overview of how that actually gives sort of really useful information in ways to understand whether or not a partition you've Whether or not a partition you've constructed of a domain or a graph is optimal in some sense. And then I'm going to talk about a related optimization problem in the world of echi-partitions. And like I said, then I'll talk about how some of this extends to the discrete problem in ways that we understand. So, you know, just as a brief reminder, let's take a bounded domain with a nice smooth boundary. Let's take the infinite sequence of eigenvalues for the Laplacian. I'm just Values for the Laplacian. I'm just all of what I'm saying could be applied to very general elliptic operators, but for this stake of this talk, I'm just going to focus on simple, just the Laplacian. So, you know, think about trying to understand diffusion in some way on a domain or manifold. And I don't really care about the boundary conditions. Obviously, if I want to think about networks, Neumann boundary conditions are ideal. But if I'm thinking about some of the problems that I've been working on recently with That I've been working on recently with some of the people in this room, maybe some kind of Dirichlet boundary condition could be useful. But I don't really care about the external boundary conditions on the domain. What I'm interested in is the interior of the domain and how the eigenfunctions are going to inform me about the geometry of that problem. And there's a natural way. So in one dimension, there's Sturm-Louvo theory, and you can really understand sort of the oscillation of the corresponding eigenfunctions relative to the level of energy that you reach for the. Level of energy that you reach for the eigenvalue. And it's very simple in one dimension. If the kth eigenvalue oscillates k time, that's the removable theorem. In higher dimensions, it's much more complicated. So in particular, we can think of dividing up the domain into connected components where the eigenfunction is non-zero. And the number of those connected components is called the number of nodal domains. I'll call that new of that. nodal domains. I'll call that new of that eigenfunction phi k. And an old famous theorem by Courant, which is beautiful and not that hard to understand once you think about eigenvalues the way all computational scientists do in terms of optimizations and Rawley quotients, is that that eigenfunction can oscillate. It can have at most k nodal domains. So that means there's something that we call the nodal deficiency, which is the maximum amount you could oscillate minus the amount that you actually do. Minus the amount that you actually do. Okay? Yeah, no, please. Always, it will become zero on part of the domain, but on a lower dimensional subset at any point above the zero energy eigenvalue. So all of these have at least two nodal sets. The question will be how many it has. And the answer turns out to be. And the answer turns out to be kind of amazingly in higher dimensions that not only is this number rarely zero, so not only do you rarely oscillate as much as you could, but in fact, there's an energy level there for a given domain. There's a k beyond which this number is never zero. You never oscillate as much as you possibly can. That's the playel theorem. And it tells you that above. Above a given energy level, all eigenfunctions fail to be Courant sharp. They will always oscillate less than they could in dimension two and higher. In dimension one, they always oscillate as much as they can. That's the Sturm-Lubo theorem. And in dimension two and higher, they almost never, essentially beyond a given threshold, they do never oscillate as much as they can. Okay, so one task in spectral theory. In spectral theory, it relates back to there is an intuition, it relates back to some looking at actually a Weyl law. And the Weyl law, the way you really prove the Weil law for the sort of growth of the average number of eigenvalues below a given level is really by sort of cardining up the domain into little squares and thinking about Dirichlet and Neumann eigenvalues on those. That gives you a rate of growth. And then comparing that rate of growth to what would happen if you get if you get What would happen if you saturate this bound gives you a soft argument that tells you that it can't, that you cannot have something like a limb soup of some ratio go to one. And that's what shows you that there is a finite energy. And recently, using things like isoparametric inequalities or Faber-Cron type inequalities, some groups have been able to quantitatively bound the energy above which this can happen or above. Which this can't happen, or above which this number delta is always positive, never zero. Okay, so for starters, I'm going to focus a little bit on this thing, which is sort of, you know, going to be the way you could think about partitioning a domain would be that if an eigenfunction happens to have, I'll use a number other than k, l. I'll use a number other than K, L nodal domains, that's an L partition of the domain, right? And you could ask: is that L partition optimized in some sense? Is it a good partition of the domain? Is it giving me useful information relating to the geometry of that domain? Well, one way you might think about that is, well, how close is it to being sharp? How much is it oscillating relative to how much it could? Is it exploring features of the domain that go beyond sort of 1D? That goes beyond sort of 1D interactions? Well, for this nodal efficiency number delta, which is a function of the eigenfunction, and I will point out that you should think of it as a function of the eigenfunction because, of course, you can have higher multiplicity eigenvalues, right? And so this number still exists for any choice of eigenfunction that you make, given that. They came up with a really explicit formula for this using an index. Using an index. And that index is somewhat simple. It says: take the Morse index of an energy function. That energy function happens to be the eigenvalue of the domains given by that partition, set by that eigenvalue, the nodal domains of that eigenvalue, under variation in some sense over the partition that it defines. Partition that it defines. Now, that's a very high-dimensional manifold space. This is a complicated geometric animal. But what they said was they defined that variational structure. They defined a calculus on that manifold of partitions. And you can show that the Hessian of that, with respect to that manifold structure, of that simple energy, actually equals the number of negative eigenvalues of that Hessian equals delta. Remarkable theorem. Okay. You can think of a partition as just any way to divide up in a domain, a k partition, any divide up the domain into k components. If I have k nodal sets, the nodal sets give you a k partition. Now, I could also look at moving that partition such that the eigenvalue in each partition would remain the same as it is, of course, in each subcomponent of. It is, of course, in each subcomponent of that from that eigenfunction. Under that variation of what are called ecupartitions, which I will get into later, on that very complicated manifold of ecupartitions, the energy is just the eigenvalue itself. It turns out that a nodal partition according to an eigenfunction is a minimizer of that energy, and as a result, is giving. And as a result, it is giving you some information about the stability of that around that partition. Correct? So on the interior, where you're considering the partition, because the partition is being set by zeros of the eigenfunction, you actually are doing interior Dirichlet. Exactly. Exactly. Exactly. No, that is a theorem that they had to prove to establish this. It's usually true. So they did it for partitions that had some regularity. We're doing it now for piecewise smooth, so you can allow interior crossings. And it is a somewhat complicated. It is a somewhat complicated geometric argument that such manifold structure exists. It involves using things like GDZIC coordinates in this very complicated manifold space generated by vector fields. And it's a thing, but it is doable. There is manifold structure. You can differentiate with respect to it. And in particular, you can differentiate twice with respect to it to compute a hash. Okay? Now, then, so Graham was a postdoc at UNC under my colleague at the time, Chris. Under my colleague at the time, Chris Jones. And we, so Chris Jones is interested in stability theory of nonlinear states in reaction diffusion equations, mathematical physics problems, and in geometric characterizations of their stability slash instability. So things like index theorems are of interest to him, and he was interested in trying to do some things that make sense in one dimension in higher dimensions. So Graham, Chris, and I started talking, and we were able to come up with an alternative formula for this nodal efficiency in a way that we think. For this nodal efficiency, in a way that we think is actually much easier. So, rather than being a complicated variation over a very high-dimensional sort of complicated manifold of partitions, we were able to say the novel efficiency, it's actually a simple index. It's the number of negative eigenvalues of an operator defined on that partition. That operator is related to a Dierschletz-Neumann map. Now, it's not just a Dierschletz-Neumann map. Let's say that I have the dividing line partition here. I could put Dirichlet boundary conditions on this component, and I have two domains. I have an elliptic operator that I solve to the right, an elliptic operator that I solved to the left. If I put Dirichlet data on here, I can compute the Dirichlet's Neumann map from the right and the Dirichlet's Neumann map from the left. Ours is the sum of those two. The modified, we call it a two-sided Dirichlet. It's a modified, we call it a two-sided Dirich-letter neumi. Okay? But that's a well-defined object, as long as that domain is reasonable. Okay, so what you do, so in fact, we also have to shift by the eigenvalue, okay? And then you take And this defines the Dirichlet's normal map from the left. So it's you put in homogeneous Dirichlet data on the boundary, it's a voltage-decurrent map. You input voltage, you output the current along that boundary. Now, you can input voltage and read the current output from the left, or you can input voltage and read the current output from the right. Can input voltage and read the current output from the right. What we do, our map turns out to be the sum of these two things. So we like that because, okay, that's a simpler index theorem, right? It's much easier for me to understand the number of negative eigenvalues of a well-defined operator instead of this complicated manifold of partitions. But actually, what's kind of amazing is what we discovered with Graham Cox, Greg Berkaleiko. Greg Berkaliko, the same one who wrote this original paper, and my colleague Shiza Kanzani, thanks to a square we did at Ames, is that these two are actually connected. There's a reason why these two numbers are the same, and it's actually not that hard to see. And so we were focused on partitioning using nodal domains of eigenfunctions of the Laplacian. But now let me tell you about this notion of echi-partition, which is related, but not quite the same. Not quite the same. So instead, I can say, okay, let me take a k partition of some of my domain, p, which is just literally take it, break it into k components. And I call this an echi partition if the first eigenvalue of each of those subdomains is the same when I put Dirichlet boundary conditions on each component of the partition. Okay? So that means there's an energy of that partition. Of that partition, which I can just define as lambda of p, where lambda is exactly that first eigenvalue. Okay? Again, as I mentioned, these echi partitions near a given partition have a smooth structure. They're, in fact, a Hilbert manifold. And when we have smooth boundary conditions, we were able to similarly explain and understand the manifold structure for this problem. Now, what we Now, what's nice about that problem is an echi partition gives you a partition of the domain. And you could argue, so, you know, let's take a very simple domain like a disk. You know, if you want a three-partition of the disk, this obviously can't be nodal domains of an eigenfunction. Why? Well, it's not bipartite. Why? Well, it's not bipartite, right? I can't alternate colors. If this were an eigenfunction, it would tell me I was negative here, positive here, and negative here. And I would have to glue two negative directions together and have a discontinuity along the normal derivative along this component. So three partitions, for instance, are hard to do with eigenvectors, with nodal domains of eigenfunctions. But it turns out they can be realized as nodal domains of eigenfunctions of models. Of eigenfunctions of modified operators. In some work, they use magnetic Laplacians generated by Erman Bohm potentials. And in the work that we do, we introduce something like a cut Laplacian, which is related and tells you you want to look for modified operators that I'll introduce in a bit where you enforce some sort of jump condition along a hypersurface. Okay. Okay, so this was studied in a lot of capacities, a lot of work by Helfair and Sungvist, Helfair, Hoffman Ostenoff, and Teresini, the many, many places. And in particular, the key theorem there is to say that if I have a bipartite minimal echipartition, that partition is the nodal domain of an eigenfunction of the Laplacian. And if I have a non-bipartite Non-bipartite echipartition that minimizes this energy, it is the nodal domain of one of these modified Laplacians of a given eigenfunction. Okay, so the relationship between how you partition and the geometry of the domain and the operator that you're working with is all blended together in interesting ways. Okay. Now, let me give you a quick overview. Like I said, You need a quick overview. Like I said, this is just an overview of what we mean by our theorem. So, again, I'm going to take the Morse index for those of you who don't think about index theory as often as I do. That's just the number of negative eigenvalues, okay? And again, I said to think about it as the sum of these two Dierschlitz-Neumann maps. This epsilon is a fudge factor that we introduced because you have to work a little bit to think about what you mean by this Dierschleitz-Neumann map if this is an exact Dierschle eigenvalue. If this is an exact Year Schlei eigenvalue, it's not well defined. So, what you do is you shift it a little bit by epsilon to push the spectrum up. Okay, question just moment operator basically like a Laplacian composed of inverse Laplacian, so presumably it has eigenvalues. It does. In fact, it's not quite. It's a Laplacian composed with an inverse Laplacian composed with a derivative. So it's a first order operator. So, it's a first-order operator, and indeed it has a sequence of eigenvalues that are bounded from below, and this number is finite. But it is an elliptic operator bounded from below with an infinite sequence of eigenvalues. And so, okay, initially what we proved with Graham and Chris and then Graham and Greg was that we could only do this if this eigenvalue was simple, namely that the space of eigenfunctions there was really just one-dimensional eigenvalue. Just one-dimensional eigenspace. But then, in the work that Graham did with Helfair and Sunkvist with Greg, we realized there's an actually relatively simple correction to this, which is that actually equals the dimension of the kernel, one minus the dimension of the kernel plus this operator. So even in the case of high multiplicity, we still get a formula for this. But now you'll notice it depends a lot on the eigenfunction that you're working with as to what that is, because this number can change based off the eigenfunction that you choose. Based off the eigenfunction that you choose as a linear combination of elements in your space. Yes. So the easiest one exactly is the square. Square has very high dimensionality of its eigenvalues. So if you look at the one oscillation k in x. One oscillation K in X and oscillation L in Y, K squared plus L squared is the same whether it's K and X or K and Y and L and X or L and Y. So you get it automatically a multiplicity two base of eigenfunctions. And you can imagine those oscillate very differently. One of them would oscillate very nicely in the x direction k times, and one of them would oscillate very nicely k times in the y direction. And that's going to give you, if you add the two together, you're going to create openings. Going to create openings, and you would expect that number to generically go down from the optimal cases where you've sort of oscillated as much as you can in either direction. And it does. You can see it. And I just want to take a minute. I'm having fun. I hope you're having fun. But to motivate that part of the reason I wanted to present on this work is that So Jingfeng talked about divide and conquer strategies. And one of the things that he and I have as sort of a He and I have as sort of a in the back of our pocket, if we have the bandwidth to solve the problem, is you know, when he talks about divide and conquer, he's often talking about when you talk about solving an elliptic problem, solving an implementation of a Poisson problem is very different from computing the eigenvalues and eigenvectors of that operator. Now, what you can see here is we are doing something. In fact, the original theorem with Graham and Chris was to, in some sense, divide and conquer, right? I wanted to know, hey, how many eigenvalues for the whole. Hey, how many eigenvalues for the whole domain can I get by looking at the eigenvalues on subcomponents of that domain? And the answer is: oh, well, compute the eigenvalues on both of these with Dirichlet boundary conditions and add in that index of the two-sided Dirichlet-Neumann map. Now, the problem is the two-sided Dirichlet-Neumann map is not defined for the elliptic operator itself. It is defined for the shifted elliptic operator by the eigenvalue you're considering. So, it gives interesting ways to compute. So it gives interesting ways to compute the spectral counting function, but using it to divide and conquer in like a multi-scale way is very challenging. People have done this for tri-diagonal matrices, and one hope that we have by better understanding this operator and how to compute it is twofold. One, to use it to give interesting ways to analyze the geometry of a network or a graph by understanding the geometry of its eigenvectors. And two, to understand how to use these low-rank the restrictions. Rank Dirichlet to Neumann map operators. Now, Dirichlet-Neumann map on here is not low rank, but I'm going to tell you in the end that a Dirichlet-Neumann map on a graph is a sure complement. It very well can be low rank if the boundary is low rank, okay? So trying to figure out ways to divide and conquer, use it for the eigenvalue and eigenvector problem is partly our motivation for understanding these operators. Okay. Okay. And like I said, so just reiterating, this does depend on the choice. Reiterating, this does depend on the choice of eigenfunction that you make when that choice is not given to you by it being a simple eigenvalue. Okay. All right. So like I said, we originally described this as arising from some sort of Maslov index, very symplectic, very abstract formula. And Greg learned about our theorem compared to his, where he knew it was very complicated differentiating over this abstract. Differentiating over this abstract manifold of partitions. And Greg also was sort of classically trained in math physics. And he said, Hey, I think we should work together. And there's a simpler way to prove your theorem using what he calls spectral flow. So let me explain that because it's a beautiful idea. And it, I think, helps to understand some of the low rank contributions, like what we really mean by what's happening here with low rank. Okay. And I think will help connect to what I will show you at the end, which is the discrete version of this. End, which is the discrete version of this that's work in progress. Okay, so let's take what did I do? Yeah, okay, I did it. If you hold on the forward button too long, it does this. Okay, anyway. So let's think of a family of self-adjoint operators, L sigma, where L is some elliptic operator. Okay. I don't really, again, I don't really care about this, but just for simplicity, let's just take a Dirichlet boundary condition. Simplicity, let's just take a Dirichlet boundary condition on the outer domain. Fix because that's not that the outer domain doesn't really matter to me. I care about the partition of the domain. Now, in order to make this sigma count, what I'm going to do is introduce boundary conditions that say, hey, along some partition, defined in this case by the nodal set, because we're talking about eigenfunction, introduce a funny robin-type boundary condition. Type boundary condition, where the sum of the normal derivative from the right-looking component and the normal derivative from the left-looking component plus sigma times the solution itself have to equal zero. Now, if sigma is zero, and also by the way, there's a continuity. If sigma is zero, what's this going to tell you? Well, it just tells you that the normal derivative to the right plus the normal derivative of the left are zero. That's a continuity condition in the normal derivative. That's what the eigenvalue has. That's what the eigenvalue has, the eigenfunction has to satisfy. So the spectrum of this operator at l equals zero hasn't introduced anything in the domain. It's basically just potential plus zero. As sigma goes up, as sigma goes to infinity, if you'll think about this, what this tells me is that, you know, if I divide through by sigma, that really what I'm making happen is that the function itself has to go to zero. function itself has to go to zero. In the sigma goes to infinity limit, I'm actually going to a Dirish lay boundary condition on the partition. And in the interim, I have a nice self-adjoint extension of the Laplacian defined on this partition. And it turns out that you can prove that you have still an infinite set of eigenvalues. The eigenvalues at sigma equals zero are the eigenvalues of L, and that all those eigenvalues are going up monotonically. B can be any potential here of sufficient niceness to make sure that the elliptic operator is well. Yes. Okay, so what happens is that from this definition, let's go back, right? What does it mean if I think about what happens if minus sigma is an eigenvalue? Because if minus sigma is an eigenvalue of the Dirichlets and Neumann map, right, what is the Dirichlet-Neumann map? The Dirichlet-Neumann map is the sum of the two normal derivatives, right? And it's exactly saying, hey, the restriction on the partition should equal, times sigma should equal the sum of the two normal derivatives. Okay, that's exactly. Now, when that happens, sigma is an eigenvalue of that Dear Schlitz-Neumann map precisely if the operator that I've used. To define it, lambda star is an eigenvalue of L sigma, right? Because I'm defining that Diri-slated neuron map not for L, remember, but for L minus lambda, okay? Well, those are non-decreasing. So it turns out, here's the argument. My argument is that precisely K, the possible number of oscillations minus the number of nodal domains. Minus the number of nodal domains have to pass through that energy. Why is that? Well, if at sigma equals infinity, I am at the Dirichlet boundary, then each component of that eigenfunction at lambda star is actually a minimal eigenfunction for an elliptic operator on that now Dirichlet domain. And it is simple by a proof, by a theorem in elliptic theory, which means I can only have at most exactly the number of eigenvalues. Exactly, the number of eigenvalues, the multiplicity of that eigenvalue has to equal the number of nodal domains. So that's how many can be left at sigma is infinity. Prior to that, I started at the k-th energy level, right? So how many have to cross through exactly the null division, okay? So here are some pictures of that flow in a couple of examples. On the left, we used a fun family of models that are Of models that are sort of continuum networks called quantum graphs that are in and of themselves very interesting from a point of view of spectrogeometry and physical modeling. And on the right, we did a rectangle. And so what you can see is some of them cross through. And here, notice that this is arc tan of sigma, so that's why it's only going to pi over two. Okay. So you see also that, by the way, sometimes. So, you see also that, by the way, sometimes these things they cross, okay. So, not, you know, they don't, they don't, they move monotonically, but they don't have to non-intersect, okay? All right. So, the question we would love to answer in general from a spectral geometry point of view is, could I look at the geometry of the eigenfunctions and tell you which one should cross and which ones shouldn't? Can I use information about the eigenfunctions? Information about the eigenfunctions and the relationship to the geometry to decide which ones are responsible for leaving and which ones aren't, which ones are oscillating a lot, which ones are oscillating too much or a little. And the answer is we can do that, but only in the case of separable problems. We can do it on the rectangle. So let me just show you a picture of this because not in a bad way. I'm going slower than I. Slower than I had planned with all the slides that I've made. So I might not spend as much time on a few things. So you can represent the spectrum on a rectangle, a separable rectangle, using these lattice points, right? Because I mentioned like K L, right? So if you think about the aspect ratio in the X direction versus the aspect ratio in the Y direction, this lattice represents the different The different eigenvalues that you can possibly find for the Laplacian on that domain. And if it's a simple eigenvalue, then at a given energy level, no other points in that lattice cross through. And if it's higher multiplicity, then maybe two points cross through the energy shell defined by that lattice. Okay? Now, what it turns out is you can do a relatively simple counting of oscillations argument, and you can say, hey, it turns out that the nodal deficiency should be equal to the number of things inside this. To the number of things inside this box. Sorry, the nodal count should be the number of things inside the box. And the nodal deficiency should be exactly the things outside that box. Inside the shell, but outside the box. That count is true. It turns out we can explicitly do the spectral flow, and not only is that count, we knew the counts would be equal, but it turns out if you do the spectral flow, these are the ones that cross, and these are the ones that do not. And these are the ones that do not trust. Okay, so it's not, everything is, as Greg likes to say, everything is connected. Now, there are complicated problems and non-separable problems because I showed you that example of the quantum graph. And it turns out that's not actually a crossing. It's what physicists call avoided crossing. And there's actually some interesting things that are happening there. In general, the geometric structure of these eigenfunctions tends to actually go and exchange through an avoided crossing. Actually, go and exchange through an avoided crossing, but it's much more complicated to understand. So, really, the geometric structure of the bottom one is actually the one that probably was responsible for the crossing, even though it's the top one that crossed eventually. Okay, but that's complicated. All I want to say is that it's complicated. Okay, the observation is, of course, that those numbers are equal, and it turns out that there's a reason for that, and I don't want to get into that too much. Okay, all right, so let me uh. All right, so let me get a little bit into how this is proved because it involves Dirichlet forms, which I know that probabilists and statisticians do care about a little bit when you think about doing estimates. So the way you can think about that operator L sigma, and in fact, the way that you can see that it's monotonically increasing, is by thinking about this from the weak formulation. So think about that, the eigenvalues of L sigma via the bilinear form B sigma, where what sigma does is exactly introduces an extra term integrated. An extra term integrated along that partition. Okay? And now, when you consider looking at the family of eigenfunctions defined by this bilinear form, minimized over, you know, apply the minimax theorem to this bilinear form, you can easily see that this is an increasing sequence of eigenfunctions, exactly the Dearsley-Tenomin map eigenvalue calculation that I showed you. eigenvalue calculation that I showed you. And in fact, it's only possible that it happens in that way. And you can show that from this monotonicity, the only thing that you actually have to check is that even though you're increasing, you have to make sure that if you cross, you cross at a positive velocity. And so there's also a sort of Hadamard variation style formula you can use for the variation of these, and you can really prove. Of these, and you can really prove that for an eigenvalue coming from below, the eigenvalue of L sigma at the crossing point is equal to the integral of the corresponding eigenvalue squared. And as long as this thing doesn't vanish everywhere along the partition, it has a positive velocity. The only way it can vanish everywhere along the partition is if it oscillates, is if it's zero everywhere that the eigenvalue. zero everywhere that the eigenvalue, the eigenfunction for lambda star is, and that means it has to have a higher eigenvalue. So all lower eigenvalues have to cross through at positive loss. All right. So we have the sequence of ordered eigenvalues. They converge. You have to show that, so you know, it's not, it's a singular problem because as sigma goes to infinity, it is a singular limit, but you can make sense of it coming from the right, and you prove that it converges to the Dirichlet eigenvalue. The Dirichlet eigenvalue, and the count is exactly as I mentioned. Okay, all right. So, again, there's a if this happens to be a degenerate space, you have to deal with the kernel of L0, and that's where you have to introduce the multiplicity of the eigenvalue. Okay? I'm going to very quickly speed over this, but it turns out you can use these same ideas to prove the original Stern-Lupel theorem if you want. And in fact, if you just take the Laplacian on an interval. On an interval, this becomes a really simple object to understand. This is a Laplacian with a family of delta functions that are increasing in strength. So now you can really see in this 1D example why it's an increasing potential. And it turned out that we were able to prove, A, the Sturm-Louval theorem, and B, to establish an exact formula for this in the case of the, oh, sorry. When there's no potential we're at. When there's no potential, we were actually able to show exactly the eigenvalues of the spectral flow. It gave really special control over the rectangle problem, which can be decomposed into these two, okay, into two versions of this. Okay, so questions on the spectral flow. What I really want you to take away from this is that there's a relatively simple operator-theoretic way to understand how to use this two-sided Dirich-Schlat-Denomin map to capture the oscillation of an eigenfunction. That's the key takeaway. Okay? Dirich-schlit-denomination. Key takeaway. Your slated map on a partition related to how much the eigenfunction generating that partition has oscillated. All right. Now, I'm going to tell you what this theorem says. I know it's too many words, okay? But I couldn't figure out how to shrink it to anything more digestible. So let's spend a little time on it. I told you there's a relationship and that there's not, it turns out not to be a secret why this. A secret why this potential perturbation, this delta function-like perturbation that's generating the spectral flow, which is indeed low-rank in some sense, because it lives on a much lower dimensional space, even though it's still an infinite dimensional number. These objects are related to this complicated manifold structure that were introduced by Berkoliko, Kuchman, and Spolansky. How? Okay. Turns out, Turns out, give me a partition. I can define this manifold structure, like I said, in terms of, let's say, some local vector field generating how to push on that partition, and that those vector fields have some amount of regularity. Now I can, with respect to those, the generators of those vector fields compute the Hessian of that energy that I mentioned. And here's the main theorem. Main theorem. The main theorem is that that Hessian can be written as a quadratic form. Well, that's not a main theorem. That's everyone knows a Hessian is a quadratic form. Okay. What's the quadratic form generated by? It's generated by the two-sided theory. Okay. So using the explicit structure of the how you generate sort of trend, how you generate this manifold of partitions locally using vector fields that have certain regularity properties. Regularity properties, we can prove indeed the Hessian with respect to that energy is similar to the operator that we had derived as well. So there's no secret. Of course, the Hessian, the index of that is equal to the Morse index of that operator. Couple of quick things. You know, we're working, you know, quadratic form has to be find on a domain. It's important that we're working. It's important that we're working in two dimensions here. Just think about this in two dimensions. It can be generalized to higher, but in the space H1. Why is that important? This we proved for smooth partitions with no self-intersection. Still cool. But if you think about a canonical nodal partition, you know, You know, you think about things that have interior crossings, right? Okay, so what we're working on now is proving that this theorem still holds, even in the case of sort of piecewise smooth things with interior crossings. And the reason that H1 half is so important is that we need to make sure, see this function rho, it has to do with the Neumann data. Neumann data of the Laplace of the eigenfunction restricted to that partition. Well, obviously, that has to vanish at an interior crossing. And I've got a one over row here, right? So I need to make sure that this thing makes sense in the neighborhood of a crossing, even with a vanishing weight. What that means is I need to show that everything that's responsible for the negative index here can be taken to vanish at that crossing. Be taken to vanish at that crossing, so it lives in the domain of that operator. That's where H1 half comes in very strongly because it turns out that on one-dimensional things, you can approximate something in H1 half by something that locally vanishes. You go to H1 half plus epsilon, you cannot. This is exactly the threshold where we can actually define that all the full, the full Hessian is exactly equal to the number of negative eigenvalues of this operator. That has cool properties because I, so I mentioned. Cool properties because I so I mentioned that we want to learn some things about geometry. But what's interesting about this is in certain examples, well, if you were good computationally, you could do this in a lot of examples, but in certain things we have analytic formulae for what these Dirichles-Neumann maps look like. For instance, on squares. So you might say, okay, or for instance, on a disk, an easy way to generate a six-partition is with this. A six partition is with this, I like to call it the trivial pursuit partition for those of you who played family games as kids, okay? And you could ask, is this a minimal partition? Okay, and the answer is no, it's not. We don't know what the minimal partition is, but we can compute the Dirichlets-Neumann map for this eigenfunction. And it turned out to give us a candidate because it told us how should I move, how should I start pushing the partition such that this. Pushing the partition such that this thing actually goes towards the minimum, right? What are the negative directions of the Hessian? That gave us a shape that looks like this, which we found numerically, and we think is a candidate for a genuine, like minimal six acupartition on the disk. So, like, and okay, that's a sort of special geometric analysis point of view, but. Geometric analysis point of view, but what I want to say is that knowing the negative eigenvalues of the Hessian, you'd love to say it gives you a geometric flow and you could go everywhere. But a lot of this is really very dependent upon being a minimizer. So it can't, we don't know how to use it to flow to a minimum, but we certainly can use it to tell you how to move towards a minimum. And then you have to try to sort of guess what's happening. Okay, so like I said, so now we can also deal with things. So now we can also deal with things with corners. Okay. For echi partitions, you have to be a little bit more careful because, like I said, not everything's guaranteed to come from this bipartite structure. So we have to introduce something we call a cut Laplacian on certain components of a domain, where we consider these sort of anti-matching conditions. It may look a little bit funny, and it should. A little bit funny, and it should because you know, if I were genuinely looking at matching conditions, I would put a minus here because these look in different directions, and I would have a plus here for a continuity condition, okay? But if my partition is bipartite, this anti-matching conditions are unitarily equivalent to the standard matching conditions, and you get exactly the eigenvalues of the little possession. But with this, it turns out that we can generate, you know, That we can generate, you know, if you introduce a partition or a cut, you can use it with these anti-matching conditions to generate a new operator. And that new operator, it turns out, you can use to understand how to find non-bipartite partitions. Okay? I'm running a little bit out of time, so I don't want to go too much into that, but a lot of this is very related to the work of Helfair and collaborators who proved a very similar theorem to this for Hermobohm Laplace. For Ermov-Bohm Laplacians. Basically, what they said is: you know, take a k-partition, and the following are equivalent. That partition minimizes my energy, it minimizes over a certain class, and it's the nodal partition not of a cut-Laplacian, but in fact of a magnetic Laplacian. Okay, so again, the nice thing is that we understand in this case that we have this beautiful relationship that I'm a critical partition if and only if. That I'm a critical partition if and only if I'm the nodal partition of a cut Laplacian, and that I can really relate whether or not I'm genuinely minimal by counting the nodal division. Okay? All right. So like I said, these are very much related to Armov-Bohm type operators. We've lately understood that these Armov-Bohm type operators are actually really closely tied to double covers of domains, and the cut Laplacian can pretty easily be seen to be strongly. Pretty easily be seen to be strongly related to a double cover type operator. So we expect that these things are really equivalent in a nice way, but the operator theoretic proofs of that are very complicated, and we don't know exactly how to prove. But I will say one nice thing about what we've got is that it gives a simple way to relate partitions to whether or not they are connected to cuts using ideas from algebraic. Using ideas from algebraic geometry like homology. And that's giving us tools by which we're able to, for instance, say in the Mercedes star case that this is minimal under a family of things where you would consider cut Laplacians for any cut Laplacian that includes a line from the center point to the boundary somewhere. It doesn't have to be this one. You could consider a cut Laplacian where you considered something like this as the cut. Something like this as the cut, and in that class of things that include this piece as part of its partition, this would be the minimal three partition because of that relation. So there are some things you can get from the geometry. Okay. All right. So this is just, like I said, a statement of that, which is related to these. You know, if you don't know what homology is, I only learned it about six months ago. So don't, you know, I'm not, no judgment. So, don't, you know, I'm not no judgment here, but it gives you a nice tool for understanding how to minimize over a large class of partitions, even in, you know, where the partitions only, you only care about how things interact with some point on the interior and the boundary, not the global structure of the partition. Okay. Okay. So, an old theorem of Helfero, Vinostov, and Teresini is that a bipartite partition is minimal if and only if. Minimal if and only if, oops, sorry, where'd I go? There, if and only if it's Curant Sharp eigenfunction Laplacian, we can do the same thing, and now we can prove that indeed a partition is minimal within a certain class if and only if it is a nodal partition of a cut Laplacian with a cut that is homologous to that class. Okay. All right. And as a corollary, again, And as a corollary, again, like within the class of things that are homologous to that cut, this is really minimizing over that family of non-bipartite papers. So one corollary of that, like I said, is that the Mercedes-Star transition is minimal among all three partitions whose boundary set contains any curve from the origin to the boundary. All right. So based on this question, I do want to show you a little bit of other. I do want to show you a little bit of other work that we've done. Check how much time I have real fast. Okay, I have just a few minutes. Relating to how the geometry of a domain impacts how you would partition it, how you would cut it, and especially as you think about other ways to generate cuts and ways to parse domains into clusters. So with our postdoc, Tom Beck, and my colleague Shiza, we were interested in sort of low-lying eigenfunctions on things that were Eigen functions on things that were near rectangles, which may sound boring to someone who thinks about high-dimensional data sets, but the reason we looked at rectangles was that there's this old result of Arthur Slom in work with Koufman that suggested that if I take a very general domain, and of course you extend this by saying very general graph, right? And I start partitioning it by first taking the first oscillatory eigenfunction, cutting it along some Along some nodal set there, and then iterating that process. Like now, take a new operator on this cut graph and then cut again, similarly with the first eigenfunction and oscillates, and iterate that process. Sorry, this figure is small, but you can see that the limiting operators look a lot like rectangles. So the conjecture is that there is structure, that ways to generate some sort of like wavelet structure on something like a domain or a graph is to iterate this cut process in a nice way. But that in the geometric But that in the geometric setting, the limit should be something like a rectangle with the given aspect ratio. So we were curious: is that stable? And what we were able to prove is it is. In particular, that rectangles, I can perturb rectangles of a certain aspect ratio, and the cut line of that has less oscillation. It is more smooth. It is closer to being rectangular than their original wavy edges. And to do that, it required a lot of very careful and detailed harmonic analysis, okay? And detailed harmonic analysis. You have to use Hadamard variation, you have to use, you're close to a rectangle, so you get to use very sharp Green's function estimates. But it's very, very, very sophisticated analysis, I would say. And it goes back to very similar work of Griez and Jarrison, who instead of bounding exactly the oscillation of those lines, they really worked on bounding only the diameter. They're just like bounds on how much it could oscillate, but not bounds on how much the derivative. On how much the derivatives change on the interior? Probably not. Maybe in 10 years. So, this is just a representative figure that we had. So, in a domain like this, what we can show is that this line, once you rescale out to the same aspect ratio, gives you something much flatter than the original. As long as that aspect ratio is sufficiently large and that n is had to be like five. His head be like five. Okay. But you could ask: okay, for that problem, we had a non-degeneracy condition which said that the eigenvalue we were considering had to be simple, but that is not always required for given aspect ratios of the rectangle. You can get, let me just show you the picture because I'm running out of time, you can get multiplicity two eigenvalues on those rectangles, and then things get a little bit wild. And what we were looking at was, hey, if I perturb this boundary, then I should get an interesting. Boundary, then I should get an interesting perturbed domain that still has a lot of oscillation. That's what I thought. But of course, it turns out once you perturb it into your crossing, it generically opens. So, you know, Karen Ullenbeck knew this a lot sooner than I did, a lot earlier than I did, but oh, that didn't show up as well as I'd hoped. Let me walk you through what I'm seeing in this picture. I only perturbed one side of this. This was work with Tom and an undergraduate of ours. And that interior crossing split open. And the nodal deficiency went up, right? Efficiency went up, right? It had four oscillations, now it only has three oscillations. Turns out we can characterize this opening very precisely. It's a hyperbola, we can say a lot about it. And the cool thing is that I told you before that if I only had one dividing line and no internal crossing, that it got flatter, that it really was better than the bound that came from the side. The opening of this is actually independent of the aspect ratio. Even if this aspect ratio is huge. Even if this aspect ratio is huge, this opening is related to the size of that perturbation, which I thought was quite interesting. So it tells you interior oscillations are very, very sensitive to perturbation. So we also did this with one Laplacian with a former student of mine, Wesley Hamilton, and Hao Teng Wu as an interesting way to do cuts. And we were interested in thinking about how these And we were interested in thinking about how these cuts apply to graphs. So, let me just, my last thing that I'll tell you here is how to do a spectral flow a little bit and think about this on a graph. So, let's take a standard graph Laplacian, unnormalized, so D minus A. Let's take a symmetric A. Okay, I'm not trying to, don't make your life too complicated at the moment. And I want to think about an eigenvector and what happens when I see a crossing along an edge, right? That's what I'm going to call. Crossing along an edge, right? That's what I'm going to call a transition. If the value of the eigenvector at one node times the value of the eigenvector at another node is negative and there's an edge between those two nodes, then I call it a crossing. Okay. Well, I can think about how to introduce effectively now a related rank one perturbation that will explore, again, a spectral flow generated by that crossing. Flow generated by that crossing, so along that curve, and there's sort of a canonical way to do it. Then you fidget with wanting to keep the eigenvalues fixed at the zero perturbation, et cetera. And you can define this as almost like a discrete delta function that would be localized in between two nodes on an edge. Okay? So you can write it in, where did I go? You can write it very much. You can write it very much. Sorry, I went way forward than I realized. Oh, I'm helps if I'm going the right direction. My apologies. Okay, so you can write it very much like an original matrix operator, plus something like a low-rank perturbation, where the rank of this is determined. rank perturbation where the rank of this is determined by the number of crossings. Turns out when the eigenvector never vanishes, which is generically true, you can do this as a, these p's are defined as this parameter s doesn't have to go to infinity, it goes to one in the natural spectral flow. And what you get, and this was my student Wesley Hamilton's thesis, what you get is, again, something saying the nodal deficiency is equal to exactly the number of count, the crossing of the count that occurs. Of the count that occurs as s goes to one for the vector flow, which is now genuinely a low-rank perturbation, because these only live on a finite number of edges where crossings occur. Okay? Okay. I'm running out of time, so I don't want to spend too much time talking about this, but I do want to just quickly say that there's a relationship here still between a Dirichlitz-Neumann map, which I'll just show you very quickly with a slide. So when I So, when I look at the two-sided Deer Slate's Norman map, I'm trying to understand solutions like this: right, this is my perturbed spectral flow, s going to one. I have my eigenvalue. I'd like to understand how the index of the operator that I'm looking at, the cross number of crossings, correspond to some kind of Dear Schlitz-Neumann map. Well, here's my point: if I rewrite this, you realize this is exactly a sure complement on the set of crossing nodes in the graph. So you can really. The in the graph. So you can really write that there's an equivalent, again, low-rank Dirschlet-Neumann map, two-sided Dierchlet-Neumann map operator, whose index is equivalent to the number of crossings that you would experience from that spectral flow. Okay, so there's a very canonical framework to do this where sure complements arise very naturally as the things to study. And I will do this obnoxious thing where you click too fast, just to the last slide. Fast just to the last slide. So, because I don't have enough time to talk about discrete cuts and discrete echi-partitions, but let me just say that they exist and that the takeaway is, in particular, a lot of what I've talked about here is a nodal set would be on two dimensions and two-dimensional crossings. Doing this in higher dimensions in the continuum, I think, is very interesting, especially if you have data sets that you think are embedded in higher dimensions. Understanding the properties of how happens. Of how what happens with the regularity of these operators and that two-sided Dirichlet Nomine map being defined for everything. You know, cusps are possible in higher dimensions at crossings, so you have some things to do. Again, asking about the geometric signature. My grad student, Andrew Lyons, first project was looking at what happens at those at high multiplicity settings and realizing that the nodal sets of boundary deformations depend very wildly on the multiplicity and the perturbation. And the perturbation. And then we also have been working on, like I said, with Shiza and Tom, we've also been working on applications to domain decompositions and how to get good elliptic boundary value methods to get convergence results for domain decompositions, but so far only for things like solving single iterations of a Helmholtz problem or a Poisson problem. Doing this for eigenvalues and eigenvectors, very much open. There's a result that does it for tri-diagonal that uses some of the low rank structure. That uses some of the low-rank structure and the fact that it's genuinely low-ranked because crossings are only happening at finite numbers of nodes, generating this to higher-dimensional or more complicated matrix models would be extremely useful. And we're hoping that some of this spectral flow and recognizing how to build these sure complement operators can help. So that's all I've got for today. Thank you very much for your attention. Okay. So, yes, so okay, good question because I should have said this and I stupidly didn't because I was caught up somewhere else. So when you look at the so one way that you can solve the Deer Schleigh cut problem with good numerical methods, Braxton has done. Good numerical methods. Braxton has done this in a lot of interesting cases, for instance. And you can even try to optimize over how that happens. And there are situations in which doing that looks a lot like finding some kind of optimal cheeker cuts in certain settings. Is that guaranteed? Not necessarily. But one thing that we're working on to try to understand is: let's say that you have, let's start with a domain, okay? With a domain, okay, even though the nice talk on the stochastic block model was absolutely related. So, one thing that we're working on is this type of problem. Let's say that you have something that we call a chain domain. You have communities connected by necks of differing widths. Connected by necks of differing widths, right? Okay, you can, without too much work, construct low energy eigenfunctions by basically combining the ground state eigenfunction from these domains in a smart way. So at low energy, the community structure appears very naturally, and you get actually like a Sturm-Louville type theorem that says you should, you know, this is a four chain. This says you should oscillate the first four eigen. This says you should oscillate. The first four eigenfunctions should definitely oscillate, they should be Courant sharp. Okay, what if you get above that, right? Well, any Courant sharpness above that should be related to interior crossing. But I just showed you that interior crossings are remarkably sensitive to perturbations, including adding necks. So, one way that you could think about using something like a clustering type algorithm or a spectral approach to this would be to say, hey, Approach to this would be to say, hey, if I have a graph data set that I don't know much about the community structure, and I'd like to give a bound on how many communities I should look for, you can try to look at something like this spectroflow argument and say, hey, what's the last Current sharp eigenvector that exists for this thing? That should be a good guess as to the number of communities that you can find. And what Tom Shiza and I recently did prove in the continuum case. In the continuum case, is that I mentioned that play L bound, that energy above which you can't have Courant sharpness, that it is independent of neck width in chains in domains like this. So we were able to prove in a continuum problem that actually the threshold above which you can prove that you can't be Curant sharp is independent of how thin these necks are, which you need to be true in order to think about that being a good way to measure for community structure. That's a Community structure. That's a conjecture for the relationship between Curant-Sharp eigenvectors and how many communities to look for in a spectral partitioning or clustering algorithm. That, you know, what we know is there's a threshold above which it should kind of be true, below which it should kind of be true. The intermediate range is very complicated. There are a lot of symmetries that could make it complicated. The other thing to keep track of is that, you know, do You know, doing these Dierchlay partitions is a means of partitioning. So it is, you can use it to do clustering. Now, when's the right thing to do? When is it the right thing to do the clustering? When will it give you the best results? That is dependent upon the problem and the graph that you're looking at. But it is absolutely a way to cluster and, let's say, partition the graph into various, into regions. Yes. Yeah. So it's a rank, P is a sum of rank one components, where each rank one component is generated by one of these crossings of the eigenvector. So let me, I think I have it written. I was also trying to show you the magnetic Laplace in point of view. Yeah, okay. So let's say that you have E0 total crossings of your eigenvector. So edges along which the eigenvector changes sign. And I can write P as a sum of rank one operators where Of rank one operators, where this k matrix is determined by a linear combination of the relationship between the values of the eigenvectors and the unit vectors localized on the nodes where the edge connects. Very hard of you know I feel I interpret it now with one of the things that that content is better than the original how we can improve the community technology but it it just looks like more couple of parties. Oh, that's I would love to get that reference from you so I can see exactly what they proved. So, because what what each of this is, you know, of course, the rank is the number of crossings, but for each one of them, it is really just a rank one perturbation. And so, as you add them one by one, you can use various tools to say what's this doing to the to the. To say what's this doing to the spectrum. Yeah, right. And that's an interesting, so the other thing about that is I mentioned that this is much easier in the case where the eigenvector is never zero and that that's generically true. What that means is if it is ever true, you probably need to perturb in some way to make it true so you can do the easier spectral flow argument. Otherwise, it gets kind of. Easier spectral flow argument. Otherwise, it gets kind of messy if you have to include zeros. You can get much higher rank because you have to introduce something called ghost points. And so, there, like for each one of these crossings, you would introduce a new vertex where you actually represent the zero set. And that gets much less computationally appealing because the rank, you actually start adding vertices to your graph. Yes. It is invariant. Yeah, yeah, yeah. This is, it is invariant. Yeah, this is just the way you've labeled them, you can then you generate this matrix based on the low rank structure that occurs between, you know, for determined by the labeling set and the edge that is in the arrangement of P. Yes. So okay, so this is an excellent question. So this is related to some things that we're playing with now, which is that the two-sided Dirichlates Neumann map flow indeed can be very nicely seen. From the point of view of layer potentials. And the really nice thing about it is that if the boundary is smooth between the two domains, then you actually, when you sum the two together, you sort of can remove the singular part and have a nice layer potential representation as just a very smooth operator. If there's a corner, however, you can't, and things get a little bit more complicated. But we But we haven't so much focused on the numerical implementation of this and ways to actually compute it efficiently. We've been more working with the abstract notion of it and when it exists. But in particular, like I said, in trying to go to higher dimensions where you need to understand more complicated partitions and more complicated regularity of your boundary set, the question of when is it defined and how well can you compute it and how can you accurately compute it is. Accurately compute it is something that I would like to understand much, much better. Okay, I do think so. And from a spectral geometry point of view, I've been working on this polya conjecture recently, which is a way to relate the difference between the Dirichlet and the Neumann eigenvalues for a given domain. And what's interesting. And what's interesting about that is you can still use kind of a spectral flow approach to get something that ends up being really related to analyzing properties of a layer potential operator and needing to get some kind of signed operator out of integrating that layer potential operator over a given contour. So I know that you can relate it to a layer potential operator, whether or not it can be used to then get either good contour. To then get either good computational strategies or good estimates, is something that I'm still trying to wrap my head around. Computationally, I think it's absolutely the way to compute these two-stratidir-shaped thermometer maps if you wanted to do this in more general domains than the analytic ones that we have been doing examples on. And I know that there are a lot of people who have very good methods in domain decomposition for computing these Dierch Latino maps on submanifolds, but I haven't delved into that. Delved into that in more ways than what Jingfang talked about, where we were looking more at solving elliptic problems and how to do very good approximations to layer potential near the boundary. I haven't played with how to use that to get good numerical implementations of this in general domain. But it's a good question. And I would love to do it because it's fun to look at rectangles and disks, but you also want to consider other more general domains. 