Again, so this is a this is a spatial temporal data. So it's useful to look at the spatial and temporal spath. So first thing here, I just create a map and then label those spatial locations. But in order to do that, you really need a spatial information, which you can grab from this object. You have the latitude and longitude, and also the station information. The other thing you can do is look at the marginal distribution for the wind speed for each location. And one thing I would tell you is if you just use the default for the R, the histogram is pretty ugly. Somehow they don't figure out a nice way to choose the binwidth. So that is something I come out with that bin width is look a little bit nicer. This blue curve is. This blue curve is so-called a kernel density estimate. So, how many of you have heard about this? Some of you. Yeah, very good. So, in a sense that this is sort of more defined version of histogram. So, basically, one thing is that the bin width I choose is probably reasonable. It's matched with the kernel density reasonably well. So, you can, so here for that exercise, you can actually do that for all the locations. And if you want to be All the locations, and if you want to be even fancier, you can probably put those different locations corresponding to the geographical location to have an even nicer visualization. And also, you can look at obviously Taiser's plot. That's a lot of detail here. So, I will highly recommend you to do something very similar to what Richard did yesterday. You want to zoom in because if you look at You want to zoom in because if you look at that, you see something going on. It's even hard to see the structure, for example, the seasonality. So you might want to zoom in to just a few years or even one year to see a better structure there. They can do that. But the other thing, I think some of that may be covered last year, maybe by, I think by probably Julie and also Dunes, who you talk about like a spatial temporal process and the simple, like it's for. And the simple, like, exploratory data analysis you can do is try to look at some kind of summary for the correlation. So, for this type series, the one thing you can do is so-called the lag one, lag one for our lag one. And then you can sort of give some idea about the lag one or the short-term correlation. This is the one of the station, and you see this scatter plot. But this is the scat. Typically, when you do the scatterplot, you have two variables. In this case, you have the set variable. Case you have the same variable, but in the x-axis, that is that variable the day before, and this is the wind speed at that location today. And you can see that's pretty substantial, lag one, which means in this case, one-day correlation. You can do those exercises, or you can, for example, so here we So, here we tune in for the very first year, and you can also do some smoothing to get some idea about the sort of seasonality. So, here I take the very first year, and then I just fit a smooth curve there. But remember, in this case, we actually have 18, I think, 18 years. So, actually, you do have more information. So, if you want to have a better estimate of the seasonality, that's one thing. That's one thing you can do. That's not the only way, but that's what I did here. So, for example, in this location, again, for each day of the year, you have sort of 18 replicates. So, if you assume the distribution is stationary. So, for each day, basically, I take the average. So, for example, January 1st, I will have 1961, 62, 63 all the way. I take the average. So, I have those average values for each day. Average value for each day of the year, and then you can fit a smooth square here. All right. All right. And also you can explore some spatial snapshot. You can have this kind of static version, but you can also make annulation to see the complex spatial temporal structure. Although the data here is pretty sparse in space, so it's not that easy to see the dynamic. To see the dynamic. But if you want to play with the if you want to play with the CAN RCN, although I'm not sure whether it's easy to fit in your computer, but if you subset a smaller region, you probably can create a very nice animation to show the spatial temporal structure. And also that's something else. So that's a challenge. I think that's actually somewhat related to what the material business covered last year is the correlation could have. The correlation could have some type of n-symmetry. So, for the wind, it's probably due to the prevailing wind direction. And that is actually a good example to explore that. And in this particular data set, all right. But before I move on, I want to see if you have any comments, suggestions about the exercise or the way I do the lecture. Yes, no. Okay. All right. So oh, the other thing I forgot to mention, I just posted a link in Slack. So yesterday, I did not have time to cover, and this is actually a very important practical step because when we do the extreme value analysis, we start with a very simple assumption. We say the random variable under consideration is ID independent. Is IID independent and identically distributed, it's almost never be true for the environmental type series we work on. So, this part is trying to give you a high-level picture of how we're going to do that under the more realistic setting for each method we talked about yesterday. Remember, we talked about the flux maximum approach. So, we have the original data, we take the blocks, we first come up with. The blocks we first come out with a big enough block and take the blocks maximum within each block, and then we fit a generalized value distribution. The other approach, which use the data a little bit more efficiently, is to set a high threshold and then use those threshold incedent and then fit the so-called general aspirator distribution. Okay. All right, so today's material will be slightly more complicated than that. More complicated than that because imagine the univary, you can think about your sort of focus on single variable at a single location. So therefore, you have a single time series to work with. Sometimes you might want to work with multiple variable. It could be different variable. For example, you want to look at maybe the win and something else like precipitation. Or maybe you want to work with the same variable, but add the multiple locations. Same variable, but at a multiple location, all right. Um, so this is an example. This is an example I totally just grabbed from R because I can easily grab the data. So, this is the third and wave somewhere in the UK. I don't exactly sure where this location is, but it must be. Along the coast, so we have the wave and we have surge. So, in this case, we are actually dealing with two variables, and you might wonder in what situation you want to deal with the trend for multiple variables. So, at least two. Uh, so at least two things we uh let uh let want us to do this. So, one is there might be some scientific motivation because many extremal problems are intrinsically multivariate. And especially nowadays, the so-called compound or concurrent extreme, which means multiple multivariable may have extreme either simultaneously or sequentially become a very hot topic. And as you can imagine, if you have And as you can imagine, if you have several iterations happen simultaneously, they might lead to a pretty big damage. Okay, so that's some scientific motivation, but that's another sort of statistical motivation. So the idea is if you have two variables and they are correlated in some way in terms of X-ray, and if you can properly model that, it is possible that you can get more precise. That you can get more precise inference for each individual variable. But that's a big condition because that is assumed that you can model the dependence, especially the dependence of the extreme well, so that you can do that. That's a very big assumption. And here we're going to explore some commonly used approach to model the so-called dependence for the multivariate stream. But before I do that, I also But before I do that, I also want to bring out another sort of concession challenge. That is, when you talk about several variables, what you mean by extreme is actually not uniquely defined. So I'm going to show you a couple of examples. Okay, so imagine, so this is a simulated wave. So, and based on the shape, some of you might guess what I simulated from. It looks pretty much like a bivariate normal. By very normal, and then so you have this elliptical shape. All right, so the first classical framework in which I'm going to discuss, although I don't like that framework that much, is so-called the component-wise maxima framework. So if you look at two variables, so one variable in the x-axis, we call it x1, the other one is x2, label in y-axis. All right. So if you just have a single variable, I think x2. Single variable, I think extreme or like the large value is sort of well defined. So, for example, in this variable, you just find out the maximum value, which is right here. Okay. Similarly, if you look at the other variable x2, so you know what is the maximum value. Okay, but what is the maximum value for when you consider both x1 and x2? So, one way is that you take the maximum value for this one and maximum value for this one. For this one and maximum value for this one, and you put them together to create a vector, which is right here. Okay. All right. So later you will see this kind of framework is sort of motivated by the sort of a mathematical convenience rather than describing the true process. Because if you look at this one, this is not a real biblical factor in this space. It's simply because we It's simply because we do the so-called component-wise maxima, and then we got this point. So, this point will only occur if you have the maxima for the x1 and x2 happen simultaneously. You have that one. Otherwise, you won't. So, this is the one of the definition. It sort of aligns with the block maximum idea. And this one is sort of the threshold incident analog. So, you can say I consider it. can say I consider extreme as for this variable exceeds this threshold and this variable exceeds this threshold and then you can say this is extreme. Okay and another way is that maybe you want to condition on one variable being extreme and you want to look at all the point because for example this might be some kind of primary variable you care about. So when you have the extreme and if you So when you have the extreme and if you also have the extreme or maybe not that extreme for the other variable, you want to consider that because it is the combined effect will create some compound extreme right here. So those are two examples. One is if you condition on this variable, x1 being extreme, you would look at those data points. If you want to condition on x2 being extreme, you will consider those. And of course, there are some other ways to define. And of course, there are some other way to define. So for example, you can come up with some kind of norm. For example, this is the L1 norm. So you say when the x1 plus x2 greater than certain number, you consider that to be extreme. And of course, you can come up with other. Okay. The other thing is, again, so for the multivariate, I will only use bivariate case to illustrate that because that's easy to visualize. Easy to visualize, and another thing is once you have a higher and higher dimension, things become pretty, very complicated. Um, all right, so here I show you two pictures, and again, so one of the central end for the multivariate string is actually to describe the cal dependence. Uh, yeah, question. All your examples, we have a linear relationship between x and y. That's correct. Um, say you don't, and it's some sort of odd shape, right? Sort of on shape, it is that your technique to more or less transform into a linear shape, or do you have uh no? So, I think those definitions do not necessarily have to tie with linear shape. Okay, so is it actually I think the point for this slide is to demonstrate that the concession challenge that there's no unique definition of multivariate trade. Although, I think your question is interesting in a sense that recently in the Recently, in the statistical extreme value community, they also start to look at sort of the geometry of the shape and try to come up with extreme. But you want to keep in mind that in some application, you have a certain region, you consider that to be like a risk region. Because if you look at a geometric shape, you can also say maybe this part is also extreme. But maybe in terms of impact, it's not that important. High is not that important. Yeah, but I mean, that's something to consider the geometry of those things. All right. So again, here I'm going to use those two simulator data as working sample. Again, you probably can guess this one is actually coming from the bivary normal distribution. This is coming from something else. And again, And again, as I mentioned, that we want to characterize the tail dependence. That is important because if you have a very strong tail dependence, that means you might have like two variables have a string event happen simultaneously. So they'll be very different than if those two variables, they are independent to each other. So we'll come back to that. But if you look at those two pictures, can you tell me which one you think have a stronger? Which one you think have a stronger tail dependence? Although I did not mention what it meant by tail dependence. Which one? White one? This one. Yeah. Why is that? Because it's right tail, the y1, y2 being larger has a stronger and more linear dependence, it seems. Yeah, that's right. So once you're sorry, I think you're just described is pretty good. One way to see that is if you look at See that is if you look at. So, by the way, so another thing I want to mention is that, so this toy example: x1, x2, y1, and y2, they all have exactly marginal distribution. And in fact, they are all normal distribution with mean zero and variance one, but they have slightly different dependence structure. So, one way to sort of characterize the tile dependence is you can look at this upper right corner. So, that upper dependency corresponding Corner, so that upper right corner right here, you probably cannot see right. So, this upper right corner means when both x1 and x2 be enlarged, and by the way, they have the same number of data points. And you can see that way more data points fall into this corner than this one. Okay. Okay. But let me, yeah. So, you mentioned tail dependence, but what I wonder is, so an extreme event, we here assume that it appears on a Here, assume that it appears on a tail, right? Yeah. So, is it because it's like a temperature wind, so that when it gets like high, an extreme event occurs? So, but what I'm wondering is, can an extreme event occur at a middle, like from on the range of the variable at a middle point? Yeah, it could. So, it depends on which aspect you are talking about in terms of extreme. Yeah, so but here we let's make things simple. So, when we Let's make things simple. So, when we talk about extreme, that means something unusual. So, either x1 being either large or small, and x2 being large or small. Oh, by the way, another thing I forgot to mention is the method I described yesterday is pretty much focused on the maxima, but you can basically apply it to the minima as well. So basically, you just negate a value and then do the same analysis and negate the question. There's also a rare event, right? So there's some publication. So there's some probably yeah so one is yeah so rare event could also maybe this is also could be a rare event if this is online data so rare event can happen here. But let me ask you guys another question. What kind of summary have you learned to describe the dependence? Like covariance correlation, right? Okay, so let me ask you another question. This picture and this picture, which one you think has higher correlation? Yeah, you can tell my logic. Yeah, the left one actually has a higher correlation, and we will see why. So, again. So again, so the correlation for this one is around 0.75, and this one, the correlation is 0.7. But when you look at the plot closely, you will say this one probably have a stronger tail dependence. So basically, the main point and the main take point of this slide is that when we want to focus on the tail dependence, we need to use our summary statistic. Our summary statistic, which is a bit more appropriate to describe tile dependence. So, this is the correlation coefficient, but essentially, so you have sort of a sort of center as the reference point. And you want to say when one variable move off, what is the likelihood that the other one will move off? But it's pretty much respect to the center. But here, you care about more about this part. All right. So, how to do that actually. So, how to do that? Actually, the idea is actually similar to this box I just mentioned. So, basically, you want to start with some kind of box right here, and you keep pushing to here, and you want to basically see how likely those planes will land into there. And to do so, there's the so-called upper tail dependence parameter. So, the formula might look a little bit complicated, but idea is actually rather simple. Rather simple. So, this one, so this is a conditional probability. This condition says that x2, so when we consider one of the variable being extreme with respect to itself, so this is the notation for the cumulative distribution function for the x-cube. And so, therefore, the cumulative distribution is between 0 and 1. And when u is very close to 1, that means we are going to look at the after tail. means we are going to look at the aftertale. So when x2 being extreme after we consider its marginal distribution, what is the conditional probability that x1 is also being extreme? Okay. Right. The one complication is that when we let and use those to infinity, we pretty much run out of the data. So typically what people would do is they actually look at a sequence of A sequence of different views, and then to see what this behaves toward to the upper end. So, again, this is the chi plus. Again, what I did here is that I take this data set and I just compute this basically this empirical conditional probability as a function of u, and we do a sequence of u. So this should be certain how random at quanta. Be certain how red and quantum. So you can see this kind of decay behavior in this case. And in this case, it seems to be stabilized. Okay, so what I tell you is that if you are going to use this as a way to measure the tail dependence, and then you will say this will have a stronger tale dependence than this one. Okay. Any question? Yes. Sorry. I'm sorry. So what are the different lines on those two colours? Oh, yeah. Yeah, I forgot to mention that. So this, the red light is sort of an amproesy of this guy right here. And those two black light, I do not exactly recall how they sort of account for the uncertainty, but you can imagine that is the uncertain band for the empirical probability, which is between zero to one. And as you can see, the To one. And as you can see, the uncertainty band becomes larger and larger because when you get close to one, you basically don't have a lot of data to compute this empirical probability. Yeah. If we change the variables, we do not expect the symmetric. This would be symmetric because you do this normalization. So each of. Okay, so that's a good question. Yeah, so Let me see. Yeah, that's in this case, it's symmetric. Yeah, but in other cases, it might be asymmetric. Yeah, that's a good point. What's the joint distribution of Y1 and Y2? This one or this one? The second one. Yeah, so what I did is, so this one, you probably, so this one is pretty standard by very standard. standard by very uh standard by very normal distribution this one again i play is something called copula so each of them i have a marginal uniform uh sorry marginal standard normal distribution but i use a so-called extreme value copula in which they have a stronger tail dependence um and that's what happened here so i think this one i use probably the uh the so-called the gumbo copula since that's what it is I think that's what it is. So the quick question. So, have you guys heard about Gaussian corpora? Any of you? Yeah. Have you heard about a famous story about the Gaussian corpora? So those people say the Gaussian corpora is the formula that killed Wall Street in the 2008 financial crisis because they say because this one failed to categorize the tell the pen. To characterize the tail dependence, whether this one can accommodate tail dependence. All right. So the very first one, the so-called component-wise maxima. Okay, so actually this one, this is actually one of the paper I work with, Adam Monaghan and Francis Reyer. So here we look at that Canadian regional climate model simulation. Here I look at the Model simulation. Here, I look at the precipitation and the wind speed. So, here, let's consider those together. So, under the blocks maxima framework, you will basically extract a block maxima for each variable separately. So, for example, for the preset, I need to make sure I know the color. Yeah, so preset, I think, is blue. Okay, so for example, for the precipitation, this is the annual maximum. Precipitation: this is the annual maximum for the first year, the second year, the third year, the fourth year. Okay, so you get that annual maximum. So, that's exactly the same as you would do for the univariate. For the wind speed, you will take the annual maximum for the first year, which is the green one, the second year right here, the third year right here, and fourth year right here. Okay, so again, I want to again emphasize that in the end, so if you do the end so if you do the component wise maximum approach what would be the vector for the first year so you actually take this one and this one combine them together as a vector for the second year they happens to be on the same day so this is actually the real vector the third year you will have this one and this one and fourth year you have this one and this one and then you put them together and try to model the dependency so this is one Dependency. So, this is one copy I think made me don't like this framework that much. But why people start with this, the reason is as follows. So first of all, they know how to handle each marginal distribution. And by doing so, they also can come up with a way to describe the health dependence. But I want to emphasize that those component-wise maxima, especially for most of the environmental process. Most of the environmental processes I look at, this case happened not very open. Okay. Any question? Okay. So in the following, so basically, I assume you still remember what I talk about, the blocks maxima approach. So basically, you know how to do the extreme value analysis for this variable and this variable. But next, we need to tell you how to model. Tell you how to model the dependence. What is the dependence between the blocks maximum for this variable and the blocks maximum for this variable? And again, they may not happen simultaneously. All right, so as you can imagine, the formula looks a little bit more complicated, but the idea is pretty much what I just described. For the block maximum, for each variable alone, we can still use the Variable alone, we can still use the generalized eternal value distribution to approximate that. So, that's something we learned yesterday. The bad news is, unlike the univariate case, we have an asymptotic multi-distribution to model the univariate distribution. We don't have such a thing for the dependent structure. But, what people typically would do is they will come up with a parametric family, try to describe dependence. And again, Dependence. And again, that's pretty much like a coglaw-like approach that you model the marginal distribution first and standardize that in a way, and then to take care of the dependence. And this slide is a little bit complicated, but the main idea is if for each variable, you say you can approximate that by generalizing each and value distribution, and you have a certain location scale. And you have a certain location, scale, and shape parameter. The question is: how to do the standardization? It's just like if you have a normal distribution, you know how to standardize into the standard normal distribution. You subtract off the mean and divide by standard debate mean. Here it's similar, but a bit more complicated. So you have the original like a plus maximum for J's variable. You search off of the location parameter divided by the scale parameter. Divide by the scale parameter and multiply by this and plus one and do this whole thing. So after you do this whole thing, both variables will have the same marginal distribution, which is a so-called unit for shade distribution, or it is the case where you have a location scale and shade parameter O equal to one. Okay. So you do this typically, you do this first, and now you look This first, and now you look at the joint distribution under this standardization. And you basically say this join CDF can be written in this way with this function sort of encoded dependent structure among those block maximizes. I guess it's a little bit complicated, but I guess you just get a general feeling. Just get a general feeling. So, basically, we know how to do the marginal, we do the standardization, and we need to figure out how to model this V function, which encodes the pelletpendence. Is there any condition on the V function? I'm sorry? Is there any condition on the V function? That lot, but I won't include here to bother you yet. For that to be a CD app, so they will have that, but they will also have something else. But they will also have something else, so there will be quite a few. So, if you want, so I can in the end, I will point you to those references. But I will also warn you that typically for the univary extreme, a lot of things still quite accessible for people outside of statistics. But when you move into either even like the bivariate case, it's not that easy to follow. But that's quite a bit of But that's quite a bit of condition here, other than it's a valid CDF. So one models, again, we look at the bivaricates. Again, think about we look at the two variables and we look at their blocks maxima together. And here we already do the standardization. So each marginal distribution will have the same H and valid distribution. And then And then this guy right here encodes the dependent structure where the strength of the tau dependent is described by this one parameter alpha. So if alpha is equal to one, that means those two blocks maximum are independent. And you can sort of derive that from here because this is the marginal distribution of HOPAN. So if those two things are independent, you just Those two things are independent, you just multiply those two together, so that should correspond to the case when alpha equals to one. Okay, and so when alpha equals to zero, that means they are sort of perfectly dependent, at least in terms of tear. Okay. Yeah. Because one way to define a joint distribution on the extremes would be to specify the conditional distribution of one extreme given the One extreme, given the other extreme, and that's kind of more straightforward because it's going to be it. Yes, I definitely agree. So, that's one approach to sort of take that direction. So, basically, and I also like that because usually the higher dimensional object is much more difficult to deal with than lower dimension. So, you want to use this like a conditional decomposition. Yeah, so that's in the end of the second part, I will talk about this. Yeah. Talk about this, yeah. Um, well, what's one challenge about though is that it breaks any sort of symmetry between the variables because you're conditioning one on the other, depending on what after your application. But this is exactly the point you raised. So this model is probably the simplest model to describe the tau dependence, uh, but this model implied a symmetry. So that means the x1 or m1. X1 or N1 conditional N2 and N2 conditional N1, they are the same. Okay, um, the other one, so this one is look a bit more complicated, but the big take-home message is that this one has two parameters, so it has to be more flexible. And the reason they introduced another parameter is they try to accommodate the n symmetry for the tail dependence. Okay, um, so in the case that if In the case that if those two are the same question? Okay, so if those two parameters are the same, so this will reduce back to the logistics model. And the difference between those two parameters tell you the sort of the level of the n symmetry in terms of the tail dependence. So up to this point, I think I don't know whether your brain is still functioning, but I want to show you some exercise at least. Some exercise at least. I think a lot of time it's much easier for you to work on something. That's why we have our exercise session, but it's usually much cleaner once you actually deal with the data. So let me show you what the second file of the exercise looks like. Alright, so in the beginning I just playlist those. So, in the beginning, I just played with those, but you can actually change that to play with other settings you want. And here is actually how I create this type block. So, I did not include those in my slide. But my point is: we are going to, the approach is similar to yesterday. So, we are first going to talk about the model, and then we are going to About the model, and then we're going to talk about how to actually fit the data to the model. But you also want to try those methods to some data set in which I also have some simple data set you can try on. But you can also use, for example, the Aaron data set I give to you and many other you can also try. You can even try the cabal wind tower. Maybe you want to look at what is the dependence at different heights. At different heights, there will be another exercise to do. Um, yeah, my point is that uh, this there's a few exercises with the code, so you can start with that and you can try to modify those. Yeah, so if you were to scale the data, I mean, it may be a linear transformation or just a Transformation or just the multiplication by a parameter, etc., by a unit, let's say four. Would the model present us with the same parameters, same alpha and beta values? Because of the location scale and the transformation. But I think there is something. I think that there's something should be very easy for you. Can actually check on that. Okay, yeah. It's a good exercise. Yeah, it's a good exercise. Thank you. All right. So in terms of parameter estimation, so once you get through this, so this whole thing right here will tell you the additional computation, which should describe the Describe the cal dependence. But after that, in terms of parameter estimation, again, a lot of times statisticians would like to stick with the maximum likelihood estimation for several reasons. So usually the maximum likelihood approach, if the underlying probability model is reasonable, is more efficient. And it's also flexible to extend to more complicated situations like a regression setting. Situation like a regression setting. Okay, but again, the bottom line is that once you have a probability model, and then with the data, so you can write down the so-called log likelihood. And for this guy, the log likelihood, so this one, I'm sorry, so this one is just probability density, but you would need that to form the log likelihood. And again, this one is just the original CDF. And you have those terms, so basically, you're You have those terms. So basically, you are going to work with that v function that encodes the tau dependence. And what it means here is that you are going to take the partial derivative with respect to the first one argument, the partial derivative for the second argument. And this is the V D N1 and D V D N two partial derivative. Yeah. All right. And then so. And then, so once you have this one, you can write down the log likelihood function, and then you are going to do the maximization to get a parameter estimate. And in this case, for the logic model, if you put everything together, there are several parameters you need to estimate. For the first variable, you need to estimate what is the location, scale, shape, parameter. Similarly, for the second one, you also do that. One, you also do that. In addition to that, you will estimate this alpha parameter to tell you what is the level of the tail dependence. Okay. So this is just a very quick example. So here we have annual maximum wind speed at Hardpoint and annual maximum wind speed at Elderly. Those two places seem to be pretty file. Place seems to be pre-file, so I don't know why they look at those. Okay, uh, and those are annual maxima. Um, so here, uh, this is the, okay, one thing I forgot to mention here. So when we do the parameter estimation, actually, there are two ways you can do it. So, one way is that you can do the univary estimation first and then use the estimated parameter to do the transformation and then to estimate. And then to estimate the dependent structure. So, this is so-called a two-step estimation approach. You could also do one step. So, basically, you put everything together, but then this guy will look a little bit complicated. You'll have a sort of a Jacobian turn to accommodate the marginal transformation and all that. Hey, okay. So, in this case, again, we have those like two variables, so therefore, we have. two variables so therefore we have seven uh wait six no uh so we have six marginal distribution a marginal parameter to be estimated and either one or two dependence parameter to be estimated depending on which model you choose okay uh so those are the estimate value and based on this you can tell here we have to do the one step uh estimation approach because otherwise the marginal parameter will be the same The marginal parameter will be the same. So, here we basically do the marginal parameter and dependence parameter together. We got a parameter estimate right here, and also in the parenthesis, there's an associated standard error. And also, we have the dependence parameter. So right here, so we have 0.71. So, they tell us that's some level of the health dependence, although it's not that strong because if this is equal to one damning independent. To one damnings independent if this approach to zero damnings perfectly, cow dependence. Okay. And here, this is the alpha and beta for the biologistic model. And again, this model is slightly more flexible because it can accommodate the n symmetric. And you can see those two parameters are very different. So that means the dependent structure might be quite an. should might be quite asymmetric and so I provide a lot like you but I should do okay so one question here so here we fit the two different dependent structure so do you know any strategy to choose if you have to choose which one you want to choose what kind of strategy you can use What kind of strategy you can use? Well, these two. I mean, in general, so if you have like if you try to fit several different models to a data set, and how would you choose which model is better? So, for example, you do the linear regression, how do you decide I want to include those predators and so on? Yeah, something alone a lot of the lock likelihood, and also maybe panel. Likelihood and also maybe panelize the model complexity. Yeah, I'm sorry, okay, very good, very good. Yeah, so that's one way you can do to decide whether you want to go with this simpler model or maybe slightly complicated model. Or maybe the other question to ask, is that worthwhile to is that sufficient evidence to support this more complicated model? All right. You can do similar thing for the Can do similar thing for the threshold incedent approach. So basically, again, here the additional complication is as following. So again, due to threshold incedant, you will have the marginal parameter, you have the dependence parameter. And here the model would be the same. So you will still either use the logistic model or biologistic model. But then from this data set, actually, the likelihood would be a little bit complicated because it would. Would be a little bit complicated because it would depend on you are going to sort of partition your data into four regions. This region is where both variables exceed their threshold. And this is when one of the variables exceeds the threshold, but not the other one. And this is the case, which is where the majority of data sitting. None of the variable exceed the threshold. And this is another way around. So depending on which situations, in this case, you will just do the usual thing. Usual thing, but for any of those cases, you will do so-called the sensor likelihood. So, basically, the likelihood contribution will depend on whether the data points here, here, here, or here. All right. And again, in the end, you got those parameter estimation. I'm sorry, I should say, by the way, those are two different data sets, so don't get confused about the numbers a little bit. Confused about the numbers a little bit. So, those are two different data sets. Okay, so you can get those parameter estimation, and then you can get the dependence parameter. Again, in this case, which is quite common that the tau dependence is usually not as strong, so which is right here, 0.676. And in this case, you can see those parameters actually are reasonably close. And so, you may wonder maybe this model is already sufficient. And again, you can, for example. Sufficient. And again, you can, for example, use the AIC criteria to make a job. Okay. One thing I forgot to mention is, do you know what those contours mean? So those mean the bibliography density. So basically, those are the fitted vibrate density curve based on the GED margin and the logistic-dependent structure. And for this one, you can see One, you can see this one is also a little bit asymmetric. That's because they're slightly different in terms of the marginal distribution. But the depender is symmetric, but this one is asymmetric. Question? Can you tell me how do you interpret rotating density plus the contours? Is it the same where you have close contours together? It's high density? Or how do you so? I would say in general, I think density is a little bit higher. Uh, I think density is a little bit harder to interpret. So, and so something like uh yeah, like a probability isolide will be easier to interpret. Yes, but this one is basically that the ideal is still the same as the density, but now we have 2D density. But if you want to look at something like the decedent probability, I think the like a probability isolate will be a little bit easier to interpret. So, that's 0.001, for example. 0.001, for example, is that the value of the transit. Yeah, that's not probably. So that's a little hard to interpret it. That's right. Yeah. And a similar way, so right here, so what you can do is again, you will have this like a So I'm not quite whether this is a galaxy count. Yeah, so be careful with this. Yeah. So the estimations on these parameters aren't done stepwise, right? First, we estimate the sigma. Oh, no, it's actually one step. So let me. So this may got some confused. Because if you do two-step approach, and then you will have the same. and then you will have the same marginal distribution the same marginal parameter regardless of which dependent structure you choose but if you do that together uh the marginal this uh parameter estimate will be different depending on which dependent structure you choose so if you go back to look at here um this one and this one they are close but they are different so that means we must estimate both them We must estimate both the marginal parameter here along with the dependence parameter. So, therefore, they are different. So, all of those numbers I report here, they do the one-step estimation. Is that clear? What's the number between the black and here? That is the standard error. Yeah, so the standard error based on the likelihood principle. So, basically, I did. So, basically, the idea you can think about is that they give you some idea about the precision into the parameter estimation. So, in this case, you can see this one, this one, so they give you some confidence that with the estimate is reasonably well. But typically, when you look at a parameter like a shape parameter, you have this number and you have a relatively large standard error. So, they tell you that this parameter is actually much, much. Printer, it's actually much, much harder to estimate. And even from this, it's very hard for you to tell whether it should be a slightly happy tail, or maybe it's an exponential tell, or even be a bond detail case. All right. Question? All right, so this is another somewhat a little bit like a confusing concept, I would say. In concept, I would say. So, in the extreme world, there's also two different kinds of dependence. They typically call it like asymptotic dependence or asymptotic independent, in which I don't like this term, because really what they're trying to tell is what is the tau dependent. So whether they are tau-dependent or tau-independent. And because they, again, they look at this matrix high, which we just mentioned before. And what this is, is basically we want to look at Is basically we want to look at this conditional probability condition when one variable will be very, very extreme. Okay, so asymptotic dependence, which means in limit, this one won't go to zero. If you have two things independent, this one will go to zero. But it's another independent, which means this chi-measure eventually will go to zero. So that makes a distinction. So that makes a distinction between asymptotic dependent and asymptotic independent. And if you remember the toy example I showed in the very beginning, the bivariate normal is actually 4 into this case. And the other one, the bivariate normal, the with gumblue corpola would be 4 into this case. I'm a bit confused by that statement. Multivary extreme value models interpret. So, so what I mean, so is it okay? So, this is a bit confusing because, in the sense that the extreme value model I mentioned is like logistic and biologistic, there will be two examples. And so, when they have, so when you have a You have okay. So, I think the better way is that those models are designed for this case. So, they're designed for handling asymptotic dependencies. That also means if your data is actually coming from the if you have a Gaussian coplaz structure, which means it is asymptotic independent, the implication is those. The implication is those two models I introduced is not appropriate. So they basically tell us that the multivariate extreme value model, I just introduced those two examples. And among many other, they do have a limitation in that they can only accommodate this one. Yes. All right. But on the other hand, I will mention that sometimes when people, especially in the People, especially in the statistician, sometimes they will get trapped too much by those asymptotic arguments. Because even you look at the Gaussian Koch law, even that is tau-independent. If you have a very strong correlation coefficient, you will still have a very strong tau dependence on the observable level. So again, so this motivates people to build models, try to accommodate. Try to accommodate this case where it is asymptotic independent, but still have a very strong sub-asymptotic level dependence. And one of the model is actually very similar to what Denise just described. Again, we have two variables, but here we want to look at when one variable being extreme, what will be the conditional distribution of the other one. Okay, so this class of models. Okay, so this class of model is by Hafner and Tai in 2004. And again, so you have this sort of like a two step. So one step is try to model the marginal distribution for each of them and do the transformation and then try to this model this conditional distribution. And they model the conditional distribution right here in a way that's similar to the scaling that we have seen for the Scaling that we have seen for the central limit theory and the print value theory. But the scaling function would look slightly different. So basically, they suggest and what they come out with this is they look at a wide range of different kind of called block dependent structure. And they say under this marginalization and under this parameterization, they can accommodate a wide range of dependent structure. Okay, again, yeah. Okay, again, G only depends on Z and not on you. That's right. Yeah. So, you know, it's in the A. Okay, so it's A and B. That's right. Yeah, but this one is probably very hot to digest. So that's a cartoon version I steal from the author. So the idea is as the following. So again, The following. So, again, we want to let one of the variables being extreme and want to look at what is the conditional distribution where the other variable will evolve into this direction. So, from this cartoon, you will have this blue curve. So, blue curve is some distribution, but there's no specific distribution under this brand board. And typically, this one I'm going to ask me sort of non-parametrically, but how the conditional distribution. But how the conditional distribution will change depend on this one will be described by this sort of location change and the scaling. So this alpha parameter here will tell you whether this conditional distribution will go up or go down. So if you have alpha greater than zero, which is this case, you can see this, the sort of the center of this condition distribution will move up. Condition distribution will move up. And if you have a beta, so beta parameter tells you sort of the scaling. So if delta is greater than zero, so that means this conditional distribution will be more spread out if you move to this direction. In this case, you can see the conditional distribution sort of shrink. So that means that is the case when the beta is less than zero. So you have the conditional distribution become much, much more concentrated. Become much, much more concentrated. Question? Yeah, so one thing I like this a little bit better than the component-wise maxima approach is when you do this, you can actually look at the original variable rather than the component-wise maxima. So you can use this, try to ask the question. So when one of variables The question: so, when one of variables, so let's say the precipitation variable being extreme, what would be the conditional distribution of wind? Because you might hear about the comeback effect, like if you ran happily and with very strong wind, it might destroy the building. So, basically, and then they will respect original better rather than somewhat artificial component-wise X. Oh, so here X tilde and Y tilde. So here, x tilde and y tilde actually, they're just the original variables, not the extremes, right? Yes. Yes. Yeah. After marginal transformation. So there are several steps. They're original. I was thinking about that because when you model the joint distribution of your streams, it's kind of like you reduce modeling. You forget about the data, your X and Y and its relationship. And I was wondering if there was a way. Wondering if there was a way you specify the joint distribution of the extremes and then embed it in this larger model for the whole XY in a way that it's consistent with that. So what do you mean by larger model? I mean like a joint distribution for your X and Y. Oh, I see, I see. So basically you model the whole thing. Yeah. I see. Yeah. Yeah, that is something I think that's still an interesting research. That's still an interesting research direction in a sense that you want to have a ideally, if you can model everything, that would be great. But typically, that is case that when you try to fit the model for all the data points, typically the behavior for the bulk will dominate. So, a lot of time you will have a not so precise estimate for the tail. Impose that. Yeah. Yeah, impose that. Yeah, impose that in some way. Yeah, so I think there's some like recent research active going on. So, in the univariate case, okay, so sorry for me to dig a little bit. So, the univariate case, you have a threshold ECN approach, but the threshold Eastern approach to choosing the threshold is not that easy and the result can be sensitive. So, there's some effort to try to model the full distribution, but try to impart. Distribution, but try to impose the generalized Pareto distribution in this linear core. And the question is how to do that in the bivary case. And for the bivary case, the complication is sort of related to this point I made quite a few slides ago. So it's less clear which like tail joint structure you want to shrink towards that. Shrink or to that, right? So, this is sort of a complication. All right, so here, just a very simple analysis from actually from that paper. So, they look at the air pollution data. So, we look at the ozone and NO2. And here they look at when the one variable being That when the one variable being a train, what is the conditional distribution for the other one? And the way they present that, so you have to see that several different curves. So those are the conditional quantile. So once you can estimate the conditional distribution, and then you can register and through the conditional quantile. So I believe they have like conditional 0.9, 0.7, 0.5, 0.3, and 0.1. Okay. Okay. All right. Those are the parameter estimates, and those are sort of the diagnostic plot in which you want to look at this. So the way to look at those plots is somewhat similar to look at the regression procedure plot. So you want to have like a flat curve and sort of evenly spread in here. So here you want this to be close to one and this. One, and this, I don't think this has to be equal to you, but you want to have to see this pattern to at least give you some evidence that the model is doing the reasonable job. All right, so I will stop here for 30 seconds for your brain to take a break. But here is the here's the remark. 10 minutes. 10 minutes. Why does the conditional extreme value model always end up with an asymptotic independence? It's not always, but basically asymptotic dependence will be the limiting case. So that will be the case when both alpha and beta are equal to one. So that's just a single point. I guess it would be a good exercise to show why that's the case. It could be, yeah. All right, so for the spatial, I will keep this thing very general. And in a sense, that I'm only going to cover one aspect of spatial extreme. So, but before that, I want to make a distinction. So, how many of you have not heard? How many of you have not heard about this phrase? Have not heard? Have not. You haven't? Oh, really? You? Okay, good. So he says the climate is what you expect and the weather is what you get. I've only been here for a couple of days, so I don't know quite sure about the climate. So here it might be hot, but I'm not sure whether it's that hot. So in San Sen, you can think about the climate is some kind of The climate is some kind of probability distribution, and where there is a draw from that. So, for example, for today, if you look at the today's temperature here at Kelowna, this year, last year, if you look at like a long period of time, of course, assuming the temperature did not change much, which is probably not the case. So for that, you can give a sense about the probability distribution that despite the likely temperature for this place. Temperature for this place for today. But today, unfortunately, it's pretty hot. So you can think about that as weather because we somehow draw a sample from that distribution and that sample is happening land on the upper right hand. So the idea, so in terms of extreme perspective, you can think about the climate effect in terms of spatial extremities. You want to know what is the know what is the marginal distribution. For example, what is the, if you look at the annual maxima, so what is the annual maxima distribution change in the spatial domain? But the weather effect will be a little bit more complicated. So maybe the question you want to know is when, I don't know, I cannot think of a nearby city. So somewhere near Columbia. So somewhere near Kelowna, if that's very hot, so what is the likelihood that Kelowna is also very hot? It might be high because the temperature is a bit large scale system. But if you look at a precipitation or some other variable, there might be different story. Okay, so the spatial span I'm going to address is essentially how the H and value distribution will varies in space. Again, this is another cartoon picture. This is another cartoon picture. So, think about this as a simplified spatial domain, just a line. So, let's say Vancouver is here, and Kelowna is here, and Calgary is here, for example. And for each location, you have a climatology, or you have a marginal distribution. But think about this is the marginal distribution. Okay, so think about this is the marginal distribution, and then we typically care about this part, the upper tail. Part the upper tail, so and the question is: um, can we model the spatial variation for that? Yes, question. No, no problem. Each of X is a point in a year in space. So each location is point in space. And here I sort of integrate the time. So those will be think about those will be the temperature value during a certain period of time. I see. So basically, we want to have. So basically, we want to characterize how this distribution varies in space. But in terms of weather problem, you can think about you're going to look at this slides and each curve represents one day. So for example, this is the two days. Okay, four. I don't know. I can, I don't know how can I make it as a win speed. Okay, so this is the win speed at this location, this location, this location, but this on the same day. Okay. Understanding. Okay, so you can see those two problems are slightly different because we're looking at how this distribution varies in space. And this is what we try to describe when one place is very hot, what is likelihood that the other place is very hot. Again, I would say the sort of state of our model try to address this one have a very, very serious issue because they call it a max stable process model. And the construction is very similar. And the construction is very similar to the multivariate ring you have seen. So basically, they are not going to look at this one. They are going to look at so-called component-wise maxima, which will be this one. All right. I have five minutes or less than that. Yeah. All right. I'm gonna yeah, all right, so um the model uh very quickly I'm going to introduce very quickly it's just the following. So actually that's a simple way you can do and it's not that difficult. So one way to do that is you can fit the, for example, you can choose your favorite method, either the blocks maxima approach or threshold incident approach. You can fit that for each location separately and then you Separately, and then you can visualize that. That's good. But what happened if you have the situation like this? So you probably recognize that we don't have a weather station everywhere. So for example, what if you want to know what is the atrium value distribution here? If you do that station by station approach, you won't be able to say what is the atrium value distribution there unless you do some modeling about the spatial. About the spatial variation. And here, the spatial variation I'm talking about is the spatial variation for those underlying parameters that govern the eternal dollar distribution. So that is a very basic idea. So it boils down to that, can we actually use the data together and to learn about the spatial variation for the location parameter, scale parameter, and shape parameter? Okay. And the standard way. Okay, and the standard way to do that is so-called the Bayesian hierarchical approach. Some of you may have seen that last year, although I would say implementation might be a little bit more complicated. I do have a code, but also come up with those prior distribution, which is a little bit mysterious to me how they came up with that. So the idea is this again. So we have the hierarchy, so we have a data level. So we have a beta level, we have later process, and because it's the Bayesian framework, you also need to put a prior. And for this complicated problem, you have a lot of things you need to decide what the prior should be used. In the latent process, typically when people try to model the flexible spatial surface, a lot of Thai people would just use the Gaussian process. So Denise talked about that last year. So I believe you guys still can see the slide. Can see the slide. The idea is that we treat each parameter, the location, and typically we will do the log transformation with the scale parameter, and we model them as each individual as a Gaussian process. And the Gaussian process is specified by the main function and the covariance function. So let me just end with this example. Again, this is the rainfall example. So we have those locations, but imagine. Location, but imagine we want to create a so-called return level map for this region. So, in order to create a map, now we also need to take care of those sort of missing values, those places that we don't have a weather spatial. So, that's why we need to build a spatial model. This is a simple one. Basically, build a deterministic linear function in terms of location scale parameters. So, you can see this surface look pretty. Look pretty rigid. And this is the output from the Bayesian hierarchical model in terms of 20 years return level. So you do see some structure here. And the idea is that I try to use the Gaussian process as a way to come out with more flexible spatial surface than this one. All right. Okay. Okay, so I guess I will probably start here because I think I run out of time, but you can see those take-home messages. Yeah. Yeah, question. You go back to the slide that we should both map. The map? Yeah. How did you come up with the one on top? Was it like the map? Oh, this one. Yeah, so that's something I skipped. Sorry. So I should not skip because I this one. Uh this one works in this way. So this one is much quicker to do, but basically you make uh you use all the data, but you assume all the data, they are independent in space. They have some structure in terms of the location parameter, scale parameter, and shape parameter in space. And the regression function you will use, you are going to use the coordinate, you are going to use the latitude and longitude. Latitude and longitude as the explanatory variable. And so, in this way, so basically, you have basically a linear regression for the location parameter scale parameter. And typically, they won't mess up with this one because this guy is difficult. And so, you can basically do a turn value regression model, but then the price you pay is you cannot accommodate a very flexible spatial circuit. So those were latent. Yeah. And this is the outcome of the Bayesian, the MCMC? So about, I believe what they did, so that sounds a little bit weird. So because under the Brayson framework, you will have a posterior distribution for basically everything. basically everything. And then you can have the posterior for the posterior distribution for the return level for everywhere. And I think here they probably just report pointwise posteriorly. I think that's what happened. What was the latency of the oh, yeah, so that's a good question because. Oh, yeah, so that's a good question because again, what latent variable means is something you cannot see. So there's a location. So those parameters you won't see anyway. But in the sense that this is the model, the model says that the annual maxima at those location condition on those three spatial surface, that is right, location scale shape ranger, they are conditional. They are conditional independent of the geomizection value distribution. So that's the data level. But there's some level that you don't see it. So basically, the process, you will have an online spatial surface, but you don't see it because you only have a finite number of data points. And of course, you also need to estimate that. So that is what I mean by the latent variable. Thank you. Any of you play with NCNC before? Oh, by the way, any of you heard about NCNC? So, what is NCNC? Markov Chair Monte Carlo. Very good. So, basically, if you do Bayesian and if you are not work with the toy model, very likely you need to some way to do the computation. And NCNC is one way to do the computation, it's sort of simulation. The computation is sort of a simulation approach, but it's a little bit more complicated than the planned version of Monte Carlo because you construct the Markov chain, and in the end, the stationary distribution will well approximate the posterior distribution. Another question? So you know, I think we can have the break. I think we can have the break. Thank you very much. This will continue in the exercise session. Yeah, so in the afternoon, I will probably also give you some idea about things you can try. And a lot of them will encourage you to try the simulation because one thing I learned from the very famous statistician. A very famous statistician from Harvard. So he said, when you start to do the statistical research, there are several different components. You can do theory, method, application, or computation. Start with computation because you always can see something and you can check whether your intuition is right or wrong. Yeah. Do you see some things working like a sense? Yeah, that's right, that's right. But another thing I think is very important is it's very important to design a good simulation. So you need to know what you want to do before you actually get into that. Oh, good, yeah. I don't know if you're not for me. I'll go switch. Is there a problem? I'll go switch. Zero problems. Yeah, it looks probably like a bit. Yeah, right today. Okay. Yeah, but thank you. And then I won't be here, but I put Tyler's contact information on. Tyler's the Information on it, Tyler's sitting with us yesterday. I wouldn't be able to see any of the cannons and I just used the historical. I will not be here to see. Exactly. Realization of the sample go through it and add up everything. What's up, man? I'll also be going towards the morning. I'll see you for the next few weeks. That a little slowly moving away. Not quite. In a sense, that a lot of time, the multivariate extreme is just a bridge to the spatial extreme. So a lot of time they would either like to do univariate stream and directly move to spatial extreme. And multivariate streams sort of the bridge, they can show what the fiber would look like. So if you look at a spatial extreme, still a lot of people do the math data process, which essentially actually essentially and so you're generating unrealized physically unrealizable fields right yeah so that's some yeah so that's some point i want to make on my career proposal yeah yeah yeah because i'm making a point what i agree um i mean you've got good uh good starts on other other other ways of approaching the problem but yeah Other ways of approaching the problem, but yeah, I want to purpose why you use this multivariate experience. Do you want to connect the temporal experience and the spatial experience, right? So multivariate experience, so it could be the case that you just want to study several variable whether they will have like a string simultaneously. So there would be one potential purpose. Would be one potential purpose. The multi-variety trend has also served as a starting point for you to build a more complex model because the spatial train single is a mention of multivariate train where you have essentially filled in many spatial locations in the simulation. So it involves many other many kinds of variables. Variable? Not necessary. So, spatially extreme, the typical case you just focus on a single variable. So, for each different location, you see the same variable at different location plus different variable. So that's why I think there's only many variable. But you could also do even more complicated case that you can deal with the spatial extreme across several different variables. So there's nothing. So if that right to hold on and the associate and CNC estimation, they only address one aspect of space agreement, which is. One aspect of spatial distribution, which is so close to the finite fact, so which means how the extreme value distribution varies from one place to the other. So, for example, let's say we have a location here in Kelowna and we have another station probably 20 kilometers away, but in between we don't have anything. But can we make use of those information about the marginal distribution? For example, here we have GV distribution, here they have GV distribution. Distribution here, we contribute distribution. And we make use of that information to say, in the middle, when we don't have any location, but we can still infer what is the return value distribution so that we can infer the return level and then we can create a return level. I mean, is there intern logic between these two places? So there's some lot because you expect whether the nearby plate. Fan weather, the nearby place, the weather will be similar, so there will be a similar level of H2. For example, the retro high temperature here, let's say 45 degrees Celsius. So the nearby location, the number may not be the same, but may be similar. So there's some kind of spatial smoothness or coherent. So what we actually do is just use the clonas data and to predict the value of the place like 20 kilometers. Place that 20 kilometers far away from the Earth. You can see it that way. Typically, you cannot just do the spatial modeling by using one location. So, let's say you have 100 locations. So, with that 100 location in this region, and by assuming that some kind of spatial smoothness, you can try to infer about the extra value distribution at a location where you don't have observation. But how can we evaluate? How can we automate our predictions? Oh, yeah, that's a very good point. So, one way to do it is you can do something similar to like a cross-validation. So, for example, this is a spatial region, you have a 100 location, use some of the location to give a spatial model, and from that spatial model, you can make a prediction from the other station in which you do have a data, then you did not include it. A data thing, you did not include a model. So, you can one simple way to do it is basically do the spatial modeling, but just use part of station and use the remaining stations of the validation data. But when we gather data that is used to predict, I mean, we need to prove that there is a correlation between the data we use and the data we will predict. We will predict, yeah. So, that's sort of uh hidden in the Gaussian process modeling. So, ideally, the data should inform you about what is the correlation in terms of those locations, scale, and shape. This is the basic assumption of all the data. I would say that in some sense, the basic assumption is basically there's some kind of spatial correlation. Spatial correlation. But to what extent, how strong or what is the characteristic of that spatial correlation is heavily dependent on the latent process, which is Galton process, in which they should learn if the dependence is weak, they should reflect that to say, okay, in the Galton process, the correlation rate may be short range so that beyond certain kilometers, maybe the predict is. Welcome to maybe the product is not that okay. Yeah, yeah, thank you very much. No problem. So, what's your background? My background is optimal control and optimal control, very interesting. I see. Optimization. I see, I see. So, you're from UBC? Yeah. So, one quick question. So, Wayne stay for the whole two weeks? Yeah, yeah. Okay, great. Because I plan to visit. Plan to visit Vancouver after this because I have a relative that I haven't seen for almost 20 years. So I tried to visit Vancouver. So you guys flew in or you guys drive here? No, flew. Actually, we can drive here, but I don't know the rules. Sorry. I must break that. I maybe break the rules. I must relate with.