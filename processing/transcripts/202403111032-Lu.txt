Understanding the class of nonlinear random matrices. I'm going to start by quickly going over some classical random matrix ensembles. We have a data matrix X. We can think about each column of this matrix as a data vector. We have d-dimensional data vectors and total of n of them. If we look at x times x transpose, that is the sample covariance matrix, it is well known that as the dimension As the dimension d, and the total number of these vectors n become large, then the spectral eigenvalue of this sample covariance matrix will converge to well-defined limits. And in this plot, the red curves are the asymptotic limits of the spectrum. And these limit densities are determined by the ratio of n over d and also determined by the spectrum of population covariance of the data vectors. But this is a symmetric. Vectors. But this is a symmetric version. We can also consider a non-symmetric version where we have the product of two independent, for example, random matrices, f and x. Now, in this talk, we're going to consider nonlinear versions of these random matrices. In particular, we're going to consider a setting where we apply a function sigma to these matrices, and sigma is a one-dimensional function, for example, a tension or relou. Or Relu. So sigma is going to be applied to each entry of the matrices. So it's an entry-wise nonlinear transformation of these matrices. Now, these type of matrices appear in several settings. For example, if you consider the kernel method for machine learning, then sigma will be the kernel function. Or you can consider the factor model, where you assume your matrix can be factorized as a product of two lower-rank matrices, but it just happens that your observation. It just happens that your observation model has an additional nonlinearity in it. So, this you can think about as a nonlinear factor model. Or in robust statistics, you want to apply an entry-wise shrinkage operator to entries of the sample cobalt matrix. For me, I first got interested in this type of nonlinear random matrices when I start to study so-called random feature model. I'm going to explain what I mean by random feature model. We're going to use random feature model. Model. We're going to use the runa feature model as a running example in this talk. Now, the main message of this talk is to show you that there is, in many cases, a very simple equivalence principle in high-dimensional cases. Roughly speaking, the principle says that a non-linear model is asymptotically equivalent to a linear model plus noise. More specifically, if we consider this a matrix A, which is applying a nonlinear transformation, for sample covariance matrix, in many settings, in high-dimensional limits, In many settings, in high-dimensional limits, A is equivalent to B, another matrix, which is a linear combination of three matrices. So the first component is an O1 matrix. The second one matrix is X times X transpose. That's the original sample covariance matrix. The last component, Z, is an independent noise matrix. Because it's a symmetric matrix, so Z is also a symmetric matrix, but upper triangular parts have ID entries. Essentially, that's a weakening matrix. That's a weakening of the matrix. And the combination coefficients are given by μ0, μ1, and μ2. And these are scalars that are determined by the non-linear function of sigma. Now this principle turned out to be quite ubiquitous and quite useful. And over the past couple of years, we have studied this equivalence principle and also used it to analyze the learning capability of random feature models and curl methods. And code methods. Alright, so we're going to use the random feature model as a running example. So, first, let me quickly set up the problem. So, suppose you want to learn an algorithm that can correctly classify two types of data. You have red points and blue points. The challenge is that in the original domain, these points are not linearly separable, so the separating boundary is not a linear surface. Is not a linear surface. A very useful idea is that you just apply a feature map. You apply a transformation T to the original data axial, and you get the feature vector A of I, and then these are the feature vectors. And with the hope that in the feature space, these feature vectors are linearly separable. If that is the case, or approximately the case, then you only need to learn a separating hyperplane. And this hyperplane can be parametrized by a single. Carmatriz by a single vector that we call W, that can be the normal vector of this hyperplane. And the way you find the optimal W is to solve a minimization problem where W is a normal vector of the hyperplane, and A of I is the feature vector, and Yi is a label, plus or minus one, depending on which class you are in. And L is a loss function, and R is a regularizer. So, how do we form a feature map? Obviously, to use a neural network where the input accurate. Neural network where the input to the neural network is origin data and output of the neural network is a feature that's A of R. But we are going to consider an extremely simplified version of this model where that is so-called a random feature model. You assume that you randomly initialize the weight matrices of this neural network and then keep these random weight matrices fixed, frozen. And then this is just your feature model. It's only a random Your feature path. It's only a randomized feature path. What F is, for example, if you have a one-layer realm that is a random initialized and frozen width. And sigma is non-linear transformation you apply, for example, reduce width. And then you consider this learning problem where you want to learn an optical hyperplane where the feature factor is generated by in this way. This is a so-called random feature model, first proposed proposed by Rohini and Ratt. Proposed by Rahimi and Brett. Their original motivation is that this random feature map is a good multicolor approximation of a corner map. Alright, so once you have this model, we want to consider quantities. For example, we want to study the training error. If you solve this minimization problem, what is the error that you will get? Or the test error, generalization error. If you are given a new independent sample that is not part of the training set, and then how well? And then, how well this classification problem setup is doing. Now, if we get to assume the A of I's are Gaussian, this kind of empirical risk minimization problem has been extensively studied in the past decade or two. Then there have been a lot of very powerful tools for analyzing the asymptotic limits of these training error and testing error. You can get very precise correlations. Get very precise curve relations. The assumption is that the feature vectors have Gaussian or close to a Gaussian. The challenge here is that because of the linear transformation sigma, we do not have a Gaussian running. So that's the only challenge. So the previous tools that have been developed for Gaussian or independent ensembles are not readily applicable. But there has been some observation in the literature first. Some observation in the literature first that there is a very useful equivalence principle. So, here on the left-hand side, we have the A version, which is a non-linear feature map. We apply a random matrix F to the data matrix X, and then we apply entry-wise non-linear transformation. On the right, we have an equivalent model, D, that is a linear combination of three parts, all one matrix, F times X, and Z, which is independent Gaussian matrix. And mu 0, mu 1, mu 2. And mu0, mu1, mu2 are scalars that can be computed from the non-linear transformation sigma. Then for these two feature maps, we can consider the training error and the corresponding testing error. I also want to mention that the B version can be thought of as a noisy linear feature map because it's linear. You are essentially applying mu01 plus mu1 times the original feature of time ducks. But it's also noisy because. But it's also noisy because you add additional noise matrix C by T. Now, it has been observed in a series of papers that these two versions are equivalent if we are working on high-dimensional problems. So specifically, the conjecture is that the training error for the A version and B version have the same limit as the dimension of the feature space goes to infinity. And similarly, you also have the improvements for the test error. For the test error. So, I first learned about this equivalence principle from this series of papers where people observed initially by numerical simulation that you can see the A version and B version have the same results. I also want to mention that similar problems were first studied in the symmetric setting in the context of kernel random matrices, and in particular this paper by Chen Singer. They showed that they didn't mention the equivalent scale. They didn't mention the equivalence previously, but they showed that the spectrum of A matrix and the spectrum of B matrix in the symmetric version of equivalent. Alright, so here is some simulation results just to convince you that indeed we have equivalence. If we look at the figure on the right, the activation function, the sigma, is tens, and we are solving a logistic regression problem. And there are two types of symbols. The circles represent the training and testing errors for the original. The training and testing errors for the original non-linear model. And the axes represent the corresponding quantities for the noisy linear model. We can see that they match very well. And in experiment, we choose a P equal to 1000. So a thousand dimensional problems we have very good match. And this is also a very useful property to have because the B version is Gaussian. And then we can use very standard, very powerful tools for studying. Tools for studying empirical risk minimization if you have Gaussian variables. So, with my former student, Osama Di Vala, we showed that for the B version, the training and testing error converge to well-defined limits. So, here C and E of F are deterministic quantities that can be computed. So, there is nothing random about it. For this talk, I don't need to show the exact form of these functions, but you can compute these forms by solving some fixed-form equations. Solving some fixed-point equations. And here in the figure, the lines, the blue and the green lines, are the erratical curves that we can obtain from the Gaussian theory. But then the point here is that the Gaussian theory, because of the equivalence principle, can also predict the performance of the nano feature map. All right. And as a side note, I also want to mention that this kind of equivalence phenomenon is fairly general. In this paper, again with Osama, we studied. We studied a variation of this problem, so-called artificial noise injection. So we're considering a problem of training a classifier, but you have a set of input data X of I, but we assume that we can add additional noisy versions of X of I. So for each actual training sample, we add additional multiple copies of noisy version of the same training sample. Then there is a similar nonlinear transformation sigma here. And then using fairly similar Using a fairly similar principle, we can derive an equivalence model for A, which is given here, B. The detail again doesn't matter, but B, the point here is it's a linear, noisy linear model. Then from that, we can study the training and testing error of these noisy versions. And there are some interpretations, for example, adding additional noise can be thought of as being equivalent to having an additional regularization term on the uh on the wavelet. This is just a side note. Next, I'm going to explain, provide two explanations for why we have this equivalence principle. And the first explanation is by using universality. So if you look at A version, A is a matrix, but each column of A is independent. So the different columns of A are independent random vectors. And similarly, Vectors and similarly for different columns of B are independent random vectors. We can compute their expectation, the first two moments. So if you compute the first moment of A and first moment of B, you can show that if you choose the coefficient mu0, mu1, mu2 using this formula, I'm going to come back to the formula a bit later. So if you have a sigma, and then you can use the speed coefficient this way, if you use this choice of If you use this choice of the formula for the three scalars, you can show that the first moment, the expectation vector of A and B, are very close. Precisely speaking, the eigenvalue norm of the difference of two vectors is a little over one quantity. Similarly, if you compute the second element, the correlation matrix of A and B, you can show that the operating norm of the difference of two correlation matrices also diminishes as dimensions. Also, diminishes as dimension goes to infinity if you choose the three coefficient this rate. So, if you have this characterization, then you can say that A and B are equivalent as a special case of the universality phenomena, because many large systems are asymptotically equivalent as long as you can match their first few moments. And here, what we are saying is we are really only matching the first two moments of the two systems, right? And then with my fourth. Then, with my former student, Hong Wu, we approved this equivalence principle in the context of empirical risk minimization under some technical assumptions. We assume that we are solving a convex minimization problem. And then we also need to assume as a technical restriction that the activation function sigma is an R function. Although this is just an artifact of a proof technique, in practice, sigma can be a non-R function. Can be a non-R function. So the universality or equivalence principle is more general than that. Under these assumptions, what we showed is the following characterization. We showed that if you compare the training error for the A model, which is nonlinear, and training error for the B model, which is linear, noisy, then the probability that the A version to deviate from any given constant C by an amount 2 epsilon is upper bounded on the right-hand side. Is upper bounded on the right-hand side by the same probability for the B version to deviate from C by an amount epsilon plus a correction term. So this correction term will go to zero as t equal to infinity. So essentially, in the high-dimensional case, the probability for the B version serves as an upper bound for the A version, and the situation is symmetric for A and B. So as a consequence, if one of them converges in probability to a given concept, then the other one must also have. Then the other one also converged to the same constant. But for the B version, because it's Gaussian, there are a lot of tools to show that indeed they converge to well-defined constants. So applying this parallelization can show that the non-linear model, the A version, also converges to the same constant. And then some high-level description of the proof idea. So there's one There's one component in the proof, it's a central limit theorem. So, what we need to show is that given a generic direction, we project the A vector and B vector onto that generic direction, W, then the probability distribution of the projected scalar must be equivalent. But because the B version is Gaussian, so this is a linear combination of Gaussian, this is the Gaussian random variable, so this can be thought of as a central. Can be thought of as a central limit theorem. I want to say that this central limit theorem was first proved by in this paper by Sebastian Gold and Galen and co-authors. And in our paper, we had a different proof, but the first rigorous proof of central Linux theorem is given in gold paper. But this is not the end of the story, because what we want to show is the result of this empirical risk minimization problems are also equivalent. Risk minimization problems are also equivalent. So there's one more step that we need to go. And then the way we show the equivalence of these results of the equivalent risk minimization problem is to use so-called Lindbergh principle. The idea is very simple. So if you want to show that a function of a collection of vectors, in this case a collection of the nonlinear feature vectors, has the same value asymptotically as a function of As a function of the p vectors, what you want to do is to create a smooth interpolation between the two ensembles. The first step, you just replace the first vector a1 by b1, keeping all the other vectors intact. The second step, you replace a2 by b2, and so on and so forth. So you can see that we only need to do it n steps to go from a to b. But the change between any two neighboring steps only involves. Step only involves the swapping of two values, right? All the remaining values are the same. So, this is a very nice, a very simple perturbation argument. Because we only need to do it n steps, we only need to show that by triangular inequality or telescope exam, each step incurs a small difference of size, a little over one of n. And you go and do it n steps. So, the total amount of change is going to be little over one. You can have either. Little of one. You can be much more precise than that, but underlying idea is just this. But to show that indeed for each individual swap you are incurring a very small size, you can do a V1 out analysis or so-called Cavly method. Then you can also apply the central limit rank that I mentioned earlier for this purpose. Next, I have five more minutes left. I want to provide a different viewpoint of why we have this. Point of why we have this equivalence principle. To me, that's a more fundamental explanation for this equivalence principle. For that, we need to start with so-called Hermit polynomials. Hermite polynomials here, I give the first four Hermite polynomials. The first one is a constant. The second one is a linear function. The second one, the third one, and so on, they are polynomials. So, what's special about Hermit polynomials? These are orthonormal polynomials with respect to the Gaussian measure. So, it's a very, very convenient So it's a very, very convenient family of orthonormal bases if you work with Gaussian random variables. Now, for all reasonable functions sigma that is square integrable with a squared Gaussian measure, we can decompute it as a linear combination of Hermine polynomials, because Hermine polynomials form a complete orthonormal family. And the mu 0, μ1, μ2, and so on, these are the expansion coefficients. And this is just one. And this is just a one-dimensional function, can be written as a linear combination of these one-dimensional polynomials. Because our matrix A is applying this one-dimensional function to each entry of the matrix, so we can write the matrix A as a linear combination of individual Hermite matrices, where each Hermite matrix is applying one of the Hermite polynomials entry-wise to the random matrix. So, A as a random matrix is a combination of these H. Been a combination of these H matrices. Each one is just a plug-in chromium polynomial to these matrices. Now, fundamentally, what the equivalence principle is really saying is the following. A is a linear combination of these Kermit polynomials in a high-dimensional unit. We say A is equivalent to B, where B can be written as this linear combination where we keep the first Hermite component, which is a constant. So that's mu0 times all one matrix. The second Hermit component, because of The second Hermit component, because H1 is a linear function, that is keeping f times x. But starting from the third Hermite component, everything becomes linear. So we're saying that all the higher order Hermite matrices can be treated as independent Gaussian matrices. So this is really a more fundamental viewpoint of this principle. And this idea can also be generalized. For example, in this paper. In this paper with my colleague HDL, we considered a setting where we have a more flexible scaling of the data. So, here D is a dimension of the data vector, and N is a number of these data vectors. Classical high-dimensional analysis considered a linearly proportional regime where N and D are linearly proportional, but we consider a polynomial regime where n is proportional to d to the power of L, where L is an integer. The power of L, but L is an integer. So then in this case, there is turned out to be also a similar equivalence principle. First, we expand A as a linear combination of these coordinate matrices. Then we say that A is equivalent to a B version, where we can keep the first few components are low-ranked components. We can keep them or ignore them. And then starting from the L-mid component, H of L, we keep it. And it has a Wishard, it has a Marchengo power. It has a Machenko-parser distribution. And all the higher-order Hamid matrices are essentially independent noise. And their spectrum are semi-circle, so they are essentially independent GOE matrices. And because these are independent GOE matrices, you can combine all of these into a single GOE matrix. And that's the origin of the noise component. Using this viewpoint, and I want to say that this generalized an earlier paper in random relativity theory that considered the case. In random latitude theory, you got to consider the case where L is equal to 1. So, this is turned out to be a fairly general principle. And in our original paper, we assumed that the data vectors are rotational symmetric of the LCA, but in more recently, we considered the case where the data vectors just need to have independent entries. There's the same incremental principle. Alright, and then this principle has also been applied in our work to analyze the learning curve of a Kernel-Ridge regression. The learning curve of a kernel-rich regression problem where the number of data can scale as a polynomial of dimension D, then recovering the so-called scale of E, and so on. So, the more detailed papers, I also be happy to talk to you offline about this. So, as a summary, so this talk is about an equivalence principle for nonlinear random matrices. And the key message is that in many cases, nonlinear model is equivalent to a linear model plus. Is equivalent to a linear model plus noise, but right-hand side is much simpler to handle. This is also a special case of more general universality phenomena, which states that many large systems are seem totally equivalent, although they can be very different in their detailed constructions. Well, thanks for your attention. 