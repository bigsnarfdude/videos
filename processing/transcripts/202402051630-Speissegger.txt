Everything's fine. It disappeared for a second. We have Patrick Spiciger, who will tell us about nested Graphian functions. Thank you very much for the invitation. Glad to be here. So first of all, sorry for those who were in Manchester. This is going to be very similar, but I am saying a few things different. But I do have to go through some definitions for those who haven't been there and to those who didn't pay attention first. To those who didn't pay attention the first time. So something will be similar here. Okay? So this is joint work with Gareth Jones and Siegfried Van Hillen. Somewhere here? It's in the basement of the internal. Okay. So this has been kind of a very uh a project. A project on a very low flame for a long time that I've been working on with Gareth for probably 10 years. I'm kind of on and off because I didn't get anywhere. And the idea is that somehow the notion of path in functions, which I will define in a little bit more, the one that you hear about sometimes, isn't exactly Isn't exactly as general as what Howansky defined in his thesis when he introduced background functions. He only introduced them as an example to illustrate what he was doing. And so, but at the time I tried to find out from various uh people have worked on Taffy functions what the difference is and and nobody seemed to know. So Seem to know. So we have been looking at that a little bit for off and on again, and the difficulty was we couldn't find counterexamples. That's one of the difficulty in that. So counterexample that would show that those two notions are not quite the same. Okay, so let me start with a recap on what Paffian functions are. So in our setting, the domain is going to be an open set of Rn. An open set of Rn, U, and you have a tuple of functions on that set that are real analytics. That's for us, it's going to be a condition for the talk. So we call this tuple Le Paffian chain. If all the partial derivatives are given in polynomials in the functions f1 up to f, yeah, see there's a typo, that should be fi. F i. Right? So you allow to plug in the first i functions into the polynomial that defines the partials of fi. But the first one only refers to itself. The second one can refer to the first and the second. It's a kind of triangular notion. So yeah, that k over there should be an R. Make sense? Those are classes. Make sense? Those are classical traffic functions, really classical because we only allow polynomials here. The most famous example, of course, is iterated exponentials. The usual exponential is a Paffian function, a Pathian chain of length one. That function, together with its first iterate, second iterate, sorry, would be a Sorry, would be a Traffin chain of length 2, and so on. Those are the most fundamental Taffy functions there. So, but there are variants for this. Like, instead of allowing polynomials, we could allow Neromorphic functions and rational functions of various types. And in fact, we will have to do that a little later, but not right now. But you might want to accommodate roots, algebraic functions in those pijs. That's all possible. Or more generally, if you're given a non-minimal expansion of the real field, you could talk about Taffian functions over that expansion by allowing the pij's to be defined. All this is done. Okay, so we'll call a function Frafian if it belongs to a Frafian j. And what I'm interested in here is just that, for the most part, the terms at zero are Faffian functions, so that the Faffian in the neighborhood is zero on the RN. Okay? Let's call that Pn, right? Let's call that Pn, right? The set of terms at zero in Rn of Faffin functions. Find on some labor. So these are all, like I said, real analytic functions in our definition, which means that they have a Taylor series and the map that sends the function to the Taylor series is injective because the Taylor series is converging. Then you can inject a few elementary. But then you can check the few elementary properties, right? In fact, this Pn is our algebra. Well, it's not too hard to see that closed under sums and products and so on. Alright, so those are all things that are well known. That's why people like to have new functions. Well, that's one of the things, of course. Another thing is Hohansky theory. I'll get back to that. So. So, right you can consider expansions of the real field by Fairfield functions. If we look at the expansion by these terms, well then it's just a reduct of R sub n. So of course we know that's only. But when it comes to global Taffian functions, so not restricted to a neighborhood of the origin, but globally defined. The origin, but globally defined on all of our end. The story is different, like the exponential function. Before Wilkes' result and the exponential function, people did not know if Rx was a minimum. So there was the first step. But in part, right, part of real case proof that our X piezo minimal consists in proving that the real field The real field together with the restricted Fafian functions, meaning the terms at zero or Fafian functions, is o-minimal and model complex. So there's a very close connection between the local and the global theory. And of course the interesting stuff happens at infinity. So if you look at the expansion by all global Fafian functions, In functions, that was first proved by Motti, that he did so in the late 90s. And this R path is actually a reduct of something called the path enclosure of the real field, where you kind of like, you can think of this as it has to do with this relativization of Faffin, where you replace polynomials. Of Fafian, where you replace polynomials by definable functions in a given nominal structure. And you start by throwing in all the Faffian functions, global ones, just over the real field. And then Wilkes' result tells you you get a nominal structure. So you can start again. You can now take caffeine functions over that domain structure. And then Wilkie's result does not tell you immediately that. Result does not tell you immediately that this is a minimal again, so you have to work a little bit differently, like relativize the result. But it does, and it stops after a countable iteration. That's how you get the half-enclosure. And yeah, and there's one of the open questions about our path, and it's still open. What is the question whether or not it's o-minimal in the language of Faffin chains? That's not known. One of the mysteries about this setting. The only model completeness results we have is a result I have with Leon, where for the Paffian closure, so for that iterated construction thing you get, you do get a model completeness result, but for that result, But for that result, we had to go back to Krovansky's general setting, which leads to something that we call asterolities instead of paffin chains. So you have these, whatever they are, they're geometric construction. And in the language of those nasty droller leaves, you get novel completeness. Paffium closure. Sir, what do you mean exactly by the language of faffion chains? Language of Fafium chains. Oh, well, you had all Fafium chains as a function. Oh, okay. I mean, okay, as a tuple of functions. Yeah, yeah, yeah. Oh, okay. And here, the language of nested relatives, you add all the nested relatives or structure to your language. As predicates. As predicates. Because nested relatives are not functions. They are. I won't get into details. I will get to something close. I will get to something close. That's what the nested Parthian functions come from. This here is generated just by the Parthian chains. It's unknown if it's model complete. But usually when you add more, you are closer to the channel. Exactly. So this seems to be more. We don't know if it is more, but it looks like it's more, and it's enough to get monochromatic. So this might be a much bigger structure going on. Might be a much bigger structure, or it might be the same. We don't actually know. It's all related to the same question. So, yeah, sometimes I'll refer to this to Horansky setting, right? This geometric language, that's the stronger one where we get motor completeness. I sometimes call that Horansky setting. And so, of course, because of the open question here, right, after we got this model completeness result, that's why we tried to. That's why we try to start to look at this question: well, the elements, like these message roller leaves, can they somehow be defined from caffeine chains? So that's what was not clear. And I believe I might have that as a question right here. No, it's just a question, not more. But it is related to this question. Whether. Um whether Wanski setting is actually more general than Fafian chain setting or if it's not. So what I'm going to do next is I'm not going to tell you what nested roller leaves are, that goes too far afield. So nested roller leaves are a kind of flag system of nested manifolds, of codimension one, one in each other. In each other, in having certain regularity properties. But the thing is, right, locally at a point, of course, you can unravel that into a functional description. And that's what I want to do here. So I want to do that to give you something to compare Taffian chains to and the more general setting. It's a little complicated, a little annoying to do. So, because, like I said, Because, like I said, the nested relative setting has something to do with manifolds that are nested inside each other of decreasing dimension. If you go to a point and you unravel that in a functional language, well then the number of variables of each of the functions is going to be one less than the previous one, because the dimension is one less. So that's not like in the Taffyan chain, where the fr number of variables is the same for all the functions in the chain. Is the same for all the functions in the chain. So that's why this is a little annoying. So n is the big ambient space where all of these functions or graphs will live. And k is the number of functions in this chain, that I'm going to call a nested chain. And let's say all everything is, because we're concentrated at the origin for this thing, right? Everything is. Defined on boxes around the origin. So here a bunch of intervals, right, for each coordinate around the origin. B sub i is going to be the product of the first i intervals, and b superscript i is the product of the last i intervals. Sense? No, not the last i, I guess it's n minus i into it. So that B i cross B sub i, super i is a box. Now, the chain that corresponds to this local description in terms of functions of whatever Holansky defined is going to be a chain of functions defined on these boxes here. So the first function is going to be defined on the box P. On the box pn minus 1. So it's a function of n minus 1 variables. The second one is going to be one defined on the box Pn minus 2, so of n minus 2 variables only. Okay? And each of them takes values in the next interval in this sequence. Does that make sense? Alright. Okay, so I have a bunch of functions like this, and I look at the corresponding two. Looks just like the graph in. And two. Looks just like the traffic chain, except it's not in the same number of variables. This one has n minus one variable, the next n minus two. So on. It keeps getting smaller. The deeper the chain is, or the longer the chain is. And so because of this condition here, I'm saying the image of each of these functions is in the next interval, in this interval. The graphs of all these guys are contained in the next box. Alright, does that make sense? That's the annoying part. So now what makes these, a function like this, what we will call nested pair field? A little bit more notation. I need to compose these functions, right? What do I mean by composition? What do I mean by composition? There's a function of different variables. What I mean is I can plug the next function in the chain into the previous one, the last variable of the previous one. You get a function of as many variables as fi plus 1 was defined on. So that's what I mean by the composition of these two. Composition of these two. I just plug the next one into the last variable of the queries. Sense? So I can iterate that, of course. If I have composed fi with fi plus 1, I can then compose fi minus 1 with that in the same way. So I can. So I can iterate that other words. Whenever I have j less than or equal to i, I can compose first, I can put fi into fi minus 1, then the whole thing into fi minus 2, so on. And I end up, for each of these, I will end up with a function with as many variables as f i, n minus i variables. Iterating composition. Compositions. And they will still end up in the same intro. But with a version, it's associative. Associative in what we... Well, since you put no paraphysis, I'll deal with it. I think it is, yeah. Pretty sure it is. Because I'm always plugging into the last variable, right? There's something in there that's. There is something in there that's a convention. And yes, it is. Yes, it is a check, actually. All right, so finally, I'm looking at, I know, that's what it is, right? Fi, of course, right? This composition, if j is equal to i, is just fi. I'm just setting it to, and otherwise, we have iterated this construction here. Make sense? No, it's annoying, but I'm almost done with this. So now finally, when I have such an iterative chain, for our chain, phi is going to be, well, fi by itself, the tuple of. The tuple of functions, and then fi plus one composed with fi, and the next is fi plus two composed with fi plus one composed with fi. Oh, did I wrote the wait? I wrote this the wrong way around, didn't I? Um j is less, yeah, no, it's not f i plus one, this should be f. It's not fi plus one, this should be fi minus one. Other than that, it's it's right. So all these all these composit iterative compositions listed from F1 to F1 composed all the way up. So each of these components is in the same variable. And finally, when I mean I have a polynomial. When I mean I have a polynomial, right, I mean this composition here, so that's a polynomial in all n variables, so defined by the whole of Rn. Composing with Vi means plugging this tuple into the last n minus i variable. No, the last i variable, sorry. So this too will be a function of n minus i variable. My inside. Okay, it's really annoying, but that's all I need to define what I mean by nested Fravian chain. So, the tuple just like that, so with decreasing number of variables, is a nasty-fravian chain if you can find polynomials pij so that all the partials of each function in the chain is defined as pij now composed with this tuple of iterated compositions. Of iterated compositions, not just the FIs themselves. If you write it like this, it looks very much like a faffing chain. Remember, this guy only involves the first i functions in the list. Just compose, not just mentioning algebraic. That's the difference. So the main example, right? So, the main example, right, just to set things straight, is Taffy and chains like we defined them in the beginning. They are an example of this. If you have a Taffian chain, and now I'm taking it to be of n minus k variables. If I check Taffian chain in n minus k variables, it's going to be a function into b k c. Let's say it's a function into b k. Now, I'm going to consider each one of these Waffin functions in the chain as a function of the number of variables that we really want for the nasty Faffin chain, just where the new variables are signed. It might actually appear. I can do that. If you do that, then you look at the corresponding phi i's, right, the tuple of all these iterated compositions, they are actually just. They are actually just g1 up to gi. Because all the variables that I'm plugging in when I compose are the variables where the gi is actually silent. They don't depend on that. So they don't appear, just compositions. So then this condition here is just the usual praffin chain condition for this kind of chain. So praffene chains are nasty praffin chains in this sense, okay? In this sense. It seems like it's very easy to go from Fraffin chain to nasty Fraffin chains. Shouldn't it be easy to go the other way? Shouldn't I just be able to do something geometric, some kind of silly trick, right? But maybe if I have a nasty traffic chain, well, I add dummy variables to those who don't depend on all of them, right? And then do something to make it look like a traffic chain. We couldn't do it. Tried, I even asked Gabriello. No. So we're a little stuck on that for a while. Anyway, so the question was open, right? Is every nasty faffin chain faffie in some sense? Can we somehow go from one Go from one to the other. So, well, it's not true. Now we know. So towards the counterexample, we get to talk about the J function, the very same that we already heard about. And in fact, the same, I'm just looking at the restriction of the J function to the positive imaginary axis where I plot the I in front of. I plot the i in front of the t, so I get a real function with real values. So just like so, this kind of curly straight j, that's my client j function, tilted by 90 degrees, so to speak. So a little lemma that we figured out a while ago. The question has been open for a while, was the j function Been open for a while was the J function faffian in the traditional sense, in any way, right? It was the Fraffian chain, this J function here, that somehow contains the J function. And it seemed very, you know, really not far. So with a little bit of fiddling around with equations that the J function satisfies and so on, we were able to show that on some open interval, Some opens interval, j, which I also call j, the j function is just, the compositional inverse of the j function on that interval, is in fact the Fafin function. But it's the compositional inverse, and how do we account for the compositional inverse? We couldn't. Once again, we couldn't. We couldn't. Once again, we could not we couldn't do that. And in fact, when it it it stayed open, the question. If it is itself Faffy, it seems so close when all you need is the implicit functions you are to get the inverse function. Say of the integral, is it an equivalent of one of the ones? What do you mean, one of the pounds? The zero of pounds. I'm not sure. I don't think so. It's just an opening talk. It's enough. Even that, if we just knew that. So we weren't able to show that. But that's another open question that was floating around. So, right, the point is somehow that if we knew that those algebras of terms, of Fafin functions, were closed undertaking implicit functions, then of course the implicit function or the inverse function of that Fafin function would also be here. But question is all that we didn't know. We didn't know. So, another way, right, we could say, ha. So, if we can somehow show that this j function is not Paffian, then we would actually also be able to conclude that the algebra terms of Paffian functions is not closed. But then, yeah, the real question that was hanging in the air. Question that was hanging in the air is: How do we actually show that some function is not paffin? Showing that something is paffin, you produce the equation if you're lucky, and that's it. Showing that it's not equipped, not baffin seemed a bit like it should be, you know, it's a bit like that's a bit like calor theory, right? It's maybe differential calories. So, yeah, as you already heard it, right? As you already heard it, right? So, Jim's result, which is based on Jim and Tom's result about strong minimality of certain equations in TCF0, actually implies that this J function is not faffed on any open interval J. So, in particular, the one that we were interested in, that inverse one, it's not part of the. It's not part of a fatty change. Right, so I'm not going to go into the proof because I think you'll hear more about this whole story when Jim talks. This really uses the model theory of differential closed fields, which is quite different from the model theory of the usual structures that seem to be more natural for real functions. Functions. But it does, in other words, tell us a few things. So these algebras p and are not closed and they're taking implicit functions. And that brought us right back to nasty praffin chains. So, right, we do know that fraffin chains are nasty. That Fafian chains are nasty Fafian, which means this function whose inverse the j function is is also an SDF. So now we can ask the same question. Are nested Fafian chains or their terms, zero, closed and not taking implicit functions? And the thing is, this actually seems very plausible because remember an SF affin chain, every ver every function in it has one variable less than the previous one. Has one variable less than the previous one. But if you have a function in there and you use the implicit function theorem to get another one, you get one of one less variable variables, right? And that's exactly how it works. Except you need one thing, and that's what I came back to. I promised to come back to. In our definition of traffic chain or nasty traffic chain, we only allow polynomials, pi, j. But we can, like I said, we can replace those by, we can weaken that a bit. So if instead of polynomials you allow rational functions that are never zero on the graph of the corresponding graph in the function, then things are, oh, sorry. Then things are going to work out just fine, as we'll see. So we do have to make that allowance. We have to allow to divide. That allowed. We have to allow to divide by other polynomials which are never zero on the graph of the phi i's. And in fact, I was cheating a little when I said that translating Oneski is setting into functional notation gives the nested Faffin chains like I did. It doesn't. It gives these Faffian chains. So this is actually the n natural definition of nested Faffin chains. Of a nested fragment change. So, from now on, that's what I mean by nested fragment change. So, if you do this, you can show, and I will in a second, that if we define, we call NPN, the set of terms of nested fraction change at zero. You have all the properties from before, they're still algebraized, everything is fine, but they're also close. But they also closed undertaking implicit functions, and it's a natural thing for them. So, here is the outline of the proof. That's the setup. If you have an S D Faffian chain and the last function satisfies the conditions for the implicit function 0 with respect to its last variable, which is the n minus k. And of course, because these are real analytic functions, we already know that the implicit We already know that the implicit function for that condition is a new function, which is also real analytic, g, in n minus k minus 1. Alright, so after I shrank maybe the box a bit, I can assume that this key is a natural candidate to add to the graphian chain. It has the right number of variables, it has the right, satisfied the right condition, right? Because it goes to Right, because it goes to zero at zero, so you can make sure it doesn't go too far. So everything looks right from that point of view, domain and so on. Question is, what's the equation? Do what you have to do. But we do know that because g is the implicit function of fk, if you compose it, meaning plug it into the last variable of fk, you get zero. Take the derivative of this equation with respect to Of this equation with respect to any other variable, this is what you get. Feed up. This guy here is not zero because of the assumption of the implicit function zero. So I can divide by that and move everything over, and I get an equation for dj, which is exactly a rational function in the corresponding terms. By composing this. But composing this G in here is the same as first composing this chain of compositions with the extra one. So it just falls up. It's the right setting to get the increased function zero. Okay, so J is an S D Faffian. It means Hoansky theory still applies to it. It's really no different. In fact, no, it's really no different. The Hoansky theorem for nasty traffic chains is the same. All right, so in other words, okay, we have a setting in which we can capture the J function as a under Horansky theory using the nested traffic chains, which are the situation you get if you. You get if you unravel Hohansky's full definition. And it's also a situation that we know that this function shows is not covered, not even locally, by Teffinchins. Now, when I first told this, you know, the Seminar at the Fields Institute, Dao was in the audience, but his first question was, oh, so are these nested traffic chains? So, are these nested caffeine chains a wire stress system? What does wirestrass system mean? It means are they closed under wirestrus preparation too? Closure under wire stress preparation implies closure under implicit functions, but not vice versa. There are lots of algebraic functions, Dauge-Columban, they are closed on the implicit functions, but not by air-stressed operation. So, it's a good question, right? So, in fact, So, in fact, if you go past real analytic functions, you almost never get, you never get a Reierstrass system, I think. There are all these theorems that say you can't, but the only Don't Rockleman classes that are closed under Reierstrass population are the convergent power. The theorems like that for other thems too. So, this almost never happens from large systems. But, of course, our system. But of course, our system of nested Pafian functions is still a reduct of all, but they're all real analytic. So we're talking about something smaller than the collection of all real analytic germs. So the question becomes that. Do we know biostrus systems that are not all real analytic germs? Anyway, so that's been kind of the obsession. We have we don't know yet if that's true, but we we have a few. But we we have a few uh results. First of all, there's a co there's a conjecture. So one way there are many ways to prove Weierstrass preparation. One way which seems natural in this setting because we already have the implicit function zero and various other standard closure properties like algebra being an algebra and so on. But something like this to prove that your system of terms admits Of germs admits Weierstadt's preparation, it's enough to show that it's closed under the symmetric function theorem, let me call it M. So what does it mean here? We write it as conjecture. So if you have a symmetric nested traffic function, and that means, there you go. Symmetric just means that when you forget the variables, you're going to change the function. You're going to change the function. If you have a nasty traffic function that happens to be symmetric, then you can find another nasty traffic function so that the original one is the composition of that one with sigma. The composition here is in the traditional sense, because sigma is the tuple, the end tuple, of the elementary symmetric polynomials. The elementary symmetric polynomials, the first one, is just the product of all. Is that the first one is just the product of all the variables. The second one is the sum of all the squares xi xj, where i is less than j, and so on. And the last one is the, what is the last one? No, the first one is the sum of all the variables, sorry. The last one is the product of all the variables. Right? So, hello. I was too close, I guess. Anyway, so right, so if it's nested Taffian and it happens to be symmetric, then you should be able to find another nested Taffian function, g, so that g composed of the sigma is x. Right? Is this true for analytic functions? Yes. So it's true for polynomials, first of all. And from that, it's not too hard to prove. And from that, it's not too hard to prove that it's true for terms of analytic functions, convergent power sequence. So, in other words, because we already know f is real analytic, we already know such a g exists. And it's unique, in fact. So, in order to prove the conjecture, all we have to do is that g has to be nasty pairing. There's no choice. So, yeah, if we can prove this, then we also get Then we also get biased transparency. It's not proved here. Here is what we can prove. That's kind of strange. So if I do have an ST graph in chain, but something a little bit stronger is true, not just that the last function is symmetric, but that the corresponding sequence with the VK that we have to plug into the equation. That we have to plug into the equations. F in equations for it. That the whole Vk is symmetric. Now it almost seems like, oh, but I'm composing with a symmetric function here, so it shouldn't be symmetric. But if you plug a symmetric function into another one, you get something symmetric. But no, because these functions here, they also depend on the other variables where I don't plug in anything, so it's not symmetric just because I'm plugging something symmetric into the class. I'm plugging something symmetrical to the last variable. I made that mistake. And I thought I had proved the projector, but Siegfried pointed out: hmm, there's something missing here. There is. Anyway, so, but if this whole chain of compositions, only the Vk1, not the other one, is symmetric, then the conjecture is true. So then this real analytic function. Real analytic function, right? It means in particular that fk is symmetric. And so it is the composition of a real analytic function gk with the sigma function. In that case, this guy is also Nasty Faffian. In fact, the chain, the Nasty Faffian chain, is the same as for F. So it's the same way, it's just that you replace Fp by this GP. So you're not changing the other equations, only the ones for the last function. Proof commutative algebra. That's probably why I still haven't figured out how to do it without that strong assumption. You mean it's purely formal? It's purely, yeah, it's pure algebra. Okay? It's just prime ideals and, you know, that kind of stuff. Yeah. Yes. Anyway, so that's what we know. It's not quite enough. So I want to finish with something else that I also did not mention an ancestry. I mean, I can tell every time I talk about nested tapping functions, it's almost as bad as when I talk about nested volumes. Nested qualities. There is like that. So the question is: well, do we really need the nested Praffin functions to get the J function? No, we could just start with Praffin chains, the classical Praffin chains, somehow close under implicit functions. That would be enough to get the J function. So I do propose a slightly simpler definition. It's still not that simple. It's still not that simple, but it's much simpler because there are no compositions. No compositions involved. So you just think of it this way. It's like you start with a Paynerian chain, which are all in the same number of variables. And then there's a block where you're allowed to take implicit functions of those functions in the path you chain you just had. They are all in one less variable. And then you start another Paffian chain in that number of variables, which is allowed to refer not only back to themselves, but to those functions that are obtained in the first step. So there's going to be the F1 j's and then the G1Ls. Then there's going to be the F2J's, which are all Fafian, but also allow these guys in their allowed equations to refer to these guys as well. We obtained previously as implicit. As well, because we would take previously as implicit compositions. But no compositions, just that. Sorry? Only the polynomial. There is a composition. There's a circle. Yeah, right. I'm plugging in these functions into the polynomial, right? But there's no composition in here. It's algebraic in all those functions. The equations are not. The compositions make the equations non-algebraic. But this is all algebra. I should probably, the new should be appearing somewhere here, too. Okay? So you just allow that to iterate. Go down one variable using the implicit function, zero a few times, and then do again a whole bunch of Graffin equations, all the same number of variables as those implicit functions. Then you take again implicit functions. Then you take again implicit functions to repeat. So that's going to be closed under implicit functions, just by definition. Those kind of fabric functions. Okay? So I guess that's probably the, so here's a picture of such a chain. First block is just a path in chain, say of n variables. Then we have a few implicit functions of those guys whenever you're all allowed to use them. Now to use this function theorem. So they would all be of n minus 1 variables. Then I could use these to find a new pattern chain essentially over these. By over I mean I'm allowed to plug these into the equations too, in addition to those new patterns. And then again take inducing functions, reduce the number of variables, but that way. Make sense? So I would call those an implicit traffic chain. Why not? And any function that you obtain that's part of such a chain, call it implicit taff. So the j function is implicit Taflin. I don't know if every relative Tafin function is implicit Taflin. That's a question for the future. You can ask all kinds of questions, but at least. You can ask all kinds of questions, but at least you do not have to define the work with compositions at this stage. The reason why we did not really consider this before is because we didn't know, but every time you go to these implicit functions, it wasn't clear if for once the theory still applies. Now we do know, because of course, these chains are nasty traffic. Because nasty traffic functions are closed under classical traffic. On the classical Trafin plus implicit functions. So we know Hawanski theory applies to these chains. Yeah. Yeah, that's what I should have been here earlier. That's what I said. I think there's one more thing. Yeah, I have no idea if our weak version of the semantic function theorem holds in this or implicit. Or implicit fun nested functions, type in functions. I can't tell if it works. One could attempt it. Might be possible. I'm not sure though. Some things, yeah. Even though there are no compositions, I'm sure there's other complications. Anyway, that's all I have to say. I still don't know more about that. Questions for Patrick? Yeah. So have you look at the chat that's coming? This is the first time I wrote down the definition. I I knew that something like this has to be that should be the right thing to s to talk about. I I wrote it a few hours ago. But yeah, no, I don't know. So all kinds of questions like this. I think, I mean, the Hoansky estimates, I think they apply probably very much the same way as they do for classical chains, because they do apply the same way for an estimated chain. Greeneset Faffian chains, if the Pij are polynomials, they just depend on the degree of the polynomials, the length of the chain, all these things. That doesn't just work for the Faffian chains, it works for nested Faffian chains. So my guess is the estimates are very similar. They should not, in other words, introduce the length of the chain. The length of the chain would be the whole thing. Each implicit function step will count as one step. But other than that, it shouldn't be worse. Yeah, but is there like an intuitive reason why you can just see right away that the estimates for this class of functions should be better than, say, working with the class of noetherian functions? Well, the class of noetherian functions doesn't satisfy. Netherian functions don't satisfy all the estimates. Yeah, I know that. That's what I'm asking. Yeah, because of the triangularity. It's still triangular because it's still a nasty path in chain. But the triangularity does have to be viewed in the geometric setting of the nasty path, the nested roll at least. That's the thing. So if you do, I think it might be worth actually tracking the implementation. Right, actually tracking the implications of Provansky theory for specifically for this setup. It's true, right? Just to get to see if it makes the estimates any really worse or not. My guess is it doesn't. Where sharply or minimal, that would be negative. Oh yes, right. So that like but that's those are the kinds of questions. Is it sharply or minimal? Um like the expansion by these things. But the expansion by these things. But that's, yeah, yeah. Oh, sorry, what are the Trubotsky estimates? Oh, I don't know explicitly. So they tell you explicit bounds on the number of zeros of such functions in terms of the length of the chain, the degree of the pij's involved. And I think that's about it, right? Yeah, to give you explicit estimates. Number one. Estimates. Number of variables. Oh, number of variables, of course, yes. So in that sense, do you know the correlation between the QR and the QR? No, I do not. Well, no, the QR just means how many implicit functions you took. And every time you do, the whole UNSCI assumes get a little worse. You extend the length of the chain. So Sharpier minimal is a way The Short Bear minimal is a way of keeping track of the number of connected components in a very uniform way. And so it's right, that's what Bign Yamini and Lobikov proved and proved for the expansion by the terms of classical graph functions. Yeah, right, the germ. Yeah, right, the germs of classical faction functions. So they proved that omega wall structure has very uniform estimates for the number of factory components of any definable set. And very uniform is it's a bit complicated. That's what sharpen domain models is about. And so, yeah, our suspicion is, or hope is, that maybe even the nasty praffin terms have said The nasty praffin terms have satisfied that. If not, hopefully, these do. But the best would be if we had one such class that's a Weierstrass system and has these kinds of problems. Then you get a lot of you will get a lot of things. So the Weierstrass theorem that we are looking for, the conjecture is certainly interesting. I have no counterexamples. I have no concrete samples for that one. Yes? So if you want to prove that the local inverse of J is Fafian, does it essentially just come down to the fact that that function's like a quotient of two solutions of these particular order two linear equations, and those particular order two linear equations you can decompose in the right way? They decompose in the right way. They decompose in the right way using the corresponding Ricard equation. Ah, yeah, I see. Exactly. So there's a chain there, right? Where you first solve the Ricardi equation and then about 10. Yeah. Okay. That's exactly what it means. That's actually a kind of general procedure what we can do with differentiate with every time you have mean for a homogeneous order. Mean for homogeneous order end equations. Yeah, if you have Riccati-type equations to characterize how solutions behave, ordinary equations, ordinary different solutions meaning away, but specifically about J. Why J, why just the restriction to zero infinity to the imaginary line? Oh, because it's a real function. Yeah, right? Oh, yeah, if you shift to it, I mean, it's still not Fafian, right? Their result works at any point in the upper half plane. Is it going to be nested function if you were to take the real path and guess what? I don't know. I'm not sure if it would be nested path. Um, if it would be nasty if no, so you because then you have to decompose it into real imaginary parts, right? And that means you're talking about two variable functions, so then it becomes a little trickier. Um so no, I have not tried to see if it's still nasty path in away from the imaginary axis. Right. Well, there's a lot of things to still look at. Still look at and then there's other functions like this ask similar questions about anyways there was a slide where symmetric polygonalism There's a general theorem that people learn in algebra courses. Like if you have a symmetric polynomial, then the composition of another polynomial would be elementary symmetric polynomial. But these are not the only generic. There are other representations. I think that's true, yeah, yeah. And not interested in well, no, I think once you know this, but you can probably also fiddle around with the other generation. Fiddle around in the other generators to get from one to the other. I don't really care as long as it's some one of them. It must be one of these because they are related to the roots of a polynomial. Yes, that is right. Because these elementary functions are related to how you compute the roots of a polynomial. If you take, if a polynomial is one variable, the roots But the roots, they appear as lin in linear factors when you uh factorize it over the complex numbers. You mean the coefficients are very much. That's right. That's why these things come. Okay, so that's why you pick this particular set of generations. And that's why this is what we need to prove virus vibration. Because virus was preparation looks for the roots with respect to the last variable. Virusual spacer can't you know. Okay, let's thank Hadja beginning. And so as a reminder, we have now we have dinner, but then uh we have a full-time session after dinner at seven thirty. So right back here in the lobby. So, right back here, the lobby.