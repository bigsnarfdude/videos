So let me start. So my presentation consists of several parts. One is actually talking in general terms about the natural gradient method. I will try to highlight. To highlight a little bit the information geometric perspective at optimization through gradient descent methods. Then I will refer to challenges in deep neural networks where the natural gradient methods become untractable. And I want to provide ways to simplify the The situation and by exploiting the structure of the underlying network on the one hand and the Fischer-Rau metric on the other hand. So it's ongoing work, so I will not come up with final solutions, but you will see the direction of research. So, first of all, what is the general perspective? So, if we are dealing with neural networks, With neural networks. Can you see my cursor? Yes. Yeah. Okay. So neural network often implements a function. So you have an input vector X and an output Y, and the network computes or implements through local computation and composition an overall function f here. F here parametrized by theta. Theta denotes all the tunable parameters of the network. So if you so this is an element of a general space of functions here denoted by f. I refer to it as the ambient space. And this function implemented by this neural network would then be one point f theta in that. F theta in that function space. Now, if you modify the theta and go through all possible theta values, you generate a so-called model. So that's the set of functions that can be represented by this architecture, by this neural network. So as you can see, this is a nice geometric object here, of course, nicer than in reality. So this looks like a manifold here, but you. This looks like a manifold here, but usually it's much more complicated. But in any case, it is a geometric object embedded in an ambient, usually infinite dimensional space, or at least higher dimensional space. And this geometric picture is also so. I'm sorry. So the target of learning is the following. So you start. Following. So you start with a function f that you want to represent by the neural network, and then you try to find the best element in the set of all functions that can be represented by the network, which approximates f in the best way. So that's the best approximation of the target function f. And usually, so you know from high school, if you want to. From high school, if you want to minimize this distance here, take the derivatives of the distance and set them to zero. But of course, in high-dimensional spaces, we cannot work like this. So we start somewhere with some f theta zero and try to optimize by applying some iterative algorithm in order to converge to the f theta, the optimal f within the model m. In addition to that, In addition to that, so the optimization is often driven by data so that it's actually a stochastic process. And often you ask the question whether or not this algorithm converges to the optimum in the limit and how fast it converges to the limit. Anyhow, so the traditional approach is using the Is using the geometry of the ambient space, which is quite natural. Here it would be the, as an example, the L2 distance or squared L2 distance between two functions in that ambient space. And of course, that distance measure is also applicable on the model M. So other space, other distance. Space other distances from the functional analysis have been applied, like the L infinity norm, the distance coming from the L infinity norm, the L2 norm, and the L P norm. So actually, one of the first publications on universal approximation by Sibenko is using this kind of distance here. And so, what I want And so, what I want to highlight here is simply the fact that we are actually dealing with a geometric object in an ambient space which carries a natural geometry. And that's the main perspective of information geometry. So, I already mentioned functional analysis. In functional analysis, you have many geometric structures and information geometry. And information geometries to particularly distinguish geometries in the context of probability distributions and Markov kernels. Algebraic statistics is related to information geometry and tries to utilize methods from algebraic geometry in order to describe such models here. So that's the general information geometric perspective. Geometric perspective. So, from information geometry, we have a Riemannian structure in the ambient space F, which is referred to as the Fischer-Rau metric. And that Riemannian structure can be induced to the model M, and that leads to the so-called Fischer-Rau metric. This is what I'm going to introduce now. So, as I explained before, so that's the model here, and the model is the image of a parametrization. The parameters here are denoted by psi, and the ambient space will be a probability, the set of all probability distributions, strictly positive probability distributions, on a sample space S. We will always assume that S is finite here. And I'm just introducing. And I'm just introducing notation here. So, this is in terms of the parameterization of phi, you can write down the first fundamental form of the Fischer-Rau metric, and it is well known, and that's a formula for it. So that comes from a natural geometry living on this ambient space B of S. Now, if you want to optimize an objective. you want to optimize an objective function L, L is usually, it can be defined on the model alone, but I always consider it as a function on the ambient space P of S here. So it's a smooth real-valued function. And now you can compute in order to optimize or minimize that function, which is often something like a distance, a cordiac liblar divergence. Divergence, you apply the you use this Fisher information matrix and try to go in the direction of steepest ascent or descent. And that leads to the notion of the gradient with respect to this metric here. So one can prove that the gradient, which is referred to as the natural gradient here on the model M On the model M in a point peak ψ, in a non-singular point peak ψ, is actually in local coordinates directly related to this expression here. So what it says is the following. You take the partial derivatives of L in the directions, in all directions of with respect to all parameters x c i. So I go. I so I goes actually I didn't say this explicitly so it has it's a subset the parameter space is a subset of R D so it has it's a vector it consists of vectors of length D so you take all the partial derivatives in here D partial derivatives and multiply with the inverse of the matrix G where G is coming from this is defined in this From this is defined in this way. Now I said the inverse, but actually, that's what you would take in the classical setting of differential geometry where you have, I'm sorry, where you have a coordinate system. In the context of neural networks, often this is not a coordinate system. It's a more general parametrization. So the G, the matrix G, will not be. Be will be degenerate so that the inverse does not exist. So, therefore, I use the symbol plus in order to denote the Moor Penrose inverse. So, instead of taking the inverse here, we use the Moor Penrose inverse and have this object here, which is abbreviated by nabla tilde. So, that's the, so to say, the natural gradient in local coordinates. Now, there is, of course, the question whether or not this represents the natural gradient. First of all, if P psi is non-singular and the parametrization is proper in P in the sense that these derivatives span the whole tangent space, then actually this nabla tilde represents. Represents the natural gradient in the sense that the image of that nabla tilde with respect to this differential here, the differential of the parametrization, gives you exactly the natural gradient. So this works well here. But I have to highlight one problem here, which caused some confusion in the literature also. So due to the fact that Nabla Tilde is using the That now Black Tilde is using the Moore-Penrose inverse here, the plus, and the Moore-Penrose inverse is based on the Euclidean geometry in Rd. People have assumed that this object here, the Nabla tilde, is actually not invariant, which is true because, so if you take the Nabla tilde with respect to a different coordinate system obtained in this way. Obtained in this way through reparametrization, you get a different object. So, you lose the invariance in this sense here, but if you project this object and this object onto the manifold so that it's an element in the tangent space, then it is invariant. So, we highlighted this interplay between invariance and non-invariance in the real. Invariance and non-invariance in the recent paper with Jesso Ostrom and Johannes Miller. You can read more about this in these two papers here. So anyhow, if we are dealing with non-singular points and the parametrization is nice, then this Nabla tilde is a nice representation of the natural gradient. So far, so good. So far, so good. So that looks very abstract, and it was not clear whether or not this will help at all in optimization algorithms for neural networks. And it was proposed by Amari in 1998. And it was quite a surprise to show that this more abstract and more complicated-looking formula gives you better optimism. Formula gives you better optimization algorithms in the sense that they converge much faster and you don't end up in points, in local minimizers and so on. Or in plateaus, you avoid plateaus. So there are many, many positive properties of the natural gradient methods, which exploits the natural geometry of probability distributions. Probability distributions. So, after the proposal of AMARI in 1998 in the context of supervised learning, it has been applied in the context of reinforcement learning, robotics, and so on, and it proved to be very efficient. Now, when you want to make the transition to deep neural networks, you have a problem because the G plus here, the G has the dimensionality of the parameters. Of the parameters. So, if you have a huge number of parameters which you have in a deep neural network, then it's impossible to invert this matrix here. So, that's a big problem. People have tried to approximate that matrix G in order to make it the method, the natural gradient method applicable to deep neural networks. And there are wonderful works. There are wonderful works on this topic by Olivier, Martens, Gross. To my knowledge, what I'm going to present to you is complementary to this work, and it tries to solve this problem in two steps. So, the first step is by exploiting the structure of the network. So, for example, if you have such a directed Such a directed acyclic networks. This is a two-layer network with directed edges. Then the Fischer information matrix has this block structure, and each block corresponds to a unit here, to these three units. And if you subdivide, if you have a different structure which is deeper, like this one, Like this one, you get even a sparser matrix. So each node here corresponds to a block in this matrix, and it's filled outside of these blocks. You have zeros so that you can actually have a possibility to invert this matrix here. And I have to say that this is really a unique feature of directed. Feature of directed acyclic graphs. If you have undirected graphs like in a restricted Boltzmann machine, this does not work anymore. So, there you will have a dense matrix, a full matrix. So, with this knowledge, you can write down the natural gradient and it will be basically, you can break it down to local natural gradients for each unit, one single and so. Single and smaller natural gradients, so to say. So, the matrices, of course, you hope that these smaller matrices will be invertible so that you can invert them separately, and this way you will have the inverse of the global Fisher information matrix. We did apply this method to the MNIST data set and compared our methods. So, this is actually the context of Helmholtz machines, and there is the tradition. And there is the traditional Helmholtz machine. I think it's in green here. Then there is an improved one, which is called the re-weighted wake sleep. That's the state of the art. This is the orange curve here. And then there is a natural gradient version of this state of the art, which is the blue one. This is what we are proposing. And as you can see, it's working, it's improving. Working, it's improving the previous methods. We are still working on this. So, but there is a conceptual problem here. And this brings me to the main topic of my talk. So, the Fischer information matrix factorizes if you apply it to the full network. So, the model is living in the full system in this probability distributions over the Probability distributions over the hidden and the visible nodes. Let's have a look at it, this diagram here. So that's the probability set of probability distributions on the visible nodes and the hidden nodes. And then we consider pi, that's the marginalization map, which assigns to each global distribution its marginal, which lives on the visible nodes. And in most cases, also in the In most cases, also in the MNIST data set setting that I just described, the loss function is defined on the marginals. So it's not so important how to represent this hidden structure. So what we want, we want to approximate a target distribution on the visibles. So L is actually a function defined on Pv, which is the set of all distributions on the visible nodes. Now you can consider the Fischer. You can consider the Fischer-Rau metric on this probability simplex and also on this probability simplex. They have their own respective Fischer-Rau metric. You can optimize L on PV with respect to this Fischer-Rau metric, or you can consider the composition, which goes this way and then this way. Then, this way. So, directly defining a function on the full system and optimize this composition on the full probability simplex. And now the question is whether this gives you the same result. If it does, then a simple simplification on this space here can be used as a simplification on this space. On this space. So that's the general idea. And as we have seen, this the Fischer information matrix factorizes on the full system. So it does factorize here. So the question that we are asking whether the natural gradient of the full system, you take the natural gradient of the full system, and then you consider the differential of that natural gradient, which lives in the tangent space here. tangent space here and then you compare it with the dire directly with the uh with the natural gradient on this space and you can ask the question whether this is uh the same uh gives you the same result and actually it does and it does because we are we are dealing with the fischer-Rau metric here you cannot expect this from other metrics but that's not the whole story that's the good news but the bad news Story: That's the good news, but the bad news is we are not working on the full system and in the marginal system, we are working in a model. So, the model is actually the model that corresponds to this network here is a set of probability distributions that lives in the full simplex. And you can marginalize this model and get the marginal model, which corresponds to some kind of projection of. Corresponds to some kind of projection of the model to a lower-dimensional space. First of all, if you project something nice, it can become quite complicated. Even if you take an exponential family, project it down, it can have singularities and so on. So that object will be much more complicated. In addition to that, as you have seen, the Fisher information matrix had block structure on this space here, but we cannot expect it to have block structure on. Expected to have block structure on this space, and this is what we want to understand here. So, we can ask the question whether this invariance still holds if we restrict attention to a model as opposed to the full probability simplex. And it turns out that this is not the case, and only very special models satisfy this condition. And I'm going to describe these models. To describe these models, first of all, before I do this, I want to take a slightly more general perspective. I will replace the marginalization by a general cost graining. So, here the marginalization was taking a global configuration and just restricting, so you have a global configuration and you restrict attention to configuration on the visible node. On the visible nodes, and that generates that gives you the push forward as a push forward of probability distributions, it gives you the marginalization. So now we consider general cost braining. So we have a large set Z and a smaller set X, and we consider an on-to-mapping from Z to X. And the previous situation is a special case. This gives you, through the push forward, Through the push-forward mapping, a map from the probability distributions over Z to the probability distributions over X. And the differential, the DX star of that push-forward map is a mapping between the tangent spaces of these simplicies here. So this is just for the notation. And now we can ask. And now we can ask the following question. So, this is the same diagram. We have replaced the marginalization map by the push forward of X, so X star, and then ask the same. And here that was the larger space, the hidden and the visible nodes, and here only the visible nodes. So, this is the same situation as before, but somewhat more general. But somewhat more general. And again, it turns out that if we don't restrict attention to a model, but take the full system, we again have invariance if we assume the Fischer-Rau metric here and the Fischer-Rau metric on this space. And we can ask the same question if we restrict attention to a model in this large space and consider the image of that model Mx, whether this invariant. X whether this invariance is still valid, still holds. And I want to specify those models for which this invariance holds. And they are specified in terms of this slide here. So this is what we had before the notation. And now if you The notation. And now, if you have the differential from the tangent space on Z, on P of Z and the tangent space of P of X, now we can consider the kernel of that differential, denoted by V P. V stands for vertical, so that's the vertical component. And then there is a horizontal component, which is simply the orthogonal complement of that vertical component, and orthogonality is taken. And orthogonality is taken with respect to the Fischerau metric. So we have a vertical and the horizontal component. And now we say that a model is cylindrical in P if its tangent space can be written this way. So you intersect the tangent space with the horizontal component, you intersect it with the vertical component, and then you take the sum of both. Of both, and you want that this equals the tangent space. It really reflects this condition, really reflects this image of a cylinder. And we call M cylindrical if it has this structure in all non-singular points. So, we have already seen a cylindrical model, namely the full model, and we have also seen for And we have also seen for the full model that the invariance of the Fisher information matrix holds. But there is a, so this full model here is cylindrical with maximal vertical component. So when you project it down, the kernel is maximal. But there is one quite interesting class of models with minimal vertical component. Component. This is the class of models for which the projection X, the cost-graining X, is sufficient statistics. And they can be obtained in the following way. So you consider for each X a probability distribution on Z so that with support equals to the pre-image of it. equals to the pre-image of x. So px, it's a probability distribution of z on z. So px of z is greater than zero if and only if the projection of z is equals x. So these are probability distributions with disjoint support. And then you take basically the convex hull, but in order to stay in the interior of the simplex, you take only Of the simplex, you take only weights of positive weights. So basically, in the simplex, you consider here a partition of Z and then the corresponding points in the corresponding faces of the higher dimensional simplex, and then you take the convex combination. So they have. They have actually the same dimensionality as P of X. So they are cylindrical. You can prove that they are cylindrical with a minimal vertical component. Their projection, Mx, is the full simplex P of X. And actually, these models play an important role in Chensov's characterization of the Fischer-Rau metric. So they are the images of the so-called market. Images of the so-called Markov morphisms, congruent Markov morphisms. And now let's ask the same question again. So we want to know when this invariance holds for which models and for which Riemannian matrix. And that's the main theorem. So basically, for every sample set S, you can Sample set S, you consider a Riemannian metric on P of S, which is denoted by G upper script S. And now you have this, you consider a cylindrical, I'm sorry, a cost graining. Yeah, so in one, you consider cost graining from Z to X. Z to X, a cylindrical model M with respect to that cost graining, a loss function L and an admissible point P, which simply means that I can write down this here. That's not so important. Then you have this equality if and only if you are evaluating these quantities. You are evaluating these quantities here: the grat on the left-hand side and the grat on the right-hand side with the Fischer-Rau metric, which basically means the following. So, this is the more intuitive or less precise description of what you see here. So, if you have the Fischer-Rau metric at You have the Fischer-Rau metric at hand, okay. If you assume the Fischer-Rau metric, then the invariance will hold for all cylindrical models. Okay, this is what it says. That's the implication between from two to one. On the other hand, if you require that this invariance holds for all cylindrical models, then this has to be the Fischer-Rau metric. You have to choose the Fischer-Rau metric. You have to choose the Fischer-Rau metrics on the P of S. So this is the direction from one to two. So from two to one, no, I'm sorry, from one to two is basically Chensov's characterization theorem. It is used in that context. And from two to one, it's by a simple calculation and exploiting the cylindrical structure of the model. Cylindrical structure of the model, you can prove that this holds. So, in other words, the only metric for which you have a family of metrics for which you have invariance of the natural gradients for cylindrical models is the Fischer-Rau metric. And this is what I mean by invariance of the natural gradient. You can consider this theorem as a kind of reformulation of Chentsov's classical theorem. Does this Does this help us? Not really, because there is a problem here. I have shown to you the network where the Fisher information matrix has this nice block structure. So, if that block structure, if that model is already a cylindrical model, then we can use that theorem and we are done. But the problem is that The problem is that this kind of models is not cylindrical. Therefore, I'm proposing here cylindrical extensions. So if you start with a model that is not cylindrical, then you should extend it. That's one way to simplify the problem by M tilde so that the projection of M tilde is the same as the projection. Tilde is the same as the projection of m, so it gives you the same model on p of x, and m tilde is cylindrical, so that you can actually assume invariance of the natural gradient. I'm proposing this as a method. I have done a few steps in that direction. So, what could it be in the context of these networks here? Networks here. So we have seen that for this kind of network, the Fischer information matrix has this nice block structure and we can exploit it and it does quite well. But we don't want the Fischer information matrix on the full system. We want to actually evaluate it on this marginal system. In order to make use of the previous theorem, we need a cylindrical extension. And it turns out that if you And it turns out that if you add to this network a so-called recognition network, which goes from the visible up, then you can define a model which is larger than the original model and has this cylindrical property. So, and that's a kind of surprising result here that we come up with the idea of a recognition. The idea of a recognition network from a completely geometric and perspective, which is different from the perspective that has been taken in the context of, for example, Helmholtz machines. When people talk about a recognition network, they don't use it in order to define a cylindrical extension. So it was a kind of surprise to me. So in this work here, I did first. So, in this work here, I did first steps in that direction, and we are currently trying to actually exploit this structure in order to simplify the natural gradient method for this kind of deep neural networks. So, let me conclude with an advertisement here. So, as you have seen, this is about information geometry, and we are currently accepting. Currently accepting submissions to this special issue on information geometry for data science. I think the deadline is by the end of January. So you have the webpage here. I would be very happy to receive your contributions on any. On any kind of information geometric aspects of data science. Thank you very much for your attention. Thank you very much, Nihat, for the wonderful talk. You have questions from the audience. Please. All right, if I may ask a question just to kick off the starting point. To kick off the study, it's a very, very rigorous and I think very solid analysis. I appreciate that. So, I always have a general question about like the natural gradient method, not having worked in this area. So, maybe you can explain to me better than anyone else, namely, how does the objective function and the natural, the Riemannian metric of the gradient that you use, right? So, how do they interact or whether they interact or not? Because I know the objective function, The objective function, sometimes it would have to be some kind of expectation of the probability. So, therefore, the probability comes in, and therefore you use the, say, like the fisherman metric, right? So, but the question more generally, like using this natural gradient methods, how does the objective function interact with the parametrization of the manifold, which is through the gradient method? Yeah, this is a very good question, June. Thank you very much. So, it's actually So, usually, you could simply apply the natural gradient method to any objective function. But you cannot expect to have a nice solution. So you could rephrase your question in the following way. You can say, how can we define objective functions that give you nice? That gives you nice solutions, for example, geodesics as solution curves or learning curves, if you use the natural gradient method. So that's the kind of compatibility that I always consider to be important. So it should be compatible in this sense that when you compute the natural gradient, the solutions, if they are geodesics, then if Then, if you in the parameter space with respect to natural coordinates, you can go along a geodesic. That's something considered to be very helpful. And you sometimes achieve this if you couple the Riemannian metric with the objective function. And this is if you couple, if you take this as a requirement, you can actually motivate. can actually motivate or even define, uniquely characterize the Kullbeck-Leibler divergence as the unique one that gives you geodesics when you do the natural gradient method. So if you take, for example, the L2 distance and do the natural gradient method, it will not give you something easy to compute and it might not give you a nice learning algorithm. You nice learning algorithms, but if you replace the L2 distance by the Kullback-Leibler divergence and the order is also important, you will get something very nice. So, and to my knowledge, maybe more implicit, this is also one motivation for Amari to actually introduce this coupling. So, if I may just rephrase this, so in your opinion, then the In your opinion, then the objective function would be in some way related to the divergence function on the manifold because the divergence function we know induce the Riemannian metric, which induces what is the natural gradient. So in your opinion, then any like good objective function, right, to if you know for any practical problems, you will need to somehow cast it in a kind of a divergence function kind of a kind of a setting, right? Is that the is that the yeah? Is that the yeah, so but divergence function is a two-point function, so entropy is a one-point function, and so the objective function. I mean, right now, the I'm trying to conceptually understand whether how to make it always make it a two-point function, right? So you can say that in supervised setting is straightforward, but in unsupervised setting, you know, that would be a challenge here. Yeah, just one remark. I mean, the object, the two-point function, it's also important to It's also important to have this coupling whether you do the natural gradient with respect to the second or the first argument. This is concerned. Absolutely. Absolutely. Rookie Cole, do you have a question? Yes, I have a quick question. Yeah, it was a great talk. But I'm interested in the special issue for the journal you mentioned. So if you can actually So if you can actually post the web address in the chat so that I can copy and paste and I can find access to the website. So if you can do that, that would be great. Thank you very much. I will do that. Thank you. Thank you. Gerardo, do you have a question? Please. Yes, I was wondering. I mean, you were talking about geodesics, and I was wondering. And I was wondering if you're trying to use the L V G beta connection or the so-called alpha connections so I was I was I was busy with pasting the link so I'm sorry I'm not good at the multi-duty. I'm sorry, could you please repeat it again? I'm sorry, could you please repeat it again? I mean, what I wonder is you were talking in maybe the learning curves are geodesics. So this makes sense using the Levitvita connection or the so-called alpha connections or which kind of connection will you use or will you try to use to so the geodesics will be the the alpha constant Alpha connections, the geodesics with respect to the alpha connections. That was what I was referring to. So if you take the alpha divergence and take the natural gradient, Levici Vita natural gradient. So this is the metric connection. Connection of the alpha divergence, let's say with respect to the first or second argument, you get as a solution curve of that differential equation, you get the alpha geodesic. So basically, depending on, so with the same Riemannian structure, with the same Fischer information, the Fischer-Rau metric, depending on which objective function you take. Objective function you take, which alpha which divergence you take, you will get a different geodesic. There is this coupling between the kind of geodesic and the objective function that you take. Yeah, I suppose you can work, you can play with the alpha, right? And then maybe you can find someone that makes you geodesic. Yeah, so if you if you take the normal pullback Leibla divergence, you will get the M-geodesics and the Will get the M-geodesics and the E-geodesics. Thank you so much. You're welcome. Woo-lin, please. Okay, so I have several questions. So the first one is that you mentioned that the natural gradient in the, I mean, in the fully observable system is going to be the same in the case. Basically, you have another one is for the joint distributions, another one is for the martial distributions, right? Is this your claim? So I'm. No, that was not my claim. So that was not my claim. That was my naive hope. I see. And so it turns out that the models for which this holds are very simple. They are cylindrical. So I specified the models. They have to be cylindrical. And one trivial cylindrical model is the full model. So if you consider the full simplex and you project it down to a smaller simplex, the natural gradient will be invariant. Natural gradient will be invariant, and the other one are the kinds of models that appear in the context of Chensov's theorem. There, you also have that invariance. And you can define all kinds of models, but they are not the kinds of models that we have in the context of neural networks. Neural networks don't give us cylindrical. So, when you have a neural network, it will give you some nice model in the full system, but this model will usually not be. Model will usually not be cylindrical. So, I guess that the following model is not true. If we consider the mix of Gaussians, it's the joint distributions. Then you can have this, you can consider the mix of Gaussian is the margin distributions, right? But you can also have a joint distributions for the mix of Gaussians, just like EM algorithms, right? So, I guess that in this setting is that maybe they're not the same for the natural gradients. That's an interesting case. interesting case I wouldn't I wouldn't expect them to be the same but it's a nice and interesting setting yeah because for example you consider that two mixture of Gaussians you could have identical components yeah yeah so in this case then yeah yeah and also is that I think is that in in your setting is that you assume that the mixture of that I mean the future information mixture you're going to have the expectation and is going to be finite or are you going to change Going to be finite, or are you going to generalize into the integral case? I'm just dealing with the finite case here. I see. Because I think, at least in machine learning literature or maybe in statistics, you usually because I think usually they just consider empirical feature, right? Yeah. Yeah, because you don't have finite samples, right? Finite data. And also, I think they don't usually doing this. That I think they don't usually doing this in doing this pseudo-inverse because they're just using these dampening terms because pseudo-inverse is type still not very efficient. Yeah, we have several aspects here. One is we have to distinguish, which is very important between the empirical Fischer information matrix and the actual Fischer information matrix. And so I didn't talk about the empirical at all. Okay, so I'm talking about the actual one. And the One and the empirical one is generically non-invertible, and they are using this damping, but not in so you could take the pseudo-inverse, the Moore-Penrose pseudo-inverse, but that wouldn't be appropriate because you have finite data. So you always have to regularize. This is why they are using different techniques. So you could take the pseudo-inverse, but it wouldn't be appropriate for finite data. Be appropriate for finite data. So, in my setting, I can take directly the more Penrose inverse. So, my question is that if we introduce these bambooing terms, do you think it's still invariants or just no? Ah, that's interesting. Yeah. I don't think it's in that's interesting. That wouldn't be invariant. That's true. Yeah. Okay. Okay. Thank you. You're welcome. You're welcome. Well, we have questions for Matcier. Mati? Hello, Professor. Thank you very much for the interesting talk. I have actually two questions. You talked about the difference between KL divergence and L2 norm. How you know what you know what is the right metric to use? The right norm to use. And if you want to consider the ground metric, is it also reasonable to use Wasserstein distance? I didn't understand the last word. If you want to use the ground metric structure, is it also reasonable to use Wasserstein distance? Yeah, so starting with the last Last question. I think it's very important actually. So we have this huge problem with classical information geometry, even though it's specified, the structures are specified through invariance conditions, going back to Chensov's theorems, they are not sensitive to the metric on the sample space. And this is Sample space. And this is really a shortcoming. And there is a lot of recent work. Guido, for example, is one pioneer in this field, trying to apply this to actually information geometry and defining Wasserstein geometry in an intrinsic way. And there's a lot more. Jun Tsang is actually editing a special issue in information. Editing a special issue in information geometry devoted to this subject, and I consider this as being very important. So, what I on the other hand, we have these strong arguments about invariance on classical information geometry. So, we have Wasserstein information geometry, which I consider, especially in data science, very important to be very important. But so, how to compare this with the classical invariance required? Classical invariance requirements, I don't know. So that's an open problem. So they basically say if you want to have invariance of some kind, then you are forced to take the Fischer-Rau metric and not the Wasserstein metric. But on the other hand, I consider this respecting the the metric on the on the sample space to be a very important point. Space to be a very important point. So maybe Jun can also say something about this. But yeah, this is something I would like to kind of chime in and also maybe invite Guido and others who have experience in this to sort of like to share experiences and views because this is a question obviously of a great importance to information geometers as well as to people in water sign geometry. So my reading into this is that somehow the information geometry of, of course, basically still like a low. Basically, still like a local kind of a vector space structure, manual structure, right? It's localized, it's basically, you know, it's a diffimorphic to iron, you know. But in Wasserston case, there, the topology, the topology somehow plays a role in, which is somewhat different than the topology that operates under the KL divergence. I mean, the KL divergence, where basically you can scramble. You can scramble the sample space, and you can basically have a scrambling or any kind of relabeling of the sample space without affecting the KL divergence. But in Watson geometry, that's the whole thing, you affect that. So somehow this invariance, the inverse properties and also the part of the somehow the resistance. Resistance to relabeling and so forth. I'm still trying to reconcile these two aspects. And my understanding is somehow that relates to the topology. Topology are quite different. Guido, do you have any comment? Yeah. Okay, sure. Why not? Yeah, well, thank you. So yeah, no, I totally agree. So I think that, you know, in Chenzo's characterization of the Fisher metric, Fisher metric, I mean, this is considering a very specific type of embeddings and mappings with respect to which the requirement emerges or is being formulated for invariance. But this may be not the natural type of mappings to consider if we are using an optimal transportation metric, as you already mentioned. So, we're not going to have this permutation invariance and we don't want it. So, maybe one will want to consider a different type. Maybe one will want to consider a different type of natural embeddings. What they could be, that certainly should respect some of the topology of the sample space, I would presume. Maybe I can mention, so I think certainly these optimal transportation metrics, they are of interest also because by introducing this topology, maybe they can allow you to do research about polyti density. Quality densities or quality measures that are not an overlapping support. And this happens a lot in applications. So you can still have a discrimination between them and an objective function to train with in those cases. So that's maybe another motivation for considering those.