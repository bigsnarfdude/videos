15 is okay, so I'll make an effort to be at minus 5. So I'll talk about constraint reinforcement learning in a minute, but you know, like the previous talk reminded me that there is a connection between the techniques that are used here and some techniques that can be used in L0 recovery. And I thought that it was interesting to. To interesting to explain these ideas of L0 recovery before I talk about the reinforcement learning. You know, these things have to do with duality. I'm going to be talking about duality. And the results are going to look very different, but the mathematical techniques that we utilize in the demonstrations are actually quite similar. I thought that was interesting to make that connection. Plus, there's another thing that Brenette told me a while back, which is that he sent me an email with the title something like a signal processing is enough or something like that. Signal processing is enough, or something like that, and this is like a signal processing view of compressive sensing. Although, you know, some people would say that compressive sensing is signal processing, but this is signal processing in the sense that I'm going to be talking about signals in time and the sampling of signals in time. So this is the L0 recovery that we all know, right? So we're trying to minimize the zero norm subject to, you know, actually, that's a mistake. This is AX equals something. So you guys forgive me, I'm a little rusty on this. Guys, forgive me, I'm a little rusty on this. Yeah, this would be A equals B, right? In any case, it's not going to change what I want to say. Another, you know, the other problem, this problem, okay, that's the one that we would like to solve. That's the sparsity, right? But this one we can't solve because it's non-convex. Instead, we solve this other one. And what we know is that under some conditions, these two problems are equivalent, which is kind of, you know, at some level related to what Related to what I don't know your name, sorry, Joshua was saying as well. Okay, anyway, so you know what you also know is that if you look at the dual of both of these problems, these two duals are equivalent. Unfortunately, this problem has multiplety gap, right? So this and these are equivalent, but this problem has positive duality gap, so this problem and this problem are not equivalent. Therefore, you cannot conclude that these things are equivalent. However, there's something interesting that we can do, which is. Cover something interesting that we can do, which is we take this problem to the limit, right? So, that's that's signal processing, right? Signal processing is actually the other way. You go from the limit back into discrete representations, by which I mean that instead of having zero norms here, so vectors, you have the equivalent of zero and one norms for functions. So, you take the indicator function of epsilon being not zero or the absolute value. Now, in that case, this would be the sparse problem. This is a problem that we would be able to solve. To solve, you can again write down both dual. What is very interesting is that in the limit, you can prove that this one, which is the dual of the L0 minimization and the primal are equivalent, okay? Which means that these two are equivalent for the same reason that these two are equivalent, these two are equivalent because it can be proven, these two are equivalent because this one is convex, and therefore these two problems. And therefore, these two problems are equivalent, which is very interesting and very appealing, right? So, for the following reason, I mean, you have this problem when you take the limit, you think the problem becomes more difficult. Actually, the problem becomes easier. And that's kind of like the same thing that happens here in the reinforcement learning problems that I'm going to call. Okay, so that's one thing. Now, the other thing that is interesting is this. Okay, fine. I mean, the minima are equivalent, et cetera, right? But there's something that I am. Et cetera, right? But there's something that I am not talking about, which is that in reality, that's not what we want, right? We want to find the R minima of this. Okay? And that can create a little bit of a catch because, yeah, the minima of this function and the minima of this function can be the same, but they are minima, don't have to be the same. Okay, so it essentially could be that this problem has more minimizers than this other, one of which will be the sparse one, okay, but a lot of which Be the sparse one, okay, but a lot of which will not necessarily be the sparse, okay? So, that's a that's an issue that will appear here as well: that's the issue of primal recoverability. And in order to overcome that problem, you need to adhere like a quadratic regularization, which you know is what we know as the elastic net. In the case of the elastic net problem, you can show that the R min are equivalents. And therefore, if you consider the problem in the limit, well, you know, you don't need to resort to the Resort to the typical sparsity conditions, you know that the problems will be equivalent just because of their equivalent duality formulations. If you want, you know, if you think that this is interesting, I can explain it better later. But, you know, let me talk about constraint reinforcement learning, which is the work of these guys and what I wanted to do today. And a lot of you know what I do, so you know, I am a systems engineer. I am a systems engineer, and therefore, my life is devoted to investigating the systems engineering cycle, right? Which begins like this: you specify requirements, you say, you know, I want a system to do so and so, you acquire some data about your system, you make a model out of that, and you settle on some operational settings. Now, what is the reason that I do learning? Well, the reason that I personally do learning is because I think that there's a promise there, which is actually, I think, underappreciated. Actually, I think underappreciated by engineers, which is that you can go straight from data to set to operational settings, you know, straight just by using some machine learning techniques. Now, that promise, however, has its flaws, right, of which we are all aware. You know, these are examples drawn from the news. And, you know, there's a bunch of things wrong with learning. Sorry, there's a bunch of things that are right with learning. I mean, we can use it to design molecules. Learning. I mean, we can use it to design molecules, we can use it to, you know, famously Liverpool Football Club, of which I'm a fan, I use this to select their team. So, you know, it's pretty cool stuff that we can do with learning of systems, even soccer teams. However, you know, these things have flaws, right? I mean, they're biased, they're unsafe, you know, now we have generative models that are untruthful, etc. You know, it's not all rosy here. And I think that there. Rosie here. And I think that the reason this is not all rosy is actually pretty clear because if you look at existing machine learning, that existing machine learning has trouble incorporating requirements. So when we design systems, we don't say, I want to maximize accuracy. You say, I want to maximize accuracy while being fair, safe, truthful, et cetera. So that's what I mean. So typical learning is just, you know, we find solutions that fit the data, learning under requirements, which is, I think, the kind of problem that we should be. Problem that we should be striving to solve. At least in engineering settings, these are the problems that we should be striving to solve. Are problems in which we want to minimize losses, yes, while at the same time being safe, robust, fair, representative, right? And we don't want to discriminate people because they're not represented in the data set. We want our generation to be truthful, etc. And I think that it's not unfair to say that the incorporation of requirements is a major gap in the current practice of. In the current practice of AI. Perhaps a better reason to think about requirements is: you know, this is an example of a system engineer system. I'm not going to describe the problem exactly here, but we have a team of red agents moving around the city. We want these blue agents to establish a wireless communication network in support of the red team. We can do that using standard techniques, and that's actually something we did pre-pandemic. Did pre-pandemic, post-pandemic, although it has nothing to do with the pandemic. You know, we solved the same problem but utilizing learning techniques. Okay, so this is essentially the same video, although the policy here is different. I mean, the policy here is not a design policy, but a learned one. And the system works just as well. And if you look at this problem, this is not a problem that you can solve just by minimizing a loss. Okay, because what is going Okay, because what is going on in this particular problem is that I have a bunch of agents, each of which has an individual delay requirement, an individual energy budget, an individual rate requirement. I also have aggregate energy requirements. I also have aggregate mission specifications and so on. So when I specify the system, I don't say I want to minimize this loss. No, I say, here there are 128 different things that I want my system to do. I need to go find. Want my system to do, I need to go find that solution. Okay, and in order to do that, you need to develop the practice of constraint learning. And what I'm going to be talking about today is not constraint learning in general, but constraint reinforcement. Okay? Now, it's quoting Wahaga. So, what is constraint reinforcement learning? Right? Well, I mean, exactly what you think it should. Learn, right? Well, I mean, exactly what you think it should be, or what you expected it to be, is a problem in which we want to maximize a reward, subject to another reward. So these are like the long-term discounted rewards that are typical of reinforcement learning problems. So here we want to maximize, find a policy pi that maximizes the accumulation of the reward R0 while accumulating at least CI units of reward Ri, except that we discount that. Reward are i, except that we discount that and we end up with these value functions v0 and vi. Now, this is a problem that, oh, and by the way, just for notation, I'm going to write the constraints using a vector notation, okay? Now, this is a problem that is difficult for the same reasons that this problem is difficult, right? Because the value functions here are not convex. And depending on how much reinforcement learning you know, Learning, you know, you may think that this is a useless slide or not. In any case, I mean, there's a lot of people in reinforcement learning that think that the value functions are linear. I will explain why that misconception arises later on. But value functions are not linear. So value functions are non-convex functions of the policy. Here is an example MDP. And these are the value functions that are plotted. You can clearly see that these are not convex functions. But if you forget that problem for a minute, If you forget that problem for a minute, you can do the same thing that you do in sparsity, right? You forget about the fact that this problem is not convex, you take the dual, see what happens. So this is the Lagrangian dual, right? So you have here a linear combination of the objective plus the constraints, and you define the dual function as the maximum of the Lagrange. Okay? And the dual problem is the And the dual problem is the minimization of the minimization of the dual function. Now, these two things are not equal, okay? But, sorry, meaning working with the dual function is not the same as working with the primal function, at least so far, okay? But, you know, working with the dual function has a very clear advantage, okay, which is that operating in the dual domain is the exact same thing as operating in the primal domain. Okay, so the only thing that happens is that now Okay, so the only thing that happens is that now you have here a different reward that depends on your dual variable, but in any case, right, I mean, you know, maximizing the dual function, that's like a standard unconstrained reinforcement learning. Okay, and for that reason, you know, finding the dual optimum, which is kind of like the best approximant of the primal problem, is a good proxy that you can attempt to solve in lieu of the primal. Of the primal CRL probe. Of course, right? I mean, there's still the catch that dual maxima are strict upper bounds of primal maxima. So I don't know that this is necessarily a good idea. You know, I don't know that these two things are equivalent. Nevertheless, I mean, let's see whether this works or not. And it does. I am showing you here a variation of the problem that I described at the beginning. This is a problem that is slightly easier because there's no mobility, but slightly more difficult because it is of. But it's slightly more difficult because it is of higher dimensions. So, we want here to maximize the sum rate in the whole network, subject to individual minimum quality of service requirements and individual power batches. Okay, so mathematically, this is how you would write the problem in reinforcement learning language, but let me not explain that. I didn't make any mistake. That's the proper formulation of this optimization, of this systems design probe. And when you attempt to do this, And you know, when you attempt to do this, it works. Okay, so I'm showing here heuristics that we utilize for solving this resource allocation problem, and I am showing the performance here of a few different parametrizations. This is a parameterization that utilizes a fully connected neural network. This is a parameterization that utilizes a graph neural network. They are both equivalent, but this is in a network with a small number of agents. Here, there's a parameterization that utilizes a graph neural network. Is a graph neural network, it scales well to actually very large networks. And you know, the gains here are notable, right? I mean, the improvement in performance is not negligible. So I am going from two units or one and a half units of aggregate rate to three units of aggregate rate, right? So your phone is working twice as better if you utilize the learn policy. There's two baselines here that are. There's two baselines here that are just sanity checks. One of them is equal power, the other one is random selection. There's another one which is WMMSC, if you know what that is. But you know, WMMSC, that's actually the solution that is currently deployed in Wallace's system. Okay, so here there's another example. So this is safety in enforcement leveling. So we want you. Enforcement learning. So, we want to reach a target destination while avoiding collisions with a number of oscargos. And you want to do this with high probability, right? Because avoiding collisions, that's impossible. Actually, that's a very interesting statement. We can discuss it later on. But avoiding collisions, period, that problem is unsolvable. All you can hope is to avoid collisions with very high probability. There's two solutions here. One of them that utilizes constraint learning. The other one that utilizes reward shaping. So that's what people do for safe. That's what people do for safe. And you can see that there's a significant difference, particularly when you consider here the out values when you want to have safety in the level of four nines or five nines. So in those situations, the use of constraint learning really allows you to reach higher levels of safety. And the reason for that is because using reward shaping, it is very difficult to balance different obstacles. Using constraint reinforcement learning, you can balance different obstacles. Reinforcement learning, you can balance different obstacles more easily. Okay, anyway, so I showed you a couple of simulations saying that this idea of solving constraint reinforcement learnings in the dual domain works, but I also told you a few minutes ago that it shouldn't work, right? I mean, I told you it shouldn't work because there is a positive duality gap here. In the same sense that you should not expect L1 regularization to work, right? But you know, since it happens in the continuous problem, it is. Continuous problem, it is possible to show that these two problems are actually equivalent. They have equivalent dual formulation. So I will explain why that is, and there's a very interesting interplay with the representation in terms of the occupancy measure here that I will explain. Now, there's another issue that arises. That one has to do with the issue of recoverability, right? I mean, the difference between a mean and an arc mean that will require An argument that will require that I introduce something that we call state-augmented constraint enforcement learning. That's just a preview. This is difficult to explain. As a matter of fact, I have to disclose that I never succeeded to explain this properly, but let's see if today is my day. And finally, we are going to address one other problem that arises in constraint enforcement learning, which is that I am very wantonly specifying CI, right? I am saying I want the safety, I want my safety to be safe. I want my safety to be seven nines. I mean, is it really possible for me to achieve seven nines or not? In learning context, I think this is a very preoccupying problem because you're supposed to not know the environment. So you need to have a way of relaxing constraints. And this will bring us to this notion of resilient learning that I will introduce at the end of it. Okay, now that's enough of a preview. Let me start talking about a strong duality of constraint reinforcement learning. Duality of constraint enforcement learning. So I already told you what this result is, right? You know, the primal and dual are equivalent. Okay, so that means that there is some sort of hidden convexity in CRL. And for those of you that know reinforcement learning, you know that this is related to the occupancy measure reformulation. And the occupancy measure reformulation, one of those things that if you know it, you know it. If not, just needs to believe me. Okay. So it's. Me, okay, so it's uh, you know, it's what it is stated over there. I mean, what is relevant is this: there is a way of rewriting constraint reinforcement learning problems such that instead of being formulated as problems in which you search for an optimal policy, you search for an optimal steady state distribution. When you write them in that language or in that alternative parameterization, it so happens that the problem is linear with respect to the steady state distribution. Respect to the steady-state distribution. Okay, so that's why sometimes people think that value functions are linear, because they are linear with respect to cupancy measures. They are not linear with respect to policies, as the plots that I showed you earlier exemplify. Now, that means the following, right? So, I can write the problem in terms of policy variables. I can then write the problem in terms of occupancy measures, and I know that these two are the same. That these two are the same. But, oh, sorry, furthermore, I know that the problem formulated in terms of occupancy measure has no duality gap because, well, it's a linear program. However, primal equivalence and dual equivalence are not equivalent, okay? Which means that it's still possible that CRL with policy variables may still have a positive. You may still have a positive duality gap. What could happen is something like this, right? So, this is, you know, let's say, right, the problem in epigraph formulation written in terms of policy variables, okay? So, this is the epigraph formulation of the optimization problem. You know, this is the optimal primal. This is the optimal dual. When we rewrite the problems in terms of occupancy measures, the picture would look like this, right? So the problem written in epigraph form is convex. In epigraph form, it is convex, that's why it has a no-duality gap. It's actually a polytope because it's a linear program. And in that case, we know that the primal and the dual coincide, well, but you know, the occupancy measures primal and dual coincide doesn't mean that the policy variable primal and dual coincide. But of course, I mean, we know more here, which is that these two problems are actually, sorry, that the two epigraphs are actually equivalent, and that's the reason why. Actually, equivalent, and that's the reason why the problem written in terms of policy variables doesn't have any loyalty gap pi. Now, okay, you know, I'm drawing pictures here, so maybe you are wondering why I'm drawing these pictures instead of just telling you the result and moving on. Well, because I'm showing you these pictures because it is very important to realize that these two sets are convex, but they are convex in very different ways. Okay? This set is convex because Is convex because when you give me two occupancy measure variables, I can take the convex combination of the occupancy measure variables. Well, you can see it. Maybe we can move this. Jose? Can we move this? Oh, it's mine. Okay. There we go. There we go. Okay. I'm getting annoyed. Here, maybe. Here. You don't need to see pen. There we go. Okay. So, you know, when you, here I am taking the convex combinations of two policies, okay? And when I trace the convex combinations of policies, I trace the convex combinations of value functions. Okay. In the case, In the case of the set written in terms of occupancy measure variable, in terms of policy variables, when I look at a convex combination of policies, that doesn't give me a convex combination of value functions. I know that I can trace the convex combinations of value functions, but I can trace that with a policy P alpha that is not necessarily the convex combination of the policies pi one and pi two. Okay, and that will have important challenges when we talk. Important challenges when we talk about primary recovery. Okay, you know, just one other thing is this. In reality, we do not really solve this optimization problem, right? We introduce some learning parametrizations. These learning parametrizations are not convex, right, which is another source of challenges. But in this case, it's not difficult to show that if the learning parametrization is new universal, the Learning parameterization is new universal, the duality gap will be small. Okay, and you know, for those of you that have heard me talk before, this is a bound in which all of the right quantities appear. So, the difficulty here depends on the parameterization richness measured relative to the discount factor. And there's also a parameter here, which is the one norm of the optimal dagran multiplier, which is a way of characterizing how difficult it is to satisfy these constraints. Okay, so that's a constant that always appears in these constraint learning problems, and it has to do Constraint learning problems, and it has to do with the fact that the more difficult it is to satisfy the constraints, the more difficult it is to warranty statistical accuracy. Okay, I believe I've been talking for about 20 minutes, so maybe I should give you a summary here of what I have been saying. Okay, so what I've said actually is somewhat simple. I told you that constraint reinforcement learning problems are not convex when formulated in policy variables, okay? When formulated in policy variables, okay? And that is true even though they are linear when formulated in occupancy measures. The equivalence of the problems doesn't mean that the original problem is convex, which you guys are all smart, but some people do make this confusion. Nevertheless, they have nodality gap, and that's because the AP graph sets are convex, but they are convex in different ways, and that will be important later. And finally, the other result that I show you. And finally, the other result that I show you is that if we use a new universal learning parameterization, these CRL problems have duality gaps, which are of order new. Now, with those structural results, I can go and try to develop and analyze dual gradient descent. What is dual gradient descent? Well, you know, there's equations here that are going to look a little challenging, but they're actually quite simple. But they're actually quite simple. Since the duality gap is small, I can solve this in the parameterized dual domain, which means that instead of solving the constraint CRL problem, I'm going to solve the Lagrangian form of this problem, meaning I'm going to maximize with respect to policies, then I'm going to find the lambda that minimizes those maximum. Okay. Now, in order to do that, I propose the following algorithm for a given multiplier lambda. For a given multiplier lambda, we find the parameter that maximizes the corresponding Lagrangian. And this is what I told you earlier on, right? So that's a good thing to do because in order to find this theta dagger of lambda, you can use whatever policy gradient method or TRPO, whatever method you prefer for solving unconstrained reinforcement learning, you can use it to maximize this Lagrange. And you then proceed this torch and maximization with a gradient descent step in the dual domain. And the reason why I can do that is because constraint slags evaluated at Lagrangian maximizers. And by the way, you can follow the colors here if you can see them. The constraint slags evaluated at the Lagrangian maximizers are gradients of the R gradients of the dual function. And this is not a difficult thing to do because that's just a set of policy evaluations of unconstrained RL problems. We need to do one policy evaluation per constraint. And the dual function is convex. That's because dual functions are always convex. This dual gradient descent will approach lambda star. And for that reason, you should not be surprised to learn that the following theorem holds, which is that under a bunch of conditions and A bunch of conditions and properly chosen constants, which are typical of stochastic gradient descent algorithms, we can find the optimal solution of this CRL probe. There's one thing that I do want to highlight, because you saw me skipping this slide very quickly, is that in reality, you cannot actually minimize the Lagrangian, right? So you can approximately minimize this Lagrangian. Approximately minimize this Lagrangian. So maybe you wonder whether errors will accumulate, but no, you know, errors do not accumulate. So this row is the error that you make on each individual step of the Lagrangian minimization. Okay, you know, that's how these curves are constructed. So this is a deeper dive on some of the curves that I showed you earlier today. Earlier today, what I am showing you in this slide are three different things for the wireless communications problem. I'm showing you the mean rate, I'm showing you the minimum rate, and I am showing you the fifth percentile rate. And indeed, you see that all of these constraints are eventually satisfied. And actually, this constraint reinforcement learning formulation is doing exactly what you would like it to do. Is doing exactly what you would like it to do, which is that you're learning policies in which the disadvantaged customers, right? That's the customers with weaker connections, they get more of the resources of the net. Yes? So at each iteration has installed a new one. That's actually like a very good question. So I want to talk about that when I talk about a state augmenter. The question that Joshua. Huh? She did not? Oh, really? She did not? Oh, really? Did I? I guess there was a statement. Did I interrupt you that quickly? Yeah. It's okay. I'm a professor. I'm allowed to do that. No, Shoshua. Okay, okay, good, good. Joshua, you correct me if I'm wrong. So the question that Joshua is asking me here is the following, right? In this algorithm that I am giving to you, right, I am saying that for each lambda that you give me, I need to solve this constraint reinforcement learning problem. And then I am going to. Problem. And then I'm going to adapt this lambda, right? And therefore, the question that you're asking me is: okay, I mean, what's the catch here? Because, you know, like you're changing this lambda, like what, every 100 milliseconds, that's a reasonable assumption to make. That's actually the true time constant of wireless communication systems, which means that every 100 milliseconds, I need to solve this reinforcement learning problem, right? So this is a little untenable. What you want me to do is to actually construct some kind of offline-on-line reinforcement learning algorithm, and that's actually more difficult. And that's actually more difficult than it sounds, and it has to do with the part of the talk on state augmented constraint reinforcement law. Okay? Mine was very simply: doesn't this take a long time? Yeah, yeah, exactly. Yeah, yeah, yeah. No, no, but I'm saying more than that. I mean, I mean, like a I mean, okay, so you are kinder than I am. So what I, what the question that I formulated for you is that, yeah, this, as stated, doesn't work in practice. And I agree with that. Practice. And I agree with that. But you know, for the time being, if you forget about the fact that you don't really have the time to solve this constraint optimization problem, this thing works. But I do want you to observe something here that is quite interesting, which is that the behavior of this dual gradient descent method does not necessarily imply conversions to the primal variables. What happens in these dual gradient descent algorithms is that typically your dual variables have some Typically, your dual variables have some oscillations. That's what I am showing you here, which means that you have some kind of policy switching, which allows you to satisfy the constraints on average. But if you look at the worst constraint at any point in time, you are really not satisfying those constraints. Okay, so there's something here that is a little fishy. I'm going to try and explain it better in a minute. But before I go and work on this state-augmented construction, This state augmented constraint reinforcement learning. You know, let me just summarize again the things that I've been saying, right? I've been saying, okay, CRL is non-convex, but you can still solve it in the dual domain. And in order to solve it in the dual domain, you need to apply some kind of gradient descent. And this gradient descent algorithm is workable. Okay, so you can prove that it works, except for computational limitations. So in order to talk about state augmentation, let's To talk about state augmentation, let me change the problem a little bit. And instead of looking at the problem with discounts, let me look at the problem in its ergodic formulation. So I am taking the time average over a long period of time instead of taking this counter rewards. And I am doing that so that the results are cleaner to stay. And as I did before, I'm going to use vector nodes. As I did before, right? I'm going to use vector notation for the constraints. Okay, so this is the dual gradient descent algorithm that I would propose that we execute. You begin with your Lagrangian. That's a rewriting of something that I've written before. You then perform an unparameterized policy optimization. Okay, and incorporating parametrizations is straightforward in these results, but. Straightforward in these results, but I will not explain it today. And then I do rollout gradient descent, which means that I am going to update my Lagrangian multiplier by evaluating a small rollout of my policy. Okay, so this is a little difficult. So let me explain it again. Yeah, it's not difficult. I mean, it's difficult for me to explain, not for you to understand. We fix a particular multiplier, lambda k, for that particular k for that particular lambda k we find the optimal policy that maximizes the lagrangian okay so for that particular lambda k we find the lagrangian maximizing policy now that lagrangian maximizing policy we execute it for a while you know between you executed for t0 units you know we execute it for you know one second let's say you know think of a 100 millisecond clock for 10 time steps you execute the policy pi target of lambda You execute the policy py target of lambda 10. That will result in the accumulations of some rewards. Okay, so these are rewards that you get from the environment. So you can observe this. You then use this as a stochastic gradient of your value function, and you utilize that to update your dual value. Okay, so let's go again, right? So you fix a lambda k, that gives you a policy pi target of lambda k. That policy pi target of lambda k. That policy pi dagger of lambda k, you execute it for t0 units of time. You then update your Lagrangian multipliers. And if you know nothing about duality, by the way, this is very reasonable, right? I'm saying if I accumulated more reward than I wanted, I will decrease lambda. If I accumulated less reward than I wanted, I will decrease lambda. That gives me a different lambda k plus one, which will give me a different policy pi dagger of lambda k plus one, which I will execute 40 units of time, and I will keep updating. And I will keep updating my multiply. And if you do that, well, you know, you can prove that this works. Okay, so this algorithm will give you rewards that are almost surely feasible and that are within a small factor of being optimal. And there's small conditions here, but nothing that should concern you seriously. Now, this apparently solves the optimization problem. I mean, it solves the constraint reinforcement learning problem. Okay, because I am saying that the time average of the rewards of the sequence generated by the algorithm converges. And in that sense, this is a solution of the CRL problem. I mean, it is stronger, in fact, because. In fact, because the constraints are satisfied almost sure. But there is a catch here, which is the catch that Josh was asking me earlier on. I mean, in practice, this algorithm is not how we use reinforcement learning. In practice, we use reinforcement learning to train one policy. Once we learn the train policy, we execute it tomorrow, correct? And you can't And you can't do this here, guys, because I am saying you, well, I need to execute this pi target of lambda k, right? And what you want is to give you, okay, what is the optimal policy so that I can execute it? I'm using, I don't know that Schoch wanted to ask me this, but you know, I'm using him as a... I'm using Schosch as a point master that should drive this point, right? But that's the issue here that makes this problem mathematically very. That makes this problem mathematically very interesting to me, actually. And I hope it makes it interesting to you. So, what we want is an algorithm that finds pi star. Because if I tell you, okay, this is the optimal policy, then you just go ahead and you execute your optimal policy. I mean, problem solved. But I'm not telling you that. I'm giving you an algorithm that solves the problem, an algorithm that generates this optimal sequence. And when I execute this algorithm, Execute this algorithm, the behavior is actually very weird. Okay, so these are the constraints slags, these are the dual multipliers, and these are the Lagrangian maximizing policies. And I want you to look at what's going on with the policies, right? So here I have two customers or two users, the blue and the red. Sometimes the blue gets all of the resources. Actually, let's look at the highlighted ones. So here, the red customer is getting all of the resources. Here, the blue customer. The resources. Here, the blue customer is getting all of the resources, while simultaneously the blue and the red are getting none of the resources. And what is driving that is the oscillation of the dual variables, right? So here, the blue variable, the blue dual variable is high, and therefore we say, oh, let's give all the resources to user one. Here, the red dual variable is high, and then we say, oh, let's give all the resources to the red user. And that. And that oscillation of the dual variables is driven by an oscillation of these constraints, right? Because here the resources are given to customer one, therefore the slack for customer one is decreasing, therefore the dual variable for customer one or for the blue customer is decreasing. And it so happens that's what the theorem guarantees is that the switching of policies is happening at the right rate. Okay? And that by the And that, by the way, is something that happens on all linear problems, right? So, this is a consequence of the fact that to the extent that reinforcement learning, constraint reinforcement learning is equivalent to an optimization problem, it is equivalent to a linear constraint optimization problem. Okay, so just to summarize, right, there's no claim. There's no claim here on the optimal policies. I'm just telling you that I am generating policies that depend on the Lagrange multiplier that are samples of near-optimal policies. Which is why you're buying something. So, you're giving us something weaker. Yeah, yeah, yeah. I'm gonna try and, yeah, yes, very good, right? So, I'm giving you something weaker. And so far, what I'm giving you is this. Okay, so you see, you know, you go, you observe the current state of the system, you observe the current values of the multipliers, you solve this constraint reinforcement learning problem, you execute the solution, then you update Lambda, and you keep doing that. And assuming that you're able to solve this constraint reinforcement learning problem, this unconstrained reinforcement learning problem is very quick. Constraints, sorry, reinforcement learning problems very quickly, you can execute this algorithm. Okay, that's what I'm giving you so far, which is a rather poor thing to give you. Yeah, yeah. Well, you know, there's quite an obvious way of transforming this into a practical algorithm. You, you know, you I will show it to you like in a couple of minutes. I will show it to you like in a couple of minutes, but I'm very happy that you at least are seeing the issues here. It's not that I am giving you something better. It is that if I don't do this, I don't know how to find the optimal point. Okay? And this I is a we. I mean, like, you know, okay, so we can formulate the problem in terms of occupancy measure. Formulate the problem in terms of occupancy measures and solve it in terms of occupancy measures. But problems in terms of occupancy measures are very difficult to solve, right? I mean, it's not that you can utilize and roll in, et cetera. So if you go into the occupancy measure formulation and you solve it there, this problem, yeah, I know how to find pi star, okay? But if you want to operate directly in policy space, I don't know how to do this. Sorry, I don't know how to find the optimal policy period. And I would say we don't know how to find the optimal policy. Say, we don't know how to find the optimal policy, and this we includes existing literature. Hold on, and I should say here: except for a few papers that claim to find this optimal policy, but they make the mistake of assuming that the value function is linked. You keep oscillating, yeah. You never converge, yeah. Converge, yeah, yeah. And actually, you can, I mean, you can use decreasing step sizes, right? And if you utilize decreasing step sizes, you can't prove conversions to regularize the okay, very good. So, the René's suggestion is the following: right, so I can go like as I did here, I can use. Can go like as I did here, I can use the equivalent of an equivalent of an elastic net, right? So I can go equivalently here and add something to my objective, and then the objective is regularized, and then I kill these oscillations, and then, yeah, I can find the optimal police. Yeah, you can do that, but you know, regularizations are regularizations, right? They don't, they give you something close to the optimal, which is not necessarily the optimal. Yes. Maybe something else I should tell you, right? I mean, the regularization has to be. I mean, the regularization has to be in terms of the policy variable. So you have to say something like that the probability distribution pi satisfies some properties, which means that it's not very easy to see what is the balance between a regularization that would allow you to numerically solve the problem without distorting the optimal yield too much. Okay, now coming back to the line of reasoning, okay, so I have no claim on the optimal policy, I just have this sequence of values. I just have this sequence of this sequence of near optimal, this sequence of policies that samples from the optimal police. You know, I am realizing here that I lied to you a little bit. So I can memorize this set of sequences, this set of policies, right? And that will be the optimal policy. But you know, you're talking about memorizing a large number of policies. Large numbers of policies. Yeah, yeah, approximately. Yeah, if you memorize these policies and then you execute these policies randomly, yes, you will be, you know, you will be attaining, you will be finding the optimal policy. Okay? No. Oh, no, yeah, yeah. Sorry, which ones? These ones. These ones most likely will be unique. Yeah, but this. Unique. But this one is not unique. Okay. Now, this is maybe, you know, like what is in a lot of your minds. I mean, at any given time, I have these policies that are not optimal. I mean, they're combined actually. What you would like me to do, perhaps, is to do the following. I can get all of these different random policies. All of these different random policies, which are kind of like, you know, hovering around the optimal, right? And you will say, okay, I mean, go ahead and take the average of that. And maybe I could, you know, but that's an issue because the value function is not convex. Therefore, when I take this average of policies, I don't necessarily recover a policy that is feasible in the original constraint learning program. Okay? So that is, I cannot recover the optimum. I cannot recover the optimal policy from the sequence of Lagrangian maximizing policies except by memorizing the sequence of policies, right? I execute this for a while and I memorize the 100 last policies and I execute that. Yeah, this is possible. It will work. However, you know, it's not very. The algorithm is iterative, right? So it eventually will converge, right? And you will know that it converges because you are going to settle into this oscillatory behavior. You know, see this thing here is growing a little bit, you know, it's oscillating, but it's growing, right? At some point, it will stop growing and it will stabilize. At that point, you know that you have converted. Then you start memorizing the policies that you see. Policies that you see, and then at execution time, you just draw one of these policies randomly at each different point in time. But you will see there's a much easier thing to do, right, which is to augment the state. I mean, I'll explain that in a second. Okay, so anyway, so that's the algorithm, right? So at each epoch, we select a Lagrangian maximizing policy, then we execute that for a while, we accumulate the rewards, we update the multipliers. And what we know is that this algorithm, okay. is that this algorithm, okay, solves the CRL problem in the sense that it generates state action sequences that are almost surely feasible and within a small factor of being optimal in expectation. Okay? But I want to disclose here that this is not the statement that I would like to prove. I said we here, but at least I would like to prove something different, which is that the algorithm solves the CRL problem in the sense that it finds a policy that is feasible. Defines a policy that is feasible, right? I don't care for the algorithm. I care for the optimal policy. But, anyways, I don't know how to do that other than memorization. So, I settled for this, you know, a slightly weaker statement, but a statement that I can solve algorithm. Okay. Now, how do we find it? How do we find that optimal policy? Well, in order to find that optimal policy, that Lagrangian maximizing policy, I need to solve the unconstrained reinforcement learning problem with these rewards. That's an issue, right? But if you look at what that step is doing is changing the problem formulation yet again, because what I am saying is actually something very simple. I am saying is actually something very simple. What I want is to find a policy pi that solves the Lagrangian maximization. Thus, instead of solving the constraint reinforcement learning problem, I propose that we learn to maximize Lagrangians. And if we learn to maximize Lagrangians, this will work. How will this work? Well, you know, this is the standard MDP, right? Here we have a state. MDP, right? Here we have state st, action state t. Here we have the transition kernel that will take me to st plus one. What I want to do is to find this policy pi star that will allow me to observe s of t and compute optimal actions at. That's the problem that we don't know how to solve. The we here is, I don't know of an algorithm that will give you that, but it's not either working in occupancy measure space or memorizing. Memorizing the optimal policy of a primal dual method or utilizing regularizations as Rene proposed. However, I do know how to solve this problem. I do know how to find optimal Lagrangian maximizers. I mean, that's just a different optimization problem with a different reward, in which I have added this state. This state lambda k, I have augmented the state with the incorporation of the optimal dual variable. Meaning, my policy now does not only take into account into consideration the true state of the MDP, but another artificial lambda k that I designed, right, that I chose. Now that means, right, that when I execute this policy AT, well, I also need Well, I also need to update this state because ST gets naturally updated by the system, but lambda k, that's like an artificial counter that I am keeping track of. Okay, and you know, this is the algorithm that we called state augmented constraint reinforcement learning because what we have done here is we have, we just simply guarding the MDP with the states ST, and we have added Lagrange multipliers. Have added Lagrange multipliers to the state space. When we add these Lagrange multipliers to the state space, we change the problem, right? So now we need to solve the Lagrangian maximizers, and we also need to keep track online of this state update for the Lagrangian dual variables. Of course, right? I mean, I have still not. I have still not told you about the addition of a learning parametrization, but you know what is going to happen, right? I can now incorporate a parametrization that will solve the Lagrangian maximization over a particular parametrization. And then the policy update here or the optimal policy here is not the actual true optimal policy, but an optimal policy that comes from. That comes from this solution of the state-augmented constraint reinforcement learning program. And yeah, that works. Actually, the plots that I showed you are the same plots, right? I mean, the only thing that I was trying to explain in the last few minutes is how to transform this algorithm in which you need to solve an MDP online by an algorithm in which you can solve the MDP offline. Solve the MTP of line, and really all that I did was to say that I needed to incorporate the dual variable in the state space. You know, Laudim at minus 15 was a joke, so I have only 10 minutes left. So, you know, let me try and give you a summary of things so far, then I'll talk for a couple more minutes and be done. So, in this second part of the talk, In this second part of the talk, I was discussing the following facts. Fact number one is that dual gradient descent generates policies that are samples of near-optimal policy. But if you have any experience with dual algorithms, you would like to take the time average here. You would like to do a convex combination of these policies and that. Combinations of these policies, and that should give you bias target, but you can't do that here. I mean, and this has to do with the way in which there's no duality gap here. I mean, the epigraph is convex, but it's not convex in the standard way of linear combinations of variables give me linear combinations of value functions. That means, right, that DGD solves CRL in a peculiar way. I mean, it doesn't. In a peculiar way, I mean, it doesn't give you the optimal policy, but it generates a trajectory that is feasible and near optimal. And the implication of that, right, is that instead of learning to solve CRL, what you need to do is just do a minimal change in your goal. Instead of learning to solve CRL, you learn to maximize Lagrangians. And having done that, you can do that by solving an unconstrained MDP with modified rewards. And that's equivalent to augmenting. And you know, that's equivalent to augmenting the MDP state with dual variables. And the modification for the online operation is that you need to keep track of these dual variables. And René, this is the reason why I advocate this instead of regularization, because the regularization will give you a sub-optimal policy, but this will give you, sorry, a sub-optimal yield, but this will give you the optimal yield, because there's no regularization. Okay. Okay. I wanted to talk about resilience, but I don't really have the time, so I'm not going to bother you with that. I can stop here and take more questions if you have it. Yes. So when one uses in a station interior point, the idea is to begin with a physical try to remake this. One uses primal fuel methods. One always finds a solution that is physical. And you expect to emerge. And so from a perspective of learning with constraints, it just seems like It just seems like it's not what I want because whatever solution can be my if I stop at any point three, I will not discuss. So why would I like to bring this one? Okay, okay, so there's a few different questions in there. Yeah, no, no. There's a few different answers. There's a few different answers then. The first answer is this: interior point methods cannot really be applied here because finding the interior point is itself the challenge. And there's a bigger issue here, which is that we use stochastic methods for these optimization problems. And in interior point methods, the moment you go outside of the interior point, the algorithm breaks down. Breaks down, and it is all but impossible to guarantee that in an iterative algorithm, you will not eventually step outside of the interior point, which is the reason why you really have no option but to work with a primal dual method. There could be other things here, right? But for a first-order approximation, a primal dual method is what you need to do because these primal dual methods, as we correctly said, they typically converge from the outside. And that's not an issue because. And that's not an issue because eventually they approach the inside of the feasibles. In constrained learning and constrained reinforcement learning, there are conditions that will guarantee asymptotic convergence of primal dual, okay? And these require the equivalent of strong convexity assumptions, which Which you do not always have, that depends on the loss function, or it depends on the value function. You can always have regularization. If you add regularization, then you can show conversions through the optimal policy or the optimal classifier in the case of supervised learning. Does that make sense as a statement? In which case, yeah, you find the solution that you want. Solution that you want, but as it happens, I like this idea of state augmentation. Okay, so let me try and tell you why. I understood the principles better, but I didn't understand what way is the difference of different so is it more easy to do? No. Yeah. Yeah, it's it's it's a very good point. I mean that that this is the point that I that that that I that I that I always struggle to explain. Like the point is this. I mean mathematically there's no difference. This is the same algorithm. Okay. What happens is this, that if I have the equivalent of strong convexity assumptions, I can tell you, look, Arbene, execute this algorithm asymptotically, this is the optimal policy and we are done. I mean, we don't worry about it again. Okay? Okay, in problems in which you do not have the equivalent of a strong convexity, in those problems, it is impossible to prove convergence of primal dual methods to the optimal value. Okay? Now, there is, however, a very simple solution to that challenge, which is to say, look, instead of learning to To solve the original constraint optimization problem, you can learn to maximize the Lagrangians. Then, you know, at execution time, you keep track of the Lagrangians. I mean, that's not difficult to do. And you use the Lagrangians to drive the switching between policies. Okay. And the behavior that you end up obtaining is something like this. Okay, you know, like when the multiplier is one for me and zero for you, just to use a cartoon example. To use a cartoon example, I get the resources, you get nothing. But then your multiplier grows and mine goes down. And then in the next iteration, you get the resources and I get nothing. Okay? So, you know, in the end, you end up obtaining the behavior that you want, even though you don't find the optimal point. Yeah. Maybe it's all your type. So, type star, you said it's not right. So, let's say it is not. Is there another sense that you could add to this that would make it neat? So, for example, maybe you want to emice people that can be So let me let me come back to this picture here because there's some the optimal policy doesn't have to be unique, okay? But there's a problem that is more challenging here, which is that the Here, which is that the Lagrangian minimizer policy or the Lagrangian maximizing policy doesn't need to be unique. Okay, so what happens in optimization algorithms, since I have this written here, let me call this X star, right? And then the Lagrange maximizes here, let me call them X of Lambda star. What is known is this that X star, which is a set, it could be unique, but in general it's a set, is a subset of the set of Lagrangian minimizes. That's what's happening in here, right? I mean, you have many more Lagrangian minimizers, maximizers, sorry, than you have. That you have optimal values. And that's what is preventing the algorithm from converging, okay? Because essentially, asymptotically, you're operating in the dual domain, right? So you're sampling around all of these points, none of which are optimal, you know? So you cannot find this optimal policy. You converge to this set. And that's as it should be because you're operating in the dual domain. So you're converging to this set. In the dual domain, so you're converging to this set, you're not converging to this other set. Okay, when you have convexity, you can take the average of all of these points, and that average will be the optimal policy. Here, you cannot take that average, that's the challenge. Whenever you have non-convexity, you cannot take the average. Okay, so now your question was. Get away that build up something so some stuff that converges towards the star, right? Yeah, okay. I think that okay, I'll understand what they got, yeah, yeah, correct. So, you know, I you know, I'm going to begin. I'm going to begin by being defensive because, at the end of the day, I'm a petty academician, right? So, yeah, this works, right? So, I don't know why you want to find five star. Yeah, whatever. That's not the algorithm. This algorithm works, right? So, you're paying a minimal price, which is now you need to, okay, so this optimization problem is slightly more difficult to solve. Okay, that's true. Solve. Okay, that's true. You need to keep track of this land. That's also true. But you're not paying anything very damning here. But on the other hand, yeah, I'm also an honest intellectual, and I think that that's an interesting question, right? I mean, how do you actually modify your algorithm so that instead of converging here, you converge here? The easy solution is to regularize the problem by adding a strongly constant. adding a strongly con a strongly convex function to the objective um which you know yeah you you may or you may or or or or or may not like that uh if you don't like that i mean there are ways of modifying primal dual problems that will take you to converge to x star right which is you know i i i i i haven't uh looked at those yet i mean but you know you can consider like adaptive regularizations Consider like adaptive regularizations. You can see whether augmented Lagrangian methods will converge here. There are things that you can do to bias your algorithm towards X star instead of X of Lambda. But those, as far as I know, haven't been developed. I mean, I have a paper on CRL convergence, which uses regularization because that's the easy solution, but there are better. The easy solution, but there are better things to do.