So, overduce Manta. Okay, fine. So, as you see, I'll talk about occurrences of patterns in random permutations. And now I'm lucky being towards the end of the week because you have seen occurrences of patterns, so on defined credit. See, but let me give you a definition. Can I? you a definition can i can i somehow get rid of this well i guess you also see some menu on the top here which i can't get rid of but uh uh we're not seeing your menu i think we're just seeing the slides okay good good uh then it's just me that uh continue the slide Continue slide, but I see most of it, so that's fine. Yes, so here's just definitions which I hope you know already. What a pattern is, occurrence of a pattern is as you've heard today. Yes, it's a substring. And I have an example in the middle. You can look at that if you don't know it. And the simplest example is the number of inversions, which we have heard several times, for example, in Matilda's talk. And yesterday. And yesterday it figured a lot in Peter's talk, also. And the permutation avoids a pattern if there's no occurrence of that pattern. So, leave that. The general problem I'm going to talk about is this. We have a random permutation, which I draw uniformly from all permutations with some given length in some given class of permutations. Of course, you could have non-uniform form random permutations. Form random permutations too, as you've heard before, but I'll only talk about uniform ones for some class. And then we have some fixed small permutation and counter number of occurrences of that pattern. And I want to study that and in particular find its asymptotic distribution. So typical problem here, or not typical, but a simple problem of this type is to find the asymptotic distribution of number of inversions in a random permutation for micro. In a random permutation for my class. And now, since we've heard a lot about permutant limits in last talks today and yesterday or so, I would point out that first-order properties of this random variable are closely connected with permutant limits. Meanwhile, one way to characterize permit limits is to say that if you, well, I talk about number of occurrences, obviously you can scale it and talk about pattern densities instead. Talk about pattern densities instead, and if pattern densities convert, that's the same as saying permit limits. You have a permit limit. But today, I will not discuss this. So, the interesting cases when you have random permit limits and therefore get random first-order properties of these numbers. But I will talk about some other cases. And in the cases I'll talk about, the number of occurrences is always concentrated. And I've studied second-order properties, and more precisely, I will only. Properties and more precisely, I will only talk about cases where I have asymptotically normal fluctuations. And in these examples, I talk about the permutant limits are trivial and what it means trivial, you can figure out yourself, but they are not very interesting in any case. So the other properties are interesting here. And also, this includes the case when the density of certain pattern goes to zero, but then you can normalize it with something else and get Something else and get interesting limits, which you don't see if you look at just permit limits. So it's related permit limits, but it's a different thing. I said a class of permutations, and by that I was deliberately vague, could mean more as anything, but very often I mean a pattern class, and that means the class of all permutations that avoid one or several given patterns. And that's a big area in its own. Here I list Area in its own. Here I lit some mostly old references just to show that this is an old area. There are also lots of recent papers, including some important ones by people here in Bannf, at least virtual in Bann, which I did not list here. And in Bona's book, more about this problem. And of course, people have studied a lot of different properties. Studied a lot of different properties of random permutations from pattern classes. So, here you have some listed just some properties that people have studied. Consecutive patterns, they were mentioned in one of the talks this week. Descents, major index, number of fixed points, positions of fixed points, exceedances, longest increasing sub-sequence, shape and distribution of individual values, and I forgot to say just the number. Well, that's not for a property of a random one, but it's related. Property of a random one, but it's related to a number of such permutations. But I will not talk about that today. So I will just, as I said, talk about this number of occurrences of some pattern. And many permutation classes have been treated both by myself and by others. And when I started this project, I hope that it will be possible to find some general results, at least that the particular results I found for special classes could. Special classes could somehow suggest some general results or conjectures, but they didn't. And then I was convinced by Igor Pach that it's not really possible to get any general results for all pattern classes. And if you want to know why, then you should ask Igor directly after this talk or maybe after the next one and see if you two get convinced. Now. Sometimes, when I give a talk, I try to impress the audience by presenting some result where I found some very complicated proof. Of course, that is a bit double-edged because if you have found a complicated proof, it may be that it's because you're a genius and have found the only way to solve a really hard problem, but it may also be because you've overlooked some simple ways of doing it. And in any case, since the audience And in any case, since the audience never sees the details of the proof, they really can't tell if the proof was complicated or not. Today I will not do that. I have done some cases, some permutation classes where my proofs have been more complicated, but I will not talk about them. And one reason is that there are also other cases done by other people, again, including I think some present today, that have been even more complicated than what I've done. But today I've But today I've chosen to talk about a simple method that solves some cases, at least a method that I regard as simple, because sometimes I feel lazy and it's nice to do things in a simple way. Now, of course, as mathematicians, we don't want things to be too simple, so they look trivial, but I'm saved here because it turns out that there were some minor obstacles, some twists that I met, so I some challenges which were not too difficult, but still some work. We were not too difficult, but still, some work had to be done. This reminds me that a long time ago, but maybe 10 or years or so, 20 years after I got my PhD, my former advisor, Leonard Karlasov, who had done really hard things in his career, he said to me that he thought it was time for me to start working on some hard problems. But I haven't got around to that yet. Got around to that yet. Maybe there's still time for it. Okay, so let me explain what my method use statistics is. So I start with case of random permutations without restrictions. So I just take a random, uniformly random permutation among all permutations of length n. That's my pi sub n. And then I have a fixed small permutation sigma. sigma, like counting number of number of occurrences of that, like number of inverses, for example. And here's the theorem. You subtract a suitable constant times n to the k and divide by n to k minus a half and it converges to some normal distribution with some variance, which I will not write down explicitly. What you should notice is that there's some power of n here, which is at least as in part we. Which is the at least as in part to mean n to the k in this case, where k is the length of the small permutation. And then if you divide by that, you see that the well, not variance, the standard deviation is of the order one over square root n of that. That will be a feature in all examples. You'll see different powers of n, but the standard deviation is always smaller by a factor of one of the square root of n, as you typically get when you sum up things which are independent or. Things which are independent, or more or less. Okay, and this was already approved by Bona, but I have a different proof now paper with Zeldberg and Nakamura. And the idea here is that we all know we can generate a random permutation by taking IID. So, I mean, I'm a probabilist here, maybe not all of you are. So, IID means independent and identically distributed. And these independent. And these independent identically distributed random variables are in this case just uniform on 0, 1. And we order them, take the ranks, and that generates the permutation. So now I start with these random variables and count number of occurrences. Well, to do that, I sum over all possible subsets of my index set 1 up to n. This is my I1 up to IM, M element subset. And to check if that is an occurrence, well, that is now. Well, that is now depends on just the order of these variables xi1 up to x sub i m. So I can write it as some function f of these variables. It's an indicator function. You can write it down yourself if you want. And such sums are called use statistics. And they were treated by Hefting in 1948. And he proved the general asymptotic central limit theorem. So I wrote down the case of inversions more explicitly if you. Inversions more explicitly if you don't, if it's too messy, the formula above. So, general definition of a use statistic is a sum of the type you just saw. We have some sequence of random variables, x1 up to xn, and usually it's an IID sequence, and we have some function f of m variables, and we apply that function f to all subsets of m of my variables. Subsets of m of my variables and sum that and get my sum. And these xi's don't have to be real numbers as they were in the preceding case. They could be vectors or random permutations themselves or whatever you like. Now there's one technical thing here, which at least I find important, is that the original definition, this function f was supposed to be symmetric. And note that in my sum here, I only sum over increasing sequence. Here I only sum over increasing sequences I1 up to IM. But if f is symmetric, you can just as well sum over all distinct I1 up to IM. Or conversely, if you take norm to F and sum over all distinct I1 up to IM, you can just as well symmetrize F and assume it's symmetric. So that's the traditional setting, and I think that's natural one in statistical, many statistical applications of use statistics. I mean, that's where I think come from. But in combinatorial applications that I've seen, I Applications that I've seen, I always need this asymmetric version where so that's a say minor extension of the RIDLET setting. Actually, in the simple case like this, you can reduce it to symmetric one. So this is a technical side, but since it is a trick using a random permutation, and this workshop is supposed to be about random permutations, I thought I'd like to show it to you. So suppose you have a use statistic as I just showed you. Statistic, as I just showed you, with f is not symmetric. And now you introduce a new sequence of random variables, y1 up to y, which are again uniform on 0, 1 and independent of each other and everything else. And you look at the pairs, x, y, i, y, and call them z, y. And now you take a new function, capital F, of M of these Z's, Z1 up to Zm. You define that by taking some overall permanent. Some overall permutations of M elements. You take your M X variables and you take a permutation of them and apply your function f and you multiply by the indicator that the y's happen to be in decreasing order if you take your same permutation. And you just said some that are permutations. So if you think about that, that means that you really permute these variables so that the y variables come in increasing order and you apply. Increasing order, and you apply little f, lowercase f to that sequence of your variables. But we know since y is independent of x's, this just means that you pick out a random permutation of your variables. And you can do that with the whole sequence of n variables. You pick out a random permutation of them and apply your u statistic to that. But of course, that has the same distribution. So, therefore, you get the same distribution for now with this new. Now, with this new symmetric statistic of the old one, and you can just apply Herfding's theorem without any further problems. Now, this trick doesn't work in some of the modifications I tell later. There, you really have to redo things, but again, that's no real problem. So, Hefting's theorem is that in general, if you have used statistics, as long as you have a second moment finite, you have convergence, and you can see you have well-mean is mean is more or less trivial compute, and the variance. More or less trivial compute, and the variance is just that for you have a standard deviation which is an order of one was quote of n of the mean, and you have convergence to something, some normal variable. And I omit the explicit formula for variance, which is a little bit complicated, but you can compute it when you need it. But there's an important thing, this variance may be zero. So, in that case, this theorem really doesn't say it's normal, it says it converts. It doesn't say it's normal, it says it converts to zero. It means you have normalized by the wrong power of n, and you can normalize it differently, and then you will get non-normal limits. And they may be more or less complicated. Such cases typically do not occur in separations unless you make them occur by taking linear combinations. So, you can take linear combinations of different pattern counts. So, here is a simple example where you take all the different pattern counts with patterns. Different pattern counts with patterns of length three with the right signs, which means that you really just count if you put take three elements in your permutation, it's in a cyclic order of them if it's one way or the other. And by symmetry reasons, this will now have a non-normal limit of this gamma will be zero. So this is a degenerate case. And actually, it turns out that in this case, with if you look at all patterns of length k, when you have k factorial once, the sum is deterministic. Is deterministic, so we say non-trivial linear space of dimension k factorial minus one, but the space of norm limits has only dimension k minus one squared, so there is a large space of degenerate linear combinations. And there's more algebraic investigation of this in paper by Ivan Zuar. We can look at so I want to just say how Hefting did this. His proof is based on the projection method. We have these M variables. I mean M variables, I mean, function f should be lowercase f is a function is a function of m variables, but you keep one of them fixed, condition on that, and average over all the others. That give you a function of one variable. And now you approximate your function by just taking these one-dimensional projections and sum all of them. And that means that you have now reduced your sum. If you substitute this in your big sum, you end up with sum of functions. Sum, you end up with some of functions of just a single of these x i's in the symmetric case. It's just an ordinary sum of them, and you get some weights in the non-symmetric case. But Hefting did the symmetric case. So you just reduce this to standard central limit theorem. And well, there was an approximation, but Hefting proved that the error in this approximation has small variance and can be ignored. And this proof also shows that the general case where the variance is zero. Case where the variance is zero is precisely when all these projections fi are zero identically. So now let's talk about the first variation, what's called a vincular pattern. You don't really know why it's called vincular pattern, but that seems to be used in several papers. So that's a pattern where you mark some entries, and that means that they have to be adjacent to the next one. As you see, this way I use here a start to mark the Use here a star to mark the winkler places. Two star one three means you have triples i plus one and j, and they now should be in this order two one three. So as I've written it, pi i plus one is the smallest one and so on. So in particular, the extreme thing is that you put the mark between any of your any of your entries, so that means that you just count the substrings, the consecutive patterns we've heard about. Patterns we've heard about. And Borna once proved an asymptotic normal limit theorem for that. And you could also make it more general and say that you can have gaps of length of most D or exactly D, but let's stick to this adjacent one. There's no real more difficulty and make it more general. So this marks in the Wenkler pattern, they group the entries in blocks. So the blocks are tied together and then different blocks are free from each other. From each other. And Hoffer proved that if you take any Winkler pattern, you still have asymptotic normal limit. And Twilter proved that this variance is strictly greater than zero, except in trivial case here. So, okay, so she did it by different methods, but then I thought, well, let's use use statistics again. And then the problem is that this isn't really a use statistic, but you can make it into one. So if you take So, if you take you take again your generate your random permutation from IID random variables XI as before, but now you sum only over with some restrictions here. So, what I do, let's take this example to simplify notation. So, here these blocks have length most two. Let me introduce a new variable yi, which is just a pair, a random vector. It's an x, y, and the next one. It's an xy and the next one xi plus one and then you see that this indicator that i i plus one and j give me a copy of my sigma in this case can be written as some indicator of my vectors i y and i j y i and y j as I written here. So I can write it in this way, which is almost a use statistic based on U statistic based on this sequence y sub i. Not really because this j has to be greater than i plus one and not just greater than i, but the terms with j equal to i plus one, there are so few you can easily see that's a negligible error. So we can add them and get a use statistic. So that gives me a use statistic. That's fine. But this sequence y sub i is not iid. Of course, each one is heavily dependent on the preceding. Heavily dependent on the preceding one and next one. But that's no problem. Because this sequence is what we probably call one dependent. They are dependent on the adjacent one, but not on any elements further away. And that's enough for central limit theorem, which is known for a long time. And Hefting's proof can be modified. And And so, cover that case too. I mean, in general, you can have M dependence instead of one dependence, it works the same way. Okay, so I went through, check the details that you really can modify this, and you get the result. Now, in this case, you can have new ways to get degenerate patterns. Here, I have an example where these just consecutive patterns, four of them, and take linear combination. And take a linear combination. Because if you see what I've done here, if you looked at it, you realize that this linear combination, it just counts the number of places we have an ascent followed by a descent minus the number of places we have a descent followed by an ascent. So obviously, the sum of these have to be either zero, plus, minus one. Zero plus minus one. So it's very degenerate. And now it's not completely explored what kind of degeneracies you can get here, but at least I found a condition for non-degeneracy. So I can show, as in stated the theorem above, that if you just take a single Winkler pattern, you have a non-degenerate case with positive asymptotic variance and asymptotic normality. So let me now proceed to other permutation. Proceed to other permutation classes where, as I said, I only treat some cases where one can use this method. So, the trick to use the method is now to find an encoding of the permutation in your class so that the number of occurrences of a given pattern can be written as a use statistic. And of course, that's not possible in general, but it's possible in several cases. So, here we have a So we have a notation now I use sometimes. So my S sub n, as I remember, that was the set of all permutation length n. And with these arguments to one to k, I mean the set of permutations that avoid these specific ones. So they don't have those patterns at all. So here's the first one. We avoid 1, 3, 2, and 3, 1, 2. And 312. And if pi n is random, again, always uniformly random in that class, then we have a theorem similar to what we had before. Now we count a pattern of some sigma, which itself avoids this. Of course, if the big permutation avoids some patterns, then any pattern in it has to avoid the same one. So we should only care about pattern sigmas that avoid one. Pattern sigmas that avoid 132 and 312 in this case. And if it has length m, we get power n to the m and so on. And this gamma is positive. I forgot to say that, but it is. And the proof here is based on a characterization structure theorem for this permutation. Permutation belongs to this class if and only if every entry is either a maximum or a minimum. Minimum. Left to right maximum and left to right minimum, I should say, you know, to be specific. And I have an old reference for that, 95. So that means we can encode our permutation by sequence, which says whether each new element is a maximum or minimum, code by plus or minus one. And note that, well, the first one, of course, is not either, or maybe rather, it's both. So it's just a sequence of n minus one elements, psi two up to. Elements ψ 2 up to xi n and this is this gives me a bijection with these 2 to the n minus 1 sequences, and therefore the code for uniformly random one is a uniformly random such sequence. So it means that the n, this psi 2 up to xi n, they're all independent identically distributed, just symmetric variables with plus minus one, probability half each. Okay, and now we take Okay, and now we take a pattern sigma. So that has a similar code, eta2 up to eta n. And then it's easy to see that we get an occurrence of sigma in pi if and only if the corresponding code symbols match. So this psi i, j is eta j. So now we can write the number of curves as a U statistic with some oral subsets of m elements, i1 up to im, of a suitable indicator, which I didn't spell out. Which I didn't spell out here, but you can write it down yourselves. Well, I did spell it out. Sorry, in the next slide. And here we have a use statistic: we have a function, we have a sequence, xi, so, which are IID, and it follows from Hefting's theorem directly. So, for number of inversions, I actually calculated the variance and found it to be 112. Now, for next. Next example, I will code them in another way. So here we have another well-known thing about taking direct sample permutations. It looks nice if you draw a picture, which I neglected to do here, but it means you first take sigma and then you put two after it and just relabel the entries and right way. I wrote it down formally, if you care to read it. And I say that the permutation is decomposable. Is decomposable, maybe I should say some decomposable, but that's the only thing I'll talk about today. If it is a sum of two permutations and decomposable otherwise, then any permutation has unique decomposition into indecomposable permutations, which I call blocks. So now let's look at a case of 231312 voiding permutations. Well, here I have a similar Well, here I have a similar theorem, but now my if I count occurrences of some sigma of this type, then the power of n I get for expectation is n to the number of blocks in sigma. And again, the variance is strictly positive. I forgot to write that. So for the number of inversions, again, I computed the variance, found it to be six. Aha, and how do we code? Aha, and how do we code here? Well, again, we have an old structure theorem. In this case, permutation belongs to this class if and only if every block is decreasing. So that means I can have arbitrary block lengths. So if I have a permutation and given block lengths, then I just know precisely what each block looks like. So there is a so the permutation is determined by the sequence of block lengths and they can be arbitrary. Because of block lengths, and they can be arbitrary. And if I know the block lengths of pi n and know the block lengths of sigma, well, let's say each block, if you have an occurrence of sigma, each block in sigma has to lie in one of the blocks in pi of n. And the way number ways to choose that is just a binomial coefficient. Then you take the product of that for different blocks in sigma. And then you sum over all possible choices of the blocks in pi that you use, and you get. That you use, and you get this formula in red here, which now looks like a use that takes the except so far is nothing random in it. But now let's see what happens if pi n is random. So, if the block lengths are ln up to ln, we know the sum of them. And as I said, each possible sequence with the same sum corresponds to a permutation. They have the same probability. So, it's what we call a uniformly random composition of it. Form the random composition of it. And now we have to use some probability and simple trick here. I take a sequence, Kappa L1, Kapital L2, and so of random variables, which are independent, and they each have a geometric distribution, parameter half. Because by then, by definition of geometric distribution, you see that probability that L1 is some number of k. It that L1 is some number, k is just 2 to minus k. And that means if you take m of them, L1 up to Lm, and probability that they are equal to some given numbers that sum up to n is just 2 to minus n. Same for any such sequence. And if you look at a little bit more closely, you see that if you let n, be the first case so that the sum of them is at least by n. Is at least by n and take the sequence of the up to this Kaplan N. Well, then the sum may be a little bit more than n, but if you just truncate the last ones to make the sum equal to n, then you will have precisely uniform distribution over all these compositions. So that's the problem. So that will be the distribution of the block lengths of permutation. So I can now construct my random permutation in this way. And so there's So, there is a small error from the last block, which we can easily estimate. So, number of occurrences is essentially: again, take these random numbers, random geometric numbers, capital L, and you take your binomial coefficients and product, as you see here, and we sum them over all overall B tuples now for the B blocks in my pattern sigma up to my calculator. Up to my capital N sub n. So again, this is a new statistic based on my now new IID sequence L sub I. But here the problem is that the sum is up to a random cutoff and not to my fixed M. Okay, well again, that's no problem because this random variable, my capital n. Counting how many elements you have to take until you reach some threshold. That is the basis of renewal theory, which studies this and related things. And if you use renewal theory, you can easily adapt Hefting's proof and include this case too. So, okay, with some twist, we can still use use statistics here. Let's go. Let's go to the next example. Here we have, well, 231 and 312 that were the one we avoided last time, but we also want to avoid 321, which means you cannot have any block of length three or more, because then you have a 321 pattern. Okay, well, but the theorem looks precisely the same way. You have, of course, a new constant, but otherwise, it looks the same. And I did compute this for a number of inversions, and you can see the exact numbers with some square root of fives appearing at the bottom. Okay, well, so here, as I at least said or more or less hinted, each block has to be decreasing but of length at most two. So it just tagged one or two one. So again, permutation is done. So again, permutation is determined by sigma block lengths, and they can be now arbitrary as long as they are only one or two and they have the right sum. So now instead of the geometric random variables before, I want some random variables which just take values one or two. And the way to do it is, as I've written in red here, you should find number p so that I let the probability of x being one is p and probability of x being two is p squared and the sum of p. 2 is p squared, and the sum of p and p squared is 1, and that is just given by the golden ratio. So you define these random variables xi, and then you do precisely as in the previous example, you take these as equals of block lengths, and again you continue to some random number when the sum is at least n, n you want. And if you condition on this sum being exactly n when you stop, so you have no Exactly when you stop, so you have no overshoot at the end. Then again, it's easy to see that the sequence of block lengths you get is precisely a sequence of block lengths of a uniformly random permutation in this class. And that means we have a, we can use that and find our use that write down our use statistics precise as before. So this is almost as in the preceding case. Again, we have used statistics, but we stop at this. At this stopping time when we reach a level, but now we also condition on reaching this level exactly. Well, yeah, that's no problem because that type of condition is also something studied in renewal theory. What happens with overshoot and if the overshoot is zero and so on. So using some more renewal theory, one can adapt have things proved to this case too. Okay, now let's come to the last thing. This is something I did last week or so, and I haven't, so I'm glad to be able to include some new results here, but it's not written until maybe next week. So, first definition here. If you have a permutation, you can always define this permutation graph. Permutation graph. That's a graph with the same vertex set one up to n, and you draw an edge ij for each inversion. By now, you should all know what an inversion is. And Asan and Hichenko, a few years ago, they wrote a paper where they defined permutation to be a tree permutation if this permutation graph is a tree and a forest permutation if this permutation graph is a forest. Graph is a forest. And it turns out, as they showed, that the class of forest permutation is precisely the class of all permutations that avoid 321 and 3412. So that is again an example of a pattern class type I've been talking about before. But the tree permutations are not a pattern class for the simple reason that a subgraph of a tree is in general. A subgraph of a tree is in general not a tree, it's a forest, but in general not connected, and therefore, a subpermutation of a tree permutation will in general be a forest permutation, but not a tree one. But I mean, nothing I've said so far says that I have to look only at classes which are closed undertaking patterns, or pattern classes. It's just that it's the ones I've studied been able to study. But here's another example. And now Other example. And now, one important thing they showed here is that a permutation is a forest permutation if and only if every block is a tree permutation. Okay, so now I want to imitate what I did in the preceding examples, where I build up permutation from these blocks by taking random blocks with a suitable distribution. And that's precisely what I did in the last. What I did in the last two cases there, a block was just determined by its length. So I only had to find a random variable, an integer-valued random variable, describing the length of a block in a simple way, the geometric ones. So this one with a golden ratio. Here, that's not true. There are several tree permutations of the same length. So I'll define a random tree permutation. So this is now, unlike all my other random permutations today, this is a random permutation. Permutations today. This is a random permutation with a random length. And my definition is just I look at a set of all three permutations and I define the probability of a specific one, they say sigma, to be p to the length of sigma for a number p, which I choose such that these probabilities sum up to one. So I get an honest probability distribution. And it turns out that we have a simple. And it turns out that we have a simple generating function, and p is actually this 3 minus square root of 5 over 2. And now I take again an sequence of independent, an infinite sequence of independent copies of my random tree permutation, these to one, to two, and so on. And I do precisely as before. And I do precisely as before. I take look at the total length of the k first one of them and I continue until this total length is at least n. And then I condition on hitting n exactly. This sum is precisely n. Because then it's again easy to see because of the way I constructed this tool that the sum of my tau 1 up to to n of n. To n of n. That is now by definition a forest permutation of length n, and it has the same probability of being any one of them. So I get my uniformly distributed force permutation of a given length. So this is really precisely as a previous example. So if I now have a pattern. So, if I now have a pattern which is another forest permutation, I decompose it into tree permutations. And then I say up to small error, we have again the number of occurrences. I do that by match each of the pattern, each of the blocks now in my pattern into one of the blocks in my big permutations, and that's one of the tor sub i's, let's say tor sub i sub j. And I have some number of occurrences. Have some number of occurrences there. Well, I don't really know what it is. Well, maybe I do, but it's rather messy to write it down. But I don't care, it's just some function of torsabi. And I take product over all my blocks j, and that gives me some function of these tors. And that's all I need to say that this is a U statistic, because I sum over all possible selection of I blocks. I said that this was up to small error, because here I really assumed that I get Here I really assume that I get my pattern sigma by taking my blocks sigma one to sigma b and putting each one of them into a different block of tau, of pi, sorry. And they have to go into separate into blocks, but it's possible that you can fit two of them into the same block, but it's easy to see that they such cases are so few, so they don't matter asymptotically. So here I've used statistics which So, here I've used statistics, which now is based on my IID sequence of random permutations instead. But as I said, it doesn't matter. Use statistics are flexible. You can put in anything you like there. So, we get the theorem again, same type as before, asymptotic normality, same. And order, what you get is now n to the number of blocks again. So, this again follows by Hafting's theorem, and now I don't have to do any new modifications. Now, I don't have to do any new modifications because I already did them to treat the preceding case. But now, let me end with a random pre-permutation. So, here I say theorem in quotes because I think I have a proof, but I haven't written it down in detail and checked all the details yet. But I didn't hope to do it next week. This week, I, as you know, busy. But it says that if you take a random prepared. Says that if you take a random tree permutation, you have the same thing. If you take another random tree permutation, or actually, it should be a random forest, even a random forest point H to sigma, you will have an asymptotic normal limit here and positive variance and so on, except in trivial cases. And the reason here is that now you have to forget what we just did. Just did that was for forest permutation doesn't help here. Now we have to look at the structure of a tree permutation, and it's possible again going back to the paper by Azan and Hichenko, one can code tree permutation by sequences of zero to one. And you can look at the runs in them. And then with some work, you can see that number of occurrences you can write as a use statistic based on. Use statistic based on now the variables you need or the run lengths in your code for this tree permutation, and then it is similar to what we've done before. But here it is a new oh, yes, I wrote it here with this coding, but here is a more complicated because we actually had to take a vinter use statistic to get this work, and also again we. And also, again, we have to use renewal theory as above and sum up to a certain number of ways to cross the threshold. So I have to combine the two variations I have used in previous examples. But as far as I can see, there's no problem to combine these two modifications and adapt Hefting's proof to this case too. But as I said, I haven't written it down yet. And that's all. And that's all. Okay, thank you. Thank you. And I want to thank the organizers also. Say that unfortunately, this is I've been invited to Band before, but this is the first time I've been able to attend. Thank you very much, Vanta. Well, thank you very much, Fanta. We're extremely pleased that you could attend this time. Has anybody got a question or a comment? Peter. Yeah, Santee, that was wonderful. Thank you very much. So you started with a theorem which says that if you take your permutations uniformly at random from the whole class, then you get asymptotically normal behavior for the number of occurrences of a pattern. Number of occurrences of a pattern. And then you move to lots of special cases where you take the permutation from one of these avoidance classes, which are very small classes, right? And if they're represented by permutons, they're represented by singular permutons or by probability distributions among singular permutons. Yes. Okay. But it seems to me that it ought to be possible to. Be possible to, even if Igor says you can't generalize it to cover all those classes, that you can generalize it to cover permutations that are selected from a non-singular finite entropy permuton. Yes, but again, remember that this doesn't work for all classes. You still need some luck to be able to code it in. To code it in a super way, or do you think you can directly take the permiton and you make use construct a random permutation from the permit directly? Right. Yes. Yes. The coding is not the issue there. No, I see that. You have the coding and you have, yes, it looks like you use statistic. Yes. I think that ought to be possible. Yes, I think you're right. Okay, we can talk about it. Yes. We could talk about in the break. Yes. Probably less room for cleverness and all this stuff you've done here. That's wonderful that you can use these codings to get it for these small classes. Yes. So a bit of a side question. So in the many examples that you've described, things can be expressed as geostatistics using this methodology that you introduced and so This methodology that you introduced, and so you get this one-half discrepancy in the exponents from the mean and the fluctuations. Are there some cases where you would expect a different behavior from this? Not when you use this methodology, then I would be surprised. Maybe I shouldn't say it's impossible, but I should be surprised if there are other discrepancies. There are other discrepancies, but I mean, certainly there are other classes where this doesn't work where the main other example is when, as I said before, you have, well, if you have non-random permutons and you have a standard deviation of the same order as the mean, because you have really convergence to some distribution and not to constant in the first order, and so on. So, you have such cases. Yes, certainly. Yes, certainly there are several such cases, but I said today I chose to talk about others. But if you have any class where you know the permutant limit and the permutant limit is a random permutant, then it follows that you will get that type of behavior for these occurrence numbers, number of occurrences. Since they will convert to corresponding things for functions of permutant. If the permuton is random, you get random numbers there. And do you think, is it possible to get something that is even more concentrated without being degenerate? That's a good question. I think it may be possible. I haven't seen it. So, I said, the only way I know to. I said, the only way I know to you can get smaller variants if you take linear combinations, but not if you don't. In this type of examples, that seems to be the general rule, but I can't claim that it's impossible. Again, it would be surprising. Because typically, you get this degenerate case with small order variance when there is some symmetry reason for the main term to disappear. Main term to disappear, and I don't see any way for that to happen with permutations, but maybe that's just because I haven't got enough imagination. Steve, you've got a hand up. Yeah, sorry, just took me a while to one mute. So besides Herfing's proof of his theorem, there's Proof of his theorem. There's other proofs, like I know of one due to Eagleson and Weber using reverse Martingale central limit theorem to prove Herfding's theorem. So I was just wondering whether using that machinery, it might be sort of easier to sort of adapt. Uh, you know, sort of adapt, uh, you know, when you say you know, you're adapting Hofdting's proof, it might be easier to adapt those proofs. Yeah, that's quite possible. I haven't tried because I succeeded with Herfding's proof and or and I haven't looked at this proof with reverse mark girls. There is a nice proof of convergence or more short convergence for. Or more short convergence for use statistics using reverse martingales, and that is something I try to adapt. But at least in one case where I needed to adapt it, it didn't work because with these complications didn't have the right symmetry, so I had to do it in a different way. But as you say, there are different proofs, and it may very well be the case that some versions are better proved using other proofs. But so far, I have Proofs, but so far I have stuck to this simple way, this single way of doing it. Okay, thanks. Okay, well, let's thanks, Vanta, very much again. So maybe we'll just take a couple of minutes. Ah, Igor, great, I can see you on screen.