To present our work. So, in this talk, I will establish some very simple connections between two classic literatures with rather old roots. And after establishing these connections, we'll basically try to import some insights from one literature to answer some open questions in the other one. Namely, to characterize the linear convergence rate of Synchor's algorithm for sparse matrices. So, here's the summary of the talk. So, one classic literature is on matrix balancing that we'll be focusing in this work. And matrix balancing is a problem that's common in many different areas, such as economics, statistics, machine learning, and optimization. So, for example, it's used to approximate optimal transport distances. And the classic algorithm in this literature is called synchronous algorithm. It's very widely used and well studied, but the convergence behavior for sparse matrices is not completely understood. So the most popular settings look at the case where the matrix to be scaled have all strictly positive entries. So in this work, we will connect matrix values. Work, we will connect matrix balancing to choice modeling literature from economics and statistics. And then, afterwards, we will leverage insight from the choice literature to help characterize the geometric convergence rate of synchronous algorithm for sparse matrices. And we also believe that these connections that we've established between the two literatures can help open doors to more results. So, here is the matrix balancing problem that we will consider in this talk. So, given positive vectors P and Q that have the same sum of weights and a non-negative matrix A, we're looking for positive diagonal matrices D1 and D0, which are essentially the row and column scalings of this given non-negative matrix. Of this given non-negative matrix A, such that the scaled matrix satisfies the target column and row sums P and Q. So the picture of this problem is visualized here. So in this picture, we have a sparse matrix, and we also have the target column and row sums. And we're essentially trying to find a column and row scale. Column and row scaling, such that the scaled matrix has the target sums. Sorry, target marginals. So the two features of this problem, the first one is that the sparsity pattern is always preserved under the positive diagonal scalings. The other one is that the solution is invariant under a rescaling of the column and row scalings. So this makes So, this matrix balancing problem is a common problem underlying many applications. So, in one type of problem, A can be understood as a detailed flow of, for example, traffic or trade between nodes in a network. And the P and Q are the marginal volumes that we also observe at these nodes. Note. And the common problem in this setting is that the detailed flows could be inaccurate or inconsistent or outdated. And as a result, they don't actually sum up to the observed node level activities P and Q. So the question is how to minimally modify this matrix A so that it has m uh exact market. So this uh question already hints at the uh KL minimization principle KL minimization principle that underlies this type of problem. And other applications include contingency tables and statistics, matching economics, and of course computing the upload transport distances. So in this work, we will essentially show that a particular logic-type choice model is also a special case of the choice modeling problem, sorry, of the matrix balancing problem with binary A. With a binary A, binary matrix A. But first, we'll briefly review the entropy regularized optimal transport because it helps us distinguish our work from previous works. So here is the entropy regularized optimal transport problem with a given cost function C and the target distributions P and Q. So usually for this part. So, usually for this problem, what is done to approximate the OT distance is we set a small value of epsilon and we take the standard exponential transform of the cost function. And if we do a discretization, we essentially arrive at this KL divergence minimization problem subject to the marginal constraints. So, it's well known that the matrix balancing problem, if the solution exists, then it always solves this KL divergence minimization problem. And if the base matrix A has all strictly positive entries, then the converse is also true. However, when A contains zeros, then the converse might fail. And this is the setting. Fail. And this is the setting that we're looking at. And again, one of the motivating examples is this application of trade, where you observe trade volumes. However, because of certain constraints, the trade between certain pairs of nodes, they're not allowed to have positive values. So that's a common problem in the trade applications. And so you can also understand this type of problem corresponding to the case when. Corresponding to the case when the cost function C, it has infinite cost at some pairs of nodes. So the conditions for the matrix balancing problem actually suggest a very simple fixed point iteration, where we essentially just alternate between satisfying one of the two large coal coefficients. And this is a well-known algorithm we discovered. Well known algorithm we discovered many times. It's called IPF in statistics by proportional fitting. It's actually even an older echometrics paper, but most commonly is known as synchronous algorithm. So it's computationally evaluated and widely used. And in this work, we'll also establish the fact that Synchron's algorithm is closely related to some classical algorithms in choice modeling. Algorithms in choice modeling. So, for example, the so-called VLP iterations, so it can be understood as a generalization of Synchoint's algorithm applied to a particular choice setting. So, the convergence of Synchron's algorithm has been very widely studied. So, as I mentioned before, if A is enterwise strictly positive, then the solution always exists and unique up to the normalization. The normalization. And this very classic work from 1989 shows that it converges geometrically, meaning that under some type of metric, the distance will shrink geometrically with a particular rate, where this t is the number of iterations. There's also a recent line of works that is. Line of works that establish near-optimal iteration complexity bounds for the specific problem of computational optimal transport. And more generally, convergence of Synchorn's algorithm has also been established for general probability measures beyond the discrete case. But in this talk, we'll focus on this the discrete case, so everything will be discrete. So everything will be discrete. So as I mentioned, when the matrix A, the base matrix A that we're given contains zeros, the solution, the existence and units of the solution of scaling for the matrix balancing problem and convergence of synchor is not always guaranteed. So the sparsity structure of the matrix A will become important. So however, these general convergence results. General convergence results that apply to general probability measures are still applicable when they're specialized to discrete setting. So for example, this recent work by Ligier established a global one over square root T convergence, I believe for the L1 distance, if the KL minimization problem is feasible and bounded. And on the other hand, there's also work in 2000. Work in 2008 by Knight that established an asymptotic geometric convergence for uniform marginals for sparse matrices if the matrix balance involved is functional. So in this work, we'll provide global and asymptotic geometric convergence rates for general marginals and general non-negative matrix A whenever the matrix balancing problem has a finite solution. Problem has a finite solution. And we'll do this by leveraging some connections to choice modeling. So I'll give a brief introduction to the particular type of choice models that we'll look at. So discrete choice data is very common in economics, statistics, and social science. So here we'll consider a very simple but fundamental model in these literatures. So we're given a total So we're given a total of M alternatives, and given a subset of these M alternatives, the decision maker chooses an alternative from this subset S. And essentially, this forms an observation indexed by the particular choice A and the consideration set S. So, and we're given n observations of this form. Observations of this form. So, without loss of generality, we can assume that each consideration set Si is a unique subset of the set of all alternatives with multiplicity Ri. However, among these Ri different repetitions of the same configuration, the actual choice could vary. So, a natural question is: how do we aggregate this choice data? Aggregate this choice data. So, a very simple but also very fundamental approach is to this axiomatic approach that essentially leverages this kind of invariance property of the choice probabilities. So, I think it's more commonly known as the independence of irrelevant alternatives. So, essentially, given J and K, two alternatives. Two alternatives. This axiom essentially requires that the ratio of the choice probabilities does not depend on the irrelevant alternatives which are not chosen. So this is a very simple and justified, in many applications, justifiable assumption. So under this assumption, essentially we are guaranteed that there exists a set of scores. Exists a set of scores. So a score is basically a positive number between 0 and 1 for each alternative, such that the choice probability can just be represented basically proportional to the score. So this type of model is more common in applications such as chess. I think in statistics, for example, these so-called These so-called Bradley, tear it loose, or BTL models. So that means that the choice set size is fixed at two. And actually, that's how, for example, the ranking of chess players is determined. It's called the Ego rating system. Essentially, you just try to solve this problem, come up with an estimator for the scores. And that's the scores for the chess players. And then for partial rankings, it's called the Placket-Loose model. Called the placket-loose model. I think this model might look a little strange to economists because I think we're more familiar with multinomial login models with covariates. So I think one way to understand this type of model is that it's like a discretization of multinomial login models with covariates. So essentially, if we think about an application where we're trying to estimate the answer. You know, we're trying to estimate the demand for different cars. We can think about essentially different covariates or different features of different cars. So, for example, Corolla versus Camera will have different specifications. However, in this particular choice model, we're basically abstracting away the detailed features of these alternatives. We're just saying that Corolla and Cameron are two different types of alternatives in the consideration set. However, Consideration set. However, the added kind of feature of this particular type of model is that the size and the content of the consideration set SI could vary across different observations. And as a result, the comparison structure or essentially the sparsity structure is going to be important for the estimation problem, both in terms of the statistical efficiency and in terms of the algorithmic efficiency. MIPE efficiency. So, our first result will essentially show that the maximum likelihood estimation problem of this very simple model is equivalent to a matrix balancing problem. So, here we write down the maximum likely estimation problem for the loose choice model. So, the optimality conditions Optimality conditions can be written in this form, where this wj here is essentially the number of times that each alternative j is chosen in this set of n alternations. So essentially on the left-hand side, this quantity is basically the frequency, the empirical or observed frequency of winds, of object alternative J. On the right-hand side is essentially the On the right-hand side is essentially the conditional expected choice probability for item J. And it's essentially matching the optimal condition is essentially matching the empirical and the expected frequencies. So it's a type of mode matching method. And I guess kind of on a rough high level, this is also kind of hinting at the type of The type of approaches, for example, in the BLP approach, where we're trying to estimate the demand for cars based on their market shares. So, on the left-hand side, this is like a market share of each alternative. So, how to solve this systems of equations of optimal conditions? So, if the consideration sets SI vary across different observations, then there's no close. Observations, then there's no closed-form solution. But however, if we look at the right-hand side of the optimal condition, we see that essentially the coefficient for sj can be separate out. So that suggests also a very simple fixed point iteration. And this simple fixed point iteration actually goes back to very old works. And yeah, essentially the uniqueness The uniqueness and existence of solutions and convergence of this particular fixed point iteration will depend on the particular comparison structure. So now we will basically reformulate this optimal condition as a matrix balancing problem. So we call the matrix balancing problem structure. We're given a sparse matrix and target row and column marginals. So, this A that we'll construct from the choice data is going to be very simple. It's a binary matrix where each row essentially is the indicator for which, so each row corresponds to a particular consideration set and it's an indicator of which alternatives appear in that consideration set. So, and then in other words, each column corresponds to a different Each column corresponds to a different alternative, and it will basically tell you which of the consideration sets the alternative appear in. And then for the target marginals, it's also very simple. So essentially, for the row marginals, it essentially encodes the number of times each unique consideration set appears in the data. And then for the column marginals, essentially it will be the number of times each of the It will be the number of times each of targeters is chosen. So, this matrix is n by m, where n is the number of unique consideration sets and m is the number of alternatives. And again, this matrix A essentially indicates the membership of each alternative into the considerable set. And so, we'll reformulate the optimal conditions as matrix balancing. So, essentially, the takeaway of this result is. The takeaway of this result is that if we solve a matrix balancing problem with the data A, P, and Q constructed as I described before, then the right or the column scalings will essentially correspond to the MLE or the MLE solution of the optimality coefficients. So, to convince you that's the case, the scale matrix is of this form. The scaled matrix is of this form, and essentially, you know, when we sum up these terms over the i's, we'll essentially get the optimality condition. And if we sum it over j, then it's essentially a normalization condition that returns RI, which is the road target. So another takeaway is that essentially that the These three quantities, the matrix A and the target marginals, form the sufficient statistics of the boost-choice model. So, essentially, that means that you could have different data sets that correspond that would yield the same maximum likelihood estimator, as long as they have the same AP and Q. And then we also show that the existence and uniqueness conditions of the matrix balancing problem are also translated. Problem are also translated one-to-one to the conditions from the choice literature. But more importantly, we try to leverage these algorithmic connections between the two literatures. So we observed in this work that synchronous algorithm reduces to the fixed-point algorithm that I mentioned before, and also this more recent MM algorithm of Hunter. So essentially, what we're doing here is we're connecting this statistical model of choice, of the square choice, to this optimization problem, KL minimization. So there are other statistical models that have been connected to this KL minimization problem. So essentially, the reason we're doing this for the discrete choice model literature is that there are recent results in this literature that highlight the importance. Literature that highlights the importance of algebraic connectivity of the comparison structure for both the statistical efficiency and the algorithmic convergence. So, and then that's essentially the insight that we're trying to take. So, in this work, we'll quantify the geometric convergence of synchronous algorithms using this quantity, algebraic connectivity, which I'll define of a bipartite graph induced by the matrix A. And then in the follow-up work, we also quantify this low efficiency for some model of traffic whose MLE is estimated from SYNCOR. So very briefly, the algebraic connectivity is defined. Essentially, it's a measure of how well connected a graph, a particular graph is. So the graph on the left is very well connected, and the graph on the right has a bottom end, which is the edge, which is Has a bubble map which is added between node 10 and 12. So essentially I'll give a definition of the algebraic connectivity in a few slides, but essentially it's based on the discrete version of the Laplace operator, which is constructed from the G sensing matrix of that graph. And the the uh the algebraic connectivity is essentially the second smallest eigenvalue of that uh well Laplacian matrix. About Laplacian matrix. So, in our work, we'll consider the bipartite graph with adjacency matrix constructed from this given matrix A. And then we will construct the graph Laplacian of this bipartite graph and define the Feedler number as the second smallest eigenvalue of this graph Laplacian. And one remark is that this is distinct from the choice A because there the A is. Because there, the A is binary, that's the first thing. And the more important thing is that we use a different adjacency matrix. Because in the choice setting, the graph is a comparison graph among different notes, different alternatives, and here it's a bipartite graph. And finally, we'll let A T be the scaled matrix after T for iterations of the course algorithm. That means that the column constraints are satisfied, and we're going to measure the progress using the. Progress using the row signs. So, the key observation for the next result is that it's very simple: that a synchronous algorithm is the alternating minimization or coordinate descent of the dual container function to the KL minimization problem. So, here, essentially, the D0 and D1 are the vector forms of the row and column scalings of the original matrix balancing problem. And then we can apply this very standard transformation. This very standard transformation to obtain this form, exponential form for the dual potential. And the key observation is essentially that the Hessian of this dual potential function is a scaled version of the graph from a classic matrix. And so that's why we can quantify the convergence rate of the coordinate descent algorithm using the Python value of Using the heightened value of the graph of facile. And as a side, this dual potential function also reduces to the likelihood function of each choice model. So this is our main global linear convergence result. So essentially, it measures the progress using the optimatic deck of the general function. So the convergence rate is essentially quantified by the feedler number. The Fiddler number over this, it's like a smoothness parameter, which is defined based on the max row and column sums of A. So as a result, essentially, we obtain a geometric convergence versus the sublinear convergence that's obtained from the Legion work that is based on weaker conditions. Is based on weaker conditions. So, this ratio essentially between the smoothness parameter and the filter number, which is the strong complexity parameter for the dual potential function that plays the role of the partition number. And the convergence rate typically depends on the smallest eigenvalue of the Hessian. But in this case, it depends on the second smallest eigenvalue because we can show that the dual. We can show that the dual potential function to the K-ninomization is strongly connected on a subspace orthogonal to the middle eigenfactorials. And the similar orthogonal condition helps us extend this asymptotic geometric convergence rate from 2008. And then, in terms of next steps and ongoing work, so I guess one remark for the previous for our covers really. For our convergence rate, is that this dependent on this implicit B is rather kind of unsatisfactory because this B is the diameter of the initial sub-level set. And so we hope to first find an efficient way to characterize or bound this B and also improve the dependence of the bound on this initial diameter of the initial. Diameter of the initial heaterates. And as I mentioned before, in the separate work, we also develop statistical models of matrix balancing and attributions to networks of traffic and trade. And finally, we're interested in generalizing these results to continuous distributions. So I think that's the end of my talk. Thank you. That's related to your ongoing work, but in some related problems, the statistical properties of the estimator also depend on this kind of connectivity of the path, in general version or collect the ring that is. Can you make that connection here you're you're looking at the link with the computation app uh what is uh so actually do you do you mind repeating the the the other Do you mind repeating the other side? If you take a linear inversion on the graph, and the connectivity of the graph is driving the rate of conversion, a more statistical. So I think this other work that I mentioned essentially provides an analog or parallel of that result in the particular model. Are you interested in the kind of the what you're doing? Kind of the what like the spec like the parameter specific in application you're more interested in the common parameters of overheads or the maybe the kind of the margin you know average margin elasticities yeah is this something that you can entertain also yeah I believe so as I think uh as long as that's a functional of the for example that if we're trying to aggregate the fix uh the individual fats then uh I believe that's doable. I believe that's doable. But we're also going to be able to it's definitely a work worth studying further, essentially incorporating the Colberts as well. Just a follow-up question. For the case of confusion, how did you define a connection? Yeah, so that's kind of what we're trying to figure out right now. But I mean, one hint is that the Braphilath lasting is the discrete version of the health trader, right? So we're essentially trying to go in that direction. Thank you very much. Thanks for all the speakers. And we can meet tomorrow at 9 for Good Go's Kingdom Test. 9 a.m.