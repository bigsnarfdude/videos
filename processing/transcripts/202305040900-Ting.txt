Take it away, Chi Ming. Okay. Thanks, Ivor, for chairing these sessions and organizer for organizing these workshops. So I'm going to talk about the low-rank and sparse decompositions, in particular used for analysis of dynamic functional connectivity estimated from a naturalistic fMRI data. FMRI data. This is a joint work with Jeremy and Fuad and Steve and Hernando. Jeremy has been working on studying the brain activity related to language processing. So he collected some movie FMI data. Then, so we are interested in analyzing dynamic functional connectivity in this. Functional connectivity in this movie FMI data. As all we know, right, so the brain network is really static, so it's changed over time. And a common approach for estimating dynamic functional connectivity is via a sliding window approach, right? So basically, you just divided the fMI time series into short window segments, and then you estimate the You estimate the connectivity matrix within each window segment, right? Then you can have a sequence of connectivity matrices to characterize the changes in the functional connectivity. You can do some thresholding, then you can form a sequence of graph. But this functional connectivity dynamics usually the stimulus induced information is typically mixed with some With some intrinsic neural processes and also some noise, right? So our problem is we try to separate this stimulus-induced dynamics from this noise. So we are interested in especially in estimating the dynamic extracting information, the dynamic connectivity information from the naturalist. Dynamic connectivity information from a naturalistic fMI, especially in a movie viewing, as compared to the traditional class-based FMI, right? So which we use a pretty strictly controlled stimuli, right? So on the other hand, we will use a resting state, which is unconstrained. But the naturalist paradigm will provide us more ecological. Provide us more ecologically valid paradigm that can fit the real life scenario. So, for example, the subject that will be exposed to mixtures of unconstrained, continuous model type of stimuli, so including audio and issues, so the video. Okay. So, we are studying a small data set of 13 subjects watching the same. Watching the same TV show. And so my collaborator, the subject, collect this database where the subject is spent hours in the scanner. So our aim is to try to separate the stimulus-related functional connectivity dynamics across subjects. So as we know, this subject will explain. This subject will experience the same stimulus. So we will expect some shared structures across a different subject. So we call it common structure here. We want to separate this common structures from the background activities. So maybe some of the subjects that do not pay attention. So that this is an individual-specific structure. Okay. And next aim. Next aim is we try to map the extracted dynamic functional connectivity to the time-varying stimulus annotations. So, let's say for example, my collaborator has annotated the auditory event from the movie. So, for example, here we have a sequence of auditory events like silent speech, speech by songs, and speech plus minds, like people talking in the Customize like people talking in the noisy background and also speak song and noise only event. So we have this time-varying stimulus. And then on the other side, we have a dynamics of functional connectivity. So we try to relate how the changes in the how the temporal changes in the stimulus might related to changes as uh I mean On the functional connectivity. So we get the idea from the computer visions, especially when modeling a background in a video. So this is a video frame. And we can decompose this video frame into low-rank components, actually, which represent the background components. So let's say, for example, So, let's say, for example, some slowly varying or static scene. So, and then the foreground elements, like some of the objects might appear and disappear, and this can be captured by a sparse component. So, you can treat this every frame of the video as a connectivity profile for a subject, right? And then you can decompose it at the low-rank components. That means the share components, the CM. Components, the similar, highly correlated functional connectivity pattern across subjects can be represented by low-rank components. And then all other subject-specific structures can be which occur occasionally in a certain subject can be represented by a sparse component. Here we propose a time-varying low-rank plus-plus model for the Brain model for the dynamic brain networks. Assume that we have a time-varying multi-subject functional connectivity matrices, sigma. T is the time window, the index for a time window, and I is the index for the subject. So we can vectorize this connectivity matrix and stack it as a column of the matrix of certain matrix Ct. Observed matrix CT. So CT is a, you can say it's a multi-subject functional connectivity matrix at a particular time window. Then we can assume a time vary, low-length decomposition where we can decompose the observed matrix CT in a low-rank component. And these low-rank components will represent the stimulus-induced correlated functional connectivity across subjects. Connectives across subjects. So you can see here there's a similar or connectivity patterns across subjects. And then the sparse components here will represent the stimulus unrelated or some background components. It can be intrinsic neural processes or some non-neural artifacts like head motions, which only occur occasionally at a particular subject at a particular time point. Subjects at a particular time point, so which is idiosyncratic or specific to a specific subject. This will be represented by a sparse component. So, our goal here is to recover this low-rank components, which represent the shared structures across subjects and also some individual-specific structures from the observed. From the observed matrix. And this is a very well-known, established problem. And there's a very well-known problem to solve this problem called principal component pursuits or robust PCA. So basically, it's just solve a convex programming by minimizing the nuclear norm of the lowering matrix and also the entry-wise L1 norm of the normal. The entry-wise L1 norm of the sparse matrix, subject to this constraint. So basically, the new query norm finalization will induce low rankness in this matrix, and this L1 norm will induce the sparsity in the sparse matrix. And this can be solved very efficiently by a lot of efficient algorithms, like augmented Lagrangian multipliers. And we And we extend this fuse PCB, okay? So, to a fuse version of PCB, drawing the idea from the fuse lasso, basically, we add another additional penalty, so the L1 norm to penalize the differences between the columns of the low-rank matrix. So, this additional fuse penalty will string the interest, the differences. The differences between neighboring subjects to zero. And this will induce the group homogeneity in the functional connectivity profile. So, for example, here, so if this group of subjects will share the same connectivity patterns, so that's when the differences will be shrinked to zero. Okay. We can rewrite this by defining a first-order. By defining a first-order difference matrix, we can rewrite this loss function as this. And we can solve this fuse PCB using the ADMM algorithm. Let's define some surrogate variable alpha one until alpha t is the last time window. Actually, alpha is just the intersubject differences. Okay. And then minimizing Minimizing this objective function one will just equal to minimizing this. So, where we add another additional equality constraints for the intersubject differences. Then we can form augmented Lagrangian functions, right? So, which we can relax this two equality constraints and with the x and t as the Lagrange multipliers. Now we try to minimize this safety function two. So with respect to each of these PIMA variables sequentially. So here to obtain the low ranks and the sparse component and the intersubject differences we can be obtained by just solving these sub-problems. And then this the algorithm will alternate between this. Algorithm will alternate between this premal step and also the dual step to update the Lagrangian multipliers. So, unfortunately, we don't have a closed form solution for tree for this sub-problem, mainly due to this linear operator A. What we do is we try to linearize this quadratic term using the first order Taylor series expansions. Taylor series expansions. And then we replace this expression in the objective function tree. Then we have to solve this sub-problem. The solution for this sub-problem is for to obtain the low-rank component is just the singular value thresholding where we can compute the singular value decompositions from this expression. From these expressions, and then after this, we just do the thresholding of the singular value and reconstruct back the matrix. So the sparse component in the inter-subject differences can be obtained by source thresholding, entry-wise source thresholding. This is an overview of the algorithm. So, as the input, so as I mentioned just now, we have a series of vectorized multi-subject dynamic functional connectivity matrix. Dynamic functional connectivity matrix X1 until XT. We can set the regularization parameters and then we just iteratively update this solving the subpoena the three sub-problems to obtain the low-rank components and the sparse components and the intersubject difference and also update the Braggian multipliers. Okay, so until Multipliers. So until a convergence. The output is just a sequence of estimated low-ranks matrices, which represent the shared dynamics across subjects. And then we have another sequence of sparse components, which represent the individual's dynamics. We did some simulations, especially we compute the relative errors of the recover low-rank components and the sparse components. We assess the effect of the increasing rank and sparsity on this relative errors. So the sparsity level is just the fractions of the non-zero entries in the S. In the S. Okay. So I think for the robust PCA, so if the sparse, the conventional method, so if based on the assumption that the sparse matrix is sufficiently sparse and the row rank matrix is low rank, so we can have an exact recovery. But the recovery may be affected if the sparsity levels is. The sparsity levels is high, so that means many of the entries are corrupted, so we have a dense noise. So you can see here, so we have a different rank. Okay, so then we increase the sparsity levels. So you can see the conventional Roberts PCA, the errors goes up, right? But for our push, when adding by adding the fuse. By adding the fuse penalty, so that the area is relatively stable. Okay. So this might be due to the additional fuse term that can smooth the noise in the lowering matrix. So yeah. Here is some real data analysis results. Especially want to estimate some stimulus-induced functional connectivity elements. Induce functional connective dynamics. So we want to check whether the estimated time resolved low-rank components are related to the stimulus in the movie. So you can see here, so this is the original functional connectivity, dynamic functional connectivity. This is across time. And this is the connectivity ages. This is average across subjects. And this shows the variability across subjects. So you can see that there's a lot of. Across subjects, so you can see that there's a lot of variabilities of the functional connectivity dynamics across subjects. So the blue line is just a mean of this individual's trajectories. This is the low-rank components, the estimated low-rank components cross-time. So you can see if we review some time-dependent structures, okay, here. So these time-dependent structures might be related. Might be related to the changes in the stimulus. So, and you can see here the variabilities across subjects are reduced. That means the dynamics are synchronized between subjects. As expected, the sparse components, of course, we can retrieve the, we can see a lot of variabilities in the dynamics across subjects. Subjects. So, here, I think what we can observe is that the lower end components may be temporarily locked across subjects, suggesting there's a potential shared response to the time-varying stimuli. Here, we look into a particular time window, so we try to convert the columns, right? So the Columns, right? So the columns vector back to the connectivity matrix. So this is at a window time T59. So you can see this is the original connectivity matrix across 13 subjects. And this is the estimated connectivity patterns based on the low ranks components. So we can see we can identify some common network structures. Find some common network structures across subjects. And then the sparse components might retrieve some subject-specific structures. And the estimated rank for this window is two. And then we try to relate this low-rank component with the audioly stimuli. So by feeding a multi- So, by feeding multinomial logistic regressions using the audio stimulus label, so we have a few audio categories like noise, music, speech, transport, and speech, and using the recovery time resolved low-rank connectivity as the predictors. And this is the connectivity profile related to noise, and which you can see distinct connectivity patterns for noise. Connectivity patterns for noise and music, so they are kind of similar, and then a speech plus noise has its own connectivity pattern, and this one is only for the speech only. But here, the connective matrix is constructed from 16 brain regions related to auditory or speech processing. So, we did suffer. So we did some further analysis. In particular, we want to verify a hypothesis my collaborator Jeremy has been working on for some time, which is to see what's the role of the motor system, especially in speech perception. So we know that motor system will be engaged during the speech productions, but But how about the motor system? And here we try to contrast the speech bus noise connectivity pattern with the speech. So this one is estimated from the lowering based on the lowering components. So we can see some increase of connectivity between this auditory and speech processing region, which is this blue region, and also with the motor and somato sensory regions. Moto and somato sensory regions, which is these purple regions. So, this might imply that the speech production region might be actively engaged, right? So, especially, which is maybe required to predict the upcoming speech sounds, especially in the noisy environment. Here we did another. Here we did another further analysis. We use this estimated low-rank components to see whether they are associated or predictive of the time-varying audio stimuli. So we just fit a simple neural network on this lower. Low rank fields, low rank dynamic functional connectivity features, and then try to predict the time-varying audio annotations. So, which is shown here. So, this serve as our ground truth. So, we have five categories of audio event, which changes across the time course of the video. And we use the leaf one up across validations. Cross-validations, and here you can see in the middle, this is the estimations for each subject and across time. So you can see the estimation is quite close to the ground truth for each subject or each time point. And we try to compare the performance with the inter-subject functional connectivity, which is commonly used for analysis. For analyzing naturistic fMRI. And you can see that our approach using the lower end component can outperform the inter-subject functional connectivity and also the ICA. So ICA here, we will use the multi-subject ICA. So then by adding another fuse penalties, so we can get even a higher So, we can get even a higher accuracy. So, this might tell us that the row rank components are predictive of the audio event. So, because they are representing some shared synchronized dynamics across subjects when experiencing the same stimuli, okay. Jiming, you have about three or four minutes. Can you hear me? Of a dynamic functional connectivity from naturalistic fMI. And we add another additional few terms drawing the idea from the fields last law, right? So to induce the group level homogenity in the connectivity profile. And we can see that by And we can see that by simulations. So, the proposed method can actually recover the low-rank component even in the presence of dense corruptions. Then, application to a real data. So, it identifies some FC changes that might time lock to the auditory processing during movie watching. So, especially the dynamic engagement of Especially the dynamic engagement of the motor system, sensory motor system, especially in the noisy environment. And we can see that it gives a better prediction of the stimuli in the movie better than the common approach using the inter-subjects functional connectivity. Of course, there's a lot of extension possible. Possible for this work. So just now we just vectorize the connectivity across each subject, right? So and stack it as a columns of a matrix. But we can extend it to a tensor decomposition. So we just keep all the, because when you vectorize the kinetic matrix, you will destroy the spatial structures. Okay. So we can extend this works to a tensor decomposition, which we keep the spatial dimension. Also, the subject dimension, and probably the time dimensions. And just now we've just applied the decomposition on each time window, but we know that the activity across time might be related, so we can order these temporal structures. And then we, after this. And then after this, we can perform some convergent analysis, some theoretical guarantee, especially how well we can recover the low-rank components by adding the fuse penalty. And okay, so just now we assume all the subject-specific ural activities and the noise artifact is. Artifact is modeled together in the sparse component, right? So, but how can we separate this two? So, this is another problem for future work. And maybe some of the noise and artifact, there's also commons across subjects, not only the stimulus-induced information. So, how can we deal with this? So, probably this can be handled during the front end, right? Sorry, the pre-processing step. So, this was already appeared in the transaction medical imaging last year. And you can also find the code in the GitHub. I think that's all for me. So, yeah, sorry, another one minute.  Yes, yeah. Okay, thank you, Ji Ming. Questions? Hello, can you hear? Yeah. Oh, thanks for the talk. It's very interesting. So I had a few questions, but my main one is: why did you work on the connectivity matrix as opposed to the fMRI data directly? The fMRI data directly, like the region of interest data. So you convert to connectivity matrix and then run your analysis. Is that correct? Yes, I first estimated the connectivity matrix and run the analysis. Yeah. So I mean here? Yeah, so why did you go to the connectivity matrix as opposed to just looking at the fMRI data and why? The fMRI data and modeling explicitly the connectivity of the data itself. Ah, so that's mean you so we can do the you mean we can do the decomposition on the raw data itself instead of the connectivity matrix? Yeah, so you model the dynamics of the fMRI data and then say like apply your low rank. Yeah, so if you do that, yes. Yeah, because if you apply it directly. Because if we apply directly on the raw data, basically we are dealing with the activity of the raw data, right? But now we want to make the decomposition on the connectivity profile itself, not on the raw data domain. So I think the result will be different, right? So if we do the decompose decomposition on the raw data. Yeah, but I think it's a good idea, so we can try that and see what's the difference. Yeah, cool. I might send you an email just to have a follow-up. Yeah, yeah. Thanks. Yeah. Hello, this is June from Toronto. Can you hear me? Yeah, yes. As you noted in your simulations, I think the identifiability issue may be pretty critical here, right? So I think you kind of avoid this problem by assuming that the individual. Problem by assuming that the individual part is pretty sparse, right? Compared to oh, so I think that you are like avoiding this problem by assuming that the sparsity level for the individual is pretty sparse, right? Yes, so we try to increase the level of sparsity. Yes, and then adding more low rank plus sparsity composition exists. Composition like exist. So, in that case, the sparsity level is pretty high. Then, is it going to be sufficient to apply two-stage estimation so that you estimate the low rank first and then you estimate the sparsity? I mean, if the sparsity level is very high, do we lose damage by like the destination? So, do you mean we estimate the low rank? Yeah, because it is just like applying a single decomposition, right? Like, I mean, like, stop you. Estimation can be converted to two stages, like low-rank first, and then like and then individual, like sparsity levels. Okay. Like, you're like using a joint estimation. Like you're like using a joint estimation, but probably it'll gain too much efficiency if the sparse level is already assumed to be high. I don't know. Okay, yeah, yeah, thanks. Thanks for a suggestion. Yeah, so hopefully we can try that as well. Yeah. Okay, thanks, Chiming. I never think about that. Yeah, we're going to move on to the next speaker. So let's thank Chiming again. Okay. Okay.      I think we can start from Lancaster University, and he is going to speak on methodologies of functional time series. Of functional time series with possible applications to neuroscience. I like it. Neuroscience levels here. See yes. But I'm interested in working on this topic and looking for collaborators. And I naively think that some of the methods that I have worked on can be applied to this area. So the idea is kind of a present