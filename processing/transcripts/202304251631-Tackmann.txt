Thanks a lot to the organizers for giving me the opportunity to talk about some ugly business. Hopefully toward the end of the talk I will convince you that maybe there's a way to make it less ugly. But hopefully most of the talk I will talk about the ugly points. Now, a few disclaimers. So first of all, I'm not an experimentalist, let alone a statistics expert. So just being here, okay, I'm going to be here. So, just being here, okay, I feel like there are thin eyes with all of you guys. So, bear with me if I have no idea what maximum likelihoods and marginalized profiling means. But I won't need any of this, hopefully, in this talk. I've tried to abstract things out as much as I could, but please interrupt at any time if somehow I'm slipping if a particle theory slang. And of course, I'm also happy to go to more detail, maybe in the copy. And okay, I should say this. Really, there are many opinions about theory uncertainties, usually as many as there are theorists in the book. Since I'm the only theorist in the room. Alright, so what are we talking about? We all know about the pendulum, right? How do we measure the how do we measure g, right? We have a pendulum, right? We count, right? And then we have a formula which tells us how to obtain this. Tells us how to obtain this G, our coordinate of interest, from our measurement of how many spheres we have. The theory prediction is this formula. And the theory uncertainty is due to the fact that this formula is not quite correct. If you go back, if you look up at Wikipedia, they'll tell you, or in your undercred days, that this thing was actually derived from the limit where the pendulum is only going a little bit out. Going a little bit out. So this is derived in some limit. And so the Fourier uncertainty is the fact that this formula is not exact. So it's not due to, let's say, the length L of the pendulum that I can't measure album target precisely. That's just the usual systematic. So it's really the fact that I just I don't have the correct formula. Okay? And so the challenge is is how do we account for the inexactness in my actual formula? In my actual formula. And so, in that sense, the field uncertainty is different. That's why it's kind of ugly, because a priori there's no auxiliary measurement I can do before I prove it. But as I said, until the end. Okay, so in particle physics, we have data and we want to get to our Lagrangian parameters, right? So we do measurements and we interpret them. And along the way, we insert field predictions. And this can be arbitrarily complicated. And this can be arbitrarily complicated, right? You can have, you know, every Monte Carlo program that you're running using some way is a theory prediction. And I want to not think about all of this complication. So for this talk, what we're going to literally do is we compare a measured quantity f to its prediction. Okay? As simple as that. And of course, this f depends on my part of interest. And exactly, you know, how I do this inversion through some black magic or just literally inputting, I don't care. Okay? I don't care. I just care about this F. And when I, well, we never know what this F is exactly, right? Just like in the pendulum case. So somehow whenever we make a prediction, we want to say it's F plus minus delta F. So we put some sort of an uncertainty on our prediction, which also involves the P i's. And so if I now use, you know, if I propagate this up, I will end up. If I propagate this up, I will end up getting my pi plus minus some delta pi, which is caused by this delta f, and that's my theory uncertainty now in my parameters. All right, and so what I will talk about in this mostly is how do we actually estimate this delta f? Okay, this is what I want to talk about. I will have one or two slides how to interpret it. How to interpret delta f. Will be very short because I don't know. And then I will tell you how to And I will tell you how to propagate it and how do you deal with correlations. Alright, how do we estimate this? So there are actually three types of approximations you can think of. So the first one is that f is a function of some small quantity x. I know x. I just know what x is. Let's say it's 0.1. So I really just need f at x equals 0.1, but I don't know the functional form of x. So what do I do at Taylor's part? Form of x. So, what do I do? I Taylor's part. Okay? So I write it like this. And I know how to calculate this guy. I know how to calculate this guy. And if I ask some of my NNLO friends, they also know how to calculate this guy. So obvious example is an x is alpha s. It's just a perturbative expansion. Okay? That's the best case scenario. There can be cases where I know the limit, but I don't know how to calculate. But I don't know how to calculate corrections. So I still know in that case, so an example is, for example, you can ask me a detail later, but one example are pattern showers. Pattern showers are constructed in a certain limit, and I know that there is a limit in which they are actually correct. Okay, I just don't know how to tackle corrections to that limit, like it's just too complicated. But at least I know that there is some sort of notion of their correct plus order x. Of delta x plus over x. Then I still know what x means. And then there's the worst case where I don't know anything. So where I basically just make up some of f. And I somehow hope and pray that it's somehow related to f dwiddle. And this is what the theorists call a model. So we have heard a lot about models in this workshop. So for me, as a theorist, a model is something that I cannot calculate explicitly. So you have to be very careful when you talk about mono-carl models. Most theorists always come. Monte Carlo models, most viewers always cringe a little bit because 90% of the Monte Carlo is actually not a model, right? Not from our point of view, 90% of the Monte Carlo is actually a prediction, right? It's a calculation. Only the last little bit of harmonization is what we would call a model. Okay, anyway, so just to get the capture on it. But you know, I will mostly talk about this case since that's already thesis and it's already complicated enough. So let me try to explain to you how we. Let me try to explain to you how we do the estimation of the stellar F. So, what we do is essentially we do the expansion in two different ways and we take the difference as our uncertainty. So, we make a variable transformation which looks like this. So, we replace x by a variable x twiddle and b zero is some number. And so, to lowest order, right, since it's important that you know the lowest order, the two are equal. Order, the two are equal, so I can expand in either one of them. And so let me do this, right? So this is the one we just had. Now let me plug this back in, I expand, and this is what I get. And in particular, you know, at next leading order, I should say, you heard about LO, NLO, LA, right? So F0 is leading order, LO, right? This thing is the NLO. This is the NNLO for the virus across. And so you see that, you know, an NLO is essentially the component. And so you see that, you know, at NLO essentially the coefficient is actually the same, and then at NLO you get something slightly different because of the speed order. And so let me imagine for a second that I do the analog calculations. So the green stuff, the following green stuff is what I know, and the red stuff I talk because I don't know it. And the uncertainty now, so having done this, we would conclude that my prediction is this central value, so I have some preference for x, plus minus delta f. Plus minus delta f, where delta f is the difference between these, right? So this essentially prime x minus x twiddle. You plug in what x twiddle was, and you will find that delta f basically is this plus high order stuff, right? So indeed, the way I estimated this delta f, it is something which is of order x squared, so it has the right scaling for the thing that I'm trying to estimate. It estimates, it gives me the size of the stuff I'm neglecting. Okay, and you know, if you were to actually do the next and next region of the calculation and include all of this, you can convince yourself if you go through it that the difference would then become order x cubed. Okay, so this thing does what it's supposed to be doing. Okay, so far, so for good. Now, this is where things get ugly because if you go back, what's the stuff that you're neglecting? All right, is this F double prime? So you might say that this is the true uncertainty. Say that this is the true uncertainty, whatever that means, right? It's the two different stuff that I'm neglecting. That's effectively what I'm doing. And, well, nothing guarantees that this is a good approximation. In fact, often it is not. But since we've been doing it for the last 30 years, somehow they're still doing it. Now, the reason this is bad is because all The reason this is bad is because often what happens is that this is, you know, quantum field theory, perturbation will quickly go up, becomes very complicated. And so the F double prime here has her talk became smaller all at once. I don't know how to do it. On the top of the human look, an experiment with the same list. You can ask me why I don't like that. So yes, so the F double prime has a lot of internal structure which is not captured by the F prime, right? Which is not captured by the F prime. In particular, once we're going to talk about spectra, it's really just not a good approximation to say that this is just three plus one times a number. And moreover, what we're doing in essentially all of physics and all of particle physics is that this b0 is the same number for every possible calculation we're doing and for every possible model. So clearly this cannot possibly be right. But we're still doing this. Just to dive this. Just to drive this point home. Alright, as I said, this is the best case scenario. It obviously just doesn't work if I only know the limit. So if I only know the limit, then what do I do? Well, I can try to somehow take the limit in different ways. So let's say if I have, you know, there's always more stuff going on, right? So what Hervek versus Perfka-Partenchavals are doing, they both go into this limit, but they're going into the limit in different directions. So they're keeping different things fixed. Direction, so they're keeping different things fixed while they go into that limit, and that's why you get different results. So they're somehow two different f of zeros, and again, the difference is all x. So that's your two-point systematic when you do for pattern charges. And of course, if you only have a model, then you can do whatever you want. You don't have no idea what happens. All right, so let me translate to scale variations. Just so for those of you who know what those mean, statisticians just ignore the slide. So x is alpha. So x is alpha s at the reference scale, x twiddle is alpha s at the different scale. Okay, that's the translation. And this p0 is just the log of the ratio of the scales. And Mary always scales by a factor of 2, and so that means that this b0 is just better not times log 2. That's really what we're doing. Never mind where the 2 comes from. And the important thing here is that this scale, or equivalently, this B0. It really has nothing to do with f. It's not a property of f. So it does not have a true correct central value. Let's look at this case where you sort of vary this. Okay, so sigma here is f, sorry, sigma is f. This is the true value. This is my NLO prediction as I vary this by zero. And there could be a case, right, where there is basically no value that actually hits the true value, right? So this should make it very clear that there is no such thing. Very clear is that there is no such thing as the correct central scale. This thing is just an algebraic parameter that I lumped up on top. And all right, so yeah, as I already said, unfortunately, it's so much convenience, so very convenient, and so prevalent that it's hard to overcome this. Do you want to say something about the number two? No. So you can ask me. You can ask me. If you really want to know, ask me later. I don't want to ask you. Every theorist has their secret. There's a historical anecdote that the two comes from, which I can tell you. Alright, so what would be a better approach? Now, it's kind of obvious the way I present the problem, right? What you want to do is, well, you consider this thing that I'm neglecting as a source of fear. Of this thing that I'm neglecting as a source of theory uncertainty. So, what would we do? We should go and estimate what F double primers by duh. So, why haven't we been doing this for 20 years? I don't know. So, now there's different levels of complications here, right? If f is just a function, so x, right, then it means those derivatives are numbers. They might still have internal structure, but I could in principle estimate them. Of course, the reason, to the defense of all my colleagues, I continuously Of all my colleagues, I continuously insult during this point. Usually, this is more complicated, right? These are like complicated functions, and so you know, it's actually hard to estimate these functions. And in fact, you know, it's a question for math, right? Like, how do I estimate some n-dimensional function where I really don't know the space in which it even lives and what it is, right? Like, so this is why you might appreciate that maybe just taking log2 times the previous function is actually not such a bad thing, because I really just don't know where else. Such a bad thing because I really just don't know what else to do. Okay. So let's do. Okay, this is a preview to the end of the talk. So you have to now suffer through the rest of the talk until we tell you more about this. Right? Okay, two-point systematics, just one very quick slide. So this is completely obvious, but it's important to say, so, you know, you have two models, you take the difference and you question whether that's the sum. question whether that's some estimate of the true delta. And again, obviously, if delta f is small, this does not mean that delta f2 is wrong, because both might just be equally wrong. If delta f is large, it does not mean that delta f2 is large, because one of them might be wrong, the other one correct. So let's say both are equally good, and even in that case, you don't really know much, because even then, this only gives you a lower limit on the uncertainty. This only gives you a lower limit on your uncertainty, right? If that uncertainty is effectiveness is small, doesn't mean your uncertainty is small. If it's large, then you know it is at least as large. So, if this really becomes a relevant source of uncertainty, then this is a hard proof to swallow, but you need to modify your analysis procedure that you're not sensitive to this. This is the ultimate. I know that you're gonna jump up and say that you can't. No, no, it's possible. I know, but I'm just saying, if you want to, if you avoid, let me put it this way, if you want to avoid. Let me put it this way. If we want to avoid the fact that we don't know here, there's not much we can do as theorists, right? Because this is the level at which we know the atomization model is being true or not. You can go and you can make better measurements and say, Piffya, I really trust Priffy, and I don't trust Turbeck, and I really trust Priffy as that's going to say more than parameter of uncertainty estimate. You can do all of that, and then you're happy, but if you really don't trust... You're happy, but if you really don't trust them either, then you really, the only thing that you can do is this two-point system adding, that's what I can say about it. Okay. Quickly, how do we interpret it delta F? So, okay. I give you this predicted F plus minus delta F and so field so in my mind when I estimate it, okay, I'm kind of thinking thinking of it as a difference to the true result. As a difference to the true result. And this is what you know. I have some order, I estimate an uncertainty. But I can, of course, never check this. The only thing I can check is if I estimate, covered the best result. And then, you know, if I see something like this, I'm happy because everything covers the highest order results. So I trust the highest order uncertainty here. In that sense, if I see something like this, then I'm a little more suspicious. But that's the But that's really all we sort of do when we do the when we estimate this delta F, right? There's no notion of any sort of one sigma or anything. But of course, once you guys stick it into your profile likelihoods and whatnot, then obviously it will always be interpreted as some sort of a one-sigma. There's always some notion that somehow it covers with some probability. Then the question is: well, in what sense is that, and what distribution? And there have been lots of And there have been lots of discussions about, you know, should you use a Gaussian for that or not? And the theorists will say no, it should be a flat distribution. And the translation is, you know, if you use a Gaussian, you're centered on my central value, but my central value shouldn't be the most likely because I really don't know, you know, I could have used a slightly different scale, whatever, right? So somehow I don't really know. But of course, a flat box doesn't make much sense because it's aggressive if you go to the edge. No theorists will really. No few of us will really say, I know 100% confidence that only in Europe, nobody will sign that. So maybe you can do some sort of mix, but then what does the flat versus tail? Okay, my opinion, just use a go. I don't care, to be honest. Okay? Whatever you fancy, right, whatever. Until someone actually demonstrates that the choice of that shape The choice of that shape really matters. And if it does, then you have bigger problems. Because then you're so sensitive to fear uncertainties, then you have bigger problems. Now, if a fearist still complains, then you can do an auxiliary measurement of their true mental distribution by asking them which percentage of you can pick citations per pa on your last paper, monthly salary, post-affa funding, are you willing to lose if the next oral is outside your code? Uncertainty, 68%, 95%. 68%, 95%. I'm only half joking. We should try this social experiment with your colleagues asking me that question. We do have some experience in certain particular physics scenarios where we know several orders and we can learn from them. Okay, coalitions. Okay, correlations are crucial. Basically, whenever you use several predictions in combination, you might use the correlation. So, here's a prototypical example of what happens in Nisu. Nick gave a very nice talk. He had this, you know, we can avoid the fear uncertainties by trying to do, you know, we extrapolate from the control region into the signal region. So that, you know, removes our dependence on the explicit theory. And that's true. So what you're doing here is you So what you're doing here is you you have some f that you want. So what you do is you take some other one that you measure and then you multiply by the ratio and you predict the ratio. So that is essentially what you do when you do an extrapolation from a control into a signal region. And that ratio then comes out of Monte Carlo or some predictions. Now of course, if you don't trust the prediction for F and you do trust the ratio, then what you are assuming is that there are uncertainties cancelled in that ratio. Uncertainties cancel in that ratio. That's what you're assuming. So, what you're really saying is that this delta F and this delta G are somehow strongly correlated, so it cancels. But how do you know that? I just told you that we estimate this by beta 0 times V0 times this and V0 times this. All of that doesn't give us anything about correlations. So, our Kerbent methods really, they They're not even reliable for the central value, and they just don't contain anything about correlations. In particular, you can have a lot of arguments about theories, whether you should use simultaneous correlated or uncorrelated scale variations, all of that. And all of that assumes that the scale is somehow a physical parameter, but it's not. So it cannot give you anything about correlation. So the minute you have any sort of shape uncertainty, we should be extremely wary about using scale variations to estimate the shape uncertainty. All right, so what we can, you know, yeah, so what we can do is we can sort of put a band-aid on it. I have a slide on that, which I probably skip. And just to say that the two correlation between these two obviously depends on whether the F double prime and the G double prime, whether they are somehow the same or not, like to what extent they are correlated. Which, of course, again, our current estimates just don't know about it. Now, here's another important case. Now, here's another important case for correlations, which is a spectrum. So often we have differential spectra, and what often happens, so now this energy here is my y now, so I can do a perturbative expansion, right? Now I need to estimate a whole band, right? So I need to do what I just did point by point, and I get a band. And usually what happens is that the integral of the upper edge and the integral of the lower edge would causely overestimate the uncertainty that I Overestimate the uncertainty that I would get if I directly calculate the integral. So basically, the delta F is not just the integral over the delta F of Y. So that means there is some anti-correlation in that band which cancels this additional shape uncertainty. And this can be quite significant. And the reason this really happens is because usually we have several of these variations and we take the envelope. And obviously taking an envelope doesn't And obviously, taking an envelope doesn't commute if taking an envelope. So, this is kind of a problem because then we kind of don't, you know, I don't really know how to propagate. You always take these envelopes and then we have a lot of stuck and we don't know what to do with it. Now, one thing, of course, you can do is, you know, I showed you, so this thing has 243 different curves in it. What I can do is I fit, I use each one of them, right? So, in this paper, what we did is we literally, so what we do is we fit this to the We do is we fit this to some data, right? We get out two parameters. Let's say this is related to the norm and this is related to the width of the peak. It doesn't really matter, okay? We get these two parameters out. And then we fit it 243 times and we get lots of points here. So this is how we propagate. So this is kind of the best trick I can do. So then I do the envelope afterwards, but this is somehow just kicking the can down the road, because now I need to somehow envelope this. Somehow, envelope this thing here. And you know, maybe I can draw a curve around all of this scatter and call that the Fear Rev1 sigma ellipse or something. But then the minute somebody wants to use that thing to predict some other spectrum, I'm again stuck, okay? Because I don't really know what the correlations mean. So I somehow need to keep these 243 points always along, and somehow it would be nice to have something better. But this is sort of what we have. Alright, in the interest of time, let me skip this. And this is just an example of how you can sort of, at the end of the day, sort of a posteriori put a correlation model in so that you get the norm right and still don't underestimate some shape effect. So you can ask me if you want. This is the SOAT ST method if you note about it. But let me skip it. And then this is what Nick mentioned. The STXS scheme basically is the same thing on steroids, where you have lots of. Steroids, where you have lots of migrations between all of these spins, and you can come up with some sort of theory correlation model. All right, so last remaining minutes, let me tell you how we might make the future a little less ugly. So, as I mentioned, what we should be doing is estimating this f double prime. So, let me try doing that. Let me try doing that. So, we typically actually know a lot about this. Even without explicitly calculating it, I can sit down and think about what would I have to do in order to calculate the screen. I don't have to write down all the diagrams, but I know what the color structure is, I know what new channel is coming in, I know a lot about the structure of that derivative series without actually doing the calculation. And moreover, I don't want the very precise value for F double prime. Precise value for f double prime. I don't really want to have very precise. I just want to get an estimate of the size. And so, what I will do is I just parameterize this thing. So, I look at the structure and then whatever I really don't know, I parameterize it. I call it parameter so that the theory uses parameter. And all we need to do then is to figure out the allowed range, and again, more. Simplest cases, numbers get more complicated if they are functions. What's the advantage? Well, hopefully, it's obvious. The whole problem becomes more well-defined. Now we have parameters which have two values. In fact, those Newsence parameters are so cool, I can actually calculate the true value. If you give me a bunch of postdocs. So, but they have a true value, okay? And moreover, they are really power-mevelable, so I can say with full confidence that if this whatever. That if this anomalous dimension or something, right, appears in this process, and this process is the same number in both of these processes. So, if I vary that thing, I get a 100% correlation from that source of uncertainty between this process and this process. So, I know correlation, like everything is under this perfect, right? Because we have two parameters that we can propagate. You can put these into neural networks into Monte Carlos, you can do all crazy things with them. In fact, you can even constrain them with measurements, because they are parameters. Because they are parameters. There are typically a bunch of them, one of seven to ten. And then I'm going to ask you: you know, if I have seven parameters whose distribution I don't know, maybe I can already appeal to the central limit theorem and pretend that the final uncertainty is somehow roughly Gaussian. So I don't even have to have that discussion anymore. And you can even reduce the uncertainties because fundamentally, because what it means is I can actually now do half calculations. I can actually now do half calculations. I can go to the next order, I can put in all the ones that I know, and maybe keep the ones I don't know open. So I can improve, even I can use partial higher order results to improve my knowledge. And if you move them into your analysis, you can even, after the fact, reduce the fear of uncertainty in the measurement if some of these parameters can't be known, can get calculated two years down the line. Okay, so all of this is great, the price to pay. So, all of this is great. The price to pay is that, of course, no fine. So, the predictions obviously become much more complex because now I need to implement the entire next calculation, like the entire next order structure in terms of these unknown parameters. I don't have to propagate them through my calculation. So, this is... Okay, so here's an example how this works. So, the ZPT spectrum is again a spectrum. So, it's a non-trivial example because there's this PT dependence, right? But now, Right, but now I'm gonna throw a lot of theory at this, okay? And I can, if I if I consider just the low PT region, I can do an expansion where I expand in Pt over M. So this is an expansion which works very well in this region. So this is, I'm doing, so for those of you who know, I'm doing a resound calculation now, but never mind. What happens if this F actually factorizes like this, so the exponentials and then the exponent is factorizes, so I can actually. In the exponent is factorized, so I can actually have a limit where the pt dependence factorizes from the x-dependence, and so I can reduce the problem of having a function that I don't know at higher order into a couple of numbers. Okay, so I'm going to expand all of these guys, I'm going to estimate the F's, and so I have, you know, in that way I can completely predict the correlations in that PT range, which is actually crucial in this particular case, because this, you know, this is un. Because this is measured to like subcommittee subpersent position, and you can do a lot of fun physics with it. But you obviously need to understand the theory and certain things very well. So, this is an example how this looks like. So, all of these different lines are basically different parameters. This theory nuisance parameters. And you can actually see what happens. You can have the blue guy is fully correlated. There's this very large guy here, which is actually anti-correlated across the spectrum. So, you get all of these components, and they do what All of these components, and they do what they're doing. Okay, I should say I'm cheating here because I'm doing it in an order where I know the true values. Okay, so what I'm varying here is actually I set them to zero and I vary them within their twice their true value. So it's kind of just an illustration. So what I really need to do is estimate them, right? If I don't know the true value. And so now this comes back to some comment about you have a lot of. We have a lot of high-order series at our hands, right? And many things have been calculated to two, three, four, five loops. Okay, so if, as long as I'm talking about numbers, right, I can, so these are basically enormous dimensions and boundary conditions and differential equations, I can actually construct a generic estimate just based on, let's say, leading colour and NF dependence to estimate the size. And what I show here. And what I show here is a lot of perturbative series. So, all of these things are connected by line. Each line is a perturbative series. And at each order, I take the coefficient and divide it by my generic estimator. And I'm not changing that estimator based on the series. So it's really just the same estimator that I use for all of these. It depends on the order, because I know that at higher order, more things get more complicated. And you see that it works actually remarkably well. And you see that it works actually remarkably well, right? I sort of everything lies between 0 and 1, right? So I'm kind of confident that I could get an estimate which is really reliable in the sense, you know, I get the order of magnitude right. In fact, I probably even get it to 50% right. All right, so this seems to work very well. Now, this is numbers. Of course, eventually there will also be cases where there are functions, and then again, this is a challenge. Again, this is a challenge. Here's a case, just for illustration, and then I'll stop, where I can exploit some of the functional forms. So often when I have a function that I don't know, I kind of know it, let's say, in the limit x to 1 or x to 0. I know some limits, right, somehow there are. And so I expand in those limits, and based on those limits, I can somehow parametrize my function again in terms of a few numbers, right? And this is basically what's shown here. And this is basically what's shown here. So never mind what these things mean. This is the F double prime, this is the F triple prime, the dashed line here is sort of our approximated central value, and the yellow would be the uncertainty, and the red solid is the true value. So we nicely capture it. And at the time we wrote this paper, I should say that this one I should say that this center line was known, and this center line was not known. So we really went out of the limb from putting that. So we had that plot without the solid line in the paper. And then a year later, people calculated the thing, and I was very happy. So this in principle can be done, at least in cases where we have some sort of some sort of information about some of the events. It's the same thing for a different different case. This round this round case. What is L P that currently? What is L P? And say again, what is L P the leading power. So this is. Ask me later. This is basically the leading X to one limit, which obviously does not work good enough. But then you take the leading X to one limit and then you parametrize the next two leading limit and X to one. And those are the X1 and X2s. You know, by adding those, I kind of get basically, yeah, that makes it good enough. All right. So let me summarize. We are in Zero's Argyll business. Hopefully, we could convince you of that. Please be aware of the limitations of our current methods. They are not particularly reliable. And the most severe limitation is the lack of proper correlations. So do not rely on them for shape uncertainties. Obviously, the best way is to avoid them, but be careful because, in most cases, avoiding means you simply just cancelling them, and cancellations depend on correlations. Now we are back to the previous point. And okay, I showed you hopefully that there is some forkness to be made. Okay, thank you, Frank. So I'm just going to start the discussion session. Going to start the discussion session because we are running a bit behind on the stop. So we do