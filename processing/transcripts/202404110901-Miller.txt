Yeah, you both welcome to the last morning of talks. Um okay, so welcome to the last morning of talks. There will be another morning tomorrow, but it's gonna be the original groups explaining what they have done. Than what they have done. There will be more information about that later. And now we welcome Esther Meter. He's going to be talking about homological data and shift theory for multiple persons. Thank you for the invitation. It's wonderful to be back in Canada. I was in Canada in January of 2020, right before the pandemic. Since then, I've tried to come three times for conferences. I've been thwarted every time. Two times by COVID, once by weather. It is nice to find an unemployment to make it back. Attempt to make it back. And the weather really fought yesterday, so that was beautiful. Alright, so this is, let's call it maybe a review or introduction to homological algebra and sheaf theory for multiple persistence. There's a lot to get through, and it's sort of going to be, a lot of this is going to be review for some of you and completely new for others of you. So I'm just going to go at whatever pace. If you have questions at any time, just let me know. I'm happy to be interrupted and to explain things further. To explain things further. I expect sort of uneven audience levels in this thing. So, this is, I think there are 14 total slides, but 10, so some of these take two slides. Other slides have substantial amounts on them. But I'll start just by reviewing just so we can get the notation and everything persistent homology. In particular, real persistent homology and uh and then I'll talk about sheaves on postets. Um I was gonna not include a definition of sheaves, but I was convinced yesterday at the bar that I probably should put in a definition of sheaves. Problem put in the definition of sheaf. We'll talk about intervals and then stratification. These kind of alternate between talking about modules and talking about sheaves. So we sort of go module sheaf, module sheaf, and then we'll talk about the connections between them, how you view things differently from different sides, and how they play off each other. So persistent homology, we all at this point have seen it so many times this week, filter a topological space by subsets of a topological space. Of the topological space. The set of subsets is partially ordered, so you get a collection of subsets indexed by a poset. And then when you take the homology, you get persistent homology. I don't know how visible that word persistent homology is. Can you remember that? Is there a way to turn off the stage? Stage lights appear to be R on there all already. So a uh so this is a Q module. So this is again a family of vector spaces indexed by a postet. And the most basic thing you could write down is just what is this family? Well it's a you could view it as a graded vector space or you could like just a set of these vector spaces along with homomorphisms from one of these hq to hq prime whenever q precedes q prime in the whole set. Again, standard definitions. Standard definitions. And we want this to be commutative in the sense that if you go from q to q double prime, if you go through two different q primes in between, you get the same map from q to q double prime. That's the commutativity that's occurring here. So for example, this is just to get us all on the same page here. So we can encode a fruit fly wing. This is an image of a part of a fruit fly wing using two-parameter persistence. This was in conversations with biologist David Wooll. And the idea is that we have points in R2. Those are the sort of intersection points. View this as an embedded planar graph, if you like. And then you can expand those intersection points like we do in persistence. And then you have these parts given as Bezier curves, 0, degree, 2, or 3. And then we can fatten them up like this. And then for each choice of a radius, that's the R, and a semi-thickness, that's going to be the S. Semi-thickness, just like radius, right? It's half. Semi-thickness, just like radius, right? It's half the thickness of the red stuff. Then, what you do is you take a sublevel set which is near the edges but far from the vertices, so as not to think too much about this. This is what you get for yours. You take the red stuff, but then you remove the blue stuff. That's the topological space. So as S gets bigger, right, as the semi-thickness gets bigger, the red stuff gets fatter and you get a bigger topological space. But as R gets smaller, you get a bigger topological space, and that's why we have the negative R over here. The negative R over here. It means, oops, wrong button. That's why you have the negative R over here. That's telling us that we should be looking at the R is decreasing toward zero. So you get something that looks like this. So this is a toy model example. This is just a little bit of the fly lane. And so you've got here three connected components, one, two, three, until this thing, until the disk gets small enough, and then you hold one connected component. You can fiddle with this. You can fiddle with this. If you don't quite have an intersection point, then what you get is when the disk is really big, you get three connective components. That's this yellow thing. When the disk is quite small, you get two connective components like this. And when the thickness of the red stuff is fat and the disk is small, then you end up with only one configuration like this. Okay. Okay. If you change sort of the whole point of this biological problem, I don't want to go into it, but if you change this stratification just a little bit, then it changes the persistence module a lot, which is a good thing, because persistence is very stable. We wanted to make it unstable in a way that was compatible with the biology. Putting a blue dot here or not is like putting a knuckle in a finger. There's something substantial about it. So when you remove the knuckle, you want the persistence diagram to change along. And if you change what the stratification looks like, And if you change what the stratification looks like, the persistence module changes importantly. Fun stuff. Okay, so that's just the toy example. Now, this notion of Q module is essentially equivalent to a whole bunch of things. And you can just sort of peruse this list at your leisure for the next minute. But the point is that there's lots of ways to view persistent topology. No one of them is the right way. But each one has its advantages and disadvantages. Sometimes you can see things more clearly from looking at it as... From looking at it as a representation of an instance algebra. Sometimes you can see things more clearly by using it as a functor, from the pro set to the category of vector spaces. Sometimes you see it better as a sheaf, sometimes as a module. It all depends on what your goal is and what you're trying to do, and what field has which tools that you need. For instance, today you'll see commutative algebra has lots of fields for doing lots of tools for doing resolutions. We want to view these things as sort of objects in computer algebra for that. But in sheet theory, you get geometry that helps, and you have. Helps and you have the ability to sort of change the topology to work with the ordinary topology instead of the more rigid sort of post-set topologies. But then there's interaction between the two. Sometimes the field, the sheaf theory, maybe doesn't have all those tools built up, like Hilbert Syzygy theorem. You can go to the commutative algebra side, and it tells you how to produce short resolution. So that's what we're going to do. So she's on post-ets. And a postet, we've talked. Post sets. In a post set, we've talked about these of course, but notation, u is an upset if it's closed when it's going up. So anytime you have an element in there, then everything above that element is also in there. And a downset is closed when they're going down. So these are examples. So u here is upset. So you've got this blue curve at the bottom, which is sort of the generator of this thing. And anytime you have a point in here, you have all the points above it. Here's a downset. Anytime you put a point in here, you have all the points above it. Upsets and downsets. Here's an example in R3. I took this from a paper by Oklinkoff. The point was that when you take limits of these sort of piecewise linear things, you get this sort of algebraic thing. These things do occur in nature outside of just persistence. They occur in multiple persistence. Okay. So this postet has what's called an Alexandrov topology. And in this topology, the open sets are just the upsets. That's it. The open sets are the upsets. You can check that this. You can check if this verifies the hypotheses for topology. And now, what's a sheaf? Just for my edification, raise your hand if you know what a sheaf is. Okay, that is most people. I expected that. But I'm going to put this thing in down here anyway, just for completeness. So, a sheaf, it's a contravariant functor. What that means is to every open set, you associate a vector space. And if you have a bigger open set, then it's Bigger open set, then you can restrict to the smaller open set. That's what the contravariance is. It's the bigger maps to small. And that's called restriction. And it's a sheaf. That means that if you have an element in the vector space associated to an open set U, then you can recover that element by knowing what the restriction of the element is to all of the open sets in the cover of that open set. But it says that it's determined by local information. Nonetheless, you should think of a sheaf. Nonetheless, you should think of the sheaf in the same way that you think of a persistence module. You should think of it like to each point there's associated a vector space. It's just that for postsets, sorry, for topological spaces, you sort of should think about little neighborhoods. But then again, in postsets, the topology has these weird open sets, which are the upsets. So let's talk about why sheaves on a postset are essentially the same thing as Q modules. These are important. So let's actually go through the proof. So let's actually go through the proof. So, this is, I mean, this is, it's not that Justin Curry invented this, but it's a really good place to look. Because he has a nice, cleanly explained proof of this. So for each point in the Poset, each element in the Poset, it lies in a unique minimal open set. That's what's weird about the Alexander topology. There's a unique minimal open set. So if you're used to looking at, for each point, you take all the open sets, and then you take these direct limits and so on and so forth, well, there's a unique minimal open set. So if you want to know what's happening near Open set. So if you want to know what's happening nearby that point, just take the smallest open set and see what the value of your functor is there. And that gives you a vector space, and that's the vector space that you should associate to that point. That's how you get from a sheaf on the Alexandria topology to a Q module. You just take, for each point in the postet, you take the value of the sheaf on the smallest open set containment. Now to go backwards, what you have essentially is that then once you have the module, you have Is that then once you have the module, you have what the values of a functor are supposed to be, the values of the sheaf are, the sections over any sort of one of these minimal open sets. And then you have to reconstruct the sections over all open sets. But because it's a sheaf, you can do that. You just take a cover, and then you just write down what should be there. And algebraically, what should be there is the inverse limit of the sections of the equality sets. You already know. So, sheaves on the Alexander of topology are the same. On the Alexander topology, are the same thing as Q modules, and you should remember that the association is that you look at the values of the sheaf on the smallest open set containing a point. That's how the equivalence comes. Any questions about that? Okay. So that's, all right. All right, now, multi-parameter persistence. So suppose that you have a subgroup inside of a partially ordered real vector space. So that's actually the situation that we. Space. So that's actually the situation that we consider. I would say most of the time, there are a lot of people who like to do quivers, which aren't subgroups. So I won't say at all that this is all the time, but a lot of the time when we're considering data applications, what we have is we have C to the N, or R to the N, the usual partial order by component-wise comparison or something like that. You should think of these things as being subgroups. They be a sub-vector space, a sub-group, whatever, but inside of a partially ordered vector space, in this case, the vector space is just. In this case, the vector space is just Rn with the component-wise partial order, that is, component-wise comparison. So A is less than or equal to B if all the coordinates of B are greater than, sorry, each of the coordinates of B is greater than or equal to the corresponding coordinate of A. It's the usual thing, right? Here, you go out and call something like this. We also have z to the n, 10 and r to the n, in which case the positive cone. So the positive cone is just a set of elements that are. A positive count is just the set of elements that are greater than or equal to zero, that succeed zero. So in the r to the n case, this is just the set of elements with non-negative coordinates. And when you take z to the n, it's the set of, again, the elements with non-negative coordinates, but this time they're integer vectors. When you're looking at, so you can have an arbitrary affine semi-group. Just for, again, my edification, raise your hand if you know what an affine semi-group is. Not so many, actually. Okay, so I'll at least let me just review what that means. Let me just review what that means. It's a finitely generated sub-monoid. So think of z to the n, and take finitely many vectors and take the sub-monoid that they generate. So you're taking all non-negative combinations of those things. So the picture that you should have in mind is you've got a cone, and that cone has some sort of integer points along the ray. There's some integer vectors you're choosing in that cone, and you're looking at all positive linear combinations of those lattice points. Yes, yes, positive integers. Coefficients of those things? Yes, yes, positive integer combinations of those things. That's right, yeah. This is more general than that. If you look at systems of equations... Yeah, so if you take, say, N2, just the positive quadrant, you can leave out a single lattice point. And then you won't get. So I guess you could probably. Well, this is a graphization of much there, huh? This is a position of what the affine symbols are. Those are the saturated ones. So they're unsaturated. So normal affine semigroups are solutions to... Take a cone, you take the lattice points in it. Affine semigroups generally, you can have, anyway, but anyway, it induces a slightly different partial order if you leave off some of the lattice points from the cone. But it's such a minor, for this graph, for this context, it's such a minor difference, let's not worry about it. Think of this as being the set of lattice points inside of a. As being the set of lattice points inside of a cone, a polyhedral cone, because we want it to be fine-rejected. But there's no reason for partial order that you need the cone to be polyhedral. So you could do this over Rn, over Rn, and you can take any convex cone to be deposited. And that gives you a perfect partial order on the vector space. Now, it might give you a pre-order if the cone isn't big enough, but let's not think about that. We're going to assume that the cone is going to be full-dimensional inside of our vector space. So it is equivalent to think about Q. It is equivalent to think about Q modules in this setting, where you have a subgroup inside of a partially loaded real vector space. It's equivalent to think about Q-graded modules over this monoid algebra. So, this is, in the case of an affine semi-group, this is an affine semi-group ring. The spectrum of this thing is a toric variety. These things come up in algebra and computative algebra and algebraic geometry, sort of their mainstay of this kind of thing. So, there's lots and lots and lots of algebra and geometry around those, which is why it's useful to know that this influence is there. Components is there. In the special case where this is just Rn with the positive orthant, Mike Lesnick pointed this out in 2015. That's sort of once you look at the equivalence, like, oh, oh yeah, it really is equivalence. There's not much to prove. You just have to realize that the equivalence is there. So of course we ordered real vector space. So once you're dealing with real vector spaces, there's more topologies available to you. So there's the ordinary topology, which is the usual one that you think of. One that you think of. And then there's the conic topology, where the open sets are upsets in this vector space, but you want them to be also open in the ordinary topology. So this is sort of, it literally is the intersection of the ordinary topology and the allotted topology. Think of the topology as a collection of open sets. These are the ones that are both ordinary open and upset. The open upsets. So this in particular has omitted those minimal open sets. Omitted those minimal open sets for a single point. You don't have those anymore because those are closed in the ordinary topology. So you've eliminated those. The difference that that makes is essentially that you've sort of set to zero all of the ephemeral modules. This is a result of, I think, Berkup and Fatima. But anyway, so this makes the topology sort of, the Kanak topology is kind of close to the Alexandrov topology. Close to the Alexandrov topology, but it's not the same thing. So, what happens in this setting is that these are equivalent to each other, but not to just V modules. So, throughout this talk, if I write a V, it means partially ordered real vector space. If I write a Q, it means an arbitrary possibility. So that's just to get things straight about which level of generality we're talking about. So, if you have a. Okay, so I expect that people will look at this line, line four, and go, what? 4 and go, what? The point is that you shouldn't pay it. You should look at line 4. Note that there is another theory out there that deals with sort of partially ordered real vector spaces. You take their cotangent bubbles and view those as symplectic manifolds. And then you do analysis on those things. And there's certain analysis that you can do, take micro support, negative polar cone. There's this stuff that you can do, but the point is it's equivalent to do it by just taking sheaves and a conic topology. When I write this word derived, what that means is you. This word derived, what that means is you often, for persistence purposes, want to view objects in an abelian category, you instead take the derived category, that abelian category. So we want to be viewing complexes of sheaves. We could do the same thing for modules, take complexes of modules, and we do often do this kind of thing in persisting. So when I write sheaf, if I forget to write the word derived sheaf, sometimes by sheaf, people who work in the derived category all the time, they just call it, when they say sheaf, what they really mean is derived sheaf, which is actually a complex of sheaves. Is actually a complex of sheaves up to the equivalent, anyway, the object in the derived category represented by that complex. But I will try to be more precise about that and actually say derive when I mean an object in the derived category. So these are equivalent to each other, but they're not quite equivalent to the sheaves. Whoops, this numbering didn't come up right. That was, I don't know why that happened. But these are not quite equivalent to being a V module. They're slightly different. They're slightly different. And that will come up. Won't come up so much as we're going to have to deal with the fact that they're not like to say. So what's the idea? The idea in general is that we have these nice sheaves, these nice persistence modules, which we've been talking about, of course, all week, which are these interval modules, or spreads, or indicator modules. The point is that you've got some subset of your post-set, and you want Some subset of your poset, and you want to put one-dimensional vector spaces on that subset in zero-dimensional, and just zero everywhere else. These are what I would call indicator modules, I'm sorry. So, for me, an interval is going to be a convex connected subset. So, that's going to be an intersection of an upset and a downset, but then I want to take a connected component of that. So, it's going to be connected in the sense that there's a zigzag chain of comparable elements from A to B, and it's convex in the sense that whenever you have A and B, you have everything that you want. And then I can make what's called an indicator module. It's what I just described. You take a one-dimensional vector space, put it at every point along your subset, and then put zeros everywhere else. That is not always naturally a module, a Q module. So in the settings, when you actually get a module out of this, it's going to be because the subset S is special. It could be one of these intervals. It could be an upset, which is also an interval. A downset, which is also an interval. It could be sort of an interest. It could be sort of an intersection of an upset and a downset. That's not an interval. So intervals can look like, in R2, intervals can look like this. They can be an upset, it can be a downset. This is an intersection of an upset and a downset, and it's connected. This is also an intersection of an upset and a downset. In fact, it's the intersection of this upset with that downset. But it is not connected. The reason it's not connected is because you can't make a zigzag chain to go from this point up here to this point down there. Here to this point down there, because this sort of angling here points upward a little bit. You can take zigzags and approach it, but you can never actually reach this intersection point over here. Even though it looks connected in the ordinary topology, which it is, it is not connected in the postet, because you can't make a zigzag chain from up here to down here. But this is still an intersection of an upset and a downset, and so you can still make a module out of it. Professor, what do you mean by zigzag chain? I mean you go up and down. So if you just you like this says comparable elements. Like this says comparable elements. So you're going to go from V up to V prime and then down to something. So you can go from here. You can go up and then down and then up and then down and up and then down and up and down up and down. It's just a sequence of going ups and downs. You can always make it literally alternating because if you go up twice you can just skip. So it's just you go up and then you go down up and down. Okay, so in one parameter, as we all know, integral modules. All know interval modules are indecomposable. This is actually true in multiple parameters. But the point is that there are two avenues. So, when you're looking at multiple parameters, you can either study indecomposables and then look at decompositions into indecomposables. You say, all right, sort of the salient property of an interval module in one parameter is that it's indecomposable, and then you run with that. Or you can say the salient property of the indicator module is that it's got dimension one everywhere, and then run with that. One everywhere, and then run with that. And these give you very different kinds of considerations when you're talking about multi-parameter persistence, because there are lots more indecomposables than just the indecomposable interval modules. So you get sort of different considerations. And there have been, of course, people which have time to do both of these things. And what we're going to do today is the second one. We're going to study indicator modules and how they relate to arbitrary module. So, how do you define sheaves constructed from interval sheaves? That's the question. So, we have these interval objects, which we know about from module LAN. And you can think of, because of the equivalence of the categories, there's these interval objects in sheaf LAN. But how do you construct arbitrary, not arbitrary, but what kinds of sheaves can you construct in nice ways from interval type objects that are sheafy? That are sheathy. So this involves stratification. So partition of a subset of the vector space V into strata. So these strata are just subsets. So it's called a sub-analytic if the strata are sub-analytic. So again, just for my edification, raise your hand if you know what sub-analytic means. Okay, not so many. So what it essentially means is that, so an analytic subset is just defined by inequalities among analytic functions. The problem is that that collection of The problem is that that collection of subsets is not closed under things like projection. So, what you do is you take the smallest collection of sets that you can get by taking the analytic sets and also allowing projections. So, you take, they have to be closed under projection and complement and finite union and this kind of thing. So, you should think of it as basically analytic subsets, but made a little bit bigger because projections of analytic subsets don't have to be anything bigger. So, piecewise linear, so we'll call it piecewise linear if each of the strata is a finite union of convex polyhedra. These are examples of sub-analytic things. Piecewise linear is a subclass of sub-analytic. It's a subclass actually of semi-algebraic, which is a subclass of sub-analytic. So, this is a stratification, sorry, partition of subset is, this is just a partition into subsets. I haven't said what it's called. Get to subsets. I haven't said what a stratification is yet. So, a derived sheaf, so just think sheaf, don't worry about the word derived here. If derived makes you happy, then think about derived. If it doesn't, then just do it has a subordinate sub-analytic subdivision if the sheaf is constant on every stratum. So, what that means is you're thinking of your space as being this vector space. So, now we're in VLAN, not Q-LAN, not arbitrary posset, but as we're talking about real, partially ordered. We're talking about real partially ordered vector space. And you're thinking of there being subsets. And think of the piecewise linear case. You've got a polyhedron there, and a polyhedron there, and a polyhedron there. And the vector spaces that you're associating to all these points, on the interior of this polyhedron, you want it to be constant, the same vector space. On the interior of this polyhedron, you want it to be the same vector space. The interior of this one, same vector space. Of course, those vector spaces can change depending on which polyhedron you're looking at, but we want our stratification. So it's going to be called subordinate and stratification. Be called subordinate. The stratification is subordinate if your sheaf is constant on every one of the strata. So that's how you sort of build sheaves out of interval things. Interval objects are objects that are constant on regions of some sort. And now we're trying to build our sheaf by saying, well, let's take where our sheaf is and let's subdivide the support of our sheaf into regions on which it is constant. That's beyond. Constant. That's the other thing. Of course, when you're looking at PL, that's sort of a much more rigid thing to do than sub-analytics. Sub-analytic sets can really be pretty general. No fractals or anything, but lots of lots of sets. Lots of sets. It's piecewise linear if you're just in a situation where, again, the positive cone is polyhedral and F has a subordinate piecewise linear. PL subdivision. That's the example I was talking about with the polyhedron F. I was talking about the topology graph. So that's how you can try to construct things on the ordinary topology. And now let's try to do it on the conic topology. So a closed subset has a conic stratification. So this thing is just going to be, it's going to be a, you're going to express your subset as the union of strata. And it's going to be, these subsets are going to be sub-analytic, pairwise disjoint. Pairwise disjoint. Again, sub-analytic, just view it as in a nice class of subsets. So no fractals in this kind of thing. We want them to be pairwise disjoint, so this is a partition. So we're partitioning S into these subsets, these strata. And we want each stratum to be locally closed in the conic topology. Remember what that means? What does locally closed mean? It means it's the intersection of a closed set and an open set. But remember what that means? It's the intersection of an open upset U. Upset U and a closed downset D. This is why, so this is sort of dual to the picture that you usually see of having sort of closed lower boundary and open upper boundary. But that's because when you take direct limits, you want to have all the open sets beneath you. So this, though the point is that locally closed in here is not a complicated condition. It just means it's an open upset, intersecting closed downset. And you want this here. Want this union, it's a partition, but you want it to be locally finite in the sense that every point, a neighborhood of that point, only intersects finitely from any of the strata. You don't want one of these things where you have sort of infinitely many rays coming out of a point or something. So a conic stratification is subordinate to a sheaf if S is the support of the sheaf. And each, okay, so again, ignore the parentheses if you don't want to think about derived stuff. You don't want to think about derived stuff. So, you want the sheaf to be locally constant on every strat. So, that's the picture that I was telling you before. It's just that now we're talking about stratifications where the strata are kind of cone-shaped, or they come from cone-shaped. They don't have to be cone-shaped at all, because they can upset and downset. You can really get very interesting shapes that way. But they all come from conically shaped subsets. Questions about this? Yeah? Why do you need sub-analytic? Why do I need sub-analytic? Why do I need sub-analytic? Sub-analytic? Because there was this. So, first of all, it's a good class of sets to work with. It's sort of an O-minimal structure and allows you to do sort of manipulations that are the kinds of manipulations that we want to do in persistent homology and in applied topology in general. But let me not scroll back to it. If you remember, there was an equivalence of things misnumbered as 4 and 5, which should have been 5 and 6. And the sub-analytic allows you to say something about the sort of analytic side with the micro support and everything like that. Support maybe like, yeah, so there was one of them that said you're looking at conic sheaves, and then there was one of them who said micro-support, and you need sub-analytic to even talk about the micro-support session. But the reality is, you could do this. So maybe your question is more, why shouldn't I choose semi-algebraic or piecewise linear? And the answer is, I could. But sort of the most general kind of setting where you want to talk about this kind of stratification is in a sub-analytic context, maybe somewhere. Context. Maybe some more general o-minimal kind of context. Did that answer the question? Yeah, I think my question was why not more general. Why not? More general. You can. You really do need it to be in some sense o-minimal. You need to be able to do these basic operations of projection and complement and finite union and this kind of thing. Because it c it comes up when you're trying to to do operations on cheeks. Like you need to be able to do basic things, yeah. Oh, this sub-analytic condition is analysis of RPD analytics. When you you can get closer to the boundary with an exponential Boundary with uh an exponential speed. I do not want to talk about that. I essentially want to be able to have uh the CDH inequalities. The reality is when you take the union of two things, you want to be able to control at which speed you are getting close to the boundary in terms of the way you approach the boundary of each piece. And the only way to do that is to impose on several interconnection. Thank you. I didn't know that. Does that answer the question? Or a list. I think there are some things here I don't understand, but it's fine. Yeah, yeah, we can talk about them at length after. When you're dealing with the objects in the derived category, you want the homology sheaves to be stratified as well. So we've just said how to sort of how to build sheaves in the conic topology from these interval-like objects. Now let's talk about it module. About it, module land. So, in module land, a module over an arbitrary poset now. So, now we've, so one of the advantages of working with modules this way is that you get generality. You cannot talk about any postet. It doesn't have to be, there's no topology on the poset. It's just we have any poset q. And it has a constant subdivision if it's partitioned into constant regions. Constant regions, each of which has a vector space associated to it. So, this is imposing the local constancy on purpose, right? You're going to subdivide the post. Purpose, right? You're going to subdivide the postset into regions, and then you're going to say, All right, I'm going to put one vector space on this region, another vector space on this region, and so on. And I want those to satisfy a null monograming condition. What this means is that you go, if you go, so you take the vector space on this region, let me draw the picture, there we see, if you take the vector space on this region, so this whole region has a K3 associated to it. This is the same picture that you saw over. So there's a three-dimensional vector space here, a single three-dimensional vector space that's associated with this whole yellow region. Vector space that's associated with this whole yellow region, and a single two-dimensional vector space is associated to this whole olive region, and a single one-dimensional vector space associated to the blue region. If I pick a point in here, the point is there's an isomorphism from this K3 to that, to the vector space sitting there, then I can use the post-set relation to go from here unto there, and then that one's isomorphic to K2. And I want that isomorphism, so that map from K3 to K2 that I get that way, to not depend on which representative in the yellow region I chose and which followed point that it mapped to. That's what normal. A point that it maps to. That's what no monodrome means. It literally means that if you do this, you don't get different maps from K3 to K by going around a loop different ways. It really is no monodrome in the sense that you're used to no monotromy. I call the module tame if it admits a finite constant subdivision and the vector space dimension. Any module that admits a constant subdivision be a trivial partition? No. No, no, no, no, because. No, no, no, no, because sorry, sorry, what kind of module? Partitioning the singletons is also a constant subdivision. It is, but it's not tame unless it's a finite constant subdivision. I'm still. Yes, yes, yes, no, absolutely. Yes, that's correct. You can always take a constant subdivision which says every point is its own constant region. That always works, but it doesn't give you tameness because we want tameness, we want a finite constant subdivision. It's the finiteness that we're really interested in. Yeah, excellent. And we want the dimension to be finite, so there's the point-like finite dimensional. To be finite. So there's the point-wise finite dimensional comes in there too. And I want to call it sub-analytic or piecewise linear if I'm in the V setting of a partially ordered vector space. And the regions themselves are sub-analytic or piecewise linear. So here's an example of a constant subdivision. So here we're going to take the vector space. So in the plane, we're going to take the whole plane with a one-dimensional vector space everywhere. We're going to take the direct sum of the module that has. The direct sum of the module that has just a vector space of dimension one at the origin. So it looks like this. It looks like this. Here's the whole plane, it's dimension one everywhere except at the origin where it has dimension two. And this has a constant subdivision that looks like this. There's a single isolated point at the origin where the vector space dimension is two. And there's the rest of the plane where it's just got the k associated to all those points. Now you'll notice that this doesn't respect the partial order in any way, right? There are points in this big region that are less Points in this big region that are less than this dark blue point, and there are points in this big blue region, the light blue region, that are bigger than this point of the origin. So, in some sense, it doesn't respect the partial order that way. It's just any constant subtitles. Now, you can make it, here's the theorem, you can make this into something that does respect the poset. So, a complex of modules over a poset, again, if complexes don't make you happy, just think of it as a module, it's fine. Over a poset has a finite Over a postet has a finite encoding, but that's going to be as a map from your poset, whatever your poset might be. Think of Q as being R2. And you're going to map it to a finite poset. And what you want is that you want that to be a poset morphism, so it respects proceed and succeed. And what you want is that your module is the pullback of something that was defined over P. So then the encoding is sub-analytic if its fibers are. So here's the idea. If its fibers are. So here's the idea. Here's an R2 module. It's the same one that I drew before. It's the intersection of an upset and a downset, right? So here's what I'm going to do is I'm going to now encode it. This is an R2 module. I'm going to take four regions here. So the map from R2, let me take the map to simplify things. Let me just take the map from this quadrant R2 to just four element lists. So don't think about the stuff outside that quadrant. It'll just complicate matters. So there's a map from this. So there's a map from this sort of quadrant poset here to this four-element poset. Here's the four-element poset. So every point in this region is, so it's a posset map in a sense that if something, if you have a comparable pair, then when you take the image, they're also comparable. So it says that if A is less than B, then in the postset P, they're images. And in the postet P, their images are also common in that sense. So, this is a finite encoding of this module because, again, on this whole big black region down here, you're getting a zero-dimensional vector space. And on this yellow region over here, it's getting a one-dimensional vector space. On this yellow region here, it's getting a one-dimensional vector space. And on this black region, it's getting a zero-dimensional vector space. You could actually have done it with only a three-element coset. You could have actually put this whole thing into a single document. Could have actually put this whole thing into a single dot with a one-dimensional vector space there, and that would also confident. Now, so the idea is that, so what does pullback do? Pullback says if you gave me 0, 1, 1, and 0, then I can pull back and just copy this vector space into this entire yellow region, which is the fiber of that point. And then copy this vector space into this entire yellow region, and copy this vector space into this black region, and copy this vector space into that black region. That's what pull back does. That's what pullback does. You can look at the notation, but that's what it says to you. So, in this example, is there any reason to consider the four-element postet value from the three-element postet in the console set? Is there any reason? I don't know. I have to think about that. So, if you're trying to present it, for data structure or something like that, I would probably use just the one region. But, I mean, it might be just what your algorithms gave you. It doesn't have to be the finest possible particular. Finest possible partition. It doesn't have to be the, so it's just any map to a finite poset. It's possible that two of these regions have the same vector space on them, which they do here, even though you chose to separate them and map it. It can happen, actually, in DS, just to say, it can happen that you're forced to sort of split constant regions because they don't respect poset structure. So the fact that there are two constant regions at the same vector space is. Um uh it it can happen before post-everything's too when you get when you take the pullback don't you get more maps? Don't get more don't you get more maps I mean some of the some of the points in certain regions don't have arrows to others at the other regions left-hand side. Ah, yes, that is true. Um let me think about uh Yes. Oh, any map that you have. So, yeah, so any map that you can have can be realized by going. So, for instance, if you have a point down here, then it maps to something up here. Is that what you're trying to say? Yeah, that's true. The how do I say? Yeah, that's what I was going to say. Once you're inside of a region, you need to use the isomorphism. Once you're inside of a region, you need to use the isomorphism of that region to fix that problem. Maybe this is the wrong question, but where does the intersection go for the benefit of the main thing? Oh, you pick one. You should pick one. Yeah, just pick one. So there is a choice involved. There is a choice involved for this particular one, yes, that's right. So being tame is the same thing as being finitely encodable. Is the same thing as being finitely encodable. What that means is simply that if you gave me a constant subdivision, like the one where you have the point in the middle of the big region that didn't respect the postet, you can force those, you can take a finer subdivision where the constant regions do respect the post-set length. And the way you essentially do it is to take all the upsets and downsets generated by our constant regions and then take various intersections of them. So it's not complicated to do, it's touchy to prove in a post-set. Prove in a post-set. It's all elementary post-set stuff, but it's sort of detailed post-etheoreic proofs. But all elementary. And so you can do the same thing. If everything's sub-analytic or PL, then these operations of taking upsets and, oh, these operations of taking upsets and then taking the intersections, those all preserve these sub-analytic or PL. But they really do use the fact that projections preserve the class of regions that you're looking at. Can you explain? I mean, this this sounds very natural to me, this theorem, so I I don't even recognize what what are the subtle details that you have to do in the proof. Maybe this is a bad question, but what is hidden that makes this not obvious? Yeah, so when you take a constant region, it's because constant regions can have this non-poset sort of looking way of being next to each other. They can be next to each other, sort of non-poset. Other, they can be next to each other in sort of non-posite ways, and you have to prevent there from being sort of unexpected loops and comparabilities. So, you have to essentially subdivide things finely enough that you get that sort of every element in this region is either less than or equal to or incomparable to everything in this region. You don't end up with regions bending around each other and having loops and this sort of thing. I'll make a post-head off on that. Yeah, and it it, again, the answer turns out to be pretty straightforward. The answer turns out to be pretty straightforward. You take your constant regions, you just take, let's say, the upsets they generate, the downsets they cogenerate, and then just sort of take all the various intersections of those things. And that turns out to work. Proving it's annoying but not, it's still elementary. It's annoying, but still elementary. So constructability. So that was over in the algebraic sense. So that was tameness for modules. We talked about sort of construction, we talked about. Sort of, we talked about sort of building comic sheaves out of interval-like objects. Now, let's talk about the ordinary topology. But this is the usual notion of constructability. So, let's fix a real analytic manifold and a derived sheaf. Derived doesn't make me happy, just a sheaf of vector spaces. So, a sub-analytic triangulation. This is going to be sort of review, but now we're allowing the, now we're taking the subdivisions to be sub-analytic. Now we're taking the subdivisions to be sub-analytic triangulations. What that means is you have a bunch of simplices. Each simplex is going to be an analytic map from a simplex into your space. This is the usual notion of a single simplex, but it's not analytic. And we want that we have a homeomorphism from our symptomicial complex to our subspace Y inside of X. And we want that the image of every open cell is sub-inlect. What it means to have a sub-analytic triangulation. Solid in a triangulation. And it's subordinate to our sheaf. You have the same thing that we said before. It really is the same thing. You just want the restriction. You want the simplicial complex, the support of the simplicial complex to contain the support of your sheaf. And you want that the sheaf is constant on every kind of symplication. Yeah. Do you know how easy it is to construct these other triangular shapes? I have no idea. I mean, it's not, not that I have no idea. There are all sorts of Not that I have no idea. There are all sorts of papers on sort of trying to triangulate semi-algebraic variables and these kinds of things. There are papers. It's not easy to do. It's doable, but it's not easy to do. I mean, at least if you don't know, but just point that it exists that if you design a few parts of specification. In semi-algebraic cases, where there's some hope of doing sort of general algorithms, there are bounds on these things in terms of the degrees of the polynomials and various other things involved. It's hard, but it is doable. But I'm not going to suggest constructing these things. This is for definition, and the point is going to be that you can ignore this stuff. Point is going to be that you can ignore this stuff because it's going to be equivalent to doing something in a comic topology instead. That's the idea. This is just what constructs, this is background. If you ever want to know what a constructible sheath is, you are now seeing the definition of constructible sheath. That's the point of this. So it's sub-analytic constructible if it has a sub-analytic triangulation that's subordinate. And you want all of, yeah, point-wise finite dimensional. So it says it's pointwise finite dimensional, and it has a subordinate sub-analytic triangulation. Coordinate sub-analytic triangulation. The point is this triangulation doesn't respect the post-set on the vector space because it's just not part of what the input of what's going on here. So there are three types of stratifications into intervals that we've seen so far. There's the conic stratification. Remember? That's where you have the locally closed business intersections of upsets and downsets. There's the constant subdivision, and there's this sub-analytic triangulation. And we want to know about the relations between these things. We want to know about homological consequences or characterizations. To know about homological consequences or characterizations. We already have the tame is the same as finitely encoded, for instance. And what's an issue here is that this conic topology is too coarse to allow triangulation. If you think about what the conic topology looks like, all the sets that you're intersecting have to be sort of upsets that are shaped like the cone and downsets that are kind of shaped like the cone. They're not sh exactly shaped like the cone, but it doesn't allow you to make arbitrary triangulations this way. It's just not flexible enough. Flexible enough. So the point is: can you detect constructability without refining the codic topology to the ordinary topology so as to make these kinds of triangle dimensions? That's the question she wants to be talking about. And the answer is yes. You can do constructability without having to construct sub-analytic triangulations. Okay, so there's not so much time left, so I want to go through this. So much time left, so I want to go through this stuff pretty quickly. So, when you present, so we need to find ways of expressing modules. So, usually we write down free presentations or injective co-presentations and that sort of thing. Here, I want to do something that's more compatible with sort of topological data analysis. Let's make an upset presentation. So, it's a homomorphism from a direct sum of upsets to another direct sum of upsets. So, if these were, so in particular, free modules are upsets. So, you can think of this as a So you can think of this as a free presentations are a special case of this, but this is allowing the boundaries not to be generated by single elements. And the co-kernel is supposed to be your module, and dually for a downset co-presentation. That's just a reminder of the notation. So here's what this looks like in one parameter. You're taking this upset, and you're mapping to it from this upset, and then the code kernel is this interval over here. You can do similar things. You can do a similar thing in R2. You take this upset, and this is now a sub-upset of that with an open lower boundary there. And then the co-kernel is the intersection of the, so it's everything that's between this blue boundary here and that red boundary. Again, you're mapping this one to that one, taking the co-kernel, so everything that's above this red thing maps to here and then it goes to zero. So, you can do the same thing for a downset co-presentation, just change all the upsets to downsets and reverse all the parameters. A fringe presentation is much more, I think it's the most interpretable. So it says that think of these upsets as you have classes, homology classes, persistent classes that are born along the boundaries of these upsets, and then they persist sort of arbitrarily up the upset. And then when she says, okay, where are they going to die? And then when she says, okay, where are they going to die? These are the deaths of the various classes. And the point is that any class that's born here has its death now in linear combinations of these things, right? It's not that you can separate, so in one parameter, you can always look at the birth of this class and the death of this class. So it would just be a diagonal matrix with births here and deaths there, that particular interval. But in multi-parameter, you have to allow yourself to have an arbitrary matrix that relates to subsets and class sets, which are the births and the deaths. So this is a So, this is a upset presentation. Now, let's talk about fringe presentation. Here, you're going to take this upset, this class is born here, and then it persists forever until you require that it actually die at this point. And that's given by a downset. So, the intersection of that upset and that downset is given by saying that, well, the image of this, when you take a map from this upset to that downset, the image is this interval. And that's a fringe presentation of interest. So, it's a birth-death presentation. Fringe really means birth-death. Fringe comes from the fact that it's Fringe comes from the fact that it's a portmanteau of free and injected fringe. But also because it's talking about the ends, talking about the end points. So it kind of works both ways. In this case, the free presentation that I want is this one. The class is born along this blue stuff, born along the blue stuff, buys along the red stuff, and that's just given by a map from the blue one to the red one. And the image is this. This is a free transition. Not a pre-read. Not free, right? It's not just an entry. It's offset. It really should be buttons. That's right. These are not free. These are not projective. They are definitely not personals. These are arbitrary upsets. Yes, that's right. If I replace your possibility with a finite possibility, this concept, does it translate to a single generator? No. No, no, no. I'm still allowing myself to use an arbitrary upset and an arbitrary downset. Upset and an arbitrary downset. So I don't require. Multiple generators. I do. Yes. For this kind of presentation. But using a single generator does give you an upset. So free presentations are upset presents. So if you take sort of, I mean, fringe presentation, you could do fringe presentation with all principal upsets and all principal downsets. It's perfectly allowed. But you are allowed also to have more junctions. Question on your second it can be a little bit confusing as a Be a little bit confusing as a term. There was a paper that was submitted at the Patagonia that was computing free choices. For them, it was really, you know, actual free and an actual choice. Ah, I see. So you just had to correct the algorithm in this. I see, I see. But you know, like this is, you know, because most things come from the binary thing. That is in fact how the proof goes. So the proof of the existence of these things for TAME modules is exactly that. You take a postset encoding, take a free actually you have to then embed the postset into Z to the N, which you can always do, and then you pull back a free resolution. When you pull back a free module from Z to the N, you don't get something that's free, but you still get an upset. And that's what happens over time. The pullback of a free thing is an upset thing, and that's why you get these things. That's why you get these responses. Are they like limits of projectives or something like that? There are unions of projectives always, because you can always just take the generators that sit along here, but they're very uncountable unions in that sense. So this one's like a flat cover and take it out of the corner. Oh, I'm under. Yeah, but they're not directed limits. They're just yeah, so they're definitely not going to be flat like that. Yeah. Okay. The point is that you can, for data, for data analysis, you can write this down by expressing what the upsets are, what the downsets are, and then writing this matrix of scalars. And these are the birth upsets, you put these classes being born there. These are death downsets, the classes die there. And then you have these scalar entries that relate these forms. And that just gives you a map this way. What did I want to say about that? What did I want to say about that? Yeah, often in data situations, these upsets are semi-algebraic and have equations for the lower boundaries. And you have equations for the upper boundaries of the death downsets. Okay. So there's not enough time to go through all this stuff in detail. There's a notion of tame morphism. A tame morphism is, well, a tame module is where the module is constant on every one of these regions. A tame morphism is where you have two modules. Each of them is constant on regions. And on the region, Constant on regions, and on the region, the morphism is constant as well. That's what it means to have a two-morphism. And it can be sub-analytic or PL, this kind of stuff. All this stuff is just making precise the notion that homomorphism between modules should be tame, again, if there's a subdivision of the base, of the postset, meaning which the homomorphism is constant on each of those regions. Upset resolution is a complex of upset modules. A complex of upset modules. Just like a free resolution would be a complex of free modules. This is a complex where each homological degree is a direct sum of upset modules. Same thing for a downset resolution. These are called indicator resolutions in general if you want to not have to worry about it as an upset or a downset. It's finite if it has finitely many sum ands, and it's sub-analytic or PL if all the objects are, all of the upsets and downsets are sub-analytic or PL. So the Syzygy theorem says that the following are equivalent. Says that the following are equivalent. So, this notion of tameness, it has of this homological characterization. Having a finite upset resolution is the same thing as being tame. Remember, tame says that you have finite constant subdivision, which we already said was equivalent to having a finite encoding. And it's the same thing as having finite upset resolution, finite downset resolution, finite fringe presentation, finite constant subject. These are all the same notion, which I take as sort of meaning that this is the right notion. That this is the right function. Yeah. How do you know that such resolutions exist? How do I know that they exist? So if you have a tame module, you construct it. So that's what the proof was before. What you do is you take a finite encoding, right? Because your module is supposed to be tainted, so it has a finite encoding. Then you embed that poset in Z to the N. And then you sort of extend the module to Z to the N, take a free resolution there, which we have by Hilbert Syzygy theorem, and then just pull it back. And when you pull back, you don't get a free resolution anymore, but you still do get an. Resolution anymore, but you still do get an upset resolution. That is the proof. It really is that short. After you've done the hard work of showing that finite encodings exist in the school. Sorry, so there is some non-uniqueness on the surface? Yes, there's nothing about uniqueness. But when it comes, when you do this pullback from Zm, is the non-uniqueness showing up in Zm? So if you take some minimal resolution, pre-resolution in ZM? No, there's a sense in which I'm up to isomorphism. Sense in which I'm up to isomorphism, there's a unique minimal resolution there in Zm. But there's non-uniqueness in how you embedded the thing in Z to the M in the first place. But for a specific embedding? For a specific embedding, you can pull back a unique minimal thing, but it gives you an upset resolution which you might not want to consider to be minimal in some sense. So you might have to minimalize. There's a notion of minimality, that's a whole other can of worms. You can define minimality of You can define minimality of upset resolutions functorially. There's a way to functorially define tops and sockles. It's actually very relevant to what you're doing in your working group, actually, these functorial notions of minimality and what's the top and what's the soccle. And how do you tell whether a map between two modules is minimal in the sense that it maps the top here, I suppose, the top there, or surjectively? Yeah, so there are notions of minimality, but you don't really see it from this proof. Proof. And it remains true when you put sub-analytic in place of tame and finite or PL. So the summary here is that tame means stratified by intervals, that's the constant subdivision. It's the same thing as finitely encodable. That means constant subdivision that's sort of commensurate with the postat. And it's the same thing as having finite resolutions by intervals. And it's the same thing as having a nice finite data structure by the normal matrixes. But just like I'm concerned, so the first two elements is the correspondence as easy as when you take a finite encoding with the button due to it? Yes. Yes, they are. Finite encoding is a particular constant subject. Didn't you have an example where you said you could have three regions instead of four? Yes. And one of those were was not general, you have three regions. Yes. Couldn't that be a problem for this? No, it just says that it has. Okay, so if you take a diagonal, just a line, but all incomparable elements. Yes. Then you don't have intervals. So that's not any. No, you have uncountably many, but there's not an equal. Yes, you're allowed to take it because it's still a constant region, but that constant region is a disjoint union. It's a totally disconnected subset of the poset. Okay, so I shouldn't. So in Thames, there is this finite thing, and I should not interpret. Is this finite thing, and I should not interpret that as a finite number of intervals. That's correct. Yes, it says finite number of constant regions, but not necessarily intervals. Absolutely, that's a very good point. Okay, that's how the proof goes. I've already talked about it now. I'm basically done, so I can beat basically done. The next couple of slides, all they do is say that you can lift this to the sub-analytic setting where you're looking at the conical topic. When you're looking at the conical topology and sub-analytic constructability and so on, that's a special case because V there is a partially ordered real vector space. The point being it's a partially ordered, right? So you can still talk about tameness there. So the theorem works for that. If you then interpret what's going on there, what you find is that if you have a sheaf that's pulled back from the conical topology, then it's tamely resolved by conical interface. So the point as a Point as a corollary, what you get is the following or equivalent. So if you have your sheet is sub-analytically constructible after pulling back to the ordinary topology, again, if you have a conic sheath, it's a sheath on the conic topology. You don't have all of the open subsets. You just have the ones that are sort of intersections of upsets and downsets and this kind of thing. But sorry, you only have the ones that are offsets. But so if you can pull it back to the ordinary topology, what that says is allow yourself to use all of the open sets. yourself to use all of the open sets. Now you can ask whether it's constructible and whatever answer you get, the point is that you could have figured it out by just looking at the upset resolutions and downset resolution system. You did not have to actually pull it back to the ordinary topology. You could have done it entirely in the conic topology without asking about self-arrowing triangulations. So this answers your question about how hard is it constructed. Well, you don't need to. You just have to construct these upset or downside things. Very good. So, the rest is just sort of special cases. So, Kashibura and Shapira conjectured things in various special cases that there exist stratifications that do so on and so on. And they're all just corollaries of this equipment. So, you can construct, if you have a sub-analytic sheet that's constructable, then you can construct this stratification of the support. It's a PL sheath, and you can construct this particular representation. You can construct this particular representative in a derived category. And the reason why they wanted these things is because for data analysis, you want to be able to approximate things, approximate sheets by piecewise linear things. But then once you do that, you want to have good computable representatives to work with. And this was saying that, yes, in fact, you do have those good representatives to work with. See, what they had worked with was the constructible derived category, saying that all the homology sheaves are constructible. But what they wanted was representatives. But what they wanted was representatives that were, you take a complex where the complex itself is constructible. So you want to have a constructible representative, and you know, all you really know is that it's that it's a homology. It's constructible. That's what this does. This is a G theorem user. So looking forward, there's a bunch of open problems. My favorite one is this one. What does the top 100 bar lengths mean in multipersistence? So we used this for single persistence. We had brains. For single persistence, we had brains and we just did a filtration, and we look at the barcodes, and we just took the top 100 bar lengths, and we listed those things in decreasing order. That was our summary, and we just did statistics on that, and we got some really good results out of it. What do you do if you have multipersistence? What's the top 100 bar lengths? What do you mean by that? There is an interesting conjecture here that's related to what Luis was talking about. Our conjecture that Rn modules have upset resolutions of length at most n minus 1. At most, n minus one. When you restrict to looking at complexes that come from relative homological algebra, this global dimension ends up being infinite. But if you allow yourself arbitrary upset resolutions, I think you should be able to do it in less than the rank of vector space that you have there. And compare, if you allow yourself, if you look at free resolutions for the modules over this real exponent polynomial rank, its global dimension is exactly equal to n plus 1. It does have finite global dimension. But I suspect that when you use upset. But I suspect that when you use object resolutions, you can always, first of all, it's multi-graded, so it should be in most M. And second of all, you can sort of always fiddle with things to get rid of one of the terms. I mean, it's like, I can tell you more about why I think m line is one instead of m. And then here's a bunch of references, which I know you can't read there, but they'll be sizably posted, and then you'll be able to see these references. Some of them almost surely need to be updated. So if if that's the case that you notice one, feel free to send me an email that says, Please update this reference and so on and so forth. Please update this reference as well as dark. Questions, Rexra? Orphism between 10 and what you have is the I mean, in algebraic geometry, where these correspondence sheaves are moduled, you are doing the same thing for constructing the sheaves, but you get this extra you are trying to impose conditions to recognize the fact that the morphism is configuration. Ah. Is this why you need this type of condition? Is that why I need this type of condition? I need this type of condition because the category. Because the category, because tame morphisms don't behave, well, they don't behave categorically well. Compositions of tame morphism do not detain this kind of thing. So I have to sort of define things quite specially. Certain kinds of morphisms are automatically tame. If you take a direct sum of upsets, if you take a direct sum of, in Rn, if you take a direct sum of upsets and map it to another direct sum of upsets, it's automatically tamed. Can't fiddle with the, because Rn is upper connection. Because Rn is upper connected, there's a big point up there, so whatever homomorphism curve on that vector space is going to be true everywhere on the entire upsets. But when you start to put arbitrary modules in, especially like this anti-diagonal line, where it's sort of a direct, it's sort of a completely disconnected, totally disconnected subset, you could a priori put any map on that one-dimensional vector space to anywhere else. You have to specify that you want to subdivide the anti-diagonal line into finitely many regions on different things. So it's. So it's, I mean, it's related to trying to, of course, the idea is to look at, you know, homomorphisms like homomorphisms with constructible shapes, like morphisms with constructible shapes. I'm not sure exactly if I, I'm not sure if I answered your question or if I understood your question properly. What do you mean is you already keep it involved in the argument just gets yeah that's right. Yeah, so there's yeah there's there's no there's no um So there's no, yeah, so there is a notion of constructible sheaf in the Alexander of topology. What you get here, this tameness, is not that. That's something else. It's different. So yeah, and this is more general than, as you say, than the partial real vector space and ordinary topologies and so on. So this is something that's sort of, it's taking just enough of the sort of what you would get from the topology on the proset, it's taking just enough of that to be able to find a good class of online systems that works also in the process as the pod end of this kind of stuff. The fallout is just options. Then I'll go. So you said that you have minimality for absolute resolutions. You have a notion of minimality. Yes. When you're working over Zn or R to the N. Okay, R to the N is my favorite position. Very good. Now I want uniqueness of many more things. Ah, okay. Uh-huh. Is that something that you think can be done? That you think can be done? No. But the manner in which the non-uniqueness occurs is something that I don't know if you can get so much a handle on it, but you can at least understand what is the manner of non-uniqueness. So when you're dealing with free resolutions, what makes them unique up to isomorphism, not unique isomorphism, what makes them unique up to isomorphism is that you know exactly where the generators sit. It makes sense to know what a minimal generator is. What is a minimal generator? Well, it's an element such that. An element such that it doesn't have anything that precedes it, right? That's what a minimal generator is. Now you can look at the vector space of minimal generators. It's actually a quotient vector space, whatever. You can look at the vector space of minimal generators and just require that you have sort of a free module that has that vector space as its generating set in some space. So you can ask the same thing for your upset modules. You can take all your upset modules and look at all of their minimal generators, of which there are uncountably many. Are uncountably many, right? But then you can ask that in your upset presentation, the upset cover, for instance, that these minimal generators of your upsets essentially map to the minimal generators of your module in an isomorphic way. So it says it induces an isomorphism on tops. You can do the same thing for the sonicles, for fringe presentations, whatever. But the idea is that minimality means isomorphism on tops. Now, once you have that, that's a functorial thing. You can always demand. Functorial thing. You can always demand that. You have problems with real modules because you might have a minimal generator, you might have an open upset. And so then you have this limit where there's no actual element there. So you have to allow yourself to sort of define sort of limiting elements. These open generators. There's complications because of the topology on RN. But more or less, you define functors that tell you sort of the ways in which you can be open. And then you have minimality is isomorphism on top. But once you have this isomorphism, you're free to. Once you have this isomorphism, you're free to break up the top into finitely many pieces in any way you want. If you take a single upset that looks like this, and you simply break this curve into two pieces and take this upset and this upset, that's still a minimal upset presentation. Annoyingly. So like you can literally break upsets because you can break the boundaries. Here's the thing, just like the anti-diagonal line, boundaries, the lower boundary of an upset is an anti-chain. And as such, it is totally different. And as such, it is totally disconnected, and you are free to put it into as many pieces as you want. There's kind of no way around that. And there's more fundamental things, like you could take sort of two curves that sort of, take two upsets, the relations glue them together over here. And then there's sort of no good way. So you're going to choose a way to extend this backwards. And you have to make a choice whether you go up or down. There's really no way around that. The non-uniqueness is absolutely fundamental to have to do it. Maybe I'll just point out that. May may I'll just point out that in the relative homological alternative sense, you're resolving by upsets. You get a notion of unique for free. And you get uniqueness. Yes, you do. But you get a very restricted collection of upsets. A lot of the upsets can't work because you have to factor through. I learned this yesterday, finally, about relative homologic algebra. Because you have to factor, the condition is that if you have any of your upsets, The condition is that if you have any of your upsets that you're allowing, when you take an upset resolution and declare it to be minimal, it means that any other upset you choose has to be able to factor through. What that does is, in essence, if you use all upsets, then I don't know if you have... Well... That's okay, but the thing is that the length of the red should be. Yeah, the length is going to go to infinite. Yeah, so if you want finished step could be gigantic. The initial step, like the color by using the. Yes, that's right. It's going to involve every possible way. It's going to involve every possible way of covering it, and it's not going to be finite at all. So, if you want it to be finite, then you have to restrict the collection of upsets. If you want it to be, if you want to use all upsets,