Media network. So go right ahead. Thanks, Chang Chang. Uh-oh. I think I'm not hearing you. Are other people? Chan, are you muted? We don't hear you. It looks like you're not muted, but I'm not hearing any sound come out. I still don't hear any sound.   Yeah, maybe try that. Yeah, just log out and back in. Yeah, sorry, bear with us. Hopefully, we can get chunk chunk sound working. It says you're connected to the audio. Use try. Hi, I still can't hear your voice. Yeah. If you have an alternate mic or something. Let me check your audio settings. I suggest seeing if you can hear yourself when you go to settings, audio, microphone, test, mic. Yeah, it's true. You could try your phone. Maybe just to move this forward, is there anybody from the afternoon that would be willing to just swap to now and then just. To now, and then just so we're not spinning our tires here, maybe just direct message me if you're willing to speak now, Tatiana, Ege, Ko, or Anna, Ege, or Anna, and the Marquis of the Constitutional Community, and that might give junk. And that might give Chung Chung more time to troubleshoot. Can you hear me? Okay. I have icon. Yes. Sorry. Um okay is it doesn't maybe disconnect the audio video um wait maybe like turn the volume down to see it better. Is it better? Sorry, yeah, it's probably because it was playing through the speakers of your computer. Um, I think I, yeah, I silenced my computer. I think that solved the echo issue, but I cannot hear well. Oh, yeah, let me let me put it on speaker, I think. Um, yeah, sorry. Um Um sorry, how to put this on speaker. Um hello, yeah, can you hear? Okay, yeah, yeah, yeah. Okay, yeah, let me share my screen. Sorry about this. Um, yeah, I don't know why problem problem solved, and we, yeah, plenty of buffer built in anyway, so yeah. Built in anyway, so yeah. All right, okay, yeah, yeah. Sorry, um, sorry about this. Um, go for it, okay, yeah, thanks. Um, I will thank the organizers for the invitation and for putting together this workshop. Uh, so today I'm going to talk about chaotic dynamics in spatially distributed networks. Um, okay, so it's well known that um, the spiking activity of cortical neurons Norms is highly variable. So, here is an example of a single neural recording from visual cortex. And you can see that even though the trial average response is highly selective to the moving directions of the drifting gratings, there's a huge error bar around the mean. And typically, when presented with the same stimulus, the spectrum patterns of a neuron can be very Of a neuron can be very irregular and varies from trial to trial. So the randomness of the spectrums is typically close to a Poisson process. With advances in recording techniques, now we can measure more and more neurons simultaneously. And here is an example of multi-electrode recordings. And you also just saw this from Sarah's talk. And this is Sarah's talk, and this is a multi-electral recording from the auditory cortex. And now, this gives us a chance to observe population responses, responses from the spontaneous as well as the evoked states. So, you can see that neurons are not independent from each other. We can see coordinated activities up and down through time, and this global fluctuation also persists in the present. Persist in the presence of some stimulus. In addition, this degree of global fluctuations also varies from trial to trial. So in some trials, there's more synchronized activity, while in other trials, it's more desynchronized. So neurons become more independent from each other. We can measure this degree of synchrony with Synchrony with spike count correlation, which is just simply the average pierwise correlations of spike counts measured over a certain time window. And numerous data sets have found that the spike count correlations can be modulated by many cognitive factors, such as attention, task engagement, and training. So, for example, in this work, it was showing that when attention was directed That when attention was directed to the receptive field of the recording neurons, then the average correlation among those neurons were largely reduced compared to when attention was directed outside of the receptive field. And similarly, when the animal was engaged, actively engaged in a task or after training for a few days, then the average correlations are also much lower. So it shows that the correlation level is Devil is an indicator of the cognitive state of the animal and is likely to have impacts on the information processing in the cortex. So since correlated variability have an important consequence on neural coding, we're interested to understand what are the circuit mechanisms that can generate correlated variability as observed in the data. In the data. In addition to spiking activity, it has also long been shown that spatial tempo, the cortex also exhibits rich spatial tempo dynamics using typically using different types of measurements, such as voltage-sensitive dye imaging or local field potential, which measure integrated neural activity over a large space. And for example, in this early work, it already found that a visual cortex can respond to the same stimulus with completely different spatial temporal patterns for different trials. And also in this recent work, it's found that in also found wave dynamics in primates MT area, and you can see wave propagation across the whole MT. Propagation across the whole empty area, and the location of wave is initiated, and the direction of the propagation also vary from trial to trial. So this results show that even on the more emissoscopic level, visual cortex also does not respond faithfully to visual stimulus. And interestingly, it also has been showing that such wave activity can also be. Activity can also be modulated, just like the spike count correlations. So, in this example, it's found that in the spontaneous states, the magnitude of waves is also much larger and waves can spread over much longer distance compared to the evoked states. So, this result suggests that the spatial temporal activity can be very closely related to the correlated. To the correlated variability found in the spiking activity. And the coherent spatial temporal patterns can generate a large amount of correlated variability in the spiking data. So what are the circuit mechanisms that can explain the structure of correlations as measured in the population responses? The previous models have proposed that chaotic dynamics. That chaotic dynamics can be a major source of variability in neural responses. A classic model called Binance Networks have been very successful as accounting for the variability of single neural spiking. So in this model, each neuron receives a large amount of exciting inputs, which is balanced by a large magnitude of inhibitory inputs. And then the next inputs to And then the net input currents to each neuron has a small mean, but very large fluctuations. So, therefore, in this case, in these networks, a neuron's spiking activity is mainly driven by the large fluctuations in input currents. So, therefore, even though the network is a deterministic network, the spiking activity of each neuron can look very random-like. Okay, so the vinous networks can have emergent variability in single neural spiking. So, this is an example rust plot from the network. It can generate broad distributions of firm rates. And the coefficient of variation of the inter-spike interval is also can be around one. So, suggest that the spec trend is similar to a Poisson process. Personal process. However, it's also known that binance networks always generate asynchronous dynamics. So, as the network size becomes larger and larger, then the average correlations would decay like one over n, where n is the network size. So, it means that for a large network, then the neurons are basically independent from each other. So, therefore, it means that binance networks cannot explain. networks cannot explain the measured significant correlations in population recordings. So we need to find other model mechanisms that can account for the correlated structure in population responses. So specifically there are two features of correlations that we want to capture. So for this we analyze the multi-electrode recordings from Electro recordings from McCaig V4 area. So, this data is from our collaborator Marlene Cohen's lab. And in this experiment, the monkeys was doing a spatial attention task and was asked to detect the change in orientations of the presented Gabble images. And we found that from the recorded population activity, Population activity: the neurons are on average positively correlated and remain positively correlated across very long distance. So, this is true for both the attended data sets and the unattended data sets. We can see that as was also showing before in the previous paper, the attention largely reduces these average correlations. And in addition, we also analyze the dimensional structure of the shell. Dimensional structure of the shared variability, and we also found that the dimensionality is also very low. So, in particular, for the unattended states, the first mode accounts for the majority of the shared variants is much larger compared to the other modes. And the interesting attention mainly modulates this first mode. So, we found that for the population responses, they are positively correlated over long distance. They are positively correlated over long distance, and it's also low-dimensional, in particular for the unattended states. So, we want to find a model mechanism that can account for these features. And for this, we studied dynamics in spatially extended vital networks. So, this work was done when I was a postdoc in Brienne Storen's lab. So, for this case, we look at spike neural networks. Spiking neural networks with a spatial structure. And we know that from the anatomical data, that a major determinant of connection probability between a pair of neurons is the distance between them. So nearby neurons are more likely to be connected, while farther away neurons are less likely to be connected. So now we include this spatial structure in our network and look at the different dynamical regimes in such networks. Regimes in such networks. We found that when the excitatory projection width is similar to the inhibitory projection width, and when the inhibitory time constant, inhibitory time constant is slower than the exciting time constants, then under these conditions, the network can generate very rich spatial temporal dynamics. We can see large waves propagating across the Large waves propagating across the whole network. And such waves are not periodic waves and it shows considerable irregularity. So these waves can correlate neurons that are a long distance apart. And also these conditions are also consistent with anatomic data and also consistent with physiology. And we can see that in the We can see that in the spatial networks, as we increasing the time constants, snappy time constants of inhibitory norms, then the average correlations now changes from being zero to being positive and gradually increases with tau i. In contrast, in a disordered network, when there's no spatial structure, then the correlation average correlation increases very rapidly. Increases very rapidly and approaches one. So, in this case, the network exhibits epileptic activity with highly synchronized oscillations. And then we can also look at the distance dependence of correlations. So, we found when the inhibitory time constant is fast, then the correlation distance dependence is non-monotonic. So, nearby neurons are positively correlated while farther away. Correlated, while further away neurons are negatively correlated. And on average, the correlation is very small. Wherefore, when the inhibition is slower, then now we have positive correlations and remain positive across long distance. And then further, we can also look at the dimensionality of our network activity. And we've also found this low-dimensional structure as was showing. As was showing from the V4 data. So we found the first mode clearly have much larger variance compared to other modes. And after we removed the first mode, the residual correlation is very small. So therefore, we found a dynamic regime in this spatial order network that in this regime, the network can produce positive correlations and also low-dimensional shared variability. In contrast, we found that when inhibitory projections are broader, then in this case, the network produces more localized packets of activities and doesn't produce a low-dimensional structure. So for this case, the shared variance for the first few modes are all very high and the first mode does not dominant, is not much larger compared to other. Is not much larger compared to other modes. And on average, the correlation is also very low. Okay, so now next we want to further explore the dynamical regimes in spatial networks. So now we are switching gear from spiky neural networks to fair race models. And this work is led by postdoc Noga Moshev and in collaboration with Bad Ormetraut. Collaboration with Bad Ormitrout. Okay, so now we consider a neural field models. Here, RERI are the final rates of the exciter and inhibitory populations. And we also have a two-dimensional spatial structure in the network. And here, W are the Gaussian spatial kernel in the network. And the transfer function is a threshold quadratic function. And each population receives a spatial unit. Population receives spatially uniform inputs and I. And so, here we want to focus on two parameters, look at how the dynamics depends on these two parameters, which are the inhibitory spatial scale, sigma i, and the inhibitory time scale, tau i, where we fix the excitatory spatial and the temporal scales. Okay. Okay, so we further consider spatial uniform solutions. So, this is a case when the ferromage does not depend on space X. Then, in this case, this system can be reduced to a very simple two-variable system. We can look at the bifurcation of the system as we change parameter tau i. And we can see that the network, when tau i is small, the network has a stable fixed point, which corresponds to Which corresponds to a spatially uniform and stationary solution. And as tau increases, the system transitions through a hopification and generates limit cycle periodic solutions. And this periodic solution would correspond to a bulk oscillation solution where all the neurons in the network oscillate together. Okay, then next we want to look at the stability of the fixed points and limit cycle solution when we consider the spatial. When we consider the spatial structure. Okay, so now we consider the bifurcation diagram for these two parameters: the inhibitory spread sigma i and the inhibitory time scale tau i. So here is the stable region. The gray region is a stable region of the fixed point. Through this boundary is the whole bifurcation boundary as was showing before. Showing before. So, as tau increases, the network would transition to a bulk oscillation solution. Whereas, through the upper boundary, as sigma i increases, then the network would lose stability as a non-zero wave number. So, means it transitions through either a two-in-hole bifurcation or a two-in-bifurcation. And means that beyond this stability boundary, the network can generate spatial patterns. Patterns. And then next, we also analyze the stability of the bulk oscillation solution. And using flow case theory, we can find this blue region is where the bulk oscillation is stable. And through the magenta curve, the network would transition to a period doubling bifurcation. And just beyond this bifurcation curve, we can Curve, we can see alternating bound solutions. So, this is a case when the network has a bound activity that alternates between two locations in time. Okay. And then as sigma i further increases, the such regular alternating bound solutions again loses stability. And now we found the network dynamics to be very irregular. So now we don't have. Very irregular. So now we don't have those nice periodic spatial patterns. And each location, the response is no longer periodic in time as well. And we found this activity can just propagate across network and doesn't seem to have any particular patterns. So it seems that such solutions can be a chaotic solution. So to verify that these are chaotic solutions, we Are chaotic solutions? We computed the maximum Nyapolov exponent, and we know that when the Nyapolov exponent is positive, means it's a chaotic solution. When it's zero, means it's a periodic or quasi-periodic solution. And when it's negative, means it's attractive state. So we found that indeed there exists a parameter regime. That's a yellow region here, where we found chaotic solutions. So these are like those very. So, these are those very irregular solutions I just showed you in the movie. Interestingly, we found that the chaotic region is in the location where the inhibitory footprint has a similar scale as the excitatory footprint. So it's roughly when sigma i equals sigma e. And interestingly, it seems it's consistent with anatomical data from cortex because we know that there are already many. We know that there are already many data sets suggest that the exactory projection width has a similar spatial scale as the inhibitory projection width in both cat visual cortex and the mouse visual cortex. So it shows that since the in the real cortex, the parameters is also lies in the chaotic region, the similar parameter space as the chaotic region we found here. Region we found here. Okay, then next we want to look at the statistics of those chaotic solutions. So first we can look at the temporal spectrum of those solutions. So here shows you three examples of solutions. So the color indicated the parameter location here. And we can see that such chaotic solutions. Such chaotic solutions have very broadband power spectrum. So it's not like those periodic solutions or quasi-periodic solutions where we can find very narrow peaks at certain frequency. Those solutions, chaotic solutions, have pretty broad frequency power. And then next, we also measure the distant dependence of correlations, and we can see that the correlation is also. And we can see that the correlation is also very isotropic and remains mostly positive across long distance. And the spatial scale of the correlations also depends on parameters. And then lastly, we can also look at the dimensionality of the covariance in those chaotic solutions. And for this, we just simply measure the eigenvalues of the covariance of the rates responses. Rates responses. You can see that the dimension seems also to be very low. Although it's not one dimension, the first mode does not clearly pop out as what we saw in the data. But overall, it's the first about 20 dimensions that have much larger eigenvalues compared to the other modes. And we can measure the dimensionality as the number of modes that account for 95. Of modes that account for 95% of the variance, and you can see that the dimension saturates quickly as we increase the number of neurons sampled from the network. So for these three examples, it basically saturates between about 30 dimensions to 50 dimensions. Okay, and then next we want to see how the network responds. Network responds when we drive it with correlated input noise. So, this can model the case when the upstream cortical area already have correlated responses and how the downstream area responds to this correlated noise. So for this one, we just drive the network with globally correlated noise. And here, sigma n is the intensity of the noise inputs. The noise inputs and C is the correlation. And here's an example of a traveling wave solution when there's a noise and when it's driven by weak noise and strong noise. So when the noise is weak, then we can see that the traveling wave solution can still maintain its wavefront, still maintain the basic shape, and just with some noise on top of it. However, when the noise is However, when the noise is strong, then it can no longer maintain the trendy wave shape and it becomes very irregular patterns, just as what we found in the chaotic solutions. Okay, so then next we measure the maximum Napolov exponents of the network when it's driven by folder realization of the noisy inputs. And we found that the correlated inputs largely expand the chaotic. Expand the chaotic region. So we can see that now the yellow region is much larger compared to the case when there was no noise. So it shows that if the network is driven by correlated inputs, then it's very likely to exhibit chaotic responses. Okay, and the lastly, we want to see how the Nyapolov exponents depends on the intensity sigma n and The intensity sigma n and the correlation of the inputs. This is, we can look at one example first. We can see that as sigma n increases or c increases, then the network transitions from a periodic travel wave solution to a chaotic solution. And the transition is very sharp for this example. And interestingly, we found that this transition mainly depends on the intensity. depends on the intensity of the correlated noise components. So that the intensity is the sigma n times square root c. So when we plot the maximum Nipple of exponents as a function of sigma n times square root c, we can see that all the data points collapse on to this single curve. So it means that the private noise path basically doesn't change much of the dynamics. It's only the correlating noise path that can Noise paths that can transition the network from being periodic to chaos. We also found similar behavior in the other parameter regime. We can see that in general, the Nyaplov exponents only depends on the intensity of the correlated components. And as the correlated noise increases, a network transitions into chaos. So only one exception is the bulk of. Only one exception is the bulk oscillation case, where we further have the NAPLOF exponents become negative, to mean that the correlating noise can actually synchronize bulk oscillations. But once it becomes very large, then it again transitions to chaos. Okay, and lastly, I want to compare this spatial temporal chaos we found in spatial networks with the chaos in random recurrence networks. In random recurrence networks. So, for random recurrence networks, they were also mentioned in the previous talks. We know that they can transition to chaos as the variance of the connection weight increases. And it was first found in shown in this seminar work by Soblinsky and colleagues. And recent work has found that networks in those chaotic regimes have many computational benefits. So, for example, they can be trained. Benefits. So, for example, they can be trained to do a variety of tasks. And so, that's generated a lot of interest to study how to train those networks and what are the constraints for training them to do different tasks. So, it will be interesting to see if there's any computational benefits for spatial temporal chaos. So, as we know in real cortex, they are spatially, neurons are spatially organized. So, it will be an interesting direction to It will be an interesting direction to think about. And the chaos in the spatial networks are also different from the chaos in the random recurrence networks. So in random recurrence networks, the chaotic activity is asynchronous and also high dimensional, where we found the spatial temporal chaos to be locally correlated and low dimensional. And in the random recurrence network, the chaos can also be suppressed. The chaos can also be suppressed by independent noise inputs. While we found that the spatial temporal chaos is relatively insensitive to independent noise and chaos can be induced by correlated inputs. Okay, so to summarize, we found that spatial networks exhibit chaotic spatial temporal dynamics with low inhibition and spatially balanced excitation and inhibition. And the chaotic dynamics. And the chaotic dynamics generates a variability that is locally correlated and low-dimensional. And lastly, the correlated noise can expand the chaotic region. Okay, so lastly, I want to thank all the people involved in the work I presented today. The first part of results on the spiky neural networks was done when I was a postdoc with Bern Storam and was done in collaboration with Marlene Covin. In collaboration with Malin Covin. And the second part on the neural field models was done by postdoc No Gamoshev and in collaboration with Bart Ermichauk. Thank you for your attention. All right. Thank you so much, Chung Chung. We have time for a few questions. If anyone wants to drop them in the chat or just unmute or raise hand. Well, maybe lead off then while people gather their thoughts. So I was just wondering about the so just about the dimensionality. You said this chaos is low dimensional. So that's as measured by So that's as measured by the number of positive eigenvalues that you found in like a spectral analysis, or did you use multiple measures to determine the dimensionality of the spatiotemporal chaotic solution? So for this, we just look at the eigenvalue of the covariance matrix and just look at how many modes account for 95% of the variance. Of the variance. Yeah. Yeah. And then I also had just a clarification question about that correlation. So you had correlated noise. Was that correlated in space? Was it just like either going from white in space to having like a global correlation and then a mix of global correlation and white noise? Or did you have some specific spatial correlation function that you used? Correlation function that you used? Yeah, so here there's no spatial scale of the correlation, just a global correlation. So basically, every neuron receives private noise parts and global components. So these are not wise noise. It's an OU process with some time constant in time. So I wonder, so did you, so you have not looked yet at what would happen if you had. Looked yet at what would happen if you had spatially correlated noise whose spatial scale was similar to that of the patterns that you're working with, because you might get some interesting interactions there. Yeah, we haven't done that yet. Yeah, but it's definitely interesting to see how it depends on the spatial scale of correlations. Yeah, we haven't measured the Nikolauf exponents yet. Okay, thanks. And then I see Joel has yeah. And then I see Joel has a question. Yeah. Yeah, this is related to the stuff near the start of your talk, Chung Chung, that you had done with Brent and sort of modeling the role of attention and correlations. And my question has to do with the difference between the networks with localized connectivity versus the unlocalized connectivity in terms of the correlation structure. Can you maybe go back to that? Can you maybe go back to the slides around that point? It'll help me to illustrate what I'm talking about. This one? Yeah, this will do fine. So, right, so here you've got you put in this like local connectivity structure and you find it changes the correlations a lot. But of course, you could take all the neurons in that local network with local connectivity, you could just move them around and keep the connectivity the same, and not, you know, you'd get exactly the same. And you'd get exactly the same neuron responses, right? They don't care physically where they're located, they just care who they're connected to. So the point is that the network dynamics really just depend on the connectivity structure, not necessarily the locations. And I wonder if there's something specific about the connectivity structure that tells you where these correlations come from. Like, is it the existence of high modularity or of some other graph theoretic? Graph theoretic quantity that you're implicitly changing by putting in the local structure to the connectivity that sort of explains the emergence of these correlations. That's my question. Yeah, so for this one, basically the connections only depend on distance. So it's already a relatively low. It's already a relatively low dimensional connectivity. So, if you just consider it like a Gaussian only depends on distance, so it probably imposes some constraints on the network. So, I think that the distance dependence allows us to analyze this in the Fourier domain. So, it's just so we still think about this. We still think of this as a physical space, not as just a graph on the network. So it's just because of those translational invariants in the network, and we also have periodic boundary conditions. And this allows us to do analysis on the look at stability of wave numbers. So the spatial patterns would have certain spatial frequency in the network. Yeah. Work. Yeah. I don't know if that answered your question. Sort of, but not quite. But I also don't want to monopolize too much the discussion. Maybe we can pick this up some other time. Thanks, Chang Chang. Okay, thanks. Maybe good for discussion tomorrow. Jay Pina, you have a question? Yeah, hi. Thanks for the cool talk, Cheng Chang. At the end, I thought it was pretty cool how How just modulating the correlated noise could really expand the domain of chaotic activity? Do you have an intuition for why that is? Why the global quality inputs can generate chaos? Yeah, so we don't have an analytical understanding of this. Analytical understanding of this. So, my intuition is that in those other regimes, so especially when sigma i is larger, it tends to generate more localized patterns. So, like in those regions, it can like bump activities, more localized patterns. And if you drive it with a global signal, it just It just doesn't maintain very well. Similarly to traveling waves, if you just drive it with the up and down signal, it's easier to destroy the patterns, I think. It just seems that it cannot really maintain the regular shapes it wants to maintain. Whereas for bulk oscillations, if it's already very global oscillating, and so for the And so, for them, for some cases, the global signal can actually synchronize the buck oscillations. That's one possible reason, but we don't understand yet. Well, and do you have, is there a sort of reason to think that brain could indeed sort of tune that sort of hyperparameter of the correlated noise to allow. To allow or disallow for such chaotic activity, depending on maybe the activity or task or whatever? Yeah, I think it seems that in the data, the network always have those very irregular spatial temporal dynamics rather than those very periodic patterns. So there may be some benefits to maintain the irregularity. So just keep the So, just keep the network in the chaotic regime. So, maybe with the correlated inputs, it helps the network to stay in the chaotic regime. Yeah, but yeah, it will be very interesting to see what could be the benefits for the chaos in this case. Thank you. Thanks. Thanks, Jay. And then Max, has a question? Yeah. Hi. Question? Yeah. Hi. That was a great talk. Thanks. This is super, super fun. And it's an important direction, I think, to sort of put these things in space. Back when I was in GÃ¶ttingen, there was a student of Fred Wolff that did some spatial sort of field models like these, for which this was in the context of studying orientation preference maps and sort of different. And sort of different sort of functional solutions. And there, I think, sort of broad chaotic dynamics would probably have been limiting in many ways. And I'm wondering, I mean, maybe this is sort of what you already alluded to, that it would be interesting to see the utility of this. But given that you have this matching in the This matching in the amount of spread with the data at the location where the dynamics is chaotic. It's how could you imagine it being a useful property given that it would corrupt sort of input information? Yeah, so yeah, so if we just consider the information of the stimulus, the chaos is not useful. Is not useful. So it's, we have done in a lot of work, we just measure the information of the Gabo images. Then stable dynamics is the best to transmit all the information. And also simply in the experiments, when the animal is directing attention to the receptive fields, it also wants to reduce the correlated activity. Correlated activity. So seems that when the animal is engaging in a task, want to do some specific tasks is want to reduce the correlation, want to reduce the chaos to improve the information. But in the spontaneous stage, since there's a lot of dynamics going on, lots of correlation, so that suggests that chaos may be beneficial for other purposes, maybe like maintaining. Maybe like maintaining the plasticity of the connections in the network or for some to more general tasks, not for some very specific task. But yeah, it's still not clear what use this can be for. Okay, thanks. Yeah, thanks. So I see Sierra has a question. I'll just remind you. Sarah has a question. I'll just remind folks: feel free to step away if you want to get away for lunch, and then we'll be back at one for Tatiana's talk. But I don't want to kill the conversation, so go ahead.