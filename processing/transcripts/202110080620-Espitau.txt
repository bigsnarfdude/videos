Variant, which is the degree of the lattice, which is minus log of the co-volume of the lattice, which can be computed numerically as the log of the root of a grand matrix of any basis of the lattice. Somehow it corresponds to the volume of the fundamental domain, which is defined by the basis here in green for the Lebesgue measure, and this encodes. And this encodes roughly the density of points of the lattice. So I said that the computation of the co-volume of the degree is invariant of the basis. And yeah, so we can take in particular any basis. But a computational question which arises from this sole definition is: if I give you this skew basis here, which is pretty ugly, how can I retrieve the good one here? So in dimension, So, in dimension two, it's quite easy to do so because we have the so-called Gauss Lagrange reduction, which basically states that you can retrieve a very good basis from any basis in a plane lattice by just taking the longest vector and reducing it using the knowledge of the smaller one. And this is done iteratively until no progress can be done after that. On after that. So, once you reach the termination of the algorithm, you can prove easily that the first vector outputted by the Gauss algorithm is one of the shortest vectors of the lattice, and that the degree of the line lattice, which is spinned by this vector, has increased compared to what you had before. And you can also prove that actually the degree, up to times the degree of this line is bigger than the degree of the left. Is bigger than the degree of the lattice plus a parasitic factor, and you can't remove it because it's actually tight for the hexagonal lattice. More generally speaking, you can prove a similar result in any dimension, and you can prove that the length of the shortest vector is controlled by the slope of the lattice, which is a normalized degree, and a parasitic term, which is only dependent on the dimension. And if you want to be more precise, you can replace this dimension. Size you can replace this dimension by the best constant you can find, which is called Hermes constant. However, finding this shortest vector is actually a very hard problem. In dimension two, it was easy, as we saw, but in any dimension, it's an NPR problem, so pretty complicated. However, in 82, Lenstra, Lenstra, and Lovach showed that there exists a polynomial delta time algorithm, which gives which for any lattice lambda would give you a vector which is of less. Which is of length 2 to the n times longer than the length of the shortest vector of the lattice. So it might seem that it's actually a lot, like exponentially bad, but it has numerous useful applications. You can, for instance, solve the simultaneous differentiant approximation problem of real numbers. You can find minimal polynomials of some algebraic numbers using same techniques, and from that you can factorize polynomials over rational. And that was the original application of the Original application of the LLL paper. Of course, it's very useful in cryptanalysis and it's of the utmost importance for the analysis of so-called lattice-based cryptography. But more importantly for us, it enables lots of computation in algebraic number theory, where any computation with ideals basically boils down to applying LLL somewhere, computing efficient normal forms, such as the Hermit normal form, or controls the size of elements appearing in your various computations. Your various computations. And you can also, of course, and this is why I'm here probably, perform computation of homology of some modular groups using Voronoi theory, and it requires a huge amount of calls to the LL algorithm. So now that we've seen that is quite important, let's just have a bit of insight on how this algorithm works and how we can improve it. So if I take any basis of So, if I take any basis of a D-dimensional lattice, say V1, Vd, then I can attach to this basis a natural filtration. So, I will start with the zero sub-lattice, and I will consider then the sublattice spine by V1Z, then V1Z plus V2Z, and so on, until I reach the full lattice. So now I can do a bit of quantization on this filtration because the object itself has a lot of information, and we would Itself has a lot of information, and we would like something which is manageable. So, what I can do is take the degree map on each of the elements of the filtration, and this will give me what is called a profile, which is just a vector of real numbers, which encodes the relative densities of this sub-lattice. Now, let's do a bit of an experiment. And if I take an index i and I use the Gauss algorithm on the plane lattice with On the plane lattice, which is the quotient of gamma i of lambda i plus 1 over lambda i minus 1, which is of rank 2 because every step in the filtration is increasing by 1, the rank. Then I lift the small element retrieved by the Gauss algorithm and I put it instead of this Vi. So, what happens is that I will not touch the degrees of the other part of the filtration, but I will increase locally. Increase locally the degree of the lambda i. So, what I did is that basically I used the Gauss reduction as a local tool for densifying the filtration in one point. And now, like a reduction algorithm can be seen as some kind of greedy work in the flag variety associated to the lattice. Basically, you have all the possible flags associated to the sub-lattices. Sublattices, and from one I will just pick a neighbor flag for which the degree has increased. And like if we do that on a little drawing, basically I take my basis and I will start by reducing the first two elements where something might appear. And then when it's done, I will reduce the second and third and so on. But maybe at one point, I can see that the new vectors will do something. The new vectors will do something, and I go back in my algorithm and I continue, and maybe I will go back. But at one point, no more improvement of the degree can be done, and as such, my algorithm will terminate. You can prove that it terminates actually in polynomial time. Actually, we can go a bit further and enhance this simple reduction. And this was joint work with Paul Kirchner and Pierre Lafouk, and we published that this year. And we published that this year. The base idea is that we can use parallelization and recursion on the rank of the lattice. So, first of all, I said that we're working on the flag variety. So, basically, we're working modulo the orthogonal group, which means that we're computing Grassmid vectors, or if you like matrices, that we're computing QR decomposition. Okay, so everything is done with fast QR decomposition. That's okay. Then, basically, as I said, everything boils down to this Gauss algorithm. Everything boils down to this Gauss algorithm applied in convenient points. What we can do actually is not apply the Gauss algorithm in one point, but apply the Gauss algorithms everywhere at the same time, as long as it's not on overlapping blocks. So instead of starting from the left and progressing along the basis, I'm applying a bunch of Gauss reduction everywhere in the filtration. And then if I want the parts which were non-overlapping to interact together, I will just shift my window and apply it everywhere I can where it's not overlapping. Apply it everywhere I can where it's not overlapping. And then I do that again and again and again until nothing happens. So now it's kind of interesting because, as I said earlier, if you apply one Gauss process, then you will locally densify your profile. But if you apply a bunch of Gauss reduction, as I did, what is going to happen is that the degree of a point will become the average of the degrees of the neighbor points. So it means that what I'm applying. So, it means that what I'm applying actually with that is I'm applying a discrete Laplacian operator on the profile space. And actually, we will have a very funny phenomena that the way this profile is evolving is very similar, has the diffusion property of the solution of the heat equation, because at each increase of time amount, I will apply a Laplacian operator. And basically, we will have We'll have something which is basically like that a diffusion of the heat, and in our case, a diffusion of the degree. And it's well known by physicists, physicists, that the characteristic time of the diffusion property is quadratic in the diameter of the space. And here, the diameter of the space I'm working in is just this one-dimensional line of length n. So basically, the Length n. So basically, the number of times this process must be repeated is quadratic in the dimension. Oh, my God. Oh, yeah. So, and final improvement we can make on that is remark that as the operation are local, we don't need to apply this small Gauss reduction everywhere, but we can do by big blocks and recurse inside the blocks. I use my LL algorithm recursively over the blocks over and over and over. Blocks over and over and over. So, this is at depth two of recursion, and this will be at depth two of recursion two. And basically, doing that, you can prove that the reduction cost can be asymptotically the cost of matrix multiplication, which is, let's say, O tile of n omega, b or b is the bit size of the entries. For the record naive cost of the original algorithm is something in n to the six and to the cube dependency in the bit size and the fastest variant by angle. The bit size and the fastest variant by Stelle and Nomaier is in O tile of N4 to the P. So that's kind of a big improvement on the speed of reduction you can achieve on over C integers. So now let's play a bit more and have a bit more complexity, algebraic complexity, and instead of working over Z, let's work Of working over Z, let's work over the maximal order of a number field. So, since we are doing geometry, just a bit of recall of the geometry we are using here. If I take a number field K, I can look at the algebra K tensor R and I can see it included in some etal algebra and I with the place at infinity. And since this is a Hermitian space, I can pull back the full Hermitian space onto this algebra. Emission space onto this algebra, and I have a very natural emission structure which basically boils down to computing a trace. Since I have this emission structure on k, oh, that should be okay, sorry, I can work in the direct sum of multiple copies of it, and I'm just summing on the coordinates. So now I can, since that I have a nice Hermitian form on my number field, I can twist it with auto-hydrontophonic. I can twist it with auto-adjoint operators, and this gives us so-called Humbert forms, which are collection of Hermitian forms at each of the places at infinity. So I said earlier that a Nucleian lattice was a free Z module of finite rank and with endo with some inner product. So an algebraic lattice will be a projective OK module where OK is the maximal order of your field of finite rank, which is endor with a Humbert form on the corresponding ambient space. So basically, now we might want to reduce this bigger object. And what we can do is say, okay, since maximal order is quite close to the integers, I should try to apply the techniques I know on the integers and reduce it in the same way. So I said that we were working on filtration or equivalently on the QR part, on the R part of the QR decomposition, and this will work perfectly fine. We can compute the QR. Perfectly fine. We can compute the curia decomposition over a number field. That's no problem. That the reduction was carried out using a recursive structure on the rank, and this can be done exactly in the same way, because we're just using module properties. And actually, something which is starting to be a bit trickier is that when we are trying to apply one step of the Gauss map lifted at the lattice level, which is called size reduction operation, sorry, something weird occur. So over Sorry, something weird occurs. So, over the integers, it just requires integral rounding. So, if you remember, I said earlier, I'm using with the Gauss algorithm, I'm using the long vector, let's say v, let's say v, to reduce the short one, let's say u. This should be a u, sorry. And then the way to compute it is to make some integral rounding of this quantity. If you just look at the minimization of the vector, it gives you this quantity. And doing integral rounding is pretty easy. However, if I translate that over Translate that over a ring of integers, it amounts to find the closest element in this ring. And so, this is an instance of finding a closest point in a lattice. So, here, for instance, I draw it for over the Eisenstein integers. And the problem is that it's kind of hard if your field is big. So, hopefully, if you carry things out, just doing the very approximately surfaces and you just do a coefficient wide rounding and everything will be fine for you. Rounding and everything will be fine for you. So, all of this Gauss step can be carried out in the same way, and we're pretty happy with that. Actually, if you want to be quite clever here, you can use the units of your field to decrease the wall condition number of the metrics appearing everywhere, and you can lower the precision required to carry out the computation in some way. So, you can even gain a bit in the complexity here. And last but not least, I said the strategy were recursive over. I said the strategy was recursive over the module, but since we're working in a number field, we might be very lucky and have access to a lot of subfields under it. If you're feeling out prime, of course. And in that case, we can do something interesting. So suppose that we have a module over OK. Here, it's a free module for simplicity. What I will do is basically say that I will apply my general strategy of reduction, and the algorithm will ask me at one point to reduce rank two lattices. Rank two lattices over OK. So, oh, fair enough, but I don't know how to do that. So, the way to do it is to descend this rank to lattice over the first subfield I can find. And I will get a lattice which is defined over a smaller ring, but as, of course, a bigger degree. And then I will apply the reduction once again. And the algorithm will basically ask me again to reduce some. Ask me again to reduce some two-dimensional lattice, but over this subfield. So, what I can do is I can descend it and do it over and again until I reach some sub-lattice over Z. And once I do that, I will apply again my algorithm, which asks me to reduce rank two sub-lattices. And I can do that because I know how to do that. And it will give me a short vector, and I can complete this vector into a basis of my sub- My sub of my lattice, just neomodularity problem. It's just applying a CLID algorithm. And then carrying out everything, I will have a short vector of this lattice here, and I will retrieve a short vector, and I will once again lift it. So find its complement to get a basis of this lattice here. And I will do that with, once again, something which is close to the Euclidean algorithm, but in higher dimension. Higher dimension. And in the end, I will manage to find a short vector, lift it, and reduce my full lattice. So I'm now doubly recursive. First, on the rank of the module, and second, on the number field and the tower of subfields I can find in the number field. So all in all, you can, for fields which have a large number of subfields, such as cyclotomic fields, I can heuristically Heuristically do my reduction in times which is quadratic in the dimension, which is even better. And last but not least, we can go even further in the reduction of complexity by exploiting some hidden symmetries that lies in modules over number fields. So, just a bit of primer on very quick primer on symplectic geometry. So, let's draw a bit of So let's draw a bit of seven difference gains between Euclidean space and simpletic spaces. So I'm taking on the Euclidean space a symmetric bilinear form and an anti-symmetric bilinear form on the simpletic side. So the transformation group which is associated to the Euclidean space is the orthogonal group and on the other way it's a symplectic group. I have the, let's say, canonical representation of elements as QR decomposition in a Euclidean space and on simple space it's a variation on the E1. Semplex space, it's a variation on the Iwazawa decomposition. And of course, we've all known in linear algebra classes that we have privileged and congenial basis in Euclidean spaces, which are the orthonormal bases. So basically, you can always find your transformation which gives you an identity matrix to represent your bilinear form. For simplex explicit, basically, you have some kind of Basically, you have the same kind of process, because here it's just Gram-Schmidt optimization disguised. And here, you have, like, let's say, simplectic Gram-Schmidt, which gives you some nice bases, which are called Darboux bases, which are two copies of identity in the reverse diagonal. So, and why is this kind of basis interesting? So, if I'm taking some basis, some bunch of vectors in my Euclidean space, and take a bunch of vectors in my simplicity space, suppose I'm doing some. Semplatic space. Suppose I'm doing something on the first vector, I don't know, reducing it, solving whatever instance. I find a vector and I'm replacing it in my basis. If I do that in a simpletic space, basically using the knowledge of the symplectic form, I can deduce what will happen on the final vector. And then if I move on to the second vector, like using reduction in whatsoever way, like LL or whatever, in a simplexic space, I will be able to retrieve some information on the Some information on the last but not least vector, and so on, and so on, and so on, and so on. But at one point, I reach the middle of the basis. And in the Euclidean space, I will have, let's say, reduced half of my basis, but in this simplistic space, I will have reduced the first half of the basis, but for free, I can retrieve what will happen on the second half of the basis. So the motto here is that having a simpletic structure basically gives you the opportunity to do only half of the operations. Opportunity to do only half of the operations to do reduction. So that's kind of start to be a bit interesting. And now I want to do that everywhere. I can do it in my previous algorithm. So if I take back my tower of number fields here, I will take, let's say, the simplest symplectic form I can imagine on the upper field here. And the simplest one is the determinant form. So just take a matrix and take its determinant. Just take a matrix and take its determinant, it's symplectic. And if you look at what it means for a matrix, for lattice to be GH symplectic, it just means that its determinant is one. And that's very convenient. Because it means that any two-dimensional lattice over this kind of number field can be seen as a symplectic lattice as long as I'm just rescaling by its determinant. So I have this kind of symmetry which can appear everywhere. I have this kind of symmetry which can appear everywhere. So now the goal of our game will be to say, oh, I can apply my reduction and save half of the operations here. But once I descend, and at one point I will need to descend here, I'm stuck. Like, where is my symplectic symmetry? And the goal here will be to find a way to construct a symplectic form over Kh minus one, which is the subfield just under, such that descending a lattice which is GH symplectic over the Which is GH symplectic over the upper field will become G prime H symplectic over the subfield. And we can do that, actually. We can find always a non-trivial linear form from Kh to Kh minus one such that this decent property is verified. And then we can do that over and over, and we can find simplectic forms over any of the subfields which has this compatibility property. And by that, I mean that if I take a two by two matrix over Kh, I want it, I want Over kh, I want its descent over any subfield to be j-symplectic. So basically, I'm finding inclusion of symplectic groups for different forms. And once I'm able to construct these families of symplectic forms, I'm pretty happy because I'm halving the computation time at each of the recursive level. So this is quite interesting because it means that if I have enough subfields, I'm not gaining a constant factor, but actually I'm gaining a factor two over each of the surfaces. So if I have a logarithmic number of subfields, Number of subfields, I'm very happy because I have just gained the polynomial factor. And all in all, for instance, here for cyclotomic fields, you can do lattice reduction in subquadratic time, which start to be very, very interesting because, as I said earlier, when you're applying all of this Gauss process, you have kind of a barrier which is quadratic in the dimension of the lattice. So here we beat. The lattice. So here we beat this lower bound, this quadratic lower bound, by exploiting some hidden symmetries inside the tower of memory fields. So to plug back to my initial slide, why is it interesting? So on the one hand, it gives faster ideal multiplication. Because the reduction for non-principal maximal orders needs you to handle so-called Steinish ideal in C. Tiny ideal in the representation of your module, and you have to do that at every step. And handling it means basically doing a lot of ideal multiplication. So you need an efficient ideal multiplication if you want to have a fast reduction. And actually, ideal multiplication reduces to the reduction problem itself, because you just have to reduce the lattice, which is spinned by the four products of generators. Because in a number field, you can represent any ideal with two elements. And if I have two ideals with two elements, And if I have two ideas with two elements, I just do the four products and I have a generating family of the product ideal. And I reduce that to get back to a true representation element. So we get a dubly recursive algorithm here. Each reduction of the lattice will call ideal multiplication algorithm, and the ideal multiplication algorithm will, in substance, call new instances of lattice to reduce. So you do that, and in the end, you will get a fast ideal multiplication algorithm. So that's interesting. So that's interesting because it improves basic virtually any algorithm which uses ideal arithmetic. So that's kind of pretty interesting. As for the comology of arithmetic groups, so the computation of small modular groups requires like a lot of reduction of small lettuces, like millions of calls. So if you can get faster reduction, you're very happy because this part won't be a bottleneck anymore. And so here And so here we have two cases. While we're reducing lattices over Z, basically, on the one hand, we can use this parallelization technique I introduced earlier so that we can leverage the multi-core architecture we have on any kind of server today. Plus, we can do, and I didn't explain that, but tailoring the bit representation. Since the lattices involved have small dimensions, you can pre-compute a lot of reductions everywhere so that you Everywhere, so that you can access to pre-reduced lattice and gain here. So, if we're working over quadratic fields, such as let's say working on lattices over a ring of Gaussian integer of Eisenstein integers, we can always force this symplectic symmetry to gain a factor of two at least. More interestingly, since we don't have access to this whole tour of number fields, we can stay at the level of Stay at the level of the field itself when reducing because it's non-Euclidean. So basically, we can adapt all of this machinery to directly work at the ring level and never have to go down to Z itself. And last but not least, since we might have access to fast enough lattice reduction over some slightly bigger fields, let's say small cyclotomics, then it could be interesting to try to look at Interesting to try to look at this comological question in for bigger fields. So, thank you for your attention, and I'm happy to take questions if there's some. Thank the speaker. Well, if there are no questions, let's thank the speaker again.