Uh, minutes. Okay. Just let me know when I should start. You're on. And I understand that this is the last day. I imagine some people already left. Quite some people. And some people promise at least join online or at least watch the video. Some people are. Some people are in the problem. But someone has to give the closing lecture. And you have all the time in the world. You can do one hour, 52 hours to get hour. I promise. I promise I will not abuse. So let me start. Shall I start, or is there something else? Yes. Waiting for. Thank you for the chair. So you can listen to me. All right. Okay. Thank you for attending the last day of the online session. We have Mark to give us the closing presentation. Mark is somebody we. Mark is somebody who needs no introduction, and he's, of course, done wonderful and fundamental work on factor models. So, Mark, it's all yours. Thank you, Rong, and thank you for the friendly introduction. And again, I apologize for not being with you in person, but it was difficult partly because of funding, partly because of the timing. Funding partly because of the timing. And well, here I'm online, and I understand that an online talk is not always as entertaining as an in-person talk. So I apologize in advance if it looks a little bit boring. So you can see my title. You can see that this is joint work and the result with many discussion, the result of many discussions with Matteo Barrigozzi, whom you heard a couple of days ago. Ago. And well, it's about factors. And we've heard many sophisticated factor models during this week with tensors and matrices. But I will go back to the traditional situation where we have panels of real-valued data. And so the title is The Dynamic, the Static, and the Weak. And as you know, the literature is. And as you know, the literature is full with differences between dynamic factors, static factors, weak factors, and many, many other ones, many more. So everything is here is about real-valued random variables, but I think that the basic ideas that I will develop remain valid in the more sophisticated matrix and tensor-valued context. And tensor-valued context. Although I have no theory, no proofs in that extended framework, but I'm pretty sure what I'm saying here also holds for such more complicated situations. And everything, of course, is in progress. There's no paper written yet. So, of course, all your remarks and criticisms are most welcome. So, as we know, the family tree of factor models go back to the early century model that has been proposed in psychometrics by Spearman in 1904. But the story of factor models in time series and econometrics and high-dimensional time series really start. Series really starts, takes off with the papers by Chamberlain and Chamberlain and Rothschild in 1983. And these papers are proposing an approximate static factor model. And why is it approximate? Well, as all factor models, it relies on a decomposition of the observation. So XIT, I need not explain the notation. Not explain the notation is the observed observation. And in fact, the models always decompose the observations into two parts, which are mutually orthogonal up to some point. And those two parts, I will always denote by chi for the common part and psi for the idiosyncratic part. And so in this case, we have a decomposition which I Have a decomposition which I will call static, and that's why I have a superscript static into ψ plus xi. And this is an approximate factor model because the idiosyncratic component is not required to be an IID process of mutually orthogonal zero-mean variables with finite diagonal covariance matrix. So, well, Well, this, of course, is the assumption that was made or had been made by Spearman's followers. So here is the decomposition, and the common component is of the form BIFT, where BI is a one by R vector of loadings. Vector of loadings. Ft is the value at time t of A R by one factor, which is unobserved. And which is a crucial novelty in Chamberlain's approach, novelty with respect to the previous literature, is that this idiosyncratic is no longer some kind of noise, it's not an IID. It's not an IID process anymore. And he's considering high-dimensional asymptotics for the first time with both N and capital T, the period of observation, going to infinity. So that was a great idea, and this idea has been exploited up to today. So his assumptions are quite mild. So we have, of course, an observed finite realization. Finite realization XIT of a second-order stationary process. And we assume with not real loss of generality that it is zero mean, has strictly positive variances, and has, I think since it is second-order stationary, finite second-order moments. So, this common component here, which is the statically Which is the statically common component, because we will have other common components when we go dynamic. Well, it's the value at time t of an unobserved series, which is also a finite NT realization of a second order stationary process, which is not observed, with covariant matrix sigma sub chi. And well, we have that as n. We have that as n goes to infinity. This is, of course, this is a reduced rank process since it is r-dimensional. It has at best non-zero eigenvalues, and we assume that these non-zero eigenvalues tend to infinity. So, Bi is a vector of loadings, Ft is the unobserved process of a latent file factor. A latent five factors, it's an R-dimensional process. Well, it is assumed in Chamberlain, but that's not a loss of generality because we always can assume this, it's an identification constraint that the expectation here is identity, and we have orthogonality between the factor and the idiosyncratic. So, this expectation. So, this expectation here is zero. As for the idiosyncratic, well, it's also the realization of a second-order stationary process, and it has a covariance matrix, which is n by n, sigma psi, and all the eigenvalues of sigma psi remain bounded. Okay, so the psi's need not The size need not be white noise. And I think we should avoid, certainly in econometric context, we should avoid calling this an error because when you say error, you think white noise and it's not white noise. And so the covariance here, the covariance matrix needs not be diagonal. And that's why we call this an approximate factor model. So that's the model. That's the model by Chamberlain and Chamberlain and Rothschild. And note that, in contrast with what will come later on, this is a semi-parametric statistical model with parameters. The loadings, for example, are treated as parameters, but sometimes even the factors are considered as being parameters. Parameters. And there's a big nuisance, of course, which is the unspecified distribution of the idiosyncratic process ψ and well, the distribution of the factors. Note that Chamberlain and Rochal do not consider the estimation problem. They just describe their approximate static factor model, and they do not impose any rate. And they do not impose any rate on the divergence of the eigenvalues of the covariance matrix of, well, it should be here, sigma chi, chi mod chi. So that was a big breakthrough. And later on, the estimation problem has been considered by papers by Stock and Watson, by Ng. And Watson, Bay and Eng, by and their co-authors, who provide a rigorous treatment of the asymptotic properties of PCA-based estimators for the loadings and the factors. But the model remains the Chamberlain and Rochelle model, plus some assumptions they need on the idiosyncratic in order to construct, to show the consistency of their PC. Consistency of their PCA-based estimators. Now, well, up to this day, this approximate static factor model is certainly the most popular tool in the practice of factor models. And to the best of my knowledge, factor models have been the only successful tool for the analysis. Successful tool for the analysis of high-dimensional time series. It has countless applications. Section two, Gewicki, Sargent and Sims, and dynamic loadings. So five years before Chamberlain and Rothschild, there had been another very interesting paper by Gewicki in 1977. In 1977, which was followed by Sergeant and Sims in 1997, not 977, obviously, this is a mistake, who had understood that if factor models, the Spearman type factor models, were to be used in econometrics, well, the time series nature of econometric data should not be ignored. Ignored. And so before Chamberlain and Rothschild, they proposed a model, but their model is an exact factor model, meaning that the idiosyncratic components are mutually orthogonal white noises. With, however, dynamic loadings, this means that in their model, the unobserved factors are loaded via filters, not via the Yeah, not with real loadings. So, this means that they have also decomposition into chi plus psi. And in this case, instead of having this chi static, which was of the form B F T, where B was a matrix, well, they have loadings of the form B L F T, where B L now. Where BL now is a matrix of filters. So, in the usual time series notation, a polynomial in the lag operator. This was also an extremely innovative idea, because indeed, in econometrics and in time series, well, everything happens with lags and leads. And so it's quite natural to consider. So it's quite natural to consider that the unobserved factors are loaded by the observations, but not necessarily at the same time. The innovation could, well, the innovation, the factor could reach some observations at time t and some others at time t plus one, for instance. There could be a lag in the loading. So this is extremely interesting from the point of view. From the point of view of applications. On the other hand, as you have seen, Geweety, Sargent, and Sims do not go all the way with taking into account the time series nature of the data, because they still assume an exact factor model with IID and mutually orthogonal idiosyncratic components. And this is a terrible limitation because. Limitation because this means that essentially you have a model of the type signal plus noise, where the signal is reduced strength, driven by our unobserved factors, and then the idiosyncratic indeed is kind of a noise. And this is something that is very unlikely to hold in the type of data sets we are dealing with. Type of data sets we are dealing with. On the other hand, the benefit of having an exact factor model is that they can go for traditional asymptotics. They just have capital T going to infinity for fixed N, and with this, they can produce consistent estimators. But just as in Chamberlain and Rothschild, their approach is an approach based on. Approach based on a semi-parametric statistical model. So they write an equation and they look at this equation as a data describing a data generating process. Okay. Well, this I will skip because I have no time. Chapter two, the dynamic. So the dynamic, well, goes back to a paper, a 2000 paper by Paper, a 2000 paper by Forney, Forney, myself, Lippy, and Reichlin, who called their approach and their factor model a general or generalized factor model, in short, GDFM. And their idea is to combine the best things of two worlds, and they combine the dynamic loadings idea of GayWiki. idea of Gilwicki and Sergeant and Sims with the high-dimensional asymptotics of Chamberlain. And so now let me present this dynamic factor model. And you will see that an essential difference is that the presentation I'm giving here, which is inspired from a previous paper. A previous paper with Marco Lippi, which avoids the spectral domain approach that was originally used in the 2000 paper. Well, this approach is no longer based on a statistical model describing a possible data generating process. I will be. Process. I will provide details on this. So this approach is entirely non-parametric. I mean that the only assumption that we make, that is made here, is that the collection of x i t, so indexed by the cross-sectional index i and time t, ranging over the integer for i and the natural numbers. The natural numbers for T is a second order stationary process with mean zero period. And well, the notation, well, the notation is the traditional notation itself self-understandable. I don't have to go through this. And the way the factor model decomposition is obtained is, in a sense, is endogenous. So you start from that stochastic process, which exists, but you don't specify anything about its distribution beyond the fact that it is second order stationary with mean zero. And the decomposition will follow from a decomposition. Will follow from a decomposition of the Hilbert spaces that are spanned by the observations. So if you denote by H the Hilbert space spanned by the whole process, so the whole collection of XITs, and you equip it with, of course, the L2 covariance scalar product, well, let us consider the following definitions. Definition one: We will say that. One, we will say that a random variable zeta, say, with values in that Hilbert space and variance sigma square sub zeta, if it is in this space indeed, it has a finite variance. We will call it dynamically common if either, well, it is identically or almost surely zero, then of course it is in any Hilbert space, or it has positive. Or it has positive variance, and once you standardize it, so you divise it, you divide it by the standard error. Well, this standardized version can be seen as the limit in quadratic mean as n goes to infinity of a sequence of standardized elements of the Hilbert space of the observations of the form, well, some W divided by its standard. Divided by its standard error here, where W is a sum, it's a double sum over all the cross-sectional, so a cross-section of dimension n, so from i equal one up to n. But over all possible lags, k from minus infinity to plus infinity, and here you have coefficients, and these coefficients should be such that the sum of Such that the sum of their squares is equal to one, so it's a normalized sum. And so here you have a weighted linear combination of the observations in a n-dimensional panel, but finite, infinite time as a process. So all the lags are here. So it's a linear, it's a A linear combination of the present, past, and future observations of a n-dimensional sub-process of the whole process of XITs. And so we look at these WN, standardized WNs, and our zeta, our standardized zeta, is such that. Delta is such that it is the limit of a sequence as n goes to infinity of these guys here where the linear combination, so the coefficients which are here are such that the limit as n goes to infinity of the variance of these w's is equal to infinity. Intuitively, Intuitively, this means that these dynamically common variables belong to the space of linear combinations of the present, past, and future observations that have an exploding variance as the dimension of the process, dimension n, goes to infinity. And note that. And note that the definition of that space does not, it's a space that does not depend on n. There's a unique dynamically common space of such variables. Because when you are using here all the leads and lags of the observations, whether you start at t and go backwards and forward, or at t plus one and go backwards, well, it's the same collection of possible linear combinations. Of possible linear combinations. And definition two: well, we will call the space which is spanned by the collection of all dynamically common variables in HX. We call it the dynamically common space. Now, it's a Hilbert space as well, and as such, it has an orthogonal complement in the full Hilbert space. Hilbert space. And so we have here this there's a misprint here. We have this dynamically common space, which is H sub DIN, DINCOM, so dynamically common, and it has a dynamically idiosyncratic space, which is the orthogonal complement. So you divide the Hilbert space H of the observations. Space H of the observations in two mutually orthogonal subspaces, which now are mutually orthogonal at all leads and legs. And the idea is to proceed with the decomposition of the observation by projecting it onto these two subspaces. And so here is the projection on the dynamically common subspace. Common subspace, and here is the projection on the orthogonal complement. And this decomposition we call the general dynamic factor decomposition of the observation. And so you have a common component and an idiosyncratic component, but of course they are not the same as we had in the model by Chamberlain and Rothschild. Note that the big difference also The big difference also is that this decomposition is endogenous. So it always exists and it is not an assumption that we impose on a data generating process, on the data generating process. We only impose to produce a second order stationary process indexed by INT. I and T. Moreover, it is a canonical representation result. So give me a process, and there is such a decomposition. And whether it constitutes the description of a data generating process or not is completely irrelevant. That's out of the picture. It's not necessarily describing anything. It's just a map. Describing anything, it's just a mathematical decomposition. And that mathematical decomposition will allow us to perform inference because these two components, well, are very different in nature. One is reduced rank and very strongly cross-correlated. We have a pervasive cross-correlation. Cross-correlation, whereas the other one is only mildly cross-correlated. And so we will use very different methods in order to analyze them. Very different statistical tools will be used for these two components. But it's not a model and it is not either, nor is it a dimension reduction. A dimension reduction technique because there's no reason for the idiosyncratic to be small. So chi is not necessarily an approximation of x. You could have that the idiosyncratic is big, and you could have that the idiosyncratic has big autocovariances, meaning that it has an important predictive value. Predictive value. So do not throw the idiosyncratic away. Now we have this relation between the definition I have been given between the definition of this common dynamically common space and the behavior. Space and the behavior when you add the requirement that the process admits a spectral density matrix, then there's a relation between this space of exploding linear combinations and the behavior of the dynamic eigenvalues of the process. The dynamic eigenvalues of the process are the eigenvalues of the spectral density. Eigenvalues of the spectral density matrix. So theta here is the frequency. Okay, and typically you will have n eigenvalues for each value of theta. And well, it's possible to show that in case you do have a spectral density matrix, then, well, you have that the limit of, say, a number. Say a number, let it be q, of the largest eigenvalues is infinity, and the q plus one, the next one, remains bounded. So there is a relation. And in the 2000 paper, it was this characterization based on the behavior of dynamic. Of dynamic eigenvalues that has been used. And many, many practitioners, when they see spectral objects, they shy away. And I think this is part of the reason for the success of the static approach, which is by far more popular than this dynamic approach, which is here. Okay, well. Okay, well, in such a case, you have that the dynamically common component chi here is driven by a number Q of innovations, which are loaded by some filters. And you have then this form for the common component, which is the form that we had in the model by. In the model by Gewicki and Sargent and Sims. But now, of course, we don't have the assumption on the geosyncratic that they have. And in order to perform consistent estimation of the loadings, of the factors, etc., well, we need the double asymptotics. Double asymptotics, so the high-dimensional asymptotics where both n and t go to infinity. Okay, so everything is asymptotically identified as n goes to infinity and can be estimated consistently as n and t capital T go to infinity. T go to infinity. Okay, so this is about estimation, but estimation is not the purpose of my talk today, so I will skip this trend, this slide here. Now, now that we know, we have seen this approach, which is an endogenous approach and is not a statistical model-based approach of the factor model. The factor model decomposition in the model, the general dynamic factor model. Maybe we can come back and do something similar with the static model. So let us revisit the static model and we can provide definitions which are quite parallel to the definitions I just gave. So definition one, one prime, we will call a random variable. Will call a random variable zeta with values now in the Hilbert space generated by the observation x at time t. So this is a Hilbert space which depends on time. You have one Hilbert space for each value of t. And such a random variable zeta will be called statically common at time t if again either If again, either it is equal to zero, but that's not interesting, or it has strictly positive variance. And when you divide it by its standard error, it can be seen as the limited quadratic mean of a sequence of standardized elements of this Hilbert space at time t of the following form. So, again, we have a linear combination of the observations, but now there are no lags involved. Everything is Are no lags involved. Everything is contemporaneous. It all happens at time t. And so we have coefficients bi here with a sum of squared value equal to one. It's a normalized collection of coefficients. And well, this w is such that when n goes to infinity, its variance explodes. So again, we are. We are, if you look now at definition two, you look at the collection of all these variable zeta, well, they constitute a Hilbert space, which we call the statically common Hilbert space, so the statically common space at time t. Now there is such a space for each value of t. And so that's the Hilbert space, which is spanned by the collection of all statically at time t common variables in the Hilbert space H X T. And again, it has an orthogonal complement now with respect to this Tilbert space at time t, which we call the static. The statically idiosyncratic space. And we have a canonical decomposition into a projection onto the statically common space and a projection onto its orthogonal complement, which is the statically idiosyncratic space. And these two components. These two components now, because they are in two mutually orthogonal Hilbert spaces, but at time t. Now we only have mutual orthogonality at time t. We don't necessarily have orthogonality with lagged values. So it all happens at time t. Okay? So So that's also a reason why some additional assumptions have to be put on the static model if consistent estimation is to be performed. In the dynamic model, essentially there's no additional requirement because we have this orthogonality at all leads and lags. Again, if you add a Add a requirement about the eigenvalues, but now the eigenvalues of the Lag zero covariance matrix, you have the usual characterization with R diverging and the other bounded eigenvalues as n goes to infinity. Okay. No, let us super switch. Let us switch to chapter three, and we have the weak, the weak, the weak factor. Now, if you look through the literature, the terminology weak factor appears in various places, met with different meanings, and one should be very careful. The first appearance of this terminology, I think it's It's Onatsky who coined the expression weak factors, but it first appears in a paper by De Moll, Giannoni and Reichlin in 2008. And it's very simple. The concept is very simple. A static factor model, in a static factor model, a weak factor is a factor which is related with a covariance eigenvalue, which diverges, it explodes. It explodes, but at a rate which is sub-linear. So it explodes, but slowly, let's say. And the problem is that PCA-based estimation methods don't pick up such slowly diverging factors. To make sure that we understand each other, I will call those. Those factors, those weak factors in the Nazi sense, I call them rate-weak factors. And there are rate-weak, they may be rate-weak factors in a static model, and there may be rate-weak factors in the dynamic model as well. At least, if we make no additional assumptions, I come back to that later. Assumptions. I come back to that later on. The same terminology about a factor being or a component being weakly common, weakly idiosyncratic, etc., has been used in a paper with my former student, Roman Lischka, in 2011. And that's in a situation where the global panel of dimension N cross section. Dimension n cross-section of dimension n is divided into sub-panels or blocks, and all the dimensions of those blocks also goes to infinity. Okay, and then you may have a factor which is common for block one, but it's idiosyncratic in block two. And in that case, it is called a weakly common factor. weekly common factor for block one, but a weekly giosyncratic factor or weakly giosyncratic component for block two. So that's a meaning which is completely different from what the rate weak factors are. Okay and there's a third a third meaning for A meaning for that word weak component, which follows from a recent archive post by Goersing, Rust and Deitzler. So it was published on archive last year with a slightly provocative and intriguing title. So I imagine that several of you have spotted that. That's an excellent paper, by the way, very, very inspiring. And the title is. And the title is Weak Factors Are Everywhere. A very good title. And well, they define weakly common components, not exactly weakly common factors, but you can define factors when you have the components. They define them as the difference at time t between the dynamically common and the statically common component of X. Common component of XIT. And in order for this to have a meaning, of course, they first show that the Hilbert space generated by the static factors is a subspace of the Hilbert space generated by the dynamic factors. And then, of course, you can write the difference. And this difference, well, it belongs to the To the common space, the dynamically common space, but it belongs also to the statically idiosyncratic space. And in that sense, it is weak. It is common, but weakly common. This third definition bears no relation to the first two ones. So this is an entirely different meaning for the word. A meaning for the word weak component. So let us have a closer look on the rate week factors, which appear for the first time in the paper by De Moll and co-authors. But it's Onatsky who has really started the The subject, and it has been followed by many others. Well, these factors are rate-weak because they impact, they are pervasive, but at a rate which is sub-linear, meaning that their loadings, as n goes to infinity, well, do not grow or rather even. Rather, even decrease as n goes to infinity, so that they are hard to detect as n goes to infinity. And typically, if you use the traditional PCA-based estimation methods, they will not pick up these weak factors. So, that's kind of a grain of sand in the A grain of sand in the gears of static factor model methods. Now, if we look at Gussing, Rust, and Diezler, here is a more precise definition. So you have the observation here, and here is the static decomposition, chi static plus xi static, and here is the dynamic. And here is the dynamic decomposition, xi dynamic plus xi dynamic, chi plus psi. And in between, so you have this difference here between the dynamic common and statically common, which they define as being weak. And it is And it is the difference between these two common components, but between the two idiosyncratic components as well. This is a very ingenious idea. And of course, it's interesting to know that the dynamic factor model is kind of overarching the static one. So the static one is a sub. So in the stack, one is a sub, what a sub-model of the dynamic. Well, what did I write here? Well, this weak factor belongs to the intersection of the dynamically common space, and so it is orthogonal at all leads and. Orthogonal at all leads and legs to the dynamically idiosyncratic component. But because it is in the statically idiosyncratic space, it is orthogonal to the statically common component at some t, but not its leads and lags. Okay, so it has a kind of a special status in that respect. So it consists on So it consists of non-pervasive lagged values. Yes, this is something else Gozing and his quotas are showing: that because it belongs to the dynamically common space, but not to the statically common, it means that it consists of non-pervasive, because if it were pervasive, it would be in the statically common. So non-pervasing, pervasive lagged values of today. Lagged values of the dynamic factors or combinations thereof. Okay, so you may have that a dynamically common component has lags that are loaded, but they might be loaded in a non-perversive way. Nevertheless, the dynamic approach to the estimation of The estimation of the model picks up these lags, and that explains why, even when the assumptions of the static model are satisfied, there's always a little advantage with the dynamic methods in terms of forecasting, etc., because, well, there's always something that in the static decomposition will go to the Will go to the idiosyncratic that in the dynamic approach will remain in the common. And when it is in the common, the forecasting methods for that guy are better because they take advantage of their impact on the other cross-sectional items, whereas in the idiosyncratic case, you cannot do that. Cannot do that. Typically, you will go for a univariate or a sparse analysis of the idiosyncratic. And I skip this paragraph on weak factors in panels with block structure. Notice that in that context, you may have a you may have You may have a weak factor in the sense of Halan and Lishka, which is also a rate weak. And that will happen when some subpanels, some blocks have dimension n, say N1, for instance, the first block. Well, this N1 divided by N, the total dimension. By n, the total dimension, might not go to a constant, it might go to zero. Then everything that is a fact in that sub-panel, because the panel is not as often, well, doesn't grow as fast as the other ones, might be a rate weak factor due to the. Greek factor due to that fact. Now, let me observe that with a very reasonable additional assumption, well, weak factors are nowhere. I mean, the rate weak factors are nowhere. And why is that so? Well, an essential feature which is all too often forgotten. All too often forgotten of the time series panels we are analyzing, and which is also an assumption which is widespread in multivariate analysis, is that the cross-sectional ordering should have no impact on the analysis, or everything should be equivariant with respect to cross-sectional ordering. And so, and this is certainly the case in most of our In most of our data sets, where the cross-sectional ordering is, for instance, alphabetical order or some arbitrary order which depends on the way the data have been entered into the computer. But that ordering should not be meaningful. And all the conclusions and all the analysis we are making. We are making of these observations should be invariant with respect to cross-sectional permutation or equivariant with respect to cross-sectional permutation. So actually, we should look at a panel as the equivalent class of all its n factorial cross-sectional permutations. So time, of course, is very strongly ordered. Is very strongly ordered, but the cross-sectional dimension is not ordered. Well, it's easy to show that if we make that additional assumption that the process, so the collection of all XITs, where I ranges over capital N and T ranges over capital Z, if this process is exchangeable, then there cannot be rate weak factors. not be rate weak factors. All factors will be linear, their explosion will be linear. And that takes care of this embarrassing notion of rate weak factors. So super strong, rate super strong factors are ruled out as well. So all the divergences of eigenvalues, etc. Of eigenvalues, et cetera, will be linear, or then you have bounded eigenvalues. And first, from an intuitive point of view, where is, yes, from an intuitive point of view, let us look at these two pictures on the first figure. Well, they are the same cross-section, but with a different order. So on the left-hand side, So on the left-hand side, the small loadings have been listed first. So you have a small increase, a slow increase, excuse me. And then at the end of the cross-section, we have the big loadings, the larger loadings appearing. And so you have this shape, which is here, a slow increase and then a much, much faster increase. And here is the same panel. And here is the same panel, but here we first listed the cross-sectional items with the heavy loadings and then the items with low, small loadings. And of course, that's what we see. What we can see is this red curve here. And they represent the same panel. So here you would be tempted to go for a very You would be tempted to go for a very fast increase in the non-observed future of the cross-section. So, this is the cross-sectional future as n goes to n plus one, n plus two, etc. Whereas if you see this, you would be tempted to say, well, cool, it doesn't go very fast. And maybe this is because I'm dealing with a weak, a rate-weak factor. Well, in both cases, you should forget about the ordering. And if you forget about the ordering and take a random permutation, well, typically you will have still the same panel taking this form. I mean, the divergence of the eigenvalue will be of this type. And then it's quite natural to have an assumption that it will go on that way. Assumption that it will go on that way, which is a linear explode. But now it's possible to show this in a more rigorous way if we add the assumption, which is extremely reasonable, the assumption that the cross-sectional ordering has no meaning. Well, mathematically, the mathematical translation of this is that the corresponding process has cross-sectional Has cross-sectional exchangeability. What does that mean? Well, the process XIT is cross-sectionally exchangeable if for any k and any k tuple i1 i k and any permutation pi of the integers one up to k, well, this k-dimensional stochastic process here, and this one where we have just permuted. One where we have just permuted the cross-sectional items have the same distribution, which means that the cross-section has no impact on the distribution of the whole process. So the process XIT. And well, this assumption has been used in a recent paper with. Used in a recent paper with Matteo Barrigozzi, Matteo Luciani, and Paolo D'Astaroni, where we obtained asymptotic distribution results for the general dynamic factor model, for the estimators of the general dynamic factor model, by assuming that this kind of future of the cross-section will be grossly the same as what we have been observing. What we have been observing. And so, this is an assumption which is both very natural and very useful when you do cross-sectional asymptotics. Under this assumption, but I see that it's six o'clock and my time is elapsing, it's very, very, very easy to see that if you look at the growth of an eigenvalue, well, it's the sum of the increase. The sum of the increases of the increments of that eigenvalue. And under exchangeability, these increments are identically distributed. Not necessarily independent, but identically distributed. And then the expectation of the eigenvalue for dimension n of the panel is just the. Is just the sum of expectations of these increments, then it is n times the expectation of a single increment, and that is linear. And if the expectation of the eigenvalue is growing linearly, well, the expectation, the eigenvalue itself cannot grow at sublinear rate or at superlinear rates. Super linear rate. So it has to be linear as well. And then the conclusion is that, well, under exchangeability, we get rid of this idea of rate weak factors. So rate-weak factors then are nowhere to mimic the title of Gozing, Rust, and Eisler. And Deichsler. Now there's a problem of undetected strong factors. Typically, in practice, for a given n, there will be strong factors, so they diverge linearly, but they diverge linearly with a slope which is too small to be detected, to be distinguished from boundedness in the finite n realization. And a realization we are dealing with. And here I would insist that, well, do not throw the idiosyncratic away, because such undetected factors, despite the fact that they are strong, will go to the empirical idiosyncratic. So they will be there and not in the common component. And if you do not keep the idiosyncratic, Be idiosyncratic. If you throw it away, as many people do, well, you are throwing away these undetected factors as well. And that only can harm your forecast, your forecast, for instance. It's really like throwing out the baby with the bath water. So, my time has elapsed. So, I would insist that. I would insist that the idiosyncratic is not an error term, it's just a non-pervasively cross-correlated component of the observation. And it is to be treated with different statistical methods than we can use for the common component. But at the end of the day, for instance, if we are doing forecasting, we should forecast the common, we should forecast with other methods, we should forecast the idiosyncratic. We can do, for instance, a component-wise forecasting or use some sparse method to produce forecasting. And we should put the two forecasts together again at the end of the day. At the end of the day, in order to produce a forecast for the axis. Okay? And if we do this, well, we don't have this problem of undetected factors because they will be used in the forecast for the idiosyncratic. And since they are not very strongly cross-correlated, or if they were, they would be detected, it means that we don't lose much by. Lose much by treating the idiosyncratic component-wise and having these undetected factors also handled component-wise. So we don't really lose them and our performance will not be severely affected by the fact that they are undetected. Now, conclusions: well, what should you bring? What should you bring home? First, the dynamic approach is nesting the static one. Second, under the very natural, I insist that it is a natural assumption of exchangeability, rate-weak factors, whether static or dynamic, are nowhere. So there's no such a thing as rate-weak factors. On the contrary, the Gersing-Russ-Deissler weak factors are everywhere. Factors are everywhere, but they are taken into account in the dynamic approach. And so you don't have to bother about them. They are taken care of if you use the dynamic approach. This is another argument in favor of the dynamic approach. Keep in mind that factor models are not a dimension reduction technique, and the idiosyncratic component should not be Component should not be discarded. Of course, you will tell me, oh, in this data set or in that data set, it's well known that idiosyncratic has no contributions. Okay, but that's for this typical data set. As a rule, there's no need for that to happen. Okay, so typically, don't throw away the idiosyncratic. You can lose much by doing that. Undetected factors, if you do keep the idiosyncratic, are not a crucial problem. And to conclude with, I'm pretty sure that these conclusions are likely to extend to the matrix and tensor-valued factor models and also to the spatial temporal ones. Of course, that will require some more work since, for instance, the dynamic of Since, for instance, the dynamic approach still needs to be developed in that context. And I thank you very much for your attention, and I apologize for exceeding my time by five minutes. Thank you. Mark, can you hear us? I'm still there. I'm done. He cannot hear us. Yeah. I do hear you. You do hear? Yes. Okay. You hear the I hear you, but the sound is not very good. It is loud, but the voices are not. I have a hard time understanding because it's kind of blurred. Beginning was very good. So if you ask a question, So, if you ask a question, speak slowly because the transmission is not very good. Yeah, the very beginning was very good, right? Is this better, Mark? Yeah, this is better. I don't know why, but indeed. I am using another set of microphone. Any questions? Okay, Mark, I have two questions. One is the exchangeability. Does that make all the times, individual time series, more homogeneous and distributions? Not necessarily because their distributions could be quite quite, you see. You see, when you look at the data as a process, XIT, a big process, well, even though it means that the data generating process itself is random. In the static case, this is an approach that has been used, for instance, by Bay, where he considers the loadings as being random. And so there's a distribution for. And so there's a distribution for that. And in this case, well, it's a little bit more complicated because it's not parametric, but with exchangeability, it means that all the cross-sectional orderings are equally likely. And the random process which generates each series in the each series X. Each series XI, for fixed I, itself is identically distributed. So they all have the same probability of being this time series and that time series. So this generation data generating aspect is identically distributed over the cross-section. That does not mean that once they have Once they have been generated, the time they are not the same. I mean, the realizations are not the same. And then you may have completely different processes for different cross-sectional items. But as a global process, the whole double-indexed process, that one. Process, that one produces a panel of dimension n, and that panel has the property of exchangeability. So all permutations are equally likely at the same distribution. So we're so we're not allowed to have sub-panels in any way. No, if you have sub-panels, then you should mitigate your exchangeability property, and only in the sub-panels you would have exchangeability. But the sub-panels will introduce weak factors, right? And they may introduce weak factors when their dimensions. Their dimensions are not linear functions of each other. I mean, so if some dimensions for some sub-processes or some sub-panels are growing more slowly than some others, then indeed you may have rate weak factors. But that's the only. Factors. But that's the only case I can think of. Right. Within the sub-panel, they will not be rate weak, but their rate will be N sub K. And if that N sub K is growing more slowly than the other N sub K's, then indeed you will end up with having a rate weak. Rate weak factors. I remember you have this paper that gradually increased the size of panel in order to detect the number of factors. Does this exchangeability come from that idea? Because that will require