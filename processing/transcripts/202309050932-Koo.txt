Moving forward to a talk about cryo EM and cryo chains, heterogeneous reconstruction of molecular assembly of semi-visible chains from cryo-EM images. Thank you. Okay, so thanks for inviting me to give a talk today. So my research concerns the preview construction of proteins from cryogenia images and then so I'm trying to like extract the Extract the heterogeneous confirmation of heterogeneity specifically from the cryogenic images. And also, while doing so, I try to make sure that the shapes are biophysically possible. So  Maybe you escape and you project the games. Okay, now we lost. Okay. Yeah. So I'll briefly go over some background on crying, but the previous presentation covered the basis on cryogen, so I can save some time. And then I'll introduce I'll introduce our method called CryChains and show you some results. And if time allows, I'll touch upon some of the exciting ideas I want to pursue in the future. Software's backgrounds. Yeah, it's already explained in the previous presentation. To save time, I'll just skip it. It's got Nobel Prize, and after that, it became the D method for imaging Uh for imaging uh biomedicals because you can get a new atomic solution on that. Wait a minute because I think you're just off your state.  So why do we want to reconstruct the 3D structure of the product in the first place, as a question? The first place as efficient. That's because yesterday, as Felix already said, the shape dictates the functions. So the protein shape, or rather the shape changes of the protein, can tell us its function. So for example, this shape protein here is a charper protein that helps other proteins fold properly. Proteins fold properly, and then it does bind with ATP with this flushing movements. So, by knowing this kind of shape changes of protein, we can influence function. Yeah, so because the proteins can have this many different kinds of conformations, that when we try to Confirmations that when we try to reconstruct these protein shapes, there can be roughly two ways of doing the reconstruction. First is the homogeneous reconstruction, which just averages over the different conformations. So naturally, you can get this blurry structure at the end. And then, on the other hand, this heterogeneous reconstruction can somehow return. Can somehow retain the representation of these different confirmations inside the method. So it's more challenging, but it's more useful for us because by having this heterogeneous representation in the method, we can correctly represent the conformation changes happening in the cry images. So here are some of the here I'm going to introduce some of the recent methods doing the reader construction from cryoin images and I'm just going to show only the generative models here. And then yeah there are some homogeneous methods but recently most methods are doing the heterogeneous The heterogeneous methods. Because it has some advantages over the homogeneous methods, as I just explained. So in general, because all generative models got this hidden variable, latent variable here. And then these latent variables are usually Z. Z, which represents the actual conformation of the protein, and then this RMT, which are the pose of the proteins in the cryon images. And also in the pipeline, they have this representation for the actual protein structure, and then they can be either vocals or implicitly as a neuron. Implicitly as a neural network or the Gaussian mixture of the atom positions. And yeah, this is our method, which I'll introduce you shortly, uses this Gaussian mixture model of the atom positions. And yeah, because we are using this atom coordinates explicitly in the method, the pipeline looks like this. So we take Line looks like this. So we take the we have this atom reference atom coordinates, and then the network tries to predict how these atom coordinates move to match the shapes appearing in the input Croy and images. And then one important thing we try to address here is that Address here instead. How do you put the order? Oh, we get it from KTD or somehow. Yeah. Start with a phone. Very low resolution. Yeah, I mean it depends on the data we get, so yeah. Yeah and yeah when we are doing um the one of the things we try to do is that um make sure that the our reconstruction is biologically uh biophysically plausible. Because the most methods they can uh reconstruct really um nice uh structures, but it's not guaranteed that um the structures are quite physically Biophysically actually realizable because most of the methods don't have this kind of biophysical constraints inside the method. So that's one of the things we try to address here. And then as biophysical constraints, what we are using is the normal remote analysis. Normal remote analysis is the harmonic approximation of protein's conformation changes around equilibrium. And yeah, I want Yeah, I won't go into details, but basically we can write down this potential energy of the protein's conformation changes around the equilibrium. And then we simplify the equation, and then we get only this second order term, which is the Hessian matrix. And then with the ideal decomposition, The eigen decomposition, then these eigenvectors are these eigenvectors u are the normal nodes. And then here the eigenvalues lambda represent the frequencies of atoms oscillations. So when lambda is small, it means that the frequency of the atom oscillations are small, which means that which represents some collective atoms. Atom movement, and then when lambda is large, it means high frequency, so it's like a little oscillation of atoms. So, in general, we are more interested in collective atom changes representing the conformation changes. We take the first few eigenvectors, like first k, for example, first k K eigenvectors, and then we add this linear combination of these first normal modes to the reference conformation of equilibrium. We can obtain this x alpha, which is a deformed conformation around equilibrium. So basically, NMA gives us Basically, NMA gives us the principal directions of conformational changes. So it basically tells us this atom moves this way, this way, this way when we're moving, like around the human beam. And then if we wanted to incorporate this NMA into the network architecture, one simplest thing we can think of it as just make the network uh predict this alpha. Predict this alpha in such a way that this deformed confirmation matches the confirmations appearing in the tri-in images. And that's our previous method. And then things good. So one caveat is that NMA is the harmonic approximation. So here the black line is the actual energy surface around the Energy surface around the equilibrium, but this harmonic approximation is quadratic. So it looks like this. So when we move farther away from the equilibrium, it's increasingly becoming inaccurate. So it basically can only represent small changes around the equilibrium. So, how do we incorporate the large? Incorporate larger conformation changes. Before I'm going to the detail, let's look at the structure of the protein. So proteins have four different levels of structure, from sequence of residues up to protein like this. But there's another kind of um structure, or rather a functional one, uh called domains. So called domains. So these domains in protein can move and fold into places separately. And oftentimes the conformation changes involve this kind of rotation and translation, the rich-bodied transformation of these domains. So we want to incorporate this kind of this matrix transformation of domains into our network to represent larger Network to represent larger conform changes. But for now, to make my life easier, let's zoom up a bit and then use a bit higher level chains instead of domains. So our method basically uses the normal net analysis and as well as the rotation entry installation. As well as the rotation intensification, so that we can represent small conformation changes as well as the large one. So this is just an auto-encoder architecture. So the input image goes into the encoder, which is just a CNN, and then it encodes this input image into the name. This input image into the latent space where you have this normal motor weights and rotation and translation latent variables for each chain. And then it goes through the decoder. Our decoder doesn't have any trainable components. It's just first deform the reference confirmation using this alpha and RNT to make the correct confirmation. Which make the correct confirmations, and then it goes to this image though, which is try the image simulator. Sorry, so you're inputting images to DMA. But in this case, in this example we showed you have two chains basically to both parts that use basically the whole path, right? Yeah, it's basically the whole time, right? So for now, we have to set manually the number of chains depending on the input data. So I'll show you some results now. So currently we only have done the synthetic experiments, so because it's synthetic experiments, we need to generate some synthetic data. Some synthetic data. So first we take two different conformations of a protein here, which is your protein. So we get this closed conformation and an open conformation. And then using this closed conformation, we deform it using renewal modes and then rotation of uh each chain randomly uh to generate the data with different levels uh of uh noise. Of uh noise. And then during training, uh we take the open confirmation and the network tries to uh infer the correct confirmation appearing in the input image by predicting the correct rotation translation and normal mode weights. And after the training, we compute the And after the training, we compute the RMSD between the ground truth and predictive atom positions. Here at the N100 is our previous method, which is doing the whole protein normal merger. And then the next three are abolition of our full remote cryochains. So the CN50 is the part chain normal remote analysis again alone. Analysis again alone, and then CR, per chain rotation only, CRT, per chain rotation and translation only. And then, if you can see here, our method is comparable to the previous method, the whole protein NMA, and looks like a bit disappointing, but look at the actual discarba protein. The actual GABA protein conformation changes. The conformation changes are quite small. So in this case, what happens is that these whole protein NMA can account for the most of the conformation changes happening here. That's why our method and the whole protein NMA is. But then this one is. But then, this one is experiments on histogram protein, which have big large conformation changes. Our method outperforms the previous methods significantly. And we looked into the latent space of the trainer model. So, from the trainer model, we embed the latent variables, the alpha and RNT separately onto the first. Separately onto the first and second principal components using PCA. And then we sample along the first principal coordinates and then reconstruct the 3D structure. And then as you can see here for no remotes, the conformation changes are quite small, but you can see that the rotation and translation induce a larger conformational changes. Fine? You could take a few more minutes because I want to just briefly talk about some ideas for the future of search. So I just mentioned I'm using the chains instead of the domains to make the problem simpler. But now it's time to actually move. Now it's time to actually move to the domain level to make the reconstruction more accurate. And then I think that we can even go lower level than domains or something like sub-domains or even some groups of residues. And then, but it turns out this domain assignments is not very trivial. It's automatic, especially automatic. Automatic, especially automatic domain assignments, is quite difficult. There are not many people working on it, and then there's a database called CAD, but these entries are not exhaustive. But we found this method basically computing the cross-correlation of the non-remotes covariance. So these are called cross-correlation matrix, and then the X and Y axis are the atom. These are the atom indices in the protein. And then when it's close to one, the two atoms move together in the same direction. And then when it's close to minus, it moves in the opposite direction. So by grouping these similar chunks of cross-correlation, like here, the yellow four, you can think of these as. You can think of these as domains. But we just inspected it by an eye, so we need some actual algorithm to do this clustering properly. So one of the ways to do is the hierarchical clustering. So hierarchical clustering basically starts with all the data points, in our case atoms. We merge two items. Merge two items into one when the distance metric is smallest. So we do this merging recursively until we have only one with all the data points. So in our case, all the data points are atoms and then we move up and then at some point we can get like subdomains and then then domains and then chains, etcetera. So I think this is one of the ways we could And this is one of the ways we could try to get the domain assignment. And another thing is that this is very speculative, I don't know how to do it, but I want to somehow simplify the ND simulation so that we can use in the deep learning architecture. And then, related to that, I think we could use topological deep learning, which is a generalization of graph neural networks. Generalization of broad neural network. So, in topological deep learning, we could encode the interactions between different lengths, not just nodes to notes, but for example nodes to face or not to volumes, etc. In this way, I think we can get more accurate represent interactions between elements in a protein or between even proteins in a cell. Um yeah, I think that's it. Questions? Yeah? So the the video on your first slide this is basically from your simulated data set, right? Yeah, just uh so close enough. I was just noticing that there seems to be some really strong clashes, right? Chains are moving, right? Yeah, the thing is, uh, of course. Yeah, the thing is for this teacher protein from the yeah from the paper they have these keyb entries for open and closed have that and then I produce this animation from Himero X so I don't know like I mean I'm just arguing that you know execute possible MD simulation so I think they're just doing Simulation, so I think they're just doing some kind of linear approximation, which is low, I think, right? And the second question that we each come from. Yeah, again, let Jeff ask. Sorry, do you try to? So when you are doing your model mode, you got in front of maybe springs? Yeah, is that how that's elastic, that's elastic, yeah. Elastic, elastic, yeah. Uh Jeff? So I'd like to know some more details about the if out of the encoder everything is the rotation and if it's all predicted sort of independently or if you do any sort of conditioning. No, it's just like a separate branch. So I think what you might want to look at is having some sort of constraints between, for instance, Between, for instance, the translations. Like, if you have the subdomains, but actually they're connected, then you can put the order at that. And a simple way to do that, maybe you've thought about this already, is just if you don't really predict, if you have five domains, you don't predict five translations, you predict four and then build one up to the other. Yeah, yeah, yeah, that's one of the ideas. Um so there's no would you do that with rotations if you had some prior information about If you had some prior information about if one was rotated here, it would probably only be within this area that it would be rotated. Or how would you couple the degrees of freedom together between the genes in a general way? Yeah, first here we don't have any like graph or graph or hierarchy. So the first I can I need to actually build the graph of the atom. In terms of the construction, the In terms of the um constraints, uh yeah, I don't know. I mean, we can go into like even the covalence of bonds between atoms, right? If there are like two links, then it's like you can't really evolve it and that kind of thing. But I'm not sure if I want to go into it makes making authentication harder. Yeah, so yeah. So maybe some reasonable just what has an angle of limitation. What's that an angle of limitation for now? First try. Thanks. One question, Jim? Yeah, sorry. This could be an IE question. Well, you mentioned biophysically plausible constraint. Normal mode analysis. Is that just one coordinate, or do you also have the energy force potential field concept? Yeah, yeah. Do you actually know and then do you have like a Do you actually know that do you have like a force field model, like a Andre or whatever? Because otherwise, essentially your method is a free analysis which compiles the space and use that removal great point. So if you have a force, then that's what constraints bashes, right? Because I'm just wondering whether you could extend this. Yeah, I mean I like the when you run in D, you will get that. When you run in D, you will get that. Yeah, it is very modified. This is just a linear passimolation. It's like there are electricals. Yeah, and I think my my other question was for the choice of the chains. I think if you just look at the chains, actually that's the one that's a little bit more. Yeah, yeah, but like actually my question is the Actually my question is that what are the chains in the KDB? Because I looked it up and then these are like a red protein, these are just the parts. If you have a complex, you know, if you have a single protein, that's another thing. But I think what you're describing is applying to more complexes rather than proteins. So I think the change you already kind of get help from the protein. I I think that's that much. Okay. And then do you have any idea about like And then do you have any idea about like domains, how to get domains? That's another question, but there's a there's a nice paper where you model building. But they have this uh idea also uh once you fit an atomic model then you define domains. And maybe you can take a look at this paper. Nature communications also. A few more questions and then go to break. Yes. Why not use the secondary structure for to define those chains? We have a alpha We have a alpha helix, so you know trying to move together, right? It's a great thing. Yeah, so we are starting from like a highest level, like a largest level. And then now we are trying to go into more low level, like domains or even like alpha elixirs or real additions. And if we move go into like a lower um lower levels, it like kind of moving towards the ND, right? So there can be like trace off in terms of computational. Trades off in terms of computational. So we yeah, we try to we're trying to go into more like a low levels. And the last question, Jeff, and the rest will be a big upgrade. So for partition, for finding the domains, you might want to, since you have a way, the phenomenal modes of getting basically a graph, you can look into optimal partitioning work with like these lumpable markup genes and use And use, so there's some papers I've looked at for that. And I just couldn't think of how to. Yeah, so once you have edgeways that are scalar, then you can use that type of theory. And you do some sort of random walk. And so there's some papers from the 2010s that you can work on. Yeah, no, I've read these closely. Yeah. Right on time.  So what what do you say? The fact is, you can just create a graph and basically some positions. No, but this is just an error from the status. For possible things. Also, you only have something to get those matrix that doesn't participate, but then you're just going to have to do it. So you're you don't have to use the so I'm just in this level I don't have a character I just want to check it's it's all of course   Yeah, but I mean, it just tries to work on flashing. So you have to install the queries to ask it. See if I really need all these different parts of my