Good afternoon everyone and first of all I want to thank all organizers to invite me to give a talk in this fantastic workshop. This is a recent research work on multiple model assisted approach to missing data problems in survey sampling. And this is a joint work with Dr. Danyan Lee from University of Alabama and Dr. Li Chun Zhang from University of Southampton. Symptom. So, this is outline for today's presentation. So, as we know, that missing data problems happen a lot in practice, contents unit and item response and has been studied extensively in the previous literatures. And lots of problems can be considered as special cases for the missing data problems, including the causal inference, statistical. Inference, statistical disclosure control, data integration, measurement error models, and the latent variable models. And there are many existing methods for handling missing data, including propensity score weighting method, and the validity of the method depends on the propensity score modeling. And imputation method and also depend on the imputation model assumption. And then to improve the And then to improve the robustness, people has been proposed using the doubly robust methods. Those methods used two different models, use both propensity score models and imputation models. So it produced consistent estimators if one of the models is correctly specified. Recently, multiple robust methods have been proposed. So instead of using only two models, people propose using multiple different Propose using multiple different models to further improve the robustness. So, here are the research objectives for our project. So, we want to propose a new class of model-assisted estimators based on multiple working outcome models. And those models can be non-parametric, semi-parametric, machine learning models. So, you can put any models you want. Any models you want. And a single non-parametric response probability model. So, this response probability model will consider cell-mean models. And we want to propose this method, which is more robust than the existing multi-player robust and W robust methods. And then we want to establish asymptotic results, including the variance estimation. And we want to evaluate our proposed methods. Our proposed methods through both simulation study and real application. Next, I want to introduce the mathematical notations for our research. So we define the final population of interest as a capital U. So in this final population, we have capital N units with a known population size. And then we assume we have a study variable of interest Y, which is subject to Which is subject to missness, and a vector of covariates x, which we don't assume there's any missness. And we assume delta i as the response indicator, and it equals to one for the respondents and zero for the non-respondence. And we consider probability sampling design such that the design weight has been defined as Wi. And for simplicity, we consider estimating the population. Estimating the population total in this presentation. And we assume there are two models. First, we assume the non-response model follows the missing and random assumption, and it has the unknown functional form of P depends on the unknown parameter alpha zero. And there's an outcome regression model and with unknown functional form M with unknown parameter beta. Unknown parameter beta zero. And next, I want to move on to our proposed methods. Our proposed methods include different procedures. The first procedure is a prediction procedure. So first thing we define our sample S can be written as SR union with SM. SR is the respondent sample and SM is the Sample and SM is the missing sample. And then we randomly split the respondents into training set, training set S1 and the testing set S2. And then the training set is selected from these respondents by using simpler sampling. And we denote this sampling process as Q. And in the first phase, we fit a shoe. First phase, we fit a suitable working model to learn why based on the training set is one. So in the second phase, then we observe the error of the first phase model in the test set is two. And then one can use any suitable working model for Y, which include parametric model, semi-parametric model, non-parametric, or machine learning models, such as regression tree and the forest, and any other methods. Any other methods to feed the model in the training set. So we denote this view as the predicted value of the study variable y for unit i with features xi. And this is trained on this training set is 1. And then for any j not in this training set, we can define the prediction arrow ej as yj minus mu. As yj minus mu. So once we define this ej, then we can express our estimator, which if you assume no missingness, it should be written as this weighted sum. And then it can be written as the first part, this is just for the observed training set, wi times yi. And the second part, instead of using yi, we replace that by using the u plus. The u plus EI. So this zeta hat can be written as this formula. And this EI are observed with prediction EI are observed in S2, but missing for all the missing respondents. So we cannot directly use this formula, but this formula will motivate us to produce our estimator. To produce our estimators. So to write down the estimators, we need to define the cell mean model. So in this research, we define this cell mean model for the response mechanism. So we assume that population U is partitioned into G cells, and we assume that the probability for each cell is Pg. Is Pg. And this position can be constructed by using the quantiles of predictor X, or you can use a parametric model to feed the response mechanism. Then you can partition by using the response model. And we will treat this X1, X, and Y as fixed constants when it comes to the inference and the very To the inference and the variance estimation. So you can basically treat this as a desire-based inference because this X and Y they're treated as fixed. So for the cell probation, one can also use some other types of modeling techniques like non-parametric model or machine learning model, which will be more robust than using the parametric model to form the cells. To form the cells. And that has also been demonstrated in our numerical studies. So let's move on to our proposed estimator. So we first define the induced probability of subsampling S1 from S given by this formula, P1. And then we can also introduce the second phase induced probability. second phase induced probability P2J, P2G. This is the probability I belong to the S2G. So S2G is the testing set for cell G. So this can be written as N2G divided by Ng minus N1G. So it has this explicit formula and can be calculated by using the data. The data. So once we define those two induced probabilities, then our proposed model-assisted estimator can be written as this formula in equation four. So the first term, this is a weighted summation within the training set. And the second term, this using the complement set of the training set by using the weighted. set by using the weighted sum for the predicted values. In addition to that, we have a third term and this third term basically estimate the predicted error because this if you take expectation for the third term actually the delta i and p to g canceled out. So those two terms canceled out. So this estimate the predicted So, predicted error for the S1C. So, okay, so then if you take a difference of our proposed estimator to the full sample estimator theta hat, then you can have this formula at the button. And you can see if you look at this formula, so if this model, mu, predict y very well, then the expectation of this year will be. Expectation of this will be close to zero, so you will have zero expectation, so it's unbiased. Even if this model is not predicting y very well, if you take expectation, is dia will be zero under the desire-based framework because this induced probability P2G, if you take expectation, this will be cancelled out. Expectation, this will be cancelled out. So we just expand this page. So in the paper, we proved number one. So we proved that our proposed estimator is unbiased estimator for estimating population total under the cell response model one, regardless of the prediction models. And then to reduce, to further reduce the variance, we propose to use this Monte Carlo-Raw Blackwell method. So according to number one, one can safely adopt any outcome model for y, but this variance can be, we can introduce additional variance due to the learning from S1 instead of SR. So, as proposed by the previous literature, we can reduce. Previous literature, we can reduce the variance by using this like average estimator. So, this average. So, the idea is that we can do this sub-sampling procedure multiple times, say k times here. Then we can take the average. And in theory, one, we show the explicit formula for the variance. Formula for the variance estimator of our proposed estimator. So, this estimator is unbiased and also has this explicit variance formula here. So, the first term is the variability due to sampling design. Second term is the variability due to the respondence and also the prediction. And then the third term here, we will have this term because that measures. We will have this term because that measures variability reduction of by this condition expectation rather than using the original estimator. And last term is added due to this Monte Carlo raw black rice method instead of the exact method. And in theory, too, we proposed a variance estimator for our proposed estimator. So, each term can be estimated by using explicit formula consistently. And then in the paper, we extend our proposed method to using multiple model-assisted approach. So, instead of just using one working model, so to improve the robustness and efficiency, we propose We propose using multiple different outcome models. Then, our proposed estimator can be weighted as this equation six. So you can see this is a weighted average from different models. And this A N K is a binary constant, either equals to zero or one. So it's an indicator of selecting model M. Model M in the case subsampling procedure. So, in this study, we define this indicator variable equals to one if model M has the lowest prediction errors in this testing set in terms of V2 hat. So, this is the second variance component in our variance estimator formula. So, this AM. So, this AMK will be equal to one. So, we calculate this V2 hat for each individual model. Then, we basically choose the model which with the lowest variance component. And so if you recall that this V2 hat sums up to V hat to MA for model M in theory too, and measures variance due to the imputation by model M. So, that's we can count. So, that's we can construct the proposed estimator to give more weight to a model that has a smaller imputation variance. And as k goes to infinity, our proposed estimator will be very close to this weighted sum estimator. And each of these terms is conditional mean. So, then in the paper, we showed theories three is very similar to the sim. Is very similar to the sim theory one. So we have we show this is unbiased and also have this explicit variance formula. And then we proposed a variance estimator for our proposed estimator. Okay, so some remarks based on our proposed approaches. So our proposed estimator is based on a single cell. single cell response probability model and this which can actually be formed based on one or several response models and this is more robust than applying the response models directly okay so because it can protect against the failure of this model and also against the extreme values for the probabilities and then our estimator adaptive assigns different weights to the multiple Different ways to the multiple outcome regression models. According to their prediction errors, that can be observed in the holdout test sub-sample. So that is more robust. They assume any one of them. So let's move on to the simulation study. So in this project, we used the seminar simulation setup as in the previous literatures. In scenario one, we generated 1,000. We generated 1,000 final populations of size 10,000 as follows. And then Yi was generated from this linear model. And then for each final population, we select a random sample of size 800 by using a PPS sampling design. And in each sample, the response indicator is generated. For example, the response indicator is generated with this probability. So basically, it's a logistic regression model. And this will lead to an average response rate of 60%. For the evaluation purpose, we consider the following transformations of the X variables. So we have we derived transformed Z variables, Z1, Z2, and Z3. So in the correct So, in the correct models, we used this Px alpha and Mx beta. And in the incorrect models, instead of using X, we used Z to transformed predictors in our models. And in the second scenario, we generated 1,000 finite populations of the same population size as scenario one, except Scenario one, except that we now used this quadratic regression model. And then this response mechanism is not a logistical regression model anymore. Note that these other models like correct model and incorrect model specified like MX, PZ, and random forest models are all misspecified in this scenario. And in terms of estimates, And in terms of estimators, we consider the following estimators. The first one is full sample estimators, assume no missingness, and this will be used as a benchmark. And then we considered W robust estimators as in the previous literature. And you can see we have four digits in the bracket here, so the first two digits. the first two digits denote whether you correctly specify or incorrectly specify the rebushing model and the second two digits specify whether you correctly specify or not correctly specify the non-response model so for instance if you have one zero one zero then means you correctly specify the outcome regression model and also you correctly specify the non-response model And the third set of estimators we consider is the double robust estimators. But the response model right now is a cell response model formed by categorizing x1, x2, x3 into 3x3x3 cells by using the quantiles. And then we consider the random forest W robust estimators. So instead of using So instead of using the quant house, we consider using the random forest to classify and make nice cells for the response mechanism. The fifth set of estimators, the multiplayer robust estimators considered in the previous literature. So for the correct models, we use MX and MZ, Mx and Px, and for the incorrect models, PX and for the incorrect models, we use MZ and PZ. Then the sixth set of estimators is our proposed estimators by using the MX model and the random forest model of Y versus X. And the response cells are constructed in the same way as W robust cell mean model. And we apply the Monte Carlo raw black rice. Raw blackwise method with k equal to 50 and 50-50 random split in this method. Lastly, we considered our proposed method with using the random forest to form the nice cells for the response mechanism. And this is the simulation study results. As you can see in scenario one, all estimators except All estimators, except the estimators which use the RAM models, show very small bias, you can see, and that they are very comparable. And then you have also have very similar standard errors. Even our proposed method, we use multiple different models and we didn't lose any efficiency and bias. And in scenario two, the benchmark. The benchmark estimator still performed very well as expected, but other estimators like W robust estimators and multiple robust estimators show big biases, which is not surprising because they use the incorrect models. And our proposed method, MMA, basically using the quantum to form the null response cell, show a little bit bigger. Show a little bit bigger bias compared to using the random forest to form the non-response cell. But they all perform much better compared to other estimators. In addition, we also calculated the relative bias and coverage rate based on our proposed variance estimator. So for this random forest estimator, Forest estimator for both scenarios, we can see we have very low radical bias, and also the coverage rate is close to the nominal coverage rate of 95%. So, this shows the validity of our proposed variance estimator. And in addition, we also applied our proposed methods to a real data application. So, we conduct a monica. We conduct a Monte Carlo simulation study based on the real data. This is a Korean National Health and Nutrition Examination Survey data of 4,929 individuals. In this application, we considered sex, H, HGB, TG, and HDL as the predictors, and they are denoted as X1 through X5. through x5 and the total cluster has been denoted as an outcome variable in our application so for for the empirical comparison purpose we generated response indicator from this model and then we want to estimate the population average so we considered similar sets of estimators Set of estimators as in the simulation study. So for the double robust method, we consider those two models. Basically, it's a linear regression model for the outcome regression, study variable of interest, and logistical regression model for the response indicator. And for the multi-pre-robust method, in addition to those two models, In addition to those two models, we considered two additional models by including both linear terms and or interaction terms for comparison purpose. And then we presented our two proposed estimators, which use both linear regression model and the random forest model. And this is the result. And this is the result. The red line, that's the true estimator based on our data set. And those are the box parts based on 100 Monte Carlo samples of this real data for different methods. As you can see, our proposed multiple assisted method based on when the forest method works very well. Forest method works very well. That's the closest one to the true private of interest. And W robust method doesn't really perform very well. It has a long distance between this and the true parameter of interest. And W robust method based on random forest and MMA method, they have a similar performance. And multiple robust method doesn't perform. multiple robust method doesn't perform as well as our proposed method. So next I will present some final remarks for our research. So in this paper we present proposed a new class of multiple model assisted estimators which are more robust than the existing W robust and multiple robust estimators. And our proposed estimators is a design It's a design-based method. It doesn't really depend on the validity of the outcome regression models. But the outcome regression models, if it's correctly specified, then it can improve the efficiency. And if we consider using multiple non-response models to form the cells, then our proposed method is essentially a multi-robust method as well. method as well. And establishing like extension of our method to other scenarios like high-dimensional or multi-wright data interesting research topics for future research. Thank you. Thank you, Sushia, for a very interesting talk. So any questions from the audience? So, while we are waiting, can you clarify? I noticed on your page 12 Yeah, so what what what is your capital G here? Your capital G here. Yeah, multiple groups for the cell. This is cell mean model for the response model. So, I mean, like, how did you define this? So, I mean, like, how would this cell is related to your sample, little S, and also the response subset, S1? Here, you can see we define this cell mean mode. You can see we define this cell main model. So, this population U partition into G cells, and we assume that you have the same response probability for each cell. Oh, so basically, you factorize your entire population. So in that case, is there any condition related to your sample, little S, and also those respondents? Respondents in detail S1. So maybe because sometimes we may be in that situation our sampled individuals would only come from a single cell, not necessarily from multiple cells. Would that sort of affect the development here? This one, I think the assumption here is the only assumption we need for research. We need for our research is the response mechanism will be well approximated by these cell mean models. So these cells can be developed by fitting the non-response model, like what we did in the simulation and the reapplication, run the random forest, or you can even run the parametric model and using the percentiles of the predicted probability to form these cells. So if you only have one cell, which means Only have one cell, which means everyone will have the same response probability. So, if that assumption is true, then on this framework, it's still okay, still like design unbiased. Okay, so is it fair to say when you partition your population into different cells, you would do that after you collect some or before you collect some, after this is after we. Collect some after this is after we collect samples. Oh, okay. After then, how would you decide like how many sales are you going to do? So, is that more like a database? That's an interesting question. I think how many cells is like, I think when we run random forest, I think we can use the cross-validation or something like that to, because basically this is to predicting the response indicator. Response indicator. This is a, we can treat this as also as a learning process because our response variable is the indicator, response indicator. Either response or not a response, then we have predictors, right? X. So if we run random forest, like a random forest, how many cells or tree method, how many cells we want at the end. Okay. So I think maybe another quick question is related to Another quick question is related to. I forgot this slide page, maybe 11 or something. So you talk about, oh, maybe I forgot. I think like you have definition of little S1, little SR. Yeah, oh, I think it's this. Yeah, so here you said you would randomly split this subset SR, which includes the responders, right? Responders, right? Into training set and the test set. So I wondered, I mean, I mean, like many times we do split our data randomly. But I wondered, because since this is in the survey context, so before you work with your data to do whatever analysis, you already have like a design-based Design-based survey weights or survey or like a sampling information or sampling design. So, would it be an issue to also incorporate that information when you try to decide how to split your collected data into test and training data? Or would that be a concern or it doesn't matter? Here in our research, it does not matter because the splitting with Because the splitting we propose is a simple sampling, so it doesn't really depend on the sampling design. So, here it doesn't matter. Okay, because I'm kind of thinking, what if you have like a two-stage sampling or you have a more complex like cluster-related sampling stages, then you may have lessed information. So, I guess maybe probably in that context, you sort of don't use random. Don't use random split. I don't know. Let me just a quick question. Yeah. As long as your imputation model, I think it's quite specified or reasonably specified. Here, I think our proposed inference is design-based. So we consider very general design here. So this is any design that will work. Yeah, but you only have like a for each subject, you have a single design weight, right? Instead of like a cluster-wise or multiple level related design weight. Yeah, but anyway, yeah. Okay, thank you. Any questions or comments from the audience? Society, thank you for your talk. I have a question. So, when you have multiple choices for your model and you choose one that minimize the prediction error, and then you use that model to construct your estimator, is that correct? Yes. And then your estimator actually is up to model selection, right? Right. Right? Right. And usually estimators doing selection will have non-regularity issues. Have you considered this? Here, let me see. Oh, you may hear. So there might be some like model selection process. Moodle selection process going on. Um, we didn't let me see here. But so, if we go back to this previous estimate, then we go back. So, I think when we derived this variance, we didn't, somehow, this variability just went away because of these two induced probabilities. So, we didn't see any necovariable selection variability show up in our derivations because of the induced probabilities. Then, which step eliminates the model selection issue? That's very magic, right? Okay, let me see. Is it because you assume model is always consistently selected? I'm thinking whether you assumed that the optimal model is always consistently selected. I think I need to. I think I need to check more closely. But in the derivations, I think probably I need to check closely for this to see how we derived. Great, let me know afterwards. Okay. Thank you. Thank you. Thank you. So it looks like any more questions? Any more questions? If not, this concludes today's sessions. So let's thank all the speakers very much for their interesting talk. Now, our last item is, so maybe Sushia, you can unshare your screen. The last item is to take a picture for everyone on Zoom. So please turn on your camera. turn on your camera if you want please also change your name so that people can easily match match the photo with the names i think this could be a good memory so are you going to look after taking this picture while people are getting ready yes uh Yes, wait a second. Yeah, sure. Yeah, whenever you are ready, you just call people's attention, then we can smile and make a nice pose. I actually, I don't use Windows. I don't know how to take a screenshot with Windows. Hi, Jay. Nice to see you. Is it this one? I think it's that. This one, I think it's that. Let's see your place. Oh, control that. Yeah, I heard your talk yesterday, right? Oh, I don't remember all the day on Monday. Monday, okay. We have technical issues. Wait a second. I'm not sure about other. I'm not sure about other people. Are you there or not who haven't turned on your camera? Maybe you want to leave a nice picture with us if you if no pressure. Thank you very much. I don't know. Do we have anybody? Because unfortunately, my computer somehow I just changed my computer somehow. Yeah, I don't use window. I don't know how to. I don't know how to make window. Can you figure out shoot quickly? Oh, yeah, yeah, right now. I think we got the picture. Oh, we haven't let people smile there. There it is. Okay. Yep. So we're going to smile and say cheese in three, two, one. Beautiful. Thank you. Thank you, everyone. Thank you, everyone, and nice to see you all. So, tomorrow we will resume at nine o'clock Vancouver time. And have a good rest of the day today. Thank you. Thank you. Thank you. Bye.