Thank you very much for the invitation to the meeting. I'm delighted to be here again. And the weather looks like the weather is picking up. Damn. Just hoping it was going to be a nice wet day. So I'm going to give a statistician's view of this problem. That's me. The statistician, of course, is me. Other statisticians in the room will doubtless have different um uh views and I'm expecting a certain amount of of of abuse from any Bayesians who might be here. Abuse from any Bayesians who might be here. So, does this go to work? You might have to arrow key, though. Not advancing. It's not. You can just do it on the score with the arrow keys. Okay, so what do we want to do? We want to aim for frequentist inference, in this case. Is it large enough? It seems a bit small to me. Is it okay at the back? David, I can read it. Okay. So, what we want. You can read it. Okay. So, what we want, we want frequentist inferences, but we want the frequentist inferences to be relevant to the data set we've actually got. We want, in the sense that we want to compare the data we actually observed to some reference, so Quentin's inference involves reference sets, and those reference sets we want them to be in some sense like the data we actually observed. We want them to be calibrated in the sense that. In the sense that the probability statements we subsequently then make with respect to this reference state are accurate. In the sense of R. A. Fisher, who said these wonderful words, we may at once admit that any inference from the particular to the general must be attended with some degree of uncertainty. But this is not the same as to admit that such inference cannot be absolutely rigorous, for the nature and degree of the uncertainty may itself be capable of rigorous expression. Of rigorous expression. So basically, what he's saying is: you might be unsure, but you'd like to know exactly how unsure you are. And the final comment I want to say is we also want our inference to be secure in the sense that the conclusion is not going to depend too much on secondary details of how we formulate the problem. It's not too strongly messed up by bad data. Sorry, Anthony. Why the word efficient is not there? Well, that will cover. Well, that would come out automatically, but we would also, of course, like it to be efficient. That comes in some sense down to the reference set. Because we want the reference set to be the correct reference set, in which case the thing would be efficient. Okay, so this goes in the wrong direction. Here's a trivial example of the relevance. And this is just a textbook example to make the point. We've got some observations from uniform distribution, unknown center at theta. Unknown centre at theta and width half on either side of the unknown centre. The minimal sufficient statistic, which contains all the information about theta in the data, is just the maximum and the minimum, and we can convert those without any loss of information to being the range, the difference between the two, and the average of the two. The range itself is just some number that tells us nothing itself about the. Nothing itself about theta, which the information about theta is contained in the average. And so what we're going to want to do is to re what we can do is to rewrite the joint density of the data here in terms of the joint density given the minimal sufficient statistic, which we write like that. There's no theta in the first bit. Then we rewrite this second part, the joint density of the range and the average in these two. The average in these terms here, and then we have this is the density of the average, sorry, the range, and this is the density of the average given the range. And it turns out that we can easily work this out in this example, of course, that the density of the average given the range is this thing, and what this does essentially is tells us that the precision of the subsequent inference is going to depend upon this A, the range, even Is the range, even though we know that the A in itself conveys nothing about the precision. And just to give you the picture, here, this is the idea. We observe, in the case with n equals 2, we observe these two points here. This is what the density, the potential support of A and T, in this case, the range and so on. This is the value of the average, this value here. This value here, and what we observe is that the value of A is this got this value up here, point is 0.7. Okay, so what we want to do is to compare this set of data with other sets of data that are going to have the same value of A. If we do that, the confidence interval we get is this, and it's perfectly calibrated. We could also ignore A and just compute things based on T, and then we would have this interval E. On t, and then we would have this interval here, which is wider and is in the sense I mean here irrelevant because it doesn't take into account the precision indicator. And actually, this is not just irrelevant, it's actually logically impossible because this interval contains values of the parameter that would be impossible if you actually observed this particular data set. So, the point is here, as I say, this is a textbook example. As I say, this is a textbook example just intended to underline this issue of lateness. On calibration, what we do here, how we get the confidence intervals, is to base our inference, tests, confidence sets, whatever, on what statisticians call, or some statisticians at least call the significance probability, or the p-value function, which is this function here. We look at the probability that the average will be less than or equal to its observed. Be less than or equal to its observed value, given that a equals its observed value, and we look at this as a function of theta. And as we vary theta, this probability here will change. And we choose, if we want, for example, a 95% confidence limit, we will choose the values of theta such that this probability here is equal to 0.025 and 0.175. Okay, so we're basically going to solve these equations here. Theta. These equations here for theta, and that gives me two limits. And in the particular example, that just gives some details here, they don't matter. The point is, this gives us a unified, simple way to compute confidence intervals in general settings. And perform tests as well, of course. Now, what do I mean by secure? Well, any statistical model, any statistical setup has got primary assumptions and secondary assumptions. Secondary assumptions. Primary aspects essentially concern the question of interest. Usually that involves a basic model structure. Maybe we assume a Poisson structure. And what we, of course, will do is to summarize the key issues in an interest parameter, which I've labelled as psi here. And then apart from the primary aspects, there'll be secondary aspects, which will often, in some cases, will involve the error structure, might involve Error structure might involve the nuisance parameters. They might be things like: Do I believe my observations are independent or do I think they're correlated? If they're correlated, I need to have some joint distribution. But that doesn't change the question of interest. It's just what the answer to that question will be. How will I formulate the answer to the main question? That's one aspect of security. A second aspect is A second aspect is which is kind of more a bit more relevant here, maybe, is that we want the inferences not to be too heavily dependent on the nuisance parameterization. And that can be useful for numerical work, but it's also kind of if you mess things up a little bit, you change the parameterization, you're not really changing the primary question. You don't want the answer to your primary question to change. So, what we aim for is invariants of what are called interest. Invariance to what are called interest-preserving reparametrizations, where we transform psi to some smooth function of psi, called it eta here, and we transform the nuisance parameter to some parameter that I've called zeta, which could also involve psi. And not all methods will achieve in this particular sort of invariance. Of course, we also hope for robustness to bad data and so on and so forth. So now So now, let me talk a little bit about complications in this ideal scenario I've just set up. Of course, nuisance parameter dam, nuisance parameters and discreteness of the data will degrade calibration. First, let's talk about nuisance parameters. Ideally, what we would like to have is this factorization here of our original data set, density for our original data set. We've got this involves the nuisance, the interest. Involves the interest parameter and the nuisance parameter. We like to split that up like this. This is the parametrization, the decomposition we saw before. And then ideally, we would have the psi in this bit of the density, the lambda totally separately, and therefore we would be able to focus only on this bit here of the light. And then we would, if we have a scalar psi, we would Scale up psi, we would then just be able to do enunciate the recipe I used in the trivial example I gave, and that would give us our ideal inferences. It just becomes a numerical problem. How do I compute this thing for different values of psi? The issue, of course, is that we don't usually get this happy situation up here. Usually, we have factorizations where this first term splits up in one of these two different. Splits up in one of these two different ways, or not at all. If we have this situation here, then there is some psi in the distribution of A. It's just difficult to extract because it's messed up by the presence of the nuisance parameters. So we could still use this, we could still apply the recipe here, but there might be some loss of information. Usually not a lot, actually. And this we call a conditional likelihood. We look at this first. Likelihood, we look at this first term here. For a marginal likelihood, we hope that there's some other marginal factorization where the t and the psi appear together, and then in this other conditional bit of the density, there's the lambda as well. And then we base inferences on this term here, and that we call a marginal likelihood. And depending on the type of model you're dealing with, you may end up be led to. You may end up be led to a conditional or to a marginal likelihood. Of course, in the worst case, there's going to be no sufficient reduction. The data themselves are essentially the minimal sufficient statistic. And then we're going to have to base inferences on some more general thing, where z, this thing that features in this last expression here, is some approximate pivot that's going to involve psi. That might be Psi, that might be the maximum likelihood estimate, it could be the likelihood ratio statistic, or many, one of many other things. And then we're in a bit of a fix because this distribution involves the lambda here. So unlike this expression here, the lambda appears, and we're going to have to replace it somehow, either by integrating it out or by replacing it with an estimate or something. So that's the kind of So that's the kind of situation from a statistical point of view. There is a set setting in which things turn out very nicely, and that is the case of what we statisticians call exponential families. And I'll put the density up here. This is a linear exponential family in what we call its natural parameterization. We've got the t here associated with psi. We've got a. Psi, we've got a here associated with the lambda, and then there's something we call the cumulant generator, which is this function here, k of psi and lambda. And this encompasses a lot of models. This encompasses Poisson models, Gaussian models, binomial models, exponential gamma, etc., etc., etc. So it's lots of models come under this general setup. And it turns out that in this case, we can remove dependence on the lambda by conditioning on this. By conditioning on this A here. And we end up back in this happy situation where we have a significance function that only depends upon psi. We have got, there may be some loss of information on psi, but not usually very much. To come to one of the comments that Bob made in his example at the end, the Poisson model is a bit special. It's an exponential family model. It has what's known as a cut. It has what's known as a cut. This is a language invented by Erlebrandof Nielsen, who wrote a book about exponential families, excellent book, in 1978. And basically what this allows is, in the case of the Poisson distribution, you can write the density in this form here, the distribution of t and a with psi in it, and f of a and some new parameter, zeta, which is a reparametrization. Which is a reparametrization of the nuisance parameter. And what this means is you can retrieve no information on psi from this part of the light. And that explains why when you integrate out psi, you end up with the same answers as before, because psi was never there when you split the likelihood up. And as I say, this is. Okay, and as I say, this is well, as I say, this term cut was introduced by Arnold Nelson nearly 45 years ago. Actually, I think it was in his PhD thesis, which was even like 1968. Okay, so that's having said all that, so these are special models, but there is something called a tangent exponential approximation, which was invented by Nancy Reed and Don Fraser. And you can use this to take arbitrary models. To take arbitrary models, arbitrary smooth parametric models, and give approximate, come up with an approximate exponential family that basically allows you to do this program here, this business, in arbitrary models with a certain level of error. Okay, so this looks like a special setup, and it is a special setup, but you can generalize it so that you can make it work in more. Work in more settings. Okay, let me talk now a little bit about calibration error before I get on to profiling and actually what's the topic of the talk. What we would like to have is if for our inferences to be calibrated in the sense of Fischer, is that if we have some alpha level confidence limit for psi, then we would have this expression here exactly for. Expression here exactly for all alpha between 0 and 1. And in that case, we would get perfectly calibrated inferences. Our 95% confidence intervals would have coverage probability exactly 0.95, our 0.9999 confidence intervals would have coverage 0.9999, and so on. There'll be no big problem about miscalibration. Usually, we don't get that, of course, because if we have nuisance parameters. Course, because if we have nuisance parameters, we have to take account of that. And what then we get is a situation that looks like this, at least in a classical asymptotic setup. And a classical asymptotic setup has p parameters, a fixed number of parameters, the sample size, in some general sense, going to infinity. Sample size does not necessarily mean, I put m here, that doesn't necessarily mean the number of observations. In a Poisson sense, In a Poisson setup, that can simply mean that the means are getting bigger because of the infinite divisibility of the Poisson distribution. Because having more observations is the same as having higher mean, essentially, in that case. So you could have n equal 1, but the mean big, and that would correspond to high information in this setup. Okay, so what do we have here? We have a hierarchy of error terms, an error error. terms, an error n to the minus 1 half, and n to the minus 1, and n to the minus 3 upon 2, and so on and so forth. And first order error is all of this stuff. It's got the first order error is n to the minus 1 half. If you could make a1 here 0, then you will have second order error. The a if you can make a two equal to 0, then uh we would have third order error. And in general, you really can't do better than third order error in asymptotic settings. in asymptotic settings. Can't hope you can try and uh use methods to to go further, but usually the error you introduce just makes the whole things worse numerically. These A, I suppressed it in the notation A1, A2, A3 depend upon the parameters and on alpha. Now in a modern asymptotic setup, we have typically the sample size is still going to Typically, the sample size is still going to infinity, but the number of parameters is not fixed. It's allowed to increase with n. And very broadly, I mean, there are lots of details, but very broadly, the results that would apply in the classical setup, Wilkes's theorem, improvements to Wilkes's theorem, and so on and so forth, apply in the modern setup if the number of parameters is growing more slowly than n to the one-third. n to the one-third. So you can have more parameters creeping into your model and things will be fine provided the number of parameters doesn't increase too rapidly. And this is kind of obvious, about as good as you could hope for actually, because the bias for maximum likelihood estimation is of order p cubed upon n. If you could do some hideous Taylor series expansions and Taylor series expansions, and that's what it turns out to be. So if you want the bias to go away and n and p are going to infinity, obviously n has got to beat p cubed. So for good inference, this is as good as you could possibly hope here. And so that's actually helpful because it tells us that improved inferences will improve e in the same setting as you could use ordinary inferences, if you want. Ordinary influences, if you like. Now, just a couple of comments about this. These are asymptotic results. So this is the result of even more hideous Taylor series, Edgeworth expansions, and so on and so forth. But numerical work suggests rather surprising accuracy for certain situations, even when the asymptotics don't appear to apply. So that's, and we'll see a bit of that later. Okay. Of that later. Okay, let's now talk about profiling, come at last to the topic of the tour. Okay, so under classical asymptotics and when psi equals its true value, the likelihood ratio tells us that the likelihood ratio statistic, this third here, the difference between the overall, the maximized log likelihood, the temperature, in Bob's terms, and the temperature at some other point is going to have asymptotically a chi-squared distribution. Going to have asymptotically a chi-squared distribution with dimension of psi being the degrees of freedom. When psi is scalar, we can equivalently take the square root of this thing, give it the sine of psi hat minus psi, and that will have a standard normal distribution, as the standard norm is the square root of a chi-squared. And both these things are secure in the sense they're invariant to interest preserving. They're invariant to interest preserving parameterizations. That's good. It turns out for these two statistics that one-sided inference has first-order error, one upon root n. But two-sided inference has second-order error. Miraculously, those A1 terms at the two ends, two sides, cancel. The catch with that is: okay, your confidence interval has 0.95 plus or minus, but in one tail there might be zero and In one tail, there might be zero, and then the other tail, 5%. So the interval can be in the wrong place because the likelihoods get shifted across due to the fact that you didn't allow for the profile. So the two-sided inference is not bad, not fantastic, but it's not bad, but the interval is in the wrong place. The modified likelihood root, which R star, which I can give the formula for. Which I can give the formula for if you're desperate for that instead of lunch will give inferences that are accurate for third order. Basically, it shifts the thing back to the right place. So you have two and a half in one tail and two and a half in the other. Or five in one tail and five in the other. And these are the higher order methods that Bob was talking about. And well, I've just enough source. Okay, so that's that. Okay, so that's that. Now, to come talk briefly about this question of simulation. The standard normal approximation to R would use this approximate significance function. It treats the R, the statistic R we're computing as standard normal. But the obvious thing to do is to say, well, why do I have to accept that? I have a big computer, I have a grant, I'll just spend some money and simulate this attempt to estimate. The this attempt to simulate this distribution instead of using the normal approximation. And what that means essentially is we just simulate lots of data sets from S data sets there, I've called the Y dagger, from the profile value. Okay, and then we replace this normal approximation at the top here with a a Monte Carlo approximation. Okay, and apparently this is called throwing a toy. Okay, and apparently this is called throwing toys. Which I always thought involved a pram. We call it the parametric bootstrap. Okay, now, so now to come back to this question of its accuracy. Stephen Lee and Alistair Young, and with Tom DeSisio in a variety of papers, but I think this is mostly Alistair's work, proved in a series of papers that this paper, this approach, has got Paper, this approach has got third-order relative accuracy in linear exponential families, conditionally and unconditionally. And in the final paper here, the 2010 paper, they showed it's very closely linked to some objective beta. So we expect simulation to do very well. It should be, basically, should give us the same results as using modified likelihood route, but by simulation route rather. But by simulation route rather than by an analytical approximation. I don't know. I mean, I was looking at the abstract and the bits of the paper last night in preparation for this. And there are some things there that puzzle me a bit. Because they say that third-order accuracy is achieved in discrete data, and I had thought that was impossible. So probably I'm wrong, but one would need to read the careful paper rather carefully to be sure. The paper rather carefully to be sure about that. I think it's not known whether in. So, this is in linear exponential families, right? In curved exponential families, which are much more common, I don't know what is, I don't think the third order accuracy could be preserved, but bootstrap considerations suggest that any way you can get second-order accuracy. So, we can knock out that first-order error. Let's now come to marginalization. So, from a statistical point of view, this involves a prior for lambda, and then we base inference on this marginal density that you see written here. And so, what that will mean from a frequentist point of view is that even first-order inference is going to depend upon that lambda, sorry, but psi, pi. If you get the pi wrong, then the inference will. Pi wrong, then the inference will be neither secure nor calibrated. You won't even get alpha in the limit, you'll get some alpha, something else. In a paper in Biometrica in 2010, David Cox and Emmanuel Wang compared this approach with conditioning to remove lambda from a Poisson model. Again, a simple example. And what they showed in that case is that you don't gain much information by marginalization instead of conditioning. Instead of conditioning, and that misspecifying the pie may give you an appreciably biased inference. And there's ongoing research that I'm aware of that is trying to figure out which aspect of the misspecification is crucial. Is it okay to get the mean right and the variance right and then everything turns out okay? Or do you actually need to get the whole distribution correct so that the inference is doesn't become fragile? The inference doesn't become fragile. Further comment is, and now we come to Bayes. You could regard this as a version of quotes pauper's Bayes. We just say, well, I've got a joint prior on both parameters. It's just that the prior on the psi is independent of that on the lambda is uniform. In which case, what will Bayesian do? They will compute this and use this to compute posterior intervals. Posterior intervals. Okay, this is just the integral over both parameters on the bottom line and the integral, a partial integral over psi on the top, and the integral over lambda at the top. If you apply a Laplace approximation to both integrals then, you will then get a Bayesian version of the modified LV. And that's going to then give you, what this will then provide you with, is a second-order approximation to the CDA. Second-order approximation to the CDF, the Bayesian CDF for continuous. So you can think of the modified, the difference between modified, you can think of modified profile likelihoods as being an approximation to a marginalised answer. So we take, instead of taking the profile likelihood, we take profile likelihood plus something, usually a half a log determinant. A half a log determinant of an information matrix, and that is in some sense numerically very similar to doing this sort of procedure. Here's just, I think this is just some examples here. I've given these, shown these are numbers in every talk I've given to Firestack. So it's an anniversary. This is just in a multi-channel simulation with C equals 10. There are 21. C equals 10. There are 21 parameters, 30 observations, one nuisance, so 20 nuisance parameters, one interest parameter. These are probabilities down the left-hand side here. This is what we would like these other columns to be. And what you see here, here, and here are what we actually get when we use these different methods. So if we use R, we get numbers that are okay down at this end, but then the bold means they've got. But then the bold means they've gone out of simulation error away from the target by the time we get down here. R star, as you see, there's no bold, so these numbers here are basically within simulation error of these probabilities. And R star B is a Bayesian version. It's not quite what I described before. It's a Bayesian version using a prior invented by Lord Tipshrani, but that is related to objective base priors invented by Priors invented by back in the early 1960s. And you can see this also doesn't do quite so well. Essentially, the problem is if you put Bayesian, if you put this with these priors, is they kind of pull the tails of the distribution in a little bit too much. So your distribution, which should be like this, becomes like this, and then your confidence in the coverage is too complicated. And just here, I think we can skip this one. Yeah, we can. One, yeah, we can skip that, I think. Yep, okay. Just to come to this discreetness, what this picture shows here, so I said there were two problems with the calibration. The first was the nuisance parameters, and I've kind of suggested how you can hope to deal with that. For the discreteness, things are a bit trickier, actually. What you see here on this picture is these are the exact coverages, these lines represented. Exact coverage, just the different contexts. Coverage produced the different confidence interval methods for values of pi for the binomial parameter in a sample of size 10. And the exact coverages, so you can see this nice black line up here. This is Klocker-Pearson, and this is what Bob mentioned. It's conservative. Instead of giving you coverage of the target 0.95, the coverage varies, but it averages out to about 0.98 or something. So confidence intervals would be. Or something. So confidence intervals would be too wide. And therefore, test probabilities would be too large. And then here you have various other methods, none of them very satisfactory. The best appears to be this dashing line over here, which is kind of related to a Bayesian version of this problem, where you say instead of having ten observations, I have twelve, and instead of having two successes, I have three. Part successes, I have three. And that kind of fixes things up. That kind of fixes things up, but there's inevitably going to be some variation in the coverage. So, just to hopefully I'm finished. Okay, so just some comments about this. Of course, the binomial example is going to work better with higher M, but the anyway, the exact intervals will give you a lot of power for exact tests, and approximate intervals, especially Bayes ones, may be preferable. ones may be preferable. I join with Bob in saying that the mid-P is really preferable in many cases. It does a kind of averaging. It's like a continuity correction essentially. Okay and then finally I think okay so just to come back to this the frequentist inferences should be relevant, calibrated and secure. If they are based on the likelihood they will also be efficient. Come back to this question hardly. The relevant The relevance involves comparing the data with a suitable reference set and often some form of conditioning, which will either be implicit or explicit. Lots of models of interest, in high-energy physics, I think, are curved exponential families involving the Poisson distribution. So we need to do some conditioning both for relevance and to eliminate the nuisance parameters. For calibration, we can't usually have exact calibration, we aim beyond the first-order approximation. We would ideally like third-order, but actually, that's not possible, I think, with discrete data. The likelihood ratio statistic profiling is first-order accurate in general, but the modified versions are conditionally second or third-order accurate, so the conditioning is built in when you do that. You don't have to think about it explicitly, it just happens. To think about it explicitly, it just happens. Simulation reduces the error from profile in special cases. I think it probably does it to second order in general, and that's probably as good as you can expect with the Poisson distribution. But it will improve numerical behaviour. And in my view, marginalization seems unwise in general unless you know the pi is, the prior is well specified. And then just final point: discreetness tends to give consent. And just final point: discreetness tends to give conservative increase. Notice that. Thank you very much. So, earlier Bob mentioned this idea of taking the nuisances and getting the results by integrating the nuisances in a small interval around them. How do you see that with respect to what you just said about In respect to what you just said about marginalization, is advising you use priors. I find that you are close to the maximum likely fit value of the nuance parameter. Does that make it reasonable? Well, it's what a statistician would call a database prior. And I'm sure that Dave has some views about database priors. Essentially, what you tend to be doing in those situations is using the data twice, and therefore your inference is too tight would be the usual kind of statistical expectation. Instead of actually marginalizing, you just marginalize. I mean, it's kind of closer to the it's a little confusing, but somehow that does seem closer to the profile, right? Yeah. But from a purely Bayesian point of view, it sounds, yeah. From a purely Bayesian point. So do it from a purely basic endpoint. Yes. Yeah, so just on your second and last point, I just wanted to try to understand a little better. Under the situation where you had discrete distributions where you could combine these cuts, it seemed like you were saying that marginalization in that case was basically pointless. Yeah. But would lead to what would seemingly be a very good answer. Yeah, I mean, basically, you're doing an integration you don't need to do. Right. So why would you do it? Well, we know that. Do you always know that you don't need to do it? I guess that's sort of, like, sort of in the case that Bob showed. It's not to say that we didn't know, I guess, but sort of if you followed the procedure and did the integration, you arrived at a nice result. Yeah. So it probably won't harm you if you know you've got it correct. Right. Well, that will you need to look at the structure of the lighting though and just check that. But yeah. I mean, of course, you could say, well, I can do it. No. One dash. As I say, one ball. His hand up first. So if you go back to the derivation of the likelihoods or profile likelihood ratio as a good test statistic from Wall, there's some kind of weighting of alternatives implied in proving that it's kind of like the optimal test statistic. So there is already some kind of prior. Is there some kind of connection between that prior for which the likelihood ratio test is most powerful and the prior to be used in our generalization? I don't think it's most powerful in total general. The name of Pearson Lemma tells you if it's simple versus simple, it's most powerful. And if you can condition out your nuisance parameters, then you have this argument for similarity for name. Of this argument for similarity from Neymar. I don't think in a general model it's necessarily the most popular. It's just a kind of a a statistical hope, but because it's like the name of the neighbour, you hope the name of Pearson never kind of applies, although it's doesn't really appear. But what makes this concrete, right? So you imply some kind of prior of, I mean, there is no universally most powerful test statistic, but if you choose a prior on which alternatives you value more than which value you value more. Value more, which you value less. There is kind of a most powerful one for that particular choice of prior, and that happens if you profile like good test statistic if you construct that prior in a certain way. Is there some kind of connection between these priors and marginalization? Actually had a prior offer that if you probably wouldn't be doing that decision to do something else. So, oh well, fine, I can have it easy and I'm happy with it. I can now be a Bayesian unhappy. Really happy because I forgot to say it's then the relevance. Of course, because the Bayesian conditions, this is kind of paramount, because the Bayesian will condition on all the data, the inference is perfectly both. But on the other hand, there's no comparison to a reference set because the reference set is the data set itself. You have to get randomness from somewhere else. Just to clarify what I was talking, so a lot of our problems in hydrogen physics have the same structure. For example, upper limits on Poisson mean black, rarely lower limits on Poisson. So if someone figures out which prior has good frequency properties, then we know for that type of problem we can use that. So historically in energy physics, it was learned that a flat prior and a Poisson mean, even though statisticians Son mean, even though statisticians wouldn't call that their favorite non-informant prior, that gives the perfect frequency answer for an upper limit. If our field did lower limits, we'd want to use a one-over mean prior to get the perfect lower limit frequency answer. So that's what I was advocating, was exploring kind of generally, I think as I mentioned on my slides, generally if you go to a metric where the distribution is kind of Gaussian, then that basically is where you can get away with it. Basically, is where you can get away with the flat prior. One other, maybe, one other comment about the prior because Nick mentioned log normal. So what happened historically there is people were using a normal prior for something, for a positive parameter, and then they would have to truncate the Gaussian at zero. And then Luc Demorte showed that a whole bunch of results from Tevatron doing a basic. Tevatron used doing a Bayesian analysis with that truncated prior. In fact, you showed with paper and pencil that the integral didn't exist with Ergo was doing numerically. And so people switch to log-normal priors, which go to zero. But then you're introducing this long tail, and so you still got to make sure that it's not causing you trouble, that if if that nuisance actually matters, then you just can't kind of arbitrarily You just can't kind of arbitrarily use a lot of games. Gamma prior would not have such a long time. Yeah, so the gamma prior, gamma priors, except for this case, I do. And underneath the hood of what I showed, that flat prior and the sideband gives a gamma prior, gives a gamma prior. I don't know if people commonly turning to gamma priors or something. Priors or something like uh t-statistics. Sometimes people use it as letter tables and statistics. Jump on log normal. I don't know. I have this note that shows how to get a gamma pair, but I don't know. I could jump into that conversation, but that would be abusing my precision as a chair. And it could also be immediately jumping into encouraging a momentarily. Louis, just to comment, you had a slide near the end about discreetness and showing the coverage, which was dipping below the nominal value, this one, which from a frequency point of view sounds bad because we don't go undercover for any value of a physics parameter. But like you said, this is specific for n equals n equals test. And you can choose, if you change that number, the