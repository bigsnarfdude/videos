Non-parametric Bayesian models, and I will try to answer the question if whether we can control the borrowing in those models. And the spoiler, the answer would be no, or at least not in a satisfying way. So I will try to explain why that happens, why certain types of borrowing are not allowed in existing non-parametric Bayer models, and so that I can. And so that I can try to solve this problem, presenting a model we are proposing, which is called N4B, and through which you can construct a flexible borrowing of information. And I will conclude with simulation studies and an application. Okay, so let me start saying some simple stuff regarding multi-sample data. What are multi-sample data? Are data collected at different Are data collected at different locations or under different experimental conditions, but that all concern the same phenomenon? So, here, for instance, I'm thinking to a number of capital J samples, each one extracted from a certain population. The only assumption for now is that the population is infinite-dimensional. So, at least theoretically, we can sample an infinite number of times from each population. So, when you have this kind of data, When you have this kind of data, I believe that the Bayesian framework is particularly helpful and convenient. The reason why that is because the Bayesian learning mechanism, which is inside the Bayesian models, allows you to borrow information from one sample to the other in a very natural way. Because, of course, if you have data from a number of populations, you can do independent analysis. But if all the data concern the same phenomena, All the data concern the same phenomena, of course, in that way you're going to waste information. So, we have to find a way to make the different samples communicate one with the other. I believe the division framework is so helpful in this situation that even if you decide to take a frequentist approach and develop a frequentist model, use a frequentist model, very easily at the end, you will find yourself doing more abahesion than a frequentist model. And saying so, I'm thinking, for example, And saying so, I'm thinking, for instance, your sign estimators or random effects and fierce models that are typically presented in a frequentist framework, but are actually more Bayesian than frequentists. Okay, so sorry for interrupting. Yes, sir. Can you close the blue banner? Ah, sorry. Thank you. Okay, better now. Yes, thank you. Okay, thank you. Okay. Thank you. Okay, so we have these data. So, how can we model them? So, a standard assumption can be to assume that the data within each sample are IID from a certain random measure. And AGOR shows us that this assumption coincides with the partial exchangeability. So, of course, you can also imagine different sampling strategies within the same sample. And so, you can think to also something. And so, you can think to also something else different than an IID from an unknown distribution within each sample. But since for this talk, I am mostly interested in understanding how to borrow information across samples, what is going on across samples and not within, I will stick with the IID hypothesis. Okay, so from now on, I will just use the two samples, but just for not addition convenience, Just for notational convenience, everything I'm about to say is okay for any number of samples. So we have our two samples conditioning on their respective distributions, PT the one and PT the two. We don't know them, and we put a prior over the unknown parameters. So the idea here is the following. Once you choose a Q, your prior, you are defining already the borrowing across samples. The borrowing across samples, the evasion learning mechanisms also across samples. But looking at directly at the prior queue and understanding what really is going on may be difficult. And so the idea we had was, okay, we should try to measure and understand the borrowing of information looking at the observables, looking at X and Y. So here is our first result in the gray box. Igor already anticipated it. I will repeat it because it's really. I will repeat it because it's really crucial for the rest of this talk. Okay, so first of all, we're going to have that the correlation within the same sample, so between Xi and Xi prime, has to be positive. But as I said, we are not so interested in what is going on within sample, but more across. If we look at the correlation across samples, so the dependence across samples and thus the borrowing, it turns out that if you have a model like here in the slide, Sorry, a model like here in the slide, you cannot achieve any value for the correlation. But this correlation across samples has a specific upper and lower bound. The upper bound is the correlation within. And this to me makes completely sense because this upper bound is saying that observation in different samples are more dissimilar than observation in the same sample. And this is exactly the idea behind multi-sample data. What is more interesting is the lower bound. What is more interesting is the lower bound. The lower bound is negative, and this is interesting and maybe surprising because it's very hard to find in the Bayesian literature, parametric or non-parametric, some prior which induce negative correlation across observations. So what is the message here is that while we are constructing a model for multi-sample data, we are choosing a prior queue. Once we have fixed the marginals, so We have fixed the marginals. So, for instance, we have elicited the prior information for one population and the other. That is the moment in which we fix the correlation within sample. But still, we have to define the dependence across samples, which is a measure of the borrowing of information. And to define this dependence across samples, we have a range of possible alternatives. If we believe that the two samples have a similar behavior, then we Behavior, then we would like to have a positive correlation across samples, and this will result in shrinkage estimates. We can also think that the two samples have nothing in common, so maybe they refer to different phenomenon. In that case, we want independence and thus zero correlation. But we can also think that there is a sort of opposite behavior between the two samples, a competitive dynamic. In that case, we're going to need negative correlation that will result. Negative correlation that will result in a sort of repulsion of the estimate. At the end of the talk, I will also provide a few examples of what happens if we try to put a prior over this correlation and try to learn the direction, the types of borrowing from the data. Okay, so what I've said so far is let's say all true for any partial exchangeable Bayesian model, parametric or non-parametric. Model parametric or non-parametric. But actually, if your prior queue is parametric, typically you can understand what is going on in terms of borrowing, looking at the prior queue. Instead, if your prior is non-parametric or really complex, it may be much more harder. And so I believe that the interesting part of this story regards non-parametric models. So now I will move to non-parametric models. Will move to non-parametric models. As non-parametric models, we're going to consider a large class, which is described here in the first line of the slide. So in this class, we have that the two unknown parameters, the two unknown random, the two unknown distribution, thus random, are almost surely discrete with certain weights for the first one is j k and certain atoms, theta k. This class includes the multivariate species sampling models that Igor was talking about, but also something else, it's a little bit more general. And so you have these two random objects. We need to define a prior distribution for these two objects, and we can do so defining the distribution of the weights and sampling decouples of atoms, i.e., from a certain distribution. So within this class, there So, within this class, there is basically the vast majority of Bayesian non-parametric models for multi-sample data. But if you go through the literature, we identified basically two different strategies to construct a prior for these objects. The first one is what we call the prior based on the series representation. It's basically the construction strategy used for defining the dependent initiative process or the power bit stick breaking. Or the Power of Stick Breaking Process. What these priors models have in common is that you construct the prior explicitly defining the PJW, the distribution PJW, typically with a stick breaking approach, and the distribution G0 explicitly. Then there is a second strategy, which is to make use of completely random measures. If you are not familiar with completely random measures, Familiar with completely random measures doesn't matter because in a moment I will give you more details. But for now, the idea is that still to define the law of PTL1 and PT2, we can also work with the random measures, mutile1 and mutile2, work on the law of mutile1 and mutile2, and then normalize these measures in order to obtain the random probabilities. You're still going to be inside this class, you're still going to have this representation, but you typically This representation, but you typically do not work directly with the PJW and Z0, but more with the random measures. So now the main focus of this talk is the borrowing of information that we are measuring with the correlation across samples. So the good news now is that whatever strategy you are choosing to construct a model, if your model is within this class, the correlation across the samples can be computed as the product. Can be computed as the product of two terms, gamma and rho zero. Gamma is what Igor just a moment ago called the probability of an hypertie. For now, it doesn't matter so much what that is because I'm about to clarify it. But for now, the idea is the following. This correlation is the product of two terms. Gamma can only be positive and rho zero can also be negative. So basically, gamma controls the absolute value of this. The absolute value of this correlation, and row zero controls also the sign. So, of course, if you want to develop a model for multi-sample data controlling the borrowing, tuning the borrowing, you should be able to control both gamma and row zero. But if you look in the literature, basically, if you have choose to use a prior based on the series representation, you can control the row zero, but not gamma. Now we are going to see what. But not gamma. Now we are going to see why. If you use instead a model based on completely random measures, you can control gamma but not your zero. And this is why I don't find these models completely satisfying if you would like to tune specifically the borrowing of information, which I don't think is a secondary aspect with multi-sample data because is the whole point of doing jointly analysis for many samples. Okay, so now I will also show you. Okay, so now I will also show you how you can control both. But first, let me spend just another minute saying what is gamma and what is row zero. Okay, so we are still in the same class of models. Here in the first line of the slide, we have our random probability measures. We define the distribution of the weights and of the atoms explicitly or implicitly. So just for a moment, I would like to look at Just for a moment, I would like to look at this model marginally. With marginally, I mean look at just one random probability measure and just one sample, for instance, the axis. If you try to compute the correlation within, this model marginally is a species sampling model, Pete Modena style, and the correlation within the same sample is equal to the probability of a tie. So, how can we express this probability of a tie using? The probability of a tie using the agents and the jumps of our random probabilities. Well, of course, the probability of a tie is going to be the probability that the first observation is equal to theta k and the second observation is equal to theta k as well. And then we sum over all possible atoms, over all possible labels. And these sum of probabilities turns out to be the sum of the expected value of the weights, jumps. Of the weights jumps squared. So, what we did in this work, which I believe is a very nice result, is that we were able to provide a similar expression also for the correlation across. So, now I'm not about to give you a proof of how you can compute this correlation, but more the intuition behind the proof. So, now we have the correlation across cells. Now we have the correlation across samples, so between xi and yj. You can express it very similarly what you were doing before, computing the probability that the first observation is equal to a specific atom theta k, and the second observation, if before we add the same theta k, the same atom, and now we can not add the same atom anymore because the two random probability measures may have different atoms. Different atoms, but we are going to have a phi K instead of theta K. What have T and Phi K in common? They have the same label, and what does that mean? Actually, it means that they were sampled jointly together. The atoms are sampled couple by couple together from G0. And then, as before, you sum over all the possible labels. Then, another term comes up here, which is the correlation across the atoms. The correlation across the atoms, why now we have these terms, actually, something that probably we should have had also marginally, but marginally we had the theta k and theta k is the same, the correlation is one. Now, the atoms are different, and so we have to take into account also the correlation of the atoms. So, now as before, if we look at the sum of the probabilities, you can compute these as the sum of the expected value of the product of the weights. Of the product of the weights associated with atom with the same label. Okay, so now we find an expression for the correlation, and it's the product of these two terms. And they are what before I was calling gamma and rho zero. So rho zero is the correlation between theta k and phi k. So this is actually easy to control if you have used a series representation approach, a stick breaking approach, because in those approaches you have Those approaches, you have defined explicitly G0 and also PJW, and rho zero is nothing more than the correlation of zero. To make it simple, G0 is a bivariate normal, rho zero is the correlation of that bivariate normal. So you can control it. What about gamma instead? Where gamma is the sum of these expected values. And actually, this is pretty hard to compute if you have used a stick-breaking approach or a series representation as we. Or a serious representation as we call it. For those that are familiar with those models, the reason why it is hard typically to compute gamma in those models is the same reason why it is typically hard to derive marginal sampling schemes for those models. So I hope that this slide gives a little bit an intuition of what RO0 and what is gamma. So maybe So, maybe gamma is what we call the probability of an either tie for analogy because marginally we have a real tie, while jointly across samples is not more the probability of a real tie, but still the probability that the two observations are associated with atoms with the same label, not exactly the same atom, but the same label. So, it's an hypertype. But I hope this slide helps a bit, but if Helps a bit, but if I was not clear, that the message is written in the last two lines of the slide. What I really want to bring for the rest of this talk is that the law of the weights of our probability measures controls the absolute value of the correlation across samples because controls gamma and so controls how much borrowing we are doing. Instead, the law of the atoms is that it's the law. Atoms is that is the law of the atom that controls the sign of the correlation, and so basically the direction of the corruption. Okay, so now as I was saying, I was saying, with the let me go, okay, with the series representation, we cannot compute gamma, we cannot tune gamma, so we are not so happy. But why with the random measures? We are not happy even in that case. So let's see. What happens with the completely random measure? What happens with the completely random measures is that we cannot control exactly row zero because row zero in those models is always equal to one. So let's see why. To see why, let me introduce the concept of completely random vectors. They are the multivariate extension of completely random measures. We need a multivariate approach because we are in the multi-sample situation. So a completely random vector is a vector of measures that when evaluated on this uh that when evaluated on these joint sets um give right gives rise to independent mutually independent vectors uh so uh what can you do with this object why this object is useful for defining a prior on multi-sample data well because if you work with the law of the completely random vector you're going to have two random measures the two coordinates of the vector that are mu t the one and mu t the two uh they admit an almost surely They admit an almost surely discrete representation, and as you see, it's very similar with respect to the almost sure discrete representation of the random probabilities I was talking before. And indeed, you can go back to the random probabilities, normalizing the random measures so that now the random probability is up to one. Of course, here I'm skipping some technical detail because, in order for this ratio between mu tilde one and mu tilde one computed on the whole. And mu tilde one computed on the whole space. For this ratio to have sense, I need that mu tilde one on the whole space as to be almost surely finite, but you can take care of that while defining the law of the random vector. And so exactly, if you look at the series representation, you can see that basically it's the same representation. The agents are the same for the measure and for the probability measures. What changes are the weights that are now normalized for the random measures? The random measures. Okay, so why with this construction we cannot do any type of borrowing? So, I believe that the intuition on why you cannot do any type of borrowing is very clear if we look at what happens when we try to simulate this object. We try to assemble a realization of mu tilde one and mu tilde two. So, mu tilde one and mu tilde two, the our completely random vector, comes with a Vector comes with the V intensity. Here I denote this with V and is composed by two parts, rho, and P0. I'm assuming the vector is homogeneous. I'm skipping some technical detail to give you the intuition. So how can you sample mu tilde one and mu tilde two? Okay, so first of all, you're going to sample the arrival times of a Poisson process with the with a certain intensity. This is a Poisson process on R plus time R plus. R plus time R plus. So we have a bivariate arrival times, and we're going to use this realization as the weights, jumps, JH and WH. Then we still need the atoms and how you sample atoms with a completely random vector. You sample agents IID from a certain distribution, P0. Here are the blue dots here. And then simply what you do is you associate a is you associate atom theta h to each couple of jumps and so you can write the series representation. So what let's say went wrong for our perspective? If I look at the series representation the atoms are the same both for mutila1 and for mutila2. I have for the same label theta h and theta h and of course this happens because I'm sampling just one atom for a couple of jumps. Couple of jumps, and this is the reason why rho zero is equal to one because I have the same atom. So, what happens here basically is that, of course, as we were saying before, gamma is controlled by the way in which we generate the weights. And turns out that using this structure with a Poisson process is a very good idea because it makes gamma computable and also controllable. And indeed, I believe that also the talk Marta will give after me, after the coffee break, will clarify a lot this part because actually she will show you that controlling the intensity here, we can achieve independence, perfect dependence. But still in this structure, we cannot control row zero. And so, actually, this structure is perfect if you want to do the classical shrinkage of the estimates, but you cannot do Estimates, but you cannot do negative correlation and so repulsion of the estimates. So, there are types of borrowing you cannot do with the structure. So, now, how can we solve this problem? If you follow me up to this point, now the idea is very, very simple. We want to keep this structure as it is because we like it, it's tractable, we can control gamma, but simply we have to sample the atoms in a better way, in a small, in a more smart way. And so, simply, we're gonna. And so, simply, we're going to still do the same for the jumps, but now we're going to sample bivariate atoms. And so, where before in the intensity I had the P0, now we're going to have G0, which is bivariate. Now, as before, I associate a blue dot with a red dot and I write a series representation. But now I have bivariate agents, and so in one measure, I will have theta h and in the other, I will have phi h. Of course, if this. Of course, if G0 is degenerate, we go back to completely random vectors as a particular case. But if not, actually G0 can induce whatever correlation and rho zero is the correlation of G0. So basically what we have done here is that we use the way in which completely random vector generate jumps and indeed we generate atoms as you typically Sorry, atoms, as you typically do with the series representation that depends on your state process. Once you normalize these two random measures, you obtain what we call an FURB, which is our prior, to which you can do flexible borrowing of information, because you should be able to tune, compute, and control both gamma and row zero, and also put an hyperflyer on these. Exactly. So So, okay, so this was the intuition behind N Furbies without a full range borrowing of information, normalized completely random measures. But there may be a problem there because actually N Furbies are not the normalization of a completely random vector. Because basically, we did this small change in the structure of a completely random vector that is to not sample from a univariate. From a univariate distribution, but from a bivariate distribution. But in this way, we break the independence of the increment that was here in the definition. Now, if you use our measure, the Furby measures, you don't have this condition anymore. And this may be problematic because maybe we could have break the tractability of this object. But here we had another simple but very nice idea, which is that is true. Our proposal. Is true. Our proposal are not a normalization of a completely random vector, but they still admit a representation in terms of a completely random vector in an augmented space, in the product space. Igor started talking about this idea of looking at an augmented space at the end of his talk. So another way to construct N-Furbi is the following. You can start with a completely random vector, but on a product space on x time x. product space on x times x. This completely random vector will have the coordinate mu1 and mu2. And now what why they are different with respect to everything I showed you up to now, because they have agents that are bivariate. They are the same for the two measures, that is what you need to have a completely random vector, but they are bivariate. And now if typically starting from a completely random vector, you normalize Random vector, you normalize the measure to get your probabilities. Now we are not only normalizing, but also projecting on two different coordinates. You can see it here. So for obtaining p tilde one, we normalize mu1, but also consider just the first coordinate so that p tilde one will inherit just the first coordinate of the atoms and p tilde two just the second. Why this idea of working in the product space is very useful because completely. Very useful because completely random vectors as well as completely random measures are very highly analytically traxable, and the fact that we are able to express our random probabilities in terms of a completely random vector really makes traxable and achievable the posterior distribution, the algorithms, and make the model traxable. And so, for instance, gamma, which was the probability of a hypertide, you recall the correlation across temple was gamma time. Correlation across temples was gamma time error zero. Gamma is a bivariate integral with this construction, which typically simplifies, but even if it is not in the case in which you have a very particular intensity for your completely random vector, it's still a bivariate integral, so you can compute it numerically and tune it. And there is the eye tactability of the object in the product space. It's also the reason why we were able to derive here the posterior. Here, the posterior characterization for our model. Here, I'm not, I don't want to go into detail about all the objects in this slide, but the idea is that this result is the posterior distribution of the completely random vector in the product space and is not only a nice, elegant result, but it's also the starting point to derive the marginal sampling scheme and the conditional sampling schemes. What may be What maybe I can talk a little bit about are the predictive distributions. So, I think that from the predictive distributions you obtain using an FURB, you can appreciate how the model is more flexible in doing borrowing. So, I will start from the second box, which is the predictive distribution you typically have with model based on completely random measure and completely random vectors. Measuring completely a random vector, so not our model, was there already in the literature. So, typically, the predictive Igoro also talked a little bit about this. The predictive is the sum of three components. The first one is the prior. The second one here, I'm predicting an observation X, so in the first sample. So, the second component is a re-weighted empirical distribution in sample number one. And then you have also a re-weighted sample. And then you have also a re-weighted sampling, sorry, a re-weighted empirical distribution in sample number two. Instead, if now you move to the predictive U obtained with our proposal, you still have the prior, you still have the empirical distribution of the sample of interest, the sample we are trying to predict. But now the last term, which is the one that represents the borrowing, because if I don't have the last term, I'm not doing borrowing. The last term I'm not doing borrowing is more flexible than the empirical distribution of the second sample because now instead of having Dirac measures on the observation of the second sample, we have a diffuse distribution which is the conditional distribution conditional on one realization on the second realization that you obtain starting from G0. G0 was the distribution from which we are sampling the agents. So basically, Agents. So basically, if G0 is degenerate on the main diagonal, we are going to have, as I was saying before, row zero equal to one. And so also here, we are going to have a DIRAC on, we are obtaining a special case, the other models. Instead, in general, for a G0, which is not degenerate, we are using the information in the second sample we can use it in different way based on the hyperparameters values. On the hyperparameters values. Okay, so basically, that was the model and what I wanted to say. I'm moving now ready towards the conclusion and I would like to do so showing a simulation study. We've done also other simulation studies, but I think that this is the most simple one, but also that really shows what is going on and what we can do with N4. What we can do with N4 visa. Okay, so we are sampling observations for two samples, X and Y, as before, from two normals, one centered in 10, the other in minus 10. And we are gonna compare the performance in making predictions for this model or density estimation, which is basically the same thing, using four different approaches. So, we are going to use a mixture. We are going to use a mixture models because, as you all know, PTD1 and PTD2, we have seen they are almost surely discrete, so they are not tailored for fitting continuous data. So, we can use a mixture model with normal kernels. But then we are considering four different specifications for PTD1 and PTD2. The first one is an independent model with the HLA process marginals. So, using model one is like to Using model one is like doing independent analysis for your multi-sample data. Model two is instead the one we were proposing and Furby's, where row zero, the parameter which control the direction of the borrowing, and row zero we use an hyperprior, so a uniform between minus one and one, and we try to learn from the data the sign of these, the direction of the borrowing. The third model is a hierarchical. The third model is a hierarchical Dirichlet process. So the hierarchical Dirichlet process was originally developed using a series representation, but it admits also representation with completely random measures and completely random vector. And this is why indeed under the hierarchical Dirichlet process, rho zero is equal to one. And so basically through the hierarchical Dirichlet process, you can do the classical type of borrowing, which is the shrinkage of the estimates. Which is the shrinkage of the estimate. And then, lastly, we are gonna use also an exchangeable model with the DHA process marginal. With this model, you have the maximum borough link of information in the sense that you are not treating your data as coming from different populations, but just as one population, as one sample from one population. Okay, let's see the results. So, in the left part, in this left picture, we have the density estimate for The density estimate for sample number one. So I'm sorry because I don't think this picture is really colorblind friendly. But so the true distribution is the blue one, is the one here. And our estimate is red and is the one that most resemble the truth with respect to all the other models. But you may think, or this may be. You may think, or this may be because of the specific data generating process we are choosing, because indeed we have generating data centered one sample in 10, the other in minus 10, and also I didn't say it, our prior is in zero. And so, probably this may be the reason. So, we repeated the experiment varying the location, the mean of the second sample from minus sixteen to plus sixteen. Of 16 to plus 16. And here in the right picture, you can see the error we are committing. So, if you're moving along the x-axis, the mean in the second sample is changing from minus 16 to plus 16. The mean for the first sample remains n. And then the plot represents the error computed and agreed we are doing in the density estimate. Our model is the red one and is the one. our model is the red one and is the one that minimizing the minimize the the error actually minimize the error in every scenario in every simulation study except when also the second sample is centered in 10 in that point there actually the exchangeable model is doing better but is something predictable i will say because when both samples are centered in uh in 10 um of course the model that they come from Course, the model that they come from the same population, and of course, the model that assumes that they come from the same population is doing better because it has less parameters to estimate. But we are thinking to apply this model in multi-sample data. So, the situation in which actually they are not multi-sample data is pretty strange, right? So, in the table here, you have basically the same results that are in the picture here. Simply, you can read the values of the Can read the values of the error we are committing. And as I was saying, our model is producing the smallest error except when the data are actually coming from just one population. But also in this case, the exchangeable model, of course, as I said, is doing better. But for instance, with respect both to the independent and the hierarchical, our model is minimizing here. Okay, so I will show you very quickly also an application we have done on. Application: We have done on stock returns and commodity returns on financial markets. So, here we are trying to predict the returns in the stock market and the commodities market. So, the first column, in the first column, you see the estimates that we achieved using an exchangeable model. So, treating the data as they were not different based on the fact that they were stocks or commodities. Indeed, both were. Commodities. Indeed, both for the stocks and both for the commodities, we have the same density and the same prediction. In the center, you see the predictions estimates obtained with the N-furbis with a prior on row zero. While in the last column, we have again the independent case. So here we are doing independent analysis, looking at the stocks alone and the commodities alone. So looking at these pictures, I believe that actually it seems like our Actually, it seems like our model is producing. Okay, so of course the exchangeable model is not doing pretty well, it's not doing well because it's predicting the same while the distribution are likely to be different. So instead, our model seems to produce a prediction similar to that of the independent model, but with a much lower uncertainty. And this, of course, because we are using all the data together while the independent model is doing the Together, quite independent model is doing independent analysis. But actually, it's not true that we are from this picture, it seems that the prediction is more or less the same, but actually we are doing better also in prediction. Here you have this measure of prediction. They are the average logarithm of the leap one-out predictions, also called conditional predictive ordinates. So, here the higher the value, the better. So, basically, you try to Value the better. So basically, you try to predict one observation at the time, leaving that out. You compute the logarithm, and then you average the average for all the observation. This is the first line. The second line is the same, but you take the median instead of the average. And so our model is also producing actually seems a better prediction. Okay, so I think that was it. Yeah, exactly. These are some. Okay, these are some references, and if you have any questions, I'm more than happy to answer them. Can I ask a question? Do you hear me? Yeah, sure. So I was wondering. Wait, your terminal. Sorry? Hold on. You have to wait. It's first Sarah and then you. Oh, sorry, sorry, sorry, sorry, sorry. I didn't notice. Oh, sorry. I mean, you can go before me too. That's fine. So I guess I just had a couple questions. One, I had a couple questions. One was first, very nice talk. It was really interesting. One was when I just, if I just have gamma and rho, and just looking at the correlation, you see that they're both, they're not identifiable. So can you learn both of them, or do you have to fix one of them and you can only put a prior on one? Yeah. Yeah, okay, so okay. I don't know if I should think better about this, but I think it's in this way. Yeah, just looking only at the correlation, no, they are not both identifiable. If you stick to the class of non-parametric model, I was talking about yeah, just looking at the correlation, they are not. Yeah, that's true. Are not they are not, yeah. That's true. Indeed, we put a prior only on draw zero, yeah. Okay, so yeah, I think that this is linked with the fact that the correlation actually is a good measure, in my opinion, of the borrowing, but not perfect. Because that's true, you can have the same value for the correlation for different values, gamma rho zero, but actually within that class of models, the models are going to be different. The models are going to be different because, yeah, gamma depends on the weights, so on the diversity, while L0 on the atoms. So, yeah, no, they are not both identifiable, but looking just at a correlation. Yeah, I answered. Yeah. Yeah, yeah, yeah. And then I guess two other quick questions. One was when I looked up, when you showed those results with the you had the histograms and then the densities. The densities. It didn't, I mean, to me, the histogram looked kind of like bimodal, and that the density looked super smooth. Yeah, okay, so this is actually, yeah, that's maybe a point. This is not exactly linked to the what we are doing across samples, but more on the marginal prior. Marginal prior. So we have the base distribution marginally. So maybe I can, sorry, I will share it again. So actually, okay. Okay. So yeah, here you're saying, right? Yeah, or like E, if I look at the D the bottom row, I think that looks, it looks kind of bimodal to me, but the just the estimate looks very. Estimate looks very yeah, yeah, I believe that this is more linked with the let's say the marginal prior we put um yeah so yeah maybe maybe that's true maybe we could have done better uh yeah yeah i should look into that probably yeah and then just one other thing that came to my mind was so you're focusing on um univariate Um, univariate data. Um, have you thought at all about extensions for multivariate data? So, for an example, I was looking at some single-cell data. So, we have this data cells by genes, and we have two different conditions where they knocked out a gene. And it could be, you know, in some genes, there's a lot of shrinkage, the behavior is very similar, but in others, you know, things went crazy, maybe there's that. Know things went crazy. Maybe there's that repulsion you were talking about. Have you thought at all about extensions for multivariate data? Yeah, we start thinking about extension for multivariate, so having vectors, and I think that actually this construction can be even more interesting there because we have many parameters of correlation we can think about, but yeah, we didn't do it yet, but yeah, we were thinking. Do it yet, but yeah, we were thinking about that. Yeah, yeah, and then maybe in the multivariate setting, you have a bit more identifiability, I don't know, between the gamma and the correlation. Yeah, that was my, yeah, that was my, yeah, as I said, we didn't do it, but one of the reasons why I wanted to look at the multivariate situation is that I think that there, even the interpretation of zero zero and identifiability should be better. So I agree with this intuition. I yeah, I agree with this intuition because I had a similar intuition, but we are still working on that. Yeah, thank you. Thank you very much. Thank you. I think there is another question by Marifa. Marifair, you want to step in? Yes. I'm sorry, Sarah. I didn't saw your hand. And I was wondering, when you choose the distribution of the atoms, the joint distribution, have you? You how do you choose it? What choice did you make in your examples? That one, G0. Yeah, well, okay, so you have to choose, of course, okay, so in the paper, we always use actually a bivariate normal for that G0. No, that's not true. Or both a bivariate normal and also something with bivariate and gamma. With bivariate and gamma, because we also use all that. So, okay, let's say to the marginal. So, D0 is a bivariate. So, the marginal distribution is still the, let's say, your prior opinion for one sample and the other. And then you still, once you define the two marginals for D0, let's say you have to define still the dependence. And of course, if your bivariate distribution is, for instance, an Bivariate distribution is, for instance, a normal, you have to choose a row zero, and so you can choose it through the borrowing. If you are a more complicated base, G0, yeah, it may be harder to elicit this, but still the point is to choose G0. You have to think of your prior opinion marginally for one sample and the other. Those are the marginals of G0. And then the dependence should reflect how. Answer should reflect how you want to do borrowing, what you believe the two samples should do together, let's say. Okay, so thank you. Is there a, I mean, I don't know if this is too hard, but I think that the two random measures share all the atoms only when you have like all the mass at the diagonal, right? Yeah. And can you put some mass at the diagonal? Some mass at the diagonal? Yeah, that's also a very good question. That's something I was thinking about. Because I think, yeah, maybe we should have a mixture like one, two, zero with the mass on the diagonal. That's where was where you're thinking, right? And something other, right? So yeah, yeah, I think that is manageable, tractable, and it can make sense. But yeah, we didn't do it yet. So thank you. Thank you. Thank you for your time. Thank you. Thank you for your talk. Okay, thank you. Thank you, Bea. I think there are no more questions here in the audience and nor the internet. So thank you again for your very nice talk and let's clap again. Thank you. So we have a break of some minutes now, I think like 20 minutes, and then we come back at 11 for those who are connected. Don't go away or just reconnect.    