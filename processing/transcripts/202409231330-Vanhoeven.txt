So, this is all based on joint work with Masood Halkhali and Nathan Pagliaroli. Yeah, I'll just get into it. So, this is the outline. I'll briefly talk about what I mean by almost commutative fuzzy geometries or frequent fuzzy geometries in general. I'll talk about its large n limit, which is Its large n limit, which is the objects that we've studied using Coulomb gas methods. I'll talk about how that actually helps you find the spectral density. And then in the later sections, I'll show some graphs corresponding to the spectral density of these models and properties of these models. So, getting into it, I suspect many of you will have. Suspect many of you will have seen what a fuzzy geometry is. So, it's a spectral triple that's based around the matrices. So, the algebra in question is n by n matrices. The Hilbert space or your Spiner bundle is some Clifford module tensor, your N by N matrices with some Dirac operator, real structure, and possibly grating. And the term fuzzy geometry, of course, comes from the analogy with the fuzzy sphere. Analogy with the fuzzy sphere or the fuzzy torus, which are MN of C based. That's the sort of basic idea behind these. And it turns out that if you look at the Dirac operator on such spectral triples, they are parametrized by the space and n by n Hermitian matrices, or possibly several copies of this. So the drag operator on the space always consists of. Space always consists of some gamma matrices acting on your spinors, and then some commutators or anti-commutators with Hermitian or skew Hermitian n by n matrices. Otherwise, these n-by-n matrices are completely free. So, this gives you a very nice setting for random matrix theory. And that's how we became interested in these. We became interested in these fuzzy geometries. So, in particular, since we're going to be doing random matrix theory with Coulomb gas, we're interested in single matrix theories. And the fuzzy geometry that we're looking at is the zero-one fuzzy geometry, which is one of the simplest given to its low signature. So, that's matrices, your vector space that you're acting on. Your vector space that you're acting on is matrices and it's acting in the obvious way. And your Dirac operator is always the anti-commutator with some self-adjoint Hermitian matrix. Other than that, that self-adjoint Hermitian matrix is completely free to choose. There is one note I should make here. Some Hermitian matrices will give the same Dirac operator since commutator with a scalar matrix is, of course, zero. Scalar matrix is, of course, zero. That's something to keep in mind for this model. So, this is the model that we're eventually interested in. And what we want to do is we want to form what we call a Dirac ensemble from this. So, we want to take the space of possible Dirac operators, curly D, and we want to put some sort of probability density on this space and look at the properties, essentially, the average properties of the. Essentially, the average properties of the under this probability density. And the density we consider is the quartic density we call this. Excuse me. So we take a quartic polynomial of d g4 d to the four plus g2 d to the two, take its trace and put that in the exponential. So this quartic polynomial we call the action or the potential associated to our ensemble. Our ensemble and using your favorite Lebesgue measure on the space of Hermitian matrices that gives you a nice probability density on the space of Dirac operators. You normalize this, of course, but as long as G4 is positive, this is nicely normalizable. And that's also the case we're looking at. We're looking at convergent matrix models. Now, since Now, since the random matrix in question is not actually D itself, it's H for which random matrix theory applies, we can rewrite this action in terms of H, and then we get what you see on the next line. So it becomes a bit more of a complicated expression, but it's easy enough to work out. And the caveat that I mentioned earlier, that this map from their mission matrices to the From their mission matrices to the drag operators of the kernel, requires you to put this extra term here. Because if you wouldn't, you wouldn't get a convergent probability density on the space of formation matrices because, well, the action in terms of D can't see a shift by a scalar matrix. But because shifts by scalar matrices are the only thing that affects trace of H. This extra term doesn't affect the actual density on the Dirac space, only on the space of Hermitian matrices. So, in short, this here with a dh defines probability density on the n by n Hermitian matrices, which is a single matrix but multi-trace, as we see some products of traces pop up. We consider pretty much any g4 and g2, only we want g4 to be positive so that this model is convergent. So as you do usually with these unitarily invariant Hermitian matrix models, is you rewrite them as eigenvalue models, which is also where the Coulomb-Gas method for eigenvalues starts to come up. So, you do this using while integration, and the term you pick up as sort of a change of volume is this Vandermonde term here. So, product of eigenvalues or differences of eigenvalues to some certain power. And this power beta, for us, it's two, but it's instructive to keep around as a parameter since you could consider or You could consider orthogonally invariant matrix models of real matrices, and you get beta equals one, or you could consider symplectic, you get beta equals four. But for us, we've so far done only a complex case with beta equals two, but it's still nice to keep it as a parameter in the model. So, writing everything in terms of the eigenvalues, the spectrum of h, we get this expression. This expression. So we take the van dermond term, we put it into the exponential, and you get this logarithmic interaction between your eigenvalues. Now, this is a repulsive interaction. So what you see happening here, essentially, is there is a potential that is trying to confine your eigenvalues. Well, this is a quadratic potential. This is a quartic potential. It's a classical double well. So, this potential is trying to confine your eigenvalues to those wells. However, because of the nature of while integration, the eigenvalues are also repelling each other with a Coulomb interaction, hence the name Coulomb gas. And you get an interesting distribution of eigenvalues within this potential. And that distribution of eigenvalues is what we're after, at least in the limit when the number of In the limit when the number of eigenvalues goes to infinity. Now, the main thing we've done in this paper or in this line of research is to consider what happens when you add a fermionic action to the mix. So rather than just considering an action in terms of D, is there a question? Because I'm happy to. Okay, so rather than just considering an action in terms of d, which is trace v of d, we add this psi d psi fermionic action term to it. Now we integrate out the psi for now. We're interested in what happens to the geometry, so what happens to the rock operator. Yeah, so what we wanted to do was we wanted to see how the geometry changes when Geometry changes when instead of just the geometric action or the bosonic action, we also consider the fermionic action that you usually see when you're doing physics with non-commutative geometry. And our goal was to see how sort of the effective action, I guess, for the Dirac operator changes in the presence of this extra term. So, this is a fairly straightforward. Is a fairly straightforward computation, it turns out. The action changes from trace v of d to log of debt d, and this is very similar to that van dermann term that we saw previously. Because this is, in the end, this is just a exercise in Grassmann integration. Your fermions are Grassmann variables, and it's a standard result in Grassmann integration or Berzin integration. Or Berzin integration that if you integrate e to the minus this inner product over dψ and d psi bar, you get the determinant of d. And the determinant of d actually straight up is this product of differences of eigenvalues that we saw as the Vandermont determinant from while integration just previously. And again, we pulled a determinant. And again, we pull the determinant of d into the exponential, hence the logarithm. All right, so that's what happens when you add a fermion just to the action. Now, there's a problem. You may have noticed that for the Vandermont determinant, we were taking a product over indices i strictly less than j, but here we're taking a product over all pairs ij, including. ij, including i equals j, for which this is clearly zero. So the determinant of d in the zero, one fuzzy geometry is always zero. So we go a little bit further than just adding this basic fermionic action to the model. We actually add a fermion space, making this an almost fuzzy geometry, co-opting the almost The almost commutative geometry language. And the way that happens is we tack on this simple C2 space where there's a Dirac operator that just tells you the mass of that single fermion. So we do not yet consider the mass to be a random variable. We fix that as an input parameter. And then we consider still this. And then we consider still this d of h, this Dirac matrix that depends on the single self-adjoint matrix H as the random variable, the dynamical variable. In this case, you can compute the eigenvalues and the thirsty spectrum and the determinants of this guy. And what you find is this expression here. So, this is just what you get out of the integral from the previous slide if you take into account this massive Dirac operator. And this, well, this shift by M, it fixes your issue with zero. It also gives you a nice language to talk about the mass of zero case because using this analysis, it's very straightforward to see. Analysis: It's very straightforward to see that the mass to zero limit is actually perfectly well defined, and we'll see that later on in more guises. Okay, so what do we get? We get the fermionic eigenvalue model. So that's the model with fermions written in terms of the eigenvalues. We have our bosonic action, we have our fermionic eigenvalue report. Fermionic eigenvalue repulsion, and we have our van der Mont eigenvalue repulsion. So that's the origin of these sort of three parts of the model. And for mass equals zero, essentially what will happen is beta and beta two will just add up together. Now, let me talk a little bit about this beta two parameter, because for the psi d psi action that I showed. Psi d psi action that I showed on the previous slide. We know what beta 2 is, it's 2, like in the complex Dyson exponent. But if you take instead of the fermionic action J psi d psi, you actually get beta 4 equals 1. Now, sometimes you take J psi d psi, sometimes you take psi d psi related to how many fermions you actually. To how many fermions you actually expect to have. But these should be beta twos, by the way. Similar to how we are keeping beta as a coupling constant, even though it's really two for our models, we're keeping beta2 as a coupling constant because it's instructive to see what terms really come from this fermionic eigenvalue repulsion. Okay, we'll see all. Okay, we'll see all kinds of trouble this thing causes us as we move on. So, what are we actually after for this eigenvalue model that I've just shown? We're after the spectral density. So, this is all based on work that, from what I know, goes back to Johansson. If you have an eigenvalue model of this form, so it's not quite. So it's not quite the form we have, but it's very close. Then you can find a probability measure called equilibrium measure such that this equation holds. So any normalized trace expression can be computed by an integral against this measure. Moreover, you can find this measure as the minimizer of this energy function. So, this is really close to just minus what's in this exponential here. There should be a beta over 2 here, of course. So, this is the basis of what we want to do. Let me talk about how that works real quick. I won't spend too much time on this because it's mostly details. Because you have a convergence. Because you have a convergent matrix model, or because you're starting from a convergent matrix model, v goes to infinity as the eigenvalues, so the confining potential grows to infinity as the eigenvalues go to infinity, which basically tells you that any equilibrium measure, any measure that minimizes the energy, so that maximizes the probability of that spectrum, will have compact support. Because if your eigenvalue is Will have compact support because if your eigenvalues are too large, the potential says no, this is very high energy. This means that if you take a sequence of measures, of probability measures that converge to a minimizer, it has limit points. And these limit points are actually probability densities. This is a consequence of the minimizers having compact support and the potential being strong confining for the eigenvalues. For the eigenvalues, then because i is weak semi-continuous, such a limit point is actually a minimizer. So you can take any sequence converging to your minimizer and you get limit points which are minimizers and importantly, which are probability densities, compactly supported probability densities, even. And then, because for the one-dimensional problem, so V here is a one-dimensional. Is the one-dimensional, it's just the potential, there's no interaction between the eigenvalues that makes the problem convex, so your minimizer is unique. So that's existence of the equilibrium measure in high speed. And then why is it the spectral density? Well, you can show that the probability measure is quite concentrated at the minimizer's. At the minimizers of the action. So, capital S is the name for the, I've given the action here, where it's larger than the minimum value plus some little constant, the probability of that full set is actually quite strongly, can't come up with the English word, the probability mass of that space decays quite strongly with the number of eigenvalues you're considering. The number of eigenvalues you're considering. So, what you do is using this estimate, this large deviation estimate, you can squeeze the expectation values of some observable between its minimum and its maximum on these sets because these sets have such high contribution to the eventual expectation value. Expectation value, you can just only consider what happens here. These are compact, so your observable has some minimum and maximum. You can squeeze it between. And then if you take the point densities for the points, for the spectra where these minima and maxima are achieved, it actually turns out that these converge to minimizers because their spectra lie in this set. So if you let eta, this little You let eta, this little bit of play you have in the energy level of your spectrum, if you let that go to zero, the associated densities to these spectra actually converge to your equilibrium density mu. So that's why the minimizer turns out to give you expectation values of your observables. Again, in very high speed. And very high speed. There's details everywhere to be checked, but this is the main idea. So, our situation, so this is Jo Anson's situation or the classic single trace situation. What we have is an interacting potential. That makes a couple of things more complicated. So, I've called them speed bumps because they will shake you up a little bit. You've got to take it slow when you get there, but it turns out to be not that big of an issue. Big of an issue in the end. So, the main thing is convexity of the energy functional is not automatic anymore. It's now a requirement on the nice interact, the potential part of your model. And essentially, it requires the difference of moments of possible minimizers to be zero. Or that's how it turns out in practice. That's how it turns out in practice. You need, for example, for the quartic model, we needed that the mean of anything that could be a minimizer was actually zero, which turns out to be true, because if the mean of your spectral density is not zero, you can shift it so that it has mean zero, and that lowers the energy of that spectrum. And similar things I expect to work in higher. Work in higher order U as well. And then there's a point where getting your point densities that we used to show that the minimizer is actually the spectral density. There's a slight subtlety there that requires you to juggle around some terms, but in the end, also that turns out to not be too big a problem. So, what do we get? Well, if we have Well, if we have a potential that grows fast enough, so it is confining more than logarithmic, and it satisfies our convexity assumption, then we get essentially the same result as Johansen. We do formulate the spectral density. We go a little bit further than what we saw in the previous iteration of this slide. Because we want to be able to take functions of multiple observables. We'll see this later. But in a sense, in essence, what this is, is this is a strong independence result. So you might consider a product of the trace of some function of h with the trace of some other function of h. Then this is saying that in the large n limit, the separate traces. The separate traces are independent. So you can pull them apart and compute the expectation before you take the product, which is quite nice. And of course, we still get that the measure we're after that gives us this is the minimizer of this energy functional. So actually, everything is quite good. Okay. Okay, so actually finding the spectral density, I think I'm going to skip over that a little bit. It's a bunch, it's actually I quite enjoyed this. It's a bunch of complex analysis with Ruben-Hilbert problems. It's quite fun. What you end up with, at least in the mass zero case or the no fermion case, is a very nice equation. Is a very nice equation for your spectral density. So dμ of x turns out to just be rho of x dx, so nice continuous function rho of x. And that rho of x is given by this expression. So square root s plus, that's where your semicircle comes in. You see it up here. Square root is plus, that's your semicircle. MP of And P of X is a modification on the semicircle that depends on your potential. So curly is some function of your potential, and square root S is again related to this semicircle for the single support case. So this gives you a very nice formula for a row. I'll show it for the quartic potential here. If the support of your equilibrium density is one interval, which you can check whether that's the case. So for now, let's assume it is. Then the equations you get look like this. So we have rho of x is the semicircle times some polynomial depending on g four and g two. And g2. And then there's two sort of loose variables in here. There is a, which is the size of the support, and there's mu2, which is the second moment of our model. And it turns out that, well, the second moment of our model, we can express in terms of A, G2, and G4. And then we have this equation, which is again. We have this equation, which is again in terms of mu2 and a, that can help us solve for a. This is a nasty equation. It's degree four in x squared, or sorry, degree four in a squared. So there's not a nice solution for this, but you can solve it and find the spectral density, like resolve. Like resolve all the dependencies, and you can actually find a and find μ2, and you get the spectral density rho of x. So this is a plot of these spectral densities from various values of G2. And I included this slide to show one of the basic things that we've been looking at, which is the phase transition. At which is the phase transition in the spectrum of H. So, so far we've been talking about the spectrum of H, not the spectrum of D yet. We'll do that later. So, this single cut phase from the previous slide where the support is just one interval minus A to A, that's what you see for this blue and this lighter blue and this greenish line. But as you lower the value of G2 into the far negative, The far negative, your potential becomes this double well. And as these wells become deep enough, the support of your spectral density actually splits into the two-cut phase, where no longer is this true, but you get two intervals: minus a to b and b to a. And that's what happens for these redder lines. Alliance. And one of the things we've been interested in is how do fermions affect this phase transition? Do they affect the space transition and what is their effect? So for that, we need to figure out how to get the spectral density for the fermionic ensemble. And the problem with the fermionic ensemble is that once you include this logarithmic term here, which is part of the interacting potential. Directing potential whenever m is not zero, you don't have a nice extension of u to an entire function anymore. Because this, well, this generates cuts wherever x minus y equals plus or minus im. So that affects the complex analysis that you need to do to get at your equations for rho. And the equations you get are Are actually these. So I've written them in terms of rho divided by square root is plus because it's slightly nicer. But this is still an equation for rho. Only now it's not a nice equation, rho equals some polynomial. It's rho is the solution to some Fredholm integral equation with integral kernel k. And this integral kernel k. Uh, and this integral kernel k just depends on the support, doesn't depend on the potential you've used, or the polynomial part of the potential, I should say. The polynomial part of the potential determines p and that's its only influence. So this makes it even harder to exactly figure out a formula for rho, but this lends itself very well to numerical solutions. So just Just by doing simple numerics, this becomes a matrix equation, a matrix problem, and you can solve it or approximately solve it to find rho. And there are, just as in the non-fermionic case or the fermionic case for mass zero, a couple of additional equations that will fix the values of A and B, which are the edges of your support. Of your support. So let me skip this and actually look at some properties, actually look at the effects of the fermions. So we can compute the spectral density for D, which because we know this is m squared plus differences of eigenvalues for h squared, it's fairly straightforward to compute. To compute. And then to get at properties of d, we compute what we've called the normalized heat kernel. So it's not quite the heat kernel because we divide by 1 over n. And this is problematic for what we wanted to use this for, because it has a limit as d goes to 0, 1. Whereas normally, to get the properties of your geometry through the heat kernel, you want to look at its rate of divergence. At its rate of divergence as t goes to zero. We don't have a rate of divergence, so that complicates things a little bit. The solution for that are what we call the spectral dimension and the spectral variance. So the spectral dimension is this ratio of heat kernel-like objects. And it gives you, if you put in a manifold spectrum, it gives you the usual dimension. gives you the usual dimension in the T to infinity limits. So it then just gives you the rate of divergence for the actual heat kernel. The advantage is this is a ratio. So if we normalize both the numerator and the denominator, we're good. So we can use our normalized heat kernel to compute this guy. The spectral dimension doesn't turn out to be too good because it's very sensitive to a non-zero smallest eigenvalue, which massive Dirac operator. Which massive Dirac operators definitely have. That is solved by going to something called the spectral variance, which has all the same nice properties, but is insensitive to the smallest eigenvalue, completely insensitive even. Okay, so let's look at some actual graphs. So let's start at the bottom for g2 equals minus 3. What we see here. three. What we see here are the spectral density for a model with g2 equals minus three for various values of the mass of the fermion. So the yellow line is a mass zero fermion, and the dotted black line is the fermion-free model. So what we see happening is something we'd expect. If we add in a fermion, the fermion has this repulsive effect between Has this repulsive effect between the eigenvalues. So, what we see is the presence of a fermion, especially a low-mass fermion, gives you a stronger repulsion between the eigenvalues. So you get a flatter density, a density that's more solidly in the single-cut phase. Because what we see happening, for example, when G2 prime equals minus five, when you have a low-mass firm, When you have a low-mass fermion, the repulsion between your eigenvalues is quite high. So you don't get a separation into the two wells of your potential. The repulsion between the eigenvalues remains strong enough to push eigenvalues out of the well into one connected density. But if you make the well deep enough at g2 equals minus six, you see that even that you do get solidly. That you do get solidly into this double-cut phase where your eigenvalues have settled into the two wells of your double well potential. So the first thing to note here is that the presence of the fermion does certainly affect the location of this phase transition. We see at G2 minus five, we see very clearly that the yellow line is still in a single cut phase because it's not zero. At x equals zero, whereas the no fermion or very heavy fermion models are very much in a two-cut phase. So that's the first column. The second column gives you the spectral density for the Dirac operator. The most interesting feature turns out to be the dip that starts to happen. Happen as your density for H gets more and more into the two-cut phase. So we see when we are quite far from the two-cut phase, when we're solidly in the one-cut case, there is no to barely no dip in the spectral density here. But as we get deeper and deeper, we see that eventually, even for the no fermion case here, there is just a really Just a region where the spectral density is zero. So you get a split or a phase transition, essentially, a split support for the spectral density of rho d itself. Then in the final column, you see the spectral variance. I haven't plotted the spectral dimension because the spectral variance turns out to be more interesting. And we see that, yes, the presence of fermions. The presence of fermions affects the spectral variance, but it turns out that this variation is purely due to the variation in the spectral density of H. There's no additional effects of the fermions beyond changing the spectral density. Now, the limit as t goes to infinity of the spectral variance turns out to be one. This is a very slow convergence, but it does converge up to one for all these situations. For all these situations. And an interesting feature of the spectral variance is this jump at the start, the sort of big peak. That is an interesting property of the spectral variance and the spectral dimension, is they are energy-dependent dimension measures. So depending on the energy or the wavelength of the light that's propagating around you. That's propagating around your object, your manifold, your reflexive geometry. It can detect different dimensions at different energy scales. So we see that this peak means that at some low energy, somehow the space is high dimensional, or well, 0.8 dimensional roughly, before sagging in again and then going to one dimensional at the end. At the end. Now, there is a question of how useful the spectral variance is in this case, because, well, there's a I'll put a pin in that. I'll come back later to the question of how useful the spectral variance analysis is. Let's take a slightly closer look at the effects of the fermion repulsion on this phase transition in H, because H. Because usually, or for the most part, this idea that adding a repulsion will put you in the two-cut phase, or sorry, will put you in the one-cut phase more works, but not quite all the time. And this, I'm not sure how interesting this is in a grand scheme, but I thought this was quite fun because. Because the potential due to the massive fermion is log m squared plus r squared, it has a sort of maximal force between eigenvalues when they are at a distance approximately mass. So it turns out that if you play with the mass, so these are the spectral densities for various increasing masses. For various increasing masses at other fixed variables, what you see happening is: well, for mass zero, we are certainly in the one-cut phase. For mass 0.5, we are still in the one-cut phase. That's this line here. But then, if we continue increasing the mass, we get these lines here. And suddenly, we move into the two-cut phase, even though we've added this fermion. Added this fermion. And this is because at that mass, the repulsion for eigenvalues that are in different wells is actually stronger than the repulsion for eigenvalues in the same well. So normally, because eigenvalues in one well repel each other, you expect them to sort of boil over into the one-cut phase. And that's still happening. However, the repulsion between an eigenvalue in the one-wheel. Between an eigenvalue in the one well and an eigenvalue in the other well should become so strong that the effect within a well is dwarfed and the eigenvalues are pushed away from the center because they don't want to be near the other well. And that actually, by increasing the mass, we can still trick or put our model into this two-cut phase. If we then continue increasing the mass, Increasing the mass, for example, up to mass eight and mass 16, that effect diminishes again, and we get back into the single cut phase with this yellow line, which is barely in the single cut phase. So, this was an interesting, I thought this was quite fun, that you can't always assume that sort of nearest neighbor interaction is the dominant effect. And this is a nice example of that. Of that. Then another thing I thought was interesting is the effect of beta2. So beta2 is a parameter that we can't really affect. We could affect it by increasing the dimension of the fermion space. So beta 2 roughly is the dimension of the space of fermions, which is 2 in our case, or 1 if you take the j psi d psi. And adding more. And adding more fermions of the same mass just increases this beta2. Adding more fermions of different masses just adds more of these kernel terms for different masses. But because we've got it around as a formal parameter, we can play with it a little bit. And we can even make it negative. So that's what we've done for the very dark blue colors. And a negative beta 2 is a And a negative beta two is an attractive fermion repulsion, so a fermion attraction, which is quite fun. And you can see in this spectral density that as you make beta 2 lower, you see it gets more and more peaked around the minima of your double well potential. So if you were actually to do beta 2 equals minus 2 for mass zero, there's a Mass zero, there's a singularity there. So this is almost minus two. But you see it peaks much more sharply. And you see that in the three peaks of rho D2. And that affects this peak in the spectral variance. And if you make it very strongly repulsive, you get this very flat spectrum. That's the yellow line there. And then you get a very flat spectral variance that barely has any. That barely has any dip after that first peak. However, as you increase the mass, the effect of beta 2 vanishes almost completely, which is also, well, interesting effect. Because I want to talk a little bit about where I want to take this project in the future, let me skip the previous question. Let me skip the previous graph and talk about what I consider to be one of the biggest issues with this current approach. And that is that we get a bounded limit. So essentially inherent in this approach, to make sense of this approach, it guarantees you that the large n limit of respect to density is compactly supported, which is not what you expect if you want. If you want the large n limit to somehow correspond to an actual geometry, then you'd want an unbounded Tiruck operator, which would actually give you access to proper heat kernel methods. So these models just can't do that in their current incarnation. And one of the things I really want to work on is to find a way to, well, fix that essentially. So one of the So, one of the avenues to do that is to play with the normalization of your model. For now, we've just used as an action trace of some polynomial of D. And that sort of automatically gives you the correct normalization for H. So single trace terms get an N, double trace terms don't get an N. And that's, well, nice for this approach. For this approach, but it locks you in to this bounded limit. So I want to consider putting some n-dependent function in front of this guy so that you get a different or a non-convergent limit in some sense. You may be able to do this with the introduction of some energy scale to consider a model in terms of d over lambda. Consider a model in terms of d over lambda and that lambda vary with n maybe. And another thing that this approach does not do very well is multi-matrix Versa geometries. So it can really only do the zero, one first geometry, which is quite a big limitation. But Masood and Nathan have been quite successful in using Schringer-Dyson equations and positivity. And positivity constraints to at least find the moments, if not the actual spectral density, of higher-order faster geometries. So there's some questions there. For example, can we use the Schringer-Dyson equations or maybe TR in general to fix the bounded limit issue? And more in line with this project, how would the presence of fermions affect the Schwinger-Dyson equations? The Schwinger-Dyson equations, which is, I have no idea how that would work. I think Nathan has worked on this a little bit, but you'd have to ask him. All right, let's skip this, because I think this is a good time to solve.