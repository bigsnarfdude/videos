Talks here. Recording started earlier. Okay. Not the talks. Talks were okay, but you know, after the talks, this was the great time. So how do you follow Ivor? I mean, we can just go. You know, it's like, how to follow a talk like that, you know? Well, but anyway, thank you for staying. So let me read it kind of word by word because. Read it kind of word by word because it's what I'll be doing is actually all of the above that's kind of written there. So, by clustering, what's by clustering? I won't go into details, but it's basically clustering of features and subjects simultaneously. Okay, multivariate data will have tons of features, but will also have these features observed in a longitudinal way. Okay, and then I was watching with Watching with trepidation when Ivor was presenting the reproducibility because I was darn, was my paper reproducible? And then I realized I'm doing DTI. I don't have to worry about it because he was summarizing fMRI. Yeah, so yeah. So, of course, I would like to say this is my work, but you know, sometimes we kind of forget to mention everybody who worked on this stuff. This was actually a dissertation paper of Caleb Weaver, who was a Of Caleb Weaver, who was a student at North Carolina State University, was one working under the supervision of Dr. Lu Xiao. Some of you might know. And these are my collaborators at Indiana University, Chu Ting-wen and Yu Chen Wu. They are both working in brain imaging, mostly in diffusion-weighted imaging. So, I won't go through that. I mean, we all know kind of the stuff. We put some Of the stuff, we put somebody into an MRI scanner and we get something out. What we're interested in here is the structural connectivity, or rather, information on the tracts from the brain. Now, it's just not anything that we will be doing here. We simplify that a lot in the analysis. This is kind of one first paper in a series, and I will tell you. And I'll tell you kind of how it started and how where we're going with that. But just to put, there are many talks on fMRIs. So just to put everybody kind of on the same page here, we'll be extracting four features from diffusion-weighted imaging. They are fractional anisotropy. You can think about that as normalized variance of the kind of water diffusivity on the tracts. There's mean diffusivity, axial diffusivity, and radio. Axial diffusivity and radial diffusivity. Okay, so how did I? So I've done some work on diffusion imaging earlier, but actually the data that we'll be using here and the whole study that I'm involved in, it's called Concussion Assessment Research and Education, or for short care consortium. We've been running this study and I'm saying we've been involved with the study since We have been involved with the study since 2014, and especially for the students in the room, I have to tell you the story how the study started. I heard about the study on May the 2nd, 2014, when I was still on my sabbatical. I was with my postdoc in LA. We're working, you know, people there, and I get this call from my work. Are you interested in participating in the study in play? Yeah, it sounds interesting. There's imaging, there's, we'll be studying concussions in college athletes and so. Concussions in college athletes and so on. Fine. May 15th, the grants get submitted, and this is non-NIH. This is Department of Defense and NCAA. May 29th, the grant is funded and it's announced on the loan of the White House by President Obama. Then it's like, you know, it can get better, yeah? Well, actually, it can because we're in a second renewal. So far, we go. second renewal so far we got about 105 million of money to study athletes to study concussions in the athletes from that study i got two r01s as from nih from ni and s as uh so to support like what i'm doing here this is from my grant from an r01 so this kind of tangential grants based on the data that have been already collected data are public i know there was a question to tom I know there was a question to Tom. The UK Biobank data. These data are public. They are by law. They have to be uploaded to something called FIDBIR. It's a repository of studies on concussions. The first time we submitted the data to that repository, we broke their system. Still to this day, it's 60% of the data in that system is our data from this one consortium. About 55,000 athletes, about 35,000 athletes, about 3,000 or so concussions. Of course, not everybody got brain imaging. We have a sub-study of that study where this data are from. So here we concentrated on 60 college football updates. 33 of them were actually they had a concussion, 27 didn't with its matching. So it's a kind of frequency matching that we're doing here. Frequency matching that we're doing here. So, those groups are comparable. What we'll be using today, as I said, there will be those four measurements on actually 27 tracts. This was the data were pre-processed by Chu Ting-gwen, who was a postdoc at that time, when this pre-processing happened. And we have this data actually over four time points, but we'll be using here three time points. These time points will be right after the injury at the time they become. At the time, they become asymptomatic. And then we have data seven days post-unrestricted return to play. So you go on the field, you get concussed, you recover, you become asymptomatic, you can go back to practice, and then you go back to play. So we have this brain imaging data. And here I'm just presenting data from DWI, but there were seven sequences they went through. So you have your resting state, fMRI, you have structural imaging like T1, T2. You have structural imaging like T1, T2, T2 star, blood flow, and so on. So, just to give you an kind of, because again, this is not, I think we're not doing as much work in the field of diffusion imaging, I think, as statisticians as the field. So, I'm just giving maybe a brief overview of such data here. So, this is one of the atlases that's used here. Atlases that's used here, we have 27 predefined tracks. So, think about it as, you know, if you work, we had this discussion about the parcelation of the cortex. You know, is it 100? Is it 200? Is it 500 parts here? There are a few atlases that are widely used. This is one of them, this 27 tracts. And here I'm just presenting some data. You know, if we had data, let's say, if I'm not mistaken, it says in corpus callosum in the middle here, and we can show. In the middle, here, and we can show the data, let's say, here, mean diffusivity data. So, here we, what we did in each of these regions of interest or tracts of interest, I should say, of course, we have data over many voxels, yes. So one of the things was how do we summarize it? And there's another kind of project I work on where we actually take information along the tract. Take information along the tract, and we treat it more like functional data. So you can think about like one-dimensional curves in three-dimensional space. This is for another time, another talk. But here, we basically took the distribution of the measurements of each track. So we created a histogram, or you can say dense, well, histogram of the data. Let's say you have 5,000 voxels on a tract. We get a histogram, and here, Histogram and here, because of the specific condition, because of a concussion, which is, I didn't mention that at the beginning, it's actually a heterogeneous condition. If we get hit from the front or we get hit from the side, how the injury happens is different. So, you know, from the side, you get more rotational acceleration. From the front, it's more of a linear acceleration that kind of impacts your brain. Brain. So, here, what I'm presenting, I'm presenting data, just you know, sample data from three tracks. One is from the, well, let's say, let's concentrate on the green one, onsonet fasciculus. So, you can see here that for one of the individuals, for the control, I mean, here we still have this kind. I mean, here we still have this kind of post-injury time point, even though there was no injury. How did we do that? We had controls matched before the case happened. I can talk about the algorithm some other time, but it's basically we knew who we should approach when somebody gets injured, when somebody gets concussed. So we try to catch them as fast as possible because there's a huge influence of exposure to hits to the head during like the football season. Head during like the football season, so we don't want to have somebody injured, let's say in August, and we get a control in October because a control in October would be, you know, not, it would be comparing couples to oranges there. So three time points, and here we just show this is the average, but however, what we are working with, it's not the average from the histogram. We're working with the highest, well, higher quantiles. The reason for that. Quantiles. The reason for that is that the majority of the tract will not be affected. Okay, so here in our work, first we tried to use whole density. That didn't work so well. So we concentrated with what I will be presenting is the 90th quantile. That's what we chose, but we did some sensitivity analysis for ATF 95. The results are very similar to the work that I'll be presenting here. The work that I'll be presenting here. So, the study, the kind of the measurements, the stuff we did. What was the kind of the hypothesis? And I'll read it to you, but so if we had concass subjects, we kind of hypothesize that there are homogeneous subgroups that share common trajectories of recovery, characterized by what matter properties in the subsets of the tracts. Subsets of the tracts. So we don't think that every tract is affected. We don't think that all the voxels are affected. There's a percentage of them, and only in some specific subsets of those tracts. So the question was, is there just, is everybody in the CONCAS group behaving kind of in a similar way? Are there differences? Are the controls, well, controls, they should behave in the same way because the only thing that they Because the only thing that they experience is exposure to heats to the head, but they haven't been concussed. So, this is nice, you know, but the question is how, how can we do it? So, you have multivariate data, you have heterogeneous data, you have longitudinal data. So, and you want to do this by clustering. Well, there was, it's not, we're not the first to do this work. The biclustering work, it's not, you know, it's not. Work, it's not, you know, it's not the last two, three years, it has been done, even the longitudinal clustering was done. But what we're doing, we're actually combining the two, we're doing longitudinal biclustering. Okay, so what has our work contributed? First was something that was actually set up nicely by Caleb. He put it together the optimization criterion that we have. criterion that we have it's uh it's uh it's a convex optimization criterion so we can actually solve this problem and he used this admm algorithm to solve this problem of course you kind of will have great ideas but will they work actually in practice so we did a simulation study actually it worked pretty well and we applied this method to data set that we started from so i won't go actually how much what time should i think 15 minutes 15 minutes. 15 minutes. I would rather. So, this manuscript is still in review. But if somebody's interested, I'm willing to share it. And, you know, I promise to respond to email. Maybe not in the next 15 minutes, but after I code is actually available. So you can actually do that stuff. It's on GitHub, Caleb's GitHub. But the setup that we have here, so we didn't go for something super complicated in modeling longitudinal. Super complicated in modeling longitudinal data because we're just modeling three time points. So we try to concentrate on the acute measurements. So I didn't tell you about the fourth time point, which was six months after the injury. Pretty much by six months, 95% of the athletes go back to normal. It's one of those nice conditions. It's not like, you know, there are some conditions where, you know, they're chronic. Once you get it, you have it forever. Get it, you have it forever. Concussion is nice, you know, because it's you get it, you recover, get it again, you recover. You might get it again, and you recover. That's just me. Reconcussions, I think. So why JK here is a continuous measure? So yes, but in the simulations and in the example, actually for us is discrete. The example actually for us is discrete. So, I mean, we can do it in a continuous way. That's not a problem because we'll be one of the things that we're fitting our linear mix models. So, we can deal with continuous time. And actually, that's a great question because what I was showing with the here, not everybody becomes asymptomatic at the same time. Somebody might take three days, somebody might take five days, or you know, or six days. The nice thing is now. Now, 2010s, 2020s, you cannot go as a football player, play again the next day, or practice again the next day. This is a beautiful thing because 20, 30 years ago, you know, it's like walk it off. You get on the field the next day. That's actually dangerous because there's much higher risk for the next concussion. And actually, the effects on the, if we don't recover, the effects are much stronger of that. Of that. Yes. So here we do discrete, but continuous is not a problem. Okay, so again, as you can see from kind of the setup at the bottom, this is kind of, it looks like a mixed model, it smells like a mixed model. You know, it is a mixed model that we'll be using here. So, here to adjust actually for the correlations of the observations, we assume that, okay. I screwed it up again. But let me see the okay. Okay. Sometimes it works, sometimes. So here, just to account for these correlations between the same observations from the same individual, we assume that those coefficients alpha are random and were interested. We're interested. So, what are the betas? Betas are the subject and feature-specific slopes. So, here we have 108 features, we have 60 subjects. So, we have all of them as, you know, they can vary. But what we'll try to do in this bi-clustering approach, we'll try to cluster them both by subjects and by features. So, again, in the Again, the setup of the model, and again, we're using here the actually, we'll be using the restricted maximum likelihood method. But now, the important point is let me spend like a minute on this slide because this is more unusual kind of in this modeling. So the first part is just your regular kind of maximum likelihood approach, but here oh shoot. So just random coefficients, not random pointing. So let me just concentrate here on two parts, the penalty parts. So the first one, the sum with some weights of the differences of the coefficients, the first part is, let me see, it's by features. You see, it's by features, the second part is by subjects. So basically, we want them to be close if they are close, but you know, if they are in different groups, they don't have to be close to each other. So we're optimizing such a criterion. Now, it looks kind of a bit scary, but fortunately, this alternating direction method of multipliers works here. And here we also have the selection criterion to choose the number of bi-clusters. Choose the number of by clusters, so clusters both in the features and in the subjects. Yes. So the data zero, the topics predicted, but they're not languages. That's right. They're subject specific, they're not random. So we use alphas to adjust for the correlations for the same observations here. No, they're not random. You're right. So if you want to make them random, Tim, give a moment to. Tim, the prior or John Bayesian approach, Mikel. Okay, so simulation study. Well, it's we also have to, because it's a longitudinal thing, we have to estimate the correlations in the four alphas and so on. Here, we used a simplified structure, so just the compound symmetry structure. Again, with three observations, you can go crazy. You, I mean, if you had more observations. If you had more observations, you know, maybe there is some more complicated correlation structure we could assume here, but it's basically the setup of the simulation. Based on the data, we kind of we did the first simulation with fixed time points, and it's just time zero, one, two, three. Another one, kind of random time points, but again, they were on the kind of uniform integers, so it's not truly random, it's just the random number of the time points. Okay, but let me show you some results here. Show you some results here. Before that, of course, we're not the first to approach by clustering, but at least according to, I'll blame it on our student, we're the first to kind of do this longitudinal by clustering. So we compare this method with some other methods that were proposed in the literature, where first the coefficients were estimated and then they were thrown in into, let's say, the sparse bi-clustering method of tan. Sparse by clustering method of Tan and Witten, or the convex by clustering algorithm, so-called Cobra by Chi and others. So, no, it doesn't. But anyway. So, here, just a simple example. So, here, this is for the simulation study, one of the settings that we did. We have three subject clusters and we have three feature clusters. What do I mean by that? What do I mean by that? For subject, yes, it's not fun anymore, but you know. So for subject cluster, we have green features that stay, green and red features that stay pretty much constant over time, black features decreasing. Second cluster, green and black stay constant, red increasing. Constant rate increasing, the third cluster, everything kind of stays approximately constant. Okay, for feature clusters, this is the setup that we have: that you know, we might have correlations between the features within a cluster, and this is just summarizing kind of what. So, subject cluster two, the feature cluster two, we see that the red is increasing. Yes, so this is kind of sorry for not having the legend here. So, these are the results from a These are the results from just the simulation example. And since we have a method to choose the tuning parameter lambda of this method, sorry, this tuning parameter was chosen, it's kind of pretty similar to the truth, where you know, if we didn't do any kind of penalization, this by clustering, this would be our estimate. This is for lambda equal to zero, yes. If we over penalize, we're kind of we're getting fewer clusters and kind of. Getting fewer clusters, and kind of, as you can see, it kind of washes out the behavior within and between the clusters. Yes? Quick clarification question? Five minutes, okay. No, not that. Your matrices there, are those the weights or the betas? These are the betas. I will talk, it's a good question actually about the weights because the weights are there to make the penalty to be on an equal footing, because you might have Because you might have 100 subjects and 1,000 features. So the two penalties, and we still have one penalty parameter. So this WII, WJJ, they are pretty much to put the two penalties since with LASO, you normalize the variables, yes, to have for the coefficients to be on the same scale. Here, with it, of course, the normalized. With it, of course, the normalization of the covariance, but those two penalties, one cannot be like 100 times bigger than the other, because then, you know, the clustering in one domain, let's say features, would drive the fitting and the subjects would be just as if it weren't there. So it's a large table, but let me just point out to you the last column here, the fourth column here, and this fourth column. This is our map. this fourth column this is our method we are presenting here adjusted rand index you can think about that as classification accuracy like this a weighted classification accuracy can say we always perform better but majority of the cases for different settings different number of individuals 50 100 different number of features 10 or 100 different number of by clusters because there are a few things we have to do here so it's not only subject features but also you know maybe Only subject features, but also you know, we might have nine by clusters or 16 or you know, whatever number we can have two by clusters for subjects, three, let's say, for the features. Okay, so it works pretty well. Hopefully, if the reviewers are nice or provide some useful comments, we should be able to get this manuscript accepted soon. Data. So, in the data, we looked at the So in the data, we looked at the 27 tracts, four measures for each tract, and they are kind of in colors. So there's axial diffusivity, radial diffusivity. Actually, we did one minus fractional anisotropy because they go in different directions, the diffusivity and FA. So we just transformed it. And since F A is on the scale 0 to 1, we can do 1 minus F A to have all the correlations positive here, or at least non-negative. The positive here, or at least non-negative. The last one is the immune diffusibility. So this was kind of the model that we're fitting in the data example to the correlation matrix would estimate the status here when we're estimating the covariance matrix. So again, it's a bit simplified. Otherwise, we would have like hundreds of parameters that we're not truly interested in. So these are results. These are results after some massaging of them. So let me explain what do we have there. Red are the concast at leads. Black on the subject side. So red, concast, black controls. Here we got three clusters for the subjects from this bi-clustering argument, from this bi-clustering algorithm. From this bi-clustering algorithm, four clusters, four features. Now, it's not like, you know, we didn't use any classification here, so we didn't know what the labels are, yes. We use this clustering approach, approach really to classification, yes, because we would like to see that, you know, hopefully there are some more than one group of the concast athletes and maybe one or more groups of the controls, but here, majority of the controls. But here, majority of the controls ended up in this cluster with the change on average being close to zero. But for this majority concast at these clusters, here we had like eight out of 11 concast and nine out of 12 concast, their behavior in their trajectories was different. Here, for this subgroup, the measures were increasing. For these subgroups, the measures were decreasing. Okay, if we just Decreasing. If we just treated that as one group and let's say use like logistic regression or something like that, that would wash out. So let me just show you. So these are actually, this is not from modeling. These are just, we said, okay, let's go back to the data and let's see from those measures for these feature clusters, how do they behave? So what we call this non-SRC, non-spore-related concussion cluster, the bottom one. Related concussion cluster, the bottom one. In the paper, actually, now we're calling it different because we don't want to confuse people. Like, I'm confusing you here. So, this is not from the knowledge that we have. We just call this cluster like that. Everything pretty much stays flat. One group decreasing, the second group increasing. So, we also looked: okay, it's one of those things when we do brain imaging. We do brain imaging. The question sometimes is: so what? You have those changes in the brain. Are there any behavioral changes? No, we just don't know. So, here what we did, we looked at, let me just concentrate on the top middle plot. This is a very simple measure. It's an instrument called SCAT. This is the symptoms after concussions. I forget AT, what it means, but basically it's a very simple instrument. Basically, it's a very simple instrument: 22 questions, and they're asking: Do you feel pain here? Do you feel pain here? Do you feel dizziness? And it's on a scale, it's like Likert scale from zero to six. So you might have, let's say, number of symptoms out of this 32, sorry, 22 symptoms, you might have 10 symptoms, you might have five, and so on. So, here, not using any information about the grouping, the group, the cluster. The group, the cluster of subjects that we denoted here in black showed no changes. Majority of them are non-concussed subjects. The other two groups, actually, they had quite a few symptoms at the beginning, 10, 12 on average. By the asymptomatic time point, and especially by the unrestricted time point, the number of symptoms was very close to zero. So, since I wanted to make sure that I have this R Make sure that I have this R package, you know, because this names after the last half and I'm really scared of Ivor, you know, it's like don't want to appear in his next paper, you know, as this, this is the guy who. So we developed the methods. And in the software, what you have, you don't have the data. Unfortunately, we can't release that data yet, but we have simulated data. So you can use, there is a whole code, you can run the code and you can get this by. And you can get this by clusters. The one thing that we'll be doing in a bit is this both more complex patterns in longitudinal data, but also what we're trying to do, we're trying to sparsify it. So we really want to see which tracks actually contribute or which tracks were affected. Here, we, you know, it's basically which tracks and which measures were affected. And I believe that's the end. This is just some literature. Thank you. Thank you. Okay. A question from the perspective of policy messaging, especially with your involvement in the consortium. Because a lot of this led then to a very unpopular penalty called targeting. And it's probably one of the most unpopular things in the sport. But it has also been shown. But it has also been shown, right, that, well, actually, I'm not sure if it has been shown, but maybe you can comment on it, that the implementation of that call has led to greater protections for the players. I'm not sure if there's empirical data on that, but that is the feeling I'm getting. But given the unpopularity and the amount of money that goes into college football, what are your thoughts on the consortium's plans, or what have you heard from the consortium with respect to, say, you know, With respect to, say, you know, this is a necessary thing for the sake of protection. I cannot make comments on targeting in particular. However, the data from the consortium and the results of the findings that we had actually influenced the college football policy. This is for me, this is even more important than stats papers. Because, for example, so I'm sorry for not answering your question. So I'm sorry for not answering your question directly, but half of the concussions actually happen before even the season starts. It's something you don't see. This happens in practice. Now in 25 days of practice, it's exactly 25 days. It's not 26. It's exactly 25, one practice per day, maximum. It's very regulated. You cannot get college football players to practice, let's say, in June. To practice, let's say in June. It's only during the month of August, maybe late July. You cannot get them to practice earlier. So even if they have unrecognized injury, especially concussion, you know, earlier, they have enough time to recover before kind of the new season starts. Targeting, one of the things, as you said, there are fewer concussions now. And I think the protocols actually became much better. You probably saw this tents on the field. If you watch college football, there are these tents on the field. They will put They will put suspected if a player is suspected of having a concussion. The reason for that, one of the reasons is there are simple tests, you know. What did you find on the web? So there are simple questions because, you know, okay, I'm really going off on the time. We're working on something, and there are already some patterns. You draw blood. Some patterns. You draw blood, you analyze the blood. In five minutes, you have a test because of the biomarkers. Do you have a concussion or no? So, what people do now is go into the stand and you say, okay, who are you playing? What time is it? What day it is? And if you had a concussion, you know, you might have trouble answering such questions. Okay, so this is one reason for the tent because they cannot see. You know, because you know, what day is it? Okay, there is a big billboard. Yes, they could see that. Yes, they could see that. So, targeting, you said it's unpopular. It's uh, and again, this is hard for me to make a comment on that, but you know, it's helpful. It's helpful for the health of the athletes. We don't want them to be gladiators. We still want them to have productive, you know, it's if you think about high school, college football, how many of those players are actually making big bucks? It's a very, very small percentage. Yeah. Yarik, just one very quick question. Step zero. So are FA diffusivity measures, are these the best measures for looking at the effect of concussion? Because Tom talked about this hyperintensity and I'm wondering whether that could be. So fortunately, concussion, so with hyperintensities, you know, I'm always thinking about like multiple sclerosis and so on. Concussion is an Concussion is an acute condition, and I mean, I'm giving probably too much information about myself, but I know I've had at least two, if not three. I'm still standing here and doing okay work. I mean, it's not great work, but you know, it's okay work. So here, even after like two weeks, three weeks after the concussion, it's pretty much not saying everything goes back to normal, but majority of the stuff, unless it's this is mild traumatic brain injury. Mild traumatic brain injury. This is not the TBI that you get in a car accident. This is not the one where you get into a coma. You know, this is still, it's, you know, it's a hit to the head, but it's still not kind of like life-threatening. Now, the question about FA, the sequences that we used, and actually my imaging collaborators would be much better to comment on that, but they are using, for example, diffusion kurtosis imaging. They are using some other measures, and I can't remember. Some other measures, and I can't remember now. There is this method called Noddy, because the practography, what is bad for if you have crossing fibers, they cannot detect this crossing place and they are not crossing like this. They are crossing like this. So there are fibers, the tract might be crossing another tract in this way. The DTI is actually not very good in distinguishing if it might turn at this crossing point. So these other methods, for example, Nodi method actually can, it's not more. It's not more one step back. How DTI is modeled, how diffusion-weighted imaging is modeled with DTI is based on tensors. So you are picking an ellipsoid to your data, and then you get redirections in this ellipsoid. That's how you get all those measures, FA, MB, and so on. So there are better methods. And unfortunate thing here is we use those methods not with such complex statistical modeling. They are not. Are not we didn't get much in terms of the results, in terms of the differences of the concussed and non-concast apps. Great, then thank you, Argon. All of our comforts.