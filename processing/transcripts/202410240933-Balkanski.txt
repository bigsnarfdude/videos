I'll talk about Fair Secretaries and Fair Predictions. This is joint work with Will and with Andres Maguri, who is a great postdoc who's on the academic job market this year. So I'd like to start this talk by motivating and describing the framework of learning of method algorithms. So the standard approach for mathematically analyzing algorithm has been voice-case analysis. The main benefit of wasking analysis The main benefit of Washington is that it provides strong obsessive guarantees, but it often leads to very pessimistic lower bounds that often don't reflect the encounters that we, the obstacles we encounter in practice. On the other hand, we have machine learning, it works well in most cases, but often it lacks non-traditional worst-case guarantees. So the goal of learning on many algorithms or algorithms predictions is to achieve the best of both worlds. So in this framework, the algorithm is given Framework, the algorithm is given as input some ML predictions. And the goal is to use these ML predictions to try to overcome some of the lower bounds in worst-case analysis, some worst-case analysis. So the two standard measures that are used in this framework are called consistency and robustness. So consistency, what it asks for, is it's the worst case performance guarantee when the predictions are accurate. Predictions are accurate. So, here, what we would aim for is to try to achieve performance guarantees that are better than the best known guarantees from settings without predictions. But we also still want to maintain robustness guarantees that will hold even when the predictions can be completely wrong. And more generally, we want to have guarantees that we interoperate between consistency and robustness as a function of the prediction error. It's arbitrary wrong. What do you measure as performance? It's more dangerous. So, for example, I will mention an example soon, but in online algorithms, we're going to look at the competitive ratio. And so, we're going to look at the competitive ratio in online algorithms, and we're going to be given some prediction as additional inputs. And we're not going to make any assumptions as additional input. It could be completely wrong. I will do an example in two slides. But the main idea here is that we just give it basically some side advice, and we don't want to, and the side advice could be completely wrong. And we decide advice could be completely wrong, basically. It's additional advice on those sides. It's regret with respect to some experts. We're not going to look at regrets, but there has been some work that has also looked at regrets in this framework. So yes, this framework has actually been studied in many different settings, online algorithm, mechanism design, with regrets. It has really taken off over the past five years. There's been a lot of papers studying a lot of different problems in this framework. There's this nice website that keeps tracks of all of Nice website that keeps tracks of all of the papers in this area. You can go check if your favorite problem has been studious predictions. And one area where it has really been mostly studied is in online algorithms, where the predictions are about the uncertainty about the future. So let's do a complete example to illustrate this framework. And this is also going to be the main problem we're going to consider in this talk. And it's going to be the classical secretary problem. Secretary problem. So in the secondary most predictions, we're going to have n candidates. They're going to have true values known by Vi's. These are non-known to the algorithm initially. And now in addition, we're going to have some predicted values. But these predicted values are given initially to the algorithm at times t equals to zero. Now these candidates are going to have they're going to arrive in a random order and upon the arrival of a candidate Upon the arrival of a candidate, we're going to get to observe its true value, and we're going to have to make the evocable decision to either select that candidate or reject the candidate. And now the candidates arrive, and at any time t we have to make a decision given all of the predicted values and the true values of all of the candidates who have arrived so far. And now our goal is that we get to select a single candidate, and we want to select a candidate whose true value is going to approximate the maximum true value overall. So, what is known about this segregative restriction problem, you can actually achieve a cognitive ratio that when the prediction error is going to be very small, this cognitive ratio is going to get arbitrarily close to 1. But on top of that, we have that even if the cognitive ratio is completely wrong, we maintain a constant probability ratio. So, here we are looking at the expected value that we're going to get from the scale that we select, and we're going to. Kinetic select, and we're going to compare it to the maximum value overall. So, this is the ratio of the value of the kinetic select to the maximum value overall. Doesn't look like we had placed any one here. Where's the one of the accuracy of the estimates? Good. So, here the role that the VHATS plays is that when you're going to make your decision. I know, but in the result it doesn't show up. I know, but in the result, it doesn't show up. So, good. So, here the way it shows up is that the prediction error is going to depend on the accuracy of the predictions. I haven't defined the prediction error yet, and I will define it later on. They will depend upon the predictions. Yes. So, this really plays a role, and this is where we see that we can kind of get this improved guarantee against the urban world when the position error is going to be small. Okay, so whenever we want to use the Whenever we want to use machine learning for making decisions that directly impact individuals, there might be some fairness concerns that arise. So some prominent applications where machine learning has been used and some important financial issues have been raised include hiring, recitability evaluation for bail decisions, and credit risk evaluation for loan decisions. And so there's been a hugely And so there's been a huge literature on this topic that tries to uncover, understand, and address these fairness issues. So the main goal in this talk is going to study fairness concerns in this framework of algorithm with predictions. And so here the potential concern is that if the predictions that the algorithm is given as input are biased or unfair, and this algorithm uses these predictions to make some decision, then To make some decision, then the decision might be unfair. So we're going to consider this potential issue for one specific problem. This is going to be for the secretary problem that we just saw. Now, there's several reasons why I think the secretary problem is a great problem to consider fairness and algorithms predictions. The first is that it captures hiring applications, where fairness is an important. Is an important issue. Also, there's been several works that have already studied the secretary problem in this framework of algorithmics predictions. Okay, so what is going to be potentially unfair, what are we going to consider as unfair in the secretary problem of prediction? Our concern is that the best candidate, so the candidate who has the true maximum value, whom we're going to consider to be the most deserving of being selected, might not Being selected might not be selected because of these predictions that can either underestimate that candidate or overestimate the other candidate. Now, this is not just a hypothetical concerns. A hypothetical concern in the algorithm that actually I just mentioned, there's actually some instances where the predictions cause the best candidate to be selected with already zero. Okay? And we're going to consider that to be unfair for that. That's a definition of effects. I mean, that's just an example of. Just an example of the formal definition. So, I'm going to give the formal definition in the next slide. Yeah, I'm just trying to work at the high level. So, now, if you're concerned about machine learning kind of like introducing some fairness issues, one approach you could do is just completely throw out the predictions and not use them and just run a classical secretary algorithm of that predictions. Now we know that running from classic from the classical security problem, we can select the best candidate authority one over E. Best candidate authority one over E. But now the main drawback of this effort is that we would completely lose the benefits that predictions provide you. We would lose the improvement ratio that we can achieve when the prediction error is low. And so I mean the question at a high level is, can we maintain these benefits from the predictions while also trying to provide some fairness guarantees and having some notions of fairness? Okay. So now let me make this concrete and give some formal definitions. Concrete and give some formal definitions, what are going to be our objectives. We're going to denote by A the index of the candidate that's detected by an algorithm, and we're going to have two main objectives. The first one is going to maintain the best known guarantees that we have from the secondary baronet's predictions. So it's going to be called C-smoothness, and we're going to try to match this guarantee that is known. And so we can say that the algorithm is C-smooth if the expected, if its cumulative ratio is going. Ratio is going to be for all instance the following. We're going to get that expected value of the kinetic who we select, achieves an approximation ratio to the maximum true value, that is 1 minus c times epsilon, where epsilon is the prediction error, which is defined as the maximum multiplicative error over all of the candidates. Okay? So it's a max error version. Yes, yes. That's not the type of guarantee we're used to giving for our prediction. So, yeah, so in algorithm with predictions, this is really standard. There has been actually some very recent work on trying to achieve what's called MAC guarantees, where you have this mostly approximate kind of like guarantees. It's also interesting, but yeah, but this is kind of like the standard for algorithmous predictions. So, and the second one is going to be this fairness guarantee. And the second one is going to be this fairness guarantee, an unsure of fairness guarantee that we want to provide some guarantee for the best candidate to be selected, regardless of what the predictions are going to be. So for any kind of predictions, we want that the probability that the true best candidate is selected with some probability at least F, and we're going to call that F. You don't distinguish between the case that the one selected is the worst. If there are many candidates above the one selected, yeah. Yes, and the motivation behind that, I would say, is that if you think about the community offline setting, what you probably want the best company. You don't even take into account the difference between Vmax and Vmax minus one. Maybe the difference in skills between them is very small, especially in the bigger terms. But that doesn't count. Okay, so a few notes. A few notes. First, like this sort of C smoothness, if we get any C smoothness for C grade C grading zero, then we have one consistency. This guarantee that if we have this guarantee and that the error is exactly correct, and epsilon is zero, then we have that we are exactly optimal. And also, F fairness implies this F robustness when the predictions can be completely wrong. The reason is that, of course, if you pick the best. The best candidate of priority F, then the expected value of the candidate you select is going to be at least an F approximation to the maximum value. This is a notion of fairness, that's a meritocratic notion of fairness, which is different than group fairness, so we have different groups. And so our main question is, can we achieve simultaneously both some smoothness and some fairness guarantees? So let me discuss these two metrics in the context of. These two metrics in the context of some previous work. So, we have this previous work of Fuji and Yoshida. The algorithm is smooth, but does not achieve any fairness guarantee. Another paper that has to be algorithmous predictions is by Antonio Lis et al. Their paper, they can achieve constant fairness, but no smoothness guarantees. And this is similar as algorithms for security problems that don't use predictions. To be fair, to this paper, they have a much weaker addition of prediction where the only prediction that they're given is. Prediction where the only prediction that they're given is a prediction with the true maximum value. Okay, smoothness for any positive C, strictly positive C, and one over E fairness. Okay. So since we cannot get this smoothness in this one over E fairness, what we're really going to be aiming for is some constant fairness. Another line of work that is related to this is to I don't know. Related to this is to algorithm security of predictions, security power is distributional advice. The main difference between algorithms predictions and distributional advice is that distributional advice, you make some kind of assumptions on the predictions, whereas in algorithmic predictions, you also want to be able to achieve some guarantees that hold without any assumptions on the predictions. And finally, there's also been some work on Ferenc in stopping algorithms, but not in the context of algorithms predictions. Algorithms predictions. So, before I move on, any other questions about these objectives? Do this make sense? Good. So, now present our results. So, our mean result is that we are able to achieve both constant smoothness and constant fairness. So, what this is saying is that we can have this constant fairness guarantee without any loss in the best-known asymptotic smoothness guarantees that we were previously known for. Guarantees that was previously known for security service predictions. So that's our main result. In addition to this main result, we also have that this holds not only for notion of prediction errors. I mentioned the prediction error where we had the maximum multiplicative error. We can also have results if we look at additive errors. And here we would have some additive guarantees and social multiplication. Guarantees and social media guarantees. We also have an extension to the K secretary problem where you can pick K candidates instead of a single one. I will mention the result that we have for this setting in detail at the end of the talk. Yes. I have a stupid question while we still have the results on the slide. So what would the difference be? Like if I randomized between. Okay. Like I just tried to compare it. So if I'm guessing what you're going at, what if you do like for? I'm guessing what you're going at, but if you do like post-property one-half, you follow 4e for the second bullet, right? So, yeah, so here, what is okay, so let me explain to everyone. So, there's often this approach with the predictions, with one thing you can do is with priority one half, you just completely trust the predictions and you would follow the predictions, priority one half, you just ignore the prediction and follow the gossip factor. Good. So, why would this fail here? Hey, what we really hope to achieve is to have this kind of like Hope to achieve is to have this kind of like one consistency. You want to be arbitrary close to optimal when the error is small, right? And now, if you run the second thing, the optimal thing. Exactly, yeah. You wouldn't be arbitrarily close to optimal because you would use. Any other questions? Yes. Is there a way to get like the classic parameterized trade-off using lambdas between yeah, the two threes? Yeah, um we I We haven't been able to, so they didn't follow as easily as for the consistency and robustness. And it's really a byproduct of our algorithm. It's really kind of like we're fixing track to achieve this one guarantee, and then there's not like a clear way to smooth out both of them. Yeah, yeah, I know it happens in the chat. Good. And then finally, we also have experiments, and our algorithm perform on both in terms of differ value and fairness, whereas benchmark algorithm, they really compromise on one of these two objectives. And actually, our algorithm, in terms of fairness, we're actually relatively close to one over for the priority of selecting the best candidates for these experiments. Okay, and now I want to go. Okay, I now want to go over, present to you the algorithm that achieved this result. So let's start from the classic algorithm for the segregatory problem that doesn't use predictions, and this is mostly just going to be to introduce some notations. So we're going to have sigma t, which is going to be the index of the candidate who arrived at times some t. How this algorithm works is that the first one of vari functions of the candidate, we're just going to reject them. And then in the second phase, as soon as we have a candidate, As soon as we have a candidate who is the best so far, meaning that its value is at least the maximum value of all of the candidates who have previously arrived, then we select that candidate who has arrived. So now this is fair to select the best candidate authority one over E, but it's not smooth. And to see what we need to achieve smoothness, I first want to make two observations about some required conditions that we must meet to achieve smoothness. That we must meet to achieve smoothness. But the first scenario I want you to consider is: what if we have this best predicted candidate? So we look at all of the predictions, we see, okay, this is the person who's predicted to be the best candidate. It has not yet arrived, and the predictions so far have been perfectly accurate. Then what happens? What must we do in that case? We pick the current best thing. Best thing with the current best thing. So, here the main observation is that in this case, you can never stop. Actually, the reason why you can never stop is that the predictions are accurate so far. And so, if you achieve this one consistency, you must wait for the best predicted candidate to arrive. Now, the other case that the other observation, if you have the best predicted candidate who arrives and the predictions are accurate so far, then by a similar observation, you must stop to be able to achieve this one consistency. And so, here, already with this observation, you see that we're going to kind of. Ready with this observation, you see that we're going to kind of like depart from like some classical secretary algorithms where here it might be the case that even on the first candidate for the rise, we must have to stop. Okay, so this is going to be very different than these algorithms. The first candidate for the rise is always the best one in the finished. Sorry, not the best one, the best predicted one. So I look at the first one to arrive and I see if it's the best one. You see all of the predictions. That's the point. You see all the predictions in advance. Yes, exactly. Yes, exactly. Yeah. Yeah, is that clear? So we have the true value. Okay, sorry. I didn't realize we got to see all the predictions. Okay, sorry, yeah, that's super important. You get to see it. Yeah, yeah. So the other predicted values, you see them all initially at equal to zero, and then there's a true values that you observe one at a time. Okay, sorry about that. Yeah, question about the settings. So the original setting, you assume that the order of tennis is nine. You assume that the order of terminates is nice. Here, do you assume that the predictions are conditional order, or are they selected and then you run the last one? Yeah, good. So, no, the predictions cannot depend on the ordering. They have to be, they're selected beforehand. So, exactly. It's going to be an instance compared to, it consists of true values, vector values, and then we have a random order. Okay. Good. So now. So now, here I'm just adding these two conditions that we just observed in our algorithm. So now we have this idea condition about what happens with the best predicted candidates and when they arrive or when they haven't arrived. Now we still don't have smoothness with these two new conditions. Another reason is not going to be because of some issues with the best predicted candidates, but with the second benefiting candidates. So here it's a bit more subtle. So I want you to consider this following. So, I want you to consider this following instance. You're going to have two candidates who have very high predicted value, one slightly higher than the other. And then the true max, the candidate with the highest value is going to be the one that has the slightest lower predicted value. So, in this scenario, if the candidate with the highest predicted value arrives first, Predicted value arrives first. Then you're going to observe that their true value is actually lower than what was predicted, and it's lower than the predicted value of a candidate who will arrive later. So you will reject that candidate. And now if you just follow the algorithm as stated, what will happen is that you will roughly basically run the secretary, this classical secretary algorithm on the remaining candidates. And now, if the second highest, if the second highest remaining candidate arrives before this. Arrives before this in this first phase where you reject everyone, you will also reject it and then you wait later on and not select anyone and be stuck with one of these lower-value candidates. And so you won't have smoothness. So here the main takeaway I really want to give you about this example is that here, whenever you reject this first candidate because you observe that it's a second candidate who has highest value, you really have to make a note that this candidate has a really high value. I must stop on it whenever I see it. I must talk on it whenever I see it able to be able to achieve my smoothness care. Second price auction. Second price auction. So the second price auction, right, you're going to give the item to the highest value, right? Yeah, so here I guess you expect the true value of the list, then you will wait for the next one. Yeah, I hadn't thought about it that way, but that was interesting. Okay, so how are we going to do this idea? This is going to be kind of like the main idea of algorithm. It's called the predicting technique. The idea is that whenever you're going to reject the candidate who is the best predicted candidate, you're going to want to keep track of who are all of the candidates who are by the future who can still give you your smoothness guarantees. So, this is going to be the set of candidates who are going. Going to be the set of candidates who are going to arrive in the future, who can still provide to you your smoothest guarantee that you want. Now, once you have noted all of these candidates, what you're going to do is that if ever there's a candidate who arrives, and they're the last candidate to arrive in this set, then you're going to stop to make sure that you get your smoothest guarantee. Now, why wait for the last one? And the analysis to be able to actually fairness is going to be important that we wait until the last one to get more important to be able to actually fear. More time to be able to achieve fairness in between. So, now this is the final algorithm. So, here I've blurred out the whole part that I described before, and now we have this added set who are all of these candidates. If ever we reject the predicted best candidates, who are all of the future candidates who can provide to us our smoothness guarantee? If one of these candidates arrives and there's still several of them in the set, then we just remove them from that set and we continue. If the candidate is the last one in that set, then we Is the last one in that set, then we just select them in between them. Good. So for the K-secretary problem, I will briefly mention the results. So here, instead of saving a select a single candidate, we can select K candidates. The smoothness guarantee that we're going to want to achieve is that now, if we look at the sum of the values of the candidates that the algorithm selects, we're going to want to approximate the sum of the values. To approximate the sum of the k highest true values, where ri is the index of the highest highest value. And the fairness guarantees is that we're going to want that for each of the k true highest candidates, we're going to take them with some probability at least a five. Okay. Wow. So this one is a bit strange because if you ran the original algorithm, you would not select other candidates. Candidates. If you had the perfect predictions and you wanted the maximal candidate, you would not try to select a second. It was the previous algorithm? No, I mean, sort of, if I think of this as there is no problem of prediction, I'm just trying to do what's best, I would not try to sometimes select the second or the third best. Just try to select the best one. So, this is kind of deviates from the original objective, you know, even with full knowledge. Yeah, so here was full knowledge. Yeah, so here was full knowledge, you would just select the K highest ones. Yes. Oh, so you're saying in the K secretary property, you're already requiring to select no in advance that you go? Yeah, you're given a budget of K initially. You're given this budget of K. And this is how many you can select. Good. And what is the capital R in the group? Yeah, this is just the instance. This is the error on the given instance. Good. So, what is our result here? We still have constant smoothness. We have this ugly term. Let me explain it to you for fairness. If k is constant, this left-hand side what it's saying is that we get constant fairness. So, if k is constant, we still have this constant sweetness, constant fairness for the top K candidates. In general, what this right-hand side term in the fairness is going to tell you is that we can get constant fairness for most of the top K candidates. Fairness for most of the top K candidates. What do I mean by most of the top K candidates? If you look at anyone who is in the top one minus delta times K candidates for any constant delta, then this term will be constant, and so you will have constant fairness. So in general, if K is not constant, there might be a small fraction of the candidates here that we're not able to select. And just as a super quick note, actually, even in the setting with the segregator with With in the secretary problem without prediction. In this, for case factories, if you ask me, can you select the case highest candidate with some probability, I'm not even sure how I would, we don't even know if it's how we would go about selecting the case highest candidate with constant probability. Classical result of one of the E doesn't apply only with K. So there is a result here that can achieve an appreciation activity that's even one minus. An appreciation actually does even one minus one over the square root of k, but it depends on the expected value, right? And so the kth highest might not be selected with constant probability, basically. Okay, so just to conclude, we've argued that bias predictions can lead to some agreement predictions that have some outcomes that we consider unfair. We studied the notion of fairness for secondary predictions. We showed that we could achieve some fairness guarantee without any asymptotic arctic loss and the best known. And the asymptotic loss and the best known smoothness guarantee that you could have known was predictions. And so, for some future directions, here we just posted the secretary problems, but would be interesting to consider some other problems that have been studied in algorithmous predictions and consider fairness for these problems. And I think there's still some improvements that can be made for the case-secretary problem in terms of municipal fairness. Thank you. What happens if when you interview a candidate, you don't get the true V, but you get an approximation of this? It's kind of a little bit more valued flavor to it, but you don't get to see the exact value. Yeah, so I think that's definitely to capture hiring, that's a very natural assumption. Yeah, we haven't considered that selling. I think it would be very natural to explore. Yeah, Will, I don't know if you have anything. Yeah. Will I don't know if you have anything else to say about that setting? Yeah, you haven't explored it, yeah. Interesting to know. I guess you would have to define the relationship between the relationship between the approximation. Hopefully, if the errors are like uncorrelated or something, maybe you can say something right. Yeah, this is more just a comment that it's related to Shai's question. I think in algorithms with predictions, especially in these online With predictions, especially in these online studies. Like, people assume you just get the prediction upfront, but you can think of it as kind of like then you're running your ski rental, like ski, you're doing ski rental or whatever the online problem is. And you can almost imagine doing like a basic posterior update as you get more information about the exact instance, which you can update your with. But no one has looked at it as far as but I think it's really purchase. Yeah, yeah, yeah. Yeah, yeah. And you have simulations with the bandit problems with the algorithms of confidence bounds that you get. Another way to also relax this assumption that you give in all of the predictions up front, there's some kind of model, this model for algorithms predictions where at each time step you're only told an advice on what you should be doing basically. And now there's a higher higher, not higher. That would be the advice for this case. And now, what's also interesting with this model is that your prediction can kind of like adapt on the instance you've seen so far, or it can't be trained with the data you've seen so far. So, I think in the secondary problem, I don't think anything is possible if you just have a prediction initially about shape or not, because if somebody tells you how you're on the first step and they're really bad, then there's nothing as interesting. But there's some other problems for which this can be an interesting problem to go through. An interest in the problem to go through. Yeah, one thing, yeah, I mean, so a natural question should request to be a little bit more. I think right in half an hour ago. 