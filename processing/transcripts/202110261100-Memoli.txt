Hopefully towards the end I'll be able to tell you about one variant of the Chromo-Waspertzling distance that is amenable to, let's say, efficient polynomial time computations. And the main drive for this talk is the idea or the problem of comparing data sets. And in particular, And in particular, to quantify the degree to which two data sets are not the same. So that calls for defining a suitable notion of distance between data sets. So I would like to answer the question, how different is a triangle from a circle or a square from a cube, etc. And when I say how different, of course, I want to emphasize that we are after a notion of a quantitative notion of difference. And the applications of these sort of ideas are clear, I think, that they could be used for applications in clustering, classification, or visualization. So to again, to visualize the goal of the talk, so imagine that you have a bag of shapes and the shapes, well, they could encode different things like three-dimensional shapes, by the way, or perhaps. Three-dimensional states, by the way, or perhaps different languages or different documents, or maybe different data sets that come from bioinformatics. And then under this sought-after notion of distance between data sets, we want to be able to construct a distance matrix containing the difference between the different data sets. And so, therefore, we want a number. And we also want that different entries into the matrix satisfy the so-called triangle inequality. By a so-called triangle inequality or something close to the triangle inequality, which is a property useful for actually searching and processing the distance matrix. And here's what I was trying to motivate before that. Once you have this distance matrix, you can organize the data set in different ways. One of those ways of organizing the data set is to actually take the distance matrix and apply some hierarchical clustering algorithm. Clustering algorithm, and then obtain a dendram, and then you would be able to, for example, automatically detect that perhaps all triangles are closer to each other than they are to shapes that look like regular polygons with more sides and ellipses, perhaps, and then that the triangles are all quite different from three-dimensional shapes. So, we want the distance that we produce to be meaningful. And here's another application. And here's another application that is actually perhaps more closely related to the title of the talk. So imagine you have a fairly large data set, X, and assume the data set is so large that you cannot compute hierarchical clustering to the whole data set at once. So then one of the main ideas that come to mind is to actually utilize the so-called bootstrap idea. So you would sub-sample. So you would sub-sample from X. Suppose that X has cardinality maybe 20 million. So instead of processing the 20 million points all at once, you would perhaps sample 1 million subsets, each with 10,000 points, and then very quickly obtain a dendram being the result of hierarchical classing for each of these data sets with 10,000 points. So then you will be presented with a whole bag of data. With a whole bag of dendrams, and you would like to actually cluster those tendrams to detect what are the main clustering structures present in the big data set. So, for that, you would need some notion of distance between dendrograms. All right. So, at any rate, so the big question is, what is a data set right now? So, I tried to motivate the problem of coming up with the notion of distance between shapes, but then I also said that shapes Then I also said that shapes may represent data sets, but let me be more precise about what is my definition of a data set. Well, one basic idea that is very prevalent in different applications is an idea that a data set should be a point cloud modular isometry. So you don't really care the location of a point cloud in space. What you care is about the intrinsic shape of the point cloud. And this is, for example, standard in chemistry. So therefore, So, therefore, we are then led to model a dataset as a metric space. And then, two data sets, X and Y, will be the same, will be declared to be the same if they're isometric as metric spaces. And one possibility for measuring distance between data sets, which we are now modeling as metric spaces, is to use a so-called Gromov House of Distance, DGH. So, all right, so then so far we have concocted the following environment. We have a notion of distance, which is a Gromofasso distance, and the collection of all data sets is a collection of all compact metric spaces, and this distance is a Gromo-Faso distance. So, I want to recall the definition of Gromof-Hauser distance, which is useful for later introducing the Gromov-Wasserstein distance. There are two main definitions. There are two main definitions for the Chromo house of this sense, which turn out to be equivalent a posteriori. But here is the original definition. So suppose you have two compact metric spaces, X and Y. And imagine you can find a third metric space, Z, which is sufficiently rich so that you can find an isometric copy of X inside Z and you can also find And you can also find an isometric copy of Y inside of Z. So now you have two subsets: this copy of X inside of Z and this copy of Y inside of Z. Because the two subsets live inside the same ambient space Z, I can measure, I can compute the household distance between the subsets. All right, but I introduced some choices. There may be many Z, and for a given Z, there may be many different ways in which you can. Be many different ways in which you can fit x into z denoted by the isometric embedding yotta x. So, Gromo-Fausor distance is defined by infinizing this quantity under all possible choices of isometric embeddings and ambient space Z. There's a second definition that is more computationally appealing. So, and I call it for. And I call the first definition the extrinsic one, and the second definition will be the intrinsic one. In the second definition, the main object is a correspondent. So, given two metric spaces, x and y, a correspondence between them is any subset of the Cartesian product between x and y, so that when you forget all the y coordinates, you obtain x. And when you forget all the x coordinates, namely when you All the x coordinates, namely when you project onto the second factor, you obtain the whole space y. In other words, r is a non-empty surjective relation between x and y. Given any relation r, in this case any correspondence, the distortion of this correspondence is defined in the following manner. So pick two different pairs in the correspondence, xy and x prime, y prime. Then Then out of these two pairs, consider the X entries and measure distance between them. And then do the same for the Y entries and measure the distance between them. So this is the distance between the X coordinates and this is the distance in the space Y between the corresponding Y points, Y coordinates. And then compare this distance to that one. And this is executed by this. Executed by this subtraction and computing the absolute value here. And then choose the worst possible pair of pairs. This is a distortion. It's a most pessimistic measurement of distortion or how much R the correspondence destroys the metric structure. And then one can define, it is natural to look for a correspondence that minimizes the distortion. That minimizes the distortion. And there's a one-half factor here. And in FIMUN times 1/2 will be the second, give rise to the second definition of chromophaus. Any questions so far? Okay. So then very early on, Carlton and Ovstrovsky pointed out that the two definitions give the same. The two definitions give the same numerical value, hence I'm going to just denote that by dgh. And even before then, using the first definition, Gromov and Peterson proved a number of different results. First of all, DGH is a real, you know, a proper, legitimate notion of distance between isometric classes of compact metric spaces. That means or that encapsulates the fact that Gromofa. Fact that Gromov also distance satisfies the triangular inequality, and furthermore, it vanishes if and only if the two spaces are isometric. The collection of all compact metric spaces when endowed with Gromo-Fausor distance is complete. And later on, more recently, it was proved that Gromo-Fausor space is actually space. Unfortunately, it's not true that, well, I mean, the infinitesimal structure of the space is actually not that nice. structure of the space is actually not that nice. There's not, as far as I know, no definite curvature that you can look for here. I mean, it's not an Alexandrop space with curvature of only zero. At any rate, the following picture hopes to somehow signal what's to come next. So please consider the left of the screen. So So the fundamental object that was used in the first definition of chromo-fausor was the notion of household distance between subsets of a given space. And then one can say that we gromovized the household distance to give rise to grammar household. Now let's look at the top here. I want to, I mean, let me say, claim that house of reasons can. Can be relaxed, structurally relaxed to give rise to the Wasserstein distance in the following way. Whereas the Hausdorff distance measures distance between closed subsets of a metric space, the Wasserstein distance measures distance between probability measures on that metric space. So if you forgot the distribution of mass and you only retained the support. The support of the measures, you will then be measuring something like the Hausoff distance. So, therefore, one obtains this analogy. So, Hausorf is to Wasserstein as Chromo-Hausorf is to something that we should call Chromo-Wasserstein. And the process that takes you from Hausorf to Wasserstein is a relaxation, and that can be made precise in some way. But anyway, so these sort of considerations is what led to seeking, originally seeking the definition of chromo-Wasserstein. Definition of Chromo-Wasserstein, which I will describe in a second. Now, Gromo-Wasserstein distance operates on more structured than just metric spaces. The input to Gromo-Wasterstein distance is a pair of metric measure spaces, which I will abbreviate by MM spaces. A metric measure space is a triple x dx mu x where x dx Where x dx is a compact metric space and μx is a Borel probability measure on X with full support. And M super W will denote the collection of all M spaces. For example, if you have any bounded, closed subset of R2, you can endow dx, you can endow X with a Euclidean distance. And if you make some Euclidean distance. And if you make some assumptions on how nice x is, you will have perhaps a restriction of the Lebesgue measuring R2, and you could re-normalize it and then obtain a probability measure mu x. So that's a way of producing a metric measure space. Slightly more structured example is that of a closed Riemannian manifold. On a Riemannian manifold, you have the GODC distance and you also have the volume measure, and the compactness. And the compactness assumption makes sure that the volume will be bounded, so you can re-normalize the volume measure and you will get a mu x. So these are two natural ways in which, mathematically speaking, you can give rise to metric measure spaces. In practice, I recently encountered some applications in which some colleagues have looked at different documents or written in different languages, and then documents themselves become Become metric measure spaces. And there's a certain cosine dissimilarity between words, and there's a way of assigning weights to the different words. And you could imagine that these weights arise from considering how often the words are used in a given document. And here's going back to the mathematical aspect, here's a family of metric measure spaces. A family of metric measure spaces, which is fully simple but plays a role in the study of the theoretical properties of the Gromo-Basserstim distance. And the family will be denoted by delta n and will be a natural number. So delta one will be the metric Measure space consisting of a single point. So that's a set with one point. That one point is denoted by an asterisk. If I have a single point, then I have no choice on the metric. The metric, the distance has to be. metric the distance has to be will contain only one one entry and it has to be zero and then i also have no choice for a probability measure it has to be the delta supported at that point now the second element in that sequence will be delta two and i define delta two to consist of two different points at distance one and each with weight say with mass one half and then i have delta three in the same way all the weights have delta three in the same way all the weights are one-third and all distances are one and so on so forth i have delta n and i will say that that that will consist of endpoints with the discrete metric and uniform probability measure and of course this family that i defined belongs to the collection of all metric measure spaces okay now uh i want to uh now um um specify what is the notion of equality between uh mm spaces Between MM spaces. And I will say that two MM spaces X and Y are isomorphic, and I'll denote that by this symbol. If, as metric spaces, they are isometric. And one of such isometries, there may be many, but I will require that one exists so that when I push forward the probability measure on X, I recover the probability measure on Y. So namely, So, namely, two MN spaces will be declared isomorphic if I can find a measure-preserving isometry between them. Let me give you another example. So, consider the following two MM spaces, X and Y. X consists of two points at distance one, but now I will say that the weights will be different. So, the weight attached to one point will be one. attached to one point will be one fourth and the weight attached to the remaining point will have to be three fourth and then y will be my delta two so the weights are one half one half i have only two points and distance is one of course if i forget the the masses the two spaces are isometric but there's no isometry that preserves the weights so hence the the two spaces are uh non-isomorphic as metric metric measure spaces so um again So again, I want to reiterate that in the setting of MH phases, isomorphism requires the existence of measure-preserving isometries. So there are two different versions of the Gromov-Baserflag distance. There's one that was constructed by Sturm in 2005 in the context of synthetic geometry. And that first version arises from a construction very similar to A construction very similar to the first definition of Chroma-Fausorf. Remember that in the first definition of Chroma-Fausorf, we started from X and Y as abstract metric spaces. Then we imagined the existence of a third metric space admitting isometric embeddings of both X and Y. Inside this third metric space, we computed the house of distance between the isometric embeddings of X and Y. Embeddings of X and Y, and then we infinized over the choices that we made. Now, in the context of MM spaces, we have more information than simply metric spaces. So we also have probability measures sitting on top of the metric spaces. So in this example, I have a mu x that lives here, and I also have a mu y that lives here. But the same structure, the same strategy works. I mean, when I say that it works, it works in the say that it works it works in the sense that it allows uh allows you to define a notion of chromov asterisk and distance so again choose any z admitting isometric embeddings of x and y and pick some isometric embeddings yota x and yota y now push forward the measure mu x under yota x into y and push forward the measure mu y under yota y and now you obtain two different probability measures one on x and one on y One on X and one on Y. But when you have two different probability measures inside the same space, you have Wasterstein distance. And that's exactly what Sturm proposed. So compute the Wasterstein distance of the push forwards into Z, and then eliminate the choices you made. A second construction is one that I explored a few years back, in which instead of emulating the first definition of chromo-fausors, I emulated the second one. I emulated the second one in which the main player was a notion of correspondence between metric spaces. So now the key idea, which I will dive into in a few minutes, is that we don't want to couple, we don't want to put spaces into correspondence via set-theoretic objects, namely the correspondences that appear here. Instead, we are going to consider couplings between the measure information control. The measure information contained in the MM spaces. And then we're going to generalize the notion of distortion and define something called the p-distortion of a measure coupling. And it will turn out to be true that the two distances are actually organized in this way. Sturm's version is an upper bound for the version explored here in item two, and they're actually different. So there will be no Karlton-O-Strovsky type of theorem. type of theorem. Any questions so far? Okay, so I want to make some comments. First of all, for the sequence delta n that I explained before as an example of MM spaces, it's not difficult to prove that, which I should put maybe an approximate equality here, that for NN quality here that for n and m different stun's version of the chromo-wasterstein distance is always bounded below by one for every possible p and the other version of chromo-basterstein will be upper bounded by something that decays to zero with n and m these two things can be solved relatively easily but there's not no time for me to show you the calculations here at any rate so what this tells us is that Any rate, so what this tells us is that there can be no analog to the theorem that is true in the case of Chromoff-Hauser business. There can be no equality because this one goes to zero and this one remains bounded below by a fixed constant for every N and M. And the other comment that I want to make is that one of the motivations that I had for exploring something beyond the construction I identified by Shtum is that a priori, computing A priori, computing Stum's version looks much more onerous than computing the other version, the second version. And I want to try and explain why. So here we will be infimizing over the choice of a coupling between the two spaces. And then there's going to be some quantity that depends on the coupling, a functional, and we're going to try to minimize the functional under the choice of a coupling. Functional under the choice of a coupling. If you have some training in engineering, you can maybe quickly get a sense for grading descent here, right? I mean, you smell grading descent, so we should go after that. And that's exactly what I thought when I'm thinking about this. In the case of Stoom's version, you're infimizing a fully general collection of objects, all possible metric spaces inside which you can embed X and Y. And of course, that looks really scary a priori. Scary a priori. All right, so to summarize, up to now, a data set is a metric measure space, and that's exactly a triple x dx mu x, where x dx is a compact metric space, and mu x is a Borel probability measure with full support. In practice, in data analysis, the first part amounts to a distance matrix, and the second part amounts... Matrix and the second part amounts to a vector of weights so that all the sounds absolutely lost. Okunda, we're not hearing you. Okunda, I think we lost him. I think he's disconnected actually. Yeah, I think he's disconnected. So the screen is frozen. I guess we should wait a couple of minutes to see if you. We should wait a couple of minutes, yeah. I think he's disconnected. Yeah, I thought it was my audio, but I think it's everyone's. He still even looks like he looks like he's online, actually, but I'm not sure. That's always a problem when we are doing things like online. You never know when your internet connection actually disconnected somehow. Well, he's online actually according to this screen. Let's wait for for a few m a few minutes. Okay, I've sent him a message, maybe. Yeah, I sent him a message actually. I mean, it doesn't look, I mean, it looks like he's on my sent him a few messages. Can you send him an email message? I think you have his email. I send him a message here on the chat. I'll try to see that. I'll try it and let me see. Yeah, the screen is frozen. Anyway, it looks beautiful. The material looks beautiful. This was a gromb-versein distance. It's beautiful. And we have like a lot of Vasastein actually. Just now we had a discussion about the Fischeral matrix. About the fishery magic and the Pasastai, and now we have the Gromo-Persastine distance. Fakuto actually has been working a lot on this grommohaused off distance. I mean, he did a lot of applications as well. In computer vision, in computer graphics. I wonder how computational is, so he said, like, it's actually the kind of the first definition, this term definition is intensive as computation. Is intensive as computationally intensive. I wonder what the second. So, but I think the claim is actually the second definition, it's actually much more amendable to computation. I think if we have questions from the audience, we can actually discuss something now. You can discuss now, actually. What do you think about the talk so far? I mean, to me, it's fascinating. It looks beautiful. Beautiful. I think it's great. We've been working on different kinds of metric measure spaces with curvature bounds for a while. And yeah, being able to tell them apart and see if they're isomorphic in different ways is always great. And making this computationally tractable is even better. So yeah, I think a question that I kind of thought about is how does this quantization procedure that is used sometimes in probability In probability, applied to these kinds of spaces because, in you know, in certain natural spaces in geometry, like smooth manifolds or orbifolds or Alexandrov spaces with a good atlas, you could also try to quantify the measure and approximate it by a discrete set, look at the nerve of a manifold or something like that, and then try to articulate. How long was I gone for? I mean, I just saw a message. For I mean, I just saw a message saying that for like I kept talking and I didn't receive any notification until 10 seconds ago. Oh, okay. So I was taking notes, so I can tell you it was when you were giving the definition of compact metric space when you had x dx at mu x. Okay. We were discussing like the sperm definition, you're like the versus like the memory definition. And you were going on like to the next slide. Sorry. Next slide. Sorry. I mean, it's funny because you received no notification from Zoom that you were disconnected, so I kept they need to work on that, actually. They need to notify people. Yeah. Here, you can go back and we can tell you where we lost you. It was kind of there. Yeah, I think it's before. The next one. Yeah, yeah, yeah. The next one. The next one. Yeah. The next slide. Yeah, the next slide. The next slide, yeah. The next slide. All right, so all right. I mean, I was saying that there are two different definitions. Yeah, yeah, yeah. One emulates the first definition of chromo-fausor, the other one emulates the second definition of chromofausorf. They are ordered by inequality, but they are not equal. Yeah. Okay. All right. And now I'm going to. You didn't hear these comments, right? This one, yes, we have actually this one, yes. I think we can start for the next one. I think we can start for the next one, right? Yeah, that's the next one. Yeah. The next one. Okay. All right. So now we're going to give you details about the construction of the Chromo-Wasterstein descent in the sense that I defined it. I'm going to just concentrate on that one. And again, the data, I mean, the information that we receive is two different MM spaces, and that's two triples, X, DX, mu X. And in practice, in data analysis, it would be useful if someone leaves the microphone. Leaves the microphone open so that I can at least know that I'm connected. So if I hear a noise, I know that that would be okay. So in the sense of data analysis, x, dx, the metric part of the data set is just a distance matrix. And mu x is a collection of weights, namely a vector containing one entry per point in your data set. And all the entries into the vector are positive and they all add up to one. So this is the landscape we have. So, this is the landscape we have: a collection of all data sets, and then given any two data sets, you can measure distance between them. There's a reference data set, the one-point data set, so that if you were to compute this distance, that would tell you something like how big or how much information is contained in that data set. Anyway, so MW is a collection of whole data sets. All right, so the main object. The main object that appears in the definition of Gromo-Wasserstein is the notion of a coupling. So, if you have two different data sets X and Y, in particular, you have for X a coupling mu X and for Y a coupling mu Y. So, you can consider all the probability measures on the Cartesian product between X and Y, all the probability measures mu, which push forward onto mu X and mu Y via the coordinate projection maps. So, namely, maps. So namely mu has marginals mu x and mu y. And let me jump straight into the notion of distortion. So given a coupling, we want to associate to the coupling, mu in this case, a quality, a measure of quality for that coupling. So for every p boundable by one, we define the p distortion of the coupling in the following way. So the distortion p of The distortion P of μ will be the average difference of distances as observed as witnessed by the coupling. So, you should think that you will pick two different independent copies of xy and x prime y prime according to this law. And then you will compute the expectation of the pth power of the absolute difference between dx and dy. And then, of course, you will take the one over p power. All right, so in a more same In a more mathematical way, I mean, as a formula, the distortion, the p-distortion of the coupling mu is the following. So, you will integrate over x cross y cross x cross y, and you will use two different copies of mu for that interval. So, you will have variables x, y, x prime, and y prime. So, then you're going to look at the variables x and x prime and calculate the distance and the ground distance between. Ground distance between those two points on x, and you'll do the same for y. You'll compute absolute value to the power p and then carry out the integral and then take the one over p power. That's a p distortion of the coupling mu. And then Gromo-Wasterstein distance is defined as one half the minimum and it's actually a true minimum in general of over all possible couplings between mu x mu y of the p distortion of mu. So one wants to find. So, one wants to find the best possible coupling. Here's one example. So, when the second data set is a one-point data set, the distance between x, any x, and that data set will be the integral of the pth power of the distance between points relative to two copies of mu x. So imagine what happens when p goes to infinity. You will obtain just the maximum distance between any two points. So this is some sort of relaxation of an oxygen diameter, and we call it the p diameter of x. And it is an exercise to, I mean, a simple exercise to verify this formula. And it all boils down to the fact that there's a unique coupling between any probability measure and a delta. All right. All right, so let me jump ahead because I lost some time. All right, so here are the main results regarding the Gromov-Wasserstein distance. So for every P boundable by one, including P equals infinity in this case, the Gromo-Wasserstein distance P is a true notion of distance on the collection of all data sets, metric measure spaces, modulo isomorphism, namely it isometric. Isomorphism, namely, it is symmetric, it vanishes if and only if the two spaces are isomorphic and it satisfies the triangle inequality. But more is true. Sturm established that, as a matter of fact, the chromo-based distance is an intrinsic distance, namely you can find geodesics whose length realizes the distance itself. And then for the case when p is equal to 2, the completion of the collection. The completion of the collection of all MM spaces is an Alexandro space with curvature bound below by one. And this is fairly interesting as a result because it may not be true that in general this is a manifold. Well, this is a manifold or is compatible with a manifold structure, but it's the next best thing in a sense. And this should allow you to contemplate things like exponential maps, heat flows, etc. Um, heat flows, etc. I think this is a relatively active area of research, namely, to exploit the nice structure that Turman covered. Okay, so all right, look at that. No need to rush. We have time, no need to rush. Okay, yeah, all right. Thank you. Well, we have time. We have time. Okay, you, I saw that I was only five minutes away from the intended time, but it's okay, it's okay, don't worry. All right, so then let me uh maybe. All right. So then let me maybe tell you one thing that you get essentially for free the moment that you establish a triangle inequality for the distance. As I mentioned before, the distance from any space to the one point space depends only on that on the space, on the space X, and it equals one half the diameter P of the space. So So, as I claimed in a slide, the distance satisfies the triangle inequality. So I will be able to say that this distance will be upper bounded by the sum of that distance and that distance. And furthermore, this distance will be lower bounded by the difference between this one and that one. This is my little maybe. This is my little maybe comment, one sentence comment that it pays off to have the triangle inequality because you can immediately write these sort of bounds that allow you to prove and obtain very quick and dirty, say, quantities that you can use to classify shapes. In this case, a diameter quantity. All right, so as I said before, well, I announced early on that And announced early on that the collection of all MN spaces with Kromov Asterstein is unfortunately not complete. Here's one way to see that. So remember the collection delta N of all metric measure spaces arising from endpoints with a discrete metric and uniform probability measure. So first of all, I think it should be First of all, I think it should be intuitively clear that there exists no good limit point for this inside mw. So what are the possible limit points? Let's see. So when n goes to infinity, you will have infinitely many points at distance one, infinitely but cantally many points at distance one with a uniform probability measure. Okay, immediately there are two problems here. The uniform probability measure, you have weights going to zero. Okay, maybe you can say fine, I'll try to deal with that. Say, fine, I'll try to deal with that. But what about compactness? I prescribe compactness for all the objects here. So, countably infinitely many points at distance one will not be compact. All right, so but then at the same time, I also mentioned that delta n and delta m are can the distance between them can be bounded in this way. So this means that we cannot have completeness. Cannot have completeness. Okay, this is a Cauchy sequence, anyway. But anyway, Sturm persisted with this and he worked with the completion and was able to prove the statement that I mentioned here. Now, I wanted to mention, to make a historical note here. The Gromov Astersein distance and Gromo-Fausor distance, they are somehow Of Hauser distance, they are somehow a blend between some classical ideas like the Vasserstein distance or the Hausor distance, and in particular case of the Chromo-Wasterstein distance of the Baserstein distance, of course, which arose in optimal transport. Maybe some ideas go all the way back to Mons maybe a couple hundred years. And then more recently, they were used in a different context, in the context of a so-called synthetic geometry by Gromu. synthetic geometry by Gromo in an attempt to Gromo van denum and Villani involved in an attempt to give rise to a notion of curvature for non-smooth spaces. So I wanted to now mention and actually signal that computing Romo-Basserstein is something you can attempt by a gradient descent, but essentially it leads to a non-convex Leads to a non-convex quadratic optimization problem. As I mentioned before, a data set in the finite setting, at least in the context of the way I model data sets, will be specified as distance matrix and vector with weight. And a coupling between two vectors will consist of a matrix. Will consist of a matrix. And this matrix will be constrained linearly. The matrix representing the capling will have to satisfy some linear constraints that come from the specification of marginals, mu x and mu y. So that's something that one should keep in mind. And then let's look at distortion, the thing that I'm trying to minimize. Distortion depends on the coupling mu in the following way. So you have to add over all the possible choices of four indices, the distance between xi and xk. between xi and xk and then the distance between j yj and yl multiplied by mu ij and mu kl without really trying to interpret what's going on here you can immediately see that distortion depends quadratically on the coupling right because the coupling appears multiplied by itself and it doesn't really matter what this is i mean i can't represent this as a a gamma with four A gamma with four indices, so some type of tensor. So I can actually unroll or say roll my indices in certain ways so that I can write this into some sort of bilinear form. I can represent mu, the matrix that specifies the coupling, as a vector. And then I can follow whatever ordering of the pairs of indices I chose to also represent this now instead as a four tensor, I can represent it as a two tensor, namely. As a two-tensor, namely a matrix, so that I get a bilinear form. So then the solving for Gromo-Baserstein one in this case leads to seeking the minimum of this quadratic form over all possible vectors that are linearly constrained in some specific way that arises from mu x and mu y. So hence the critical question here is, is it true that for some for any metric dx dy that you give me that this gamma Y that you give me that this gamma matrix will be positive semi-definite. And unfortunately, that's not true. So the type of problems that you end up having to solve, and let me rephrase that, the type of quadratic problems you need to solve will not be convex in general. At least you have gradient descent that you can apply, and that's one of the techniques that has been exploited often. And then there are some ideas coming from entropic radio. Coming from entropic regularization that accelerates these sorts of calculations. So, now from a theoretical perspective, and the pursuit is that we would still like to identify a notion of distance that, at least for some subclass of data sets, can be computed in polytime. And that leads to the pursuit or searching for lower bounds with the hope that it is easy to tweak the definition. It is easy to tweak the definition in some way to obtain some quantity that also depends on a pair of spaces, x and y, that in some cases will be easier to compute. Now, that's something that is an approach that has led to some interesting developments, but it's not the one that I want to talk about today. So today I want to do something a little bit different. And the particular Variant of Chromo-Wasserstein that appeared in my title is that of ultrametric Chromo-Wasserstein. So, that will require that we constrain ourselves to a sub-collection of metric measure spaces that I will call all the ultra-metric. Well, the metric part of the data set will actually satisfy a stronger constraint than simply a triangle inequality. So, we will end up defining a notion of distance that I. Finding a notion of distance that is tailored to this sub-collection. And I wanted to mention that this is a joint work with Axel Monk, Cheng Chao Wan, and Christophe Weitkand. And there's a paper on the archive with these ideas. So an ultrametric space is any metric space x dx which satisfies the so-called triangle. The so-called triangle, a strong triangle inequality. In the standard triangle inequality, you take a plus, but in the strong triangle inequality, you take a maximum. You require that when you go through an intermediate point, in this case, x double prime, the distance between x and x prime is bounded by the max of the two partial quantities. And one fact here is that ultrametric. One fact here is that ultrametric spaces are in a very definite sense equivalent to dendrograms. And dendrams are, of course, rooted trees. So they are very constrained in terms of structure. And this is a typical picture that you should keep in mind when you think of dendrames. And dendrams are typically produced by hierarchical plasma methods to explain, to represent the way in which one obtains different partitions of a given space by. Of a given space by specifying a certain spatial scale. So, recall the structure of the chromobus and distance. So, you are given X and Y, and then you seek a coupling, mu, that infimizes, which minimizes distortion. And there's a parameter P that is the one that you use for the power of the absolute value here. Of the absolute value here, and the starting point of the new variant is to actually recast this distortion inside a certain family of other distortion functions and then taking certain limits. So, first of all, notice that I could define a function lambda one from positive reals cross positive reals into, well say non-negative reals, non-negative reals into non-negative reals. Non-negative reals into non-negative reals, taking a per AB into the absolute value of the difference. If you do that, then the p-distortion can, of course, be written like this, where instead of writing this box here, I will write this. But of course, that's kind of silly, but in reality, what I have in mind is defining lambda q for every q bounded below by one. And I will define lambda q. Define lambda q as a generalization of this as the function taking a b as a pair into a to the q minus b to the q everything to the one over q and absolute value. And what am I doing here? Well, first I will define a pq distortion of a coupling where instead of using delta one I will use delta q here. And keep in mind what I'm doing, what is the effect of using delta q here, lambda q, I'm sorry. Here, lambda q, I'm sorry. Lambda q actually penalizes more heavily differences in distances that are large, right? Whereas lambda one doesn't really care where the difference in distances happens. A difference of distance five at any scale, spatial scale, is the same to this one. So, with this notion of PQ distortion, we could define a PQ. Define a PQ grom of asterisk indications if we wanted. And if you did that, you wouldn't gain much in a computational sense. The complexity is the same. You will still have to infimize something like this. You will still have that structure. Maybe gamma will depend on Q, and in general, you will not gain, you will not be able to make gamma have positive eigenvalues. But the thing we observe. But the thing we observed is that something interesting happens when you let both p and q go to infinity. And that's the complexity changes completely. And all of a sudden, you are able to identify a polytime algorithm for the calculation of the resulting distance. So notice that when q goes to infinity, well first recall the definition of lambda q of a v was a to the Q of AB was A to the Q minus B to the Q to the 1 over Q power. As Q goes to infinity, this defines the following distance. Lambda infinity, the following function Lambda infinity. Lambda infinity of AB will be zero if A is equal to B, but if not, it will be equal to the maximum between A and B. That is a critical yet simple observation. So imagine you start here. So imagine you start here with A and B, which are different, and then you calculate this number and you let Q go to infinity. What you recover is a maximum between A and B. So we can then define the ultrametric distortion of a coupling. This P alt, I retain the parameter P, but now instead of using delta one, as I originally did, I will use, sorry, lambda one, I will use Lambda infinity. I will use lambda infinity of the pair dx dy. And this will consist of my definition of the ultrametric Chrome of Asterstein of order P. And this is a P ultrametric Chromovasterstein. Now, this definition is actually, of course, structurally related to Chromov Acerstein, but it satisfies some different properties. So if I denote by UW the collection of all Denote by UW, a collection of all ultrametric measure spaces, then for every P between one and infinity, UGWP is a P metric on UW. So what is a P metric? Well, a one metric is just a metric or a distance function, but a P metric is any metric that satisfies this strengthened triangle inequality. So if you forget the P's, the exponents that appear here, this is just the triangle inequality. Just the triangle inequality, but I will now require that you know the triangle inequality holds even when I take a power p. And in particular, imagine what happens in this inequality when p goes to infinity. When p goes to infinity, the right-hand side will become a maximum between the two quantities. So an infinite metric space will be exactly an ultra-metric space. All right, so going back. All right, so going back to the claim number one, can you hear me? I just want to double-check because I haven't heard any. Yeah, I'm hearing you, but yeah, you need to hurry up now, though. Yeah, I'll be done in one minute. Yes, thanks. So then for every P between one and infinity, U, G, W, P will be a P metric, and this P metric will be an upper bound to the standard Chromobaserstein. And the main result is that when P is infinity, when you let the P parameter, the usual Chromo-Wasserstein parameter of The usual chromo-Wasserstein parameter or Wasserstein parameter goes infinity, this actually becomes an ultrametric on the collection of ultrametrics. And furthermore, this quantity can be computed in polytime when the input is finite. So hence, that allows me to actually realize this picture here. So if I have a whole bunch of different ultrametric spaces, well, dendorams, but as I said before, ultrametric spaces and dendrites are categorized. Ultrametric spaces and dendrams are categorically metrically equivalent, then there's a natural way of associating a dendram to the collection, and that dendrum, I mean, of will have some application in situations where you apply, say, bootstrapping in the context of hierarchical clustering. So in the joint paper with Monk1 and Veitkamp, we have some applications to the classification of phylogenetic trees arising from influenza. And I also want Influenza. And I also wanted to mention: perhaps I've noticed that in this group, there's some interest about Riemannian, quasi-Riemannian structures on collection of data sets. Stum has written extensively about it recently, and he identified a certain quasi-Riemannian structure on the collection of all finite metric metric spaces endowed with a p equals two version of. Was two versions of Chromova search time. All right, these are some references. And thank you. Thank you very much, Fakundo. Do we have questions from the audience? Somebody is raised. Mazir? Hello, and thank you very much for the very interesting talk. I have a question about the Torah. The theorem that you mentioned, S-term theorem on the lower positive bound for Alexandrov Kiracher. And you said that he basically uses a complete version of the space to prove that theorem, if I'm not wrong. That's correct. So is it possible? Because you said that this space is also a geodesic space, and we are talking about. And we are talking about the metric measure space. To have a weaker notion of curvature, like Ricci curvature, Laura found for the Ricci curvature. Can we talk about some other notions of curvature, like Olivia Ricci curvature, which is basically defined on metric measure spaces? And because it is also geodesic, we can just talk about Ricci curvature of closing of points in the space. Points in the space. All I can say is perhaps. I mean, I don't know for sure. But I mean, what you're saying sounds reasonable to me. Okay, thank you. As far as I know, that is not treated by Sturm in that paper that I'm referring to. Because I'm asking, because in that case, for the reaching curvature, we don't need completeness, we need a Polish metric space, basically. Space basically. And because it is a geodesical space, it makes everything much easier, I guess. June, please. I'm sorry. Thank you very much. Yeah, thank you. June, please. All right. Yeah, okay, thank you. Yeah, very nice talk. So I would like you to comment a little bit more on this distortion measure, which is a four-point function, right? So you have two points in X and two points. Right, so you have two points in X and two points in Y. So, in particular, if say you have the transport map, which is some kind of isometric, if you have some isometry for the transport map, then this distortion will be zero. So, in some way, it measures the distortion of non-isometry, right? So, I wonder, so yeah, you can use this form, but I wonder, just in general, can you say something about the general way of defining or you? Defining or using the distortion function because this can be very general, so you don't have to use this p-norm. Yeah, I'm not sure that. Let me say something, though. I mean, related to what you mentioned about maps. So, as an interpretation, suppose that you are in a situation where you have a map phi between X and Y with a property that when you push forward the map the the probability measure mu X under the map V, you obtain mu Y. Under the map v you obtain mu y okay so uh this will be actually something that gives you a mu. So let me call the following thing. Okay, and this will be a coupling between mu x and mu y. So this is a very particular and I would say benign situation in which the two parallel measures are such that you can find a transform map between them. And for any transform map, you can actually give rise to a couple. You can actually give rise to a coupling between the two probability measures. And I will call it μ sub phi. So then, an interesting question is: what is a p-distortion of this μ? And here's the claim. The p-distortion will be, well, instead of having to compute this integral over four factors, in reality, it will become an integral over two factors, two x factors. And it will be the distance in x, x prime minus distance in y of phi x. In y of phi x phi x prime to the power p and then you average that sure yeah with a one over p so that is uh one way to interpret uh the distortion and of course I mean this is um much closer to the standard intuition right that you you want to take some sort of average of the way distance as measured in X. Distance as measured in X is distorted when you try to measure it in Y by computing this distance. Is that satisfactory? So, I guess I want to push you to think, is it possible to define like non-like norm-like kind of a measurement of distortion? So, in particular, say, if I use a like, no, we can cook up like a KL divergence type of, you know, on D's, on D. So the D is just a positive real. Just a positive real, right? So you have essentially a difference between the two positive reals. So you, and you, so the integral, if I use something other than this p, p norm, I use, say, like, you know, like a KL divergence, you know, like quantity. Ah, I see. So it is fully general in that sense, yes. So there are colleagues who have looked at things that are, I mean, I mean, you can think that you have anything, right? Some x, x prime, y. Anything that depends as a function of data that we can call x, x prime and c prime, y, y prime. Right. You can use it in that level of generality. Yes. But I haven't. The optimal transport problem. The optimal transport definition of the problem is defined on a cost between X and Y. So you have some. y so you have some cost function which is like this x y and x x y prime y x prime y and x prime y prime so you have all these as given the cost function so i suppose this distortion and this cost somehow they interact in in some way you know just uh which cost uh sorry i didn't catch the cost so you have a cost function defined on x and y in the optimum transport case right you have a cost function a cost function transporting A cost from transporting some elements of x to elements of y. Yeah, you mean this one and that one, okay? Yeah, no, no, this would not be the cost. This is x to x prime, not x to, say, y. This is in the same space now. Oh, yeah, yeah, yeah. Your cost is from x to y. Okay, so I wanted to mention something. So, at priori, we don't know how to compare data on x with data on y. We don't have some alignment between. You know, some alignment between x as a space and y as a space because we don't have that alignment. We clearly don't have a cost for matching a point in x with a point in y. And as a way of circumvent that, we match pairs in x with pairs in y, or we compare the pairwise properties in x with pairwise properties in y. All right, thank you. Thank you. That answers my question. Thank you. Okay. Well, thank you very much. And I think I'm not quite sure whether we I think I'm not quite sure whether we have to stop now, actually, so we're already kind of past the time. But actually, David has a question. David, are you still there? Yes.