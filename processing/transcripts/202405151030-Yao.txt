Lighting a few things now. So, if after the talk you think I shouldn't do these things, keep there a part of useful ability. And Ching Yuan Chang is my long-time collaborator. And when I have a difficult problem I couldn't solve, I just asked him. And Ching Feng, she is a assistant professor at University of CENI now. She was a PhD student and postdo at LICE. And postdoc at ACE, and Pete McDonald was the postdoc of ERIC at NPM. Here's outline, I give a very brief background why we look into this. And the model we propose is kind of auto-reversible with all the M. And uh the goal is to try to reflect some well documented uh starlight feature. Starlight the feature in network data like transitivity, density dependence, and persistence, and so on. And we also spell out the relationship with the popular temporal random graph, exponential random graph models. We talk about stationality, then we go through two versions of maximum likelihood estimates. A second version, improved version, enjoy less steady. Produce version enjoy the standard parametrical rates, although the network parameter may not change. Another feature is the asymptotic distribution of memory is not necessarily where a process is mixing and well-behaved, then it is normally distributed. Then I give you one or two data to illustrate how to data. Illustrate how to dynamic networks. If you type in this word in uh Google, there are lots of thousands of paper pops out. And the early attempt is kind of evolution analysis of network snaps. You take a snapshot of network, try to see some feature how it evolves. And I think, yeah. I think uh yeah, of course what I put on the slides is very incomplete incomplete list of the reference, but uh there are a few people featured here. In terms of what we try to do with the dynamic network with dependent age, I think there are two popular approach. One is the conditional independent process, conditional are some latent process. Are some latent process. You just say, upload the network. The edge are independent given some latent process. Since the latent processes are dependent plus different edge, then you build this kind of dependence. Another popular approach is this temporal exponential random graph model. As we know, the exponential random graph model has some non-trivial difficulties. One is the normal. Once is the normalized constants depending on the parameter, which is difficult to handle. One way to get out of that is again using the conditional independence. You observe networks over time, conditional the history of network, the current network, all age are independent. The transition or B you follow some exponential uh distribution function. Then you get all those uh difficulties. Difficult things. We wrote a few papers using the AR network, but those are for the independent age. The nice thing about this AR framework is it's very simple. You can say a lot about how the process behaves, like a stationary and asymptotic normality of your estimate, and so on and so forth. And I think we started with something very We start with something very simple. You can build a structure on it. For example, you can put a random block model on that. You can also incorporate some change points to say the process over time changes. There is a more recent work on the block logistic AR model with independent edges. Independent ages. Estimation for all those, most of them are using some computational intensive assets, MC, MC or variational EM algorithms. So the goal of our approach is using an AR framework with dependent edges. Then we can model the stylized feature explicitly. And we also provide some theoretical. Also, provide some theoretical governance. This is our general framework. We are dealing with networks observed on the regular time intervals. We assume over the time there are P nodes which do not change. And what changes is the ages between nodes. So the X T is the adjacent node. Is the adjacence matrix at time t. This xijt is the status of h between nodes i and g, which takes value 1 or 0. For the simplicity of presentation, we assume it's undirected networks. That means the JSON matrix is symmetric and the man diagonal is zero. We don't allow We don't allow ourselves to for this talk. The key in our assumption of the model is this alpha and beta. Alpha is a transition probability to form a new H. That means at time t minus one, there is no H between nodes I and J. Next time, say H four. And this transition probability depends on the history of network, not only the states between nodes I and the G, and also other images. This is where the dependence comes from. But what we do assume critically, like what I mentioned in the previous literature, is conditional independence. Conditional on the history of networks, the agents at the time T. At the time t are independent of each other. So, therefore, we can specify this transition probability using i and beta. I phi is the probability from new age. We assume the dependence on the history only up to the lag M. This is where this ARM comes from. And beta is the probability to dissolve the existing H. At time t minus 1, says H. 1 says h, x prime says no h. Again, it depends on the m legs factor. So, with this assumption, because here we can specify this transition probability as we like to reflect, for example, transitivities. Once we have this notation, we can collectively write the conditional probability for node, for existing age between nodes. Existing H between nodes I and J at time T, given its M history, is in this form. Dependent alpha and beta and status the previous time. This is just collective representation of this two formula. What we have is we have a network, adjacent matrix process, which conditional on a past. past, HGR independent, and follow a bainounced distribution with a probability gamma ij t minus 1, which is dependent on alpha, beta, and the status xij and time t minus 1. I think this is pretty clear. It's very simple setup. Yeah, any questions? Yes. Yes. So how does it really depend on Xt minus 2, T minus 2? Depend on x t minus 2, t minus 3, through t minus m through this expression. Okay. I will be so basically say if the function defines a transition hold, that will be in the next slide or two. So you are always a few minutes quicker than speaker. Thank you. Okay. I'll give you an example. This is transitivity model. For transitivity model, we specify the model. For transitivity model, we specify the transition probability alpha igt depends on cosai i. Those are the nodes' heterogeneity parameters. Some nodes have more age, then the cosai i will be big. Others have fewer ages, so the cosine i will be small. Then, the probability function is defined in terms of two functions u. In terms of two functions uijt minus one and vijt minus zero. The uijt minus one is basically the number of common friends of nodes i and j at time t minus one. Because this will only be one if both of them are working. So this is what we typically see on Facebook or in PE. They have a documentation. You and someone else have a You and someone else have a common friend, so you should become a friend. This is just uijt is the common friend of i and j at t minus 1. We expect when uij t minus 1 is big, then probability to form a new age. The vijt minus 1 can be seen as a kind of distance measures because this term will be 1 if be one if k is friend of i but not friend of g. So this is distance measure. So if this is big then the probability to form a new age measure is small. Very intuitive. The formulate transition probability in this way. The beta IJ is a probability to dissolve an existing edge, so which should Existing H, so which should be impacted positively by V and negatively by U. So write it this way. And eta by IJ, again, not heterogeneity parameters. Questions? So you may think, why are we using the same AB here? Yeah, you can use them differently. The truth is, when you estimate the conditional probability, Estimate the conditional probability, especially when network is sparse, you don't have so many ages. Therefore, to estimate the conditional probability from edge to low edge is difficult. So, if you make using different AB here, then estimation for the A D here is much, much harder than here. So, by using the different heterogeneity parameters, Heterogeneity parameter cassava and beta, we hope there are enough variations in the formula. Okay, so this is the transitivity model. Now I'll have answered your question. Yeah? Density dependence model basically say if these nodes have more ages, then it tends to form new edges. New edges. If those nodes have few ages, or networks have few ages, then the probability to dissolve existing edge is also magical. So the same way we formulate it as a kind of transit unit. Just use your intuition. And again, we allow this note, heterogeneity parameters. Probably now, let me go back. One thing here is One thing here is the parameter AB is what we call a global parameter, which related to all pairs. For parameters like casi i is only related to the h connected to the note i. This is the local parameters. We have to deal with them separately, as they have different convergence rates as well. We have two. And cassa, eta are local parameters and all other parameters are mobile parameters. And what we formulate is a little bit convoluted, but what we have is this is like a network density. We take out the pi and g because we also calculate the node i's density, how many relative pages here. And so this is what we call a density dependent model. What we call a density-dependent model. And the persistent model said if network continues to be in one state, then it tends to be stayed in this state for longer. So this is kind of a obsolete. Okay. The connection with the temporal exponential random graph quadrants say especially in the They are, especially like Schweinberg and his cold wrote quite a few papers based on the extensive research of the exponential random graph model. Basic conclusion is it's difficult to deal with that. And more than that, typically it generates networks which do not represent what we see in the most practical situation. But if we impose a conditional independent Impose a conditional independence. In the context of temporal exponential or random run, all this problem disappear. You don't have a normalized constant. You don't have this kind of degenerated network on this. So if we use the conditional independence, then the jury is a special case of ARM. It can be But it can be written in this form. There is another which related to Peter's talk earlier. They call the separable term, which just says the conditional probability can be separated in the time domain with spatial domain. In that case, if we impose a conditional independence on age, then again, it's a special case of AI. It's a special case of ARM. Okay, station nine. Unlike the AR network we proposed earlier, the behavior of a process is very clearly identifiable. And for this one, it's unclear, but then we figure out actually. But then we figure out actually there's a quick and easy solution. So if we write alpha beta in this form, it's a function of the past status of network and governed by a parameter. And f and g itself are homogeneous functions in the sense they do not depend on time t. So, we assume for the simplicity m equal to 1 L1 and alpha, sorry, alpha beta and F and G are strictly between 0 and 1. So, this is irreducible homogeneous marker chain. With how many status? 2, 2p times p minus 1 divided by 2 states. But, nevertheless, it's But, nevertheless, it's a very simple and well-behaved microchain. By the fundamental theory for microchain, the process implied is stationary and ergodic. This is only holds for fixed period. Thus ergot ergodicity means if you take average in time, it converges the average in space. But In space. But there are so many states that naturally you would think if you take the average in time, it may take a long time for processes to converge. But luckily, what we look at is not really this p-time p process, and typically it's kind of summary statistics of this XT. And we did a lot of simulation, it converges pretty fast. Yeah. But nevertheless, Yeah. But nevertheless, when you do the asymptotic analysis, we typically allow P diverging together with the N. N is the number of P over the mass of time series. Therefore, in this case, the sample mean may no longer converge to the expectation. So, therefore, we do not using Do not use this thisity in our analysis. I think in the past I got quite a few statements. I was not so careful from some of you in the audience. But this time we did a little careful. When P and N converge together, we don't have error dist. We don't have the ball management. We need to perform careful analysis. So, what it says is the stationality is not asymptotic. Yeah? But Yeah, but so if the error got this it is. Yeah, when p is fixed, everything is fine, when p is not, then you need both paths. Okay, somehow in the network literature, I have an impression when you have a dependent net age, things become more difficult. A lot of network literature pursues age. And also in this case, we have a divergent network of parameters, like this not heterogeneity parameters that divergent. But what we do is we have this condition of independence. This really makes things much, much easier. Based on that, we can build a logical difference and which enable the asymptotic of analysis. I mentioned already. I mentioned already, the limiting distribution of MIE are not necessarily normal, but typically the process is well-behaved. Lower dynamic holes. Okay, I wrote it already. There are two sets of parameters. One is called local, one is called a global. But in general, we can collect all parameters. We can collect all parameters of C tau 1 to C tau Q. Yes, sorry. So I'm just wondering about how the E growing, and are you envisioning a network where people are sort of coming into the network and remodeling that process or something else? Because I'm not as sophisticated, at least not yet. So what you see is typically you have observation of network data. Observation of network data, the n is smaller than G. So, this is if you want to find the asymptotic approximation. Okay, clearly. So, you were talking about a series of different data analyses, each with a different N and a different P, and the N and P are chrome. Yeah. Looking at that because you're not, like, you're sort of seeing that work over time, and that over time, the, well I guess sort of some. I know what you mean. Actually, I took an even simpler view. I have a given network data set with fixed P, fixed N. N is smaller than P. Therefore, if you want to find a pro estimation, okay. Okay, we line up the parameters as theta one to theta q. And we collect all global parameters into a set of curly G. So this is the L'Paramet, sorry, the indices of global parameters. That means all this gamma by JF, which is conditional connection probability, is evolved in this parameter. Therefore, the complementary set of this G. Set of this G is the local parameter and global parameters. The key assumption is that we assume the number of global parameters is fixed. Like in our transitivity model, we only have two A and B. But the local parameter may diverge because together there are two P local parameters. The trouble with local parameter is since, like I said, when you write the log vector function, you sum over for all ij pairs. They are p times p minus 1 divided by 2, such pairs. So if the local parameter only relates to one node, so the likelihood function becomes insensitive to this local one. If we use the whole likelihood to do the estimate. The whole likelihood of particular estimation, US software. The rates is slow. So, therefore, we define a local parameter that only collects those IJA pairs related to the L's parameter. This is called a curly SL. So, this is a partial like partial log like. And then you build a future chain, take the conditional expectation. The conditional expectation, conditional for the term t, conditional are its past, past. Then this minus this is a partial sum of the micro conditions. So this is how we learn asthmatic analysis. Okay, initial estimate for the global parameters, although we like, say we define the partial like partial likelihood is a The partial electric hold isn't the global electric hold. So you do an estimation, you even estimate all the local parameters, but you only take those components, which is local parameters out, as your estimate for the local parameters. For the local parameters, you're using each partial local likelihood function to estimate everything, but you only take else estimate from this local likelihood. Estimate from this local actor, this form level. So, this is our initial estimate. Convergence rates, which is not fast, for example, for the three model, we look at the transitivity model. There are two P local parameters and two are global parameters. The rates for the global parameters, which is a finite number, Which is a finite number, is this. The length for the local parameter is much, much slower. Therefore, to have a consistency for the local parameter, you need a log P to be a small order of square root of n. But to have consistency for the local parameter, you need to do p to be much smaller. Okay. Improve the estimate. The idea is like to when The idea is like to when we just talked about estimate one component of parameter. Then we treat all other parameters like a nuisance parameter. Then to project the score function. So this is partial electrical function. And how much time do I have? Five minutes. Okay, that's good. Okay, that's good. Um this appears uh but I think you should have done line that I finished though. Mind I finished early. This is a score function. You take a derivative of the log life function. Then the score function can be written in this form. What you do is you do a projection. Project if you estimate, the goal is to estimate the else component of parameter. Then you do a projection in the direction. In the direction which matters for this IELTS component in this bit, kind of complicated optimalization problem. Then, once you have this projected score function, you find a solution. That's an estimate for the else component. So, what do you do? This is projected score function. You fix all other components at initial estimate to estimate this one. Estimate this one, then do it again. Do it again. The nice thing about this procedure is for each component, you have a finite parameter convergence rate. Because this is network data, like I said, if it's a global parameter, the total number of observations you have done observed networks, and each network have a P times P minus 1 divided by 2 pay-offs data. So the total observation. So the total observation is in the order of n times g squared. The standard rate is square root of g squared times n. That's it. A local parameter is the same thing. But the price to pay for that is there are three tuning parameters which specify how to choose the tuning parameter. Thank you. Okay. Under some regularity conditions, you can prove this improved estimate for the L component is asymptotically converged to this quantity. That is a standard normal distribution, and capital L is the probability limit of this partial sum. This is related to the score function. And if this reduces to a constant, then it's asymptotically normal. And when the round-run covariance matrix converges, then we have this couple that converges to this constant. So the condition to require that is basically the low-larger number codes for the process when n and p goes to infinity. If you really want to have an asymptotic normality, you can normalize it with some random terms. Then, this is based on the results in the book by Hall and Hardy. You get the asymptotic normality, but the term normalized constant here is stochastic. Okay. Let me Let me show you one real data analysis. This is the email interaction with a media-sized Polish manufacturer company in January to September 2010, which was analyzed in the paper here. We take 106 participants out of original 106. Out original 1006 or 70, because some of them just have one or two emails. That means if you put them into networks, they are hardly connected to each other. So we divided this time period into 39 weeks and XIJT equal to 1 if participants I and J exchanged at least one email during T. So this is how. So, this is how we define at all. To gain some insight, let's see some summary statistics first. So, this is a plot of agent density, key. For each time key, you just count how many agents here and divided by the total number. So, this is called agent density. We have 39 weeks. Clearly, you can see something happening here. The first regime and second. The first mutine and second the mutine we have. And this is the density of forming new H or dissolved existing H, like say this D02, which is at time T minus 1, there is H, at time T, there is no H. Then you divide it on total numbers. So again, you can see this blue line is the density of forming a new edge. Forming a new age, red line is the density of dissolving an existing age. Again, there are two regimes. First, 13 points, the last 26 points should be treated differently. Okay, let's recall the transitivity model. There are two quantities which reflect the transitivity curve, U and V. U and V. And I said UIJ T is at time t, the number of common threads of I and V. And V I J T is like a distance measure between I and G and time. So we define a U L L is the integer. The set of a collection of triples I G T set I G T, that is at time T minus 1, there is no H between I and G. And I and J at time T minus 1 have L common friends. So, therefore, if L is large, it's more likely next time there will be H between X, up between X, up between nodes R and H. So, this is U L. So, this is UL. We define a subset UL suplex 1. Say among all those triples at time t, z is the h. So-called transitivity means the cardinality of ul1 divided by that of ul increase when l decreases. That means more friends, you have. That means more friends you have, more likely you have a new venture. So, this is kind of summary statistics we're trying to see. In the same way, we define this L is I J T and the distance between I and J at time T minus 1 is the L. At time T minus 1 is a is the H. Then we count how many of them are H dissolved in X time. That's the U. next time. That's a u that's a V L z. So again, we expect that this ratio increase as L increases. Really say this transitivity behavior in the technique. So this is what you see. This is the plot of the ratio U L1, the cardinality of UL1 divided by that of UL against the column. So as you can see, So, as you can see, indeed, this increased size. For this data set, if you have common friends, you have some email, and you have common friends in a few minutes' time, it's more like a common friend. The size of the dots indicates the sample size in each of these sites. And this is the dot of V, this ratio. This ratio against AL, we also expect it's increasing. It's increasing, but it's not super linear. And backwards? This is the plot. This is estimated for period one, first 13 data column. First 13 data points. The A is a coefficient in front of the U and B is a coefficient in front of U. So as you can see, there's a negative correlation between notes, heterogeneity, cassai, against item I. That means employees who tend to grow new. Employees who tend to grow new age also tend to maintain a disease. Someone who is active tends to be always active in teleconnection. This is the period two. The transitivity effect is stronger because this A is bigger. And the dots, dots have a meaning, a circle. Has a meaning. The circle are size, the color according to the hierarchical level in the company. The smallest, the black circle, are the ordinary work in the company. And the manager, the circle is bigger, and this is the CEO. And what you can see is that there are more for the estimated nodes, title genes. Node heterogeneity parameter cosai, which is the propensity for this node to grow new age. This for manager, the mean value is 0.68, for ordinary work is 0.2. So manager tend to more accurately email, but this does not transform to the high level and like the CEO, probably CEO, only talk to a few people. Obviously, you'll only talk to a few people. Comparison with other models using AIC and PIC, we compare it with the AR model. This is a simple AR network model. Say the transition probability from L H to H is a constant alpha. And uh this is all you can do is uh H wise. Let alpha beta be L. Let alpha beta depend on I mean those are two, A R and single A. Or edgewise mean model, you just assume those are IID by only distribution with probably PIG. Or say the PIG is controlled by the nodes of VG. So there are four candidate models and there are no AG dependent models for those four models are all T-dependent. Four models are all independent features. And there is no dynamic dependency in those two models. The number of parameters differs quite a lot. First model only has two, then second is p times p minus one, and then half of that. And last model has parameters. And the AR transitivity model we try to compare with has 2p plus 2. So this is what So, this is what you see for pre-liter one, the first 13 data points, according to both AIC and BIC, the transitivity parameter. And for period two, those last 26 data points, the BIC pickup, the transitivity model, and AIC, the transitivity model, is second best after the HYC medium model. I'm happy to stop here. Because the second example is just repeat. That can be done to that. Can you do prediction? Oh. I do have prediction. I do have predictions. You can finish your part. Yes, prediction. We do the period too. There are 26 networks here. So we use first and train data points to fit in the model. Then try to do one step, two step, and three step. And based on fit in the model, Based on the fitted model, then, yeah, okay. So the combined results is presented in this ROC curve. There are three curves. Here is the step one prediction, this step two prediction, this step three prediction. And what we have is a transitivity model which is this curve. Not the best, third best, and in all three cases. All three cases. Then global AR model and HWIS AR model, degree mean model and HWIS mean model, and the previous stages. So, for prediction, so over parametrized model better than this transitivity, which is somehow positive, but probably the AR structure doesn't AR structure doesn't completely catch the dynamics there. So therefore you're using this overparameterized model. But the transactivity model is better than, say, like using previous edge improved updates. And in your model, if you combine this with like a you can do that. Yeah, you can do that. This is what Jing hinted in his first talk. He heard my talk in DC. We can do something together. Did you oh, that's in fact. Mission criteria straight away, or do you have to adapt them to your network? And also, second question: we know that AAC and BAC are one-step ahead, so they minimize a one-step higher prediction, but multiple step higher predictions. So yeah it's uh it's always uh It's always like what the people said, all model is one, especially when you come to network model. Regression model is different. Time series is AR model, AMA model is different. They're based on some kind of where justifiable. But when we propose this network, it's kind of based on how we think the transitivity is and how this transitivity can be. And how this transitivity can be represented by the function we wrote. And to do goodness of fit for proposed model to the data, of course you can use it with JavaScript both. But still, this is just some kind of aspect of applicant data. We were thinking whether there is a way to assess Way to assess, say, this proposed model is adequate for this data. Probably you ask me. Yeah, I can give you something. But indeed, it's a big problem. Yeah. And uh yeah, Jonathan talk is very impressive, especially in how it's distribution between art. Uh We had some experience that suppose that you have a network as a constraint, they have fixed number of triangles, fixed number of two-star agents, and fixed number of ages. As long as they fulfill some kind of very losing condition, you can generate lots of net network for those kind of properties. GM sampling. Yeah, GM samplings are not easy to do. We in practice, you have these adjacencies which and blocks, but you say if they talk, it's important. But what happened if you talk what many times? What happened if you talk what many times? Uh you are saying if I define a different time interval like like like this example, I use one week as a to define one unit time. Let's say you fix one week. Yeah, but in a single week, two people other time can talk to people. Sure, but you you sort of are you afraid of bullying? Okay. For what we did here, we use a very limited unlimited information, just say they're talked or no talked. You can use this weighted network to incorporate that. And then for some data, this link to the matrix time series, if we need transition matrix, is Transition matrix is the count is bigger, it can be treated as a kind of continuous matrix time space. But for small counts, which is quite different, we haven't looked into that. So can we relate your question? So if we have continuous time and then we observe the edge and the in vector time step, instead of observing Instead of observing just the snapshots of the network, so it becomes a continuous time. And people sometimes model each individual pair of edge as a performance forecast, but that doesn't incorporate any dependence. And the structure can be incorporated into the network confidence. This reminds me of continuous time series confidence. Let's forget about network. There's a time series. You observe it continuously. Continuous. If you want to use the exist standard discrete time series model, you chop it into a regular interval, then you do it. Or you really try to model it continuously when things happen. Therefore, the observation you have is kind of irregularly spaced. So it's continuous times model. Yeah, and for different edges, they can also happen at different times. Sure, sure. This is multi-variant continuous time series concepts. There are some work done, and I think all our theory rather than Broadware did a lot. Probably most time in Univariate continues. They extended its AR or ARMA into continuous. Quick G um convey to the managers while you get the as you said the behavior. Yeah, they are certainly more active within within each other. You don't have with a super sparsity that much in a and you have few data points. Does the data count like max email the CEO just send to everybody? I think so. As long as the email manager sends an email to everybody. But the manager didn't send out. The CEO didn't send out. More questions about the prediction. So since your model is generating the probabilities, how did you do the prediction? Oh, I would think you always have a threshold. Because those are predictions based on your threshold of probability. So probability greater than 0 is here, probability greater than 1 is 8 is here, then you move it around. Using the predictable actual edges for the next bleak mask would have also the mask would have a different comparison than this shot, shop Lunchtime? There are a lot of questions. We are held free time this afternoon. And if you have not yet had a plan, you are encouraged to talk to people during the lunch. I know that some people still have available spots to their cars. So just feel free to talk, get connected to other people. You've got to connect it with other people. You are you're thinking more of the landscape. Did I mean maybe something that's really