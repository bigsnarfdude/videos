Yeah, thank you very much for the invitation. It's been really interesting. So it's not going to be very non-linear, I'm afraid, but yeah, whatever. Oops, I don't know what I, yeah. So I'm going to talk about some joint work with some co-authors on Bayesian inference using two types about on nonlinear problems. One, which is an imposed nonlinear problem, which is a famous deconstruction. Which is a very famous deconvolution problem. And the second one is slightly different, is hidden Markov models. And what I'm going to focus on are inversion inequalities that we need when you divise an inference there and what you can do. So I'll just go back to basics. And so you have a data y and parameter theta is just two fixed notations with some likelihood of theta. So theta is infinite dimensional typically, and you put a prior on theta, so it's biasing. Yeah, there you go. So it's biasing. Yeah, there is a typo here. So here it's so theta is followed some prior pi, so pi is a prior, which is a probability distribution of theta, and the posterior is proportional to the likelihood here times prior. And the aim of the game here is to do inference on some aspects of theta. And so everything is done in Byzantine context based on this posterior distribution. So what you want to recover is a posterior distribution and you want to learn what it does. Want to learn what it does. So you can do estimation. For instance, you consider the posterior mean and that's a point estimator. You can do prediction if you want to estimate to predict new data. You can do testing. You can do all sorts of things. Sorry, I'm going to go a bit further. You can testing, you can construct credible regions, which are measures of uncertainty, which is what Richard talked about in the first day. Thursday. And you can also do model selections, which is our testing, so which is based on the marginal likelihood, and the marginal likelihood is this object. What I'm going to talk about is the behavior of this posterial distribution. So I won't talk about measures of uncertainty there in infinite dimensional setup when in inverse problems. So the first inverse problems I'm going to talk about is a sort of the very famous and well-known deconvolution model. Well-known deconvolution model, where you have data yi, which is xi plus epsilon i, epsilon i is a noise. So you assume that the distribution of the noise is known, and what you want to recover is the distribution of the signal xi, which I call px. So this is a good deconvolution. So what you observe is the distribution of py essentially, and you want to recover px. And I'll be talking mostly in cases where you have densities. Cases where you have densities. So in this case, f epsilon is the density of the noise, fx is the density of the signal that you want you're interested in, and fy is the density of the observations. And so we're being Bayesian here. So now I put a prior on the unknown object. So I'm putting a prime on px. And typically I'm going to put a prior fx. So from that, once I have the prior, then I can recover the posterior distribution. So now I have n observations. So now I have n observations, and I'm going to let n go to infinity. And the posterior distribution on the set B is the ratio of this integral over B of the likelihood times the prior divided by the normalizing constant. So if you're interested in the behavior of the Poissier distribution and you want to sort of understand whether you're learning something, the first thing you can look at is what calls Poissier contractions rates, which means that you're looking at neighborhoods of the two parameters. Two parameters, so either f0 or f0x, and you're looking at neighborhoods for some with respect to some metric d that shrink to zero at the rate epsilon n, such that the posterior distribution of these neighborhoods still goes to one. So the posterior concentrates on these small neighborhoods, and you want to find the epsilon n as small as possible, but you still get this posterior contraction here, where the posterior probability that goes to one. So, and so in the direct problem, then In the direct problem, then I'm looking at neighborhoods of the distribution of the observations, so that's an easy problem in a sense, or it should be easy. In the indirect problem, I want to recover the distribution of the signal, and so I'll be looking at losses that directly target these distributions. Right, and so now, so typically, and that's sort of this is more for the direct problem, there. This is more for the direct problem. There is a very well-known strategy now to play with these games, which is called, which I usually call the Gaussian Rendovat strategy, which is because that's the people who sort of started the story and actually did most of it. If you want to prove the air contraction rates on some kind of neighborhood like that, you have to study two types of conditions. One is a cubicle condition, which means that you won't. Which means that you want to prove that the prior mass of Kuildbach-Lebland neighborhoods of your observations, so that's the direct problem in a sense, is large enough. So, what I mean by large enough, it means I'm looking at small neighborhoods of size epsilon n squared. So, in the deconvolution problem, it would be like if you sort of research everything by, we divide by n here, n disappears, and you're just looking at epsilon n squared. And this primar mass needs to be greater than exponential of minus c n equals. Greater than exponential of minus c n epsilon n squared here. So there is this balance between bias and complexity, prior complexity, which is a bit like a bias variance bias that will be trade-off. So that's the first one. And it doesn't depend on the on the metric D here. It's purely based on the variable divergence. And the second condition is the testing conditions, which means that you need to construct some testing function phi n, which has type one error going to zero, and type two error, which is exponential. And type two error, which is exponentially small, which is what I have written here for functions. So, if you're looking at the functions that's far away from the true function at a distance epsilon n, then the type 2 error here is bounded by an exponential term like that. And so this type of approach is very efficient if you're looking at some metric D, which have statistical distance. What I mean is that you're able to construct this test. That you're able to construct this test. And this statistical distance works well for the direct problems. So, typically, if you have a nice distance for the direct problem, you're able to construct something like that. But in the indirect problem, then it doesn't work so well. So, what we studied with Katyas-Kretscholo is the case where we want to recover the signal, and we are looking at a specific metric for the signal, which is a Wasserstein contraction, a Wasserstein distance. Wasterstein distances, they are well known. Distances, they are well known now. Yeah, they are super important and they are all over the place in machine learning, in particular. And essentially, what they are is like the certain p is you're looking at the minimum coupling, coupling expectation of coupling between x1 and x2 to the power p, where x1 has the marginally follows from p1, x2 marginally follows from p x2, and the coupling is a joint distribution of x1 and x2 that verifies this marginally. Next to that verifies these marginal distributions. In the specific case where you have univariate observations, then the Wasserstein has a very simple form, which is this one. So that's the L1 norm of the CDS. And so that's we'll be working on in R because we'll be using this very specific form here. And so the aim of the game now is to look at these posterior contraction rates I described before for the For the noise, for the signal, so px in Vasso strain y one. And so I recall the model. So you have data yi with noise with signal and plus noise. fx is unknown, f epsilon is known. And I'm going to call f hat the characteristic function for whatever. So f hat epsilon is. For whatever, so f hat epsilon is a characteristic function associated to the noise. And it's a well-known fact that the smoother the noise, the harder it is to recover epsilon, the the signal. And so I'm considering here mildly posed problems because I'm considering noise f epsilon that are beta smooth. In other words, these characteristic functions go to zero at the rate t to the power minus beta. t to the power minus beta. So there is like some sort of slightly stronger assumption here, but it's not so much. And so the main ingredient that you use once you have these posterior contraction rates, if you want to do this posterior contraction rate in Weiss time, is to construct an inversion inequality. So the reason why you have to do that is that you can you use the general theory to play with the direct problem. To play with the direct problem. So you get the posterior contraction rate in the direct problem. And now, what you want to know is that if I have this, if I know that d of fy and f0y is less than epsilon l, what can I say about the vast search time between fx and f0x? And that's, and to do that, you need to have an inversion inequality. Unless you do something else in the Poisson distribution, but sort of the most natural way to do it is to play with this inversion inequality. To play with this inversion inequality. And that's typically, if you do Bayesian frequency inference, you would do something else. You wouldn't need these inversion inequalities. But for the Bayesian inference, it's very important. And I think in inverse problems, sometimes you call them stability estimate or something like that. Yeah, that's what I mean. And so the sort of the big part of our paper is to prove an inversion inequality from the direct problem to the indirect problems, looking at two different types of. Problems, looking at two different types of metrics in the direct problems. And so we have two cases. So I'll start with just this one, the case one. So if the smoothness of f epsilons of the noise is greater than one, so then the inversion inequality that we're considering under like very weak assumptions, so we're only assuming under the for the signal that it has a moment of order one is bounded by some log n term which I don't care time Some log and term which I don't care times the L1 norm. It's a bit of a mess what I've written. It's the L1 norm between F1 and F0Y to the power one over beta. So this is a purely analytical inequality. So why does it depend on n at all? It's just because I cheated. Yeah, I'm assuming. Yeah, it's a good point. So it's under the context that this is smaller. Okay. So somehow there. smaller okay so somehow there is a log of i could have written log of uh fy minus f0y okay uh here in like it could have been log of l1 norm of fy minus f0y but i'm yeah it's we under the assumption it's a good point yeah i was i didn't change it but i did a shortcut so what you mean is that the modulus of continuity is is like there's this hundred term and there's a logarithmic term as well yeah yeah so this is remember that well yeah yeah so this is remember that we asked because i was uh i wanted to make the you have a different from actually you can yeah you have a different formula and when you you you start from like a marginal formula which is totally analytic and then you are you work in this context where this is less than some n to the power minus kappa for whatever kappa and then that that gives you this all right thanks yeah so so that that that term is typically a log of that term so this term is a log of that term to the Term is a log of that term to the absolute value, typically. Under the assumption that this is small enough, but actually, you don't really need that much. And so that's the first setup. And then I'll give you the consequences of working out this assumption in terms of finding a rate for this guy. And then the second, I'm going to erase that. So the second case is where. Cases where so this this is only true for beta greater than one and but so there is a there is a like a minimax theory that exists for recovering uh the signal in Versus Chin distance in the frequencies literature and what they get is a rate for this guy typically for beta greater than the half the rate is n to the power minus one over two beta Over 2 beta plus 1 times some login terms, it doesn't really matter. And so, and I explain when you have this, why you might get this rate. But the question is now, what happens if beta is less than one? So, between beta, when beta is between one and a half, you can use the second inequality where you get, I'm going to use this, you get this, yeah, this. Yeah, this upper bound in terms of so you but you compare the Wasserstein in the indirect problem with the Wasserstein in the direct problem. But then this time the exponent is two to the power two divided by two beta plus one. So of course that means that you need to to be able to get a good rate there, you need to have a good rate there. Okay, but what are sort of the implications? Actually, before, so I'll talk about later on, I'll talk about So I'll talk about later on. I'll talk about the implications in terms of rates and how you can hope to get sort of the optimal rate there using these two types of inequalities. But before I go there, you can actually improve on this rate there by making more assumptions on these distributions. Because for this innovation, I wrote it with a density, but you don't need the existence of the density. You only need this sort of moment condition. you only need this sort of moment condition. So if you're ready to make more assumptions on Pf0x, then you can improve on the rate. And in particular, you can if you assume that there is a density and that the density has a sobolis, smoothness, which means that a sort of sort of a sobolis, not quite sobolis-mounet. So in other words, this integral of that's the characteristic function that the Fourier transform of the density. Characteristic function that the Fourier transform of the density has a sort of goes to zero at a certain rate, in a sense. Or if it has a holder, so it's sort of similar. So alpha has the same meaning. So this is alpha is the level of smoothness of F0x in a sense. If it has a local holder condition here with L1, L0, which is in L1, then you, instead of having this assumption I have written there, you this Inequality, you have an improved version where the exponent that you get are better. The larger alpha, the better the exponents, of course, because you're making more assumptions and so you get the better rate. You get a better inversion inequality. Sorry, just a bit lost. So the beta measures the decay of the Fourier transform of the aero density, no? Yeah, so yeah, exactly. Yeah, so yeah, exactly. Don't you need a lower bound for the decay of the Fourier transform rather than an upper bound? So, so, so, yes, I maybe there was a typo. So, essentially, you we are working in the context where f hat epsilon, I don't know what I wrote actually, but f hat epsilon is bounded from above, from below by a t to the power bit. I mean, I'm making it very rough, actually. You write slightly different. Yeah, yeah. And so, and so, and yeah. And actually, we're asking, as I said, we're asking a little more. So, we're asking that even the derivatives have this, like, yeah. So, yeah, the reason F epsilon is one over F hat epsilon. So, that's why it sort of was disturbing. So, R epsilon is one over F hat. And so, I'm asking that if I derivate it, then I get. I derivate it, then I get sort of what I want to get for the first two derivatives. I mean, the derivative zero. And so if you take j equals zero, then you get what exactly what I was saying. One over f hat is like t to the power minus beta. And if you get the first derivative, then you lose one power for the one over f hat. So the gamma density satisfies this? Yeah, yeah. Laplace satisfies this, for instance, yeah. Well, gamma, gamma is uh Well, gamma, gamma is gamma is a bit different because gamma is not on R, it's on R plus. So, in a sense, you have something a bit different coming in somehow. If you are on R plus, then you have even more information. But that we didn't try. I mean, you could get our bound, I think, on the gamma distribution, but my impression is that with gamma distributions, you should get better bounds. Because there it's like, if you imagine if you take. Because there is like if you imagine if you take if you have an exponential distribution just on zero plus infinity, then there is a jump, and so it's easier to see the jump, you know, so you should be able to estimate x better. Okay. So here we're looking at densities that are on R, the whole real line. You see what I mean? Sure, but I mean, I guess a gamma density might still satisfy your conditional. Yeah, yeah, I think so. The gamma still satisfies this assumption, except. Assumption, except that you have to make sure that the density, the characteristic function is not equal to zero. And I can't quite remember how it goes, but I think it works. So, yeah, I'll give a few examples of what work and what doesn't. Yeah, so you have an improved rate in particular. Yeah, so you've got these two sorts of rates. I don't want to bore you with this sort of result, but you have alpha that comes in. And essentially, the way you can think about this is as in the direct problem, Fy. Fi, Fy, F0 Y would have regularity alpha plus beta. But then there is a price to pay to go back to Fx. But the price is not quite the same as if you didn't have any alpha. And so once you have this inequality, what are the sort of typical expected rates that you could hope? And to understand the typical rate that you could hope is to try to figure out what's the expected rate in the direct problem. So what would be the Problem. So, what would be the expected rate for this object or that object? And so, if you think about it, so f hat of epsilon is like t to the power minus beta. So, imagine that now to understand what's the expected rate, I'm going to not only assume that it's bounded from below, but it's also bounded from above, so that I have a smoothness, like I control the smoothness of my fy. So, fy is the convolution between f epsilon and Of convolution between f epsilon, and now I'm not making any assumptions on the distribution on the noise, it can have no densities. So, if I make no assumptions of p0x, then what I know is that this f hat, this characteristic function of the Fourier transform of f0y, times t to the power beta minus a half times the log t something, is in L2. So it's some kind of, it's not quite a soboleft space, but it's a bit like a weighted sobolef space, and that's sort of. And that sort of describes this minus a half here. But it's a subof with beta minus a half and not beta. Okay. And now, if you sort of forget about this logarithmic term, so you're only working on this beta. So if you are in a subolef beta minus a half space, then the minimax rate for a subolef is n to the power minus beta minus a half divided by two beta. So in the direct problem, if you only take into account the fact that you are in a subolef space with You are in a sober F space with smoothness beta minus a half, then you expect this rate for the direct problem in L1, okay, for f0x. So, here now you're going to plug in in my first bounce. So, here I'm going to plug in n to the power minus beta minus half divided by two beta, and I'm going to get a rate there. So, if I do this, yeah, I get a suboptimal rate, so it's not quite what I want, but if instead of But if instead of assuming that I have, if I make a slightly more stronger assumptions, I'm saying that P0x has a density, which is in L2, then all of a sudden I'm not into, yeah, there is a typo here, I'm not into a subolf beta minus a half, but I'm in a subolf beta. And then up to a logian term, the minimax rate in the direct problem is n to the power beta minus two beta plus one. And then I recover the vast sostein rates for the indirect problem, which is n to the power minus one. direct problem which is n to the power minus one over two beta plus one which is what I want and so uh so what with the inversion sort of the stability estimate that we get for this inversion inequality if we make no assumptions on the smoothness of F0x but we still assume that there is a density F0x then you hope you can hope to recover the minimax estimation rate in vaster minimax estimation rate in vasse time for estimating estimating f0x vaserstein one so that's uh you it can be useful because then it means that you don't have to look at very specific estimates you just need to know the rate of your estimator and you just know to need to know that your estimator can be written as a convolution so it's an f epsilon convoluted with with some some f hat x so when beta goes to zero you get When beta goes to zero, you get r rate one over n in the limit. That doesn't make sense. No, no, no, but beta is greater than it has to be greater than one. Yeah, at least it has to be greater than a half for this reason. So I'm in the context where beta is greater than a half. So what's the second index of the soboleph space, the minus one half? So yeah, so this center is just a logarithmic term. So it's not quite a sobolef because you have this logarithmic factor there. So the space I'm So the space I'm I'm calling this space H gamma T or gamma alpha gamma di designs the power there and this trump design denotes so this denotes the power there. So it's a weighted version of the body space. You see what I mean? So it's a set of functions such that the Fourier transform multiplied by these weights are in L2. It's just like a logarithmic It's just like a logarithmically corrected Soble F space. Exactly, exactly. So there is some kind of little weight that you're adding, and so it changes purely the power in the login term. And yeah. So the reason why you need to have this logarithmic addition is because you want to have something that's in L2. And so it helps. Yeah. Yeah. Okay. That makes sense. Thank you. So now, so, but all this theory has been is really for. Theory is been is really for, as you were saying, recharge beta greater than one, really, because I'm using the second inequality, which is the L1 norm. Okay, and now if I have a noise which has ill-positness or a smoothness beta less than one, but greater than a half, then I'm using the second inequality, which is actually, so I have rewritten it, the sort of the analytic version of it. Version of it, yeah, which is the one with the Vassarstein on the direct problem. So somehow I wrote the old version. So to go from here to what I showed before, you just have to say to find this is true for any edge, and you find the edge that makes the right the best upper bound on the right-hand side. And so, um, and in order, so the thing now is that, so this is a vaster stainer for the image for the observation. For the observation, the distribution of the observations. And so, typically, this Vassarstein distance is of order one over square root of n because you are in dimension one. So, and so if you can prove that in your direct problem, you have a Vassarstein of order one over square root of n here, which is exactly what you would expect. So, if you have an F tilde that has that can be written as F tilde epsilon, F tilde Y is F epsilon convoluted with some F tilde. alone convoluted with some f tilde 0x then f tilde sorry then and f tilde has this vasserstein distance between with with f0 f0y of order one over square root of n then using the inversion inequality you get the minimax rate for estimating f0x and that requires no i'm i return it as if i had a density but you don't need a density that's Density, but you don't need a density. That's true for any distributions. Okay, but of course, you need to prove this control there first. With frequency estimator, this is something that has been done in some contexts, and so you could use the result that exists in the frequency literature to plug that in and get your result on effects. In the Bayesian framework, getting square root of n convergence rate for the Wasserstein is less obvious, especially in Less obvious, especially mixture models, but potentially this could be done. And so now I'm going to apply this sort of inversion inequality to the mixture to a specific context where the noise is Laplace. Just a quick remark that in dimension one, the root n convergence rate in a Wasserstein distance follows from the second paper I have with Ismail Castillo. If you choose the right multi-scale space, it's Multi-scale space, uh, it's it follows directly from our results, yeah. Yeah, but but you would need to prove, I agree, you would, yeah, I agree. So, you could use your result, but you would need to prove to apply your results to the context of a convolution modela. But at the forward level, at the forward level, yeah, okay, okay, sure. No, you're right, sorry, because you need this relationship, yeah. So, in the direct problem, we did it, but yeah, okay, fine, yeah, yeah, you cannot use an extra, yeah, you can, you cannot choose an extra. An extra, yeah, you cannot choose an external estimator that converges without thinking that you are in the convolution model. Yeah, you're right, yeah, but you're otherwise, yeah. But I probably the way to go would be to use your methods and to prove something on the LAN, but it's not obvious how to do it. No, I agree, yeah, sorry, yeah. And so, um, and so now, so if you are in the LAPLA, so that's a very common noise, in particular, it's used in people when people do a privacy, a differential privacy. Privacy, a differential privacy is a yeah. So the noise is a Laplace distribution, and then it corresponds to beta equals two. So I'm in the context where beta is greater than one, and so I can use my first inversion inequality where I compare the Wasserstein in the indirect problem, fx, with the L1 norm in the direct problem, fy. And so to be able to, as I explained before, to be able to get the minimax traits for fx, I need to For fx, I need to make sure that in the direct problem, so for the L1 norm and fy, I have the minimax rate, like n to the power minus beta or two beta plus one, essentially. And so the way, so if you are, it's a mixture model for FY, and so you work, the way to do it is you put the first thing you can think about is to you put a deal share process on the mixing distribution px, so this fx I was talking about, which is not putting a prior. Is not putting a prior on densities, but it's putting a prior on distributions. So I'm not, if I do that, I'm not assuming that I have a density for fx. Okay, so I'm not smooth in a sense. So if I do that, then there was a paper by Advan der Warten Gao, can't remember his first name, where they get a rate for the direct problem, which is n to the power minus 3 over 8, which is not. Which is not so that, and then you apply your inversion inequality, and you get n to the power minus 3 over 16. So, it's not n to the power minus 1 over 5, which is what you want. I'm not counting log n terms here. So, what we did to improve the rates is to now assume that you have a density. So, you assume that you have a density, and so you're going to model your distribution to model a density. To model a density, so you're going to put a prior on a density and not on a distribution. So, if you put, I'm not going to say px follows a geostate process, I'm going to say px follows a geostate process mixture of normals. So, I'm going to model my density as this mixture of normals, which is a very common prior for density estimation. So, it's a location mixture of normals. And I'm going to assume that my density of 0x has light has light tails. Light tails. Okay, so under this assumption, then you can prove. So, I mean, there are results on estimating densities using mixture of normals. But here it's more complicated because your FY is a mixture of Laplace. Well, the mixing distribution is itself a mixture of normals. So you cannot quite use the same trick as was used before to approximate f0x by fx, like that. By fx, like that, it's more complicated. But you can still work out that you get in the direct problem the rate n to the power minus two over five, and so that leads to the minimax rate up to a logen term in the indirect problem for px. Okay, and that's without assuming that f0x is smooth. So I'm just assuming my density is, I'm just assuming that it exists and it's in L2. And of course, it's L2 because it has this sort of nice tails. So, if I'm adding additional assumptions on the smoothness of f0x, if I'm saying, okay, now I'm using the same prior here, but I'm assuming that f0x has some smoothness alpha. In particular, if I want to keep the sort of soboleph because it's a bit like sobof smoothness we are talking about, so I'm going to assume that my f0x has sobolef smoothness alpha, which I have written like that. So, it's not. Written like that, so it's not exactly f0x, it's like a different. Actually, you can improve the things and you can get rid of these terms. Now we have improved our bounds. So, if f0x has some smoothness alpha, sober f-type smoothness alpha, then you improve your direct problem rate and you use your improved inversion inequality, and you get sort of the minimax, what you assume is the minimax rate for this problem, which is n to the power minus alpha plus one divided by the alpha. n to the power minus alpha plus one divided by two alpha plus five essentially which is corresponds to uh yeah so that's that's uh and we think that it's a direct problem it's a minimax rate because um this is a minimax rate for estimating things that are simpler uh than p0x in vasserstein uh obtained in a paper by whole and some co-authors uh actually so what i've said in Actually, so what I've said in the case of Laplace can be extended to more general densities than Laplace, the construction that we have considered to make the approximation in the direct problem. And it's actually true for the Linux distribution. And why is that it? It's because the Laplace has a characteristic function which is like that. So it's very simple, it's very nice. But if you look at the generalization of this characteristic function by that one, so instead of a two here, you put a beta, then you end up with what's called the Linux distribution. So it's used in particular for cell estimations. Then you can work out an approximation scheme to approximate a smooth function by A smooth function by Fy by a mixture of normals mixing with a Laplace, with this with a Linique, and you get exactly the same kind of results as I said before, but with this sort of beta smoothness for the noise. And that works well. And so you get exactly the rates that I was describing before, which is n to the power. Describing before, which is n to the power minus one over two beta plus one for the indirect problem, and you get n to the power minus beta over two beta plus one for the direct problem, for beta greater than one. So you get that rate with Linique for beta between one and two, and you get that rate for the direct problem for beta between one and two. Beta equals two is Laplace, and beta less than two is Linique, general Linique distribution. So that's more general than just Laplace. More general and just lapse. Yeah, and so that so the so this is one context so which where because you're Bayesian, if you want to get posterior contraction rates, the natural way of doing it is to work in the direct problem and do these stable estimates. That's what Richard was talking about as well on the first day. And unless you have very specific forms for your posterior distributions, there is no Forms for your posterior distributions, there is not much else you can do. It's very hard to do anything else, actually. The reason why I'm going to talk about a different topic now is because there are situations where you don't want to do that in the inverse, which I still call inverse problems, but there are like more statistics, but they are a bit like inverse problem. And I thought they were interesting, and that's why I wanted to talk about. Was I wanted to talk about them, and it's a situation where actually being Bayesian helps you. So, in the first situation, being Bayesian doesn't help you because the frequency proof for proving this posterior convergence or this convergence of the estimator in Western Stein is much simpler than trying to work out what's the inversion inequality. Working out the inversion inequality is harder. In what I'm going to describe now, To describe now, being Bayesian, so having to play with a posterior distribution, which is a distribution on the parameters, actually helps to construct, to get a contraction rates on the parameters. And that's this example. So it's a different model, totally. It's still a mixture model in a sense. So you still have latent variables. So the way the model is constructed is you observe. Is you observe some Y's still, so the Y's are still my observations. The X are still the latent variable, so a bit like it's not quite a signal, but it's a latent variable. And but this time the latent variable, I'm assuming that they belong to a finite state, a spinite space, so they take only k possible values. And the distribution, the conditional distribution of yi given xi equals k is fk. I'm not specifying what it is. And here I'm going to consider the context where And here I'm going to consider the context where my xi's, they are not I D with distribution fx, but they form a Markov chain. And it makes a lot of difference here. And I explain why later on. So they form a Markov chain with transition distribution Q, the form Markov change means that the probability of Xi equals K given the past depends only on the last values and is Q X I minus Y K. So the Q is this is this transition probability, it characterizes the Markov chain behavior. Of chain behavior, and so if I have this model, then I essentially what I have as parameters is the transition matrix Q, which is finite-dimensional, and the emission distributions, which are this conditional distribution of yi given xi equals k, fk. So these models have been all used a lot in the frequency in the statistical literature for many problems, but usually fk are parametric. So you consider they are normals and or whatever. Normals and/or whatever, and here the key thing is that if you have a transition q such that the determinant is different from zero, then you don't have to make a parametric assumptions and you can assume them to be anything. So I'm not making any assumptions on the FKs. I'm just and the aim of the game is to estimate both the FKs and the Q's. Okay, so the only assumptions I'm going to make later. The only assumptions I'm going to make later on is to say that the FJs have a density with respect to Lebesgue measures, but to begin with, I wouldn't have to do that even. So to go back to sort of direct and indirect, so inverse problems sort of way of thinking about it is that the sort of direct problem of associated to what I've just described is if I were to study the distributions of my of three concepts of a number of consecutive observations. Of three consecutive consecutive observations. Now, my observations are not independent anymore, they are dependent. So, if I look at the distribution of, let's say, two or three consecutive observations, so L equals one, two or three, then this is this object. And it's an object that depends on the transition matrix Q and on the emission distribution FK. So the direct problem consists on studying the behavior of the posterior distributions, if I do posterior inference, for estimating these G functions. Estimating these g functions, yeah, these g densities. So, I could do it from a Bayesian perspective, or I could do it from a frequency perspective. Both exist in the literature, and so posterior contraction rates in the direct problem corresponds to looking at probabilities of neighborhoods around the true distributions in the L1 norm for these densities of consecutive observations. Okay, and so I want to recover this epsilon. And so, I want to recover this epsilon tilde. And so, epsilon tilde is a rate indirect problem. But typically, this is not necessarily the object you are interested in. This can be of interest if you want to do prediction sometimes, but it's not exactly what you want. So, if you, what you want is going to be Q and F. And this time I'm not going to say how to recover Q, because what I want to talk about is these inversion inequalities. And the inversion inequalities assume. And the inversion inequalities assume that you already have a good estimator for Q. And we have ways to do good estimators for Q, and so I'm not going to talk about that. But before I want to say that we can do good estimators for Q as an F, I want to, there is the first question to be answered, which is, can I identify Q and F? What I mean by that is that if I have G's, which is something that I can get from the observations, can I recover Q and F? Over Q and F. And the magic of the story is that if the determinant of Q is positive, by looking at the consecutive, the density of three consecutive observations, then if the densities are the same for different sets of parameters, it means that my parameters are the same up to label switchings. So this is totally identifiable. And that's super nice because it means that you don't have to make parametric assumptions on your emission distributions. Distributions. So now that I know that it's identifiable, I want to know how well I can recover my parameters, Q naughts, and these emission distributions. Okay. Judith? Yes. Does this identifiability have to do with the discrete nature of the state space or of sorry of the yeah, of Q? So it's it has to do yeah, you have to so there is a a new paper and I Is a new paper by Elisabette and Ismail Castillo and people like that, and Queku, actually, where they have even weaker assumptions for identifiability, and they can show that you can identify in the like in the, so it's in the park, for instance, in the deconvolution problem. If you don't know the distribution of the noise and you don't know the distribution of the signal, you can still estimate the noise and the signal under certain assumptions, which are very weak, but they don't get any rates. Very weak, but they don't get any rates. But already being able to prove that you can estimate it is already quite a big step. So, here, like, what's the there are two things here. One is the fact that you have a discrete state space and a finite state space for the latent X is a key, is a key quantity. And the second key quantity is the fact that the determinant is strictly positive. If the determinant is zero, for instance, if you assume that the X i's are I D. The XI's are ID, so you are in the mixture and not in a hidden Markov model, then the model is not identifiable. You can, there are counter-examples and things like that, and it's and so what makes it work here is that the determinant is strictly positive. So the fact that you have a structure in the latence, which is a hidden mark of a mark of chain, is a bit like a perseverance, give you some signal, enough signal on where the states are to recover. To recover the successive states inform each other in some sense, right? Exactly, exactly. So it's a bit magical, it's a bit magical, but it's, yeah, you don't know, you never observe the states, but there is enough perseverance, persistence maybe between the in the in the Markov chain x behind the scheme to be able to recover the q and the f in theory. And so what's what's yeah, and so what's uh What, yeah, and so what's uh, what I'm not going to talk about is that you can actually estimate q at the rate one over square root of n, and we can do that. But what I want to is to sort of emphasize is how do you recover the f' once you know something about the g's? So, if you know some contraction rates in the direct problem, can you say something about the indirect problem and also in the smoothing? So, there are two things that you want to estimate: the f just one more thing. I mean, we're running out of One more thing, I mean, we're running a bit late, and I mean, in having a break, maybe one, two more minutes, okay. So, I'll be super fast. So, so, so you can have inversion inequalities. So, I'm going to skip the F and I'm just going to say in two words, I mean, I try to be a bit brief, how you can recover this object, which is the smoothing, it's called the smoothing, it's which is the distribution of the X I for an observation I. For an observation I, given the whole vector of observations. And so, if you want to do clustering or segmentation, that's the sort of object you're interested in. So, that's what you want to recover. And so the aim of the game is to recover these guys from these guys. And so, that means that requires an inversion inequality. And so, I'm not going to be super fast there. And so, there is one inversion inequality that existed in the literature that was. That existed in the literature that was obtained by Elizabeth. So that's my smoothing things, which was not, unfortunately, in terms of the L1 norm, but it was in terms of the empirical norm of the distance between Fk and F0K on the observations. And that's a nasty object. So then there are two ways to go back to this. If you have a sub-norm convergence rate, then you're done, because a sub-norm convergence rate there gives you a convergence rate there. Convergence rate there. If you split your sample in two and you use half of the sample to estimate the parameters and you do the other half to estimate the smoothing, then it also works using frequentist method. What's nice is that if you use the Bayesian approach, you don't have to do that. So you can use the data, the same data, to estimate F and to estimate these objects. And because you're integrating out F. f and essentially you're going to integrate out y you can use the l1 concentration rates for this object to get a rate epsilon n so you get an epsilon n rate in the direct problem and you get an epsilon n rate in the indirect problem so it's not an imposed inverse problem it's a well posed inverse problem and i'm yeah i'm going to yeah it's it's well it's a it's a funny treat but i'm going to skip that thank you