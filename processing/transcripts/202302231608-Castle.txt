Okay, so I want to talk about, I think this is called restricted trichotomy conjecture of Zilber, which is just the Zilber trichotomy, but for strongly minimal sets interpreted in an algebraic imposed field. And by interpreted, I mean you're forgetting some structure. So you're taking a constructible set, and then you're sort of taking some, but not all, of the structure from the field. And so you could lose some of the nice structure. Structure. You could, some interpretable structure might not know what the topology is, it might not know what a smooth point is or anything like that. But assume it's not locally modular and you want to interpret a copy of the field. And okay, there's a somewhat long history of this problem, which I don't really want to get into. But I should mention it's something that's kind of useful in algebraic geometry. So, partial. Geometry. So partial cases have been shown to solve a problem by silver. And it's equivalent, by the way, it's equivalent to saying just get any structure interpreted in an algebraically closed field. And if it's not one based, then you get a copy of the field. Okay, so if you start thinking about this, the first sort of natural thing to do is you split into cases by the dimension. So you have this set M, which is the dimension. So you have this set m, which is the dimension, and it could be one, or it could be bigger than one, but it really should be one. Because if you interpret a field, then you get a non-orthogonality between m and the field, and every field in an ACF has dimension one. So it really should be one. But a priori, it doesn't have to be, because you know, you're forgetting some structure, so you could take some higher-dimensional set and still put a strongly minimal structure on it. Okay? But you get this natural. But you get this natural dichotomy into one-dimensional, higher-dimensional cases. And, okay, the one past result that I want to point out: the one-dimensional case was treated by Hassan Sustertov in a preprint that's been up for some time in a couple of different forms, but it, okay, so you have the one-dimensional case, and it leaves open the question: look, you have to have dimension one. So, basically, to prove the whole thing, you need non-local modularity. Whole thing, you need non-local modularity to just imply that the dimension is one. So main result, that's true, well, in characteristic zero. So you have complex constructible set and then a non-locally modular, strongly minimal structure. All the definable sets are constructible. Then you get a copy of the complex build. Okay, so you don't, it's completely independent of the It's completely independent of the dimension. Okay, and the proof is, okay, nice thing about it is that the proof is largely uniform. So it's not like you just do this higher dimensional case and then quote the other thing. It's like largely just one proof starts to finish. So self-contained. And because this is neostability, I have to say there are variants. So you can do the same question, but you replace. Same question, but you replace the background field with an ACVF or an o minimal field, and okay, the method has to be changed a little bit, but it still works. So the higher-dimensional question you can do in ACBF and in ominimal fields, but those are not really available yet, so I'm just going to stick to the character 6-0 ACF. Okay, however, I should say if you have ACVF in higher dimensions, then you can actually prove the whole conjecture because. And you can actually prove the whole conjecture because you just, okay, you have some strong and minimal structure. If the universe is dimension one, you're done by Hassan Susterthag. If it has dimension bigger than one, you put evaluation on the field, which is still interpretable in that, and then you just quote that one. Okay? So it's like a soft claim that the full conjecture is done. Good, so I'm saying in characteristic P, So I'm saying in characteristic P, okay, ACVF is in all characteristics. Okay, so you have this theorem, same characteristic zero, but you can adapt the proof in ECBF in all characteristics, but you need the valuation. So the proof needs the valuation is a little bit more complicated, so I'm just going to stick to this. Okay, so conventions. Okay, so conventions, I just don't want to say anything wrong. So I'm going to put these things up there and then you'll all forget them. So technically, I want to add countably many constants to the field and work with this thing K, but I just want certain things to be zero-definable. Any notion that has a geometric meaning, I'm interpreting in the sense of the geometric meaning. Okay, so generic is something which implies actually smooth, it's in the sense of the full field. Field dimensions in the sense of the full field and so on. And if I want to say the corresponding thing with this structure m, I'm just going to put an m next to it. So can I ask something? What's the difference between the complex number and any characteristic zero algebraic equals field? It's about positive characteristics. Ah, that's the difference. So the full conjecture is about positive characteristics? That's the difference? That's about complex or not complex. Full conjecture is algebraic equals fields. Without any restriction on the characteristics. And I'm saying I can do the complex numbers, but. I can do the complex numbers, but then you get characteristic zero, and to do characteristic p, you use evaluation. Okay. And I'm going to assume, okay, I just want to assume these things. So all zero-definable sets in M are zero-definable in K, so it's just more convenient that way. And ACL, did I say that? ACL of DMP set in M should be infinite, because then you can weakly eliminate imaginaries. Imaginaries. And if I want to take dimensions in M, I'm going to say rank. That's probably the most important thing on this slide. So Morley rank means in the sense of this M. Dimension means in the sense of the field. I hope that's natural. Because I'm going to use the word plane curve so often, I'm not going to say M plane curve. But a plane curve, to clarify, means a morally ranked one definable in the structure. Definable in the structure M subset of M squared. It does not have to be strong and minimal. Okay, and I'll remind you of those things if needed. Okay, so the first thing to do, I should say, okay, there's a history behind the problem, and et cetera, but I don't really want to go into it because what I want to do is basically say as much about the proof as I can. So that's what I'll do. I'll just sort of go into the proof. Sort of go into the proof. And so it'll be kind of an informal talk because I just want to explain the ideas. Historically, the main thing you have to do, so that's like the starting point, the goal of the entire thing, is to approximately define tangency of plane curves at a point. You have some two families of plane curves. You have an intersection between two of them, and the two curves are tangent there. You want that to be something like definable. Something like definable. You're not going to get definable, but maybe there's a dependence or something. And the way you're supposed to do it is you say, okay, okay. You have a tangency, you have two intersections kind of turning into one intersection. So you're supposed to say that tangent is like. It's like fewer intersections. Okay, you can count finite sets in a strongly minimal structure. So you just do that and you say tangent things should intersect this off and then other things. This is like what you're supposed to do. It's just really hard to make it work, but you're supposed to do that. Okay, so say that works. This is the historical strategy. So the main challenge, the real goal is to find tangency, but this is what you do. Define tangency, but this is what you do if you've done it. So non-local modularity gives you a rank two family of plane curves. And if you pick a point, you get a rank one subfamily through that point. And if you kind of move things a bit, you can assume the point's on the diagonal. And then you can think of field elements as slopes. So if I have a plane curve and I zoom in at a point, it's like a finite correspondence, which you kind of think of as a local. Correspondence, which you kind of think of as a local function. So you could take its derivative and you get an element of k, and that's like a slope. And then you can sort of think of interpreting that as taking the whole family of plane curves mod tangency. So if you have a notion of tangency, you can quotient by it, you get slopes. That's extremely imprecise, but of course using group configuration theorem, and you can just trust me, you can make it this nice. Okay, so you have a copy of the field. Now you want to do operations. Now you want to do operations. So to multiply you do this composition. Okay, it's the natural generalization of composition from functions to correspondences. But you can't really do addition, okay? There's no natural addition. But if you have a group, you can do addition. So if you already have a group structure, you just add it pointwise. And then you, okay, there's a natural analog of addition, pointwise addition for plane curves, and it gives you addition of slopes. Okay, so this is like, this is by the chain. Okay, so this is like this is by the chain rule. You get multiplication of slopes. This is not the sum rule, like the normal thing. It's by the higher-dimensional chain rule because the group operation might not really be addition. But you can check it's by the higher-dimensional chain rule. You have like two pieces of the slope of the sum, and those come from the chain rule, and they add. Okay, so if you have Anything, you can get a copy of multiplication. And if you already have a group, you can also get a copy of addition, and so then you do one and then the other. Okay, so you get multiplication, then you have a group, and then you start over, and then you get two operations on top of that, and you get a field. Okay, so that's like approximately how the argument is supposed to go. But the main point is you need to do this recovery of tangency and then. Tangency, and then okay, the rest is sort of numb. Okay, but this is really hard. So, why is it hard in general? Well, everything I said on this slide is if the dimension is one, really, and if the dimension is bigger than one, okay, the thing that mainly fails is this. The slopes at a point don't form a copy of the field, and you don't really get a group of slopes in a family of them. So, hard to run that part of dimensions bigger than one. You have to consider slopes at non-smooth points. To consider slopes at non-smooth points, because once you start composing, you're requiring that all the curves pass through a point, and then you start composing, you're going to introduce some branches. And okay, slopes are not as well defined, and it just gets really annoying. Most important problem, the Bezou theorem fails for a really annoying reason, which I can try to draw. So, this is two intersections turning into one intersection. Turning into one intersection at a tangency, but now pretend that each of these curves has an isolated point, also. And that as these two things go together, the isolated point collides here. And then somehow you have this other point here, which is actually on this curve and creates a new intersection. Hope that made sense. Point is: if there are isolated points on the curves, then this can. On the curves, then this kind of doesn't work, and you have to work pretty hard to get around it. Okay, so the Hassan-Sustotoff paper, one of the main accomplishments is getting around it, but they're using that the dimension is one. And higher dimensions, isolated points here, could really be things of any intermediate dimension. The reason that's a problem is that they're not definable in a strong in the reduction. So, finite sets, you kind of use that. Finite sets are definable, and that goes away. And that goes away in hired images. Okay, so the sort of mantra here is to gather all the things that go wrong and put them into one thing. It would be really good if there is a test for tangency that only cared about what happens near the tangency and that you only ever had to check it when everything is smooth. Okay, so somehow we want a test that doesn't. How we want a test that doesn't care about random collisions at extra points. And okay, that's what happens. So here's the approximate new strategy. So I'm basically going to say this is a different way of detecting tangency. The main point is that tangency can be expressed in terms of closure. This is not a new idea at all, but somehow it's it was not clear that it would work and it worked. That it would work, and it worked. So say we have two families of curves, and I need to take sort of maximally generic tangent intersections. And what I mean by that is that you take a generic curve from one, a generic curve from the other, and a point which is generic in each of the two curves. And okay, then it's a non-transversal intersection, so tangent spaces overlap. You can write that down in a closures table. Statement. So the set P is going to be, you look at all the pairs of curves, and to each pair of curves, you assign pairs of distinct intersections, just like the set of pairs of intersections. And to say that you have a double intersection at this x hat is saying that you have pairs of intersections converging to the same point written twice. Yeah? So, okay, these two things are like the x and x prime. They converge to the x hat, x hat. Questions on that? What was it? FR? P? Oh, frontier. Sorry, yeah, frontier. It means closure minus the set. Okay, so there's a natural translation from a non-transversal intersection to a closure statement. I mean, I'm not saying why that is, but it's like standard and not very geometrical. So I might say later. Okay, so that's. Okay, so that's the main observation, and then the goal is just can we prove something about the structure M can recover frontiers. So if you could take your tangency, convert it to a statement about frontiers, then prove something about the ability to detect frontiers, then you'd roughly be be done. Okay? That okay, that that works, but yeah, that works. Say that works. The result is that you get a result on definability of tangency, and it's a really nice thing because it works always. So, in the one-dimensional case, I think you kind of have to work somewhat hard to arrange some families where you can detect tangency. And here, you just get all families, and it doesn't matter what the pens is. Okay, so that's the strategy. You want to detect frontiers. Frontiers, you want to use that to detect tangency, and then you want to sort of go from there. So I'll start by just stating precisely what the results say. This is a really weird proposition. Any either. Okay, so embed the universe into an affine space for constructible subsets of For constructible subsets of affine space, they're the same. The closure of a constructible set. Yeah. Okay, so this proposition is sort of the main innovation, but it's kind of confusing. So I'm going to state it and then go back to it a lot. So you have a definable set in M, and I need it to be definable, like I need it to be in M to the N, and it has some rank R, and it's And it has some rank R, and it's definable over A. And then you take a point in the closure and assume each of the coordinates is generic, and by that I mean in the sense of the field, so it's like a smooth point. So assume each of the coordinates is generic over the empty set, then firstly a natural thing happens, which is that the Morley rank of the whole point over the parameter set is at most r. So this is sort of like a weakly detecting a frontier point. A frontier point. So the frontier points are sort of, you'd like to say you have a strict inequality, but the weak inequality is not so bad. But you want to say that actually the weak inequality can sometimes be a strong inequality. You get this really funny condition when that works. So, okay. Suppose you have an actual equality. What can you say? If you pick an I and a G, If you pick an i and a j, what is the next? Closure. Closure. Sorry, closure. Okay, so if you have a quality and you take any i and j such that the ith and jth coordinates of a generic point of x are independent, then the ith and jth coordinates of the closure point are independent. All in the sense of m. It's a kind of confusing statement, yeah. So I'll go back to it, and one of my main goals in the talk is to try to help that statement make more sense. Okay, but this is the main thing. The proof of this is horrendously long, and it's sort of the main tool for the entire project. Okay, and the point of this condition to, again, what it really does is it allows you to sometimes replace weak inequality with strict inequality. Replace weak inequality with strict inequality. Because it says if equality happens, something funny happens. Okay, and okay, again, neostability. If you're over ACVF or you're over no minimal field, this actually works. You get exactly the same statement. Proof is slightly different, but you get exactly the same statement. Okay, now you use that to prove this. So suppose you have two zero-definable faithful families. Faithful families of plane curves. So faithful means plain curves have finite intersection. And you take some curve in one, some curve in the other, and some point in the intersection. And I basically want to say every possible thing that could be generic is, with one exception. Okay, so one curve is generic in one family, the other curve is generic in the other family, the point is generic both in the ambient space and in the Both in the ambient space and in the one curve and in the other curve. But I'm not saying anything about the dependence between these two, because then I'm going to say, okay, they intersect non-transversely. So these two tangent spaces intersect inside the tangent space here. Okay, and again, this is like something I saw said. Notice that because I said this point is generic in all three of those sets, it has a well-defined set. Of those sets, it has a well-defined tangent space, and you can naturally see these two as contained in this, so that makes sense. Okay, so assume all of that, then the two curves fork in the sense of M. Okay? And the nice thing here, it's any families of curves at all, and it's completely insensitive, or it doesn't matter what the dimension of m is. The same proof works the whole way. Okay. Okay, and then what about variance over A C V F r rh minimality? Well, the same thing holds, but you have to be careful about what you mean by tangency. Tangency has to be replaced with topological multiple intersections. That means this. That means two things converging to one. Okay, so if nearby curves intersect more than once in a neighborhood of the point, Uh then this becomes true. How how do you define the tiny space? So is it in M or in these are constructible sets and I'm working only at generic points. So in a in a neighborhood it's just a smooth variety. Take the tangent space of that variety. No, no, but the standard spaces are calculated in structure, M or the original field. There's no such thing as a tangent space in M. It's just some random strongly minimal. The original field. Yeah, yeah, it's in the original field. The whole point is that the conclusion says that this is something that the redundant can detect, despite the fact that the information is very non-obviously definable in the screen. So you have no idea what your structure is, you just know it's not locally modular. Just to be sure, T cat and U cat fork means which are dependent? Yeah, it means they're literally they fork over the MP. In this M is strongly minimal. The types fork, okay. Yeah, the types fork. They're independent. They're not independent. Yes, okay. Yes. Yes. The type of T doesn't fork? Sorry, the type of T forks and the type of U forks. No, because I don't know what the ranks of these standards are. They're not in R is great. But isn't M stronger than M? Yes, but these are just some tuples in some potentially really big sets. I just mean, okay, I mean the Morley rank of this pair is less than the sum of the two lines. Oh, okay. Over the empty set. Okay? Okay, so I mean this is problematic theorem because if the tangency is expressed in the original structure, then this theorem doesn't seem to say s something purely in the structure M. Well exactly, there there's there's no such thing as as tangent space in the original structure. But do you have like if one two three holds then it's false implies the tendons intersect number three? Oh no. No, because I mean I mean i i look in the in the end you're interpreting a field, so you expect that the structure M is approximately just an algebra of those fields. So there's a lot more ways that forking could happen in tangents. Working could happen in tangents. Okay, any more questions? So I'm just trying to say that: okay, assume a bunch of things are generic so that you're only working at smooth points, and then take two plane curves, which intersect non-transversely at some point. Then those two curve indices are dependent in the sense of the strongly minimal structure m, meaning that m detects what's going on. Okay? And you get this. And you get this by this previous thing I said. So this is the tool that you use to prove this. Okay. So now, okay. What else do I want to do? I'm going to quickly summarize, okay, I hope quickly summarize how you actually proved the theorem from this tangency detection result. But, okay, it was not surprising that you could do this because it's sort of the outline. That you could do this because it's sort of the outline of what happened before, but there's some differences, so I'll try to highlight those. And then after that, I'll spend as much time as I can sort of talking about this closure proposition and tangency detection theorem. Okay, so how do you know the dimension is one? This is something that was not known before. Well, there's this fact in algebraic geometry, the purity of the ramification locus, which, okay, I think it's maybe slightly more general than this, but this is what I. It's maybe slightly more general than this, but this is what I need. If you have a morphism of smooth varieties of the same dimension, you take the set of ramification points, that means where the derivative goes, the derivative is not injective, sorry. Either that set's empty or its dimension is pretty big. It's codimension one. So it's either empty everything or codimension one. Well, non-transversion intersections of plane curves can be seen as ramification points. Curves can be seen as ramification points. If you take an appropriate map, if two tangent vectors collide, you can use that to construct a certain projection, which gets you a non-injective tangent map. So you have a translation between non-transversality and ramification. So basically, you set up an appropriate map. You have to construct one ramification point, but that's really easy because you do that by intersecting a curve. Really easy because you do that by intersecting a curve with itself. Once you do that, you apply this purity of ramification, and you get that basically there are non-transversal intersections in codimension one. Okay? But if you have this tangency detection thing, then you can see that in the structure. Okay, so what you get is there's some definable set in the reduct, which is saying something about tangent intersections, and it's going to have codimension one. So m defines these two sets. Defines these two sets with dimensions differing by one, and you kind of play around with that a little bit, and you get the dimension to be one. It's not so hard to go from here to here. Okay, so this is actually fairly straightforward. That's much easier than interpreting a field. And in the variance and ACVF nominality, it's the same idea, but you have to work harder. You particularly have to work quite a bit harder. Particularly, you have to work quite a bit harder in a minimality, but it ends up working. How do you get equi-dimensional varieties? How do you get equidimensional varieties? I mean, the projection that you take is... Okay, so you have like, here's one family, and then here's another family. And then you take the set of intersections, family of intersections. Okay, so this is the. intersections, okay, so this is the something like X T U where X is in C T intersect D U and you project I to T times U. And that's, this is the map where you have a ramification point. You might have to sort of delete some bad points, but generically, two things here have a finite intersection here, so you have the same dimension. Okay. All right. So now, okay, a little bit, yeah? How do you know that the generic or generic in the sense of the Zariski topology either generic or the current? Oh, why is it you saying why is a generic in the sense of the field, a generic in the sense of the s the weaker structure? Of the weaker structure? And you will feel n, right? I mean. Yeah, yeah, yeah. So you can track dimensions of definable size. It's an easy lemma that if x is m definable. Inside m, yes. Yeah, x is undefinable, then the dimension in the sense of the field is just. In the sense of k. Yeah, in the sense of the field, is just a scaling of the Morley rank. So it's the dimension of Of the universe times the Morley rank of x. Okay. Okay, and that lets you compute. Okay, so in particular, if this goes down, then this goes down. Okay. Thank you. Okay, so a little bit about how much time do we have? Here's like two minutes about what's different in the construction of the field, okay? We want to use the historical strategy, but I'm only talking about tangencies at generic points, and historically you always use a point on the diagonal, because you want to be able to compose curves and have the points still be there. Points still be there. Okay, so there's something weird going on because you only have generic points. And so, yeah, the operations aren't well defined on a family of curves that are generic. So, what do you do? You just don't pass to a subfamily. You take a rank 2 family and you consider all tangent intersections of all curves in the family. So, a slope now means a linear map between different tangent spaces. And I like to think of it as something like instead of getting a group, you get a group weight, then you. Instead of getting a group, you get a group where then you specialize to a group. So, how do you do this? Well, if you have a generic curve in your family and a generic point on that curve, you think of the tuple as a code for the slope, and by the slope, I mean the tangent space to CT at this point gives you an isomorphism between these two tangent spaces. That's the slope we're going to say. So, you think of TAB as a code for that slope, or if you want, you can add in. Or, if you want, you can add in all of the realizations of that slope. And then you prove that you can detect operations, but in a group-away sense. So now you have going from a point B to a point C, you take a map here and a map here. You compose the two, and then you can check that the code of the composition is algebraic over the codes in renewables. And this is almost trivial because, okay, why is it true? It's because if you. Okay, why is it true? It's because if you have a plane curve realizing this and a plane curve realizing this, you compose them, it's going to realize the composition. And so by this tangency detection thing, the codes fork, or the types of the codes fork, and you kind of compute what that means, and it gives you exactly the statement. Okay, so this is actually somewhat easy. Also, you can do addition, and this is surprising because you don't have to get a group before you. You don't have to get a group before you do addition. So it's a similar idea, but it's harder because you don't have a group. But what you do is you just pretend that you have a group. You take some set gamma in m cubed of rank 2. You just say, let's pretend that's a group operation. And you plug it into the same definition of sum from before. And what happens is the higher dimensional chain rule is still true. So you still get some sort of sum of slopes, but now they're sort of scaled. But now they're sort of scaled by some things coming from the chain rule. So you have a linear combination instead of a sum. But you kind of play with it a little bit and you can fix it. So it's something like you get a linear combination of the two slopes and then you sort of compose with something that cancels it out or something like that. Okay, and then at the end, okay, you have this detection of composition and addition, and think of it as like you have a tangent bundle configuration. You have a bunch of tangent spaces. You have a bunch of tangent spaces, you have all the maps between them, and you can compose and add. And then you play with that, you get a field. Okay, and in particular, you don't have to go through a group, which is kind of nice. Okay. So possibly the most important thing I could do is tell you about why this closure proposition makes sense. Maybe what I should do is remind you of what the closure proposition is. So this was: you have x, m definable over a, and you have some point x hat, this is a big x, in the closure. Then what you get is that the rank, oh, and all coordinates are generic. Okay, then the rank of the whole point over A is at most the rank of Is at most the rank of big x. And equality implies this funny condition that if for all ij and xi is independent in the sense of m from xj, okay, let's say over a in the sense of m for generic x bar n. X bar and X, then same for X pattern. Okay, so this is the statement. I want to attempt to tell you why that is somehow relevant to what's going on. So suppose we have these two families and a generic non-transversal intersection. Kind of play around with it a bit. So you can assume after sort of doing some elimination of imaginaries that T and U. Elimination of imaginaries that t and u are just powers of m. The reason you need that is for this, all these things have to be generic. And then you take the same set I said before. So you take pairs of distinct intersections, and you notice that you put this point twice, it's like a double intersection, it's in the frontier of those. You have to check that that's true, but it's not hard. Okay, then you further have to check that you can take the rank of this set to be the sum of the ranks of the two families, and this is what. Sum of the ranks of the two families, and this is what Frank was saying. So it's like the each pair, one thing from here, one thing from here, should usually have a finite intersection. You trim away the places where that's not true, and you can assume this is true. Okay, so what's the goal? You have these two points, and you want them to fork, be dependent. So you want the rank of the pair to be less than the sum of the ranks. But, okay, little t is generic, and big t, and same with the u's. Big T and same with the U's. So that's the sum of the ranks of these two sets, and that's the same as the rank of P. And the whole tuple here with the X's is algebraic over TU, because it's a finite intersection. At least we can assume it is. So it's the same as just saying the rank of this tuple is less than the rank of P, and it's a frontier point, so that's what you expect. Okay, so that's the translation. So it's somehow enough if you could say that frontier points have lower rate. Frontier points of lower rank. Alright. Well, so you try to show that. So there's some past papers which did similar things in the o-minimal setting. So you have a strongly minimal group interpreted in an o-minimal structure. All three of those papers said some variation of if you have a point in the frontier of a plane curve and the rank drops, meaning that it's algebraic. Meaning that it's algebraic. So I kind of want to do the same thing. Like the initial goal is: can we do the same thing, but for all definable sets and without a group operation? That's way too ambitious. It's very hard. But the first kind of miracle is that you can get the weak inequality. So there is an adaptation, it's a very long adaptation, but there is an adaptation of the argument, but you get the weak inequality. Of the argument, but you get the weak inequality. Okay, and you need this, all coordinates are generic, but that's not so important. But the weak inequality is useless. Okay, we, this computation I did before, you really need the equality, the rank to strictly drop. So what do we give up? Well, the attitude you should take is that we have one witness to the rank, which is the rank has to be at most the rank of p, but if we could find some other special property of the point, we could make the rank. The point, we could make the rank potentially go down by one and then it would strictly drop. Okay, so it's like you have some partial information, but you need one more drop in the rank. Well, what does the partial information give? It means that, okay, outside of non-generic coordinates, you can contain the frontier of p in a set of the same rank. Okay, so think of q as like m's sort of approximation of the frontier of p. Sort of approximation of the frontier of P. If we could show that this point W hat is not a generic in Q, then you'd be done. Okay? Because that wouldn't ring drops. Well, it has an obvious special property. It has two pairs of equal coordinates. So x hat is a point in m squared, so it's got two coordinates, and it's the same point twice. So there's two pairs of equal coordinates. And so let's see if that's a special property that drops. Let's see if that's a special property that drops the rank. Okay, so you take h to be the set of points which have those two equal coordinates. Then you intersect with q and hope it drops. Why would you expect it to drop? Well, q is some sort of reflection of p, and you can show that, okay, after maybe tweaking a bit, h does pick out a proper subset of q, or a smaller subset of q for the p. Okay, so h is just this hyperplane thing. Is just this hyperplane thing. So you can, okay, all these things are you can arrange, so okay, you have to tweak some things, but you can arrange that H picks out a small portion of P, and so hopefully that transfers to Q. Seems reasonable. Okay, so you rewrite the goal in this way. So the new goal is you have an undefinable set X. Can we contain, outside of non-generic coordinates, can we contain the frontier in an M-definable set Y so that the ranks So that the ranks are the same, but also if you take a zero-definable hypersurface in the sense of m determined by only a relation in two variables, then y is not going to somehow jump inside this hypersurface unless x already did. Right? And then you run the same proof, but you keep track of hypersurfaces. Okay, so you have to do the entire proof again, and at every step, you have to. And at every step, you have to do a few tricks to think about: okay, if there's a hypersurface here, then can you arrange the set Y to avoid that hypersurface as long as X did? Okay. And what do you do? Well, you have your frontier point and you show that in the set P that I was looking at, you can arrange that those two coordinates are generically independent. That those two coordinates are generically independent, which is what you want. Because then you can go back to the statement and you say, okay, these two coordinates are generically independent. So for the closure point, they're also independent. And by the statement, if equality holds in the ranks, then they would be oh sorry, if equality holds in the ranks, then because those two coordinates are generically independent in the set, that would pass to the frontier point. That would pass to the frontier point, but it doesn't because they're equal. Basically, you have equal generics, they're dependent. So that means that the rank has to strictly drop and then you're done. Okay, now how much time do I have? Okay, then I stop. All right, any other questions for that? Alright, any other questions for that? Once you have dimensionless one, but you can just use a user and discuss it. Yeah, you can. But it's like a feeling to have a self-contained thing where you use the same tool set to do the higher-dimensional pasting and the one-dimensional paste. And then you prove that this is a normal case. Well, it's an okay, so M interprets a field, then so does the background field. You know, it's like K interprets M interprets F. That means that F and K are isomorphic according to K, but so M might not be bi-interpretable with K. But yeah, it's a copy of the same K. We'll be again in 10 minutes. Yeah, five.