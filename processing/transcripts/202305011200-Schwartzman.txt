Perfect. All right. We are on. So Mu invited people who wanted to do topological data analysis. So I want to do topological data analysis. But I know how to compete with GPT. I realize that. All right. So this talk is more about inference and maybe geometry, less about topology. About topology. But before I start, I do want to say that I'm super excited to be here. And I imagine that most of you are. And well, not only because I think it's the first conference that I attended the last four years or something like this, right? Especially such a nice place and it's such a great group. And yes, I'm actually grateful to be able to tell you about what some of the things that my group has been up to in the last few years that we feel like they never. Few years that we feel like they never happened, but we still work throughout it somehow. So, I want to tell you about spatial inference. So, what is spatial inference? So, I was thinking in general statistics, we think of two modes of inference, testing and estimation. In testing, we answer the question, a question which is of the type of yes or no. Is the null hypothesis true or is it not? In estimation, we try to answer this question: how much, what is the Answer this question: How much, what is the value of this particular parameter? So, spatial inference tries to answer a different question, which is the question of where. So, where things are. And what is a best example of a question of where things are than in brain mapping, right? Like, for example, in fMRI. So, what do we do in fMRI? So, the classical analysis, which was already started by Keith Horsley 30 years ago, is a multiple testing approach where we Where we test for activation at every point in the brain. It's like going to every voxel and say, voxel number one, are you active? Voxel number two, are you active? Voxel number three, are you active? Like that, 500,000 times. Is that efficient? Well, it's worked so far, right? Pretty well. But there's a difference. Something has happened in the last 30 years. And one thing that has happened is that data sets have gotten much larger. And of course, that's good for many reasons, but statistically, what Many reasons, but statistically, what does that mean? So, you probably know that if you try to test an all-hypothesis, a continuous hypothesis when theta is equal to zero, what happens is you increase n? Eventually, you get a significant result, right? Whether it's important or not, it eventually gets significance. And that's what's happening now. So, here's an example of the UK Biobank data set, and Mark was talking about it too. Take a thousand. Too. Take a thousand subjects here as a particular contrast, faces, minus shapes. You do this test and control the final device error at 0.05, and you get this SWAT of regions that are active. And so it's not giving you enough localization. If you increase the sample size, you get even more. Anyone who really wants to interpret these results would not really think that, would not care about these entire regions, would really focus on the ones where the activity is really high. So there would be an additional level of selection there. Right, so there will be an additional level of selection there. So, can we get better localization now that we have access to bigger data sets? And that's what these scope sets are about. So, it's about targeting directly things like, for example, where is the percentage bold signal, meaning the difference between these two conditions, greater than 0.2%. And these are the regions that we get. So, there's much more localized. You can identify what are the anatomical regions that correspond to them more precisely. Respond to them more precisely. So, what are these copesets? So, copesets are coverage probability excursion sets. So, what is the goal? We want to estimate the set where the mean function is greater than a threshold. That's the general setting. 0.2% bold difference, let's say. So, we're producing three sets here, the yellow, the red, and the blue. The yellow in here in the middle is the point estimate. That's where we think that the signal is greater. That the signal is greater than the threshold, but the point estimate is not enough, right? We want to get an assessment of uncertainty. What is the difference between somebody who knows statistics and somebody who doesn't? Somebody who knows statistics puts standard errors on their estimates. And if our goal is to estimate spatial extent, we need to put standard errors on the spatial extent. So we're doing this by these sets. The red is the inner set that tells us that the set that we're looking for is at least as large as the red set. As the red set, but no larger than the blue. So it's giving us bounce. And the idea is to be able to claim this with certain confidence, let's say 90%. All right, so what do I want to do for the next 20 minutes that I have? I'll give you a little bit more detail on the COP sets. I'll tell you about the performance in practice, some interesting issues, and I'll go to simultaneous copesets at the end, which I think is the most exciting thing that we're doing now. All right, COP sets, mathematical sets. All right, COPSETS, mathematical setting. Let's start with S being a compact domain. Let's say the brain doesn't have to be connected, so you can have two hemispheres which are disconnected, and that's fine. Suppose we have a means function mu that we're targeting that we want to estimate. So this could be a coefficient in a linear regression model that corresponds to that contrast, for example, between those conditions. So that's mu. And you have an estimator mu hat. How do you get the estimator? Well, you set up the model. How do you get the estimator? Well, you set up the model, you fit the model, you get muhat, right? All right, what are we trying to estimate? This set AC is the locations where mu is greater than c. Easy. If we have an estimator mu hat, then how do we estimate it? Well, just plug in, right? Get all the points S where mu hat is greater than c. That's a hat, that's a yellow set. Easy enough, right? Done. Well, the difficulty is in how do we assess? difficulty is in how do we assess the uncertainty on this. So we want to produce a red set, a hat plus, and this blue set, a hat minus, in such a way that the true set that you're looking for is contained in between the red and the blue with certain confidence, let's say 90%. How do we do that? How would you construct those sets? Let's think about it for a second. So here's the construction that we came up with. So I just told you, let me go back, that Told you, let me go back that the yellow set A hat is obtained by just thresholding mu hat. U hat is greater than C. Then one thing that you can do is look at the image from the side now. So here, this vertical axis is just the intensity, and mu hat is going up and down like this, right? And the black line here is the level C. So if you threshold your mu hat at level C, you get the yellow circle. That's all the points where mu hat is greater. Now, take this line and just go up. Take this line and just go up a little bit by some delta, and you get the red set. Go down by a little bit, some delta, and you get the blue set. So the blue set is containing the yellow, containing the red, so you get the nested sets. What is this delta? So delta is going to be a certain number of standard errors. Some a. A is this one tau number of standard errors. How many standard errors? 1.95. No, right? It has to be, but it has to be something larger than that because we're controlling error. Larger than that, because we're controlling error. So, this is the main theorem that we have in the first paper that started this line of research in 2018 that determines how this is done. So, the idea is the following. We need some assumptions, which I think are reasonable. So, you need the function mu to be continuous. It has to be not flat at level C. And that is so that you really have a crossing at that level. So, that's important. You have this continuous, consistent, in asymptotic gap. Consistent and asymptotically Gaussian. And I think the regression models that we usually fit, I think, are okay with that, the asymptotic Gaussianity. And the theorem says that if you evaluate the probability of containment that we want, that the red and the blue set bound the true set in some way, that probability converges to the distribution of the supremum of this random field G. So what is the G, first of all? G is the limiting field. The limiting field of mu hat. So if you take hat and you standardize it properly, then it's going to have a Gaussian distribution at the end. So G is standard normal marginally, but it has a spatial correlation to it. Yes, Martin. Very good point. In this construction, we did do it symmetrically because Symmetrically, because the situation where we have convergence to a Gaussian field. Right, but what if there's no activation? You probably wouldn't want a symmetric. So the symmetry here comes from the noise distribution. But we do have a, I'll show you at the end that we wanted to do this with effect sizes and coins D, and then you get skewed distributions, and then symmetry doesn't work anymore. Yeah, that's a very good point. All right. So that's what G is. What g is, and we have to take the supremum over the boundary of the set AC. So, two interesting things about the result, which I think are quite cool. One is, why is it the boundary? Why the boundary? So, if you think about it for a second, then maybe going back to this picture, if you are way inside the red set or way inside the true set, then the signal is going to be high enough, and it's probably going to be inside anyway. Probably going to be inside anyway. If you're far out, then it's probably going to be out. So, where do the errors happen? Where is it that you have no inclusion? Where do you violate the coverage? It happens exactly here, right? Where you're very, very close. Where the signal is very close to the level C itself. So here your signal noise ratio is higher. Here's where your signal noise ratio is the smallest. So that's why you get the boundary. How do you implement this then? So if this is the So, if this is the limiting distribution, if you want that probability to be at the level 1 minus alpha, then take the right-hand side and set that to be 1 minus alpha, which means that you just need the 1 minus alpha quanta of this distribution. That's all you need. How do you get that distribution then? Well, you need two things, right? You need the boundary. Well, you have a hat, right? That's a yellow set. Just take that boundary. Good enough. That's easy. The part that is a little bit harder is... The part that is a little bit harder is to estimate the distribution of this suprema. And I'll tell you a little bit more now about how we do that. So, for those of you who know family-wise array and Mucha, you're probably thinking, yeah, of course, Causicinematic formula, right? That's how you do it. Well, that's one of the methods. So, going back to the example, let's say you have a first-level, second-level regression model, and you fit this regression model at every point in the And you fit this regression model at every point in the brain. I'm going to skip the details because I really would rather focus on the concepts today. But in general, the general recipe for this procedure is that you fit this signal plus noise model or point-wise regression model. In fact, actually, if you think about it for a moment, you realize that any regression model, point-to-regression model, can also be written as a signal to noise model, if you write it correctly. So they're almost the same thing. Write it correctly. So they're almost the same thing. What is important, though, is that you can get residuals from that fit. That's where the regression helps, that you can get residuals. Why are those residuals important? That's because remember that this limiting field G has a spatial covariance, spatial collision structure to it. And we don't know what it is. But the residuals know. It's embedded in there, right? And so that's what you do. So you use this standard. And so that's what you do. So you use this standard procedures, you standardize them, and based on that, you can estimate the distribution of that supremum. And again, once you have the distribution of supremum, just set the one by itself at point out. Yes, question. Bootstrapping a supremum to the ID is to get a little bit of easy about it. I know, I know, I know. I'll make a comment about that in a moment. But before I do that, just a few simulations just to go through this. I don't, the details don't really matter, really. For example, here. Really, for example, here's a three bumps with an excursion set we're trying to estimate, and we added some non-stationary noise that is smoother on one side, less on the other side, and temporal correlation that may be changing over space, adding non-stationary Gaussian noise, Laplace kernel, Laplace noise, all kinds of things. And it works. That's the bottom line what I'm trying to tell you. You increase the sample size, the copsets get tighter around the two sets where they should be. Tighter around the two sets where they should be. And then for basically already 100 subjects, you're already at the confidence level that you need. All right, so it's great that it works. But should it really work? I was actually surprised that it did. For several reasons. One of them is that I already said this method does not assume any kind of spatial stationarity. And that's important. The covariance can be quite arbitrary. I think that's good because most of the methods that I know about, in terms of family-wise arrayed control and multiple testing, or even cluster inference, they need to assume that you have some stationary noise. You can really estimate those parameters. We're not doing that because we're not estimating the parameters of the noise at all. We just need the distribution of the supremum over the boundary, right? Over the boundary, right? So, why I don't be honest, I don't understand really why is it that do you have a completely arbitrary correlation structure? There's no hope of estimating the correlation at all because it's arbitrary. So you have many more parameters than data points, right? The sample size, even is it thousands, is still not enough to estimate it. No hope to estimate at all, but somehow we can estimate the distribution of the supremum, even though despite. Even though, despite Mark's doubts about it. So, how to do it? We explored several methods. The multiplier bootstrap seems to work the best. So, how do we do a multiplier bootstrap here? The Gaussian multiplier bootstrap, for example, works by taking the residual fields and multiplying by Gaussian multipliers and then taking averages. That helps because if we know that our limiting field should be Gaussian, then why not try to see? Shield should be Gaussian, then why not try to simulate the Gaussian process already right away? But we use the residuals in there because they encode, they somehow have the right covariance in there. And that's how this works. Now, something surprising is that you can change the Gaussian multipliers by the Machine multipliers, which are plus minus one, runs much faster. It's not Gaussian, and it still works. And I just attended a few lectures. I just attended a few lectures on the bootstrap by Dimitrius Politis at UCSD, and he makes a big deal about the fact that the bootstrap has been proven to work with linear statistics, and it shouldn't work with things like the supremum. So I don't have an answer to your question, Mark. But it does. Maybe one reason for it is that perhaps it is somehow a smooth function of the data because another method is. Another method is the Gaussian kinematic formula. So that is the formula that Mu was appealing to, where you can express the distribution of supremum as expected or characteristic and be written as an integral of certain constants that can be estimated. So in fact, this actually is sort of a low-dimensional quantity that can be estimated. I think that's a reason why this works. All right, moving on. I'm telling you all these good things. There must be situations where the method doesn't work. Well, here's a situation where it doesn't work. Where it doesn't work. Spatial resolution is an issue. Everything that I told you about is for smooth domains, continuous domains. Images always come in a discrete lattice, right? And one thing that we discovered as we were doing this work is that our coverage was always too high. Over coverage, overcoverage, overcoverage, coming down from above. And why does that happen? Why does that happen? Well, it's not a bad thing to have overcoverage, right? But it's not efficient. Well, the issue is actually best illustrated. So, once we here, this simulation just shows us you increase the resolution, you get to the right place. The reason for it is essentially this one, that the boundary where the errors are happening sort of falls in between boxes if you have a discrete lattice. As you increase the resolution, you can really. As you increase the resolution, you can really tease out better where the errors are. In this example, this is a ramp signal where the signal is going up from left to right. The true boundary is the vertical line. So for a low resolution, it may look like the no errors in here. There's no overlap. But if you go with the resolution, you would see that, ah, there's a violation there. And the most important issue, I think, of it all is more of a conceptual one. They give me a chance to smile before this picture. How to choose this level? I told you, 0.2%, 1%. How do you choose this? This has been a big criticism of this work. Now, we as statisticians, we tend to think that if We tend to think that if there's some tuning parameter in a procedure, right, we say, well, we let the it depends on the context, right, in the problem. So let the sentifact expert decide what that value should be. Sadly, I've discovered with enough, I've got enough experience to realize that they don't know. They don't know what to choose. What are they doing instead? Well, they will try 0.2%, they will try 0.5%, they will try 1%, right? Eventually, every 1%, right? Eventually, choose the level that makes the pictures look the best and fits better with the anatomy, right? And we know that that produces, that's a selection bias problem, right? This is bias. So the errors are not controlled. So what do we do? Then one solution is something that I learned from Yelly Gemman and the selective inference people, which is that you make the procedure robust to that particular action, which is in C. So you try to come up with a position. So you try to come up with a procedure that is actually that controls the error simultaneously for all choices of that parameter, simultaneous over all C. So that's something that we've been working on so far. And before I show you the picture, we investigated this. So the idea is the following. How do we produce scope sets in such a way that no matter which level C you choose, you always get the coverage that you want? We just need a different plant. Buanta, maybe a little larger. What should it be? Well, the solution turned out to be surprising to me, but then once you think about it, it kind of maybe seems obvious. I'm not sure. It's super simple. What you do is that you threshold confidence bands. So if you have a signal, again, looking at it from the side, so let's say the black line here is the estimated signal, and you have a method, a way of constructing simultaneous confidence bands for that function, which is given by this. Which is given by this gray area here. Then take a threshold, let's say the threshold is zero, for example. The places where the black line exceeds the threshold, that's the yellow set. Here it's in green, but it's the same thing. That's the estimated set. Wherever the line crosses the lower confidence band, then you get the red set. Whenever it crosses the upper confidence band, you get the blue set. It will be this set right here. It will be this right here. You can do it in a threshold that you want. It turns out that if you have a simultaneous confidence band at 95% level, then threshold in this way produces scope sets that give you simultaneous spatial coverage at the same 95% level. This is quite remarkable. And so, what are the advantages of this? Well, by doing it this way, investigators can try several levels. Actually, it's okay. Can try several levels. Actually, it's okay. Fine. Try whatever levels you want. Choose the one that you like the best. Errors still control. And not only that, but you can, it makes it very general because as I said, if you have a method for constructing confidence bands, then you can threshold them. It gives you a way of also visualizing confidence bands. That's another thing. How do you visualize simultaneous confidence bands in 2D or in 3D? Ever thought about that? In 3D, ever thought about that? In 1D, sure, you're just trolling, right? How do you do it in 2D or 3D? Well, how do you visualize surfaces in 2D or 3D? Level sets, right? So the same thing. So now you have level sets of configures bands actually have a meaning in terms of spatial inference, which I think is cool. Downsides, of course, I need to give you downsides. This is more conservative for sure. Anytime you want to allow the user to have more freedom to do things, you're going to have more conservative inference. More conservative inference. So there's always a price to pay. I want to finish up the last section, just giving an overview of the various aspects of this work that we've been doing. The original paper came out in 2018. We only had a, that had a theory, but we only had a 2D application at that time. The original problem, I didn't tell the story, but the original problem actually came from climate data in trying to estimate regions. Trying to estimate regions, geographical regions where there is an amount of temperature increase that is greater than two degrees centigrade. So it was a climate problem, really. And then Tom Nichols saw it and he grabbed me immediately and said, no, we have to apply this to neuroimaging. And we started working on that then. And that has been most of my work since then, applications to neuroimaging. So we were able to apply this through in 3D. This is through in 3D. So, in 3D, things get just a little bit more difficult computationally, graphically. The spatial resolution issue also becomes more of a problem, really. And then we try to apply it to Cohen's D effect size images. So, Cohen's D I mentioned that before to Martin. That was one solution to the level problem, because one idea was, well, maybe we don't know what is 0.2% bold, but we know what in effect. 2% bold, but we know what an effect size of 0.8 is, right? So maybe that would be better. That turned out to be harder simply because coincidence has astronomy distribution and doesn't covers a Gaussian so fast. What else? Intersections, the use of conditions. Is a brain area responding to both auditory stimulus and visual stimuli? For example, both at the same time. So this is interesting. Simultaneous COPSITs, that's what I explained to you. So we have two papers. That's what I explained to you. So, we have two papers: one which is finite samples, another one which is asymptotic in connections to testing. And this is exciting. We're actually developing this quite a bit. So, things that we're working on, confused regions for peaks, for example. That's another feature. It's a topological now. Finally, I get to say the word. Peaks are topological features that identify where is the largest amount of activation in a region. And we can identify them, but where is the spatial extent? How much? But what is the spatial extent? How much should it move? Accuracy, spatial accuracy. Super resolution fields. How do we increase resolution in images? Even if they come at a level of two millimeter voxels, can we increase the resolution to increase accuracy? Everything we can. COPS is for clusters. Everything that I just told you about is for excursion sets. So wherever the signal is greater than some amount. But can we claim the same thing about individual clusters? We think we can. If we can do this, I think this is very exciting because it would be. This thing is very exciting because it would be a completely new way of doing cluster inference that doesn't require stationarity of the noise. It can give you a spatial accuracy for every cluster. So I think it would be cool. And connections to testing and everything else. So to summarize, the main idea was that I want to do spatial inference and answer the question of where the effects are. I want to replace testing by directly estimating those excursion sets and estimating those clusters. In terms of the construction, the reasonable construction. In terms of the construction, the original construction was taking the level C going up and down by a quantal, which you can get by the supreme of that distribution, for simultaneous COPSIT, just stression of simultaneous confidence bands. And I think this, I hope I've convinced you that at least for me, for us, this work has led to many interesting things about not only brain imaging, but about just inference in general. And I definitely want to recognize here more. Recognize here, more than recognize, just tell you that there's all these wonderful people that I've been working with. It's been great, and I hope to continue to work with them and some of you perhaps in the future. Thank you. When are y'all getting hungry? But let's take a few questions. Martin. Yeah, I had no idea what time it was. I didn't see your five minutes warning. About four or five minutes. All right, good, good. All right, good, good. 12:05, it's 12:31. So, all right, great. So, it's really exciting, and I think it's similar to my talk will follow up. This is gonna be a good follow-up. Oh, perfect. I wonder if I would do something because we know something about the brain, we know something about boundaries, we know something about the spatial. We know something about the spatial. So, can you incorporate that maybe using like a probabilistic atlas that says the likelihood that you're that you've crossed over as some sort of prior in refining these sort of bounds? Does that make sense? Hello? Yes. Mandy just left, but I think Mandy has priors, right? She has priors, yeah. I think Mandy has a version which is more based than WCF. Based on in WCF, something similar. Similarly, in this context, good point. Let me think about that. Definitely, very, very good idea. Yeah. So just a priori, I would say that you could, you could, yes, maybe. You could, yes, maybe restrict the domain. Let me get back to you. Okay, thank you very much for the nice talk. It was very stimulating. Oh, great. So I am Anna Segubi from Caust. I'm working with Professor Hernando Umbao. Oh, yeah. Okay, so I have some questions related to identifiability. You didn't mention anything about identifiability issues. And the reason I ask about this is intuitively, I think that those kind of settings are not. Think that those kind of sets they are not unique, so they're not unique. I mean, you can deform them a little bit and they will still give you the same level of confidence. So, like, did you think, I mean, did you think about this kind of thing? So, can you clarify which way they're not unique? So, for example, you still, so you're just looking for the probability that the true function is above some level. Yeah. Okay, so that. That region that contains the higher region can be deformed a little bit in one side and you can compensate from another side. Yes, yes, yes. Excellent point. Okay, now I see what you're saying. So let's bring back some picture here. So I was thinking maybe you need to do some kind of regularization or something. Yeah. So the claim so far that I'm making is for the entire excursion set, right? Defined as any point. So the set would be any point. Any point, so the set would be any point where the signal is greater than the threshold, and that set could have disconnected components. I have topology to it, right? It could be formed of this. For example, the red set here is formed of all these pieces. So it could be disconnected. And it's true that because we're just computing here the probability of inclusion overall, it could be that you could get an error here or an error there, and you would not be able to distinguish them. And you would not be able to distinguish them. Yes. So, what you're asking about is precisely where we want to go next, which is to be able to identify specific clusters. So be able to claim that perhaps you're controlling the air rate pretty well here, but maybe not there, right? Because it could be also that this spatial correlation is different spatially, right? Or it could be that your thin is sharper here than it's there, and you feel you usually have more uncertainty, you have less. So these things. So, these sets are reconstructed also by taking, as I said, the distribution of supremo over the entire boundary. And the boundary is the entire boundary, right? So, in this setting, we're not distinguishing between them, but we want to distinguish them. Now, so how are you distinguishing them? How do you distinguish them? Then you need, this is where we need the topological structure of the signal to be able to spread into clusters. So, if the signal has a, I don't know if. Has a, I don't know if the right word is to stravas separable is not right, is not the right word. But the topology of your estimated set is going to be close to the topology of the original signal, the original excursion. So, in the original excursion, you really have four components. Then, with large enough sample size, with good enough signal-to-noise ratio, eventually the yellow. To noise ratio, eventually the yellow set and your estimated set should also decompose into the same connected components. And that's the moment where you may be able to attach each one of the estimated clusters to one of the true clusters. And to do that, you need, I think this is what we're studying now. What are the assumptions that you need for the true signal? And so you need isolated critical points, for example. So we need to study topology. Yeah. Excellent. Very good. Exactly. Very good. Yeah, yeah, sure. Happy to ask. Okay. So I have a comment and a question. Maybe I'll start with the comment. So I'm not surprised at all that this works with the supremum because what you're doing is exactly the same as what we are always doing in change points. And there we are looking at the statistic and then take the supremum. And there's many papers that prove that the bootstrap is consistent in this setting. By the way, including one that I have with Dimitris. Way including one that I have with Dimitris. So I don't know why he said that. Oh, then you should send it to me. I want to, you know, okay. Um, now the setting in those papers most of the time is, of course, for stationary time series. But given that you are dealing with the multiplier bootstrap, which effectively is a local bootstrap, it's not surprising at all that it should also work for locally stationary time straight. I would love to see the paper. Yeah, so well, yeah, I think the one that I have with Dimitri is not as a I have it, but Dimitri's not a local, it's not a local procedure, so that won't work. But I can point you to some other papers. Great, thank you so much. And then, my question, you kind of answered it partly because right from the beginning and the second slide, you showed the functional central limit theorem. And that was the point when I started wondering why you wanted to pick a C at all. Because you could clearly, in particular, if you have a bootstrap, it's very easy to get the simultaneous confidence bands. So, why not just report them? Because they just contain much more. Them because they just contain much more information. Right. So the difference. So here I have both, right? I have a simultaneous confidence band everywhere versus the coopsets. Yeah. So the difference is precisely in the quanta. So if I construct simultaneous confidence bands over the entire domain, they're going to be wider, right? And then I get the simultaneous coverage. But if I really only care about certain level, let's say just a level zero. Let's say just a level zero, for example, right? Then, what I'm showing here is that by putting simultaneous confidence bands and thresholding them, this is actually the usual thing that people do, right? You put simultaneous confidence bands and threshold them at, let's say, level zero. What you have is actually something that's a lot more powerful, really, because you're allowed to threshold anywhere. But if you only want to threshold level zero, then you only need to take the supremum over the boundary, which gives you a smaller quantile, and it gives you actually narrower intervals. Right. Yeah, so it's possible that at the end, I'm coming back to simultaneous coffee and math, and I didn't gain anything. Yeah, but maybe the fact that we're gaining more in terms of spatial inference, maybe that's the game. All right, well, thank you very much. A lot of questions about interest generated, so thanks, I don't know.