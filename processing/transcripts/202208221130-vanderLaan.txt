Yes, so this talk is about sequential adaptive designs and surrogate markers. It's joint work with Aaron Hudson and Weng Singh Zheng and Ivana Malanika. Yeah, that's really the most to collaborate right now for what I talk about today. Let me see if I can move this one second. There we go. One second, there we go. So we'll talk about what we call an oracle circuit outcome, and then we get into adaptive designs, evaluating different and learning the best circuit outcome for the sake of patients, where the surrogate is used to make treatment decisions. And we'll talk about the new methodology for which is a super learner for choosing among different adaptive randomization strategies. Adaptive randomization strategies. Okay, so the first thing is just goes back to a paper by Peter Gilbert and Price, and it's called Estimation of the Optimal Circuit Based on a Randomized Trial. And the idea is the following: We consider, let's say, a randomly sampled subject in which we observe baseline covariates, then let's say binary treatment, and then a different time point. And then at different time points, we observe covariates, time-dependent covariates and potentials and outcome type thing, and then a final outcome at the end. And let's say treatment is conditionally randomized so that conditional causal effects are identifiable. And then you can define this conditional mean of the outcome given the treatment, the coverts, and all the history you have observed up till time k. So at time. Up till time k. So at time k, like six months or something, you can define this conditional mean of the outcome, which is now just a single score and it's a summary of the whole history. And that guy is actually a very interesting circuit. It's an unknown circuit. That's why we call it an oracle circuit, but it has the property that if you that the causal effect of the treatment on this circuit On this circuit is identical to the causal effect of the treatment on the final outcome. And that's conditionally on all the covariates, true as well. Let me see. Somehow I can't. Okay, I'll hide somehow maybe. Let's see, does it work? Yeah. And then, yeah, you can also make an insert. And then, yeah, you can also make in certain situations, you might be lucky enough that the treatment gets blocked by these time-dependent biomarkers or coverts, whatever you measure. And then this surrogate marker will not depend on the actual treatment arm. Yeah, this paper is then also applying this in an actual randomized trial. And of course, you can learn maybe the surrogate on one part of the data and then use it for the next trial. And so, you can see an evaluation of that in that paper. So you can think about that you consider a group sequential trial where patients are, let's say, are enrolled potentially in continuous time. And each time when they enroll, they generate a record, like treatment coverts, some treatment. And then over time, at the level of the individual, like maybe the whole thing lasts 12 months, you measure variables, outcomes, and final outcomes. Variables, outcomes, and a final outcome. So then, at a chronological time t in this actual study, right, which were patients enroll over time, then at that time t, you already have a whole bunch of patients have been enrolled previously. And so you could then use, obtain an estimator of that oracle circuit based on all the data you have so far. And one could then also estimate the, for example, the average remote effect on this oracle, of this estimated oracle target, and even obtain comms in the vault. And you can do that also conditionally, and that could then be used to set randomization probabilities for the next patient. So in this way, you can actually learn your surrogate, which is an estimate of the R. Surrogate, which is an estimate of the Oracle surrogate, and also start using it to guide your treatment decisions for the next patient. So that's the kind of design we'll talk more about. Except before we go there, we should acknowledge that this is just one way of learning even a circuit and potentially learning from past data as well. And of course, it's nice to know that it has these great properties. So it's definitely an interesting. So, it's definitely an interesting way of doing it. But there are other considerations when you select a circuit. As we said, the conditional mean is unknown, so it needs to be estimated. In some trials, there may be a long follow-up period prior to the final endpoint, so you would have not that much data if you do this in an ongoing way. And so, you can think of this as that, yes, it's a nice Oracle surrogate, and when you go for it, Oracle surrogate, and when you go for it, it might be a relatively unbiased object to go for, but it will also be subject to a lot of variance. And so, therefore, it's certainly sensible and probably more effective many times, certainly in the early parts of the study when you don't have enough data to consider simple circuits and that may perform better in bias-variant trade-offs. And then, of course, there might really be circuits which just make perfect sense to. To from a subject matter perspective. So, in many settings, one can identify a sensible circuit outcome a priori. For example, if you have a final outcome, which is cancer-free survival at two years, maybe a circuit outcome could be the tumor size every three months after enrollment. Choosing randomization probabilities for the next patients to improve, let's say, the overall performance on the surrogate outcome. Overall performance on the surrogate outcome, like you try to estimate the optimal rule with respect to that surrogate outcome for treating, and you use that to set your randomization probabilities. It's unlikely for these types of circuits that that will be harmful. The conditional effect of treatment on the circuit may be biased for the conditional effect of treatment on the final outcome, but nonetheless, optimal rules may still be very close. Optimal rules may still be very close because, at the rule, it's not really about the exact conditional treatment effect. And simple survey will be more precise than a conditional mean estimate. So, the point has been that there are surrogate, there are different ways of learning that. How aggressive are you? When you use your machine learning, you use paramedic models, you so on. And then, of course, there are many other ways of generating. Course, there are many other ways of generating circuits have a kind of objective evaluation of all these strategies for generating surgery outcomes. And so, for that, we could use sequence. So, we have a longitudinal study which is run for many months. For many months, and each enroll patient is followed for K months. So, K might be whatever, six months. Every patient has a six-month record. At enrollment, assign treatment A with randomization probability to that patient, which of course that is then using a randomization probability at time T, which that will continuously be changing over time in response to the history. So, but anyway, at any point in time T, when a patient comes in, you have your randomization. In you have your randomization problem. So at each follow-up time, we observe a surrogate outcome for the patient. So you can think of that, let's say, month one, we have a surrogate, month two up to month six. And what you can now do, and that's particularly useful if the final outcome takes quite some time relative to the overall length of the study, is that it takes a long time before you even have any information on the final outcome. Any information on the final outcome. So these trials would last too long if you want to make them adaptive. On the other hand, you quickly have data on two mods or outcomes: three malt and formal. And so you can then say, why am I not trying to learn from the past what is the optimal way of treating patients for optimizing the mean circuit outcome? And so this way we can already start running these adaptive designs and actually have enough power. And actually, you have enough power to do that. So, nonetheless, you have to decide, am I going for surrogate at time one, surrogate at MOV two, circuit mon three, or month six? And that's a biased variance trade-off. The more recent surrogates, you know, is the ones which really are almost like a final outcome, they're less biased for the optimal rule. For the optimal rule, potentially, but there is also less sample for them, so that's just a biased variance trade-off. So, yes, so you have then this issue that at any point in time, I can use the history, I can learn the conditional treatment effect of treatment on circuit at month one, circuit month two, up to circuit month six. But then I have six guys and I choose among them. And yeah, and then on top, you know, maybe you have other circuits. So for that, So, for that, we need a new methodological advance. And so, for that purpose, we said, okay, we're going to propose what we call a superlearner for choosing among different adaptive strategies for setting randomization properties in response to the path. And so we call that a super learner for adaptive designs. So, some criteria for good adaptive design is that you like to make create a high Create a high mean outcome. If the outcome is, let's say, a positive thing. But you also need exploration. And because otherwise, if you don't explore enough, you don't have enough randomization, then you cannot learn anymore. So that's always the big challenge in these adaptive designs, like the multiple banded problem as well. How do you find that balance between exploration and being aggressive about learning from the past and say, oh, wow, the past tells me this, let me go for that? Tells me this, let me go for that. So, at any time t, we can use data collected as of time t to obtain like a target max-like estimator for the mean of the final outcome under a particular adaptive line. So, we ask ourselves a question, what would have been the mean final outcome if everybody would have run this particular strategy for setting randomization problems in the past. And that's something we can work out and we can develop a team of leadership. We can work out, and we can then develop a T MLE for it. And we did that, and that's a very unbiased T MLE because it uses the known randomization probabilities which are known in this adapt design. At any point in time, you know what the probabilities are. And so this estimate informs us how high the average outcome might have been if we had run that proposed strategy from the beginning of the study. And then at time t plus one, we choose the adaptive design that maximizes the confidence interval lower bound. So in other words, instead of just using the estimate as the criteria. Of just using the estimate as the criteria, so we have an average outcome under each possible adaptive randomization strategy, like surrogate-specific in this case. And we are not just choosing that, we construct a constant of all and then take the low-bounded constant and optimize that. And so in this way, we also take into account that we want to have not just respond to a signal, to a point estimate, we want to respond to a real signal. Respond to a real signal. If we see that the convolence bound gets larger, that's meaningful. And if that's the also makes sure you also explore enough. So in this work, we use the superlearn to select between K adaptive designs based on circuit outcomes. This method is more generally applicable for selecting among many types of adaptive designs, in others, different estimates of the Oracle circuit we talked about, different estimates of condition. About different estimates of conditional treatment effects on the oracle or on the simple circuit. So, there are many. And then, of course, you also have how do you choose your expiration program? So, even though I've learned like a best estimate of the conditional treatment effect on a particular circuit, I still don't want to just use the corresponding optimal rule for the next patient. I still need expiration, so that's another choice. So, there are all these choices of how to set this randomization probability for the next patient response to the past, all these different strategies. Different strategies, and we have no idea a priori what's really best. And that's why we need this SuperLearn idea where we're actually just going to learn from the data what does work the best and then use a confident model that also takes into account the uncertainty and then optimize that. Now let's evaluate how this works. Here's an simulation where we do we have five mod one. Five, month one, month two, up to month five. We have an outcome, circuit outcome. The final Y5 is the final outcome. We have some baseline code, some treatment. We run this experiment from TS1 to 50. We enroll 50 subjects at each time point, and we compare different adapter designs, in particular, like one which always flips a coin, and then one which sets the randomization problem by learning from the past what the conditional trimmed effect is. What is the conditional trimmed effect on the circuit outcome? And then, of course, we could have done that in different ways: how aggressive you are, and all that, but here's fixed. And then finally, we have our superlearner, which actually will learn among these five different strategies based on only using circuit one, only user two up to circuit five. But then this superlearner will actually continuously might flip from. Actually, continuously might flip from one strategy to another depending on what the superlearner tells us to do. And so that itself now runs itself an adaptive random design. So the adaptive design based on the circuit outcome would work as follows. You flip a coin at TS1. If YK has not been observed, you keep flipping a coin once there is. You keep flipping a coin once there is some data on that particular circuit outcome, then we start learning from it and actually set the randomization probabilities in response to the conditional human effect on that circuit. The actual adaptive design for this superlearner is continuously computing these TMLEs for the performance of each possible adaptive strategy for setting the randomization problems in response to the past and then chooses the And then chooses the computes the constant volumes and choose the winner, and then chooses that particular randomization probability from we just described for that specific circuit outcome. So these are how it works. And so here in this simulation, we have five target outcomes, and one is the final outcome. You can see them plotted here as a function of a baseline covariant. And when the And when it's positive, you should treat, and otherwise, you should not treat. And you see that they're all five are somewhat different. And so at the second, I'm sorry, at the second thing, the second picture, you see the actual optimal rule. And you see that for surrogate one, you would decide to switch to no treatment earlier than for surrogate two. So you do see that, and the surrogate five is, of course, the right. Five is, of course, the right one. So that says keep people on treatment up till the buyer market equals two. And so you see here there's actually a reason to like the final outcome better than the early circuit because they make somewhat wrong treatment decisions for some time. So anyway, but let's see what the superlearner does. And so here are the probabilities of the super of the yeah, each adaptive. The yeah, each adaptive design choosing making the wrong decisions, so choosing giving the wrong treatment. And you see here the purple one is the this is the adapt design based on super learner. So that's this guy here. And then and then yeah, here are all the others. And you see this one makes a lot of mistakes, circuit one, and so on. But circuit four and five, they are doing very well with low probabilities. And so you see that also with the cumulative probability of choosing the sub. Also, have the cumulative probability of choosing the sub-optimal R. Here's the cumulative probability of choosing the sub-optimal R and regret in the end, the end of this whole thing. And you see here does well and then this one was even better. That's it. That's why I'm basically find out. That's the dev design basically will find out, but at least it's close to that. And similarly for the regret, which the overall regret is the overall average performance relative to the optimal thing. And here you can also see how what the superlearner does in terms of choosing which strategy, which server is it going for. You see at the beginning, it doesn't really know yet who to go for, but eventually it really. Who to go for, but eventually it really switches only between circuit four and five, which is actually the right thing to do. There are many more simulations we have done, all kinds of different scenarios, also scenarios where circuit one is great, right? Even though it's an early circuit, it actually has the same optimal rule and therefore there's more information for it. So actually, and then it starts using that a lot. And so, so there is so that's what you will see when you do this super learner across different settings, different types of simulations. It keeps coming. Of simulations, it keeps coming up at the top. And while if you stick to one approach, you are going to be wrong in certain cases and maybe very good in others. Okay, so the last little time we have left, I don't know where we are, is want to talk about another example of an adaptive randomized testing to optimally control the infection rate based on also on surrogate outcome. So, we here consider a scenario where we have n individuals, they have a longitudinal data structure over time, and they involve at h time t an indicator being tested, a subject can be tested, and also correspondingly measurements on things like their risk profile, network information, but also their actually observed infection status. So, in other words, if they are tested, what is it? If they're not tested, will be just set equal to zero. And then, of course, there's the actual latent infection. And it's not that important. Then you can also actually write down a likelihood for this overall original data structure across n subjects, but these subjects are connected from networks. So these likelihoods will be looking like a product over time. A product over time. At each time, you get this product of conditional distributions of whatever happens at time t on them in response to the past, where the past can also include their friends, any history in the friends and themselves. And then there is in this likelihood also the actual way you set your test your, yeah, so the decision to test that person for COVID, let's say, yes or no. And so at the end, this is just. And so at the end, this is just a little in a sample size one problem. And yeah, so this is the challenge now. How do you set your random testing strategy? Who to test? And let's assume you have only so many tests you can assign it every time T. And how do you do that? And for that, it helps to have a certain outcome. And the circuit outcome we are using here is we define here this. We define here this is the proportion of positive cases among the tests. So it is not written well, this, but it's the summation of the YIT divided by the sum AIT. So among the people tested, how many are there true positives? And we are going to consider the case that every the way we set the testing is always the same number of tests are assigned. Number of tests are assigned. So, this is that means that this just using this sum of the number of positive cases, proportional positive, is actually now a good way of evaluating how well your test vector, a vector of binary variables is performing. And we call G star IT the conditional distribution of how to set the test cross. The test cross the n subjects. And so you have this thing is really just a vector for at time t is a vector of probabilities. And we are interested in evaluating, so we're using counterfactual mean proportion of positives among the tested under strategy G star T. So this is what would have been the average number of the proportion of true cases among the tested letters. Among the tested, let's say, if you would have carried out this particular strategy for setting your testing. And that is something we can identify from the data, again, because we know these randomization properties, so we can actually develop some kind of estimator of it. And as final criteria for evaluating a particular strategy for setting the vector of tests at time t in response to the At time t in response to the past, we use then the average performance up to that time where you are. And so that's for that, we can then again compute and target makes a larger estimator. You can also do an IPW estimator. And then we can construct a constant in the wall and then use again the bound of the constants in the wall to choose the best testing strategy. This was work with Ivan. This was work with Ivana and Jeremy Coyle and Maya Peterson. And so they applied this actually during the time of COVID to the UC Berkeley campus. So they tried to model the system there with all kinds of components and using some agent-based model and setting running simulations. And then these different strategies for setting tests was all based on learning from the past. Learning from the past, the risk for every subject by using essentially machine learning, super learning, we use online superlearning with all kinds of approaches. And then based on your estimate of these risks across all subjects, you then might rank them. Or there's different strategies of using these estimates of the risk across people to decide how to set, who to test, right? That I have, let's say, 500 tests to assign every time, who among all the people you would. All the people you would assign the test. But either way, it was always using SuperLearner to decide how to learn the risk. And so there were a whole bunch of strategies. And there are also very simple things where you just look at symptomatic people. And there's also a perfect scenario where you do the optimal Oracle job. And there's all kinds of choices. And either way, what you see here is You see, here is the TMLE selector comes in. That's the super learner. You see the performance average cumulative incidence. So that means the proportion of people are infected in your population. And you see that the superlearner does well relative to some maybe other simple strategies. And of course, the perfect cannot be needed, but that's just there. And this is for different number of tasks you assign at each time point. It's for 200. Each time point is for 200, it's for 400. And so you can see that across these different ways of running these testing strategies, how if it keeps popping up and the superlearner stays at the top. And here are some more pictures where you can look at final cumulative incidence. And again, the superlarin is here and here, so it's always with the lowest level of infection rates. Of infection rates. And this talks about what it chooses across the different trajectories. And just as final slide, some concluding remarks. So adaptive designs combined with superlearning can be used to objectively evaluate search outcomes with respect to a particular utility, like keeping the infection rate low or getting optimal mean. The optimal mean clinical outcome for your patients. Different estimation strategies of the Oracle circuits could be used to generate different adaptive strategies for setting randomization problems in response to the past. Our Superlearn for Adaptive Science, incorporating all kinds of candidate served outcomes, is able to robustly optimally adapt randomization problems in response to the observed history relative to betting on any particular strategy of setting these treatment randomization problems in response to the history. Response to the history. And that's it. Thanks so much. Thank you, Mark. I have it on, right? Okay, yeah. Okay, thank you. I have, okay, I'm just going to jump in with a question. Actually, it's more of a clarification. So please tell me if I summarize this incorrectly. So is it correct that? Is it correct that? So, you're not really talking about a setting where we have some like surrogate S where we're trying to say if it's good or not. You're saying, here's a way to construct this like Oracle surrogate. I can guarantee you it, you know, has these nice properties. And then here's a way I can use it to do this adaptive randomization. Is that right? Yeah. Is that right? Yeah, that's essentially right. And then also saying, you know, the Oracle circuit is an Oracle circuit and therefore is great, except it needs to be learned. And so in the end, you still have, if you want to make this part of a system, like a self-learning system, like where patients need to be treated, there are still so many, it also has disadvantages relative to just something very simple, right? Like, oh, here's a surrogate. I already know a tumor size or whatever. And so in reality, there's a big competition going on. Reality, there's a big competition going on between many possible surrogates, and at the end, if you have an objective criteria, like saying, Hey, you know, I'm going to use it for my patients to make treatment decisions. So then the objective criteria is that you want a great clinical outcome, a minimal death rate or minimal infection rate. And so then you can start running an adaptive design of the type we discussed, incorporating in particular the superlearner, which then figures out over time what strategy and in particular what surrogate to use. Particular, what surrogate to use, but at the end, it's not even just the choice of surrogate, it's also how you estimate the circuit. All these things then play a role. But of course, we could have done this just with saying, you know, here are my simple circuits. I don't want to use the Oracle circuit. I don't want to do that. That's fine too. But I do think it's an interesting object, let's say, this Oracle circuit. And so, yeah, it could be utilized for that purpose. Okay. And so, is there any Okay, and so is there any, despite its name, is there any way that the Oracle surrogate would not be wonderful? Like if, you know, let's say the Y, if you have Y1 through Y5, and there's really only a treatment effect, there's like no treatment effect on Y until exactly at Y5. And all your baseline stuff and your time-dependent stuff, like that's not helping you at all. Is that some extreme case where this oracle surrogate would be oracle surrogate is only relying on that because you apply it, let's say, to a future study, right? So if I have the oracle circuit in my hand, I had to learn it somewhere. And it relies on the conditional mean of the outcome, given the past. And so if that changes to my, if my future study has a very different conditional mean outcome. That it has a very different conditional mean outcome as a function of the covert treatment and time-dependent biomarkers. Yeah, then you might not do well because you learned it in a way on the wrong setting. On the other hand, it still transports to any future study which has the same, has a different distribution on the covariance and the treatment and the biomarkers. So it's really the transportation only depends on getting to a setting where it has the same conditional mean outcome. Same conditional mean outcome, or not same, you know, it doesn't have to be the same, but then it becomes less perfect. So that's really where you, yeah, where you can, where it cannot perform well. But the moment you have that, then it's a perfect circuit because it's actually the conditional treatment effect of the treatment on that circuit is identical to the conditional treatment effect on the clinical outcome. So, any decision you make on that surrogate is as good as you would have made it on the clinical outcome. Of course, if you get. Of course, if you get it right. Right, right. Okay. Miguel, can people unmute? Is that possible and talk? Or can they only send questions in the chat? Okay. Yeah. For those of you sending stuff in the chat, just go ahead and unmute and talk if you're comfortable. Hey, everybody. My name is Andrew. Thanks for the great talk. My name is Andrew. Thanks for the great talk. My question was just whether, you know, how these adaptive designs, whether it's the treatment allocation or the testing allocation or the randomization probabilities, how that might impact downstream analysis of the trials. Do you need to integrate this design or does it limit at all what kind of methods you could use to, you know, someone could later use to come back to the data and do an analysis? Exactly. Yeah, that's a very good question. And that has to do. To do uh so, in a way, that's where you know what is the goal is an important question, right? If the goal is to just optimize the infection rate or death rate or the mean clinical outcome, then this kind of superlearner which evaluates based on that criteria will do a great job. However, if the end of the day, there are other things you want from your data, and which is, for example, that it's not just the performance for the patients, but it's also It's not just the performance for the patients, but it's also about having power in evaluating that one thing is better than the other. Then, yeah, then that becomes important as well. And that starts putting a constraint in particular. And that's something we could build in the criteria, right? It would just change the criteria and then the superlearner would respond accordingly. But then, yeah, then you also start saying, hey, you know, I want to keep making sure that I have enough power, like 80% at this. Power like 80 at this alternative for answering this question or multiple questions, so it becomes something we have to start building in. And what will happen, it will be forced not to go for the fully optimal thing for the patient, potentially, because it might contradict that. And, you know, but then you have more power for learning questions like that. So, yes, it's a very important question. At the end, we have a lot of work on using TMLE to get formal robots. To get formal, robust inference for any question you want to ask based on this type of data generated by such an adaptive design. And that all works. It's all based on these randomization problems that are known. So these estimates are completely unbiased. That's based on martingale theory. So everything works. But nonetheless, the power will depend very much on how aggressive you were in pushing these randomization probabilities towards almost deterministic. Because if you push them to deterministic, then there's no exploration. Are missing, then there's no exploration anymore to really answer other questions. And so it would force you to keep these probabilities bounded away from zero to make sure you still have enough power for any other question. Does that make sense? Yeah, thanks. Okay. Because Layla handed me the microphone. Peter, did you want to ask? It's the microphone. Peter, did you want to ask your question? Yeah, let me ask a question. It's different than what I put in the chat, but yeah, thanks, Mark. It's great to see your updated work. So, my question, if you think about two major application areas, one would be on one extreme, say patients that say after three or four months of follow-up in a clinical trial or series of clinical trials. And the opposite setting, the other extreme, would be, say, prevention studies where. Would be, say, prevention studies where only 1% of the participants have an endpoint over four years of follow-up. Could you comment on how this adaptive approach you've outlined might perform differently or how you'd need to set up the method differently for those two extreme situations? Right. So, one was where there's very few events happening, you need a long-time follow-up, and the other was. A long-time follow-up, and the other one's yeah, like a prevention setting where it's a very, very rare event, takes a very long time. So it's hard to learn based on clinical endpoints or it takes a long time. Whereas the other one, you can learn quickly about clinical endpoints. Yeah. Yeah. So in this particular, now firstly, let me put straight. The first thing, and that's what I told Aaron as well, we need to talk to Peter Gilbert to see if there's good, interesting applications, right, in realistic settings. So, in other words, that's on the agenda to talk to you guys. But anyway, our on. But anyway, I run you work with now. So, anyway, given we haven't done that yet, right? So what we have done here is we use as criteria still in the super learner to learn, to get the performance on a final outcome. Now, if there is no information on that final outcome, yeah, then we will do poorly, right? Because there's just no information. Poorly, right? Because there's just no information. And essentially, what they will, it will not do poorly. It will just essentially keep randomizing and flipping a coin. It just has no information. On the other hand, what you can do, and that's what we have built in into this thing, is we just decide what we call the final outcome. In other words, if there is no, if the only, and then requires, so we might say, you know, in the beginning, the best, the latest where we have at least some information is circuit five. And then maybe later. And then maybe later it becomes surrogate seven, and so on. So we start what we call the final outcome, and it's which is the one we're going to use in the superlearn as criteria to choose between different adaptive strategies based on earlier circuit outcomes, is then there's enough power for that, but it will keep moving. And only so in your extreme case, this would mean that we would run essentially a trial based on circuit outcomes. We would make treatment decisions based on circuit outcomes. If you don't even have that, Based on circuit outcomes, if you don't even have that, then it's over. But then it's like keep flipping a coin. But if you do have circuit outcomes, even though the real final outcome takes forever, then we can still run this. But we would essentially be making treatment decisions based on a certain outcome, which might get, you know, move up and more and get closer and closer to the final outcome. So that's what would happen. Yeah, and then the other situation is, of course, nice. And then you, but anyway. Then you, but anyway, the motivation for this whole circuit outcome was because we had a lot of work on adapt designs where you have group sequential adapt design, you have your first group, you finish it all, then you go to your next group, and then you use the data. But these things take a long time if the clinical outcome takes a long time. So this works great in sepsis trials, where it's just one month, and you can do the next month and the next month. But in situations where you have to wait a year before you get an outcome, you cannot run these group sequential trials in any decent amount of time. So that's why, you know, I. Decent amount of time. So that's why I wanted to kind of build insertive outcomes as a way to still run these types of trials where we can adapt in a reasonable amount of time. And so that's why I think these circuit outcomes could be very interesting and provide us then a way to still run adaptive designs where you set randomization probabilities in response to the past. And then, of course, you have to believe that these certain Have to believe that these circuit outcomes are reasonable, that they're not crazy and that they're not harmful. But if you feel reasonable about that, then I think this could be a nice extension towards these adaptive designs in which you can run in a reasonable amount of chronological time and still be adaptive as you keep learning on how things are working for sure without them. Okay, yeah, thanks. Yeah, yeah, it makes me. Thanks. Yeah, it makes me think that it's basically a scenario where you basically have to make bets because you have to make decisions on patients. And so it's better to use a surrogate that's not yet validated than to do a coin flip. Exactly. Exactly. Exactly right. Thanks. Yeah. So this is Mike Elliot. And I just wanted to follow, I think you partly answered what I was going to ask with your reply to Peter, but are you assuming you have a full vector of? You have a full vector of observations, including the final outcome, at least for a subset of your sample, and then you're bringing new people in, and you know, follow them, or that would be nice. So, if you don't have that, how are you going all the way out to the end? Are you just sort of taking away is basically the prediction for the outcome just based on the current observed surrounding? Like, yeah, I mean, if in the beginning, If in the beginning, firstly, we think that we choose essentially a final outcome, what we call the final outcome on the fly. So in other words, we need something which has some data. And so if the final outcome doesn't have data for two years, then we are not going to just keep saying, oh, let's run the super learner based on the performance on that final outcome. No, no. We are going to choose one of the earlier. Yes, and then, but if you already have maybe based on some whatever previous or initial trial, previous trial, already something going, fantastic, right? That could help you. But that's not necessary. As long as you have early circuit outcomes. And along the same lines, just to clarify, your Oracle assumes you're. Your Oracle assumes you're having early versions of the actual outcome of interest. So the surrogates in the sense of something that's related, but not the same thing. Right. Yeah. The Oracle Circuit is defined with respect to a real outcome of interest. And so you better have data on that. And so that is really where I think for the Oracle circuit, you kind of want to have a real first study to learn it in a decent way. Or have a situation where every patient doesn't take much time to follow up to get that outcome of interest. So, yes. And I think in the paper with Peter, Peter knows more about that, we did do an example of where we learned it on one randomized trial and then transported it to another randomized trial and that kind of thing. Okay, I had another question, so which kind of combines multiple things of what's been said, but is also related to what the mark before you talked about. So as you said, we could just use not the oracle surrogate, but just some simple surrogate and do this procedure. So my first question is, like, how good does it have to be? How good does it have to be for me to be confident in doing in using this procedure? And then second, only like medium confident in it. And then I this procedure and afterwards, this is related to Andrew's question too, afterwards tried to use the data to now like evaluate the surrogate again. It kind of seemed It kind of seems like what you know the talk before this was talking about when you try to evaluate a surrogate, but then there's you've introduced you know a little bit of you've introduced some additional things because you've made some decisions based on the surrogate, you know, as you go. So doing a very simple approach to quote validate the surrogate is probably not right, if that makes any sense. Right, if that makes any sense. I mean, if I think one thing you're talking about is if you learn the optimal surrogate based on whatever data you have, that is itself an estimator that has its own variability. And if you then also start based on the same data, start saying, hey, let me now. Do something with that. I mean, yeah, then we are using the data twice, and that has some challenges. But in this case, when we're talking about this kind of ongoing study where we are only using the past patients to learn things and then start making decisions for next patients, that's where this will this. Will this kind of uncertainty in the estimator of the optimal circuit will become part of the performance of that? And so the superlearner would then start potentially in the beginning, it will start choosing the very simple circuit, we don't have any data dependence. But over time, it might start choosing more and more towards an estimate of the optimal circuit because it starts actually doing better. And so that's why it's nice if you can. That's why it's nice if you can run this kind of study with all very non-data-dependent choices, and then, of course, at different time points as well, as well as potentially data-driven estimators of an optimal circuit, which then again, in the beginning, wouldn't probably be used at all, but then over time might become important. But there's also, yeah, there is also ways to say that if you as you as You estimate the optimal circuit, you can also get, and then you estimate, let's say, the average treatment effect on that estimated optimal circuit. You can use that to still get a constant of all for the actual estimate of the treatment effect on the final outcome. So, there is a way to link it all so that at the end, we still have a concerned quantification for what you actually really care about. So, there's some. So, there's some work on that. Thanks, Mark. I'm going to follow. This is Dennis Daniel from RAND. I was going to follow up on what Layla was saying. I think that what Layla is concerned about is even if you estimate the optimal surrogate, unless you're sure that it fully mediates the whole treatment effect, it may not be actually, you know, it may be perfectly useful in assigning treatment probabilities in your. Probabilities in your example because it's better than nothing. But if you wanted to actually take it into a future clinical setting, you may want to have some assurances that it is capturing, you know, the lion's share of the treatment effect. Then I think these issues about how do you actually evaluate it when it's involved in assigning the treatment probabilities become really yeah. Yeah, I that there's if you're forced to run a study with patients, you want to do a good job with patients, is one thing. If you want to use this kind of adaptive design, which learns about what kind of surrogates are good and actually learns the surrogate on its way as well, then at the end, am I going to bring this surrogate to the next whole trial? Is then still something to be answered. Something to be answered. And that would require its own methodology. And I think, yeah, in essence, you kind of want to link it now to what will it, how does the treatment effect on that claimed surrogate, that suggested surrogate, how does that relate to the treatment effect on the clinical outcome? And with some real confidence of all, that kind of thing. So, yes, that is. So, yes, that is these are all different goals and they require its own methodology. Yeah, could I interject here? This is Mark, the other Mark. I wanted to go back to the question as to whether you have to have a really good surrogate to adapt the randomization. There is an example not with the Oracle surrogate, but with the PCR, which, as I showed, is not at least a surrogate that you would. Least a surrogate that you would use to predict the treatment effect on the true endpoint, but it's an endpoint that people use in and of it, you know, for because it has value in and of itself. As you may know, there's been a trial called the I-SPI2 trial in early breast cancer that was designed by Don Berry, and they use the PCR to adapt the randomization. So, in a sense, it's very analogous to what we've seen, except it's not optimal in any sense. But they use a surrogate which hasn't been validated to predict the true end point. To predict the true endpoint. But anyway, the adaptive randomization. And that I think is fine again if you are in a not in a confirmatory mode, but in a discovery mode. You try to do the best you can with whatever you have available now, but you don't intend to validate this surrogate in the same trial or using the same data, because the fact that you adapt the randomization based on the surrogate, in a sense, will, in fact, and I think this is what Laila was after in her question, will make your Her question will make your, you know, will make your evaluation of the relationship between the surrogate and the troin point much more difficult to assess because you obviously act based on the surrogate and then you want to show that it's a good predictor of the treatment effect on the troin point. I don't think you can do both in the same trial. I think it depends on whether you want to confirm the validity of a surrogate or use it for what it's worth now and do discovery. That's at least the argument that the ISPI is. That's at least the argument that the I-SPIC2 folks have used because we've argued that the I-SPY2 design is pretty objectionable. And you can't really reuse the data from the I-SPY2 trial for any other purpose than the purpose that they pursued, which was to try to find good drugs in a fast and efficient manner. And I think for that purpose, it was probably okay. But in terms of predicting what the effect of these new drugs will be on a trend point, the I-SPY2 is. Will be on the trend point. The ice by 2 is worthless. Yeah, I think you make all excellent points. And it's good for the reference as well. And yeah, and I think you can still, based on this data, learn things, some confidence intervals and that kind of thing, but to call it an objective evaluation might be a far shot. In particular, because the confidence evolves, for example, we can. Like, for example, we can use the data to learn what the average remote effect on this claimed circuit would be at a constant wall. But that constant wall might be quite wide. I mean, there are ways to adjust for the fact we use double data, right? We have stuff on that. But at the end, it will be a relatively wide constant wall because this study wasn't powered for that. So that already means you don't have that much information for truly claiming this is something you can bring to the next trial. Uh, prior. So, I'm with you on that. I do think you can do stuff, you can still learn from the day and still it can help you, but not more than that. Yeah, thank you. That's exactly what I was, that's exactly what I was getting at. So that's perfect. Yeah, thank you. Thank you for helping me out there. Yeah. So, Mark, for the other Mark and Gear, have you ever, just to stop putting, Mark may. Just to stop putting Mark Manderlin on the spot for 30 minutes. Have you guys ever been asked or successfully carried out an analysis where you were asked to, quote, validate a surrogate, but perhaps not something as extreme as this where the randomization probabilities were adapted based on the surrogate, but as you Based on the surrogate, but as you said, Mark, you know, something, some decisions were made based on that intermediate variable, and you still were trying to evaluate the surrogate. Have you ever done something like that? Like, how did, and if so, how did you do that? Why don't you go first, Mark? Yeah, yeah, I was going to say we've tried to do stuff in this direction, but not very successfully, unfortunately, so far. Unfortunately, so far. But one of the problems that we are interested in is to try to do a better job at interim analyses of trials for the reason that you have a lot of information on the surrogate and very little on the trend point, typically during an interim analysis. So that's one situation where you would really love to use the surrogate, you know, for making a decision to stop the trial or continue the trial. And this is actually quite difficult. And so we haven't gone very far in, there's been some work done by colleagues. Some work done by a colleague, you know, especially Tomasz Bozikovsky and one of his colleagues, have looked at this. You know, it turns out that if you don't have a fully validated surrogate, you just don't go very far with this approach. If you have a fully validated surrogate, it's no problem. But if you have kind of a lousy surrogate like PCR or something like that, it's just not helpful. And so even if you have a lot of information on the surrogate, it just doesn't help. This is back to this problem of the one minus R squared. Know the problem of the one minus r squared that George showed, where if r squared is too close to zero, you know, you get a lot of uncertainty and your prediction is bad, and so your incrementalities is bound to be very unreliable. And we really don't know how to progress on that front. It's actually the cartilage between the two that is the issue here. If that's too loose, what are you gonna get from one bone on the other? At the same time, if it would work and the surge. Time, if it would work and the surrogate would be fully validated, maybe by that time, practically speaking, the surrogate would have become the true endpoint in the first place. So maybe, but that's opening a completely different box. Multivariate outcomes, and then I'm thinking of GPC also, Mark, would be a different route forward, other than surrogacy, because there is always this thing of because there is always this thing of validation that is uh that is that is intervening and that's actually frankly bothering you here yeah so so i mean the answer to your question is leila i guess is we haven't really done much progress on that front and we're kind of stuck to be honest yeah well thank you it's it's uh helpful to hear your thoughts mike did you have something you were Mike, did you have something? You were mentioning something. Yeah, sorry. So one more thing, go back to Mark Bussy. Sorry. I'm pronouncing your name also. But I think it was kind of terrifying to see that it seems like everyone seems to have gone to just using this surrogate marker, which is not predictive of that doesn't seem to be a good marker for the very reasons you mentioned, that basically there's just no way left for the There's just no way left for the real world to run trials. That's true. But I mean, this sort of ties into stuff I'll talk about, the surrogate paradox. It just seems to me that, you know, there could be several things going on. I guess maybe if the treatments aren't very effective, it's hard to measure those. But it also could be this idea of like unmeasured confounding. It's clearly there's this nice relationship between the marker and the outcome. But maybe there's, and there's an assumption about the biology going on there, which maybe isn't fully correct. I mean, this is. Fully correct. I mean, this sort of gets back, you know, outside, you know, my or most of our wheelhouses, right into the sort of biological piece of this. But it's disturbing to me that that's not being considered. Setting aside the fact that I guess there is, I mean, the fact that you reach this marker is feeling from a quad of life or mental health perspective, really good. So maybe in that sense, it's okay. But yeah, it's a little frightening to me. Yeah, yeah. But the tension, Yeah, yeah, but the tension, you know, the tension, Michael, is really between the fact that PCR is a lousy surrogate for long-term endpoints, but it's an interesting endpoint for clinicians to look at anyway, in and of itself. I mean, if a patient has a PCR, the clinicians know that the prognosis of that patient all of a sudden is far better than otherwise. And that's useful information. You know, they have to act on this. And the fact that they act on it actually makes it even more difficult to show that it's a surrogate. Even more difficult to show that it's a surrogate in the future because they will use very aggressive treatments. And so, patients who reach PCR will have much less aggressive treatments than those who do not achieve PCR. And that confounds your effect on the true endpoint in a manner that you can't adjust for. Because again, I mean, you could use causal inference to do the sort of kind of actual reasoning, as I said, but because all patients are treated according to the surrogate, you don't have that option anymore. And it's really. Anymore. And it's really difficult then to know what to do, except to just forget about the long-term endpoints. And FDA is kind of cornered badly because they've kind of opened the kind of worms, you know, themselves. They have this guidance where they say PCR is an acceptable endpoint for approval. However, you have to show long-term benefits. The question is, how do you do it? Once you have a drug approved, you know, patients are going to get it. And so you, in a sense, make it impossible to. In a sense, make it impossible to look in an unconfounded manner at the long-term endpoints. I have a quick question for you as well, Mark. So is the, for example, this Oracle surrogate says that the conditional mean of the final clinical outcome, given everything you measure, and you can of course go focus on the subset. So that means in particular, if you have a whole vector of biomarkers, you end up with a score, which is a function of all these biomarkers up to that point. These biomarkers up to that time point, even the profiles, and maybe even baseline coherent. Is that even something you can imagine would be acceptable at some point, that kind of score, as long as it gets validated? I think it's a really cool idea. I mean, the problem in cancer is that we don't have a lot of biomarkers to look at, in a sense. You know, now people begin to look at circulating tumor DNA and stuff like that. But up until very recently, there was absolutely nothing to look at except the There was absolutely nothing to look at except the tumor size, which is well known not to be a good surrogate for anything. You know, it just doesn't predict the survival time or the time to disease recurrence and progression. So, but you're right. Perhaps when we have very, you know, very detailed ways of looking at biomarkers in the blood, you know, born biomarkers that are easily measurable, perhaps then having a panel of such biomarkers will become will completely change the game. That's possible. Completely change the game. That's possible. But at the moment, it's been very disappointing. I mean, we've looked at PSA, for example, prostate-specific antigen in prostate cancer, which is cool because you can measure this in the blood, you know, regularly and see that it's going up and down. And people believe that it would be highly predictive and it's not. It's not at all an accepted surrogate, you know, by even by clinicians. So there's a lot of variability in that biomarker, and I think people need better biomarkers than that. Better biomarkers than that. But if we had a panel of biomarkers, I think your ideas would become certainly very useful for oncology. Great. Yeah. I think where it could also be relevant is in the area of vaccines, which is a world of its own. But think of COVID, for example. We don't have a good correlative protection, and arguably it's a complicated combination of aspects. Complicated combination of aspects of humoral and aspects of cellular immunity. It's maybe good to keep an eye on that area as well, because it could potentially be very useful there too, Mark. Yeah, yeah. This is where Peter Gilbert also has something to say. I'm sure he thought about that. Yeah, yeah, I mean I actually think that neutralizing antibodies are a pretty good surrogate endpoint for COVID vaccines. It's just we have to talk about for what endpoint. We know it's performing pretty well as a surrogate endpoint for short-term COVID when the antibodies are measured to the vaccine strain and we're looking at COVID endpoints with strains that are like the vaccine strain or only minor genetic variants. So there's all these open questions of how we're going to. So there's all these open questions of how will the surrogate behave when we look at Omicron strains or look at people that have different patterns of vaccination or histories of exposure to the virus, etc. And I think for severe disease as an endpoint, T cells are probably especially important. So combining antibodies of T cells into a surrogate might give a better surrogate. But just for acquisition of any symptomatic COVID, I think that a simple surrogate based on neutralizing antibodies is working okay. Antibodies is working okay. I had another question for Geert and Mark. I really see the appeal of the meta-analytic approach when you're in a setting where you have a bunch of trials, but if you're not in that setting, but let's say you have a really large trial, would your recommendation be to split your sample into a bunch of sort of pseudo-trials and analyze it as if it were a meta-analysis? It as if it were a meta-analysis, or what do you think about that? I personally think that if there is no basis for doing that, then it's going to lead to very little because you really need, let's say, an organic replication. And what we've done a long time ago already, of course, is using centers as kind of quote-unquote circuit for trials. Now, that may work because settings, etc., so that there is. Different, etc., so that there is a natural population that you draw from. But if you just split the sample, you're going to get basically cloning rather than sampling from a genuine population where members are similar, but also genuinely different. So I think there should be somehow a natural hierarchy and not an artificial one. Simulations actually underscore that. Yeah, I would agree with that. Yeah, yeah, I would agree with that. But it's a good question, isn't it? Because sometimes, and I'm thinking of rare diseases now, which become increasingly common. I mean, even cancers become rare diseases now because they are classified into subtypes that get smaller and smaller. So it's not unlikely that we will end up with a lot of rare cancers in a sense, on a molecular basis. So, your question then remains: I mean, what do we do when we don't have many trials? And perhaps we have only Many trials, and perhaps we have only one, and it's not even big, you know, because it's a rare disease. So, I think in that case, the metroanalytic approach just goes out of the window. We just don't have the data at all. And in that case, we need to find some case. I mean, that's why it's nice to have, in my view, at least, a panel of approaches and you choose the one that's most appropriate given the data that you can actually have access to. Yeah, thank you. I realize we're a little over, but that was great to. Little over, but that was a great discussion. Thank you. Um, yeah, I did want to just mention really quick that one thing I've been struggling with is the even more extreme case where someone came to me with a data set that had 30, it was a pediatric study and it had 30 kids in one arm and 32 in the other. And they asked me to look at a surrogate. And I was like, I, everything I've developed is non-parametric. The kernel is like this whole. Metric, the kernels, like this will absolutely not work at all. And I really struggled with what to do. And you don't want to say, like, oh, no, we can't do this at all. And I also don't want to fit some simple linear models, given, you know, I knew I have shown that that's not great. And anyway, it wasn't, it was survival stuff to make it even harder. So I think the very small sample size setting is also, it's a setting where, you know, surrogate. Where you know, surrogates are almost more, people are literally dying for surrogates in those small sample-size settings, and is one that I haven't seen a great solution to yet. But we will talk about that more this afternoon. We have more talks, so we'll come back at two, and then we have our panel this afternoon where we'll talk about or discuss the connection between. The connection between these different frameworks, although we haven't touched on all the frameworks yet, but we'll do our best. And then practical considerations, which I think we have started to touch on, and transportability, which I think is what I'm going to start with with that panel, because that has been mentioned a lot. So if you, you know, let's say we all agree miraculously that something is a good surrogate using one study or a set of studies, and now we want. Or a set of studies, and now we want to really use it in a future study as a replacement of the primary outcome. That is the whole goal, that's the whole point. What assumptions are we making about the transportability of the surrogate from one to another? You know, are we assuming the patient populations are similar? What exactly are those assumptions? And what happens if that doesn't hold? So, more to come. More to come. But thank you all, and we will see you at 2 o'clock Central.