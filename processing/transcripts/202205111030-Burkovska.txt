Thank you for the introduction and also for the invitation. I'm sorry that I couldn't be present there, but I'm very glad to attend this workshop even virtually. So before I start, I want to mention that the work I'm presenting today is a collaborative work with my two collaborators, Marta Delia. My two collaborators, Marta Delia and also Christian Luza, both from Sandia National Lab. And I also would like to acknowledge the funding support from the US Department of Energy. So in this work, we are going to study non-local diffusion models, and we already heard the talks discussing about such models. Probably I will repeat a lot of things which we already heard, but nevertheless, I want to recapitulate some. I want to recapitulate some things about what you know about normal diffusion and anomalous diffusion. So, we know already that for normal diffusion on the microscopic level, the normal diffusion corresponds to the situation when mean square displacement of the particle following Brownian motion scales linearly with the time. In the case when it doesn't happen, when mean square displacement scales with some Square displacement scales with some proportionality constant, let's say gamma, then we speak about anomalous diffusion. And then, depending which value this proportionality constant takes, we can speak about sub-diffusion, super diffusion. Here you could see also the snapshots of movement of the particle following either Brownian motion on the left or anomalous diffusion process. The distinctive feature of anomalous diffusion is that for this Is that for this type of processes the particle can experience? It doesn't follow no longer continuous random walk, but it can experience long jumps or flies. And probably the most prominent examples of for anomalous diffusion processes, and we've heard about it already, for super diffusion. This would be, for example, fractional Laplace models, where in this case, this proportionality constant would be one of. This proportionality constant would be one over s, where s is a fractional power. I will, in a minute, I will just go into this definition. If you think about sorry, no, there was no question, sorry, yeah, okay. Um, but we also have like the prominent example for the sub-diffusion model would be time equations involving time fractional. Involving time fractional derivatives. In this talk, I'm not going to discuss about those. I solely focus on super diffusion models, those particularly involving fractional Laplacian. And the fractional Laplacian is given as the following integral operator. We already seen it before. So here I just would like to mention that if you look at it from if you look at the fractional Laplacian from probabilistic point, Class and from a probabilistic point of view, then this kernel gamma, which is given as in the following form, represents the probability density of is related to the probability of particle jumping from point X to point Y. Whereas S is related to the intensity of this jump. The fractional Laplace on this option. The fractional Laplacian, this operator, is also a generator. It's also well known that it's a generator of the Leviticum diffusion processes. In my work, in the work I'm presenting today, we are going to consider a little bit more generalized kernels, more specifically the kernels where, in addition to S, we also introduce one more quantity delta. So more specifically, we are going to consider a true More specifically, we are going to consider a truncated version of this kernel, and truncation occurs by this parameter delta. So, in another words, we are now looking to the operators where non-local interactions do not occur globally, but now localize to a certain wall of a radius delta. So, from a probabilistic point of view, what it means which role delta plays is simply Which role delta plays is simply imposes the upper bound on the length of the jump. In this form, if you consider non-local model in this form, it provides us a more flexible framework because we, depending on the value of delta, we can look at the so-called models, more localized non-local models, sorry, for the tautology, or more models where we have more global non-local interactions. More global, non-local interactions. Overall, we can also think, we can look at delta and s, these parameters, delta and s, as a certain parameterization of our model. Let's say I denote them by mu. And then we can in this situation when we now want to use these models in practice, the main question arises how to set up correctly, how to find, identify correctly. How to find and identify correctly this parameter Î¼, which in the present case I focus solely on delta and s, but of course it could collect other parameters of the model coming from either right-hand side, the diffusion coefficient, or any other various sources. And in this case, we are interested on identifying these parameters. So, as I said, in this talk, we focus on identifying only delta and s, but um, so But so, in other words, we are now studying the non-local operator, which is parametrized by parameter mu, which will be simply delta and s, and given into the following form. Before I go into the details, I want to point out that the difference, because in this work, we studied the so-called integral fraction of a placian, but there is also the notion of spectral fraction. But there is also the notion of spectroflunctional Laplace, and we heard about it. While these definitions coincide in certain cases, like for example, on RAM, if you consider problems posed on RAM, in general, the functional analytic framework differs significantly for these two approaches. And we cannot adopt the techniques from the analysis of the spectral fraction of Laplacian to the integral fraction. To the integral fractional Laplacian here. Okay, so the identification problem we set up via a minimization approach, that is which we pause as following. So we are interested to identify parameter mu, which is given as follows. And we do it in the following way, where we minimize the following objective functional j, which is given by Which is given by a squared misweed term plus some regularizer, which I specify a little bit later. And we consider it subject to the following of fractional PD constraints, let's say, where we consider just a steady state non-local problem subject to the homogeneous volume constraints. And here I just wrote again to recall the non-local operator which we consider. Local operator which we consider. So the definition of the truncated Laplacian, don't you need to scale the constant with delta two? Sorry? In the definition of the truncated operation. The constant, don't you need to scale it with respect to delta two? Somehow you get some consistent operation. So, if you want to, if you want to, that's true. So, and that's thanks for this comment. So, if you want to be consistent, so for example, if you want to obtain that our non-local model in a case of vanishing non-local interactions, that is, in a case when delta goes to zero, will converge to the local model, then yes, it's true. There should be a delta, it's missing. Yeah, yeah. Thank you, Pablo. Missing, yeah, yeah. Thank you, Babo. Um, okay, yes, uh, however, this constant will play really very minor role, but yeah, anyway, thank you. Um, okay, so while this problem may look like a simple one from the first glance, uh, it possesses quite challenging, quite some difficulties. Some difficulties. So, the most obvious one is, of course, that the function spaces depend on the power s. More specifically, the function spaces we are going to work, these are the fractional sobler spaces. And therefore, also the regularity of the solution changes with respect to s. This makes the analysis difficult, of course. So, here you could also see this just snapshots of Monty's solution, how the solution changes with respect to S on the left. So, you could see also the regular. S on the left, so you could see also the regularity of the solution changes, and here how the solution changes also with respect to delta. The second challenge I would say is that if you want to study the minimization problem, we of course need to also study the smoothness of the promethritosis solution map. So, and here the question is, what do we know about the smoothness of know about the smoothness of delta of the map delta s to u so first of all if you just look at the kernel itself um we could see immediately that with respect to delta we have a moving discontinuity of the kernel so here i just which you could see on the plot on the left and with respect to s we have a singularity which also worsens uh uh with with decreasing s therefore um if you just look at the differentiability of the Just look at the differentiability of the kernel, which is, of course, it doesn't bring us anything. However, later we will see that we can still get quite good parametric regularity results for the solution. Okay, and the last point we also kind of need to tackle is that it is well known that when we work with non-local models, specifically using like integral fraction Laplacian model, we need to tackle. Model, we need to tackle the fact that using, for example, financial industrialization would lead us with the loss of sparsity in the underlying system. And this, of course. Anyone else? No, we're not. No, we're not. She's gone though. No, she's still there. Can anyone hear us? No, um, no. I was keeping the microphone. That's why when you want to enter. If someone says, I don't think that's a you forget to ask me so I can see here. She's still on. Oh, okay. Now she's gone. Okay. Maybe we do make something. She said she was going to Ukraine. But didn't she? Wasn't it something about Ukraine? She's from Ukraine, yes, but she's saying something. I thought she was coming because she had to go back to Ukraine. Because of parents. No. You have adventurous fantasy. Have adventurous fantasies. Well, I'm concluding it from Delaware. Ah, Dela Hus. Okay, this is too nearby. Yes, at least right now. So, should we try to get in contact with her by email? Get in contact with her by email. I will try to send her email. I mean, it's clear that she's now the career. Thanks so much, yes, you can. Yes, he can. He knows he does. You're talking about me? Yes. Yeah, I'm looking for Orlena's phone number. I think I have it. Yes, I was also going to ask maybe someone has her phone number. Yeah, because if the internet is not stable, maybe she won't get the email. Yes, you're right. This one. Is Vanya here? But she could also be a candidate for knowing her number. Maybe Akil. No, I don't have her phone number. I could email her, but I mean, that's maybe Akil, do you have her phone number? I don't know if she's. Sorry, no, no, I don't. Like Avner, I have her email. That's about it. Okay.       So I just sent him my phone number because I had no other idea on how to do it correct. Great, thanks no no. Akira called her on her back power of communications.  Hello? Ah, great. Jotik. Hi. I do not know what's going on today. I have some issues with the internet. I really hope during the other talks, it happened the same. I hope it wouldn't happen during my talk, but I was a bit unlucky. I will try to resume my share screen. My apologies for this. No problem. No problem. We are not in a hurry. So okay. I'm just not sure at which point I got interrupted. So just let me know where shall I just resume? Yes. Somewhere here, maybe. Maybe the next slide also. The next one was about the challenges. Yes. Okay. Okay, so the challenges. Okay, so the challenges probably here. I just spoke about loss of sparsity of the matrices. I hope probably that was still here. It okay. So to address these challenges, we set up certain goals. So first of all, our main goal is to develop rigorous and efficient optimization framework, which we also analyze. And at the same moment, we keep the computational. Keep the computational complexity of this approach at quite the optimal rate. For this, we are going to rely on some already previously existing works to reuse certain results. More specifically, we are going to rely on the work of Einsworth and Gozer, where the artist developed efficient finite element framework for the assembly of the fractional Laplace model. Of the fractional Laplace model. And we are also going to reduce certain results from our previous work on model order reduction. Okay, I also want to mention other relevant works in this area. So first of all, there have been many works studying optimization or permit identification problem for non-local models. However, mainly this works consist. However, mainly this works considered either identifying the right-hand side to the diffusion coefficient. The works which are closely related to the one we consider, more specifically the identification of the fractional power, have been also done. However, I think the first works have been from Sprecols and Faltinocchi. However, those works are focusing on identifying the fractional power in the spectra of fractional plastic. Power in the spectra of fractional Laplacian. And the functional analytic framework of those models are very different. I also want to mention there have been interest from various others to identify fractional powers using, for example, machine learning algorithms, but by no means, of course, this list is extensive, just snapshots of some existing works. Okay, so let me talk now. Let me talk now a little bit more in details about the problem. And first, let me start with discussing about the differentiability and first of all also the variational formulation of the problem. So we consider the variational formulation of our underlying. So for the variational formulation, we are going to consider the fractional ensemble of spaces. I go here rather quickly. I go here rather quickly. We have already seen this in previous talks on Monday also. Just to familiarize this notation, so by HS ominga, I denote the fractional symbol of space, which also incorporates this homogeneous volume constraints. And we have the following bilinear form. Actually, it's a whole variational problem here, defined as follows. And to study the parametric regularity of the solution. Regularity of the solution, we are going strongly rely on the actually special regularity of the solution. So we on Monday, we already heard in talk from Juan Pablo about this topic. In our work, we focus on the regularity presented by GRAB. More specifically, I just recall that for a smooth domain omega and certain regularity of the right-hand side, we are Regularity of the right-hand side, we obtain the following regularity of the solution. And more specifically, the maximum regularity which we can get independently of the regularity of the right-hand side is S plus one-half minus epsilon. And this is what we are going to use actually in our work. So we are going to impose regularity on the right-hand side, such as H one half minus epsilon, to obtain the maximum regularity of the solution. So there is a So, the results of graphie develop for fractional Laplacian, but they can also be extended for the truncated fractional Laplacian, which we study here. Of course, this result, the special regularity results have been also extended to Lipschitz domains and also weighted sobolous spaces. But in the context of this work, we just focus on this regularity developed by Grap. Okay. Okay, so now we consider the variational formulation of the minimization problem, which is stated as follows. So for giving where we're now looking for a parameter mu that minimizes the following objective functional given as before, but now subject to the fractional PD constraint posed in the variational from in the weak form. Variational from in the weak form. So we also make a specific choice of the regularizer which is given as follows and satisfies the following conditions. And this choice has been already used in previous works, more specifically works like Spreckels and Voldinocci. And this choice ensures that our parameters S and delta remain in the admissible range. We also define the reduced cost functional. Find the reduced cost functional and we consider the following minimization problem. By standard arguments, we can show existence of the minimizer as well. Okay, so now let's come to the question of parametric regularity. So we actually touched already this question in our previous work, where we've been developing model or the reduction framework. Model or the reduction framework. And more specifically, so what is interesting here is that independently, not independently, but despite the fact that we have limited special regularity with respect to S compared to the local case, we can actually obtain quite good parametric regularity with respect to S. So, more specifically, we've been able to show that the derivative of the bilinear That derivative of the bilinear, that bilinear form is infinitely many times differentiable with respect to s and also bounded and locally Lipschitz continuous. So just a little bit about derivative. So the derivative, when you look at the derivative, you could see that it looks quite similar as the bilinear form itself. The only difference here. The only difference here is that we have an additional logarithmic term that worsens the singularity. And the main idea behind the proof, how to show the boundedness of the derivative, is actually to use to invest a little bit more from the special regularity to be able to bound this additional logarithmic term. Therefore, here in the boundedness of the derivative, we get additional. Uh, we get an additional uh factor of. I don't know if you could see my highlight here on the screen. Um, so additional factor from the regularity, which we need to invest into into the special regularity of the solution. Okay, so these results we derived for the case of finite delta, but of course, we can also extend it very easily to the case when delta is infinite by seeing. By simply realizing that the bilinear form at delta equals infinity could be obtained as bilinear form at evaluated at some fixed delta plus a low order perturbation term, which is smooth in, I'm not, which is which is just L2 term actually. So there is no S dependency here. Excuse me, I just close the window because I think it's. Excuse me, I just close the window because I think you can hear outside noises. Okay, so this was about S differentiability. Concerning delta differentiability, the situation is less positive. So, if you look at the derivative of the bilinear form with respect to delta, we can notice actually. Delta, we can notice actually that now we have not we have double integral, but now one integral is given not over our n but over the or integral over a ball but a surface of the ball. So in this way we of course avoid the singularity which we had before but and we can show the boundedness of the derivative of the bilinear form. However, if you want to go with the higher derivatives Go with the higher derivatives of bilinear form with respect to delta, we are limited by the special regularity. More specifically, we also show that bilinear form is holder continuous with a holder constant exponent theta. And here, for example, if you could see from the inequality here that, for example, having a Lipschitz continuity, we already need to require you to have. We already need to require u to have h1 regularity. Therefore, going with the higher derivatives with respect to delta, we will be limited by the special regularity. Okay. This was concerning the differentiability of the bilinear form and parametric regularity of the solution. So now we've been able to combine these results to show. Results to show the differentiability of the parameter to solution map, which is not by s, and we can show that it's local Lipschitz continuous and also Fouchet differentiable. And now, using a chain rule, we can also derive the derivative of the reduced objective functional small j and derive the following optimality system. So System. So in this approach, even though we have two-dimensional parameter space, we follow an adjoint-based approach to keep a generality of this framework. And so we also define the adjoint equation and the state equation. Okay, so next question is about the discretization. Discretization. So we employ a linear finite element with the discretization. And we define the following discretized minimization problem. And we also define the following optimality system. And we employ the gradient-based optimization algorithm. And if you looked at the gradient-based optimization algorithm, we can immediately like. That we can immediately like we already know that this would require many evaluations of the bilinear form and also the derivative of the bilinear form. As I mentioned already previously, in the context of the fractional Laplace, integral fractional Laplace model considered here, this leads to high computational cost, not only for the assembly as well as For the assembly as well as for the solution. So, while for the bilinear form itself, we can employ the algorithm developed by Answer and Glossa to obtain the quasi-optimal complexity of n log n. We do not have this algorithm available for the derivative of Berlin form. And in addition, we also still need to deal with the fact that we need to do it multiple times. So, what we are doing going to do here, we are going to invoke and Do here, we are going to invoke an affine approximation, which, first of all, not only avoids the assembly of the derivative of the bilinear form, but also limits the number of total evaluation of bilinear forms. So, what we do, we now replace the bilinear form by its affine interplant, which I denoted by A tilde. By A tilde, and we obtain and then we consider the minimization problem with respect to the new bilinear form A tilde and the corresponding optimality system. So the final approximation which we are going to adopt, we borrow from our previous work where we used a Chebyshev interpolation. So here I would like to mention that To mention that we consider this a fine interpolation only with respect to S and not with respect to delta for several reasons. So one reason we consider Chebyshev interpolation with respect to S because it's quite intuitive. We know that we have very good differentiability with respect to S. So we can use spectral approximation to get Spectral approximation to get an expect an exponential convergence of the error. However, with respect to delta, we are a little bit limited, not a little bit, we significantly limited in our parametric regularity. And in this work, we developed certain interpolation techniques. However, we do not adopt them here while we think it wouldn't provide significant speed up. So, what we do in this establishment interpolation, we represent the interpolated bilinear form, which is denoted by a tilde, as the sum of bilinear forms computed at certain values, Sm and some parameter-dependent quantities. And the same we do with respect to the derivative of the bilinear form. So, now we need to just have to deal here with the derivative of theta sk. With the derivative of thetas. So now again, we need to, because we do it for delta equal to infinity, and since we do not want to redo this process for different deltas, what we can simply do, we can just again use the fact that bilinear form at the fixed delta is given just simply as the sum of bilinear form at delta equal to infinity plus some correction from n. And we adopt this decomposition again now to just to deal with the affine interpolation of one bilinear form for a specific delta. And then we just need to assemble this correction terms, which are which are quite nice from computational point of view because they do not contain singularity anymore. So they are not so computationally challenging. Okay. So. So, as I mentioned before, using the Chebyshev interpolation as our fine interpolation approach and invoking the fact that we have C infinity regularity with respect to S, we could actually prove that indeed the error in the interpolated balinear form and Berlinear form itself has spectral order of conversion. Has spectral order of conversion, the same as error in the derivative of the bilinear form. And combining these results, we can also show, we can also derive the error in the state solution and on the joint solution. So, which would compose now of two terms. So, the first term is the error coming from the finite element discretization. So, this is a classical error which we would expect. Would we which we would expect, and then we have additional error eta, which comes from the final interpolation. So we also derive the error in the reduced cost function, derivative of the reduced cost functional. So, and here when S smaller than one half, we obtain the same two contributions, minus from the femme error, plus the error contribution from the finite regulation. So, for the femm error, we can. So, for the FEM error, we could see that for s smaller than one-half, we obtained the error of the same convergence as what we would expect from the femme error in O2 up to the log term. However, for S bigger than one-half, we get slightly less. Okay, and the reason for okay, I wouldn't go into the reasons for. Okay, I wouldn't go into the reasons, so if somebody is interested, can ask a question later, I can explain why. Okay, and then we also, when we, since we have now two contributions, one for the error contributor for the FEM approximation and another for the interpolation. So now if we choose the number of interpolation nodes in such a way that the error would be proportional to the fem error, we obtain that actually the total number of interpolation nodes would scale as log n. Not would scale as log n, where n is the number of degrees of freedom of finite element approximation. Okay, so some numerical results here. So we consider one and two-dimensional numerical results to test our developments. So we consider also different cases when we have an analytical solution and When we have an analytical solution and when we do not have an analytical solution, then we use just finite amount solution for the reference. Okay, so first we started, we will find numerically the convergence of the interpolation approach. So here on the left, you could see the convergence, just a finite element convergence of the error between the solution and the solution. Solution finite and this between the finite element solution of the interpolated problem. So, here we recover as predicted just the finite element invergence, h to the power one half. And on the right, you could see the convergence with respect to the affine approximation, with respect to the number of interpolation nodes. And we could see a nice exponential decay of the errors, which is what we expect. Errors, which is what we expect. So the stagnation here is not due to the interpolation. This is just a quadrature error. And we also verify numerically that total number of interpolation nodes scales as log n. Okay, now when we look at the error in the derivative of a reduced cost functional, so with respect to fem, the error for the femme approximation, we could Femme approximation, we could see actually that numerically we obtain a slightly better result than theoretically. However, it is optimal for s equal one half. And with respect to the affine interpolation, we could see exponential convergence of the error, which is which confirms the thread. Which confirms the theoretical results as well. Okay, so here are some also plots for the verification of the computational time. So on the left, it's also it's n equals missing one and two on. Okay, so on the left, it's in 1D and on the right it's in 2D. These are just the computational timing of assembly of bilinear form together with the correction term C, which is needed when we decompose. C, which is needed when we decompose the bilinear form into the finite delta and some correction term. And you could see that both in 1D and 2D, we obtain the same results, what is predicted, that is using this final clustering approach together with a fine interpolation, we obtain the complexity of the algorithms would be n log to n plus 1n, where n is the One n, where n is the number of degrees of freedom. Okay, and the last numerical example is that we just look into the empirically looked into the stability of the algorithm by testing it by considering it on different discretization meshes. So, by varying age, we could see that the number of iterations stays relatively. Stays relatively constant, which suggests that empirically the algorithm behaves in a stable manner. And I think with that, I will just conclude that what I have presented. So we have seen a rigorous framework how to identify fractional power and extent of non-local interaction in the fractional appliance models and how to do it efficiently. It efficiently. So, more results. So, these results you can find in this paper, but I also put two other references for the results which we introduced in our work. Thank you with that. I will conclude and welcome any questions.