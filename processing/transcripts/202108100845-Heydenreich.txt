I would like to speak about this work, which is joined with Nanan Hau, who is a PhD student here in our group in Munich. So it's about scale free percolation. You see here already a simulation, and I shall explain first, like give a motivation for the model, and then focus in particular on the distance problem. So here's a little motivation, a few buzzwords that I assume kind of are well known in this context, but somehow so. Well, known in this context, but somehow, so we want to study a mathematical model for real-world networks, for complex networks, as a random graph. And somehow, which is maybe a bit unusual to some people here, that I immediately want to study that graph in an infinite volume limit. So it has infinite size in the sense of infinitely many vertices. And then scale-free means, well, my understanding is that the degree distribution. Such the degree distribution of a fixed vertex of a given vertex, it's a random degree. Somehow, the distribution of that is a Pareto or Paolo has a power law, or let's say an approximate power law to give us a bit of flexibility. Then the next world is small world. So what does it mean? So it's often defined as the typical graph distance. So the number of edges on the shortest path is logarithmic in the number of vertices, but since we have infinitely many vertices, Number of vertices, but since we have infinitely many vertices, we kind of assume that the vertices are embedded in some space, say it's a Euclidean space. So, and then we want that the graph distance, so the distance on the graph is bounded logarithmically or say polylogarithmic in the physical distance between these two endpoints. So, that is kind of a somewhat unique. Is somewhat un there is kind of a version kind of of small saying what small world is for infinite graphs. And then sometimes it is even called ultra-small world, and then if this is log-lock. And then there's this notion of geometric clustering, which basically means that this network has like many triangles, and that's explained often with homophily. And this geometric clustering appears very natural also if we embed our vertices into space and say somehow short-range edges. Space and say somehow short-range edges. So, if like the two vortices are close by, then if then they are connected with vibratory, then if they are further away. So, this kind of this whole setup as kind of I was putting it down here kind of naturally requires a spatial embedding. And so, I would like to study that spatial random graph. So, in order to motivate the model that I want to come up with, here's a start. So, the first try. So, the first trial model would be the Norse Hai2, which is somehow in this Chunglu class, and maybe many of you have heard it. So, that's a model with only finely many vertices, okay? And we equip every vertex with a random weight, and that weights have already a power law of a certain parameter, 12, which you can fix any number larger than one. And then you can give a lower bound if you like or not, and some, but somehow you want that this has these weights have power load distribution, and then the probability that. And then the probability that two points, say X and Y, are connected is somehow this one minus e to the minus some new parameter lambda, which we can think of a population parameter, and then the product of the two weights of these two vertices and divided by the total number of vertices. So you see here a simulation of this model. So you see some here, this big red dot has a very high weight. A very high weight, and that's, I think, already on the logarithmic scale. So, these very high weights say kind of trigger very good connectivity of that vertex. That's a rank one model because somehow this is only a product of these two things. Lambda, you can think of a percolation parameter and sense that the higher lambdas, the more edges there are. This one minus e to the minus is, well, I mean, one way to squeeze everything between zero and one, but you can also think of that as. But you can also think of that as having like a Poisson number of edges between x and y, and the Poisson parameter is precisely lambda times this product divided by n. And then you kind of merge all these edges together, and then you would also come to this notion. Good. And the degrees have indeed a power law with this model, which is maybe not surprisingly because you kind of plug in a power law as a weight distribution, you get out. Law as a weight distribution, you get out a power law. A second model which I use as inspiration is long-range percolation, which is like coming from statistical mechanics as well. So this is already an infinite model. So the vertices are the elements of Z D. And so between any pair of points in Zd, I make an edge with a certain probability, which depends on the geometric distance between. Geometric distance between these two points. And somehow I use something very similar: one minus e to the minus, and it's again the same lambda, but then I divide by this Euclidean distance between x and y to the alpha. So that means somehow the further these points are away, the less likely it is to draw an edge. So I have again this parameter lambda, which is a pervasion parameter, but now I have this exit parameter alpha, which characterizes geometry. So if alpha is zero, then I basically any Then I basically, any edge, no matter how long it is, has the same probability. Whereas, if alpha is large, then somehow I pay a high price for long edges. Yeah, then I kind of favoring very short edges. That's kind of what's a model. This is locally finite in the sense that degrees are almost truly finite if and only if alpha is larger than d. So the alpha should be not too small. Otherwise, we have not locally finite. Okay, now I want to put these two models. Want to put these two models together. So, this Norznaitu model, which comes from this, has this inhomogeneous structure, and the long-range model, which kind of catches the geometry. I want to put these two into one model. And this is what is then known as scale-free percolation. So, I take as a random graph model the vertices are the elements of Zd. Every element in Zd is assigned a random weight. So, somehow I still put the weights from the I still put the weights from the Laura-Scheicher model, and then I make a connection between these two between any two vertices with this property: one minus e to the minus, and then this again the population parameter lambda, the product of the two weights divided by the physical distance to the alpha. So we have the three parameter. Again, alpha was kind of controlling the geometry. Somehow, the bigger alpha, the shorter the typical edges are. TOUL was responsible. Tor was responsible for the weights. So, if Tor is large, that means that kind of most of these weights are rather small, whereas a smaller value of Tor is kind of favoring like a very uneven distribution of the weights. And Lambda was this edge density parameter. In a paper by Diefen von der Hofstedt and Dohimstra from 2013, this model was proposed and coined scale-free percolation. point scale-free percolation and then in another paper by depress hazer and wutrich shortly afterwards it's called long range inhomogeneous long-range percolation which is also kind of a name which is not not not inappropriate and a finite version with finitely many vertices was considered by bringman koch and langler under the name jerk or geometric inhomogeneous random graph so i stick to scale free population which was also later used in follow-up papers by these authors Used in follow-up papers by these authors, at least for the version when there are infinitely many vertices. And this, of course, you can kind of study that with finally many edge with finitely many vertices and then take a limit, which is like a finite size. Or if you like, this infinite graph is a local limit of that finite graph. That's a model. So if you have questions about the model, then there's a good. Then now is a good moment to ask. You see again simulations of Nauschreitu and long-range population. You see, somehow the long-range population is very homogeneous, whereas in this scale-free population, somehow due to the weight structure, you're creating this hubs here. And this hub kind of makes real long connections, whereas like, I mean, others have only very, like, if you have a small weight, then you typically have only short connections, and there's like there's a hub somewhere in your neighborhood. So mice is more. So, why is this model interesting? It has a very rich phase diagram. So, you have the three parameters, in particular, the alpha and the tor. So, they are playing very nicely together. So, the alpha, again, was for the geometric aspect, and the tor was for the more topological aspects. And there's really this nice interplay between these two facets. And I would like to kind of show you a few properties where this is. A few properties where this is becoming visible. So, the first one is somehow when you look after the degree. So, that was the first thing. You look at the degree distribution. Degrees are always easy because you basically, I mean, sum up the probability that you're connected to all other vertices. So, there are these two regimes where the graph is not infinite. This is the one when alpha is smaller than D. So, some other vertical line you see alpha, and here you see this parameter tor. So, when alpha is less than d, then When alpha is less than d, then in this regime, we are not locally finite. And this already inherited from long-range percolation, which also was not finite. And in a way, long-range percolation is stochastically lower than this one, because you can obtain long-range percolation if you just put all the weights to one. Weights to one would give you long-range percolation. And if that one is already has infinite degrees, then this scale-free percolation also must have. This scale-free percolation also must have it. But then there's a second line, which is called gamma, and this is somehow the line gamma equal to one. And gamma is really coming as a product of alpha and tor minus one divided by d. So that's like a second parameter, which is playing a big role in this analysis of this model. So when, if tor is smaller than two, if there's so if the whites have infinite mean, then there's the second thing. So somehow in this regime, there's also. So, somehow in this regime, this also degrees are infinite, whereas up here we have indeed a polar degree. So, here we are really scale-free, but this polar degree has now different parameters. So, the white set is parameter with a tau there, and this one has now the parameter with a gamma. So, this and this, you really see that the geometry is kicking in and is changing somehow this parameter tor, which was there before. Okay, so that was line gamma equal to one. Now let's look at the question: Is there an infinite component or is there a transition for which parameter is? Somehow in this case, there is an infinite component as soon as lambda is positive. So basically for all the positive lambdas. So the critical lambda for population is zero, sometimes called robust. In particular, also there's a regime somehow where we are locally finite, but still we have an infinite component. Finite, but still we have an infinite component for all positive lambdas, yeah. And then there is a phase transition in the sense that there is a critical value. So for small lambda, all the clusters are finite, all the connected components are finite, but for large lambda, all the connected components are there's one infinite component. And that's true for all dimensions. And then when alpha is larger than 2d and gamma is larger than 2 in this regime, we are basically behaving as normal. Behaving as normal short-range percolation in the sense that in one dimension there's never an infinite cluster, but in two dimensions there is such a phase transition. So here we are like in short-range percolation as if we are next neighbor percolation. Then below this, here we were infinite, but these are the three interesting, the two interesting regimes: this one and this one. So now we're coming to distances. Coming to distances previously to the new results that I'm going to that I want to present here. So again, remember somehow below gamma equal to one, there were infinite degrees, and also below this blue line, somehow here it's very strong. Somehow, if I take any two vertices, no matter which I'm taking, the graph distance is always at most two. It's a very strong bound. Very strong bound. Then here we have the so-called ultra-small world. Somehow the graph distance is of the order log-log of the Euclidean distance. And that will give a more refined result. Then here we have up and lower bounds of lower bound log and upper bound poly log and the question is what's the right exponent to which I'm coming to in a second. In a second. And here, again, as I said, I mean, up here, it's like as if we are in short-range calculation. So, basically, the graph distance is of the same order as the Euclidean distance between two points. So, somehow, the fact that you have the whites and you have a few of these long ledges don't really help you asymptotically. I mean, all the statements are asymptotic. And I don't care about constants. So, that's what I mean with the as an interesting phase diagram. An interesting phase diagram in this model. So, let me give a few of these or sketch a few of these arguments. So, this is this model, somehow that again the model down here. So, when the graph is at most two, so this is very easy obtained. So, by somehow saying you have two vertices here, this one and this one. And so, given these two vertices, then you find annuli, which are kind of in increasing order, and you have to kind of find the right radius for them. Find the right radius for them. And then you take only the largest, the largest weight in the annulus. So, somehow, in A4, for example, say the largest one is this one. And if you take A4 like large enough, then it's so big actually that you can that there's a decent chance that it's connected to both these vertices. And you do that somehow, so you have to kind of You have to kind of tune the radius of this annuli. So you make it so big that somehow the biggest one has really very strong, and somehow it's so strong that it kind of has a good chance to overcome all this long distance. On the other hand, you don't want to make it growing too fast because if you make it growing too fast, then kind of the ones that are out there have kind of a hard time connecting, kind of bridging all this. But somehow, so you do it in a way that somehow the probability that the largest, the largest vertex. The largest vertex, or we call it the dominant vertex, is connected to both of them, is uniformly bound from below, and then Bullet Cantelli immediately gives you that one of them will have success. Well, in fact, infinitely many. Good. Now we come to the more interesting, a little more interesting. So that's a log-lock regime. So this gamma is in between one and two, and this alpha is at least d. And then the graph distance between two points, x and y, divided by a log-lock x and y. This is actually going to. log log x and y this is actually going to a constant so um and this concept is can be is explicitly uh explicitly described let me try to to uh to argue how that is so somehow let's suppose i mean here's x and here's y so how's if uh yeah let me get maybe the the the the upper bounds and so the the upper bound is kind of uh yeah programmatic for this type of of ultra Of ultra ultra-small regime. So we let's suppose we're taking the line between x and y. And now on this line, we have taking circles. So, and the tokens should be disjoint. So, let's say maybe this one, the first one, and then we have smaller ones, then yet smaller ones. Okay, so this is supposed to be circles. Oops, sorry. Oops, sorry and so on. And so you kind of do that with certain scaling. So all in all, you have about log lock of these. Lock lock of, say, the distance x minus many bolts. And in each of these balls, again, you use a dominant vertex. So the denominator is the one with the highest white, say it's one here, here, here, and so on. And then with very high probability, you have all the neighboring ones are connected. So this is connected. This is this is so. So, in that regime, so when gamma is smaller than two, somehow the weights are growing so quickly that somehow with very high probability, somehow all these dominant vertices are connected. And so the dominant vertices alone are enough to form the connection. And this is kind of the point I want to make in this talk: somehow, in the ultra-small regime, it's enough to look The ultra-small regime, it's enough to look at the dominant vertices because they are kind of governing the whole behavior when it comes to distances. And then you need to get some extra arguments somehow from the last ones. So there's like an order one on both sides, and you need an argument to connect to the destination, but that is kind of doable. And then for the lower band, you have to argue: like, if there are not enough of these. Argue: like, if there are not enough of this, then kind of it doesn't really help. So, the fact that then you have not enough to form such a connection, and with hyperbolic, like no connection will be formed. So, that will be the lower bound. So, now we come to the logarithmic regime. And so, there's really a distinction somehow in the log log regime and the log regime. So, in the log log regime, I said somehow only the dominant vertices are enough, whereas in the log regime, Are enough. Whereas in the lock regime, the ones with the high weight really have to work together. So it's not enough to be a champion. You also need to collaborate somehow in order to build connections. And so here's our new result with Nannun. So for scale-free percolations in the log regime, so we have a lower and an upper bound. The lower bound is logged to the delta one, and the upper bound is locked to the delta two. And the upper one is locked to the delta two. And so delta one is this one, and delta two is that one. It somehow only differs by alpha one and alpha two. And somehow here you see the corresponding alpha. So alpha one is basically alpha minimum. And here you have gamma over two times t, and here you have gamma minus one times t. So it looks somehow very similar, but in this regime, gamma is larger than two. So the alpha two is actually larger than Larger than alpha one, and alpha is somehow by definition is less than equal than that. So, in particular, somehow this is the case here. So, there's a regime when these two are the same, namely that is when tor is larger than three, because then all these are alpha. And then this is just all these are delta. And then there's a regime when they're not the same, but then they are, at least, they are improved upper end. Then there are at least there are improved upper and lower bounds. So, the upper bounds, I should say, which comes from delta, that is directly inherited from long-range percolation. Somehow, again, the argument that long-range percolation is the same as scale-free percolation if I put all the weights to one. I forgot about the whites. Then I can easily make a comparison and somehow, and I get an upper bound on the graph distance in particular. In particular, and for language population, in this paper, Wescope, which is very influential and it's really the key somehow to this new result, we obtain that directly. And so for the result which is more relevant somehow, we need to adapt the arguments in the homogeneous setting of long-range population and to kind of combine that with the weight structure and use the weights to estimate that. So let me quickly say something about. So, let me quickly say something about the upper bound, maybe. So, here, sorry, here's a picture of the results. So, in this regime, we are now well set. Somehow we identified the right order. And in this regime A and B, somehow the exponent is delta one and delta two, and here they are both yet matching. And somehow here it's the same as the upper bound, delta, and here it's really some. Delta and here it's really some strict lower bound, strictly smaller than delta. Yeah, let me say something about the upper bounds. So again, here that would be the statement for the upper bound. So lambda is lambda c. We do have an infinite cluster and we are conditioning on being in that infinite cluster. So gamma is larger than 2 and alpha is in between d and 2d. And then this graph distance is spammed above the fibrobility by a log to the delta 2. log to the delta two plus say epsilon for any epsilon so so here's a lambda somehow this you obtain by by simple calculation if in scale free percolation with torn two between two and three um you have three points x and y and z so let's say and x And X and Y are further away than Y and Z. Let's say here's X and here's Y and here's Z. It only needs to be on one line, but somehow the main thing is that X and Y are closer together than somehow the probability that X and Y are connected and Y and Z are connected, somehow I average also over, I mean, I integrate out over the intermediate over the weights. Then this is bounded above and below by, somehow it's a longer. somehow it's the longer one to the minus alpha and the shorter to the minus alpha times tor minus two so tor minus two is a number between zero and one in this regime here so that gives the bound somehow if i condition on the weights of x and of z say if that is somehow a and b is then they're constants of course depending on a and b somehow with the same thing and then if they are of say in some sense have is have a comparable In some sense, have a comparable order. So then we would be well set because then we would have here this alpha times tor minus one over two, which was this delta one bound. So that was the lower bound. But we cannot kind of always make this assumption because it can very well be that there's a very long edge and a short edge, and this can be favorable compared to like having two equally, two rather equal sized edges. So we cannot work like that. So somehow what we wanted. So, somehow, what we want to do is somehow take the probability that x is doubly connected to y. And this is somehow probability if he has x and here's y. And then here's certain A, then we want a probability that this is connected. This is connected. And then this should be at least. This should be at least the probability that x is connected to some point in A, connected to point Y. And this one can bound from below by constant over X minus Y to the 2 alpha 1 minus D beta. And then if we have the connection from X to Y, so that's a lower bound somehow on the probability of having being connected of graphlessness too. And then the And then one can kind of, if there's such a path here, then we can always take two of these together and somehow saying like make shortages somehow these two, these two, these two, and these two. And then somehow use this as a lower bound for this probability, which gives an upper bound on the distance. So in this way, we can combine. So, in this way, we can combine the weight structure that we obtain here with the model for long-range percolation in a rather easy way. For the lower bound, we have to work much harder and we have to use this hierarchical structure of the concept of hierarchical structures of this group and combine this with the weights here, and then obtain this lower bound. Okay, I see. Okay, I see my time is. Is it kind of up? Seem the idea it's over. Oh, sorry. I'm sorry. I kind of lost that. And then I stop here. Thank you very much for your attention. Thank you very much.