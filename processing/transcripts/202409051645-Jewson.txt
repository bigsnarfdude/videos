For finding a time where I could give this that wasn't in the middle of the night. So, thanks, and thanks to those of you who have stayed around for the last session of the conference. This is the second time I've given an online talk at a conference in Oaxaca, so hopefully third time lucky for me getting to Oaxaca. Okay, yeah, so for those of you who don't know me, I'm Jack. I'm a senior lecturer at Monash University in Australia. And this is some work I did last year with Chris. Did with last year with Chris Holmes, who's Oxford, and his wonderful PhD student, Sarah, who I think is now at Google DeepMind. Okay, so protecting data privacy is increasingly important as we start to work with applications that involve some kind of sensitive data. Differential privacy is the established statistical framework to quantify this. I think some people might disagree with that, but for example, the US Census is now releasing dates, is now releasing statistics that are differentially private. Statistics that are differentially private. It's certainly a convenient statistical framework and it's kind of become the adopted one. Achieving differential privacy requires the introduction of some kind of noise. And I'm going to try and motivate that for you today. And so doing either reduces the power of your analysis or adds some kind of bias in terms of what you're doing. So I talked about adding noise. And so one way you could think about adding noise would be to sample from a Bayesian posterior. And so this turns out to be quite a convenient method. Be quite a convenient method for DP estimation, but it has in before we started working on this, it had kind of incredibly strict bounding assumptions and also introduced quite a large amount of noise. You were introducing a lot of noise in order to get differential privacy. What we did in our work is we, rather than sampling from a kind of a traditional Bayesian posterior, we sampled from a generalized basin posterior targeting the minimization of the beast divergence. I'll define all these things for you between the model and the data generating process. And we showed that this could provide this could. And we showed that this could provide differentially private parameter estimation, that was much more precise than what current methods could do. We could both, we could kind of facilitate such an approach for methods that weren't currently available, and we were more precise for kind of models that were currently possible. And so, this appeared at NERIPS last year. Yes, so let me tell you a bit more about this. So, yes, this machine learning analysis are increasingly being done with sensitive information. Some examples of these. Information, some examples of these, some kind of personal user preferences, electronic health records, or defense and national security data. And this raises concerns about the extent to which the results of those analysis leak information about those who took part, so the members of the sample. You want to release the result of your statistical analysis, but you don't want that to potentially identify any one person who may or may not have been part of that analysis. So we want to draw conclusions from data without releasing that information. information. But if I anonymize, if I anonymize my data, then surely that's enough. Surely then everything's private. That's not the case. So what we mean here by anonymizing would be kind of to remove anything from the data that could identify a person. For example, maybe that's names and addresses or something like that. And so that's what people were doing. And then there's this example from Sweeney in 2002 that showed basically just anonymizing the data is not enough to guarantee privacy. What they did is they. What they did is they cross-referenced some anonymized, I guess anonymized here in inverted commas, but they cross-referenced some anonymized publicly available medical records and also publicly available voter registrations in the US. Basically cross-referencing the two data sets, they found that both data sets contained zip code, date of birth, and sex. And as a result, they were able to uniquely identify some records in the data, including the governor of Massachusetts. They were able to basically obtain the medical records of the governor of Massachusetts without. Records of the governor of Massachusetts without that person wanting to release that themselves. And kind of the key takeaway from this: so that the medical records were themselves anonymized, but when you ensure privacy, you must account for any information, any other information some kind of attacker might have. And that's the problem. They didn't account for this publicly available vote data set. So that's why kind of anonymization is not enough and we need something else. And so I. And so I said that privacy was going to require noise. So, yeah, so to achieve this notion of differential privacy, you require some kind of noise. Well, to achieve privacy full stop, you require some kind of noise. And it's kind of a motivating example for this. I really like this one. This is a very old reference here. And the paper is called Randomized Responses, a Survey Technique for Limiting Evasive Answer Bias. And so the other is here: you're interested in some binary outcome, and it's the answer to a sensitive question. So, for example, you could ask, I think this is the paper was most. Could ask, I think this paper was motivated in sport. So, have you taken performance-enhancing drugs in your career? And this is my maybe more up-to-date question: would be: will you vote for the far right at the next election? So, the interest in this is in theta, which I define as the probability that X is one across the population. But people will generally not answer this honestly. And this kind of biased any kind of standard estimating procedure downward. You'll under-predict the people who vote for the far-right because people will generally be embarrassed to respond to that. Be embarrassed to respond to that. So, what Warner Warner came up with, rather than the person just responding yes or no, they flip some kind of biased coin with probability P. And so, with probability P, they respond to the truth. And with probability one minus P, they lie. And so, from this kind of data collection procedure, you can come up with an unbiased estimate for theta. Okay, it costs you a little bit in terms of precision, but at least you get. A little bit in terms of precision, but at least you get the variance of your estimate has increased, but at least you still get something that's unbiased. And the idea behind this is that you can answer honestly, and the noise gives you some kind of deniability. So, yes, I said I was going to vote for the far right of the next. Oh, well, that was just because the coin, that's because the coin told me to lie, right? You can tell that to your parents, and they might not disown you. Okay, so that's kind of the ideas behind why differentiating profits are important and why noise is going to be a factor and kind of why. A factor and kind of what I think is important is that so some noise needs to be added, and we want to add as little noise as possible. But anyway, just to just to kind of put in the in the exact framework I'm going to use, so we're going to look at a kind of specific case of this where we're going to look at model estimation. So there's going to be some private data sets, some kind of sensitive data that consists of feature labels and features, labels and features, sorry. And that's, yeah, some sensitive data inside. And that's some sensitive data from some kind of data holder holds this data. The features are going to be d-dimensional, and we have some kind of labels. Well, that could be either continuous or discrete. So, yeah, the trusted data holder basically takes this data, they fit a model whose parameters are theta in some way, and they're going to then release some function of these parameters. So, that's going to be my theta tilde d is something they're going to release. And yeah, this likelihood function f describes it. Likelihood function f describes the relationship between x and y. This is kind of one particular example of an inference exercise you might want to do. Okay, so the kind of most, certainly the most used framework, most used and analyzed framework for privacy is differential privacy. And so differential privacy quantifies the extent to which releasing this theta tilde would compromise the privacy of any single one observation. And so here is the definition. So we let D and D Here is the definition. So we let D and D prime be any neighboring data sets. And what a neighboring data set is a data that differs by at most one observation, so at most one feature label pair. So D and D minus one are the same for N minus one observations, and the Nth observation is different. The order of that doesn't matter. And then we have a randomized parameter estimate theta tilde that is epsilon delta differentially private. So epsilon delta are two kind of constants, both greater than zero. If for any For any event, any output set A, this inequality holds. So the probability that the parameter was in A given D is only an X dimension in omega away from the probability that that same parameter estimate would have been in A given different data. So the data that's differ by one observation. So I'll have this epsilon appearing here and a delta appearing here. So epsilon bounds this log likelihood ratio. Delta is thought of as a kind of a probability of violating that bound. So delta is generally taken very Violating that band to delta is generally taken very, very small. Some people say, Oh, are you looking for the minimum delta and the minimum epsilon? The way I think about this is, as a statistician, epsilon delta is set by somebody else. Maybe it's the data holder, maybe it's a regulator, some kind of policy maker. And you as the statistician have to come up with kind of the most precise estimate you can for fixed epsilon and delta. And I'll consider kind of some different values for those as we go along. And yes, I talked about a randomized parameter estimates. This is not kind of a random estimate in the traditional. Kind of a random estimate in the traditional Frequentis way, like it's random after conditioning on the data. So we need to add some noise to our analysis. And yeah, so this estimate is random even if I fix the data set. Okay, so why were these neighbors? So the idea here, you have these D and D prime are neighboring data sets in the worst case scenario, an attacker could know N minus one of the observations. And then even in that case, where this attacker knows what the N minus one observations are, you still want to protect that. Are you still want to protect that nth observation? They can't, even if they know n minus one of them, be able to work out who the nth one was. And that's again this idea that the final observation can hide behind the noise. You knew n minus one, you think you know I am, no, that was just down to the noise of the estimator, of the of theta tilde, the noise added to theta tilde. Okay, so what's a kind of standard method to achieve this? So, the known as the sensitivity method introduced in this in this wallpaper is kind of a popular privatization technique. And the idea is. Popular privatization technique. And the idea is whatever functional that's going to be released, make H my functional. This could be, for example, some kind of sufficient statistic of the analysis, the likelihood or the loss function that you're using, or maybe some gradient of that if you're doing some kind of gradient learning, is perturbed with noise that scales with its sensitivity, where the sensitivity is how the absolute kind of the maximum distance that function can change between these two data sets if I change one of the observations. So this is these d and d prime here is still kind of. D and D prime here are still kind of neighboring data sets. That sensitivity is the maximum amount that function can change if I change one observation. And so the idea here is if my noise scales with the sensitivity, then the noise can overwhelm any single observation's contribution to that statistic. And again, that person can kind of hide behind that noise. So the problem here is that, so this in principle works, but bounding the sensitivity often requires bounding or truncating input or output spaces. Various things need to be done in order to actually. Need to be done in order to actually get this bound to hold. So, here's an example. So, a simple example just looking at maximum likelihood estimation for logistic regression with some kind of ridge parameter penalty with hyperparameter lambda. So, Chowdhury and co-authors showed that if I estimate my maximum likelihood estimates, that's theta hat D, and I add some Laplace noise, so that each of these Z's is an independent. Each of these z's is an independent draw from a Laplace distribution. If I add some Laplace noise to that theta hat and I return theta tilde, that is differentially private. So the noise scales depends on n, it depends on lambda, it depends on the differentially private parameter, the differential, yeah, the different privacy budget, they call it, epsilon. So it depends on epsilon as well. But if I calculate my MLE and I add noise to it, then I can release these to tilde and that will be differentially private. And this works and this is great, but this kind of a key thing driving this to work. Kind of a key thing driving this to work is that the log likelihood of logistic regression is convex, or the negative log likelihood is convex, and thus this is why this works. So this works for convex loss functions. It doesn't work beyond convex loss functions. And so we were kind of publishing this in machine learning. And so lots of modern machine learning models don't have convex loss functions. And so kind of while that method of charger is great, if you're doing logistic regression, beyond logistic regression, you can't use it. And Can't use it. And so that kind of came up with Abadi and authors developed this differentially private stochastic gradient descent. So what this does is it tries to take advantage of the fact that you're training a machine learning model and stochastic gradient descent is already subsampling the observation. So you already have some kind of randomness in your stochastic gradient descent trajectory. And that's for differential probability, that's good. We need some kind of noise. And so DPSGD adds. DPSGD adds further noise to the subsample gradients and it also clips them because most gradients are not bounded. So it adds some kind of clipping to the gradients. It truncates the gradients at some point B and that basically allows for this bounded sensitivity. And then you can come up with some kind of modification of a standard gradient descent update that would be differentially private. And so you can tweak here. So new parameter, old parameter minus some step size. This, this is the I have the gradient, I clip it and I add noise to it. And so Add noise to it. And so, some kind of the privacy that you get from this depends on basically the noise that you add and the mini batch size, however you clip it, and the runtime. Okay, so these are two current things you can do. So, in logistic regression, if you have some nice convex us function, you can just add noise to where you estimate. Or if you have something more complex, you kind of have to add noise in your learning trajectory and you kind of return the endpoint of that trajectory. And so, Chowgi gives a transparent inference problem. And so, Chowji gives a transparent inference problem. So, I could return my theta tilde. It would be very easy to construct a test on where theta hat was based on theta tilde. But it doesn't generalize more or less beyond logistic regression, anything without a convex loss function. DPSGD in principle applies very generally. I can now apply that to any model I could originally train with stochastic gradient descent, but it biases analysis in a kind of a completely intractable way. Analyzing exactly how theta tilde. Exactly, how theta tilde depends on theta hat would be very, very tough. And so, yes, we add some bias, we add some noise, but we don't have any kind of transport fashion to deal with that. And both of these methods add a lot of noise. If you look at papers, you'll see they often end up using an incredibly large epsilon that wouldn't in practice guarantee very much privacy because they're having to add so much noise. Sorry, because they want to avoid adding lots and lots of noise. So, to get kind of a reasonable privacy level, these guys both have to add a lot. Level these guys both have to add a lot of noise, and so these are kind of the things that we want from our model. So, ideally, we want a generally applicable method. Ideally, we want consistent estimation. So, Charlger, yeah, Charger, I think, is consistent. Proving whether DPS GD is would be a tough challenge. We want something that's at least minimally consistent, adds as little noise as possible, and has minimal bounding assumptions. That's what we want, and hopefully, we're going to get there. So, firstly, I said at the start that sampling from a Bayesian posterior is shown to provide. From a basis in posterior is shown to provide DP estimation. So that was shown by Wang in 2015. So, firstly, if the log likelihood is bounded, so that we'll come back to that because it's generally not, but if the log likelihood were bounded, then one sample from a Gibbs posterior, where the Gibbs posterior here just means that I'm raising my likelihood to a power w. So here this would be the standard posterior. Now I'm raising it to a power w. I'm writing this in terms of this exponential. I'm just rewriting this in terms of the exponential. I'm just rewriting this in terms of the exponential of a log. So I just take my w down here after logging. So that's my Gibbs force theory. And then if I set that Gibbs weight w to be epsilon over 4b, where b was the boundaries of log local, then I get differential privacy. So the introduction of the w was just to allow, kind of allow you to scale the privacy to any value of epsilon. So if you could do this, one sample from your base posterior would be differentially private. And you can kind of a sample from the posterior. And you can kind of a sample from the posterior is a consistent estimate of the posterior mean, and thus it's a consistent estimate of the data generating parameter if the model were correctly specified. And just my kind of caveat here, I guess I'm kind of saying Frequentis words. I am Bayesian because I think it's correct and better, but in this paper, that's not what we're doing. This is kind of, it's kind of fun to write this because actually, we're being Bayesian here because it's convenient rather than because we believe it's kind of methodologically better. Or, yeah, they were, and then, and then we're. Or, yeah, they were, and then we're going to be as well. And yeah, so this, this, these are kind of the first people doing this, and this was pretty cool, but this idea of bounding the log likelihood basically doesn't hold for anything, right? Even logistic regression, kind of the simplest classification model, y is only zero or one, I have a bounded output space. Even in that case, my log likelihood is not bounded because as my probability of something goes to zero, this log blows up to infinity. So, even in logistic regression, actually, this doesn't hold. This, this, this, this, uh, this this this uh requirement here and so you can relax that slightly so um in this paper by minami uh they relax this and again they use they use the the fact that the logistic regression is convex and they say okay so if you add some the weight now depends on some other things uh not worth talking about too much but you for logistic regression you can kind of set a weight uh in order to get this uh basin sampling to work so again we have something that works really reasonably nicely for logistic regression um Yes. Okay, but it doesn't at the moment. So one posterior sampling sounded like a convenient way to add noise. Kind of, we know our posterior has some kind of well-founded notion of noise in it. And so sampling from it potentially provides a pretty reasonable estimate. But you couldn't apply this in very many cases. And where you could apply, even that Monami case adds quite a lot of noise. And so we came in. Our paper proposes this beta divergence Bayesian one posterior sampling. Posterior sampling, uh, and it kind of goes like this. So, it has been noted for some years now, so that there's a kind of this inherent link between differential privacy and robustness. Differential privacy, I was kind of perturbing my data by one observation. And if my parameter didn't change under the two neighboring data sets, then that was good. I was going to be more or less private. And that's also what you want from robustness. You don't want any one observation to be able to kind of change your analysis massively. So, if an estimator is robust, then any one observation can't overly affect it. Then any one observation can't overly affect it, which in turn means any observation shouldn't be detectable from the results of the analysis. And kind of practically, a robust estimate basically has this low sensitivity, how I defined that, and thus you have to add less noise in order to overwhelm that low sensitivity. So what we really do is we combine this idea of one posterior sampling with some notion that I really like of robust estimation. So kind of our framework for doing this builds on this work of. Builds on this work of generalized Bayesian updating. So Bissry, Holmes, and Walker showed that a posterior update of this form, where I take my prior and I multiply it by the exponential of a negative loss function, provides some kind of coherent method to update beliefs about the loss theta naught L that minimizes that parameter after I observe data. Okay, so that's going to be kind of the tool that we use. That's going to be the tool that allows us to bring this notion of robustness to a Bayesian analysis. Robustness to a Bayesian analysis. If I go back and think about how I recover this Gibbs posterior that we started with, well, the Gibbs posterior is recovered when this log likelihood is this minus W times, sorry, when the loss function is this minus W times the log likelihood. But the problem, of course, was that this log likelihood, this log score is unbounded. Particularly, the problem with logistic regressions is that as F tends to zero, log F tends to minus infinity, and that's why that was unbounded. Why that was unbounded. And so, OPS has struggled as kind of a general purpose tool for DP estimation because bounding the sensitivity of the love likelihood is difficult/slash almost impossible. But this framework of Basiri et al. kind of provides us flexibility to come up with a kind of generalized posterior based on a different loss function that might have bounded sensitivity. So, that's what we're going to do here. We can't just use any loss function because remember, I still had this, I had this model, so we wanted to estimate the parameters. So, we wanted to estimate the parameters of a model. And so, we can't just use any loss function that depends on the parameters. Like, it really ought to still depend on that likelihood function because that was what I wanted to estimate. And so, a particular case of this that I like is this beta divergence loss function. So, the loss function used by standard Bayesian updating was this minus log likelihood. And so, my beta divergence guy replaces the log likelihood by this one over beta minus one f to the power beta minus one. And so, if you take beta to one, if you just look at this term and you take. To one, if you just look at this term and you take beta to one, that recovers the log. So it's kind of a small generalization away from the log based on this parameter beta. And then this final term that I have in blue no longer depends on the observation. I'm integrating out this d bar, and that's kind of a correction term that makes sure that minimizing this is still provides a consistent estimate if the data comes from the model. So why is it called the beat's divergence loss function? Because minimizing an expectation minimizes the beats divergence. And I think I said this. So beta goes to one, you recover the negative. So, the beta goes to one, you recover the negative log. Likely, that's what the Gibbs posterior was doing. But a key feature of the form of this is that, unlike that negative log, as f tends to zero, this guy minus one over beta minus one, f to the power beta minus one, goes to zero as f goes to zero. For as long as beta is greater than one. So the odd is beta greater than one here is providing some notion of robustness, which is also going to provide us, which is also what's going to provide us differential privacy. Note this update is actually completely general. Is actually completely general. It doesn't depend on the form of the model. I can put any model in here and I can update it. I can minimize the beach divergence loss for that model. So I can apply this for any choice of prior or density SAS mass function. That's the kind of generality that we wanted. Okay, so how am I going to get differential privacy? So our plan is that rather than bounding this log likelihood, which is very, very difficult, we're going to replace in the generalized Bayesian update the log likelihood. In the generalized Bayesian update, the log likelihood with this beast divergence loss, which is naturally bounded when the density is bounded. So, this is the kind of condition I'm going to need to get differential privacy. So, if my model density or mass function is bounded, so then not the log density or log mass function, the density itself is bounded. There exists some m greater than zero, the upper bounds the density. Then it's pretty straightforward to bound the sensitivity of my beta divergence loss function to the change in one observation. in one observation and so doing gives me differentiary private estimation so under the under those under that condition uh a draw theta tilde from my beta divergence base posterior pi uh l beta theta of d is differentially private where this is the where this is my my epsilon and so uh building on um the generalized bvm that jeff miller proved in in jmlr we can then say that okay Proved in JMLR, we can then say that, okay, theta tilde is a consistent estimator of the kind of long-run beta divergence minimizing parameter. And then, if the data was generated from the model, if that's only an if, but if the data was generated from the model, if our model is correct, then theta tilde is also a consistent estimate of the data generating parameter. So, if our model is correctly specified, if we have data from the model, then we're learning about the same parameter as we would. Then we're learning about the same parameter as we would if we use this Gibbs posterior. We're just learning about it in a differentially private manner. Okay, so some examples. So, the one condition I had was that I required that the mass or density function to be bounded. And so, if I consider binary classification, and I write this pretty generally, so I'm just taking the logistic function here of some mean function, m that depends on my features x and some parameters theta. Some parameters theta for an arbitrary mean function. So I'm doing some kind of binary classification. Then clearly, this, because it's the logistic function, is the mass function is bounded between zero and one, independently of this functional form. Whatever I choose m to be, this cannot be any greater than one. So that immediately gives me this bound I need for the beta divergence. And thus doing this means I can guarantee differentiable privacy like this. So my epsilon here will be two over beta minus one. And thus, if somebody gives me a value of epsilon, I can reverse engineer. Value of epsilon, I can reverse engineer the value of beta that gives me that, and away I go. And so, whereas the one-pozzer sample could prove it just for logistic regression because it needed it needed this convexity, here this works for any mean function. If you put a neural network in there, p is still bounded between zero and one, and this still holds. You would still, your parameter estimates would still be differentially private. So, not only, okay, it works, and we also. Okay, it works, and we also could do the kind of standard one-poster sampling in this scenario. So, I just want to show you why this piece divergence is actually better. It gives you more precise inference. So, this dotted line is the standard logistic function. And then, so the Gibbs posterior required this W to be 0.09 to get dp with epsilon equals six. Whereas under this beta divergence, the kind of equivalent beta parameter was 1.33. And so, if I plot these, the blue line is the kind is the value. Line is the value is what the loss function looks like from the Gibbs post theorem. Basically, because that W is so small, the logistic function is almost kind of completely flattened out. Whereas the way that this B2 divergence thing, this B2 is 1.33, okay, yeah, it's flattened somewhat from the logistic function. So I'm losing, I do lose some efficiency from the data, but it tracks it much, much better. So these two both gave the same differential privacy, the same level of privacy. It's just that the way the beta divergence adds that kind of balance. adds that kind of bounds that sensitivity is a it takes less away from the data thing it looks more like like the logistic function that's where you're able to get more precise inference so i mentioned that this would also work for neural networks so yeah modern machine learning is not just being done with logistic regression people are using neural networks and yeah the our the way our bound worked it didn't depend on the exact form of m we it just it was bounded for any m and so we can use a neural network in there uh previously you would have In there, uh, previously, you would have to use DPSGD for this, and now you can just now you can actually use this kind of one posterior sampling. So, we've been able to kind of bring one posterior sampling to a new a new class of models. And it also works beyond classification. So, consider now Gaussian regression. So, we want to predict some continuous y, again, a general mean function and some kind of residual variance. There is kind of an extra condition here. So, in order for this density to be bounded above by m, you do. To be bounded above by m, you do need a kind of minimum on the variance. So, sigma squared needs to be strictly greater than some s squared. But if you're doing this as a Bayesian, you'd be putting an inverse gamma prior on sigma squared. You would be effectively assuming there was some s squared that sigma squared was bigger than. So, while this is annoying, I don't think this is actually at all restrictive in applications. There's always always some kind of residual variance that you know your variance is bigger than. And you can always kind of, okay, if you didn't, you could always just add a bit of noise and then you would. And so, in this case, then the density is bounded. It's bounded above by this guy. And again, using that formula I had for what epsilon was, you can get DP estimation for your value of epsilon. Again, why does this work? So, this wasn't possible. Gaussian regression is pretty tough to do with DP because the response is unbounded. And this is kind of why. So, in the dotted line, I plot what this, the dotted line is kind of the standard least squares, the standard squared error loss. Least squares, the standard squared error loss. W here would be if you were to try and do one posterior sampling, with I can't remember exactly what W is here, but some value of W, this is what it does. So it flattens out that least squares. So the sensitivity is less than it would be under this guy, but it's still unbounded. Still, this will still, this kind of trajectory here will still go off to infinity here. And here it'll just go off slower. On the other hand, this beads divergence loss, so around kind of around zero, this beads divergence loss tracks the least squares. Divergence loss tracks the least squares. It looks a bit like the least squares, but then away from zero, it eventually kind of comes back on itself. It actually plateaus. It comes back on itself and plateaus and becomes absolutely flat. And so the way this B divergence loss, yeah, it becomes completely flat. And that's why you can get this differential privacy. That's why you can actually bound the sensitivity of the beta divergence loss here. And that's why, while you couldn't do one posterior sampling for Gaussian regression, you can do it with the beta divergence. Okay, so everything I've told you, hopefully. So, everything I've told you, hopefully, sounds very promising. And the experiments are going to show that it was promising. I have done one tiny bit of cheating that I need to own up to. So the one posterior guarantees that we proved, or we proved that one posterior provided, one posterior sampling provided the DP estimate of a sample from the exact posterior. But the posterior is typically not available in closed form, and so we would normally use MCMC to approximate this. use MCMC to approximate this. So the question then is this still DP? There is a formal, I have a word answer here. There is kind of a formal answer to this. So if the chain is ergodic and you run it for long enough, then yes. And this is, I think most people at this workshop are based on so this is what we do, right? We hopefully come up with a nice MSMC algorithm and we run it for long enough. And then we're happy that that model's our posterior. And so what we did in our experiments is we used STAN and we assumed this was satisfied. And that's what almost everybody had done, or the previous papers. Almost everybody had done, or the previous papers had done in the literature as well. Uh, this only, I think, as Bayesian statisticians, this is enough for us. Uh, it's potentially not enough for some people in the DP community. They want to know that exactly the estimate that they're given is differentially private. And so, if you wanted exact, exact guarantees, uh, there's kind of a parallel stream of literature that looks at like rather than analyzing the posterior, you actually analyze the Markov chain generating the samples. And so you kind of add noise to that chain in order to get DP there. Chain in order to get DP there. This is kind of, I guess, is ongoing work. I don't have anything to show you, but I have a strong suspicion that this piece of divergence phase will make these methods better as well. And that's something kind of I want to check out. Okay, so just to show you some experiments that this works. So this is a simulated logistic regression case. And I'm looking at the, so I have some true generating parameters and I look at how the log MRSE of my theta hat versus the true theta changes with. hat versus the true theta changes with n observations on the x-axis and i have epsilon small getting to big so bigger epsilon is less privacy so this is really private and okay in this case and so our method is this orange line here so for epsilon really small okay most of the methods do the same i'm plotting this this is for logistic regression so i could do this chowdery and the one posterior sampling of minami so for epsilon being very small okay there's not not really a lot to say here uh but for epsilon any bigger than But for epsilon any bigger than 0.2, we see that this is our line, is this orange line? And as we kind of collect more data, we can see that we're getting this consistent inference. The RMSC is going down to zero and going down much faster than these other methods are. Okay, so it seems to work pretty well for logistic regression. So that was a simulation. We did this for some real data. So now I don't have an idea of this RMSE, but I do have. Don't have an idea of this RMSE, but I do have some kind of test set error. So I'm now plotting these, these are ROC, AUC curves on the test set. And so once again, we're these purple lines on top. So this is not very much privacy, and this is a lot of privacy. And you see, and again, the X axis is the yeah, sorry, this is sorry, this is not an ROC curve. This is what the AUC would be, where N is the X-axis is increasing. And again, you see that our orange method is appearing on top. Orange method is appearing on top here most times. Okay, so yeah, it seems to give us more, it seems to kind of add noise in a more efficient way or add less noise than these competing log-likelib methods do. We also looked at neural network regression. So this wasn't possible with this one posterior sampling, but it was possible with DPS-GD. And again, same plots, basically, these are test set RMSE. This is a very private case. This is a slightly less private case. And yeah, I guess the summary is. And yeah, I guess the summary is we're doing pretty well here, particularly as N, particularly for big ends. Sometimes, like in for small end, there's some cases where DPSGD is better, but as you get, as N gets bigger, we do kind of win out, which is really nice. And this was, yeah, this was a case where this one posterior sampling was kind of previously not possible, and people were having to use this DPS-GD. Okay, so I think I've pretty much run out of time, maybe lots of time. Maybe lots of time. So, just to conclude, we showed that B-divergence one posterior sampling produces consistent parameter estimates that are epsilon zero DP, provided the model's density or mass function was bounded from above. The B2D-based one-buster sampling was able to improve precision over standard DP inference for logistic regression. And also, so it was better in cases where one posterior sampling was possible and extends to more complex models such as neuronwork classification and regression. And then, just as a further And then, just as a further work, so here we just looked at producing point estimates. Here, we were only interested in producing some kind of one estimate of theta. What if you wanted to do inference as a Bayesian? You'd probably want some multiple posterior samples. So, is there some kind of trade-off with the inference you can do there? And we relied on this MSMC chain being ergodic to achieve DP. That was probably fine for logistic regression. I expect that was not fine for neural networks. The experiments we did were only with relatively small neural networks. We're only with relatively small neural networks. Yeah, I guess you wouldn't expect Stan to come up with a ergodic Markov chain for a neural network. And then just finally, is DP actually what people want? It is the most convenient statistical framework to do this. And it's the one people at the moment seem to be using and doing research in, but it's very, very restrictive. You have to have these two neighboring data sets. For any possible output, for any possible value of any of the n minus one or the nth observation, this has to hold. And it does give you very strict privacy guarantees. has to hold and it does give you very strict privacy guarantees but often the noise you have to add overwhelms overwhelms got the thing so is is there a is there uh yeah another definition that might allow us to do a bit more okay thank you for staying with me to to the end of the workshop any questions Great talk. Oh, this is Dhutika, by the way. Hey. Hey. So, one thing I was wondering about in generally in this area, and I'm probably going to butcher the things I'm going to say, so ignore my words, but hopefully you'll understand my question. But do you require bounded loss functions, or do you require the size of the optimization steps to be bounded? So, as in, you're saying for the. the as in you're saying for the you're saying my method or the or in general in the area yeah so depends so it depends on exactly what you're doing so if you there's basically two ways you either come up with some end estimate and add noise to it uh and that's what this chowder paper was doing they were able to do that because things were convex in this sgd uh you end up but you you basically assume you release the whole trajectory so you need each one of those updates and that update depends on the gradient so you need the gradient of the loss to be bounded loss to be bounded and so the assumption is that the great the the the gradient must be bounded or the optimizer involving the gradient must be must require bounded updates i guess if you so the way it's presented at the moment says that because this is doing sgd so that it's just following the gradient so it needs the gradient update but i guess yeah if you had an optimizer with bounded updates that would be enough okay because this uh it reminds me of like uh giacomo zana Of, like, uh, Giacomo Zanella and Livingstone are doing this sort of stochastic Barker's dynamics sort of work. Uh, so people have so some of this DPM CMC stuff, uh, people have used Barker's algorithm to do that. I don't think I, off the top of my head, I know exactly what Barker's algorithm does, but people have used Bark, people have added noise to, yeah. So, it's certainly like Metropolis Hastings is pretty difficult to do this with. Uh, people have done it with Barkers. Okay, awesome. Thank you. Hey, Jack, it's Trevor. Nice talk. I had a question, and this is probably a very stupid question, but it seems like if you, would you be able to sort of adjust the privacy level per, let's say, data point in your data set by having individual data parameters for each one of them? I'm not even sure if that's a sensible question. Maybe you can comment. You are slightly faint. You're slightly faint, Joe, but I think you said, can you have some kind of I think you said, can you have some kind of privacy per observation? Was that more or less your question? Yeah, can you hear this one better? Yes. Okay. Yeah. So that's exactly what I was asking. Yeah. So there, yeah. So this is, so this, I'm working here under what is called, well, I've always just seen called straight differential privacy, where, yeah, I assume that kind of each data holder, each, sorry, each, each observation is happy to give their data point to some kind of trusted person. And that trusted person collates all of the observations and then releases something. All of the observations and then releases something. There is another framework called local differential privacy, where it's like each person doesn't even trust the data holder. And so they kind of add noise to their individual observation often. And that's called local differential privacy. And so that's like an extension. And yeah, like we could absolutely explore whether this applied there or not, but we haven't yet. Would that correspond to just having a beta parameter for each one of your terms in your total loss function, or would it be something different? Like, naively. Like naively, I feel like yes, but I don't know. Is the honest. I've sat down with somebody to just think about this briefly, and I don't think it was quite as simple as that, but I haven't thought about it quite enough. But it's yeah, I don't know. Okay. Thanks. Thanks, Trevor. Any more questions? If there's no more questions, I'll take it. Thank you very much for staying to the end. And hopefully, I can come to Oahaka next time.