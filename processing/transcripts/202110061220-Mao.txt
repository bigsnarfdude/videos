Maggie's talk. Maggie did a postdoc at Samsey, and then she joined eBay, and she's going to tell us about experimentation. Yeah, thanks, David. Yeah, so yeah, like during the last two, three days of this workshop, we all know that computational atomic. That computational advertising somehow related to experiments, like the Eyo talked this morning, like the geo experiments. And also, Tim talked about using experimentation to measure the survey brand, to measure the brand lift using survey. And also that's also A-B tested randomized groups. So like computational advertising is a lot being related to the experimentation. Being related to the experimentation. And today, I'm not like other people talking about more advanced experimentation designs, but I will focus more on the like the basic classic A-B test and I will talk about the best practices. Okay. So here is the agenda today that I will first talk about the backgrounds put as situated in the situation. In the situation, what I'm talking about. So, I will talk about what's online-controlled experiments. And then I will talk a little bit about the experimentation platform and eBay, and what are the common use cases. And since I'm going to talk about best practice, so I will talk about a checklist that I formed for eBay. And then I will highlight two battles for ensuring the experimentation best practice. The experimentation best practices because people don't like to follow them. So, the first battle is about mirroring hundreds of metrics versus one metric. And the second battle is about the power analysis versus agile development. And then I will conclude with a summary and discussion. And also, I would like this talk or presentation to be interactive. To be like interactive. So, if there's anything that is unclear that you have comments or questions, please feel free to raise your hand or just speak. There's no problem to me. Yeah. Okay. So, like the online control experiments, also known as A-B test, and it's the golden standard to testify ideas, to quantify improvements, and build causal relationships. And the idea behind And the idea behind A-B test is very simple. You randomize the user into two groups, where the A group is the business as usual, what the existing system is, and the B groups is like existing system plus some new feature. And then you compare two groups to see how they perform like. And if the randomization performs good, then the difference between two groups are the pure causal improvements. Causal improvements like brought by this new feature. Okay. So, and the idea is simple. And eBay has its own in-house self-service experimentation tool that's serving for like every domain that at eBay. So it's not like every team will do its own A-B test, rather, we have a centralized platform that everybody can come. Platform that everybody can come here and use the platform and click a few buttons to start an experiment, and then we will give them a report and they can see how their new feature looks like. And we have like thousands of experiments running per year. That depends how you calculate experiment. That could be the number could jump up a lot if you calculate it in a different way. And we support all kinds of different use cases. And here is a graph that I built for. Graph that I built for eBay to summarize what are the different use cases that they are using the platform, not necessarily A-B test, but they using platform in a different purpose. And in here, I want to highlight two use cases or two types that may be useful in later understanding my later slides. That is the classic A-B test. I call it a classic A-B. I call it a classic AV because I don't know what the word is. That is, so when users have a new feature, they come to conduct an experiment, they expect some metric to improve. So they want to see that our platform is being optimized by some way. So that's like the classic A-B. And there's another kind that we call it tech migration is like the back-end service migration that they used to have like an older. Used to have like an older tech, like call it text deck and a new text that. So they want to like move from the old text that to the new text that. And in this process, there is no intention to improve any metric. All they want is that everything stay the same, no break, no harm to any of the metric. So that's the two kinds of like typical use cases. And yeah, come back to our platform. So because this is So, because this is an in-house self-service platform, so it's the program managers who like who owns the new feature and also the developers who develop the new feature. They are the main players of the experimentation platform rather than data scientists, analysts. So, with that being said, is that they are a group of people. They lack a statistical background, and this will lead to a lot of bad practices. A lot of bad practices they don't know. It's just like human nature, they just perform this way. So, for example, like the cherry picking, which means that they kind of like choosing hundreds of hundreds of metrics, which I will talk about later in detail. And then after conducting experiment, they pick one of two metrics that appear significant results and using that as a guidance to make a conclusion. To make a conclusion. And that is cherry picking. That's not good. And that's one type of bad practice. And also not a bad practice is the p-hacking, which means that the user, like the user, because we will give user like a scorecard or report about all the metrics, how they move every day. And the people will come to the platform and see the metrics moving up and down. And they kind of have a behavior that, okay. Have a behavior that okay. One day that my metric got the stat the statistically significant result in my desired direction, and okay, I will stop the experiment and I will claim a win. So that's also not good. So those are like examples of bad practices. Yeah, so like when I joined the eBay last year and my leadership team is like, okay, we need to prevent this. Yeah, and so that's. So that's my job to ensure that the experiments running at the eBay are following the best practice so that we can make sure that the users running experiments and running it correctly. Yeah, so I was laughing because I think of the other day, the first talk given by David Hodg saying that industry is like like like urgent files like uh like put off with urgent files so to me i uh and uh and to me i think it's this is like urgent file something that's going wrong and i need to do something to prevent this bad practice from happening so uh what i did is to um so that so set up like the standard like what is the best practice so i came up with this checklist that is like a step by step uh Like a step-by-step guidance for a user to run the experiments correctly. And there are a lot of items in here in different phases of experimentation. And I won't talk about the details. I will just highlight two battles that I find interesting and they are unsettled battles. And so, if you have any suggestion or comments, feel free to raise it up. So, the first battle is about mirroring. Marine. Maybe I stop here a little bit to see if anyone has any questions before moving forward. My comment is I love the idea of a checklist. I wonder if there are too many things to check there, but still, I'm delighted that you did that. And I think that's probably a practice that would be valuable at many other industries. Thank you. Yeah, we're trying to build this checklist into the platform itself. Into the platform itself, so that user don't need to worry about this. Yeah, thanks for the comment. Okay, cool. So that's jumping into our first battle: mirroring multiple metrics versus one metric. So we learned from textbook that a hypothesis test is testing the non-hypothesis versus the alternative hypothesis. So for example, in here, mu1 and mu0 are the means of the metric of interest in the control group and the treatment. interest in the control group and the treatment of group respectively and the noun is saying that the two groups are the same and the alternative is saying the that's not the same and all we talking about is that is only one metric and all the following up studies are for like one metric but at eBay we monitor all kinds of metrics so I put a graph here describe what the buyer journey looks like at eBay and the you know Journey looks like the eBay. And that eBay is like two-sided market. So we also have the seller journey. But I just only put here to describe how complicated things are just in the buyer journey itself. So people has a lot of touch points. We call it touch points saying that a user engaged with eBay at different site, different modules. So those are like Modules. So those are like, and each touch point, each everything will generate some metrics. And that's like monitoring the whole system from different angles. So there are a lot of metrics. And as an experimenter, they will examine a lot of metrics to make a decision. Yeah, sometimes 20, 30, and sometimes 100. That's possible. And this will. And this will impose a severe inflated false positive rate problem. Yeah, so and I will talk a little bit more about this inflated false positive rate. So we know that if all the metrics are independent, then we can calculate this false positive rate. It's a closed form and we can calculate it. But since we, even though there are different metrics, but they are marrying things, marrying the same thing, the eBay ecosystem. Thing the eBay ecosystem in a different angle, so all those metrics are more or less correlated. So, for example, what I listed here, the first one is like the home page click-through rate and module level click-through rate. So, I give this example is that because they like the home page and the homepage module, they appear in the same page, but it's just mirroring the mirroring different location. One is marrying page level click-through rate and the other. Marrying page level click-through rate, and the other is just marrying the module-level click-through rate. These two metrics must be correlated somehow. So that's the first example. And the second example is like the number of transactions and the total revenue. Yes, so the number of transactions, if we sum up all their transaction value, then that's the total revenue. The total revenue. So, those two measures are also related. So, more or less, these things are related. And maybe, like, maybe like maybe the homepage click-through rate and the total revenue, these two may be less correlated because they happened like far away apart in the whole buyer journey. So, that's maybe less correlated. But it's so with this complicated situation, then the false positive rate is a False positive rate is a little bit harder to measure what is exactly the false positive rate is. And moreover, some metrics have different roles in an experiment. So for example, like one of the use case that I mentioned earlier is the classic A-B test where user want to improve something for a new feature. And in this experiment, some metrics are chosen because users think that this new feature could improve. New feature could improve those metrics. So, for example, we will ask like user what is their primary metric, and that is often something that they want to improve. And in this case, a false positive means that the user would launch a feature even though there is no improvement. So, that's one kind of metric. And there are also other kinds of metric where we call it the Gario metric. It's like they It's like they like the experimenters will not expect things to improve, but they would expect the things to be neutral. As long as it's neutral, it's fine. So in this case, then the false positive means that they would fail to launch your feature while it's actually with no negative impact. And this actually will be more clear if it's in the tech migration, like non-Ethereum. Tech migration, like non-authority case, where all metrics are in fact to be neutral, like if there is actually inactive impact, but uh, but user, but uh yeah, oh, oh, the false positive is that if the false positive is that there is actually the data shows there is a negative impact, but there actually is not, then that will be some hinder like left behind. That suppose we deliver. Left behind that supposedly deliver the migration, but it's the schedule is far behind. So, all these things. So, what I mean here is that false positive has different meanings in different experiments, use cases, and in one experiment, it could have different meaning for different metrics. So, which makes things even more complicated. And the last point is that, like, we like uh we we talk maybe when we when i was a graduate student i talk a little i talk up a lot about like a false positive rate but but when you say false positive rate rate to the business people they kind of it's a little bit hard for them to understand like um okay i know there is a false positive but like how bad things are so this false positive rate is difficult to to to to for people to understand To for people to understand from the business point of view. So, if I don't know if there is something that is better than false positive rate to marry the thing, that would be awesome. But I don't know. Yeah. So that's the false positive rate. And so with that, so we are saying that, okay, one metric, but a user tend to select a lot of metrics. They will look at a lot of metrics in order to make a decision. Therefore, we Therefore, we kind of agree on a compromise. So, we ask the user that it's okay that you select the multiple metric, but just don't select that many. So, we asked the user to clearly define what their decision metrics are in the beginning of the experiment. And we asked them to select no more than six decision metrics, so give them a boundary, just a little. The boundary, just look at the six, the six, no more. So, this will like relatively quantify the false positive rate. And the user will also have their launch decision tree, means that they will, so after experiment ended, they will follow the launch decision tree in here. So, for example, this is an example in here, like to make decision whether to launch this feature or not. So, this will like control the false positive rate. Control the false positive rate to some extent, but uh, but there must be more false positive rate because it's examining multiple metrics, so it's inflated a false positive rate. Um, yeah, so I kind of like compromising here, and I don't know whether this is okay or this is something that we don't need to worry about that much because something because metrics are correlated, so it's not. Because metrics are correlated, so it's not as bad as I thought. So the battle is kind of in the middle that we set up a rule that no more than six decision metric. Yeah. So other than setting a threshold like this way, there are maybe other alternative solutions like industry, other in the other company talks about. In the like other company talks about combining the decision metrics into one single overall evaluation criteria. This is a good idea because this will make things easy that just the single metric and then make a decision. But it's still hard to make sure to implement in this in the practice because people will naturally look at other metrics in order to make a decision. And also, there are like other companies talk about lowering the alpha value for Garyo metrics because Garyo metrics kind of is like they have a prior that Gario metric is believed that wouldn't be impacted by the new feature. So, using the Bayesian language that a relative Language that a relatively lower alpha is okay for this kind of metrics. So that's also another way to controling the false positive rate. And also there were other people talking about using the Bayesian hypothesis testing framework, for example, the Bayes factor to replace the frequencies, the two. The frequentest two sample A t-test. So that's also like these alternative solutions are taught in the industry and maybe implemented by some other companies. But that's how we ended up the solution at eVay. And we are still trying to find an optimal solution to make sure that we That we can make things moving forward, but also like controlling false positive rates. Yeah, so that's the first battle. Any questions before moving forward? A comment, if I may. When you have lots of metrics implicitly, you're making lots of hypothesis tests, and for each of those, you get a significance probability. But these tests are, as you say, are dependent. So you cannot use Fisher's rule to pool the p-values into sort of Into sort of one summative conclusion. However, there is some literature on how to pool correlated p-values. And I'll send you an email with a link to that paper at some point in case that might be useful to you. Yeah, that would be highly appreciated. Thanks, David. Sure. Let me comment a little bit. I'm not an expert in clinical trials, but I heard. Clinical trials, but I heard that for the people in pharmaceutical company or something like that, the first important topic for them is so-called end point of their clinical trial. And I think this seems to be very, very similar to your topic. What do you measure? And over there, the same thing. Even for the recent vaccines, they were talking about how long a patient will stay in the hospital or whether this patient actually in the end will... whether this patient actually in the end will die or whether they will actually have symptoms for the COVID-19, et cetera. And I think their consideration is mostly to say what is the goal of the vaccine or do you hope your vaccine to have this advantage or that advantage? Those are actually being considered more. Just a comment. Yeah, thanks, Jahua. That's a very good comment. Yeah. So yeah, so I read a little bit on the So, I read a little bit on the clinical trial when they have multiple endpoints. They kind of use the BH, the Benjamin Hashberger procedure to, that's like a stepwise correction to control the false positive rate. So, things a little bit different from like tech industry and the clinical trial is that clinical trial is more like more conservative. So, they even, it's okay that they missed. It's okay that they miss some vaccine that may be effective as long as it's not like hurting people. But for a tech company, they are more aggressive, even though this new feature may not have that strong impact on the business value. They still want to push it forward to make it progress. This means the loss function is different. Yeah, yeah, exactly. If you kill a person, that's very, very bad. But if you lost one second, But if you lost one cell, probably it's not that big deal. Yeah. Okay, thanks for the comment. Let's move on to the next battle that I am still fighting on, which is the power analysis versus agile development. So agile development is also a word that exists in the tech company. So they want to develop things quickly, quickly, as if it's more quickly, the quicker the better. So that's their goal. So that's their goal. So, and well, in A V test, like two sample T test, we know that we need to conduct the power analysis before the experiments start. So, like, how much sample would be needed in order to detect a fast size like that? So, so, and you may be surprised that, like, at the scale of eBay's traffic, because eBay, it's a, it's, it has a eBay it's a it's it has a lot millions of users visiting eBay every day um like at this scale the experiments still need more samples it's true we need more samples because for two reasons one is that many experiments are effective only for a small subset of our user so it's not like everyone will join an experiment um it's some some feature will just impact a very small amount of user and it's very hard way User. And it's very hard for them to have enough sample to read to interpret results. So that's one reason. And the other reason is that many metrics are volatile. So we have, like, especially those revenue-related metric. So we plotted the histogram, the distribution of what the metric, revenue-related metric looks like, and they are highly, highly skilled, and they have a large variance. Variants. So, all those things will make the experiments in order to detect some signal becomes very expensive. They need a lot of samples in order to detect the signal. Yeah. And so that's one point. And the second point is that the interpretation of not statistically significant result, we know that there's not enough evidence to reject the null hypothesis. And when the the um when the when when like the product managers developers heard about the sentence they they are saying okay not enough evidence so if i keep it running uh more time then that's will we'll generate more evidence then we could get some like statistical significance so the experiment is not telling me that my feature is doing good is because it's not running long enough so that's actually like um that's that's that's That's not good behavior, just bad. So, this is also a point two. And the point three is that the power analysis, we know that it needs an effect size. And this effect size determination is subjective. So, it's like how much effect I want to detect. So, this and this user is like sitting by the product manager. Sitting by the product managers and developers, and this can be small and large, it's just like a subjective measure. They don't, we don't have a quantifiable measure for them. We have some guidance recently to pull them from historical data to provide them some guidance. But in general, it's a measurement that hard to quantify, hard to have an accurate estimation. And meanwhile, And meanwhile, eBay has an agile development culture. So there is a need for cheap and faster experiments. So users kind of want shorter experiments, then they can interpret the results. So there are some complicated issues in here. So with this, we kind of agree on the following two points that we encourage the user to choose a large The user to choose a larger effect size so that one to two week data collection duration is enough for reaching larger than 80 percent power, and one to two weeks like more shorter duration are preferred. So this is so this is more like constrained constraint the cost of the The cost of the experiment, like the duration time, and let the user play with the trap, play with the effective size so that we can make sure that like experiments can finish within one, two, four weeks. So this is also something that I not feel that comfortable with, but I have to deal with it. So that's one thing. And also related to this point, Related to this point, we are encouraging users to choose more sensitive decision metrics. So, for example, like the revenue metric, they are very volatile and they have large variance. So, we are kind of saying to them, like proposing new metric to them, for example, they can truncate, transfer the revenue metric into a binary metric. This will naturally reduce the variance. They accept this, but they, so what. This, but they so when they make the decision, they will not only look at the binary metric, but also the revenue metric. So it's just like adding a more adding one more metric to their decision tree. So they were still looking at the decision metric. They're still looking at the revenue metric. Yeah. Next point is not that relevant, so I will skip that. So that's the actual, so that's like a description of. So that's like a description of what's the power analysis and the agile development battle we have been facing. And this is a show, it's just to tell you that, to give you an impression that eBay used this traffic calculator to determine the number of weights needed. So, as previously I mentioned, that we have agreement on that no more than six decision metrics. So, we have a traffic calculator. So, we have a traffic calculator that supports six decision metrics, and then they can play with it in order to determine how many weeks is needed for that experiment. We're not talking too much details about how to use it, but to give an idea about this traffic calculator or power analysis, per se, in specific language. And so, after the experiment end, we will provide. Experiment end, we will provide user a report, a scorecard to list order, not end, but like during, during, once the experiment starts to collect data, we will provide this report to the user to show what the metrics looks like, the difference, confidence involved, those things. So you can see that we actually separated the decision metric and a supportive metric so that we encourage. So, that we encourage users to only use those decision metrics to make a decision to control the false positive rate. So it's also like, as I said, it's an unsettling battle because user will still look at a supportive metric. We even have a more like the fourth tab, there is a old report metric. It's like combining all things together. Metric is like combining all things together, so they still won't look at a lot of metric, even though we emphasize they do not. Yeah, so that's one thing. And in order to prevent the user from p-hacking, so we give them a sample size progress. So like that, that the sample size needed is the number of is getting from is derived from the power analysis. So we give them bar, make sure that. Bar, make sure that tell them that first to look at the sample size progress and then look at the confidence interval so that this will to some extent prevent the p-hacking issue. And since there are some metrics, as I mentioned, some metrics are highly volatile and have large variance. So it's almost hardly for all of the metrics reaching. all of the metrics reaching the sample size needed in order to uh sample in order to reaching the 80 power so you will see like at the end of at the end of the data collection there are some uh metrics are still not uh have the sample size progress bar for that metric are still very short yeah so that's that's more that's so that's motivate because that's motivates us to give a guideline to To give a guideline to interpret results because we can, in an agile development world, we cannot let the experiment run forever to meet the sample size requirement. So we kind of give them like step back and give them a guidance like, okay, for the primary metric, you need all the sample size needed. But for the other metrics, it's okay that the sample size is not being met. Being mad, but so being mad, but we also want to control if it's too less. So two samples, too less sample size, that's also not interpretable. So we kind of give them something in the middle. We call it underpowered. So if the sample size is between 20% and 100%, then we would say that this. Then we would say that these results are partially interpretable. So, to prove because it's not reaching the full power. And by this partially interpretable, we mean that if the results are, if the metric are statistically significant, then the user can interpret the direction of the difference rather than the magnitude. So, this is a driven. Uh, this is uh driven by we did some simulation on the type sign and a type magnitude error, and the result is showing that if the sample size is not reaching the minimum, the sample size required, the type sign error is low while the type magnitude error is kind of high. So, we kind of have like this guideline for the user that, okay, if in the For the user, that okay, if in this situation they can interpret the direction but not the magnitude. And if the sample size is not like less than 20%, we ask user to not interpret, to just give a lower bound of it. And you will also notice that if not static, in this case, our sample readout is like neutral effect. So this is also like a prevention of user keep the experiment running. User keep the experiment running forever. So, just like saying that if the sample size is already reaching this, have a certain amount, then your metric still not get a statistically significant result. Then this is a sign, there is a highly probability that your feature just have neutral effect on the metrics. So, that's our guideline for interpreting results. For interpreting results. Okay, so to summarize, so in this talk, I showed two battles for experimentation best practice. One is mavering hundreds of metrics versus one metric. The other is power analysis versus agile development. And just to show that in order to, because we also build a tool to continuously monitoring the best practice. Continuously monitoring the best practice and making a score to see how different people are doing in terms of experimentation, best practice, so that we can driven more experimenters are following good behavior. So, this is something we do at eBay. And thank you.