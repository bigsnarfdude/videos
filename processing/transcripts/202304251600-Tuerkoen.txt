So we're happy to have for the last of this afternoon session Malik Turkin who will speak on the low concavity and fundamental gap estimates on services of positive significance. Yeah, thanks so much for the introduction. Many thanks for the organizers to give me this opportunity to give a talk at this workshop. And yeah, today I'm talking about two projects. Talking about two projects. One is based joint with Gate Khan, Hein and Professor Way, and the other one is with Gate and Professor Way as well. And yeah, let's maybe recall the eigenvalue problem. We have seen it already today a bunch, but we go back to the complex case. So suppose omega is a smooth complex manifold. The classical eigenvalue problem is a closed eigenvalue problem like this. Omega doesn't have a boundary. There's also interesting questions. There's also interesting questions for the cases where omega does have a boundary. Very famous boundary conditions are the Trishi boundary condition, the Neumann boundary condition, and the Robin boundary condition. The Robin boundary condition is interesting. The fact that alpha is positive, I just do so that the eigenvalue becomes positive. It can be negative as well. But the Robin eigenvalue is interesting because it kind of interpolates between Trichy and Neumann, right? As alpha is equal to zero, we recover the Neumann boundary condition. We recover the Neumann boundary condition. If you divide this equation by alpha and let alpha go to infinity, you recover the Tirich boundary condition. So the Robin eigenvalue is really like an interpolation between Bird and Robin. Okay, so these are like really famous classical boundary problems to consider. And in either of these cases, because we consider omega to be a connected domain, then the first eigenvalue turns out to be simple. Value turns out to be simple. And I don't know if you can see that, but the inequality here is strict, and I made it in red. So we already know maybe where we're going towards. We're trying to consider the difference between the second and the first eigenvalue. Hein already gave a nice talk yesterday about that. So a really interesting question is: what about lower bounds for quantities like that? Okay? And very classically, there is And very classically, there is very standard results. For example, the Richie-Lowest estimate, so if omega is a closed manifold which has a Richie lower bound, then the second eigenvalue, which is the first non-trivial eigenvalue, is bound to be low by n times k, where k is a Richie lower bound. Very classical. And because this is an Einstein workshop, very recently, two weeks ago, there was a result proved by Won and Wohl. One and rho, and that says if you have a closed connected Einstein manifold and the sectional curvature is founded below by this positive constant k, then the second eigenvalue, if it's not n times k, it must be actually larger or equal than 2 times n plus 1. Very recent result, just very two weeks ago. But this is really the closed manifold, so like the closed manifold. So now, is there a question? Yes. Is that a question? Yes, it's a strict inequality. Exactly, it's a strict inequality, right? Like if this was equal, then we already know it must be a sphere, right? By the Hobarta theorem. So really it says if you're not a sphere, then the gap must be actually very big, larger equivalent to standard metric. Say again, can we just move this to the Say again, can you just put a strip here? Yes. Oh no, the equality case only says that the covering is isometrical sphere. But I don't understand that inside quite very well. It's just really very recent. Yeah, that's at least a quality case. Not going to be a sphere, right? The sphere always must be called entities. Person to like. Okay. Okay, yeah, so for now, but for today, I mostly will talk about the image state boundary condition that there's only as a domain, and we assume also that only has convex. Okay? That's going to be the main focus of the ongoing talk. Yeah, please. Yeah, I mean convex, geodesically convex, right? So like let's say two picks, two points in geodesic, weird geodesic always lies inside the domain. Unique geodesic always lies inside the domain. Good question. We're going to need that to be precise that matter. Okay, so let's see what we can do. So we talk about the spectral gap for the Dirichlet boundary condition in convex domains. Let's see first maybe in RN. We already have seen results yesterday presented by Hein, but I would like to go over some of them very briefly as well. So if I want to prove something second eigenvalue minus first eigenvalue, how does it come up? First eigenvalue, how does that come up? Well, take ratio of second over first eigenfunction at the complete Laplace of it, right? Somewhat very natural, and then the spectral gap really comes out here as a first-time eigenvalue of this Barcary, of this weighted Laplacian. So it's fairly interesting. So now the question of getting a gap estimate is reduced to getting an estimate of the personal of the normal eigenvalue of that weighted Lafrasch upgrade. Weighted LaFash upgrade. Okay, I wrote it that way because it's a Barclay-Emory way, so I hint at that a little bit here. And then, okay, Hein explained that result yesterday very detailed. They obtained by gradient estimates this pi squared over 4D square estimate using the log concavity of the first eigenfunction. This is all really RN though. Okay. Then The next result that I would like to mention, which was also mentioned already yesterday, but it's really worth mentioning again, is the proof of the fundamental gap conjecture by Ben Andrews and Julie Klatterbach, where they proved really a three nice Diesper estimate where D is the diameter of the domain omega via preservation of modulus of continuity along the heat flow. And that really gives a sharp estimate of these normal eigenvalues. So that's very interesting. There was some That was in 2011. Okay, the key, so the really difficult part of the proof of the gap projector is this estimate here. And I'm emphasizing that because we're going to talk about this in a manifold. We are still in RN here. This two-point estimate is really the key. And this is really proved by using a two-point maximum principle. And the way to prove that is basically adding this guy to the other side, defining the functionality. This guy to the other side, defining a function z of xy, which depends on two variables, and then applying the maximum principle to it, showing that this function must be negative. That's how you obtain that result. And that's fairly involved, as you can imagine, to take derivatives of that beast. And this gets even more difficult on manifolds. So, the natural question really: I'm a geometer, it's a geometry component. What does it look like on manifolds? What can we say about estimates like that? And the key really is the logon cavity of the first eigenfunction. So, Hein already mentioned a lot of results concerning the negative curvature. Today, I would like to really focus on the positive curvature case. So, two results I would like to mention that are really influential for the work that I'm going to present later. It is the first paper by Li and Wang in 87, where they use the homotopy method, the deformation method, to prove the concavity of the first eigenfunction. the concavity of the first eigenfunctions and obtain the gap less than pi squared over d squared where d always is the diameter of the domain okay so you define the latest exactly so it's basically hence over here basically the same idea um as heine explained yesterday it's good to maybe go a little bit more in detail because we don't need this so suppose you have a convex domain omega can you see the color okay i expected that I expected that. Let me see the time. Okay. Let me try this. Oh, that's good, right? So suppose you have a convex domain omega. And so log on cavity, let's say, is not known at this point, right? But take a very small ball, then you're locally looking like Rn. And Rn is already known. So small ball, the manifold is Rn. So take a very small omega sub zero and then take a family of domains that fill out omega in the smooth way. Omega, in the smooth way. Okay, and I'm going to explain this method in detail in a second because we're going to use that to prove our first results. But it's good that I have the picture up already. And then definitely work is, has to be mentioned here is a paper by Shou, Lily Wang and Professor Wei, and Professor Wei and Sho, all here. At least Professor Dai Shou and Wei, Professor Wei. So they proved actually really free price for Free passwords for convex domains in the sphere, and they followed the Andrew Sclutterbach program. However, the manifold version of the estimate that I showed before is quite a little more involved, right? So, of course, geodesics comes somewhat in, and the precise estimate is this. This is really also the most difficult part of the proof. So, again, to prove this is really a maximum principle. You put these guys to the other side, to find a function z of x. side to find a function z of x y and apply a maximum principle to it and try to infer that this function is negative okay so suppose there's positive maximum and then you can derive a contradiction by that assumption okay so this is the history so far most was already mentioned by hind but if there's questions please um let me know okay so yeah constant sectional curvature the world looks really nice and World looks really nice, and it's very natural to ask: okay, how about other manifolds, right? Like non-constant sectional curvature. And the following results are an attempt in trying to understand what a gap estimate would look like in these spaces and with the techniques that I just presented. And the first result is a follow-up. This is joint with Gabe and Professor Wayne. It's the first project that I mentioned earlier. First project that I mentioned earlier. And suppose you have a surface of dimension two with positive sectional curvature. If you satisfy this C2 condition for the sectional curvature, then the Hessian will be negative. So it's log concave. Hession of the log of the language is equal. Okay, so now of course you read the statement and then the first natural question is which manifold will satisfy that, right? I can write something down and if no manifold satisfies that there's not Down, and if no manifold satisfies that, it's not a good result. So, let me sell that result. Of course, fear satisfies that. Well, that's boring, we don't sphere already. But okay, let's see why. So, we plug in derivative zero, and then the eigenvalue can be estimated by Big's estimate, which is like the standard estimate for the first zero-schelti eigenvalue. And you can always make sure that this inequality holds true, right? Okay, so what about non-concepts? Right, we want to talk about non-concepts actually curvature. There, um, one example. One example that TAPE actually verified on Mathematica is ellipsoid, which you know is non-constant section curvature, but also cannot wiggle too much, right? That any perturbation you're satisfied with smooth as possible. Exactly, exactly. Yeah, absolutely. Any C4, right, Goviche is a second derivative of metric, but mass is a fourth. Metric. Not mass is a fourth derivative, then of the metric. So any fourth order deformation of sphere will satisfy that. So that's kind of nice. Ellipsa is one of them, right? There might be other ones that I have to say. Okay, so that's the first result. Yeah, please. What happens if I should see? What if this is not true, this is equality? No, sorry. Oh, then this is this equality will not work true. Then this is this equality will not hold true anymore. Is there a range of ratio between this life that we produce? See, this is uh so you mean if there's not no concave anymore? That's a really good question and unfortunately very difficult to answer because all these to compute all these things, right, what can you compute? You can compute balls with separation of variables or products and both are really really difficult already on the website. It's a very good question. I unfortunately this is I unfortunately, I mean, the looks of it should be the M, right? Not the confused. The looks are M, right? Yeah, but you can. Oh, I say only one confused. It should be M, yeah. My bad. Thanks for pointing it out. Because I gave this presentation already three times. Nobody ever noticed that before, so I'm not going to fix it. But so, I mean, you've answered your question, they depend very much on what domain activity is like. So we need some domains for. Maybe some domains related still. Absolutely. Yeah, absolutely. So on the domain omega, you have to satisfy that. Okay? It doesn't outside of omega doesn't matter really. But yeah, we just want to quantity the version, which is like this everywhere in the manifold. This inequality has to be true. Great question, though. Thanks. Okay, now I talk about gap, right? Now I gave the low concavity estimate, so it's very natural to ask: okay, what about the fundamental gap? Fundamental, yeah. And if you remember earlier, I wrote the Laplacian, the weighted Laplacian is the Markfield-Emory version. Now I have an estimate on Hessian, so I have an estimate on the Markfree-Emory curvature, which naturally gives an estimate on this Neumann eigenvalue. And that was, this is a paper by Ben Andrews and Lehman, where they computed, they compared these Norman eigenvalues of the Bachy-Ritchie-embry curvature bound and computed that explicitly in terms of that bound. That explicitly in terms of that bound. So we get this nice estimate here, right? So we get a positive contribution. Pi-square with d-square plus some positive number at the minimum of the sectional curvature in the domain only. Okay, three pi-square with d-square is the optimal one in our end. This is the one that the best one we know so far in the sphere. So it's a little bit unfortunate that there's no three in front of the pi square for d square, but luckily I have a different result today with us. Have a different result today with us where we get a little bit closer to that result, namely the following. So, this is the second project I mentioned earlier. Suppose, for simplicity, we scaled a metric such that the lower sectional curvature bound is one, then if the curvature doesn't wiggle too much, then roughly speaking, that is the Nabla, the supremum norm of the gradient is smaller than one, basically satisfied. And for domains who are not too big, we can actually. Are not too big, we can actually prove this as which is getting closer to what we actually expect. Because note that if the sectional curvature is constant, that means upper lower curvature bound equal to upper curvature bound equal to lower curvature bound, then this term will vanish it and we recover the 3 pi-squared with e-squared, which is the known result on sphere, right? Which is the work by Professor Way, Professor Dai, and Shaw that I mentioned earlier. So we recovered really that. We recover really that's a little bit more. So the problem this this estimate comes from sorry, could you phrase that question again? Yeah, does this estimate somehow is it somehow related to the bound of the width of the Of the gradient of cover? Um no, I don't think so. The gradient bound, so actually, there is no gradient bound in the yeah, so I just make to make the C2 condition easier. The C2 condition normally is like a line of like naval one. There's actually like comes this one to a power of one, to a power two, to a power three. It's really complicated, and I just simplified it. And this is also not. One. This is also not optimal. This is really, like, the constant can be sharpened here, but I just wanted to simplicity for every value somehow. Like, right, so the constants are not sharp at this point. I just wanted to give the easy, really readable results, right? But yeah, this is a proof of trick. Good question. Okay. So I would now like to spend some time on explaining the proof of the first and the second result. The first result already was mentioned partially by Hein yesterday, but I would like to go over that again because it uses a beautiful deformation method. Namely, already we said, okay, in a small ball, omega of zeros, this very small ball here, we already know this estimate is true. That is, the hash is smaller than minus. True. That is, the hash is smaller than minus couple over two. So now, if we let's find a family of domains such that we fill out the whole domain, if the claim was not true, then there must be a time t0 at which the maximum, if I get this guy to the other side, is equal to zero. Okay, now that Scrim's maximum principle, that's exactly what we're going to do. If we take this direction where the maximum is achieved, extend this to a nice vector field in the neighborhood, then we can... Field in the neighborhood, then we can actually apply a maximum principle to this resulting function. As shown like an vector field is a smooth function, right? So then apply a maximum principle to that and we obtain an inequality. The idea is then to make this inequality fail. Very, very natural. Okay, so these are really, yeah, this is really the key. To consider this as a function in a neighborhood when we extend this back up in a neighborhood, and this becomes a function to achieve this maximum. And this becomes a function to achieve this maximum proportion to it. So, yeah, by convexity, the maximum is in the interior. So, all these things I leave, you know, I put under the rug, but they will all work out. Then, okay, let's do it. Let's compute it, right? We're in a manifold. As Hein pointed out yesterday, this is a little bit difficult, but why not? Okay, so we commute the indices. That's a well-known can read in a book, right? The indices. Right? The indices, and as you can see, okay, it's really difficult. There's a curvature derivative coming out here and here. So we have to deal with that somehow. And also, there is a covariance derivative here and a second derivative. But since V is the log of the first eigenfunction, it will satisfy an equation. Namely, V will satisfy this PDE here. And since covariate derivative on trace commute, so you Covariant derivative on trace commute, so you can plug in Laplace here and reduce the amount of derivatives that hit on V. The ultimate goal is to reduce that inequality to something that does not depend on V. We don't know where the maximal is. Notice V blows up at the boundary, the derivative of V blows up at the boundary, so we want to get rid of V. We have no control. Doing so, okay, compute further, plug everything in. Okay, I think these slides don't make sense, but let me put everything together and... But let me put everything together and we really obtain this, okay? And what is nice about this? There's still some V's left, right? But we have a V1 square and we have a V1 here. Now what can we do? Complete square, right? We complete square and beautifully, this comes out. And by the maximum principle inequality that's negative, to make contradiction, we just assume this is a positive. That's a proof. And you get a contradiction. Okay. So in your serum, how sharp is the one over 18? The one over 18. Actually, this can be improved as well. So there's a choice we have to make, basically, if you go back. Yeah, so there's a choice we have to make because at the end of the day, either we Either we want here so we can improve it to a 12, but the diameter stays the same, or we improve the diameter, like make a large domain, but the 18 stays. So it's one or the other. So 18 is optimal if the array is very, very tight. It's close to Nuclear, you say? Yeah, in some sense. But, you know, unfortunately, I don't use the word sharp in all this work because I think we are so far from understanding really what's happening that we don't really know what the sharp estimate looks like on the. Know what the sharp estimate looks like on the manifolds yet. But yeah, the nice thing is if you let the curvature go to zero, the pinch goes to zero, then you recover a sphere result. Okay. Further questions? Yeah, so this was a proof of that estimate where phi square over d square comes out and we get a positive contribution from the curvature. And now let's talk about the proof for free phi square over d square, this estimate here. So I would like to give first a brief summary of steps of the proof. So here we follow the Andrews-Clatterbach program again. So the first step is really to prove that super-local cavity estimate that I mentioned earlier from the sphere paper. And what we can do on a surface of non-constant curvature is we can obtain this estimator. So remember, oh, actually, that this is actually a mistake. So I'm going to fix it in a second. So remember in the result earlier, instead of an L, we had a D, the diameter. However, because the curvature is non-constant, we have some error term coming in. That's a little bit of price we have to pay for non constant curvature. Pay for non-constant curvature. And that's also where the 18 times pinch actually comes from, because we cannot do D here. We just get L, which is D plus some error term. Okay, so we have to a little bit larger integral, basically. And then maybe I define the T and K. I guess most people are familiar with that, but let me just write this for simplicity. So for positive K. positive k we have that t and k is really the square root of k and tangent square root of k. This is from comparative geometry. I think most people have seen it before, but I put it in here and I had it on the earlier slide as well. So I actually should have had that before. But now from completeness it's here. Okay, so that is the first part of the proof. Of the proof, and then the second part is this sharp Neumann eigenvalue comparison, which also is the most difficult part of the proof. No, sorry, that's not the most difficult part of the proof. The Sharp-Neumann eigenvalue comparison is really, once you have that estimate, it's really straightforward following already from previous work in one of these two papers. In both papers, there's already a way to get it. The second one gives an elliptic proof, and there's a parabolic proof to... And there's a parabolic proof to execute that comparison. So that is really, once you have this estimate, then you're good. Then you have a nice gap estimate. Now, yeah, the really difficult part is to prove this. And as I mentioned earlier, how was it done in sphere and in RN? It was really maximum principle. So take this guy and put it to the other side and you define a function z that depends on x and y. Show by maximum principle this function must be negative. Principle, this function must be negative. Then you have proved that estimate. So, this is really the most involved part of the proof. It was already very difficult on sphere, but here it's even a little more involved. Let's try to see why. So, the major difficulty is actually, yes, so exactly. So, I define this function that's what I just mentioned. I define a function z of xy, which is just get the guy to the other side and try to apply maximum principle to it. And try to apply maximum principle to it and hope to infer this function must be negative. That's how Sphere worked, that's how Rn worked. Maybe Manifold works like that as well. Turns out that's not the case. Intuitively speaking, one encounters very difficult terms when computing the derivatives, right? So here, x, y is like always this minimal geodesic connecting x and y within your domain. This you can vary, but if you take derivative, you get the copy field. Take another derivative, you get variations of the copy field. You get variations of Jacobi photo. That is really, really difficult to deal with, and it does not seem to work that way. That is not the right answer to the problem from the low-concavity problem on manifolds with non-constant curvature, so it seems like. Instead, one has to come up with a different approach. Maybe some kind of modification of the argument. That's what I'm trying to explain now. Okay. Right. Switch. Okay, there we go. So the first step is really putting a step away from the Android Slatterback program where really there's a type of sorry about that. There's a type of sorry about that. Should be substats. Should be substitutes. Yeah, I have something to do after this. The first step is really to say, okay, maybe this function we want to have a bound on, like the upper bound for our difference of our gradients here, maybe this function can be more complicated than only a radial function. That is the first step we have to accept. Of course, it makes the computation much more complicated, right? It makes the computation much more complicated, right? Like distance function, we can take derivative law, that's very well understood, but it does not seem to work. So we have to be a little bit more open. That comes to a price. So the function that makes the maximum principle work is this guy here. So we are always in a very energy convex domain, okay? So that guy is well defined. But so let's try to understand what happens here. Take two points in your convex region, x and y, and connect. And y and connect them with the minimal geodesic, gamma of x, y. Now, this corresponds to one of that then you solve this ODE here on your interval minus d over 2 to d over 2, where d is the distance between x and y. And you impose the boundary condition that the function is 1 at the end. And then what you do, you take derivative, act is obtracted. Yeah, and then Yeah, and then magically this function will help you to make the Maxim principle work. So really it's two-dimensional, so these are basically the Jacobi fields that appear. Just times a parallel vector field. But in higher dimensions it's really, really more complicated. But we already have a candidate for higher dimensions, but I'll speak back to concerning that a little later. Concerning that a little later. But yeah, this is really the key step of the proof: is to use this function. And intuitively, it makes also sense, right? Because as we said earlier, taking derivative here comes Jacobi field. Take another derivative, you get variations of Jacobi field. And of course, when you take derivative here, you get variation of Jacobi field. These are basically Jacobi fields. Jacobic fields, the Jacobic fields in two-dimensional fields. Right? So then you intuitively hope to deal with these bad terms that come out from the Maximum principle. And it turns out that this was a lucky choice that it actually makes. Yeah. C is F, is it? Oh, yes, it's a good point. Actually, so C equal to F will work, but we want more, right? So that's a good point. So what we're going to do is F. That's a good point. So, what we're going to do is f is going to be a c and then plus a function which is going to be radial. And d is always the distance. Because note that in sphere, in sphere you have, okay, C is really nice, right? C is, okay, so this is really ODE. And sphere kappa is equal to 1. And spherical kappa is equal to one so we can solve very explicitly. And then take derivative and put everything together, we get really this is the tangent k. So then we really come back to the situation we had in the sphere paper earlier. And then this guy will help us to make the max principle work. But yeah, F will be some, like, put some radio part. Because at the end, you want to use some ODE. End, you want to use some ODE, we want to compare it to ODE to get a good estimate. That's the key. Okay, now having accepted that big step, which is really really, really difficult, I think that's really the main part of our work, then, okay, find the function psi such that the maximum principle works. Even more difficult, but what turns out to be the right function. Out to be the right function is a perturbation of the standard kind of OBE you consider here that was also mentioned earlier in the talk with Julie and Hein yesterday. Namely, you take psi to be the log of a function phi1 overline prime, and then phi1 overline satisfies this ODE, where you now get the paid price for non-construction. get the pay the price for non-constant curvature of time prime and equal to minus it okay note on sphere this term is gone and that's exactly basically recovers the work by Sheto Professor Dai Lily Wang and Professor Wayne these two papers subsets of these two papers if the curvature is constant this term is gone the world is much nicer Is gone. The world is much nicer. Can you read it? No. Which part is the problem? All of it? I can read T intersected K. Oh, yeah, my handwriting from the first grade. I try my best. Let me try it again. Maybe black is a color to go. So consider the OD5 prime prime. I wish there were a plus. It's unfortunately minus. That makes it worse. Minus, that makes it worse more difficult. Minus for TNK upper bound minus TNK lower bound i prime equal to minus lambda. The look of this function will make it work. So that's a lucky find again, and it's nice because it really recovers the result on the sphere. Okay. Okay, can anybody read that? Is that okay? Yeah, even in the back? Okay, cool. I'm gonna fall over this if that's not too long. All right. Now let's move further on to the proof. Okay, this function doesn't really look like the function I promised. What did I promise? I promised this, right? So now there's another step you have to do to... So now there's another step you have to do to get this guy, and it is a Riccati comparison. Really, really beautiful. Namely, oh well, yeah, I have it here. Beautiful, but yeah, now we've seen it twice, it's good. Applying the Mikali comparison, we will be able to prove very standard ODE that this inequality holds true and then notice that we have Notice that we have to choose L such that pi square over L squared is equal to lambda one. And yeah, so as the curvature becomes constant, right, this term becomes zero. The first eigenvalue becomes pi squared over d squared, hence d. So as promised, L is asymptotic, is close to d up to some error. Okay, and that is really the most part of the proof because now we prove that estimate that I mentioned to be the key. And the rest of the program of the annual statuber program is really known. It's a straightforward comparison once you have this estimate. That's already very known. So I don't want to spend time on explaining that proof. Okay, and for the last slide, I would like to discuss really various questions that are still open concerning these problems. Namely, okay, so first of all, I present your result for a surface, right? It's very natural to ask what happens for high-dimensional manifolds. There, so far, it's very difficult because It's very difficult because let me maybe give the intuition over why this is so difficult. You always, if you, so even for the one-point maximum principle result, you are unable to extend it so far to manifold. I have had a mention because if you do this maximum principle, let me maybe go back to that slide, you obtain terms. Here, you obtain terms this one and this one. Right? Minus two, minus two, curvature, curvature, and then have a hash in here and nabla nabla b. So for response to, it will be a term we have the curvature, tensor times the action plus nabla v, tensor, nabla v. And you now want to use. And you now want to use the equation because we always want to get rid of v, right, in the maximum principle. So we now want to use the equation La plus V plus Lambda V square equal to minus lambda. We always want to use that. Otherwise, we have V there. We don't want that. So now this does not seem to be possible as long as this is a diagonal matrix with safe entry everywhere. But this does not even happen in CPN. CPN, you still have the error where one direction of the vertical is 4 and the other ones are 1. So even that The other ones are one. So even that seems to be difficult. And we are unfortunately unable to use that technique so far. Really, because of that, we want to use that, right? But we just can't. So what comes out is a trace of this. And as long as R is not diagonal matrix with the same entry on the diagonal, we don't see how this can be applied. And there's a lot of terms that depend on V, which can be very big, close to the boundary, and that's exactly what we don't want, right? And that's exactly what you don't want, right? Okay, that's the intuition. Why the manifold case is so difficult for higher dimension, even CPN. And maybe because we're in an Einstein-Manifold seminar, what is interesting to mention is all these derivatives that appear here are Richie derivative. So on Einstein-Manifold, they vanish, and in fact, as resulting terms, you get Terms you get, they're much, much simpler for Einstein manifold. But yet, this is the overall problem. As long as we cannot fix that, there is no hope, even for Einstein manifold, to obtain nice gap estimates. Okay, and so that's the first point I wanted to discuss a little bit. And yeah, product of sphere, same problem, right? Very simple manifold, but yet this is not going to. This is not going to be helpful. And now, an interesting question is that was also raised yesterday by Hein: is: okay, in general, to obtain interesting gap estimates, what is the right condition to impose? Curvature bound would mean, okay, for negative sectional curvature, is that enough for local concavity? We don't know, unfortunately. That's a very interesting, interesting question. Our work suggests it probably is derivative of capital. It's probably a derivative of curvature, but that is really just due to the techniques we know. So, the techniques we know really use derivative. Curvature has to be very small in order to make the maximum principle work. And our work kind of points in that direction. And lastly, I would discuss. Okay, I still have two minutes. Perfect. Lastly, I would like to discuss the briefly mention that for the Robin eigenvalue problem that I mentioned at the beginning. Eigenvalue problem that I mentioned at the beginning. They're actually, it's still open in Rn, even. So the first eigenfunction does not necessarily document case. And so Robin was really right. Partial B plus alpha U equal to zero on the boundary. And one naturally expects that when alpha is very large, right? Remember, you divide by alpha, alpha goes to infinity, then you recover generously. Alpha is very large. And you recover generally, although very large, you should get a gap as similar to the gap projection because the eigenvalues are continuous in respect to alpha. But the first eigenfunction is not even not concave, which was proved by Berlin Andrews, Julius Lafenberg, and Daniel Hammer. So there even... It's open in our end. What is the right thing to do? It's a very open, very interesting, difficult problem, I think. Okay, so thanks so much. Okay, so thanks so much.