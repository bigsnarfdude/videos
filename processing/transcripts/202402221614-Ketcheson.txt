I had promised Andy that my talk wouldn't say anything about machine learning. But during the week here I realized there was something that I could say that I think is interesting from the point of view of machine learning. It's not actually machine learning, but the motivation is machine learning at least. Which is why my talk title is now ridiculously general. I could be about to say anything. So there will actually be three parts to it. So I'm going to give you like three small, short talks. I'm going to give you three small, short talks with some common threads between them. And hopefully, there's something interesting for everybody. Okay. Yeah, real quick. All of these people contributed in huge ways to what I'm about to present to you. Really, I mean, most of this is work done by one or more of them. So these are the three parts. The first part, I'll title it and throw a little I'll title it in the form of a question. When you train a map, what is the flow that you actually get? And I'll explain what that question really means. And then the second and third parts are what were under my original title. The first will be explicit Runge-Kutta methods that almost preserve energy for Hamiltonian systems. And the last one will be a general approach to modifying any time integrator to make... Time integrator to make it respect the energy dynamics, whether that's conservative or dissipative, of a dynamical system. Okay, so yeah, what is the flow of a trained neural ODE? So I found this question really interesting when Takaharu brought it up back on Monday. And I thought, oh, I think I know how to answer this question. So I'm going to try here quickly. And I'm really interested. And I'm really interested also in opinions on how interesting this question is, because maybe it's not interesting, I'm not sure. Okay, so I'll start with a really simple example, the harmonic oscillator and explicit Euler. This is basically the example that Dr. Hari gave us at the beginning of his talk, right? So we can write this system this way. It's just a linear system of two ODEs. It has this conserved, you know, the Euclidean norm of the solution is conserved in time. And if we apply Euler, In time, and if we apply Euler's method, the explicit Euler method, to it, well, Euler just says, let's step tangent to the true solution. The true solution is a circle, so Euler is always going to step out of the circle to a bigger circle. So the energy increases, and then at the next step, some more. And we all know this will end in tears. But what if you train a neural ODE using Euler's method? Euler's method, right? And you train it so that this, I'll call it G, right? This new neural network function minimizes the difference between what the output of your neural network and the true solution. Or this could be data coming from something, but underlying it, there's some dynamical system. And I'm going to sweep under the rug the most difficult part of machine learning, which is training, and just assume that we can train a perfect people. And just assume that we can train it perfectly, right? And ask, what do we actually end up with when we do that? So training enforces this, but we can ask, how well does it enforce actually that that right-hand side of our neural network is the approximation of the right-hand side of our dynamical system, the flow, as I'm calling it. So what's the nature of this flow? Not only how accurately does it approximate it, but in what Does it approximate it, but in what qualitative way might they differ? Especially, for instance, with respect to the energy. Okay. Yeah. So this picture should look, if you remember really well Dr. Hunter's talk, there was a picture like this, right? So here's our system, and with appropriate initial data, the solution is just this, right? And we're going to train it so that this holds. The true solution is that blue circle. Is that blue circle? But if you then go and solve exactly this equation, of course it depends on the h that you chose, right? The time step size, but for some particular time step size, if you go and solve this accurately, you get that black curve. Now, this shouldn't be a big surprise. Euler wants to add energy to the system, so when we train this right-hand side, it says, oh, I need to remove energy from the system. I need to be dissipative. So then when we solve Need to be dissipative. So then, when we solve that exactly, we get this dissipative behavior. Nothing too surprising there, but I guess it's worth thinking about. But the map we're getting is not exactly the same one. Of course, as you reduce the time step that you're training with here, that black curve will get closer and closer to the blue one, right? It will be spiraling in more and more slowly. Because, okay, what is that actual function g of theta, or g theta? Function g of theta, or g theta. You can actually work out what it is. I'm going to explain how I came up with this answer in the rest of this part of the talk. But you can see it's expressed here as a power series in H. This part is exactly this, the original system, right? That's the part that doesn't depend on H. And you see that the next term, whoops, which is O of H because Euler is first order accurate, right? This is just, you know. This is just a diagonal exponential decay. And then there's more stuff. And you might look at this and say, hmm, those kind of look like sines and cosines, maybe divided by h, it could be exactly right. And there's a nice homework problem where you use some trig identities here and work out that, you know, this, if you substitute it over here, exactly fits you back to this. For this one, you can do that and just work it out with some basic calculus and trigonometry. So, basic calculus and trigonometry. But I want to answer this general question. If you're given an ODE, step size H, and some time discretization, I'll call it psi now. So psi takes you from Yn to Yn plus 1. And then you train this map G. So I'm using theta star for the train parameters, the trained values of theta. And so we train this map so that when we apply psi now with G, we get exactly the solution at a later time. Exactly, the solution at a later time. What is the nature of the solution of this differential equation? So you might be interested in this because you're trying to identify the physical system, the nature of the physical system, by determining gene. Or maybe you need to apply this neural network with a different time step as part of some bigger multi-physics problem. Because if you go and apply your neural network with the original time step that it was. With the original time step that it was trained for, it's going to work just fine. Okay, so I want to answer this question, and I claim that in general this can be answered using V series, and the answer that you will get will be a power series in H, and these two will agree up to order h to the p, where p is the order of accuracy of your numerical integrator. That shouldn't come as a surprise. Okay. So, so, of course, explicit. So of course, explicit Euler is the worst case example, which makes it really easy to see in that previous example how these functions differ. You use a fourth or fifth order method, these differences would be much, much smaller. Okay, so we have some experts on these series in the room. I'm pretty sure we have people who know nothing about them in the room. I won't try to give a general introduction, there's not time. I'm just gonna kinda advertise them and show you how you can use them. them and show you how you can use them in this particular case, glossing over a lot of details. So what is a B series? It's a power series in H. Here the sum is over rooted trees, so there's one term in the series for every rooted tree. And each term is a product of some numbers and some derivatives of the product of derivatives of f. So this capital F sub F is what's called an elementary differential. We need this. Yeah. And this is just what you get when you differentiate F and use the chain rule and the product rule. Let's see. This is h raised to the number of nodes in the corresponding tree. This is some function on trees that we won't worry about. And these are the coefficients that determine this u that determines this particular B series. This first term, right, This first term would be one if this series represents a map, and zero if it represents the right-hand side of some different equation or a flow. And you might challenge me on this, but for all intents and purposes, all numerical methods can be expressed as B-series. Okay, if you're really interested in the strict truth of this question, then that's cool too, I think. But I'm not going to go with these questions. Yes, sorry? Close enough. Yes, sorry? It's close enough, is it? Yeah, we're probably close enough for today. Okay. Can we say all B-series methods can be represented in this series? Yes, okay, that's fairer. Okay, all the methods you have used, right, I would be surprised if we can come up with the non-examples. Anyway, I don't want to get thumbed up on that. Okay, simple examples. So the explicit Euler method, this is already written as a B series. There's nothing we need to do. The implicit Euler method, this is not a B series. Implicit Wheeler method, this is not a B-series because here on the right-hand side we have yn plus one. This should all be in terms of yn. So we need to expand that thing, right? And it turns out to be, you know, the first few terms look like this. Here, so this elementary differential is just f, our little f. This one is the derivative of little f, the first derivative. These two are the two terms you get after you differentiate f twice, both place, and so on. Okay. And another Uh okay. And another one that will be really important is the exact solution map. And I'm going to write that, the coefficients of that one as e. Of course, we can work out what those coefficients are, but we don't do that right now. Here's another, maybe a little more interesting example. So all two-stage second-order Runge-Kutta methods can be written in terms of one parameter and their coefficients. And their coefficients look like this, and you can write out the B series that corresponds to that map. It turns out a lot of the terms actually vanish. You only get the Bush trees here. Okay, so what can you do with B-series? Why do you care about these things? You can, I mean, I think 90% of what people do with them is order conditions for numerical methods, but that's maybe the easiest or simplest thing you can do with them. Or simplest thing you can do with them. You can study what happens when you compose two methods by composing the V-series. You can also do backward error analysis, which is the question of what problem am I actually solving exactly when I apply my numerical method. Or you can do kind of a conjugate of that, which is what we'll be interested in here. This idea has the term modifying integrators attached to it, which I find this term really confusing, but I'm going to explain what's actually going on. Explain what's actually going on there. Okay. You can do other things. All right. So, what do we mean by modifying integrators? Just this. So, we have our time-stepping method psi. We can write it as a V series. This is a map. And the coefficients of that V series, I'm going to denote them by A, A of tau for each tree tau. And then I have this flow that corresponds to my, in our case, trained neural network. In our case, trained neural network. I'm going to denote its coefficients by B. And what we know, because we trained the neural network to do this, is that if we apply the map given by our numerical method with the right-hand side given by our neural ODE right-hand side, then we'll get the exact solution map. This is what we've forced to be true. But this is a relationship between A, B, and E. These coefficients. And E, these coefficients of root of trees. And we know A and E, or we can work out A, we know E, and we want to figure out B. And it turns out you can invert this relationship and solve sequentially starting with the lowest order trees and working your way up to find these coefficients B. I'll give you a tiny flavor of what that's like. So there's a beautiful paper. This is my favorite one. There's a beautiful paper. This is my favorite one by Chartier, Heyer, and Blumart, where they explained one way to do this. It's not the only way, actually. You can find other approaches. So this substitution operator is known by this star. And we can find, if we have, we actually want to invert this thing, but if we have one B series substituted into another like this, we can find the coefficients of the resulting B series in this way, where what do these things mean? What do these things mean? Okay, so this is an example for one tree, right? Tau here, we're going to work it out for every tree if we want to do this, or all trees up to some order. What you do, you take all the subsets of edges of that tree. So here there's 16 possibilities because there's four edges in this tree. This associated forest is what you get if you remove one of the subset that you picked. And the skeleton is what you get if you collapse all the edges that you didn't pick. If you collapse all the edges that you didn't pick and keep the rest, right? Okay. And then you compute here, so this is the associated forest and the skeleton, and you compute the sum of products, and it gives you some number, right? And because of the structure of these equations, they turn out to have a triangular structure in terms of the trees, so you can solve them sequentially. And you don't have, they're not all, you know, infinitely many of them coupled together, fortunately. Okay. Unfortunately. Okay, so this is a pain, right? And if I do this, I will do it wrong. And this is just for one tree, and I need to do this for all the trees, or many trees, right? And there are a combinatorially growing number of them as you increase the order. So when I wanted to actually do this in our real example, I threw up my hands and said, okay, I have to teach the computer how to do it, or I will never get the answer. Yeah. So, right, this is very error-prone. At least, I can't do this. At least I can't do this correctly by hand. I think it's really infeasible for almost anyone, except at very low orders, meaning like two. And even using Mathematica or Maple doesn't immediately make it simple because you have to manipulate all these sub-trees and forests in such complicated ways. So, what I did was to fully automate this. So, I wrote a package in Python. It was very slow, and I got my collaborator. And I got my collaborator Hendrik Manocha and said, Can you make this faster? And he rewrote the thing in Julia, used a bunch of tricks, and it's super fast now. And now you can do this, right? So you just, in one line, you say, here's my right-hand side, here's my dependent variable, my time step, coefficients of my Roman Dakota method, and I want to go up to this order, right? And it gives you, you know, so like your Rungakata coefficients could have parameters in them, symbolic. Could have parameters in symbolic variables. Yep. I think I'll show an example of that actually. Yeah. Okay. Unfortunately, Julia prints these in like random order. I don't know how it fits the order of these terms, but they're all there. And this takes an instant to calculate. This is for explicit Euler and the harmonic oscillator, right? Terms up to order h to the pi. Okay, so we wrote this as a software package, just in case. This is a software package, just in case you actually want to answer this question for a real example. I think this is a nice way to do it. Yeah, like I said, it's in Julia. You can go up to about order 10 before it starts to get slow, and you might want to go get a coffee or something. A little bit less than 10, it's almost instantaneous. Yeah. It's on GitHub, but you can install it with the usual Julia package installation. Package installation. And then it does all of these things that I've been talking about, including these modifying integrators or telling you what is the map of my trained neural OTE. Yeah, so this is how I got this answer. That's what we were just looking at. And just to show you, because this one you can really work out by hand using, like I said, some trigonometry. Here's a more challenging example. So this is a non-linear predator-prey model, Locha Lterra. Fray model of load kiltera. Still pretty simple, but okay, a lot harder than the harmonic oscillator. And here's our generic two-stage second-order Mung-Peter method with a symbolic parameter in it. And here's what you get. Here, I've only gone up to the order h squared terms because, of course, it's much less here now. There's a lot more there. But you can get these answers. And then you can ask, you know, what are the energy dynamics of this thing, right? Because this, of course, has periodic solutions, right? Solutions, right? So if I train with this method, then I actually train it to be facilitative or add energy or whatnot. Okay, that's all I'm going to say about that. If there are more questions, I can take them now about this part. All right. If you use this for anything, I'd be really interested to hear from you how it goes. And also, feel free to tell us if it doesn't work for you or ask for features. Okay. Okay, so part two. This work was done by a really talented undergraduate intern at Kaos last summer, Gabriel Bargros. And yeah. So this is maybe the most basic first thing you do when you learn V-series is you say, okay, I have a run Gakata method. How can I express the map that is that numerical method as a power series in H? And it looks As a power series in H. And it looks like this: where the coefficients now are these phi functions that come from these phi sub i functions, which are in the last column of this table here. And these a's are the butcher coefficients of your method. So these are just some expressions. You've seen these things before if you've ever looked at order conditions for Rumpier-Cutter methods. One way to find them. Okay, so to check the order of a Rumbi-Cutter method or design a high. Order of a Runge-Coder method or design a higher order Runge-Coder method, one way to go about it is to write down its B-series and then write down the exact solution B series and demand that these things match up to some order. For all trees up to whatever order you want for the Lung-Kata method to happen. Okay, so I want to talk now about the energy preservation in Lung-Kata methods, which also is going to be related to some algebraic conditions on the coefficients of the B-series. The coefficients of the B-series. And there's this beautiful paper that Elena and Brynjolf and some other very smart people wrote. I really love this paper, actually, that explains how to do this. They characterize precisely exactly which B series are energy-preserving. And it turns out, so B-series, of course, form a vector space, a linear vector space, and these energy-preserving B series are a linear subspace of that set. Subspace of that set. And that's because the elementary differentials, the conditions for energy preservation, take the form of, you know, these elementary differentials have to appear in certain linear combinations, and only those linear combinations. This is not from their paper. This is our restatement of their necessary and sufficient conditions. But maybe I won't ask you to read it. I'll just kind of try to give you a flavor of it. Yeah, here. Yeah, here. So here's a tree. I've colored the leaves, meaning the nodes that don't have any children, distinctly because there's going to be, okay, so these, well, one thing that I should have pointed out in this theorem is that here you have a linear combination of two elementary differentials corresponding to two different trees, and there's a plus or minus one between them. So you either are allowed Between them. So you either are allowed to have the sum of these two trees or the difference of these two trees. And if your whole B series can be expressed as some linear combination of these pairwise signed sums, then it will be energy-preserving. So what are those pairs? This is not in your paper, but we needed a term for them, so we call them energy-preserving conjugates. So you start with this tree, you pick a leaf, we call the path of all the We call the path of all the nodes from the root to the leaf, to the chosen leaf, the trunk of the tree. So you strip off, you pull all the branches off of the trunk, you flip the trunk upside down, and you reattach the branches. You're nodding, okay, so that's how you think of it too, right? Yeah. Okay, there's a nice little picture of the paper that explains this, but I wanted to give a concrete example. So for this tree, for instance, you know, with the red note, the trunk is just the root. Node, the trunk is just the root, right? And there's only this sub-tree coming off of it. So you get back the original tree. But with the green node, you have the trunk being these three black nodes here. So then you have these three nodes all being sort of peeled off the trunk and being reattached. And you end up with this tree. And what this tells you is that you can have this link. You can have this linear combination, where really I should write big f sub little f of this tree, right? That's the elementary differential that's meant here. But we get that easy and start with writing the trees. And you can have this linear combination. You can't have this tree by itself because actually it's conjugate to itself, but with a minus sign, so it minus itself is zero, and you can't have that. Yeah. So you have many, many. So, you have many, many pairs like this, and you have to check whether your B series can be written as a linear combination of them. Of course, you can write this as a system of linear equations, linear constraints on the coefficients of your B series. So that's on the right-hand side here. You're going to use, and these are conditions on the flow of your Runge-Koda method, not the map. But what we most naturally think about is the map. So there are some conditions relating the coefficients of these. Relating the coefficients of these two B-series. And then there are conditions relating the traditional Butcher coefficients to the map. So if you go back along all these relationships, eventually you get some energy-preserving conditions on the Butcher coefficients. This is just some tedium. And they look similar to the order conditions for running quadrants. They're different conditions. They're related, some of them. Of them, but those are what you have to satisfy to be energy-preserving up to some order of accuracy, right? Not exactly energy-preserving because there are an infinite number of conditions. For instance, to be energy-preserving at each order, you have to exactly satisfy the quadrature condition, the bushy tree condition. So, this is why you can't be energy-preserving to all orders within the realm of traditional Mungakota methods, because they can't satisfy all the quadrature conditions. All the quadrature conditions. Okay, and we say the method is pseudo-energy preserving. This is an analogy to pseudo-symplectic methods of order Q if the local energy error is order H to the Q plus 1. So typically, a Runge-Kata method that is order P would be pseudo-energy preserving of order P also. But you could try to design methods that maybe. Methods, you know, that maybe you know, maybe your method is only second-order accurate, but it preserves energy to sixth-order. And maybe this is a way to get more efficiently accurate solution if the accuracy you care about depends mostly on the energy. So I think in another paper that at least one of you wrote, or maybe both of you, it's pointed out that with two stages and second order, you can get energy preserving order three. Energy preserving order three, and it's there's a unique method that does that. Similarly, with three with two stages and third order, oh, sorry, these are backwards. Three stages and second order, it should say, right, you can get energy preserving order four, and there's a unique method that does this. So what we did was to work out, or what Gabriel did was to work out these algebraic conditions and then run, you know, with MATLAB searches for coefficients that satisfied all these conditions. And I wouldn't. Conditions. And I wouldn't, this is still work in progress, so don't take the last couple of lines of this table too seriously. But this is intended to tell you, with a given number of stages and classical order, what's the highest value of energy-preserving order you can get. And I think we still don't have the energy-preserving order seven conditions coded up, and it's a little doubtful whether MATLAB's going to be able to solve this anyway. So these numbers may be. Numbers may be it all the numbers in this table, all the real answers are at least as big as the numbers here. Let's say that, and I would say the first four rows are believable. I'm confident of them at this point. Yeah, and it's true typically for Hamiltonian systems that if you preserve the energy better, even if your classical order of accuracy is the same, you'll get more accurate answers. So this is. So, this is that Lodka-Volterra system again. The correct solution is basically this green line, this periodic orbit. The red one is what you get, and sorry, the dots are too big, so you just see a blob. But this is second-order, two-stage second-order Rungakuta, which has pseudo-energy-preserving order two. The blue one is a method, all of these methods have classical order two. The blue one has energy-preserving order four, and the green one has energy-producer. Preserving order 4, and the green one has energy preserving order 5. You can see the improved dynamics. And this is the energy and the error over time on a log-log scale. You can see, actually, in this case, the green one, which is energy-preserving order 5, seems to have a larger error coefficient, right? But after a long enough time, it still wins out, right? The blue, which is energy-preserving order 4. Okay. Actually, I should say though, if you use these pseudo-symplectic methods, you get very similar results to this. So it's not clear to me whether using a pseudo-energy preserving method is better than using a pseudo-symplectic method. And I think there are some interesting maybe errors. I have a question. Yeah. What about trying to do both simultaneously, pseudo-symplectic and I have no idea. Is the intersection of those two senses? Maybe it's not possible. This means it's not possible. I think it was possible. Okay, I would actually have come to you with this question. But you have the. Because we have the characterization of both of these. They're both linear subspaces, right, of B-series. But you don't know if the intersection is non-speakable. Another thing that you could think about is this conjugacy that we also discussed in this paper. If it's conjugate to energy preserving, then you get essentially energy preserving. Yeah. You buy a little bit more freedom, right? So you're almost energy preserving, which is anyway what you're getting here. Yeah, yeah, yeah, yeah. So except that uh if you get it uh complicated, then you get it for uh arbitrary long time and so on, so Time, that's all. I don't know exactly what happened to say, I think you drifted in yeah. So, eventually, the dynamics that you usually see here, if you just look at the global error rather than the energy error, right? So, if you have an exactly energy-preserving method for a Hamiltonian system, the error will grow linearly over time, whereas for a generic method, it will grow quadratically, right? With these methods, what you see is that it grows linearly for a long time, a longer and longer time, the higher your A longer and longer time, the higher your pseudo-energy-preserving order is, but then it becomes quadratic. Yeah. So by the way, do you see any dependence on dimension? I mean, for certain dimensions, some trees or elementary derivatives vanish, right? But it's not true anymore in higher dimensions. So, in your conditions that you work on, do you see this? So, we're not. So we're not assuming the dimension is small here, right? So these conditions are the most general ones, right? But can you get any simplification by saying we restrict those substitute dimension tools? Maybe, I don't know. I haven't thought about it. Well, I have a question. So if you apply this kind of methods, right, also in synthetic methods in nitrogen, for a very, very long time until you start to see noticeable drops in Noticeable drops in the number of significant digits. And it seems that some methods lose significant digits much faster than some other methods. Have you done any study of these kind of energy preserving methods? We're doing tests right now. So this is unpublished work, right? It's still work in progress. Gabriel, of course, had to go back and finish his undergraduate thesis, but now he's doing some numerical tests here. Hopefully, it will be up on archive in a month or two. We're looking at the error over time. Because I was referring to the length of time scale at which you probably need to switch to quadruple accuracy in order to make sure that what you compute is accurate. Well, okay, here, because you're not exactly energy preserving at all, right? I mean, I don't think you ever need quadruple precision. No, no, but if you just No, no, but if you just in terms of the accuracy function, finding steps to a fixed time step for fixed finite time for a very long time, in a sense, I don't really care whether the energy is completely observed or right, you're just driving your whole truncation error down. Because at some point they just lose completely lose uh it seems to our you know, based on our investigation, at some point the the speed at which you lose significant digits, like Which you lose significant digitions, like accelerating. Maybe this is related to this shift from linear to quadratic growth of the global error. I don't know. I'm just extremely curious about this. I'd be interested to see examples you have. Okay, I asked my student to compile for their stuff. And I have examples coming from not this project, but other ones that might shed light on that. Okay, so let me go into part three, which was the originally going to be the main part of my talk, but I'm just going to give a quick idea of what it is, right? So this is like a cheap, and I would say a fairly crude, but works better than it has any right to way to enforce either dissipation or energy conservation. Dissipation or energy conservation. If you know your energy functional ahead of time, with explicit methods, right? So, yeah, because you can always do this with implicit methods, but that's explicit. Okay, so with apologies, I'm going to change notation for this part of the talk, just because these were written originally as different talks. So, u is my dependent variable now. The right-hand side is still F. Okay, and we have either some conserved invariant quantity. Some conserved invariant quantity denoted by G, right, like mass, moment, and energy, or some dissipated quantity, also. I'll just use g. We want to enforce this discretely, and we're going to use the Runger-Kutter method. Again, just to reinforce the notation here, I'm using Y for the intermediate stages, maybe confusingly, and U for the actual solution of the time steps. Okay, and A and B, which will be important. And A and B, which will be important. So I'm going to focus on Rungen-Cutter methods, and then I'll talk about the generalization of other methods. A and B are the, as usual, the butcher coefficients. Okay, I won't try to say too much here because there are many people in the room who are experts on things that are on this slide. But suffice it to say, there are many methods for, especially for imposing energy conservation. And if you want to know all about them, I think the best reference is this book. Book. But they're mostly implicit. Or there's also, the exceptions being here, projection and discrete gradients. And I'll say a little bit about projection. So orthogonal projection, the idea is you have this maybe energy conserving manifold you want to stay on. Your numerical method says, I'm going to step to here. And then you just say, okay, I'm going to force you back to here. I think in the book, they have a quote in German that. They have a quote in German that says, You know, if you will not submit, I will resort to violence. Which gives you an idea of how, you know, Ernst Heyer sees this idea. And they show that it can do bad things. The main thing is that it keeps you on this manifold, but often you also have linear invariants that you want to maintain, and this will destroy those, right? Generically. Very often we have mass or angular momentum or something, right? Or something, right, that we need to keep. And by the way, linear invariants are preserved just for free by Ring of Kata methods, linear multi-step methods, et cetera. So we'd like to not lose that property. And losing that property can often just destroy the solution accuracy itself. Okay. Yeah, and let me just emphasize: so, you know, even like some of our favorite Runga Kata methods, like, you know, the classical method of kara, fourth and fourth order, or the, you know, Four-stage fourth order, or the three-stage SHP enducuta on very innocuous-looking problems, they can perform, I'll say, badly, meaning they don't preserve the energy dynamics at all, right? Explicit methods just aren't good at that. So here's a non-linear version of the harmonic oscillator. I'm just dividing by the norm of u, but the solutions are still circles. It's just that now, if you have more energy, you go around faster, slower, or I guess slower, right? Faster, slower, I guess slower, right? So here's the exact solution here is in black, right? And blue is what you get if you use SSP33, and you get something very similar with Kutta's method. Interestingly, it doesn't grow forever. It actually asymptotically grows to some other orbit. But, yeah. Okay, so relaxation methods. This word is way overused in applied math, and I kind of regret. Overused in applied math, and I kind of regret attaching it to these methods now, but it's too late, I guess. So, okay. So, if you integrate some system, some dynamical system with the Ringer-Kutter method, and you just ask what is the change in the Euclidean norm of the solution from one step to the next, you can massage the result into these three terms. And mainly I care about this one. So, this is the So, this is the inner product of an approximation of the solution and the derivative of that same approximation. So, this, of course, is related to the time rate of change of the norm of the solution. So, in fact, you can show that this term is just some quadrature approximation to the integral over one time step of the time derivative. I'm going to call it the energy here, right? And so this will vanish if your system is considered. So this will vanish if your system is conservative. It will vanish exactly, right, or up to rounding errors. And if your system is dissipative, then as long as these weights b are non-negative, this term will have the correct sign. It will also dissipate energy. But these two terms, you can't really control them at all. They include inner products of the right-hand sides of stages of the Render-Kaudel method. So they could have any sign, and they're just going to mess up the energy density. I'm going to mess up the energy dynamics. So these ones are good, we're fine with this, but we'd like to get rid of these. So how do we get rid of those? Well, we do something, you know, on its face, kind of stupid. We just say, all right, my numerical method, this is the last equation of my Bungle-Kutter method, right, that updates the solution, except I added, you know, this arbitrary fudge factor gamma here. I'm going to multiply the length of my step by gamma. I'm not altering. Gamma. I'm not altering at all the intermediate stages. Just the final update of the solution. And if I do that and I now compute the change in the energy, I get this thing, which is still an approximation of the right integral, as long as I think of my new solution as living at this time. And I get these two terms, similar to the ones before, but with this gamma and gamma squared here. But since this one is linear and this one's quadratic in gamma, I can choose gamma so that they add up to zero. Choose gamma so that they add up to zero. Yeah, so I just solve for gamma that makes this vanish. In this quadratic case, I can solve exactly for it. It's this thing. And if I choose that value of gamma, then I get this. And this will be conservative if my system is conservative and decipitive if my system is decipitive. No matter how big my delta t is. Pretty cool for an explicit method. Of course, if Of course, you might be asking, well, you just did something totally different from what your numerical method was supposed to do, so do you still have an accurate approximation of the solution? And the surprising answer maybe is yes. But first, why do we call this relaxation? So it's in analogy to linear algebraic solvers, where you use under over relaxation. The idea is that this new solution is a linear combination of your previous. Combination of your previous step solution and the one that your Runge-Kata method wanted to pick, but you're now, if gamma is bigger than one, you're over-relaxing, and if it's smaller than one, you're under-relaxing. Or you can see it as this combination, not necessarily convex, because gamma might be bigger or smaller than one. If you go back to that nonlinear oscillator example, here's what the energy does with standard Runge-Kata method. Does with standard Rungakata methods. These are second, third, fourth, and fifth order methods. And here's what it does with this relaxation, right? I mean, we're enforcing this, right? So it better happen. This is 10 to the minus 15. So it's just grounding errors. Geometrically, what's going on here, right? So orthogonal projection says do this. Relaxation is actually just doing this, right? It's not so different. We're just saying, like, keep going in the same direction. But the big advantage of that, But the big advantage of that, right, this is, I think, why it's better, right, is that you preserve linear invariance. You keep those linear invariants strong. So if you have mass conservation, it's still there. And so in contrast to orthogonal projection, where there are well-known examples that it damages the accuracy of the solution, in every case where we've tested this approach, it improves the overall accuracy of the solution. Oh, I have an Accuracy of the solution. Oh, I haven't explained why it is actually accurate at all when we multiply by this gamma. Oh, before I say that, this method could have been inspired by this popular psychology book, where we say you must know ahead of time there will be setbacks. When they come, relax. This is the key to getting on track here. Oh, I want to emphasize: this is not a totally original idea. There were very, very close were very, very closely related ideas developed long before. And actually, I found it in this book from 1984 and just added a little bit of, you know, to this idea. Let's say. But there's a group, particularly this group in Spain, that did a lot of developmental slides. Okay. So the method attains the same local order of accuracy as the Runga-Kutta method that you started with. Why is that? It's because it turns out that you can show this gamma. Turns out that you can show this gamma is asymptotically close to 1. It's so close to 1 that even though you've altered the method and you're no longer satisfying the order conditions, you're violating them by an amount that is the same size as your original local truncation error. Just no bigger than that. Okay, I'm not going to show the proof of these for autotype. And you could also ask, you know, does this gamma always exist? Well, in the quadratic case, it does, but we also apply this. We also apply this where the energy functional can be any nonlinear function. And then you can show that there's a condition that is almost always satisfied, unless you're super unlucky, it's fully true. And then there exists a unique value of gamma closed to mod. In fact, if it's convex, then you don't even need this condition. And in practice, you find that you. And in practice, you find that you can use this with, and you can find this gamma value with step sizes that are similar to what you would be able to use with an explicit time-stepping method. Okay, so what was going to be the rest of my talk, but got shoved out in favor of saying something about machine learning. You can do this where your conserved or dissipated functional is. Conserved or dissipated functional is an arbitrary nonlinear function. You can apply this not only with Roman Guetta methods, but with linear multi-step methods, general linear methods, splitting methods. The idea is just that your method tells you, I want to step to here, and I was here, and you just go a little further or a little less far in that same direction until you hit the manifold. Or at least in the conservative setting, you can think about it that way. And you can also do this with multiple. And you can also do this with multiple functionals. So we have tested it like conserving both mass and momentum for nonlinear Schrodinger. Now for nonlinear Schrodinger, mass means like quadratic functional, right? So these are both nonlinear. And we're looking at, you know, for kinetic models, right? Conserving mass, moment, and then energy. A huge caveat, of course, is that you're conserving this in a global sense, right? There's no local conservation property. There's no local conservation property here, and this will sort of move energy around between different modes, absolutely. So, that's something to be aware of. It's not like a symplectic method that really has a nice local property. But like I said, everywhere that we've applied this, it seems to improve the accuracy, and that's the same thing I've been told by other people I've been using and stuff. Nice approach. I'll stop there. I wonder, regarding this very last part, so I wonder how different approach from the UC, say, Marauji multiplier. You apply the standard and then you impose the energy conservation. That also results in some kind of correction of final solution. Do you mean orthogonal projection or something different? Because sometimes people refer to that as, or maybe you know the answer? Oh, okay. I guess I'm not sure about the approach that you're referring to. Again, I apply the standard on the quota, right? Yeah, the S stages, and then for the final stage, I know if I apply it the standard one, it won't conserve the energy. And then I maintain the I maintain the energy conservation in zero. And that will result in a correction in fact. Yeah, I think this is exactly the orthogonal projection method. But maybe I'm wrong. Well, because we have a similar approach. I mean, none of our fighting is does in the end achieve a similar effect. Sounds sounds interesting. Sounds sounds interesting. Yeah, uh very interesting talk uh for us, especially to this to this. Um uh I I just want to make a remark because you were saying in the beginning that all the methods are RP series methods. Yeah, okay, great. I know that this was not an important point, uh but but I think that actually the last methods that we could propose, they are not impactful. In fact, if you fail that that because the reason is that that uh if you have a P series method, a fixed P series method cannot preserve all energy functions, right? A fixed P series method that you actually have a method that does that, right? Yes, that's a really good point. It actually brings me to something else interesting that I've been doing with Andreas Meister, I'm a student of his, which is, you know, what if you Which is, you know, what if you just allow your B-series coefficients to depend on the solution somehow? Then you can do a lot of crazy things. Okay, but yes, we can agree that this is outside the normal definition of B-series. So these are, I guess, that type of method, right? Where you could write them in that way, but the coefficients depend on the same. Something a little bit more serious, maybe, is that if you don't have the B-series properly, you also usually lose the fine invariance, which A fine invariance, which is very important in the cupboard methods or in B series methods in general, I think. So the fact that if you make a change of variables, a linear change of variable, it is not respected by your method in a sense, right? Which is crucial for syntacticity. This is a very clever method, anyway, I think. This is a very clever comment, too. So can you actually interpret this method as an adaptive step size selection, right? Because you're multiplying your delta T there with gamma. I mean, I know the stages equations don't do that, right? Yeah, so don't do it. That's the critical thing to keep in mind, right? Is that we leave this alone and delta t appears here, right? Yeah, we don't multiply this by gamma. Yeah. You just multiply this by gamma. Yeah. Yeah, yeah, so I see that you are not doing it for a status, right? Yeah, but um I mean in a sense this means you're taking different factors there. So if you interpret it as an adaptive method, right? So and you are multiplying your delta t by gamma, it means that you're correcting each time in your running measurement the alphas by factor of this gamma, right? Yeah, so you can think of this as a new rug-cutta method where the B's all got rescaled. Yeah. And like I said, now you don't satisfy, the B's don't add up to one, they add up to gamma now. So you're violating the order conditions, but the residual of the order conditions is of the same size as your truncation size. So we wanted the step size to be multiplied by gamma, right? And then of course. Right? And then, of course, your formulas for computing the stages isn't true anymore, but it becomes true if you modify the alphas. If you correct for the step size, right? Yeah, oh, you want to go back and just absorb that gamma into the coefficients. So, this is like an adaptive step size method where you are correcting the stage projection, the stage computation in each step. Yeah, but I think. Yeah, but I think so. So, one thing is, I think you will, I'm not sure if you will lose accuracy. We do interpret the new solution as being at this time. And if you don't make this adjustment, then you do lose one order of accuracy, right? If you still suppose that you are approximating the solution at tn plus delta t. Uh probably non nonlinear invariance of a loneliness. Yeah, okay. So I said you called me out, right? Because I claim these are explicit methods, and they're not, right? Okay, for the quadratic energy setting, they're explicit. For a general nonlinear functional, right, there you have to solve a scalar algebraic equation at each step. Equation at each step defined gamma. So this is almost explicit because that's a lot cheaper than solving even a linear algebraic system of the size of your system that you need, essentially. But yeah, they're technically kind of all nose texts. I was just wondering how you can solve the gamma exist in the internal gamma. One thing that we should maybe, or maybe you've already thought about this, because I know you already knew something about these is whether the money. The method that you and Jeff have developed. Could this be useful there? R reverse coding is the crucial. Yeah, and I have no idea whether this has that. So thank you to