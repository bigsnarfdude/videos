Hi, Grace. Hi, Debbie. I think it's one o'clock. It's 101. Why don't you go ahead and start? Okay, yeah, thank you. So I hope everyone had a good lunch break. Yeah, so first I'd like to thank Debbie and the organizing committee. David and the organizing committee for making this wonderful gathering so that we can learn many new contemporary topics. And also thank David for inviting me to speak here. So this is a joint work with my former PhD student who recently graduated. Okay. Okay, so I'm going to start with the basic learning framework and the objective here. And I'm interested in this particular setting, in which case I have an outcome variable, which basically represents a survival information. So in this sense, this variable is non-negative. And I also have associated p-dimensional random vectors called covariates. So we are thinking this X change will influence the value change in T. And the objective is try to find a certain suitable function of X so that this function could be used to predict the value of outcome T. And to do this, could the covariance depend upon time as well? Could they vary? Could they vary? At this point, I'm thinking of baseline covariates. Yeah, definitely, this could be generalized to time-dependent covariates. So, but in certain sense, we could also think of x could be any covariance at a given time point. So, we could also think of this way. And to find a suitable function, so I'm restricting myself to a class of functions, which are typically. function which are typically continuous and bounded in infinity norm. I'm using a script f tilde to represent this class. And to indicate how well the prediction could be, so we introduce these so-called loss functions, which is denoted by L. And this L function is usually we are interested taking this as a convex function in the second argument, which is related to is related to the value f of x because our covariates x and our outcome t could be both random so to incorporate this uncertainty and randomness we want to assess the loss functions from the probability viewpoint by taking expectations and our objective is trying to minimize these risk functions over the risk functions over the class of our consideration script f tilde here. So we want to find this little f0 function so that it could be well predict our outcome. So this is a basic setup we are interested in here. To do this, our objective is trying to find a minimizer F0. But there is one difficulty here because we don't really know the joint distribution of the joint distribution of outcome t and covariance. So in this case, directly minimizing risk function related to expectation of loss function is not suitable. And a common strategy to get around this difficulty is we would try to use the sample information and use an empirical average to approximate the expectation. So a practical way is we would try to find f half Would try to find f hat here. And this f hat is a minimizer for the sample average of the loss function evaluated over the entire sample. And to use this strategy, we need a key requirement. So basically, that means we assume we have a random sample. And also for this random sample, we assume both measurement of outcome. A measurement of outcome t and covariates are available. So basically, we need complete observations for a random sample. However, in certain applications, for example, a big area is related to survival data analysis. In that case, we know censoring is a typical feature. So we can't really guarantee all the outcome would be observed. So in this case, how would we handle this kind of learning problems? kind of learning problems. So the second part is I'm going to particular focus on learning survival data. In this case, keep in mind, one of the feature is we have sensoring outcome. And to feature this, I'm going to introduce a typical key notation. CI represents a censoring time for our survival time TI. And since not everyone will be observed, some could be censored before the end of Censored before the end of this study. So we have an indicator function called sensory indicator denoted by capital delta i. The actual observed outcome will be yi, which is a minimum value between hypothetical survival time, all this sensoring time. And the actual sample data will involve this yi, xi, and delta i. So I'm using this. So, I'm using this notation C D to subscript to indicate this is a censored data. And so, given this setup, then we cannot really directly use a traditional framework to nerve like a prediction function F0 because our risk function won't be able to express in terms of observed censored data. So, in this case, this motivated us to come up with a new loss function or a class of new loss functions in order to be expressed in terms of directly observed sensor data. So, this is our goal here. And how can we find this kind of modified new loss functions? So, I'm going to, in the following discussion, I'm going to use notation L star to indicate. To indicate this is a new function, but this function won't be expressed in terms of original survival time t. Instead, it will be expressed in terms of y, which is a minimum value between survival time and sensor time. And what kind of new loss functions will do a good job? Definitely, we want this function to be connected with our original loss function L of loss function L of T and F. In such a sense, if I'm going to take conditional expectation with respect to conditional distribution of sensoring time given T and X, then my newly proposed loss function will recover my original loss functions. And the main difference between L star and L would be expressed in terms of available data. And so the following discussion will look at what would be the typical way we can come up with reasonably adjusted loss functions. And one of the adjust functions we propose is motivated by traditional Barclay-James formulations. So this is a very well-known approach, which is usually used in survival data context. In survival data context. So, the idea here is when you use when you really conduct traditional survival data analysis, we know our outcome depends on sensoring indicator information. If it's observed, then we have actual survival time T. So, in that case, this YI will be TI. But for those censored subjects, we won't be able to observe TI. So, instead, we cannot really use. We cannot really use this value. Then we just use conditional expectation of ti. Then we use delta i to indicate for that particular subject, this newly constructed observation called pseudo observation will be actually true observation time or related to conditional expectation. So, in that case, we use yi star to indicate this pseudo observation. And the motivation of this introduction. this introduction of pseudo observation is trying to recover our original survival time information in such a sense their conditional expectation will be identical so this was the basic motivation of james buckler james formulations so that is applied to survival time or sensoring time now we just mimic the same ideas but we apply to loss functions as if And if we can observe this arrival time, then this original noise function L would be computable. So that's why here we attach this indicator delta I. What if for those subjects who are censored? So for those subjects, we introduce this new part, psi yi xi. And this psi function is actually the conditional expectation of original loss function. Loss function given survival time. So, we want to indicate this conditional expectation is not purely for capital T. Instead, we are looking at those subjects who are sensor. So, that's right here. Our conditioning variable is Ti greater than our evaluation time little y here. And using this formulation, we can show that the expectation. The expectation of the proposed Berkeley-James loss function, its expectation will recover the expectation of original loss function, which means the expectation of this proposed loss function will recover our original risk functions. So this gives us one strategy of developing a modified loss function. And here there's a quick comment. And here there's a quick comment. This modified noise functions require the involvement or knowledge of this conditional survival function of original survival time t. That's because of the introduction of this psi function, which involves a conditional expectation given t and x. So this is a payoff that when you come up with more. When you come up with modifying loss function, which is expressed in terms of observed data yi, so we need this additional information of conditional survival function of t now if we change our viewpoint to a different angle, that means I don't want to sometimes we don't even know what the conditional survival function of t, right? So do I am I able to do So, do I am I able to do something else? So, the answer is yes. Then you might naturally think, I'm going to just look at those subjects who are not censored, which means I'm going to use my original loss function and attach delta i to indicate I only evaluate those subjects which are not censored. But unfortunately, if you only use the delta i times original loss function, this new loss function won't. New loss function won't be very meaningful because it will give us some biased result. And to make it unbiased, then we just attach this inverse probability weight by incorporating sensory information. So this idea has been broadly used in other contexts, such as missing data problems. So this is so-called inverse sensory probability weighted loss functions. And we can quickly show this. Show this modified noise functions, we'll recover our original risk function again. And of course, using this strategy basically involves the understanding or knowledge of sensoring process. So here this g function G C of given X represents a conditional survival function of sensoring time. So you can see if we use the different strategies to look at the same problems, trying Look at the same problems, trying to come up with a loss function which would be expressed in terms of observed data yi and covert, then we may require different knowledge, either conditional survival function of survival time or conditional survival function of sensoring time. So of course, sometimes you may wonder which approach would be better. This depends on the context. Depends on the context, which kind of function you would be more likely to be modeled or knowing well. So, these two strategies gives us two different angles to look at the same problems. Now, another natural thought is, am I be able to combine both knowledge or information? So, the answer is yes. So, then the third strategy is we can further augment the previous strategy. Augment the previous two ideas by combining them. So, this so-called augmented inverse sensory probability weighted function, which means we're going to start with this inverse sensor and probability weighted loss function. And then I'm going to add a new function denoted by gamma. And this function is actually, again, expressed in terms of available data yi and also including sensory information indicator delta i and covariance. Delta i and covariance. And this gamma function, you can see that involves the psi function. And this psi function was basically required in my first Barclays-James formulations. So that means this new function involves both knowledge of censoring process and the original survival process. So in this sense, it looks like this new function is more restrictive. Is more restrictive because it requires the involvement of both components, g function and capital F function. But it turned out adding these two pieces in this formulation does not really make us more restrictive. Instead, it gives us more flexibility. This is so-called double robust properties, which has been well known in other contexts. So the message. So the message here basically says, I don't really have to know both f function, ft function, and the g function simultaneously. If I'm able to know one of them, if I'm able to correctly specify one of them, then this formulation could guarantee the condition, the expectation of this modified loss function will recover my original true risk functions. So this is a functions. So this is a game by introducing both G function for sensoring process and this FT function for the conditional survival process. So here we just quickly discuss these three possible strategies to modify our NOS functions, right? So once we are able to work out a modified NOS function, so I'm using L star to denote these NOS functions. To denote these loss functions. So its function form could be dramatically different from my original loss function L. But the game here is I can make it workable or computable in that sense. I'm going to plug in my observed data YI, which is a minimum value of survival time and sensoring time. So now my initial objective of finding the minimizer of this function becomes minimum. This function becomes minimizing this sample version, which is based on the construction of modified loss functions. And now I also have some additional comments here. It turned out this formulation gives us a natural way to think of minimizer problems. But as we noticed in the previous discussion, this L star function actually requires the conditional survival functions FTE. survival functions, F of survival time, or this G function of sensoring process. So sometimes we need both, sometimes we need one of them. But no matter which one you need, in general setup, these functions are unknown. Then again means this modified L star functions is not really available yet. Then what do we do? So to get around What do we do? So, to get around these problems, we come up with an approximation version of this L-star, which means since I know because the involvement of F and G function, then I'm going to try to use our data to come up with some reasonable estimation for f function and g function. So, if I'm able to come up with the consistent estimators, then I just need to. Estimators, then I just need to use plug-in ideas to replace those unknown conditional survival functions by their consistent estimators. So, if I'm able to do that, then I'm going to use L star hat to denote the approximate version of the adjusted noise function L star here. So that means our final goal after becomes, I'm going to first incorporate censoring information. Sensoring information by using the observed sensor data to work out adjusted noise functions. And because adjusted noise functions involves the additional requirement of a survivor function for missing, I mean, I mean, for censoring process or survival process, then we're going to use L star hat, this new function to be my objective function because this function is. Because this function is computable. So, our ultimate goal is trying to minimize this sample average version for the workable objective function L star hat. So I'm going to use f hat to denote this final estimator of function f. So this would be our ultimate goal, which is expressed in terms of Express in terms of available data. Then the remaining part is: how would we conduct this minimization problem? So now we are facing a situation of trying to look at optimization problems, right? Then the next concern is how would we do that or implement that from computational viewpoint? So this part, I'm going to discuss imputational perception. Going to discuss imputational procedure for that optimization problems. And so, in the literature, we have some available approaches, like using boosting approaches, such as use the steepest gradient approach. And this method basically is treated as a boosting approach in a sense. We are going to sort of start. We are going to sort of start with some weak learner, and then we are going to try to update those weak learners iteratively multiple times. And then we hope in the final stage, we can learn information from weak learners and eventually we can come up with a good learner or a strong learner. So the idea is trying to introduce iteration. Trying to introduce iterative update procedures, and the previous procedures could have a weak learning information, and the next stage would have a stronger learner information. And in this, this basically this iterative idea is not really brand new, like in our ordinary Newton-Raphson approaches, we also use these iterative ideas. But in here, the only difference is basically we're going to focus on our law. We are going to focus on our loss functions, and since our loss functions are bivariate functions, which involves two arguments, so we are going to focus on, remember our second argument of loss functions involves our objective function f. So, when we try to use loss functions, we may want to look at its gradient with respect to the second argument. So, that is basically involves Basically, it involves a second argument is related to a function. So, when you look at its gradient or derivative, then the gradient for a function is not done in our traditional way. So, a practical way is suggested by many authors. For example, one approach is we are going to try to sort of discretize or parameterize our Parameterize our function f in the second argument of my loss functions. And how to parameterize these functions? And definitely, we want to combine our sample information. So in this case, I know my sample has n observations for n subjects, and each subject has one set of coverts x. So in this case, we would be able to focus on evaluating f function and each observations of the coverts. Observations of the coverts for each subject. So we end up with n different so-called parameters or pseudo-parameters here. And then we would be able to use this kind of like similar to Newton-Lapsen ideas. Then we're going to update the estimate of f functions from my m iteration denoted by f superscript m. So this is my previous estimate for f functions. I want to ask. For f functions, I want to update it to my next estimate fm plus one. And how can I update it? I'm going to end this negative gradient. So this is so gradient decent here. And so that's why in here, my red term involves a partial derivative of my objective function, my mass function here. So, and also when you do this, you need to. So usually in Newton-Laphson. So, usually in Newton-Raphson approach, sometimes people just take one as stepwise, but sometimes you want to control your update by making the increment bigger or smaller. So we also introduce this step size alpha hat here. So this delta L actually represents a partial derivative with respect to the second argument of loss functions. And I only need to evaluate with respect to those. Evaluate with respect to those values relative to my observation in my sample. And the step size alpha is determined by trying to minimize the objective function again. But the only difference here is I'm going to evaluate my loss functions relative to my previous iteration information. So that's why this L function here is computer. This L function here is computable. So, this is the basic idea of steepest decent algorithms. And then you might wonder, then how would we actually implement? So, in practical settings, we are going to specify loss functions. And there are a couple of useful loss functions that have been used widely in the literature, such as L2 long or L1 long or Huber loss function. Or Huber loss functions. So you may notice for L2 norm, it's widely used because it's convenient. For example, its derivative is directly computable. What about L1 and Huber functions? Then you may notice L1 and Huber functions are not necessarily differentiable everywhere. So this brings up another question. What if I'm using, I'm specifying? Using, I'm specifying different loss functions, L, and this L turned out is not differentiable. Then this decent approach does not seem to work, right? So what do we do? Then it turned out people came up with this modified version for non-differentiable loss functions. We can modify this delta L with the so-called sub-differential. The sub-differential here. So, what do we mean by sub-differential? So, this is basically defined in this context. Looking at the convex function psi, a vector is called a subgradient if it satisfies these inequalities. And basically, for these inequalities, it is satisfied by any gradient functions. And when you look at sub-gradient for a function at a certain point, Subgradient for a function at a certain point, then maybe sometimes subgradient is not really unique. So, in that case, we may collect them together and call this set as a sub-differential. So, we use a similar notation delta per psi to denote this function. So, this is a particular way when we encounter the non-differentiable loss functions. And another possible difficulties when we implement this boosting or Boosting or like a gradient decent approach is if we directly use the gradient decent update, for example, if we look in our context by replacing NOS function with the L star hat, then in that case, when we really work with this partial derivative with respect to the second argument, then we might end up with unbounded functions. In that case, when you really update it, even if When you really updated, even if you try to make your stepwise small, then we may still end up with a situation that the final estimate may be unconstrained at all. So we may end up with unbounded functions, which might introduce so-called like overfitting issues. So to overcome this difficulty, usually people can come up with this strategy. We can try to impose Strategy. We can try to impose additional constraints to make sure each updated estimate of f functions could be contained in a certain class. So this is basic ideas to introduce constraint. And then what kind of constraint are we going to introduce? So here I'm going to just use this script C to indicate a certain class of constraints of our interest. So usually we like to work with smooth. We like to work with smooth functions. And since we also want our final estimates to be sort of bounded, so that's why we look at a script C to be a class or a subclass, including continuous and bounded functions. So once I introduce this, then I can modify my previous gradient descent algorithms by replacing the sub-differential or differential delta L. or differential delta L by this H function, this red piece. Then what is this H function here? So it turned out determination of H function could be simultaneously conducted with my step size. So in that case, I'm going to jointly find this step size and increment part H functions by minimizing my objective functions evaluated and the previous iteration. And the previous iteration. So the key here is this Fm is available and my M iterations. And then you can see I can restrict my increment component H to be taken from the class of function of our interest. So in this sense, then we can guarantee no matter how many iterations you are going to do, then the final estimated. Estimated functions will be always founded. Now, then the remaining question is how to specify this C class. And definitely, depending on context and depending on user's interest, there are many different ways to do that. So here I'm just show you a quick idea. Sometimes people could use, or we can consider use these additive structures to specify any function H in this C class. function h in this c class by adding up the whatever function for each component then each h1 h2 hp functions could be specified by by like a regression spline or even like sometimes we can consider a pubic spline to specify all those functions and then another question is when you do these iterations when would we stop right you cannot really do them You cannot really do that like infinity times. So, in this case, we can introduce a stopping rule. And usually, we could evaluate our mass functions evaluated and two successive estimates of f functions. Then you can use multiple ways such as squared error or absolute error as a stopping rule by comparing with a certain tolerance level or threshold values. And also, as I mentioned, when we use I mentioned when we use our modified functions, then we need to come up with an estimation of conditional survival functions for survival time and sensoring process. And this step could be conducted by using available approaches in the literatures. For example, we could use this relatively robust approach, random survival forest, to work on conditional survival function of T. Survival function of t. And all you can use a kernel conditional coupling Meyer estimation to do that. And of course, like maybe a simple way is we could also introduce some modeling for survival process or even sensoring process by using like a Cox model or AFT model, those semi-parametric or parametric approaches. So there are multiple options to do that. Now I'm going to finally show you why this approach makes sense. So we need theoretical. So we need theoretical foundations to justify those procedures make sense. So the first theorem tells us if I'm going, of course, under the glaring conditions, starting with initial values, then I'm using f superscript m plus one to indicate the update and iteration m plus one. It turned out if I'm going to increase my iteration times to infinity times, then To infinity times, then the resulting loss functions, I mean, the resulting risk functions will converge to these minimum values. And this basically tells us our proposed algorithm works. It will converge if I increase my iteration number infinity times. So this is discussed under the assumption that my L star function is available. star function is available. But as I said, since L star function involves conditional survival function for T and sensory time, we actually work with L star hat, not L star. So the next theorem tells us, what if I'm going to work with L hat start, what would be the result? And definitely we are not really interested in repeat the algorithm infinity times. We would be interested in evaluating. We would be interested in evaluating the final estimator F hat is good or not. So, this theorem tells us if I have a large enough sample, sample size approach infinity, then this working risk functions will actually converge. So this basically tells us the limiting behavior of our workable risk function function R head. And it also tells us, implies the consistency. us implies the consistency of the proposed estimator F hat. So this theorem is discussed under this setup. I have infinitely many sample size. But in reality, we always have finite sample, right? So we also are interested in working with characterizing the difference between finite estimator F hat and its target value F0. So the next theorem basically tells us. So the next theorem basically tells us this. If I have a finite sample information, then we can expect the difference between estimator and the target value F0 cannot be arbitrarily small. It must be nowhere bounded by this quantity, which involves the sample size n. And so that means the differences cannot be arbitrary small. If you have a very small sample, then you may expect to increase. Then you may expect to incur large finite sample bias. Now, I'm going to, so I think I run up the time a little bit. I'm going to quickly ramp up this discussion by show you some numerical flavors by looking at these implications. This is a breast cancer data, which involves 295 women. And those women were followed up during period of 1984 to 1995. 1984 to 1995, and when they were diagnosed as a breast cancer, so they were 52 or younger. And of those, among those study periods, it turned out only 79 women died. So this gives us a large censoring rate, 73%. And also, from this Vidiverse paper, they found about 70 genes were Genes were regarded as associated or useful for tumor diagnosis. So, in this case, we are interested in understanding how to use this gene information X to do prediction for survival time T. And to evaluate the performance of proposed estimator F hat, we are interested in looking at this unconditional survival probability. Survival probability FT here. So, how this might perform for this particular data set. And also, we are interested in the following questions. If I was given particular loss functions, then how differently these three proposed adjusted loss functions may perform? And on the other hand, if I give you an adjusted approach, then which loss function would be preferred to choose? Would be preferred to choose. So I want to look at this difference as well. And another interesting question is: in traditional survival analysis, Cox model is the most popular models. So I also want to see if I'm going to apply Cox model to this setup, what might handle? And so to evaluate the finite sample performance, here I'm going to divide my observed data into two parts, training data and validation data. And then I'm going to And then I'm going to use training data to estimate this margin overall survival function FTHANT, which involves my previous proposed function F hat. So we use an empirical version to estimate it. And for censoring information, we just use a traditional Kaplan-Meyer estimate. And then we are going to use validation data to evaluate the difference between my The difference between my unconditional survival functions with the empirical version or indicator function here. So then I'm going to divide my patients in validation data in according to their survivor or sensoring information. So then compare either 0, 1 with this estimate of unconditional survival functions. Then we also incorporate the dependence. Incorporate the dependence on censoring ship to adjust the censoring effect. So, this is so-called the briar score functions. So, we evaluate these functions for validation data. And then that is defined for each time point. Now, I'm going to combine all the time points by integrate this together. So, we look at this integrated Briar score. Then we're going to evaluate this measure to see how. measure to see how the performance of the proposed method. And to alleviate the deviation effects of dividing arrangement data into training and validation data, so we also use a cross-validation procedure here. And also instead of just looking at a single original data set, we want to repeat this process multiple times. So we also introduce this boost tramp process. This boost tramp procedures by repeatedly generating new data and evaluate this integrated briar score function here. So this graph shows the performance of using different approaches to report the box plots of integrated briar scores. So the first three box plots came from the L1 loss functions by using three different functions by using three different adjustment approaches. The middle three is based on using L2 loss functions and then next three is based on Huber function and then last column represents the application of Cox model. So you can see if I was given a noise function, then these three adjusted approaches turned out the augmented inverse sensoring probability weighted strategy. probability weighted strategy tend to approach to perform the best. And for any given adjusted approach, it turned out Huber loss functions outperformed the other loss functions. So in general context, you can see all these proposed approaches gives us a better performance than Cox model here. So I'm going to ramp this up. So in this case, we look Case, we look at survival data, which typically involves sensoring responses, and we propose a boosting estimation approaches by modifying traditional gradient descent strategies iteratively to update the target functions. And we also come up with three adjustment strategies to modify loss functions in order to incorporate censoring effects. And so here I just want to emphasize our goal here is different from traditional survival analysis. Different from traditional survival analysis, which focuses on inference based on model building. So, in here, our target here is trying to do predictions. So, we want to find the optimum functions of covariance in order to predict survival times. So, when we evaluate the performance, we focus on reducing prediction errors. And we also develop theoretical results to justify the validity of this proposed method. So, thank you. The proposed measure. So, thank you very much for your attention. I'll stop here. That was lovely, Grace, and you covered so much ground. I'm really impressed. Thank you. Yeah, that was terrific. I wanted to ask if you have seen spatial sensoring as opposed to time sensoring. We haven't really looked into that particularly, but I think definitely that's that's an interesting yeah, that definitely would be an interesting setting, a problem to consider. So I think I would assume, I mean, I haven't really do deep thinking yet, but I think my quick intuition is it's durable by falling. By following the same principles, but I guess the difficulty will be, so you would have like an also spatial feature involved. So I guess maybe when you introduce the censoring ship, that might impose additional involvement of the adjustment function. But I guess the same principle could be applied in a sense. Could be applied in a sense. You start with the ideal situations, assuming my survival time T is available. I assume my survival time T is available, my covariates are available, then I just use that as a starting point and then gradually build up by modifying those missing informations. Yeah, I think that's an interesting. yeah i think interesting yeah interesting uh context to yeah to to further explore something along this line came up twice yesterday uh mamadou yoke was describing uh how uh advertisers can track a cell phone through a store if you activate a special app and so you can observe the person while they're within the store and where they are and what they're doing and then when they leave They are and what they're doing, and then when they leave the store, they become censored and invisible. The other place where it came up was in my discussion of navigating the Fox News subnetwork. As long as you are part of the Fox News website, an advertiser can track: are you in the entertainment section? Are you in the politics section? Are you in the world news section? But once you leave Fox News, you become invisible. But once you leave Fox News, you become invisible again. So that would have both spatial and temporal censoring mechanisms that could be relevant to a computational advertiser. Right. Yeah, I think, yeah, definitely, I think one of the further directions I'm thinking to explore is maybe we can also incorporate this situation that some covariates X are not available, so they could be missing. So they could be missing. So in that case, then we won't be able to really work with p-dimensional covariates, which is our starting point. Then maybe I may have some missing covariates there. So our missing coverts could be like random missing or like a missing completed at random or missing not at random. So in that case, At random, so in that case, uh, incorporating different missing mechanisms could be also interesting, yeah. So, I think, yeah, definitely, I feel like there could be some interesting implications for this kind of approach. In terms of the covariates, ones that would matter to advertisers would be things like age, gender, and income bracket. And some of those would be. And some of those would be missing, but some of them would be known imprecisely. Google can make guesses about my age, gender, and income bracket. And for some people, they'll be very confident. But for other people, they say, well, it looks like he's maybe 70% likely to be over 60. And so that's not missing data, but it's a different type of uncertainty. Of uncertainty. Yeah, yeah. Yeah, thank you very much, David, for like commenting on this. Yeah, so actually, yeah, I mean, like another context I'm thinking is we can incorporate measurement error, which is one of the errors I've been interested in for quite a long time. So I think like incorporating measurement error issues in this development could also give the development more realistic to handle. More realistic to handle some real data. I mean, for example, like for a talk today and yesterday. So, when you really have those like online data or whatever, like a written data, so sometimes you won't be able to precisely indicate, for example, if I like a movie, I may like both like a horror movie or drama movie or surreal movie, but sometimes I'm not really able to clearly indicate. Clearly indicate my own preference and a real level which reflects my true feeling. Because sometimes I may not be even able to do that. So I think in that case, we could think of ourselves in an ideal setup, but our actual available data may not be identical with the true ideal measurement, then there would be a discrete. Measurement, then there would be a discrepancy. Then incorporating those discrepancies could be put in the framework of measurement error, I think. Yeah, definitely. That's a very interesting comment. Yeah, I think Nancy put up her hand. Yes, Nancy. Yes, so I put a question in the chat, but I maybe put it in too soon. It was at the beginning of your talk, and I wondered if by minimizing the empirical risk you had, we're at risk. empirical risk you had were at risk of overfitting but then when you actually implemented it you had a training set and and you tested the you evaluated the risk i guess on the validation set instead so that would that would help with that but but that led me to another question and my other question it's uh even goes back to the buckley james approach you have to divide by an estimate of g and if you don't have much sense And if you don't have much sensor data, then you can't estimate G very well. I'll just wait till you get to the slide where it's quite obvious. Right there, for example. So you need to divide by G and G. G might not be estimated very well if you don't have very much sensoring. But if you have a lot of sensoring, then you could estimate G well, but then you won't be able to estimate F very well. So I feel like there might be some, although you sort of be some although you're you showed your results were converged to the right value i wondered if there's a problem with the variability or the robustness of these i know they're doubly robust for consistency i guess but i'm just wondering about the the problem of really getting you know very small weights for g for example simply because you don't have very much information yeah okay yeah so thank you very much nancy for this uh these two questions so for the first questions in So for the first question in terms of overfitting, I think yeah, usually that is a concern when people use like a iterative approach. So if you keep doing that, then what if we might like overfit data? So that is, I think that is quite like a common concern in this context. So that's why when people use this stopping rule, they need to sort of carefully They need to like a sort of carefully set up a threshold. So, you don't really make your threshold to be extremely small to get some overfitting results. So, so that's why choosing, I think. So, basically, this is related to one of the comments here. So, what about the scoping criteria? So, in this case, I guess this might involve a little bit art in a sense. In a sense, when you compare your squared error or absolute error with a certain tolerance levels, in our numerical studies, we use 10 to negative 4, but I guess maybe some people might use it 10 to negative 6. I know like usually when we do inference, trying to estimate parameter values for a certain model, usually, at least in my own experience, we like to make threshold value to be. Threshold value to be 10 to negative 6. We want to make it extremely smaller. But when we do this kind of function estimates, we sort of make our tolerance level to be a little bit larger. So because you want to avoid this overfitting situations. Of course, I mean, I don't know, is there like a general ways to guarantee no overfitting at all? So I guess sometimes this might involve a little bit of numerical. This might involve a little bit of numerical explorations or maybe experience, I guess. But I think it looks like many papers in the literature about boosting approach, they would think one of the good advantages of boosting is we can't say no overfitting at all, but it turned out overfitting does not really happen very often. This reason was because of was because of this so basically it's it could be related to could be related to this increment in a sense when they choose this stepwise function alpha it could be pretty small so that they won't really so so so they every updated value for each iteration could change very slowly so that Very slowly. So that because of this iteration steps is too slow and too, I should say, its slow increment may help to avoid overfeating. So it seems many numerical experience suggest recommend boosting approach has such a nice feature. Such a nice feature to give us not frequent situation of encountering overfitting. So, and your second questions in terms of this, for example, for this inverse ideas. So we have this g function. So because g function appears in the denominator. So what if this g function is estimated to be zero or nearly zero? Zero or nearly zero, then that would blow off this formulation. Then you would end up with a huge value of this modified function, right? So that's definitely a very good and careful observations. So when we, I think like in our numerical implementations, especially like I think usually most censored data, most survival data has a Most survival data has a good portion of censorship. So, usually it could be higher than 30% many times. And in our case study, we have over 72% censorship. But when we, then another concern is how would this concern change our theoretical results? So, in our regulatory conditions, one of the conditions we assume is our censoring. Our sensoring process will have a conditional survival function to be bounded away from zero. So that will ensure theoretically we won't be in that situation. And in the meantime, for survival function for survival process FT, we made a similar requirement as well. So those kind of requirement conditions is not Is not surprising in a sense. Like, even for missing data context, when people use inverse probability-weighted GE approach, then they also have a like a probability for missing data process. So, in that case, as a denominator, so in that case, they also require that missing is probability is bounded away from zero. So, I guess that's that's my this one. That's my responses to your questions. Yeah, definitely. That's a very careful observations. So, a quick follow-up: when you did your example, you had pretty high percentage of sensoring, and then you split into test and training and validation sets. Did you preserve the relative proportions of sensoring in those two sets? Yeah, so actually in our numerical In our numerical implementation here, we didn't really try to preserve a certain percentage of censorship or not. So we sort of just, yeah, I mean, like, yeah, we did keep an eye to make sure we won't be in a case, no sensory for either training data or. Training data or validation data. So, we did try to make sure there must be certain censorship, but we didn't really require particularly what percentage of censorship must be. Of course, if it's like 1%, it's virtually no censorship. So we do make sure no such extreme situations would be happening. And also, we try to do this by using a boost tramp or page. By using like a bootstrap cases, by repeatedly generating our data to try to see, um, to try to create a sort of like a pseudo-simulation setup to see how the performance might be.