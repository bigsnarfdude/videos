The conference at the time that I submitted the abstract. So, this is a now always time for something completely different talk, except that the theme of this session is apparently people named David. So, let me tell you about work that I've been doing. This is also a matter of the new toy being the shiniest. So, this is fairly recent work with my graduate student, Max Ruth. And this is in the context of something that I've been involved with for months. With for five years now, a little bit longer than that, because I was involved in the writing of the proposal too. So, the Simons Foundation has a collaboration in the math and physical sciences between plasma physicists and applied mathematicians. There's about a dozen institutions involved now. There's might be something missing there. Two phases of the collaboration. So, up until August 2022, I was 2022, I was a member of the collaboration, and then last September I took over as the director. So it's been a very exciting thing to work on, and of course it's exciting for the application, too. So before I talk about the maths, let me tell you a little bit about plasma physics. So the goal for this long run is to be able to have a system where you can smash together tritium and deuterium. Tritium and deuterium, so these are two hydrogen isotopes, fast enough that you can get helium. And so the output of this is a neutron, which goes flying out at very high velocities into a blanket, and an alpha particle, which is essentially a helium nucleus, which you try and keep inside of the plasma for as long as possible so that it can drop off its heat. So the thing that you need in order to get fusion is some combination. Fusion is some combination of high density, so you have to have a lot of stuff that you're working with, high temperature, so it has to be going pretty fast on average, and high energy confinement time. So this is not quite as simple as you shouldn't be leaking out hydrogen. You actually want to leak out not the hydrogen, but the helium, but you want to leak it out at the point where it's slowed down enough that it's imparted most of its energy on the plasma. Its energy on the plasma because the heat from the fusion reactions kicking the helium nuclei faster is what heats up the rest of the plasma to keep it moving. So there's a couple of different ways that people think about doing this, and a key in all of them is trying to confine the plasma. So in order to understand plasma confinement, it's useful to remember a little bit about electromagnetics. If you've got charged particles that are moving through a straight magnetic field, they will tend to gyrate. Magnetic field, they will tend to gyrate around magnetic field lines. And the stronger the magnetic field, the smaller the gyro radius. If, on the other hand, you curve the field around, you still mostly follow the magnetic field lines, but there's some averaging effects, and so you drift off. And so, if you want to have a confinement system where you're bottling up plasmas based on magnetic fields, and you really do want this because this. And you really do want this because those are at millions of degrees, and if you try and bottle it with anything solid, you'll melt it pretty much immediately. We need to be able to compensate for this drift. So the way that we do this is with what's called rotational transform. And you have some real-life experience of your own with rotational transform if you've ever used a honey dipper. So if you use a honey dipper, you dip the thing in, and then you slowly. Dip the thing in, and then you slowly rotate it so that, on average, the honey is falling towards the center of the dipper. You can do essentially the same thing with the plasma by having these magnetic field lines not just wrap around, but wrap around in a way that spirals. So there's two ways that people think about doing this. One that is the most well-known, I won't say the most well-understood, but the most well-known is the Tokamak. Well known is the tokamak. So a tokamak is an axially symmetric magnetic field. You've got two sets of coils, one of which is generating magnetic fields this way, one of which is generating magnetic fields this way. It's axially symmetric, and in order to get that rotational transform so that rather than things just going one way around, they wrap around the outside surfaces, you have to have a current inside of the plant. Have a current inside of the plasma itself. So, on the one hand, this is very simple, geometrically simple. On the other hand, that current inside of the plasma tickles current-driven instabilities in the plasma. And the major challenge for designing tokamax is that you have to keep it from falling apart because of these current-driven instabilities. Those instabilities are size-dependent, which is part of why things like the Eater experiment are the size of a very large building, because the bigger you make things, Building because the bigger you make things, it turns out the easier it is to keep them stable on the side. So, Eater is from the Russian, right? But I don't know what it means in Russian. Okay. So, EDER is the International Tokamak Experimental Reactor in the south of France. This is an artist's illustration because it is still under construction after a very long period of time. Construction after a very long period of time. There do exist smaller experimental tokamaks, but this one isn't in operation. An alternative way to get that rotational transform that doesn't involve having a current inside of the plasma is to vary the geometry as you go around. So, this is a cartoon picture of what's called a stellarator. So, a stellarator has these very funny-shaped magnetic coils, and if you look Magnetic coils. And if you look at the surfaces of the plasma, those say, look at the outermost surface, that outermost surface does not look like a nice smooth donut as you go around. It looks more like a collid. So it varies, right? It usually has some periodicity. This is a picture of the Bendelstein 7X device, which has a seven-fold periodicity, although the 7 from the 7X and the 7 from the periodicity are different signs. So this is actually. So, this is actually something where we have experimental devices, not generating fusion, but generating very high-temperature plasmas. You can keep the discharges running for minutes rather than milliseconds or seconds at longest. And there's a lot of really interesting experimental results coming out of this. The target, by the way, for this is 15 minutes. And the thing that limits us is not the stability of the plasma, it's the cooling. It's the cooling. So there's a gigantic water tank at this place, and it turns out that you're generating enough heat that after about 15 minutes, or actually less than 15 minutes right now, you've heated up all the water to superheat it, and you have to shut things down for a while. So if I look at the behavior of particles traveling through this plasma, a convenient way to do this is to look at a cross-sectional surface and say, what happens? Surface and say what happens as a particle travels around and transits multiple times through the device. And depending where you cut it, you get different pictures. But if you cut in any given section, which we call a Poincaré section, you can draw a dot for every time that you come back and hit the surface. And this gives us what's called a Poincaré plot. So this is a Poincaré plot for a magnetic field that's a vacuum field. A lot of the stuff that we do is with vacuum field. Of the stuff that we do is with vacuum field configurations. It turns out that those are still relevant, even if we're not necessarily going to run in vacuum, for two reasons. One is that it tells you how things work in startup, and it tends to be hardest to keep things confined in startup. The other one is that in accelerator, because you don't have a very strong current inside of the device, which we need in order to avoid tinkering these instabilities, you also don't have gigantic shifts. So most of the magnetic field comes from the external coils. From the external cores. So, the thing to notice as you stare at this is there's a lot of structure in these pictures, right? So, if I start off a particle on one of these surfaces, and this is not following the guiding central orbits, they're called, this is not following what the particle will actually do, but just sort of the zeroth-order approximation of what the field line does. The field line will come back and hit again and again and again, and in many cases, it will fill out these quasi-periodic orbits. So, there's in So there's invariant sets to the dynamics, and what we're really interested in is what is the shape of those invariant sets? Where are those invariant sets? Because these invariant sets form transport barriers. So things that start on a surface will tend to stick on that surface. Things that end up in some of the boundary regions, you can't see it very well here, you'll have islands where you've got sort of something that, if you were to look at it in 3D, If you were to look at it in 3D, it's a tube that goes around multiple times. So it's not just going around and hitting different points on this surface, it's going and doing something like hopping there, to there, to there, to there as you go through. But we're really interested in, as much as possible, having these nice nested flux surfaces, having a minimal amount of chaotic field lines where it's hitting wherever, and very carefully structured magnetic islands. Magnetic islands because that determines many of the confinement properties of these magnetic fields. So, here's a zoom in on some interesting features of a different configuration. This isn't the Wendelshein 7X. This is the National Compact Acelerator Experiment that was designed at Princeton, although never fully completed. And you can see if you look at the structure of the magnetic field line. Structure of the magnetic field lines here in this Poincaré plot, you have magnetic surfaces on the inside. That's what you really want. Those are giving you the ability to provide thermal isolation so you can go from millions of degrees centigrade to basically room temperature within a meter or two. You have some places where you've got magnetic islands, this jumping back and forth. Those don't get quite as good confinement, but they're not terrible. And then you have this chaotic region on the outside. Have this chaotic region on the outside, and in those chaotic regions, the temperature of the plasma tends to equilibrate a little bit more. So, you've got very little thermal isolation from that. There's a little bit, but that's a topic for a different day. So, the plasma mechanics are interesting and a lot of fun to deal with, but actually, generating the magnetic field line flow map takes a little bit of time. So, most of the things that I'm going Of time. So, most of the things that I'm going to show you today are with a much simpler dynamical system with which we can illustrate the same types of problems. So, this is sometimes called the standard map or the Cherkoff-Taylor map. It's on the torus cross R. So, all of these things tend to be periodic in one way or the other. So, it's quite simple that we're going to have x incremented by y mod 1 in each step, and then mod. Mod one intercept, and then y is going to be hit by some sining floidal function. So it turns out that this map, like the flow maps for the plasma, is a symplectic map. So the plasma physicists call the magnetic field flow a one and a half degree of freedom Hamiltonian system, which made no sense to me for the longest time. And then I figured out that this was because they were thinking of the angular coordinate as their time coordinate. Angular coordinate as their time coordinate. So you do that, and it starts to make sense. So if we look at the structure in these maps, we see lots of things in the Poincaré plot. We see what are called X and O points, which are hyperbolic and elliptic periodic points. And those tend to separate interesting structures like our circles or our islands. We see invariant circles where you're just sort of going around and around on a connected quasi. Around on a connected quasi-periodic orbit. We see islands where you've got these connected components to your quasi-periodic orbit and you jump back and forth between them. And then you've got regions of chaos. And so what we would really like to do is not just draw pictures of Poincaré plots and stare at them. What we'd really like to do is to be able to put things about these Poincaré plots inside of an optimizer and optimize properties of the magnetic field. And in order to do that, we would really like to. And in order to do that, we would really like to be able to identify these types of structures as cheaply and automatically as possible. So there's a few different ways that you can do this. We've already talked about the make-up array plot and eyeball it, which is sort of the state of the art in many places, but is not entirely satisfactory for optimization. If you just care about invariant circles, there's something called the parameterization method, and I'll talk about that for this slide. About that, for this slide. And then I'll spend most of the time talking about things that came out of the idea of forming a function whose level sets are invariant sets for this flow. And that's going to actually take us to modeling the dynamics for a single field line in a way that's connected to the level sets picture, but provides some additional insight of its own. So that's the plan of attack for the rest of the. That's the plan of attack for the rest of the talk. So, the parameterization method is for finding an invariant circle. So, the idea here is suppose that we have f as our map from the domain to itself. So, that's the thing that we're iterating. What we're going to do is write down a parameterization for the circle or for something that's topologically a circle. So, we'll write it in terms of a Fourier. Of a Fourier expansion. I'm doing it with complex exponentials. You could do sines and cosines if you prefer. This is going from theta into R2 or into T cross R. And that's going to give us some shape. And now we want to determine the coefficients of that shape so that f of z of theta is equal to z of theta plus omega. Omega is what's called the rotational transform. It's going to come up with the properties of To come up with the properties of omega turn out to be really important. We'll come back to that later. So the parameterization method says, okay, if you want to fit something like this to solve that equation, what are we going to do? Discretize everything in sight and do an optimization. So we'll solve a nonlinear least squares problem where we say take an equally spaced set of points along the z of theta curve and make sure that they map to z of theta plus omega under the action delta. Under the action event. You need two additional constraints in order for this to make sense. One of them, which basically constrains the phase, right? So z of theta and z of theta plus tau parameterize the same curve. The other of which tells you which of the curves you're looking at, because if you remember the invariant circles that you saw before, we didn't just have one at the time. We had these parametric families. They come in nesting families. So usually, if you're going to compute things this way, To compute things this way, you'll start from, say, a fixed point of your map. Near a fixed point, you can write down what the invariant circles look like at first order. It's a nice linear map at that point. And you can use that as a starting point for a continuation process. So you either need to do continuation or you need to do something where you've got a good initial guess because this is a non-linear solver. And usually we do continuation. Notice that this doesn't tell us very much about chaos, doesn't tell us. Us very much about chaos. It doesn't tell us anything about chaos. We actually have the possibility of running into resolution issues as you get close to the boundaries between your circles and chaotic regions or between your circles and island chains. And the problem shows up in that you need to have a very large number of Fourier components in order to capture these increasingly pointy shapes. So there's a lot of things that we don't get with. So, there's a lot of things that we don't get with this, but this is a useful starting point for thinking about other methods. A second thing that we could do is to say, you know, if I'm interested in, not the chaotic regions, those are hard, but if I'm interested in at least characterizing invariant circles and islands, what if I say those should look like level sets of some function? So I'll have a label function. This is sometimes called a flux label in plasma physics. And I want to learn that flux label from data. Flux label from me. So I will, for example, take a kernel type of approximation. So I'm going to say, let's write down a function h whose level sets are supposed to be invariant under the action of f. So h composed with f is supposed to be equal to h. And I'm going to write down an ensense for what h should look like in some simple linear space. And then I'm going to, if I say h of xj. If I say h of xj is yj, h of f of xj is yj prime, I'm going to do something that minimizes the smoothness or minimizes something that controls smoothness of my function. So I want the label function that's not crazy. I want it to be close to some target function. I need to do something like that in order to keep it from just collapsing into a constant, which is about as smooth as you can get and is nicely invariant and is completely useless. And I would like it to be the And I would like it to be the case that, at least on the data set that I've sampled, I maintain invariance. So the yi should be equal to the y i palace in the iser chase. There's several different ways that you can set up this optimization problem. There's not a unique solution, right, because there are many functions that will have the same level sets. And so, exactly the right way to do this, we're still fiddling with the details of that, but this is certainly. With the details of that, but this is certainly a plausible way of dealing with things. This is something that we actually thought about a little bit because one of the classic ways that people look at invariant sets is in terms of what are called Birkhoff averages. So the idea behind a Birkhoff average is that you're going to think about turning an average in time into an average in space. So suppose that I've got some measurable function. Some measurable function. So h is going to be my measurable. It doesn't actually have to be c infinity, but that'll make things easier later on. The Birkhoff average is going to be something where I say the K-the Birkhoff average, I iterate this map k times, and I take the average of the image of x under each of those points. So on each of these invariant circles, there's typically some measure. And what this is doing is computing the expectation of h over The expectation of H over that circle under the measure. Move to a different circle, you've got a different invariant measure. And so this, assuming that H is in L1 or that the X's remain confined to a compact set, either one works. This converges almost everywhere to the conditional expectation of an invariant measure on an invariant set. This is something that is well known in the dynamics of measure-preserving maps and is very useful. And it's very useful. So, you might ask: what's the error behavior of this thing you're doing averaging? The answer is you can get additional information by looking not just at what the average is, but how quickly you converge the average. So if you've got a nice regular quasi-periodic orbit, it turns out that the rate of convergence is basically 1 over k. K is the number of samples that you've taken. If you've got something that's chaotic, this looks much more like random draws or quasi-random draws from I random draws from some region of space, and you get the type of averaging behavior that you would expect from Monte Carlo, which is to say 1010. So this is great, right? So I now have a tool that allows me to, at least in principle, not only look at computing a function whose level sets tell me things like periodic orbits, but I can also get out information about what field lines are chaotic and what field lines are irregular. But of course, 1 over k and 1 over root k are both pretty slowly convergent, right? So this is great, and it converges in the long run, but at some point we're very tired of the long run. Apologies to Keynes and apologies to Pete Stewart, from whom I stole this quick. He was talking about stationary iterations of various sorts. So, what we would like to do is to, oh, I showed you that all the way back. That a little bit. So, what we'd like to do is to do something that's going to compute the Birkhoff average, but do this a little bit faster. And this is really where the Simons collaboration research sort of kicked off, which is Jim Meese, who's at CU Boulder and is a dynamical systems person, said, you know, there's this idea from a few years ago of not taking averages, but taking weighted averages. And if we take weighted averages with the weights tuned in the right way, which might Way, which might be familiar if you think about sort of the sequence acceleration literature, we can potentially accelerate the convergence, at least for these regular points, from 1 over k to faster than k to the minus m for anything. So super algebraic. You can actually get down to exponential, but that requires some hypotheses on the regularity of orbits. Okay, so we've got this weighted Birkhoff average idea, and this is And this is where I started scratching my head and saying, wait a second, this sounds very familiar. Maybe there's something else with it. And in order to understand that something else, it's useful to look at this problem from a signal processing perspective. So let's go back to our picture that we had with the parameterization method. So z of theta is going to be our parameterization for an invariant circle. We want f of z of theta to be z of theta plus omega. We can expand z of theta in terms of this series and complex. Of this series in complex exponentials. And now, if I look at what happens if I iterate this over and over again, you're going to start, if you start at 0, 0, you're going to then go to z of omega, z of 2 omega, z of 3 omega. So z of t is going to be a Fourier series where we replace that theta with, that should be theta plus, no, that's theta plus. We replace that theta with theta plus omega n t, right? Omega n t, right? And so I can rewrite this in terms of z of t is z hat of n times psi to the nt, where psi is going to be on the unit circle. So this is e to the 2 pi i n. And similarly, if I observe something at these points, the thing that I'm averaging, I can expand that series too in a similar Fourier state. So I'll have h of t, which is my h of z, is going to be a Fourier. is going to be a Fourier series with coefficients hat n and then psi to the nt as the solution. And if I think about what is the average in this case, the average is going to be the constant term of this series, right? Everything else, as you average out over time, is going to drop out. The Birkhoff average should converge to that H hat norm. If we look at what the weighted Birkhoff average is, If we look at what the weighted Birkhoff average is doing in terms of these coefficients ψ, what we really have is that the standard Birkhoff average is going to involve a polynomial filter, which is a cyclotomic polynomial. So something that has roots all around the unit circle. The coefficients are all one. But we can change that polynomial filter in order to get good behavior. So let's do this in pictures, right? So here's the picture of what sampling from one of our periodic orbits looks like. So you jump from one point to the next to that x to the next, going around on this. If I look at my x and y coefficients, there's some behind the scenes parameterization of this circle, and you're jumping on non-discrete points on that parameterization. And if I write down, okay, what's the Fourier decomposition of this thing? So I write things down in terms of its coefficients. Down in terms of those coefficients, what you see is not unsurprisingly, you've got a contribution at zero, that's the thing that you care about, and then you've got these additional contributions at the multiples of the frequency radio. And what the standard weighted Birkhoff average is doing is saying, okay, what we're really interested in is the behavior at zero, so let's filter out everything else, right? Filter out everything else, right? And so we've got a filter polynomial, the standard filter polynomial that corresponds to Birkhoff averaging, the cyclotomic polynomial with roots that are, or with coefficients that are all one, decays rather slowly as you go away from zero. But if you say, I really want to suppress things away from zero, you can come up with other polynomials that are going to be smaller out here, maybe at the cost of a little bit of additional width close to the origin. Close to the origin, but as you increase k, you can make this a tighter and tighter and better benefit. So that weighted Birkhoff averaging is already enough to give us super algebraic convergence, but of course, if you think about this from a signal processing perspective, if you're able to get some information about this distribution in your coefficients, you should be able to do a much better job with the filtering, right? And so you can do adaptive filtering where you look at the signal and figure out where are these multiples of omega. out where are these multiples of omega, and that's where you're going to put your zeros in the filter plot. And so by doing this, you can essentially kill off all of these things that are contributing a lot of extra stuff to your weighted average and get a much better approximation to what's going on in the earth. So here's the adaptive weighted Birkhoff picture. So we're going to come up with a filter polynomial that should kill off those coefficients. coefficients at e to the 2π i n omega for different n's. And how do we adaptively choose that filter polynomial? So I will come back to this, but before I go too much further, let me say you can't do this always. In order for this to work, you can't have a lot of mass that's very close to zero accumulating. And so you need to have two things in order to avoid that. Two things in order to avoid that. One of them is you have to have a smooth enough circle that there is sufficiently fast decay in the Fourier coefficients. The second one is that you have to have a sufficiently rational omega. So one of the things that is fascinating and furiating about this is every couple of days we end up having to dive into some number theory in order to deal with this sufficiently irrational business. Rationalize necessary. Okay, but if I was to ignore this and think about other situations where you see similar types of things, one of the places that we know how to do this is in the extrapolation literature. So connecting to what Hans was talking about with Anderson acceleration, except this is not trying to accelerate a fixed point iteration towards its fixed point. This is accelerated convergence of something else. So what we need is to So, what we need is to form an auxiliary sequence. And the way that we're going to form the filter polynomial, we have to get this auxiliary sequence is we say, let Ht be our sequence of observables. We're going to look at differences between Ht and Ht plus 1. That allows us to kill off that constant coefficient. We don't want to filter that out. We need that. And so that's going to give us something which looks like a sum of lambda n minus 1 times your h hat of m lambda. One times your h hat of m, lambda the m, lambdas of m to the t. This is, in most of the situations where people use reduced rank extrapolation, going to involve lambdas that are bigger or less than one. And now you're going to see coefficients that minimize the sum of the c k e k plus t subject to the ck summing up to one. So you say, I want an average, and I want an average that, if I look at this, is going to be essentially saying if I apply. Essentially saying if I apply this average to the sequence and I apply the average to the shifted sequence, the difference should be as small as possible. So there's standard analysis for this. This is not a technique that we came up with. There are a few tweaks. One is we do the vector version of Ruce Rank extrapolation rather than the scalar version. That's what I described anyhow. We've got rectangular angle matrices that come Rectangular angle matrices that come out when we solve this least squares problem. You do FFTs for solving that. Solve the least squares problem with LSQR, at least when we get big enough. We want to constrain for time reversibility. So we're interested in filter polynomials where the zeros are all on the unit circle. We really don't want them inside and outside the unit circle. So by saying I really want a polynomial whose coefficients are palindromic, that guarantees that anytime psi is a root. Time psi is a root, so will be psi inverse. And so we constrain that. And then we measure the convergence of this thing adaptively by looking at residuals. So the fly in the ointment. So I told my student Max, we should try this out. We tried it out. It worked great. We went back and we said, okay, what's the standard error analysis? And Max reminded me that the standard error analysis involves looking at these eigenvalues lambda. Eigenvalues lambda, or these rate coefficients lambda, putting them in reducing order and saying the extrapolated average is going to eventually go like lambda to the j plus one to the 2k power. But if all of your lambdas are on the unit circle, this does not help you at all, right? So we're in a slightly different situation than the standard Blue Strength extrapolation picture, and so we need to do something a little bit different. And so at a very high level, what's the flavor of that something different? It's exactly these two conditions that I talk about. It's exactly these two conditions that I talked about earlier. You need the coefficients to decay fast enough so that you're not trying to filter out very high-order coefficients. And you need this sufficient irrationality. That sufficient irrationality has to do with powers of psi not getting too close to one. If the powers of psi get too close to one too quickly, then even if the coefficients are decaying nicely, you still build up mass close to the origin that's hard to filter out. Close to one that's hard to filter out. Of filters are close to the markets aren't to the correctly. Okay, so let's take a look at how this works. So if I think about the difference between the weighted Birkhoff average in which we do non-adaptive filtering and reduced rank extrapolation in which we do adaptive filtering, I get slightly different convergence behavior. So this is for a number of different orbits, same set of orbits for both pictures. Of orbits for both pictures. The x-axis is too small to read, but it goes up to 10 to the 5th here and up to 10 to the 2nd here. So you're going to, in both cases, have pretty rapid convergence. The convergence is somewhat faster with the adaptive methods, which is what we hope for. But in both cases, actually, if you run for enough iterations, there's a pretty clear distinction between chaotic and not chaotic. And so you can sort that out. And so you can sort that out much more quickly than you could if we were just looking at the standard workoff almost. So we can look at these measures and these measures determined in terms of the least squares residual to determine does this look like we've got chaos or does this look like we've got a regular point that's on a quasi-periodic core. And if you look at the picture for the standard map, Picture for the standard map, I forget what the threshold is here, so it's 10 to the negative 8 and 10 to the minus 4. You can see that you do indeed have some regions that are legitimately chaotic. Those show up in red. You've got lots of places where you've got invariant circles in blue. There's a few places where it's actually difficult to distinguish. Those tend to be associated with exactly this difficulty that I talked about before, where you have a rotation number which is very close to being. Which is very close to being rational. So you haven't quite gotten to the point where it's switching to an island or it's chaotic, but it's enough to interfere with convergence. So this was all well and good, and we were excited about this. And then I give Max credit because I think he pointed this out to me. He was like, wait a second, we're building a model for what the dynamics of these things are. And what we're interested in also. Are. And what we're interested in ultimately is saying what are the invariant circles as well as where are the chaotic regular regions, right? So why don't we use the model to tell us what the invariant circles are rather than plotting them out? So how is this going to work? We can form the filter polynomial with coefficient c. That gives us a set of frequencies, natural frequencies, and we expect that we can write down these invariant circles in terms. invariant circles in terms of e to the 2π i omega t, right? We just happen to be sampling at integer points, but you can fill in in between. So if you can learn not only the frequencies, but then the coefficients to put in front of the frequencies, and that's a simple enough thing to do, then we can figure that out. So what are we going to do? We'll form the filter polynomial. We'll find those natural frequencies in terms of a polynomial root finding problem. We'll sort Finding problem. We'll sort, and I should say, by the way, this is a polynomial root finding problem, which is about as well conditioned as you could possibly hope for, right? Because you're looking for roots all on the unit circle, right? This is the nicest thing that you could ever look for for eigenvalue problems or polynomials for polynomial problems. We'll sort out, we'll say where are there large contributions to this signal. We'll take the 10 or so most contributing frequencies. We need to identify where there are rationals. Identify where there are rationals. If there are rationals and we've got fast convergence, what that indicates is that you're on an island chain. So if you've got a rational of type T over Q, you're actually hopping between two different distinct components in your periodic orbit. And so we can then say, let's look at the sequence of every Q step of this iteration that will identify one of those islands. We can do reduced rank extrapolation on that to identify what the rotation is. To identify what the rotation number is. If we don't have any rationals, we say the largest of these is going to give us that rotational transform on the. And then we get the shape and these characteristics of the circles and options. So this works pretty well. So this is a picture from us of the picture and this side of that standard map, the Chirkov-Taylor map that I mentioned earlier. The Cherkoff-Taylor map that I mentioned earlier. But what happens if we're interested in something that's a little bit more realistic? So here's a picture where we've got a thousand random trajectories. We run them all for 900 steps. We draw them points, right? So this is going to be a Poincaré plot. This is for a cross-section of a Stellarator configuration that has not been built, but is certainly under active design at the University of Wisconsin. Dell configuration. Delt configuration. You can see that you've got a core where you've got nice invariant circles at this point in the configuration space. You have an island chain here that you have to worry about. You've got a bigger island chain here, and then you've got some stuff on the outside. So what we're going to do is run this. We're going to use filters of at most size 300. Tmax is going to be 900. So we're going to scale the size of the filter with the number of The size of the filter with the number of time steps that we use for the fitting, but we're not going to set them equal to each other. And residual tolerances of 10 to the minus 6th, then we also have to have a tolerance for how close are you to a rational. We said that's a 10 to the minus 6th as well. And if you run this thing, you get this beautiful split where here's the picture that we had for our Coingrade plot, colored by the residual. Colored by the residual, so you can see some places in red where you expect to get chaotic behavior. This is what you actually see if you do the diagnostic to say where do you have chaotic field lines and where don't you. So you can see that in the core, actually there's a surprising amount of chaos in between some of those invariant circles in the core. But in the core, things are pretty good. You've got this nice region where there's lots of invariant circles. You expect good confinement and then some island chains and then chaos on the outside. Island chains and then chaos on the outside. This is actually what you hope for in this configuration. The islands are used to direct the plasma so that it hits in the right place. So, this is what's sometimes called a diverter. If you let the plasma hit anywhere on the wall, you melt the wall pretty quickly. These are your invariant circles, and then this is the island chain structure. And this can all be derived automatically using this extrapolation technique. Okay, so this is work that is. I have two draft preprints that I need to comment on and that should go up on archive. But I still consider it somewhat early. There's lots of things that we're still figuring out. So there are, I think, a lot of pros to this method. So you can do this classification of chaotic versus regular field points. Field points. That's pretty important, and we can do it much faster than lots of the ultra normals. So that's exciting. We can recover the structure of the invariant circles and islands much faster than you could by just sampling and drawing a Plumbering plot. And you don't have to do the initial guess or continuation that you would have to do with the parameterization method. This is also parallelizable over trajectories, which is nice. So the Poincaré plot, each individual trajectory is Each individual trajectory is something that you have to run sequentially, right? So if you run it for a million iterations coming around, even if each one is pretty fast, it takes a while. What are the commons? One of them that I mentioned, and this is across all sorts of methods in this area, is this Diophantine condition. If you're close to low-order rationals, everything tends to fall apart. This is not unexpected. Unexpected. We actually expect physically things to kind of fall apart when you get close to low-order rations. There's also some additional linear alpha cost versus the weighted workoff. So being adaptive is good, but being adaptive is not free. So it makes things converge faster, but particularly if you're taking a few thousand integrations, you're now solving the least squares problem. That's a few thousand columns by George. Columns by two or three times more rows. One of the things that I like a lot about this is that you can, if you take this perspective not of Birkhoff averaging, but of learning the dynamics and then projecting things from the dynamics, you can consider going beyond this magnetic field line flow and actually considering the thing that we would really like to learn about, which is what happens with the particles, including the drifters. So this guidance center proximation. Approximation. And that's stuff that we're working on with Chris Albert at TU Gross right now. In the higher-dimensional case, the picture for invariant sets is much more complicated, but this model of the trajectory philosophy should still work. So we're pretty excited about this. So that's everything that I had to say, which puts me right on time. I do want to leave with two URLs. One is: if you're interested. One is if you're interested in playing with the code and want to tell us, for example, how to do that least squares problem faster than we're doing it, the code is up on GitHub. And this is one of a few projects. This is the newest and shiniest. I've also done stuff with optimal design out of coils under uncertainty and with direct optimization of alpha particle flux with a couple of my students. And there's lots and lots of problems here. Here. It's a set of very smart people who have been not necessarily talking to a broad applied math community for a long time. And so I think there's lots of opportunities for people to pick up problems in this space. With that, I will finish. Thank you very much. Questions? I think you said this at the beginning, but I wonder if you can give like a summary. I wonder if you can give like a summary statement about how handling these various mathematical issues, what does it tell you about the fusion? So these magnetic flux surfaces form transport barriers. So the rotational transform matters too, but assuming that you've got a good rotational transform profile, what tends to happen is that particles that start off To happen is that particles that start on a surface will stay on the surface. Particles that start in an island chain will stay on that island chain, but they'll diffuse a little bit. Particles that start in the chaotic regions will diffuse all over the place. So if you look at the temperature profile for these stellarators, if you want to have a very sharp temperature gradient, you better have nested flux surfaces. And so we care a lot about the nested flux surfaces close to the core for that reason. The other thing that I think I mentioned in past. The other thing that I think I mentioned in passing, but didn't say a great deal about, is at the point where the alpha particles in particular slow down a little bit, and also at the point where you're hitting the surface of the wall with neutrons going pretty fast, right? So stuff spalls off and goes back into the plasma tube. You want to get that stuff out. And so another key piece of this is optimizing with transport properties close to the boundary. And so it's not just that you want nice, nested. Want nice nested flux surfaces close to the interior. You also want a combination of chaotic regions and the islands close to the boundary so that you can guide those impurities essentially out through the chaotic regions. And the islands form barriers so that you're not uniformly hitting the wall and melting everything. More questions? Anyone on Zoom has a question? If anyone on Zoom has a question, please feel free to go through it. Okay, if not, well then thanks very much again.