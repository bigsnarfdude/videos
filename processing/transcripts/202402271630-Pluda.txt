So today I'm going to speak about the network flow, which is my most loyal companion of the last 10 years. I have been introduced to the problem by my mentor, Matilaza El Novaga, almost 10 years ago. And then along with Beway, I worked on the problem with several friends. Today I'm going to present a paper mainly in collaboration with Gerald Slira, Reis Mateo, and Marguerite Spaitz, and also good. Against sites and also good market set down. And then the last part of the talk is basically open problem. We have only partial results, but we have written together with Yoshiro Tonegawa in an isosports. I recognize. Okay, what's the context of my talk? The context of my talk is mean clear flow. I think I don't have to remind this audience what is the mean temperature flow. Anyhow, the mean cooperator flow is one of the best at the geometric evolution problem. Problem. It is variational. Indeed, it can be interpreted as the gradient flow of a very functional. So, Swiss means that during the evolution, the surfaces decrease with area and not only decrease, but with respect to the steepest descent, with respect to well-chuged scalar product. And from the PD point of view, it's a second-order PD. So, there's been amazing results about this flow. About this flow, I wanted to mention the result by Wisconsin of 84. They said to us that when you have convex surfaces, this surface remains convex and some point shrinks to a point. And you are also able basically to understand when shrinks to a point. However, we all know that if the initial surface is not convex, the problem is not fatiguing, and the flow can develop singularity. There has been really amazing results about the mean purposure flow. I cannot mention really all the results, and also there has been really And also, there has been really great development in the last two years. But before results on surfaces, there were curves. Even if nowadays seem to be simple, the description of the evolution of the curve, surely the complete description of the curve shortening flow has been one of the greatest achievements in the 80s, one of the first achievements in the field. So, what is the curve shortening flow? In the case of the current short flow, we have again, but Flow: We have again that we have the gradient flow this time of length, and this means that the curves evolve every point in time in the direction of the curvature. But in this case, since it's only a curve, we have only one curvature, it's nothing but the second derivative with respect to our claim parameter. Okay, beauty of simplicity, Grayson theorem. Any simple closed curving plane that evolves under curved shorting flow. Under curb short inflow, at some point becomes convex and then becomes rounder and rounder and shrinks to a round point at the final time. We can also say that when there is shrinking, the curvature blows up, and this is an easy consequence of gauss magnitude. Not only, it's also possible to really have an estimate of a loss span of a curve, and the maximum time of existence only depends on the enclosed area of the initial curve and nothing else. Even better, we can say that nothing bad happens till this maximal time, but the flow becomes also smooth. We have a smoothing effect, so even if we start with regular initial data, for a positive time we are actually seeing it. Okay, what can we say more about this since this problem is complete? So my aim is to pass in substance to singular surfaces, and I want to start it from the very beginning. It from the very beginning. What's the first example of simpler surface that you can have in mind? That's not a manifold but can be still parameterized and it's really easy to deal with. I want to speak about the flow of network. A network is a connective set in the plane. Connectives were only because connectivity is maintained. If we have different connective components, we have work differently. So we compare. But it is composed of violently many regular curve with regular, I mean, but if I have a parameterization, the parameterization is. But if the parameterization parameterization is regular, gamma dot dot is different from zero for every x, but are also embedded and as smooth as we wish, it's not the point here. But these curves meet at their end point in junction. They don't self-intersect, they don't intersect one with the other, apart from the end point, where we have this junction. And this junction can be understood as the simplest example of symbolic. How we do the right flow? Again, we think. Again, we think about the effect that it is a variational problem. So, this means that we do a variation of a network. So, we consider a network and a variation, and in the variation, we have to allow the junction to move. The junction is not fixed because we want to decrease the global length of the network. It's clearly the junction is to move. And we do the first variation of the length functional. And what we end up with, we end up with a part in which we have a minus the curvature exactly as expected. But if you want to understand, But if you want to understand minus the curved as minus, the curved virtual is minus greater than the length, we have to put zero the boundary part. So in this case, we have a boundary part that are maybe this fixed point, but also the junction point. And if we want to really understand minus kappa is the gradient, this part disappears. So if we consider the simplest possible example of only three curves, what does this mean? So tau is the unit tangent vector and then The unit tangent vector, and I have to put zero the sum of a unit tangent vector. And in the case of three curves, it means nothing but that at the junction, the curves from angle all equal of 120 degrees. So, however, the situation becomes more complicated. Yes, I'm in R2, but with I'm in the plane. Yes, but this computation is in general local, so you have always this balancing function. So, even in Function. So, even in more general situations, you have that sum of the tangents equal to zero means understand. So, the situation becomes more complicated when we have multiple tangents, well, not only frequence meets, but more than three-quarters met, because this sum of a tangent equal to zero, it's not clear exactly what does that mean. It can mean more than one thing. So, basically, it means that the two angles are equal in pairs, but they are not fixing the angles, even if. The angles, even if there are only four curves. However, again, the operational point of view comes in our end, in our help, because what we realize is that if we have a junction that's not triple, we have more than three curves meeting, then if we split the junction in two triple junctions, in this case, the length of the network sharply decreases. So, this means that for most of the time, we expect to see only triple junction network in the evolution and not more. Network in the evolution and not multiple junction. And this is true in any case in which we have more than three junctions, precursors, not only in the case of four clips. You split the junction in triple junction, the length sharp decrease. So if you want to look at the original point of view, this means that you have a jump, minus 15 curvature. It's very convenient to split. So this means that at least from the very beginning, we can start to think about the evolution only in the ambient of network only. Only in the ambient of networks with only triple junction. And when at some point we will deal with also other cases. But let's start with the simple one. Okay, so this is for the variational point of view. What I want to do, I want to study strong solution, classical solution. So I really want to arrive to write in a PDE system. And so to do so, I have to impose some boundary condition. So again, let's start from a very simple case of only network mimicry called junction. Case of only network, we have a triple junction. And for simplicity, if we have some external termini like this, but where only one curve arrives, we fix it in the plane. Or if we are on the torus, for example, we can impose some periodic boundary condition. And then my computation of before, what was saying, that if now I parameterize the curves from 0t times 0, 1 into f2, I have again the first equation that is exactly the curve shorting. That is exactly the curvature inflow. The normal velocity is the only velocity that is imposed from the variational computation and is equal to the curvature. When I want that network, it remains a network during the evolution, so that all of the gene is preserved. And to do this, I can interpret the triple junction as a special boundary point. I can ask what all the filters are matching at the junction. And the other condition that I did before is that the sum of the unit tension vector is equal to zero. Again, with the angle. Equal to zero again with the angle condition 110 degrees. Then, as I was saying, I have extra boundary condition of a single terminal. So, this is my EPD system. So far, so good. What are the two things that I really want to stress? First of all, the fact that the junction are free to move during the evolution for basically two reasons. One, the easy one, because if we want to decrease the global length of the network, if the junction were fixed, clearly I could only Clearly, I could only connect the fixed junction with segment, was not a very interesting problem. And also, from the PDP point of view, it was not only not very interesting, but almost never solvable, because I was imposing all this condition, plus also the fact that the education is fixed like this over the problem. So, clearly, the junctions are apt to move, and what it is what makes the problem interesting. And another point that I really want to stress is the fact that we have also tangential motion. So, we are used. So, we are used to the fact that we impose only the normal velocity because this is what comes from the first vection of length or the area in general. However, here I have three curves and I have that the sum of a unit tangent vector is equal to zero. I'm in the plane. Sum of a unit tangent vector is equal to zero means also that the sum of a unit normal vector is equal to zero. And when I have a junction that has to remain attached, so the velocity pair of the three curves has to be the same. Of the three curves has to be the same for the curves, and if I have to move normal direction for all the three curves and I have the sum of the normal that is equal to zero, this means that the curve doesn't move at all at the junction. Clearly, this is not what we want. So, the variational point of view said to us that we have only a normal prescribed normal velocity, but we will surely have a tangential version. So, for us, this is an advantage because written like this problem is the generative, it's a second. Is the gene? It's the second, yes. Yes, sure. So we have at the summer unit tangent vector, then you have at the junction has to remain in the junction. So the velocity of the three curves has to be equal at the junction. But if the velocity was all in normal direction, it has to be equal. This means the velocity has to be zero. So clearly you have also a tangential component of the velocity. A tangential component of the velocity. So we have from the variational point of view that we are fixing this thing. But this is equivalent to saying that you have a velocity in which you have normal component plus a tangential one. And this can be turned into an advantage because basically I can choose this guy here. So if I write this thing here, exactly, exactly. And if Exactly. And if I write this thing here in parametrization, I end up with a quadrilinear in PDE, second order, but it's degenerate, because I have a degenerate direction. And then I can choose piece properly in such a way that my motion now is piecemark. It is a very nice. So for all this constraint, that is also often. Okay, so this is the problem. So far, so good. So the question. Good. So, the question could be now: why I have to be sent to this word talking about a one-dimensional problem for another 40 minutes, but seems to be pretty easy. Apart from the reason that it's beautiful, the problem is beautiful, not good. The point is that what I'm doing here is like I'm really trying to develop a theory of strong solution for the mean conversion flow, even in presence of singularity. I clearly know that it is the most basic one. I clearly know that it is the most basic possible example because the junction would be like very bad singularity, but this is really the first step towards a more general theory of higher dimension surfaces. And what I really find very interesting of a network flow is that we enlarge the class of possible singularity of a mean curvature flow, in the sense that this flow is going to develop some singularity that are only topological singularity, but with bounded equipment. But we've bounded the Kubernetes. This is a complete phenomenon in the realm of mean covers flow. We know that for smooth surfaces, when we have a singularity, the Kubernetes flow plus R. The super sequence is plus infinity. Instead, I'm going to show you some singularity in which B doesn't happen. And B's are not only singularity that exist, but expect them to be frequent to be the generic singularity of the flow. So B is completely new phenomena. Business completely new phenomena. And another point of view of this problem is related to application. So, my aim is to close the gap between simulation and theory. So, if you look at simulation, why very interesting and people in the applied community are interested to this problem because the network flow can be understood as a very simplified model of the evolution of grain boundaries. So, when green boundaries arise, if you have a metal, If you have a metal, the structure of a metal is called crystalline. So, this means that there are crystals but marble ones with different orientation. Then, okay, you can eat the metal. This is standard procedure done for creating metal with some desired property, for example, stiffness or conductivity of these kinds of things. And when you hit the metal, this grain rearrange, remove, and the polycrystal crystal dynamic structure try to rearrange in a way that all the In a way, but all the different crystals are oriented in the same way. And what you see, you see the boundary with grain, but it wars. And it's been proven that it warms proportional to the motion by temperature. Species should be simplified. It's more complicated, the real problem. However, the natural flow is like the basic version of this problem, so very interesting simulation. So if you look at simulation, there are really a lot of simulations because they are useful in the flight community. They are useful in the flight community, you realize that there is a common feature about all species simulations that are around: a coarsening type behavior of the flow. What I do mean with coarsening type behavior. So, I mean that there is a simplification of the topological structure of the flow during time, during evolution. So, you start with something like this, a network with hundreds of triple junctions, hundreds of edges, hundreds of grains, and then you. Hundreds of grains, and then you see that during the evolution, we have topological similarity in which two or more junctions polish. When you restart the flow, you have something that is simpler, simpler. And if you look at all the simulations which are around, we always end up with something that is basically only one grain. So, this is completely common in a simulation, but there is no mathematical properties. So, the result that one would like. So, the result that one would like to prove is this one. You start with an initial network with very complex topology. Then you want to show that there exists a solution in the maximal time interval 0t. This evolution can have some singularity. One would like to prove that the time singularity is isolated, so finite number. And when you reach the capital T, and when there is basically a dichotomy, either capital T is finite and everything disappear exactly. Everything disappear exactly in the case of the closed curve. You can, for example, think of a network with the shape of theta. You expect everything to disappear, a center point. Or maybe you are in a case in which you have fixed some boundary condition. And then you expect in this case that the time existence is plus infinity, and you converge to a critical point of a length functional with a straight segment whose topology is. Whose topology is drastically simpler to the one of the very beginning. What is very difficult to prove in this theorem, because it's not a theorem yet, is the fact that the topology is drastically simpler. So a lot of part of the statement are already being proven, but the difficult part would be this simplification of the structure. Something that I would like to let you notice here is that I have only written the converged to a network with a straight segment, but I have never written that it's with. But I have never written that it's with triple junction. Because within general is actually not true. We have an example of a situation in which you converge something as a quadruple junction at infinity. So this singularity topological singularity can also develop at one time infinity. Okay, so this is what I would like to prove. Sure. You should think about your own potatoes. You should think about you around the Taurus. I'm like trying to, yes, I'm trying to draw in a fundamental domain of the Taurus. So it's one hexagon, basically. So it's one guy because yes, I have. Or in a case, maybe only one is not what you can actually prove, but I expect to see few grades. This is the point that I try to underline later. Okay, so this is what I would like to prove. And now the very What I would like to prove, and now for the first half of the talk, I would want to talk about the fundamental results in this direction. So, some of these parts of this theorem are already proven, and I want to tell you what the thing that we are able to do because it's the geometric part. Okay, first of all, I would like to give you a little bit of idea of what Mussel does. First property is the phonyman law, which is very unique and applied to unity. In the Bernoulli and the applied community, this property was already being derived by Malins, someone in applied mathematics, in B15. So, consider a bounded region. I use a lot of words for bounded region. I use the word grain, cell, solving. I'm fixing everything up. But it's bounded by a loop composed by less than six curves or m curves. And when you can compute the evolution of the area, let's keep in mind. The area. Let's keep in mind that we have a strong solution, so we can do all the computation, but we want basically a smooth solution. And the evolution of the area depends only on the number of curves that bound below because the entity is the integral of kappa and we use Gauss only, a generalized version of Kauspune for angles with angles. And it's exactly the number there. So this means that if m is less than six, the grain is going to sink, where it increases. Where they decrease, m equal to 6, where they remain the same, and because 6, you have a magnification, you know, becomes bigger. And from this computation, you also can prove that when the length of the loop goes to zero, then the curvature blows up, the alternative curvature must blow up. And again, this is a combination of Gauss-Bonley filtered inequality. Inequality. So, this is the basic property, and then I have to tell you what happens at singular time. Suppose that capital T is a singular time, then when T reaches capital T, one of the two things where it are predicted or both happen. Either the inferior limit of the length of at least one curve of the network is zero, or where two normal groups should become subbounded. So, the second. So the second is kind of clear also from the issue before. We have seen from this inequality when the lamp goes to zero, when the coupon should go out. The interesting part of the story is that it can happen one without two. And this is the new phenomenon in the Big Bridge. So first of all, singularity actually. So not only in simulation, but we have a lot of analytical examples of singularity. Of analytical examples of singularity that we can prove analytically. Here are some of them. The easy one is the one in which it will severe. I've told you that when I have less than five curves, the area shrinks. So it's some point if I supermediate figure, you know also that the length shrinks. And when there are two situations in yellow, in which only one curve goes to zero, the length of only one single curve goes to zero, and two triple junction polish. And to triple junction qualish without the vanishing of the region. In this case, we are able to prove that the curvature remains bounded. This is what we have called the type zero singularity, and the name is being suggested to us by Tomil Manet, because we wanted really to emphasize the fact that there is no blow-up of the curvature in this singularity situation. Okay, so this is actually two analytical examples. Two analytical examples. And so, since these are only topological singularities, one may wonder why we don't continue the flow of terrorist singularity. It's really natural, so the temperature is not blown up. It's true that my system makes any more sense because I have an equation that's not described anything. However, it's really reasonable to go past singularity. And I want to underline a little bit which are the main difficulty to go past singularity. To go past singularity, remaining in a PTE setting. And I also want to explain why I want to remain in a PDE setting. So consider a very simple example. Suppose that it's singular time you have four curves like in this case. When first issue, usually have more than one solution. So only by symmetry, if you have the first one, you also have the second one. And then if you also allow the Also, allow the disconnected solution. You also have this one, this two here. So, surely you have to have a definition that can deal nicely with non-uniqueness. And apart from dealing with this, we would like also to count how many solutions we have. So, we need the definition strong enough to pass singularity, but also to count multiplicity of solutions. And then, there are difficulties: the jump of topology, jump over tangent, jump over. Jump of the tangent, jump of everything. So clearly, we cannot have something that is C1 in this situation. Because surely this must be a solution. Moreover, what I want, I want that between singular times, the flow is still related to a flow, what I had before. I still want to write my system with the sum of the junction equal to zero and so on. And then I also want a little bit of continuity of the evolution of the network, as I said. Evolution of the network, as I said. Why I want this at least continuity? Because if I put weak in the definition of solution, then I don't know what happens in the yellow region. Okay, I've not told you before, but since I have my jungeon around, I cannot use maximum principle for this problem. However, if I think about this very simple setting, I can imagine to use level set formulation. Set formulation: I can suppose that I have once my set is here, possibility one. And in this case, I'm going to split and having the last of my feature. But we all know that in this case, we are going to develop fattening. The yellow part is where the critical critical set. And so, what is the problem there? But if you use level set formulation or any weak formulation. Leverset formulation or any weak formulation, we don't know what happens in the yellow region. So I want a definition solution that is strong enough with the convergence strong enough in back in time when I go to the singularity to avoid this kind of problem. So I really want to define the flow past singularity, but there's only these four things there. Enough removed. Okay, so this is my aim. And now I try to tell you what we did to have this to reach this situation. I started. I start by considering a simple setting. My initial data is only a fan of alpha lines, so everything is straight. So, in this case, it's kind of easy constructing a solution. So, if I have only as initial data like alpha lines, doesn't matter how many they are. I have done in the picture only for specificity, but not important, then there exists always at least one solution of the flow. And how is this solution? This solution is in a span. The solution is an expanding soliton. So I draw the picture in space-time. And how this expanding soliton is something that is non-compact and there's some non-compact branches that are asymptotic to these alt lines as much as you want and flows out of this guy. And it evolved by magnification. So this means that in the time slides, I simply see something at least. I simply see something that is with the same shape always, and it's coming bigger and bigger. And what is important is that, under this assumption that you evolve by magnification, the evolution reduces to an ODE. So I have a dimensional reduction. This is the ODE. It is also possible to compute the scaling factor. Maybe I've read somewhere later. Because clearly, the spongy faults are going to play. Because clearly, these banking salts are going to play a role. And for everybody, veterinos mean to butcher, vegetinos the shrinking salt don't play a role, which is not so surprising to be semio D and factories like this. And what's the point of this result? The point of the result is that the proof is purely variational, it doesn't use even the ODE. So maybe one can proof. So, maybe one can cook up a proof with usability, but it seems to be very difficult and not very general. And the main point here is that the spanding soliton are again critical point of the length in some sense because they are the critical point of length with respect to this metric. Drinking soliton in between minus, which a lot of people probably know, in very noise. And what is good about this with the negative curvature, so seems a little bit like way. So, it seems a little bit like vehicle polymetric. So, to find the solution to a critical point of length, you can basically solve a standard problem. So, find a solution, a guy of minimal length with a given boundary data. And the given boundary data is given by the direction of the alpha lines. And the solution almost all the time is more than unique, so it's not unique. And surely, all the spanning solid on our solution to be. Solid on our solution. So basically, all minimum, all critical points are a solution. So, this is a very special case, but clearly has to play a role in my description. And what is possible to group like this is that all spanding solids are solution, but it's not possible to prove like this, but there are no other solutions. We also would like to prove that there are only responding solids. So, our proof also is helpful in this case. Okay, let's start talking about the general case. Clearly, I want to come back to this straight thing in some way. And when I want to look very close, the singular thing, basically, what I usually do belong is what I'm going to do. Okay, what is the situation? The situation is that I have four simplicity, four curves, doesn't matter in general. I brought everything in this case. I expect to see a solution with five. Expect to see a solution with five curves, like I showed you before in the picture. So, what I have, I have basically that every curve gamma goes from 0, 1 times 13 to R2 times plus. I can imagine a map like this. So, I have my initial data here. But it is clearly a non-regular network. And I want to resolve this singularity. I want to resolve this singularity. And then I also have to add an extra curve. And this has to be parameterized in something, but when t is equal to zeros disappear. So my extra curve, gamma 5, is going to leave in this p, but it is 0t and x is between 0 and less root of truth, square root of 2t. Okay, so this is vector. Okay, so this is the extra curve. Okay, what's the point here? It's that the singularity, the bad point here is only the 0, 0, both in the domain and in the range. All the rest is very nice. So what I expect, I expect if I designularize with this point, I also desingularize the solution. And how I do this by blowing up for them. So this is standard also in a linear problem, but if I don't have infinite. Problem, but if I don't have infinitely many matching conditions at the corner, I can only have this move even of each equation till the corner. I have a smooth solution everywhere else, but not to the point. So, what I'm going to do now, I'm going to blow up the domain in 00 and I'm going to do it parabolically because it is somewhat similar to weight information. So, basically, what I'm going to do, I'm going to Basically, what I'm going to do, I'm going to remove the point zero zero, but it is the bad guy. And parabolic and blowing up from the metric point of view simply means simply changing coordinate. From the geometric point of view, it means that I remove the point and I insert the inward pointing parabolic normal bundle. Doesn't care, what I care today is the coordinates. So, what does this mean? But I've changed the local coordinate. But I have changed the local coordinates around this point and I standardly use parabolic border coordinates. So, what does this mean? When I have done my block, my new space has to be with a natural topology and the smooth structure in which the new coordinate and the function lifted there are smooth. It's true that working with polar coordinates is not that nice because I have sinuses. Because I have signs around. So, for the computation, it's much more convenient working with projective coordinates that I'm going to insert now. So, I can do computation in this case. So, I insert my projective coordinate around here. But before this, let's focus on the fact that here we had something that at the corner and at the boundary. In my new version, I n I have an extra phase, an extra boundary phase, an extra point. An extra point. This extra face is my front face and is defined by rho equals zero. And when the omega varies from zero to pi over two, when I select my bottom face, that was the boundary face where I had my initial data, it's time it falls to zero. So when you coordinate this w is equal to zero, and then I have still the other die here that was x equal to zero. And this in my new. In and this in my new coordinates w equal to chi over two. So, what I had here, yeah, the initial datum that I still impose here, and here I had my add-in condition, based on the fact that the curves were attaching and they had some of the unit tangent vector equal to zero. This condition here and here is going to remain. So, since these coordinates are not very nice for the computation, I'm going to use this kind of our coordinate, but still keep in mind. R coordinate, but still keep in mind the fact that I have the time, so I scale differently the time in the space. So these are the projected coordinates, and this projective coordinate are nice over the front face. TO equal to zero is defining the front face. However, they are not nice in this point because I have both s and two that are going to zero. So I have basically s that goes from zero to infinity. And in this corner point, I need another. Corner point, I need another set of coordinates. So we have three different sets of coordinates, absent usual one everywhere, corrupt coordinates here, and projective coordinate here, and this other set of coordinates at the corner here that can describe the entire thing with the computation. Okay, so this is the domain. I have explained very in detail the domain because the range is basically the same. What I still keep in mind is. Same, what I still keep in mind is the fact that I have a parabolic equation, and what I have also to keep in mind, but again, zero zero zero is going to be the only problem. So, I do again my parabolic blow-up. So, basically, I put the spherical coordinates, so I again paraboloid here, and again, I add a new front face. So, in some sense, the geometry of the space is going to be more complicated, but the PD is going to be much more simpler. This is the general idea. More simpler. This is the general idea. Also, here to describe the entire fiquid smooth field, I will need three sets of coordinates if I don't want to use spherical coordinates. And today I'm going to focus only on the coordinates that I put on the front face. The other ones are similar to the market. And what I do now is that I want to leave my marker on this face, both on the domain and the range. And then I want to write the PD on these new spaces and see if I'm able to select it. Spaces and see if I'm able to filter it. So I have my couple d and gamma that is to be lifted in something like this when I change the coordinate. And I think what I want to stress is the fact that 2 equal to 0 is the defining function for the front base. And in the new coordinate, 2 equal to 0 can be interpreted like where I put the initial data. So I lift everything. So I take my equation, which is quite a nice equation pair, and I change coordinate both in the range. And I change coordinate both in the range and the domain, and basically if I change the coordinates, I end up with this equation. This is my motion equation for every curve. And if I want to solve my P problem, I don't need only my motion equation. I also need boundary data, initial data. I told you that if I block this point here and I end up in something like this, over here it was still reasonable to have a varying function. Reasonable to have a varying condition. So I still have here I had that gamma one was equal to gamma two was equal to gamma three. Here I would have that gamma one eta one and also that sum of a unit tangent vector and this is obtained simply putting the condition again and the sum of a unit tangent vector is the same or simple unit coordinate. What is missing to write my PDF What is missing to write my PDE and to solve the Cauchy Direction problem? The fact that I also need an initial data. The initial data is a little bit less simpler to compute because to equal to zero seems to be problematic. However, one realized that to to of eta x time equals zero x equals zero, it is zero. So the initial data has to solve this equation here. As to solve this equation here. So there was in the original equation a par form with toe the top that is zero. And so the initial data has to solve this equation here. And surprise, surprise, what is the solution of this gal here? This is now it's an OD. I have only an S, I don't have any more time. And this is nothing but the spander equation in the new coordinate system. So this means that I know what I'm going to see on the front phase. On the front phase, I'm going to see spanders. Front phase, I'm going to see spanders, and this was the play of the game. So, we have done all this thing exactly to see this. So, what I showed you till now is basically the asymptotic expansion of all this thing. It's not the end of the paper, clearly. One has really been to prove that some like this exists. First of all, I know that a spanner exists from my variational point of view, if we prove. So, something where I can put an initial data, and when I have to. Data and then I have to prove that the solution of this expansion exists up to an error term. And then the error term has to be rapidly vanishing. And when I blow down, I can define my solution. And then, okay, we fixed the point argument. I can redo, but I have a real solution, not an approximate one, because this is approximate. And this is the story. And so, what I have now is a good definition of the block of singularity and also the existence. What's the main point here? What's the main point here is that now in the Bronach space, the number of curves that I see at the singular time a little bit later on is exactly the same. So I can write the KPD system with an equation of each curve. And so this is going to be my solution for singularity. And then I can also define what is something that is continuous setting in the original space by blowing down. There is a natural blowing down map. So I can suppose that my able. So I can suppose that my evolution in my blow-up space converge as much as I like to initial data, as much as regular requirements, but not suppose infinity. We want the derivative. And like this, we have finally the flow plus singularity. What's the main point of the story is that in this description, the expander come out naturally. There are really the initial data they are, or if you look at the expansion of the terrier series, there are zero-order terierosides. Terrier serial, there are zero-order terrier serials. So it cannot be, there can be only expanders, nothing else. So this means that I have as many solutions, as many expanders. So with the definition of solution, I have existing solution, and I have also a complete result about multiplicity of solutions. So I can see all the solutions that I have microlocally and a spander that is compatible with the direction of a unit tangent vector at a singular point. At the point. Okay, so this is what we are able to prove with all of this where I want to go. So clearly, this was the first step to go to a long time behavior. Describe what happens later on. And it's also the first step to having this very precise multiplicity of solution and count your solution, classification of solution, to have like some claim towards this coarsening phenomenon. Since now I know that the collection of solutions, the set of solutions that come after singularity is one-to-one with a sector spander. I don't know how many they are. And I can also have some subsequence result. First of all, if I have a singularity in which only five at least at most five curves concur, so a singularity of three, four or five curves. Three four or five curves when all possible spander have no loops. So, by a basic Euler characteristic computation, I can compute how the evolution simplifies the topology for the singularity. Because I have something with a loop that disappeared, I have something without a loop that come out of the solution, and I can com compute that the total number of curves decreases at least by three, and the total number of a triple junction decreases at least by two. Triple junction, it is at least by two. So, this is a very strong indication of a simplification to singularity. Again, this is only an indication, it's not a complete proof, because I'm still not able to prove that at singularity only so I concur. So, it's very reasonable why I've told because I've told to you that when I have disappearing of the region, this means that the loop disappears, and the loop will disappear can have at most five curves. However, it's possible that a lot of However, it's possible that a lot of loops, cluster of loops, disappear altogether. So, this is something that one has to prove that it's not happening. Then, the main point number two of my claim of parsing is only formal computation that I want to show you. Then I also want to explain why it's only formal and why it are the difficulty to pass through a complete theorem. So, let's suppose that we are, let's say, of the torus or something like this with a periodic boundary condition. With a periodic boundary condition. Let's suppose that the fundamental domain is the square of one and one. Then I can put n square grains, so a lot of loops. And if I put n square grains, then the total length of my network is a border n. The average area of each cell is a border one of n square, and the average length of each loop is a border one over n. So already here we can see that this is formal because formal because. This is formal because formal because doesn't exist by average grain, however, let's go on. Then, along each group, I have this inequality that I've shown you before that is done combining Gauss-Bondituram and Goldenequality. And the constant C is different from zero when I don't have hexagonal cells. So, anytime that I have something that is not an hexagonal, the C is different from zero. So, this means that till the So, this means that till the number of hexagon is still of order n square, like the number of grain at the very beginning, I can pass from an inequality on a single loop to an inequality on the entire network. Because I simply do an inequality on a single loop times the number of grains. So, I end up with C divided 1 over n times n. So, c times n q. To close my formal computation, I use the gradient for structure. It's very simple to compute. It's very simple to compute the derivative of the length. It's the gradient flow, so we have done this on purpose. So it's minus the kappa square, and the length is over the n. So I can put everything together and I obtain a differential inequality in which I have T of n less than equal to a constant, which is positive, times NQ. And then I solve the and I end up with the fact that every End up with the fact that the average area of the surviving grains grows linearly. So some grains are going to disappear, but the one that remains, I have an average area that grows linearly like this. What is also an hypothesis on all this thing, the fact that I send the number of grain to infinity for very short dust. So basically I'm starting from a dust, really, really a lot of grain. Okay, so what Okay, so what's the main difficulty here is to have track of a percentage of a non-hexagonal cell during the evolution. So why is this so difficult? Well, I show you a very simple example of a singularity, but is what expect to be the general singularity. So two triple junctions equalish, one length disappears, like in this case, then you restart the flow, and you know what to do. In particular, In particular, I can go from here to here, C1, so I know exactly the angle that I'm going to see here, and I'm going to see angles that are 120 and 60 degree. In such a case, I have only one expander that comes after, close out. So this seems to be extremely clear, extremely simple. However, what happens around of this is completely unclear. For example, I can have six hexagons. I can have six hexagons. So again, here in between, I exactly have the same singularity. When I have my singularity, when I restart the flow, and I end up with two pentagon, and two hectagon. But with the very same local picture, I can have two hexangon, one pentagon, and one heptagon, and again, I end up with this. So you see that from the local description, I have no information of the global one. So it's very, very difficult to keep track of the number of an exact. Of a number of the exact answer. And then finally, there is another difficulty that maybe turns out to be an advantage. Let's think about it together. So I know that I have steady state. Which are the steady state? Network with triple junction, straight segment. So basically, the hexagonal partition is a steady state, doesn't move because it's average straight and not bringing coupons to the junction. So the question is. So the question is, are there also attractors and how big is there basing on attraction? Yes, we are also attractors. Yeah, how it is expected. So this is our result with one percent. We are able to prove that if we have a network that is starting close to a network with a triple junction and straight segment, then the flex is for all time, very close, it should close. Closer, it should close, and this guy is going to converge to any infinity, a network that has the exact same length and topology of initial one. So, it's really true that they are a structure and this is stability result. What is improvable about this stability result is the fact that IRF H norm. I have this because we have done this result with Simon inequality and we needed this kind of space, but one would like to have. But one would like to have L infinity or L1 there. And also the fact that the initial data has to be close to a network of only triple changes. These are two, if you want, weak part of the segment. And this seems completely to contradict all my theory about worsening because it's exactly the opposite result. However, we are able to prove some bound on the basin of attraction this guy. So, what is the stability result in the network flow? It is exactly the same version, the evolution of the local minimality result of critical points for the length function and the static problem. What can we say about this? Okay, suppose that your n star is a net of multiple junction, straight segment, so it's all a local minimizer of the n function, and it's not only a critical point, it's a real minimizer. And suppose that d And suppose that d is the length of this short range. Well, with a calibration argument, we are able to say how big it is a delta neighborhood in which all the other network that live in this delta neighborhood are going to have a length that is greater or equal than the critical point. So we are able to compute how big is the neighborhood in which this guy is the global minimizer. And how big is this thing? Is this thing some effects that slowly minimize trivial? So there is to prove what is nice about our approved by calibration that we have this boundary, but we know how big is the basin. And what's my main point here that our basin is depending on the length of the shorter edge? So this means that the more complex is the network, more edges is going to have, and so much shorter and shorter become our delta, smaller and smaller become our delta. And so this is our main indication of effect. And so, this is our main indication of the fact that the basing of critical points, the wire complexity, must be much smaller than the basing of a structural critical point with bigger complexity. So, putting all together, what are my three main points towards this coarsening phenomena? The fact that in a lot of situations, topological complexity is not decreasing to singularity of the one that we expect to have in reality. The fact that I have a formal computation about the I have the formal computation about that the areas of venging grain is growing linearly, so bigger grains are going to be smaller ones, and the fact that I have an estimate of the volume of what's happening performance. So with all of this, I have understood that the theorem cannot be always true. I cannot always say that I arrived in one single green. So I surely have all the casing in which I have hexagonal partition to take into account. So what I propose is So, what I propose as a possible result is only this one that has a probabilistic interpretation. I think one cannot do better. So, suppose that you choose randomly your endpoint in R2 and you construct the initial data of the network flow by taking the Voronoi partition associated to this given point. So, in a casual way. What I expect to see, and this is only an idea, is very far from being true, is the fact that there are To be improved is the fact that there are going to be exist at 5 that depends on n, and it is negligible with respect to n, such that the probability that the limit network has more than this Ï€ cell has to go to 01. So clearly, this is only a possible research direction. I would really like to prove this or to collect more information this direction.