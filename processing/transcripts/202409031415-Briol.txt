The organizers, and I'm very sorry that I can't be there in person. So, the topic of my talk today is going to be, as the title says, Gaussian process regression. But there's going to be a small twist on Gaussian processes in the sense that I'm going to focus specifically on what happens if the model is misspecified. And more specifically, I'm going to look at cases where. I'm going to look at cases where you assume a certain likelihood, which happens to not hold because you have extreme observations. And I'm going to look at how to make Gaussian processes robust in those kinds of settings while still maintaining some of the kind of computational convenience of Gaussian processes, namely the conjugacy. And so the work is mostly the work of my PhD student, Matthias, who is co-supervised with my colleague. Who is co-supervised with my colleague Jeremiah here on the right-hand side? Okay, so let me get started. So, we've already seen quite a few talks about regression. So, just to kind of settle some notation, so what do I mean by Gaussian process regression? Well, typically in regression, you have some covariates, which are going to be my xi's, and some responses, which are my yi's. And these responses are just evaluations of the unknown function at the covariates up to some additive noise. additive noise okay and so when i say gaussian process regression what i mean is i'm going to place a gaussian process prior on f and i'm going to use a normal zero sigma squared model for my epsilon i's now of course you can do gaussian process with other kind of observation models but typically people tend to use this normal one because you get some computational advantages in doing so namely you can get a closed form estimate a close form Can get a closed form estimate, a closed form expression for the posterior, and this posterior stays again Gaussian. Okay, so yeah, why do people like Gaussian processes? Well, firstly, they're like a very flexible and interpretable model class. So to fully determine a Gaussian process, I need to pick a mean function m and a covariance function k. And if I know about certain properties that I expect my true Certain properties that I expect my true function f to have, let's say some smoothness properties, some periodicity, sparsity, et cetera, what I can do is I can directly go and embed these properties by selecting my m and my k appropriately. And so that makes them kind of fairly nice. I guess another reason that people might like Gaussian processes is because you can kind of embed them in the whole kind of Bayesian framework. So you can get an entire posterior on the function f. Posterior on the function f. And so you have kind of epistemic uncertainty. But more importantly, this posterior is available in closed form typically. So if my likelihood is Gaussian, then because I have a Gaussian prior, Gaussian likelihood, if I go back to Bayesian stats 101, I'm going to get a Gaussian posterior. And here, the same thing holds, except we don't have univariate Gaussians, but a whole stochastic process, but with Gaussian marginals. Gaussian marginals. And what's nice about this is that basically, you know, as I get more and more observations, the only thing I need to keep track of is what's my mean and what's my covariance. And I have a closed form formula for how to update that. So for these many reasons, people tend to really like Gaussian processes. Yeah, so let's have a look at what happens if I kind of apply a Gaussian process to a very simple problem. So this is a toy problem where This is a toy problem where I have a very nice smooth function, which is given by this dotted line, and I observe some data, which are these black dots, which is the function up to some additive noise. Now, if I fit a Gaussian process and I look just at the posterior mean, I'm going to get something like this. And, you know, the GP is nice, it's flexible, it's good for modeling smooth functions. And also for things that are highly nonlinear. So, here, you know, as we'd expect, the fit is very good. All right. So, that's typically the sort of plots you'd see in Gaussian processes talks. But really, the sort of settings we should sometimes consider are settings exactly like this one, where here I've got the same data being plotted, except I'm injecting some extra data points, which are these red dots. Extra data points, which are these red dots, red crosses, sorry. And these are observations which the kind of statistician might not know about, or they might not know that they're kind of different from the others. And here I'm going to call them outliers because they're quite extreme relative to kind of the rest of the data. Now, of course, in this kind of example, it's really obvious what the outliers are. Well, one, I've kind of highlighted them in red. In red, and also they're really extreme. So, you know, I do my one-dimensional plots and you can spot them. But, really, in more than one dimension, it might not always be kind of trivial to kind of identify these outliers. And so, these sorts of situations can be kind of what you face with real unfiltered data. So, you might have outliers because your measurement instrument is broken. It could be simply because your kind of observation model is Observation model is really far from Gaussian, maybe has some heavy tails. So, this sort of thing can happen. And so, we were kind of interested in what happens in this setting with Gaussian processes. Now, if I just use the exact same Gaussian process that I used in the kind of two slides ago, this is the fit that I get for my posterior mean for the Gaussian process. And as you can tell, it's pretty bad. So, what's happening here is that just happening here is that just a few kind of outliers are really dragging down the posterior mean um and there's there's very few outliers relative to the kind of majority of the data but this is still enough to kind of uh make the kind of uh posterior mean bad and therefore potentially my future predictions bad um and and really the problem here is that my assumption that the the noise is normal zero sigma squared is actually wrong zero sigma squared is actually wrong, but I'm assuming that it's correct and I'm just going ahead and using GP regression. So really what I'd like is something more like this. So this blue line, which we call robust and contribute GP, which is the method that I'll kind of propose in this talk. And as you can see, does much better in terms of like prediction and is able to not be completely like dragged down by a few outliers. Outliers. So, the kind of whole point of this talk is going to be how do I get this blue line? And here, such a blue line I would call robust to this kind of setting of outliers. In particular, what I'm going to be looking for is a way of getting this blue line in a computation efficient way. And so, that will be the kind of two topics of this talk: that's the kind of computational side and the robustness side. Okay. Okay. Right, so before I do that, I should probably kind of dwell into the literature, look at what's been done before. So firstly, if I kind of just type robust scouting process in Google, I'm going to find 100 million papers. So there's been loads and loads of different algorithms that have been proposed here. Some just for regression, some specialize to certain applications of Gaussian processes. So, one example is So, one example is vision optimization. So, that's a kind of very popular application. But let me try to kind of summarize the main kind of trends in this literature. So, trend one is basically, well, we know that the model for the observations is wrong. So, we're just going to try to get a better model, right? So, if we think we might get extreme observations, what we could do is just have a heavy-tailed model. Have a heavy-tailed model. So, you could use like a student T or a Laplace or something like that. And then, hopefully, that's a better model for your noise, and therefore, kind of your GP should give you a better fit. And then the second class of approaches is kind of outlier detection and removal. So, it's basically doing in a kind of automated way what you could have done by looking at my plot, looking at which ones were red, removing them, and then fitting your standard GP. them and then fitting your standard GP. So I'll mostly focus on the first class of models of methods in this talk, mostly because the second class you can always kind of use in combination with the first one. So you can always remove outliers and then fit a more complex model to the rest. Okay, so if I focus on this first class, well, the main problem with this class of algorithm is that it makes Gaussian processes very slow. And obviously, Slow. And obviously, if you've ever used a Gaussian process, you already know that the Gaussian process can be quite slow to start with. And here you're just making your life a lot harder, mostly because you've broken conjugacy. So recall when I started, I said if you start with a Gaussian prior and then use a Gaussian likelihood, you get a Gaussian posterior. Great. But now you're changing your likelihood. And so Gaussian plus something else might not be anymore a Gaussian. Be anymore a Gaussian posterior. And so, what this means is that you need some sort of approximate method to approximate this posterior. And this is going to typically require quite a bit of computation relative to having something close for. So various of these algorithms use something like MTMC, Laplace, variational base. So they have like varying costs depending on how good their approximations are. Their approximations are, but yeah, they're all slower, and some of them are kind of slower, and also they give you better approximations. So that's not so good. And so our goal was really to get something robust and fast. Okay, so let me just kind of illustrate this. So here in this table, I've got the time and second that it takes to fit GP or various extensions of GP. GP or various extensions of GPs to kind of some of these UCI data sets. And so there's four different data sets of varying size and dimensions. And so I'm just giving you the kind of time and seconds to fit the GP and all of the hyperparameters. I've not really talked about that and I'll get back to it later. But of course, there's typically some hyperparameters you need to pick. So GP is just a standard GP. TGP is a GP with a student T observation. With a student T observation model, MGP is GP with a mixture of Gaussian likelihood. And what you see is that what happens, just to pick out the worst case scenario on here, is that fitting a TGP, so a GP with student T likelihood, can take up, I don't know, let's say 15 times more than. More than using the standard GP, and that's purely because we've broken conjugacy, and so we've had to use one of these approximate methods to approximate the posterior. Now, I don't remember exactly which one this uses, so it uses one of these kind of standard GP packages that you find for Python. And we've just used kind of the default method they've got, and this is the time that it returns for the GP and the TGP. So you could have a higher time. So, you could have a higher time or smaller time depending on which of these approximations you use, but the point is it's expensive. Okay. Now, I would tend to argue that for this reason, a lot of practitioners are quite naughty. And so they won't really tell you this. But what they tend to do is they tend to use Gaussian processes, assuming a Gaussian likelihood, even though they know this is not really true or this is not really correct. And they do this partly for. And they do this partly for computational convenience, but also because these other algorithms become a lot more complicated and perhaps fiddly. But actually, this is not really something new. So if you go back far enough, you know, all the way back to the kind of start of the field of robust statistics, you have this paper by Huber, which has these kind of interesting quotes, which I've kind of relayed on this slide because I thought they were particularly relevant here. So here he's talking. Relevant here. So, here he's talking about not Gaussian processes but univariate Gaussians. And he says the following: Gauss was fully aware that his main reason for assuming an underlying normal distribution was mathematical, i.e., computational convenience. So he already kind of alluding to the computational advantages of Gaussianity. And then the next quote is, this raises a question which could have been asked by Gauss, what happens if the true distribution deviates slightly from the assumed Deviates slightly from the assumed normal one. And this is exactly the problem we have. So, you know, we're kind of quite a while later. Now we're not looking at univariate Gaussians, but we're looking at Gaussian processes. And this is exactly what's happening. So people are using Gaussian assumptions just for convenience rather than because they think it kind of holds. And perhaps we should be doing something slightly different to make sure that that's not potentially leading to kind of dramatic consequences. So, what I'll present is essentially this paper with the kind of same title as the talk. So, let me kind of start with that. So, just to kind of settle some notation, here's just Bayesian inference for GPs. So I start with a prion f and so here I'm kind of vectorizing everything. So, all of these bold things are just function evaluations at all of the x coordinates. x coordinates so x1 to xn bold x is just a vector of x's and bold y is the vector of y's so here's this side is mostly i'm sure most of you know what bayesian inference is but it's just a settlement um so i start with this prior i update with my likelihood and i get a posterior now what we're going to do is actually deviate from this approach and look at generalized bayesian inference so in So in generalized Bayesian inference, you still start with a prior, but now you're going to update your beliefs using something else than the likelihood. So you're going to use a loss function that encodes in some way your beliefs, but it doesn't necessarily have to be a likelihood. So typically you update with exponential of minus some loss function just to kind of ensure that things stay positive. And depending on how I choose this loss function, I'm going to get. Function, I'm going to get hopefully a reasonable generalized posterior, which will behave very similarly or in some ways similarly to a standard posterior. In the sense that if I'm careful with how I pick this loss, I might be able to kind of embed some of the same semantics we typically have for posteriors into these generalized posteriors. Okay, so things like quantifying uncertainty. Again, I need there's a lot of caveats, and really this choice of plus is important, but this is the sort. This choice of loss is important, but this is the sort of aim that we have. So, a new distribution representing uncertainty, which may have some other properties which you inherit from this loss function. Okay, so why is it called generalized base? Well, simply, if I take the loss to be minus one over n the log likelihood, then I recover standard base. So, okay, I have a special case. So, here now it's a generalization. And we know that. And we know that this special case is the best way to update beliefs if the model is well specified, i.e., in our case, if the kind of observation nose is really normal zero sigma squared. But obviously, you know, I've just given you some examples where that might not be the case. And so now, you know, if we update again with the likelihood knowing the model is wrong, actually, we lose anyway all of these kind of notions of quantifying uncertainty, et cetera. Certainty, etc. And so a key question is: well, in these settings where our model might not be well specified, how should we choose this loss function? So there's been quite a lot of work in this kind of topic. I'm sure many of you in the room have also kind of thought about this. So I'm not really going to reash all of that, but I'm just going to kind of highlight some of the design choices that people make for this loss function. Make for this loss function. So, typically, the loss function is chosen if we care about robustness in order to kind of induce some form of robustness to mild model misspecification. So, obviously, like if the model is completely wrong, then you know, whatever loss you pick, you're not going to get something accessible. But if your model is just a little bit wrong, for example, if you've got, you know, data which is a tiny bit corrupted by outliers, like in my example, then you want something where your posterior still remains sensitive. Still remains sensible. And typically, people choose some form of statistical divergence between an empirical measure made out of the data and their model for the likelihood. And that's what they use for this loss function. So there's been lots of work on that. So examples of divergence that can lead to robustness, there's beta, alpha, there's the MND and the KSD. And I think we've talked about those a little bit already. Think we've talked about those a little bit already today. Okay, so in this talk, what I'm going to do is I'm going to assume that you're happy with me using generalized space, and I'm going to choose a loss function which will serve two kind of, well, which will lead to two things. One is a computationally convenient generalized posterior. I.e. I'm going to try to get something that maintains conjugacy, which was a kind of key property of GP. Which was a kind of key property of GPs, whilst also giving us some robustness. That will be the kind of design choice for my loss function. Okay, so to do that, I'm going to introduce a statistical divergence called the score matching divergence. So it has several names in the literature. Some of you might know it as the Fisher divergence or the Iverinan divergence. And basically, what this divergence does is it looks at PQ. Is it looks at p and q, looks at their densities, takes the gradient of the log density, i.e., the score function, then compares that of p and of q. So you take the score of p, the score of q, and you say, how close are these two things? So if you look at this plot here on the bottom left-hand side, sp is basically the score of p, and sq is the score of q. And you can see they kind of deviate a little bit. And so by how much they deviate tells me how different. By how much they deviate, it tells me how different p is from q. Um, now here it's nice I can look at this plot, but obviously, ideally for divergence, I need like a number. So I want to have a value that tells me how far is p from q. So what you do to get this divergence is you just look at this score and you square the difference and integrate over the space, and then you get a fixed number. And so that's the score matching divergence. Now, Now, some of you that work more maybe on statistics might know about the E-Varenan scoring rule. So, this is essentially the statistical divergence that you get from the E-Varinand scoring rule. If you're more from the machine learning world, probably you've heard of score matching in the context of diffusion models. So it's basically the tool that's being used to fit diffusion models. But here we're going to use it for generalized base. Here we're going to use it for generalized base. Okay, now we're not going to use exactly score matching, we're going to use a slightly extended version of score matching, which is going to have this weight, which is here in blue. And you can think of the weight as basically emphasizing certain regions of the space where I want my scores to match more. So the weight here could be kind of let's say high near zero and low near minus. And low near minus 10 and 10. And so this would essentially emphasize the region near the middle and say, I want the scores to match well, particularly well near the middle. Now, here it might not be super interpretable, but very soon you'll see that this weight will be really critical. So this will be the thing that allows me to get this robustness. And so I'll talk a lot about this weight as we continue. Now, here I'm working with Now, here I'm working with a kind of regression problem. So, I don't have just two marginal distributions. I'm actually going to have to work with conditional distributions. And so, what I'm going to do is use a slightly extended version of the score matching divergence, which is specific for conditional distributions. So, here you should think of P as my kind of Gaussian likelihood. And all this kind of extended version is doing is saying, well, we're looking at conditions. Doing is saying, well, I'm looking at conditional distributions. So to get a number, I'm going to integrate over the conditioning variable as well. And that's it. Okay, so hopefully you're still with me. So what I've got so far is a statistical divergence, i.e. a way of measuring the distance, distance, not in the sense of metric, but of statistical divergence between P and Q, so two distributions, in terms of their score function. In terms of their score function, and it's weighted by some weight w. Now, there is like one step, which is a bit of a fast step that I'm doing, which if you know about score matching will be trivial. If you don't, you'll just have to trust me on this. Basically, what you can do is do integration by part on the inner integral. And what you end up with, if you do this, is basically. Is basically that the expression splits into two terms, one which essentially doesn't depend at all on P and one which does. So I'm going to write this the one that doesn't as a constant C. And the other one actually ends up just being an expectation. So you can just approximate it with a Monte Carlo estimator, meaning that if I evaluate the score matching divergence between P and an empirical measure made out. P and an empirical measure made out of data, I'm going to end up with exactly this expression. Now, P is my likelihood. Okay, so I've talked about statistical divergence, it has some weights, and I'm going to use it within GenBays. So let's see what happens if I use it within GenBays. Well, recall we're working with Gaussian processes. So I have a Gaussian process prime F, and I have a normal model for epsilon, my additive noise. My additive noise. Now, if I look at the formulas for Gaussian processes, I get a closed-form posterior mean and covariance purely free conjugacy. And these are the expressions that I get. So here, big K is the gram matrix. So it's the matrix of evaluations of the covariance as pairs of points. IN is just an identity matrix. And M is the evaluations of the prime mean function. Evaluations of the prime mean function. Okay, so these are very well-known formula that you probably see in every standard GP talk. Now, if instead of doing standard base, I go to generalized base and I start using score matching, what I end up with is actually something very similar. So if you look at these formulas, they look pretty much identical, except my Pasira now just has a slightly different Pasera mean as Pasira covariant. posterior mean and posterior covariance. And the difference is basically highlighted in blue here. So what happens is that there's this kind of identity term, which typically appears in the standard GPU, which is now replaced by a diagonal matrix where the weight appears. And what's happening here is that I'm essentially placing a different weight on each of the observations. Okay. The other thing that happens is this term, which was just a prime mean before, now becomes the prime plus a term that depends on the weight function again. But apart from that, all of the formulas are the same. So already here, you can see I've kept conjugacy, it's just in a slightly different form. I now have a slightly more flexible method, which has this weight function w, which I haven't told you anything about. But at least you should be convinced. But at least you should be convinced that implementing the right-hand side or the left-hand side will be roughly the same computational cost. So here we've got conjugacy and we've got this is what gives us this kind of computational efficiency of the method. Okay, so here I've just kind of replaced these formulas here just for the RCGP posterior. So let's think about what happens for different weight functions. So if I choose my weight function to be a constant, function to be a constant here a very specific constant so that everything matches well then I just recover standard GPs okay so here what I've shown is that in this case you know I have a generalizations of GPs if you want which is not that surprising given I'm doing generalized base okay so I recovered standard GPs now if I make the weight depend on X but not on Y what I recover On y, what I recover is iteroscedastic GVs. So that's Gaussian processes where your likelihood has a variance term which might vary with the covariates. So that's kind of nice. But here what we're going to do is we're going to use a w which has both x and y, in which case it's not any of the standard GPs that you're used to. And what we're going to do in particular is we're going to choose the weight function to induce some robustness, right? I promised you. Robustness, right? I promised you robustness and conjugacy. I've delivered conjugacy so far. And so I need to give you robustness. So the sort of weights that we're going to use are like this. If you're a kernel person, you'll recognize that this is basically an inverse multi-project kernel evaluated as y and minus m of x, sorry, y and m of x, m of x being the prior mean. m of x being the prior mean. If you're not, maybe you can get some intuition from this figure. Basically, what's happening is that my weight is being decreased as I go further away from my prior mean. So if I think about this just intuitively, I'm saying if I get observations which are really, really far from my prior mean, I'm just going to downweight them so that they impact my posterior less. Okay. Now, how Okay, now how much I downweight them typically depends on this hyperparameter C, and I'll talk about that in a moment. But here, the blue curve is essentially this weight function. Now, we could have used any other kind of bump. So we could have used something that looks a bit more like a Gaussian. But really, what we wanted was something which, where, you know, at the tails, you go, you kind of decrease, but not too much because we don't want. Decrease, but not too much because we don't want to assign zero weight to extreme observations, because otherwise it'd be kind of a bit too extreme. We might actually ignore some points that we should actually care about. Yeah, so downweight the extreme observations, but not too much. And so here, if I go back to my initial plot, basically what this is doing is it's all these red dots, I'm placing low weights on them, and all of the black dots, I'm placing high weights on them. And that's how. Weights on them, and that's how I'm getting this kind of nice blue curve that I had at the very start. Okay, so I've kind of given you some intuition as to why this might lead to some form of robustness, but really I should kind of convince you a bit more seriously that this will actually be the case. So, what I'm going to do is I'm going to talk about how we can prove some form of robustness. And so, I need some sort of measure of Need some sort of measure of the robustness of a posterior. And what I'm going to use is what's called the posterior influence function. So the posterior inference function is essentially something very similar to the inference function. So if there's some frequentists in the rooms, I guess not that many, you might know what the inference function is. But the inference function, you can think of it as a way of measuring how much, let's say, one data point could really impact. Say one data point could really impact an estimator, or in this case, one data point could impact a posterior. So, what I'm looking at is some posterior distribution, so the posterior on F given the data D, and I'm going to compare that posterior with the same posterior, except I'm allowed to take one of the y values and vary it. So I'm going to look at the distance between the original posterior and the posterior where I'm varying one point. Varying one point. And this is going to be called my posterior inference function, and it takes in that one point. Okay. Now, the distance here I'm using is the KL divergence just to measure kind of the distance between the posteriors. And typically in the kind of generalized phase world, what you would say is that a generalized posterior is outlier robust if this posterior influence function is bounded. What this means is that there's only so much that your posterior will change, even if. That your posterior will change even if you make one observation super extreme. So, if I think about it in terms of maybe the figure that I have here, I could take one of these red dots and throw it to infinity, and I would want my posterior to not be impacted arbitrarily badly. Okay, so the theorem that we have in the paper is actually quite a bit more general than the one on this slide, but I've stated it for simplicity. I've stated it for simplicity like this. So, here I'm just stating in terms of the weight function that we actually use. In the paper, it's you can have many other weight functions and the properties are quite mild. But anyway, so for this weight function that I've told you about, then I can show you that RCGPs are robust in the sense that the supremum of this posterior inference function is actually bounded. Now, Now, this is not the case for standard GPs. It's really trivial to show. It should be also kind of intuitively obvious for anyone that's worked with GPs. Here, I've got a plot of these posterior inference functions for the GP and the RCGP, and you can see what happens. So basically, if I YM is the original point, and I'm starting to move it by changing the value to Ym C, and you can see that if I make YMC far from Ym, the percent. Far from ym, the posterior inference function for the standard GP just blows off to infinity, whereas the RCGP1 does not. And that's purely because we've chosen one of these weights, which essentially downweights extreme observations. So we're kind of stopping that from happening by design. Okay, so at this point, what I've kind of told you about is a way of doing Gaussian processes with generalized base. I've shown you that. I've shown you that if I'm clever in how I choose my loss function, what I can get is something which is conjugate. So I keep this conjugacy, which is really nice on a computational side. But also now I've shown you that you can also choose it so as to get both conjugacy and robustness. So the only thing that I haven't really told you about is hyperparameters. So there's a bunch of hyperparameters like this constant C, or in fact, all of the hyperparameters of the Gaussian. In fact, all of the hyperparameters of the Gaussian process, let's say that could be prior mean hyperparameters or kind of covariance function hyperparameters. And so I need a way of choosing those. Now, typically when people kind of rely heavily on conjugacy for GPs, what they do is they use empirical Bayes. So if they were like, you know, very serious Bayesians, what they would do is put a hyper prior and then look at, you know, what this implies, marginalize over, et cetera, et cetera. Lies over, etc. etc. But this typically makes you lose conjugacy. So, what you do instead is you just maximize the marginal likelihood. And that's great. But of course, it doesn't really make sense in our setting because in our setting, we've kind of agreed that we're not well specified, i.e. the likelihood is wrong. And so I don't really want to use the marginal likelihood to kind of fit my hyperparameters. kind of fit my hyperparameters. So instead what I'm going to do is I'm going to use cross-validation and I'm going to use cross-validation with the predictive that I get from generalized base. So this PW, maybe it's kind of bad notation, but it means the predictive that I get with my posterior GP, which is generalized, the robust one. Now, the nice thing about this is that in the same Is that in the same way as you can kind of get very efficient computation for the one-out cross-validation for standard GPs, you can get that for these robust GPs? And so, in particular, with a bunch of like clever linear algebra metrics, I can essentially do gradient-based optimization at a not unreasonable cost. That's kind of nice. And in particular, the fact that I'm using the predictive from the generalized posterior. From the generalized posterior ensures that I'm not impacted too badly by outliers and things like this. Okay, so let's now have a look at performance. So my kind of methodology part is over. Let's look at how it works in practice. So I'm back on these kind of four data sets. And now the numbers that I'm reporting are the mean absolute error. So, if you compare GPs and RCGPs, you actually find that their performance is essentially identical. So, basically, in the well-specified setting where we've not got any outliers, RCGPs does equally as good as TPs. And in fact, so does the TGP. So, the TGP is doing about the same, but remember, it's much more expensive, right? Okay, so that's good news. So, at least in this kind of well-specified setting, it's not like. In this kind of well-specified setting, it's not like we're paying a huge cost for being robust. However, what I'm going to do now is I'm going to start introducing some outliers. And so I have two scenarios. There's this focus scenario where the outliers are clustered in both X and Y. And then this asymmetric outliers, which is basically saying my noise is only one-sided and heavy tails. Now, obviously, like in this kind of toy plane. Obviously, like in this kind of toy plot, you can spot again straight away what the outliers are. But here, the data sets are maybe up to like 15 dimensions. And so it's not so easy to go and pick what the outliers are. Now, what you notice here is that in both settings, the RCGP does really good, but it certainly does way, way better than the standard GP, which is not robust, so we're not that surprised. It also tends to do way better than the TGP, so the GP were student T likelihood. So the GP were student T likelihood, mostly because these settings, well, they're not very well modeled by a student T, and so purely here because the data is one-sided, you know, the student T isn't going to be very good. And you can see that map out in these mean map series errors. In some of the settings, the kind of mixture of GP, sorry, the mixture of normal likelihood for Mixture of normal likelihood for this GP does tend to do quite well or similar to us, and that's mostly because, actually, for this asymmetric setting, that is actually the data generating process. So, we did use a mixture of Gaussians to kind of generate the data. Okay, so that's good news. Now, here I'm back to kind of computational cost, and I've added one column. So, for this RCGP, and what you can see is that. And what you can see is that, so here I have seconds again for fitting the GP and all the hyperparameters. And you can see that it's orders of magnitude faster than all of these extended likelihood methods. Now that's great. And it's certainly about the same as the standard GP, which is also very good. And that's purely thanks to conjugacy. Then we started. Then we started playing with a bunch of applications where people were excited about using GPs. So, one of them is Bayesian optimization. And what we found was that if we looked at some of these standard toy problems for Bayesian optimization, you know, these kind of hard functions to optimize, we inject some outliers in them. Well, we can get a very nice, robust approach. So where we don't get too impacted by outliers. And this maps out to the acquisition functions. This maps out to the acquisition functions. So the acquisition functions typically depend on the posterior, and they tell you where to go and evaluate next. Now, if you have a robust posterior, you also get a robust acquisition function. And so that's really nice here. So what we notice is essentially that the method is robust in the sense that the regret doesn't blow off super fast, but it's also much faster to run than, let's say, your TGP. And finally, I... Um, I we also kind of thought a bit more about computation. So, one thing that people like to do is to make these GPs faster. So, instead of the standard qubit cost, bring this down to linear with some variational approximations. And here, you can essentially use our same approach to make fast variational GPs. So, here is one example on one of these UCI data sets where, you know, if you have a lot of, sorry, this is a toy example. Sorry, this is a toy example. Sorry, where if you have a lot of observations, the cost is roughly similar as these SVGPs, but you get something more robust. Okay, so that's me done. So in terms of kind of conclusion, I think GenBase can be super nice to induce robustness and efficiency. So not just robustness, which is the main focus of the kind of community right now, but I think we can also be quite clever with how we. Also, be quite clever with how we choose our loss to gains on the computational side. In the case of GPs, well, the nice thing is you get robustness and conjugacy specifically, which is one of the best ways to be computationally efficient. And so we've been able to achieve something which no other competitor has managed so far. I think there's loads of exciting things we could do with this GP approach. So in fact, we're kind of looking at that now with one of my incoming PhD students. With one of my incoming PhD students, so I think we could extend it to other settings where people are excited about GPs, could be multi-output, multi-fidelity GPs with other kinds of information, let's say derivatives or integral. All of that you can still make conjugate and fast thanks to this approach. That's it. And I think generally this approach of making things fast and robust can be helpful beyond the Robust can be helpful beyond the kind of GP world. So, there's been a few other papers where we've looked at this. So, one of them is change point detection. So, here you can see this other paper we've got where you have some outliers. You want to detect when there's a change point, but not when there's an outlier. The blue method is our robust approach. The green dots are what the standard Bayesian online change point detection method finds, and you can see that it always gets triggered by design. Can see that it always gets triggered by these outliers, whereas ours doesn't. And the cost for the two methods is the same. So that's again kind of robustness and conjugacy. You can also do it for the Kalman filter. So this is an example also from ICML this year where we've built a robust Kalman filter. So you can see here on the left-hand side has been some more extreme observations. So the kind of dot is where we get some observations. The gray line is the truth. Is the truth and the lines are prediction? And you can see that the right-hand side doesn't really get impacted by these extreme observations, whereas the left-hand side does quite significantly. So, this right-hand side is just this robust and fast Kalman filter that we propose in that paper. And finally, we've also looked at this in more classical statistical problems, mostly these kind of doubly intractable problems, where we showed you can get some of these same things. So you can get Some of these same things, so you can get fully conjugate, um, doubly intractable problems, which is quite cool. Okay, and that's it. Um, so the paper is here in case you're interested, and I'm also very happy to answer any questions. Thank you. Any questions? Yeah, I have a question. Um, maybe it's a thing. Question: Maybe it's a silly one, but so like in this generalized base approach, like could you manage to like recover if there's an implicit error term, like you know, like the epsilon? No, you say the Gaussian IID, like you have at least like some kind of approximation to see if there's like a kind of like an equivalent model where you have there, like and get an idea. There, like, uh, and get an idea at this model, what kind of error terms like it could be targeting, let's say? So, I'm not sure there's necessarily an equivalent model that would give you this generalized posterior, mostly because here the weights depend on both x and y, so it makes it a little bit more complicated. So, I'm not sure it's that easy to do that. We did not do that anyway. Um, maybe I would need to pick. Maybe I would need to think about your question a bit more in detail, but we haven't done that. Yeah, thanks for a great talk. I was just curious if you've looked into, you know, like a big challenge for generalized phases is calibration and uncertainty quantification making. Like, you know, you can prove consistency for the stereotypes, but it's typically. But it's typically pretty challenging to get well calibrated in a frequentist sense, like uncertainty quantification. Have you looked into that for this type of generalized space? So we did not look into it, if you mean in like a formal kind of theoretical way. No, I mean either practical too, like, yeah, yeah, like, is there a practical way of, you know, like changing likelihood such that you still get nice and. Such that you still get nice uncertainty quantification, or it just looks better, you know. Yeah, so here in this example, so typically there's this thing called the learning rate that you get in generalized space. And you may have noticed I did not have a learning rate. And one of the reasons is that most of how we kind of change the uncertainty is through a C parameter rather than the learning rate. And in practice, we did try a bunch of different ways and we Ways. And we found, we've basically got a way of choosing C, which says something along the lines of I expect about 5% of my outliers, of my points to be outliers. And that's, and we then have a formula to give you the C you should choose. Typically, all the experiments we've run, we've actually obviously not picked exactly the same proportion as appliers as the corresponding C, because otherwise it'd be cheating. But we found that using something like that actually gave you pretty decent uncertainty. That actually gave you pretty decent uncertainty quantification. Now, it is definitely a heuristic, and you could probably do better, but at least it seemed to do pretty decent on what we were doing, or at least all of the applications we were looking at. So in particular, like if I think about some of these problems, like the Bayesian optimization setting, where you're really using your posterior uncertainty to kind of query new points, if we had really bad posteriors, I don't think we would have had a very good Bayesian optimization algorithm. Had a very good vision optimization algorithm, and again, you could kind of see that work fine. So, this heuristic seems to work fine. Um, whether it's perfect, probably not. Is there more to do? Probably yes, but uh, it seems to do quite well, thanks a lot for having me. 