I'm sorry we made a few changes in the corner. We're running a few minutes late, but hanging there. All right, thank you. It's really great to be here. Throughout the course of the meeting, I've heard the sentiment that there are a lot of really exciting questions and data sets, but not as many solutions. And I often am thinking from a different perspective where we have lots of solutions and we're looking for problems. And so I think this is really great. And so I think this is really great. And I think although I don't particularly work on wearables, a lot of the methods that I do think about are quite relevant to this application domain, and not just relevant, but probably newly relevant. There have been a lot of really exciting changes in the space of Bayesian nonparametric modeling and Bayesian deep learning. In particular, I and my group work a lot on two types of model classes, Gaussian processes and Bayesian neural networks. And Bayesian neural networks. This talk will be more about Bayesian neural networks, you know, given limited time. But if you do work with Gaussian processes or time series data that is big and might have a lot of missing covariates, et cetera, and you're trying to discover some interesting structure and perform an extrapolation, we've been developing a lot of methods for those problems specifically, and we have a package, gpytorch.ai, so maybe after the talk or So, maybe after the talk or sometime later, if you feel like checking that out, we have a lot of examples and so on. And often, I think many of these kinds of problems can be just dropped right into those examples. So, I'll start with just a simple pattern recognition question. So, suppose we have these airline passenger numbers recorded monthly, and we'd like to make a prediction far into the future, in this case, say up to 1961, and we're going to consider three choices for fitting these data. So we've got a straight line in choice one, a qubit polynomial in choice two. A cubic polynomial in choice two, and a 10,000-order polynomial in choice 3. So I'd like to see a show of hands. How many want to go with the linear function? Okay, the cubic polynomial? Really? Usually people like the cubic polynomial. Okay, 10,000-third polynomial. That's what I would go with, and that's what I'm going to argue for in this talk. In fact, I think it's not flexible enough. And the reason is that we actually. Is that we actually believe that none of these choices will probably be a great description of the real generating processes for these data. But we'll be able to get closer to the ground truth with some setting of those coefficients, those AJs, in the 10,000th order polynomial than we could with the straight line or the cubic polynomial. In fact, these two choices at the beginning here are actually just a special case of the third choice. Well, but the plot the plot is made in such a way that really The plot is made in such a way that really it favors linearity there, right? But if you look carefully, you see clearly there is correlation from year to year, you know, there's the time series. So, probably you have to take that into account and induce a lot more flexibility to do that. If you blow it up a little bit, so if you don't make it so small in the left, you immediately see all the patterns there. You go up and down and up and down. It's very clear. There's all sorts of structure, yeah. So there's quasi-periodic trends and so on. So there's a lot of structure that would be ignored by a straight line, but I'm assuming people don't want to choose this. But I'm assuming people don't want to choose this third choice for fear of overfitting. Maybe we'll sort of run through the data points nicely, but then we won't have a solution that generalizes very well. And so we can resolve this. And some of the next slides will be sort of talking about this resolution. But I'd like to say also that this is sort of a choice that a lot of us have already been making implicitly. It's actually quite common right now to use neural networks that have tens of millions of parameters to fit problems with maybe tens of thousands of data points and find that we. Thousands of data points and find that we get really good generalization. And so the key to resolving this sort of paradox, how we can use this kind of model class and still get very good performance, is in not conflating the flexibility of a model class with the complexity of the model class. And so Gaussian processes are actually a great example of a model class that has an infinite number of parameters and an extraordinary amount of flexibility. Gaussian processes with RBF kernels are often universal approximators. Kernels are often universal approximators, but they're a very simple model class. If we look at likely sample functions under the prior, they're often very well behaved, they're very predictable in a sense, and this is why those kinds of models can generalize. So visually, we can try to understand it sort of in this diagram. Sorry, it's a little bit small. But on the horizontal axis, we have all possible data sets. And on the vertical axis, we have the evidence for a particular model. And so that's the probability that we would generate a data set if we were to randomly sample. A data set if we were to randomly sample from the parameters of our prior. So a model with poor inductive biases could spread its mass very evenly over different types of data sets. So it isn't saying that certain types of solutions are more a priori likely than others. And that's not good. So this is a flexible model, and it won't generalize. It won't really contract very well if we observe more and more information around some kind of true solution. On the other hand, we can have a very simple model, like we could go to that. We could have a very simple model, like we could go to that choice one and say, Let's put a standard normal prior over A0 and A1, sample from that prior, we get different straight lines with different slopes and intercepts. We can only generate a very small range of data sets. And this is a proper normalizable probability density, so that means we're going to give a lot of mass to those particular data sets, but we're not going to be able to explain very much at all. And then we can see another model here, which actually has very wide support. So it's saying that there are many different solutions that I don't think are likely, but I think. That I don't think are likely, but I think they might have some at least arbitrarily small probability of being a correct description of reality. And so, if I'm trying to honestly represent my beliefs in the modeling process, I'm going to include those solutions in my hypothesis space. But I'm not going to make them very likely. On the other hand, we might have other types of data sets, like structured image data sets, which we're trying to model, and we want to make those very likely. And so convolutional neural nets are an example of a model class that manages to essentially achieve both. Manages to essentially achieve both flexibility and reasonably good inductive biases through having various symmetries in the parametrization of that model. If I were to use something like that huge-order polynomial from a Bayesian perspective, I would have to be careful about the prior I chose on these coefficients. If I was thinking about what kind of prior I'm inducing in function space, which is much more important than what kind of prior I have just over parameters, if we're just thinking of, I mean, the parameters in isolation have actually no meaning. Then I would want, for example, maybe the Then I would want, for example, maybe the variance of that prior to decrease as I increase the order of the coefficient. So, in that case, if we were to look at sample functions induced by having such a prior over these parameters, they would often be much better behaved than if we just had, say, like an isotropic Gaussian prior over all those parameters. So we just have to be a little bit more thoughtful in how we use these flexible models, but I would argue that flexibility is what we want because it's going to enable us to converge around something that is a better description. Converge around something that is a better description of reality, as long as we're careful about our inductive biases. And I outline this sort of philosophy of model construction and generalization in a new preprint we've posted recently. So I'd like to take another quick poll. How many of you have used neural networks before? Okay. How many of you trust the predictions of these neural networks? More than other models that we're using? No. Than other models that we're using. No. Exactly, right? So we know for sure that quite often, for instance, these models are overconfident. So if we're using a convolutional neural network for classification, often the highest softmax output is much larger than the actual probability associated with that class label. This, I would argue, comes from ignoring what's called epistemic uncertainty, uncertainty over what sort of setting of parameters is actually correct for that particular Is actually correct for that particular problem. On the other hand, neural nets also can represent many interesting, different, complementary explanations to a given problem. And I would say this sets them apart from a lot of different models in statistics and machine learning. In particular, when we train these models, we typically have some objective function. So if it's regression, it could be something like regularized mean squared error. If it's classification, it could be the cross-entropy loss. It could be the cross entropy loss. But this loss, if we sort of visualize it in one dimension as a function of parameters, is very, very multimodal. And the modes have different properties. They may all have very low loss, or at least the ones that we can discover easily with SGD. But the functions corresponding to these different settings of parameters are actually quite different in many cases. So these modes aren't purely due. So these modes aren't purely due to symmetries, for example, in the parametrization that give rise to equivalent functions. And so from that perspective, it's actually quite arbitrary to say, I'm just going to run SGD and find one of these solutions and then use that as my solution, even if it gives me sort of okay performance in some setting. Why not consider these other solutions? And that's fundamentally what a Bayesian method is trying to do. When people talk about Bayesian methods, they often focus on the problem. Methods, they often focus on the prior. But maybe the secret here is that while the prior is important, just like many modeling decisions are important, regardless of whether you're Bayesian or not, the real difference between a Bayesian method and a non-Bayesian method is not the prior. We see the prior actually show up in many different non-Bayesian settings like L2 regularization, et cetera, in what are called Bayes classifiers, which are not actually Bayesian. We just sort of have a generative. Bayesian. We just sort of have a generative description that we invert using Bayes' rule and then train through maximum likelihood typically. The difference is marginalization instead of optimization. So that is to say using every possible setting of the parameters and then weighting those by their posterior probabilities. So that would be to say, let's use all of these solutions and just weight them by our posterior. And in fact, not just the modes, let's try to sort of look at a flipped version of this curve and integrate. Look at a flipped version of this curve and integrate over everything. And I would argue this will make a bigger difference for neural nets than it will make for virtually any model class you could imagine. Because the lost surface has this kind of structure, and because these models can represent many different complementary solutions, and because the posterior is extremely underspecified by the data. So, although we are successfully training sometimes neural nets with tens of millions of parameters on tens of millions of parameters, Tens of millions of parameters on 10,000 data points, the posterior is extremely diffuse. Whereas if you were to use maybe a logistic regression model on a problem with a lot of data, you would probably see a fairly sharply peaked posterior. And in this case, if the conditional predictive distribution, this y given our parameters, isn't varying that much where that posterior has mass, then maybe there isn't actually going to be a huge practical difference between a Bayesian and a non-Bayesian approach. Bayesian and a non-Bayesian approach. Basically, just treating the optimization solution as some approximate inference procedure where we say our posterior is described by some delta function centered on one of the local optimum. But in the case of deep learning, there's going to be a huge discrepancy. And then the next question is, well, if we feel that the Bayesian methods have a lot to offer in this space, what are the challenges? And the challenge here is this integral is extraordinarily high-dimensional, and the posterior landscape is. And the posterior landscape is extraordinarily messy and hard to navigate. And so, as sort of towards trying to sort of reason about these problems, we started actually thinking about some of the geometric properties of these lost landscapes. So, initially, it was believed that if we retrain a neural network using a standard optimization procedure like stochastic gradient descent, we would find different local optima, often actually pretty close to global. Often, actually, pretty close to global optima, that would appear to be isolated. So, if we tried to walk on a direct linear path from one to the other, we would incur very, very high loss in between, and that's true. But it turns out, as we showed in a paper a couple years ago, that there are subspaces along which you can actually walk from one solution to another without really increasing the loss very much at all. And so, there are a lot of details behind how you can discover these subspaces, but you would. How you can discover these subspaces, but you essentially always can. The curve won't be a straight linear path, but often it can be very simple, like a quadratic Bezier curve or some kind of simple polygonal chain. And what's even more interesting, and just to be clear, this is a visualization of a two-dimensional subspace of a really high-dimensional parameter space where every point in this plane is some linear combination of three settings of the neural network weights, and then the loss evaluated at that point. And then the loss evaluated at that setting. But this is just a slice basically through a very high-dimensional loss surface. So, what we found is that as we moved along these connecting curves, it wasn't just that the loss was near constant, it was that the functions that we were getting were often very different. So, these paths didn't exist, again, due to symmetries in the parametrization. They existed because there were just these vast. Existed because there were just these vast regions of the lost surface that actually provided all sorts of good and complementary explanations for the data. And in fact, in some subspaces, they were often very clustered together. So you see all these points, and all these correspond to different parameter settings that have basically the same value of loss, but give us different functions, different explanations for the data. They're very hard to discover with normal optimization. So normally when we run SGD, So, normally, when we run SGD, we sort of converge to the periphery of this set. So, we started thinking, actually, just from the optimization perspective, how we could at least get a more centered solution in this set, because that would be kind of like a rough approximation to an ensemble of some of these models. And so, we could achieve that just by increasing the learning rate so that it plateaus to kind of a constant level and then take kind of an equal average of the parameters. So, we're sort of spinning around this set, and then we find a central point. We found that even a simple sort of trick like that actually. A simple sort of trick like that actually improved generalization quite significantly on a lot of problems. So, this was called stochastic weight averaging on it. Do you mean that as you walk along the path, you get very different functions, even at like close steps along the path, or just like as you go from one end to the other, you can get very different sort of results? Even close steps. So, I actually have a sort of visualization of something like this in the regression setting. So, on the right panel here, we have one of these connecting paths. On the left panel, we have the associated functions. The left panel, we have the associated functions and how they're fitting the data. And so as we move around in this space, we see a reasonable amount of functional variability. And by marginalizing over that whole mode connecting region, we actually can get this predictive distribution. And we can see that it's not a bad description of epistemic uncertainty. So that we can visualize this by seeing, well, how does our predictive distribution change as we move away? Predicted distribution changes, we move away from the data. So, as we move away from the data, there should be a lot more sort of like different possible explanations. As we move towards the data, the number of consistent explanations for what we see kind of decreases. Whereas if we were just modeling noise uncertainty, in this case could be called aleatoric uncertainty, then we wouldn't necessarily see that behavior, and certainly we don't see that behavior if we're working with kind of standard neural nets. So are are you saying then the way that you follow this path between the mini local men is by having a constant learning rate? Is that what you're saying? Or controlling the learning rate? Because this is the impression I got through that picture on the blackboard. So that was just an optimization idea. So we evolved this, sort of briefly, I guess, into an approach for approximate Bayesian inference. So the trick there is just start with a normal learning rate. Just start with a normal learning rate, decay as usual after about 75% of eatbox, but decay to a much higher constant learning rate than one normally would. And then compute a lone-rang plus diagonal approximation to the covariance matrix of basically the iterates that you get here. So you can act like these iterates are samples from some Gaussian distribution. And there's good theory to kind of support that. And then compute their mean to get the mean of the Gaussian and compute this approximation to the covariance. And basically the mean of this would be kind of what I was describing. Of this would be kind of what I was describing here for optimization. Once we have that Gaussian posterior approximation, we can sample from it to form our Bayesian model average. And just doing that actually increased the reliability of a lot of these models quite significantly. So here we have confidence minus accuracy. So confidence is basically the highest output of our classification model, and we're subtracting the accuracy associated with using that class label across a number of different. Class label across a number of different problems, some of them involving covariate shift. So in green, we have standard training. We see that it's above this horizontal curve, which would represent a well-calibrated model, which means it's typically overconfident. And this is for different levels of confidence we have here. And then the other curves represent all sorts of other tricks and ideas to try to recalibrate these models. Blue is what we get when we follow this, just this simple procedure. And the thing that I sort of like about this is it's very scalable and practical. This is it's very scalable and practical. So, a lot of standard Bayesian machinery doesn't apply very directly to these kinds of high-dimensional problems. A lot of the variational approximations take forever to train or to find a good solution for. MCMC often really struggles in these kinds of settings as well. This is sort of a simple idea that actually gets pretty good performance and just leverages geometric information that you have already basically from doing SGD training. Here we have another example where we can see that this. Here we have another example where we can see that this also scales to relatively big data sets like ImageNet with like a million data points. Here we have some covariate shift problems. So in these cases, all the models are a bit overconfident. But this is a setting where it's very hard to use some of the heuristic alternatives like temperature scaling and flat scaling because those are kind of determined through cross-validation, which is hard if actually you're Which is hard if actually you're going to deploy a model on a somewhat different data set from which it was trained. Whereas a procedure like this doesn't necessarily require that kind of validation and tends to actually do a lot better in these kinds of situations. So we did a lot of stuff to kind of justify this procedure, that it was capturing some reasonable description of the posterior landscape, etc. And some of this involved visualizing the lost surface in this subspace. Surface in this subspace of principal components, basically spanned by the SGD trajectory. And this made us wonder: like, could we possibly actually just do inference directly in some subspace like this? And so, simple idea, basically we just collect all of the parameters of the neural network and say that they're equal to a linear projection of some low-dimensional vector, z, plus some offset. And the goal is to do inference. And the goal is to do inference in Z space rather than in W space. And this might be D by 1, this could be D by Q, and this is Q by 1. And when we specified this projection matrix basically just through the principal components of the S G D iterates that we get through regular That we get through regular training or through using SGD with this modified learning rate schedule, so we're kind of traversing through high-performing but different models. We could actually project down into maybe even five dimensions and capture quite a huge amount of variability of the function. So even though the function might have tens of millions of parameters, the contention is that there could exist some low-dimensional subspace that actually captures a lot of variability of that function. And if we can marginalize in that low-dimensional subspace, Marginalized in that low-dimensional subspace, we could probably get a much better answer than if we were just to do conventional training. And would you please take about five more minutes? Of course, yeah, thank you. And so we did that, and we marginalized in that subspace, and we saw that we could get, so this was sort of a different subspace we considered that we discussed. We saw that we could get pretty good quality predictive distributions. Here we're visualizing the predictive distributions for regression using. Distributions for regression using various different subspaces. So, this is just a random projection P matrix. We can see that it's not doing a great job of capturing epistemic uncertainty. The uncertainty doesn't really increase as we move away from the data. So that's a good kind of sanity check in general. The PCA subspace does a lot better. This mode connecting subspace does better still. We had sort of a special procedure for discovering that subspace based on looking at kind of line integrals of cross-entropy. Grills of cross-entropy. And we saw there was sort of much better accuracy as well as negative log likelihood as well. I feel like a lot of the methods in Bayesian deep learning so far have focused mostly on just the quality of the uncertainty. Perhaps an overlooked advantage of Bayesian methods in this space is also benefits in accuracy. And I think you'll get that because you're basically ensembling a lot of really high performing models. Performing models. All right, yeah, that's all. Thank you. Any additional questions for Andrew? Swanna wants how much is P. Is that a random matrix or? So there are many possibilities. Here we have just a random matrix, so that's not so great. Here we looked at a principal component subspace, so just to be super clear, we create some matrix A, and it's called A and its columns will be different SGD iterates minus the mean of the iterates. So W1 minus W bar, W2 minus W bar, etc. And then we can do a partial SVD on this matrix, and that gives us P. And often this can be projected into a very low-dimensional space. And this will actually give us a big boost in performance and in calibration. And we found that these methods also work really nicely for problems like semi-supervised learning, where you might have a lot of unlabeled data but not You might have a lot of unlabeled data, but not very much labeled data. Often you can derive very coherent likelihoods over both and sort of train a model that will give you very good performance. Also, for things like fuse-shot learning or problems where you want to use a neural network that has really great inductive biases, say for images or some other problem, but you don't have a lot of data to train it with, and you want sort of reasonable predictive distributions. Often these ideas are starting to work early. These ideas are starting to work really well. So, I would say this is a change over the last six months. We're starting to see a lot of activity in this space that's becoming practical, even though there's been some sort of interest in trying to make these methods work for some time. Any other questions? Do you have results for that first motivating example to the degree 10,000? Yeah, you can actually show. So, there's a lot of confusion about this, even in the Bayesian community. So, actually, I went into this a bit in this. So actually, I went into this a bit in this very recent preprint. But in Bishop, basically, one of the canonical textbooks for machine learning, which is a great book, he has an example saying that the Bayesian evidence is a good way for selecting between models. And he actually uses an example where he considers polynomials of different order. And he has a spherical Gaussian prior over all the coefficients. You can analytically marginalize it and create a histogram basically of evidence versus model order. And he shows what he calls Occam's Hill, where like the low-order models are penal. Like the low-order models are penalized, and sort of the high-order models. It seems superficially sort of pleasing because it aligns with what we're taught in kind of conventional statistics. But I think those results are purely an artifact of a bad prior. So if you actually just scale the prior with the order of the model and then recompute that evidence plot, it actually asymptotes towards infinite order models. And if you go ahead and use the biggest possible model you can store in memory, it performs better than if you use, say, the sixth-order polynomial or whatever was being favored in the Or whatever was being favored in the bishop example. And there are actually results for that. Okay, let's thank Andrew.