Oh, who speaks to it? Andre is he he tends to arrive at the last I think it might be due to the fact that we start on the hour and we spend some time in journal. I would suggest we loop with a couple of things. So that's some scheduled start at 9:01 in the German 7. I think we're are you I think we're I think we're at a form right now. So, welcome to our last month, Month's Day of the Meeting. And we have Joe Conning of Notre Dame and some of the authors about multi-graded implicitization. Yeah, thanks for having me. It's been a wonderful week. Really excited to have been able to participate. So, today I'm going to talk about a Macaulay 2 package that Ben Hollering and I worked on. On at the University of Chicago last semester at a workshop for algebraic sequence. So the main problem that we're interested in is something that you all know really super well. So we're talking about implicitization. So just to remind everybody, even though you probably don't have to, you take a polynomial map between vector spaces over a field. So these FI's are polynomials. Of polynomials, and what we're interested in finding are we're looking for implicit equations that cut out the Zariski closure of the image, right? So, everybody knows how to do this. You write down the corresponding ring homomorphism, and then you pick a term ordering and compute a Groger basis, and then you're basically done. As long as you picked a good elimination order, you just look at the terms in. In the, I guess, the nth elimination ideal. So that's great, and it completely solves the problem, right? Unfortunately, growth bases are hard. That's why you should be finding the fixed points. Yeah, no, no, no, that's not true. No, no, at the bottom. No, here? Yeah. Why wait, why is it? Yi minus Fi of X. It's okay. Every time you gotta go, you're right. Okay, yeah, my bad. Yeah, you're totally right. Okay, so I think I copy and paste, so that mistake will repeat, but anyway, so we're interested in solving a simplicitization problem. And well, not the one that I wrote down, but you know, the one that you actually want to solve. And the problem is that it's difficult because Groger bases are hard. And so, in algebraic statistics, a lot of times we have lots of, we have these problems with huge numbers of variables. The problems are typically coming from, you know, you have some sort of combinatorial object like a graph or a sequential complex that's telling you how random variables are related. Like maybe it's telling you conditional. Conditional independent statements, or it's literally telling you how to parameterize the statistical model. And these things often have a lot of structure. And so sometimes, so even there's a large number of variables, there's still some hope that we can use some nice combinatorics to find growner basis for these. But even if you can't find a full growner basis, you know, there are some kinds of results, like there's identifiability results and Identifiability results and entrance of parameters, and even some hypothesis testing that you can do and get away with, even if you don't have the full idea, even if you just know a few polynomials in the kernel of the map from before. And so that's sort of what our package is going to do is it's just going to find some low-degree polynomials in the ideal that hopefully can execute problems. So it's nothing too. Well, yeah, anyway. So let's look at an example. So this is an example coming from phylogenetics. So there's a lot to unpack here. But what you should take away from this is we have this phylogenetic network. And so at the leaves, these should be sites and DNA sequences of four different species. And And we put some sort of Markov process on this network. And we have these dashed edges because, sort of, at this species here, that's some sort of hybridization of these two parent species to be a new one down here. So this is something that's actually observed reasonably often in smaller organisms like bacteria and viruses and so on and so forth. And so on and so forth. And you might be like, where is this parameterization coming from? So, the first thing I'll tell you is you can see that it's parameterized by a binomial, it's parameterized by binomials. So, if you look at the first monomial in the term, that will tell you the ideal if you delete one of these edges. And then, if you look at the second, then that'll tell you the parameterization for the tree if you delete the other edge instead of one. So, you imagine like if you just You imagine, like, if you just look at this first term, that's what you get if you cover up E8, and then if you look at the second term, that's what you get if you cover up E5. And you might wonder what these Q's are. So really, the original statement of the problem is in probability coordinates, but there's a linear transformation. It's in Spring Fourier transform, and it turns this into a much more well-controlled. Control parameterization. So you have 32 or how many? How many very 64 Q's? Okay. And then we've got this condition, the sum of the GIs is 0, 0, so that would give you half of those. Quarter of those? Quarter of those, yeah. So it's 4 to the 4th divided by 4. Fourth cubed? Oh, sorry. Okay, 4 cubed. Gotcha. Okay. Yeah. Is it easy to say how? Is it easy to say how you get those codes? I said those exponents? Oh, these are not exponents. These are these are these superscripts. Oh, they're superscripts. These superscripts are corresponding to edges. Oh, so there's just 12 more variables. Yeah, so there's 40 variables because the Gs are coming from C2 crescent. So if you put this into, so I did not put this into a computer and asked for a grover basis, but I know people who did, and they said that auto cluster. People who did. And they said that on a cluster, it took 100 days and it didn't finish. So this is, so even it looks sort of combinatorial and nice, but even this is just like too large to get anything at. And we certainly expect there to be quadratics and cubics and cortics and things of low degree that are useful. So for example, if you just know the, so if you sort of look at the space of all of these phylogenetic. Of all of these phylogenetic networks. If you just know the quadratics, that should be enough to tell you the topology of the underlying network that your data is coming from. So even if we don't get the full idea, which we certainly won't, just knowing just the quadratics is useful. Are there any questions before I move on? All right. So, also, the last thing I'll say is, you know, Thing I'll say is, you know, I've talked about implicitization. There's this multi-graded part, and so you can see we factored out this monomial out front, and that sort of indicates to us that there's some sort of Z grading out in front. And in fact, you can think about it a little bit, and you'll realize that, well, there's four of these A's, there's four A1s, four A2s, four A3s, and four A4s. So you should think, okay, maybe there's this. So, you should think, okay, maybe there's this Z16 grading. But it turns out that there's each one of these A1, A2, A3, A4, each one of those only gives you a Z3. So altogether, that's telling us that there's a Z12 grading on this ideal on the kernel. Z to the 12th, yes, not Z12, Z to the 12. And then, of course, they're homogeneous in the regular sense, so that gives you an extra Z. So there's this kernel. So, there's a this kernel will be graded by C to the 13th. No, it's large. It should be like it's on the order of 0.6. It should be on order of double whatever that period was. There's okay, so there's this paper by Sam Martin and Paper by Sam Martin and Elizabeth Gross, and they would tell you exactly they have a formula for it for these types of things. They say there's a Z, is it the 15th grading? Z or the 13th. Okay, good, because you get something from the binomial as well that has some homogeneity. Yeah, no, actually here it's only the standard. I think he's already captured that. Okay, okay. Okay, so let's just remind. Okay, so let's just remind ourselves again, everybody knows what this is, but the multigradings that we'll be considering. We'll be considering multi-gradings by matrices. So the same number of columns as number of variables. The degree of xi will be the ith column of whatever our grading matrix is. If you have a monomial x to the beta, so beta is a positive integer vector, we can find the degree of x to the beta by multiplying by n. By A. And we say that the map is multi-graded if you have a grading on the domain and codomain and phi respects it for monomials. And then we also say that the grading is positive if the integer kernel of this matrix intersected with the positive orthant is zero. So this is this basically ensures that the dimension of like the The dimension of like the each graded piece of the ideal is finite dimension, otherwise, you hard. And then, yeah, so when you look at this matrix A, you know, the things that we'll be considering today, everything will be graded in the usual sense, just the gradings will be extended further. And the way that you can tell that by looking at the matrix is that the all ones vectors in the row space. Great. So the question is: well, and again, there should be Y, so we should fix that. But the question is, is like, how do you actually find multi-gradings? Well, so if you open up, so I put theorem here and we prove it ourselves in the paper, but it's certainly been known for a long time. It's certainly an angler-shot instances so well. And it's probably adopted for that as well. But basically, all you have to do is you have to find is you have to find weight vectors on x1 through ym so that when you take the initial ideal with respect to that weight vector of the generators of the elimination ideal, you get back the same thing. Now, when you do this, if you just say, okay, well now I'm just going to focus on taking the first n coordinates, and then I'm going to stick those as the rows in a matrix, that will give me a matrix A. A matrix A that defines a multigrading for the kernel of C. So it's not too hard to show this, it's really pretty easy. And the important thing is this is super easy to find, right? So finding these a basis for these, or finding these BIs is almost instant, right? So we can instantly, so even if you don't know anything about the problem, if you just know what the parameterization is at the beginning, then you can find You can find a big multi-grading if it has one. Right. Let's look at an example. So, this is the pluker embedding for GR24. Again, everybody knows this. So, we have our pluker coordinates, then we have the coordinates in some 2x4 matrix, and we're mapping Pij to the corresponding minors. And if you compute, And if you compute, if you want to know how this thing is graded, well, you form the elimination ideal, and then you solve the linear algebra problem from the previous slide, and you get something that looks like this. And so, this is obviously very large. We only care about these first six columns. So, this matrix here defines a grading for the kernel of feed. Joe, one question? Can you just go back one slide, please? Yeah, yeah. So, for this theorem to hold, don't you need that the xi minus y xi are reduced to that basis? Yeah, well, they with respect to any order. Well, no, so they are with respect to with these BI's, right? So, this is this is uh this is basically saying that the homogeneity space of the Rodner fam that defines a torus. That defines a torus action on your variety. Yes, but for that, you need the thing that you. I don't think you need it. He just wants these to be weight vectors. This is saying that these are weight vectors for literary pieces. Yes, but he claims like that the PIs are a set of bases, but he claims that he has every well, but yeah, for that to be a weight factor of some linear relations, and you're going to take the bases for the And you're going to take the basis for the kernel a little bit. Right. Is it okay? I'll think about it a bit. Okay. So yeah, I want to show you how fast this is on an example real quick. So Macaulay 2 booted up here in VS Code. Can everybody see that? Should I make it bigger? Unfortunately. Okay, so we have this package multi-graded implicitization. I should be smaller. So this is make the fluker coordinates and everything else. So I'm just making the map here. And so, the matrix that I just showed you all, there's this function in our package called maxGrading, and it just solves that problem from before. So, this is the matrix that was on the slide. This is sort of the verbose version because it's returning the target grading as well. That's never used in computation. So, if you just call max grading by itself, you'll get this matrix here. So, it's just the first six columns. And this defines Six columns. And this defines our grading. And you might ask, hey, is there really a five-dimensional torus acting on GR24? And the answer is no, because the rank of this matrix is only four. So there's only a four-dimensional corpus map. And that's coming from the fact that we cut off most of the columns, so the rank drops. But that's okay. And we also prove in our paper that this is. In our paper, that this is essentially a maximal grading that you can find. So you could potentially find a larger grading, but it will require some change of coordinates. So this is like, if you don't put any forethought into the problems, it's the best you're going to do. By change of coordinates, you mean a linear or multiplicative change or? Any kind of change of coordinates that you can come up with. But a linear cool. I see. So this is like the largest subset of the torus you get from your. Subset of the Taurus you get from your choice of ordinance. Yeah, yeah. The diagonal torus. Yeah, yeah, yeah. And it's really easy to pick up examples where it's like clearly. Yeah, that was the formulation was the processing evaluations. So there's examples where like you clearly get something that's too small. So like if you take a torque writing, that makes it up in a bad way. That's what I was going to do. So much more should you. Okay. Okay. So let's go back to the slides. Okay, here. And then, so what are we trying to do is we're saying like, okay, so what we're going to do is we're going to, we now have this grading and it refines, you know, regular total degree. And so what we're going to do is we're just going to like look at each grading component and then we're going to solve a linear algebra problem. So and And so, for example, if we look at the degree 211 minus 1, then we can encode a generic, so here's a generic polynomial in there. These columns are basically the column vectors for each of the binomials, and we're just looking for the kernel of this matrix. And if we do that, we see, okay, well, then after then there, if and only if C1 is minus C2 is equal to C3. Minus C2 is equal to C3, and then you get the fluid relation. So, obviously, this example is small and easy, but we can find it quickly. And yeah, so actually, rubber bases are faster for this example. So, one of the main optimizations that we use, so actually, I want to go back to the cycle quick. I want to go back to the cycle. So, the thing is, is you know, if you everybody sort of knows that you can do this if you have total, if you have regular z grading. You just say, I'm going to look at all the degree D monomials. I'm going to map them over. I'm going to look for linear relations, right? So what we're doing is the problem with that approach in general is that the number of monomials grows really, really fast, you know, if you have a lot of variables. So, the idea here is that if you have a lot of variables, So the idea here is that if you have a large multi-grading, then instead of solving one large linear algebra problem, you're solving lots of smaller ones. And hopefully that speeds things up a little bit. The main speedup that we have, though, in our package is this parameterization coming from algebraic matrix. So if you take your parameterization to start with, your ring map, and you look at the Jacobian. ring map and you look at the Jacobian. So we're mapping the ijth entry is you look at phi of xj and you take the partial with respect to yi and then you fix some subset of the variables x1 through xn. Then you actually know if the xi's are going to be algebraically independent sort of modulo kernel of p. Module of the kernel of B if the rank of this submatrix corresponding to those columns is full. So that sort of gives you a thing where it's like, okay, I have this graded piece. Before I start, you know, computing a kernel or anything, I'm just going to check to see if this is full ranking. If it is, then I'm going to continue on to the next thing. So, in practice, so for example, for this. Practice, so for example, for this phylogenetic problem from before, this allows us to skip like if you want to compute up to the qubits for those things, it allows us to it allows you to skip over 90% of the graded problems. So just as a quick example, here's a little Toric map, a little monomial map. There's sort of an obvious grading matrix. Here's the Jacobian. And if you look at the degree 5, 1 part of the ideal, degree 51 part of the ideal that's just generated by the single monomial x0 squared x1 so the support there is just x0 and x1 and if we take the corresponding columns in j the rank of that matrix is two so we actually don't need to set up the computation so that's sort of an obvious example but you get the picture okay and so this is our main algorithm is is basically you you start off with is basically you you start off with um you start off with your polynomial map and then you also input a degree a total degree d that you want to compute up to and uh what we're going to do is we're going to return we're going to return a minimal generating set up to that total degree so you just initialize everything uh so you you're generating set g this is going to start This is going to start off to be empty. This B, this is going to be the monomial basis for each multi-degree that we'll have to compute. And then our Jacobian matrix. And then what we do in here is instead of computing these ranks symbolically, we just plug in random numbers. Because if you plug in random numbers, the rank can only drop. And so, okay, maybe you did something you didn't have to, but so what we do, what do we do is, okay, so for each. Yeah. Yeah. Yeah. Yeah. Yeah, yeah, we do. Yeah. And we keep track of it throughout the computation. So that's what I was going to say just here is we start off with these PIs, and this is, you find all the multi-degrees in your given total. Multi-degrees in your given total degree. So maybe you're like, you're looking at cubics, you break up all your cubics in the different multi-degrees. And then here I'm just saying, it does say just take all the, like, look at the whole thing. But actually, what we do is at each step, we trim it. So we say, like, hey, if we have any quadratics, how many ways, look at all the ways that that sort of multiplies into this multi-degree. And then trim this basis down. So you're going to be looking over a smaller set because you know that certain things. Because you know that certain things will be in there already. So you can search over the matrix that we set up a few slides ago that can be made smaller. It's not very hard to do. We do keep track of that. So we will return a minimal variance. So yeah, you do this. You say, okay, I'm going to look at this multi-degree. I'm going to do my algebraic matrix. Do my algebraic Matroid check if it's full rank, then okay, just move on, and then otherwise, we're going to just put through a generic polynomial of that multi-degree and then find all the relations for the CIs that work there and then pick that basis. And then, oh, yeah. Do you have step via interpolation now? I mean, it's it's. I mean, it's setting up me setting up these matrices. So, this is the way you should read this: if I transfer P1, 2, P3, 4 through phi, then that's going to be this monomial minus this monomial minus that monomial plus that monomial. And then I'm literally just looking for linear relations about these columns. But yeah, you could definitely try to do so. But yeah, you could definitely try to do some interpolation. So these matrices get big, but the point is: if the multi-grading is big enough, they stay small for long enough to find some things. Right. And then, and then, yeah, so that's the basic idea. So for this phylogenetic model from before. This phylogenetic model from before, I maybe forgot to mention it was called the Kimura 3 parameter model. So this is a place where Grosvenor bases failed, and here are some of the results that we got. So instead of running it on a cluster, I just ran it on this MacBook, which is a couple years old. And so if you have four leaves, like the one in the picture, this is the one from the picture. If you look at the degree two, there's 2,080 monobials. There was a There was a there's a the grading rank was uh 13. What does that mean? The number didn't grading? This is the Z13. Z13. Okay, die multiplied. 13 residual tortoise. Yeah, okay. There were 17, 20 multi-degrees, and then we actually with the there's with the algebraic matriarch, you skip all the 12, and then you find 12 minimal generators. I see so most of them just have like a single control. Yeah, so your computers get so small that by the time you arrive at that point, it's not a computation you are a problem. Yeah, okay. And since it's the lowest degree, they'll all give you generators for this. Yes. And then, so if you then in the next degree, you say, okay, now there's 45,000 of them, it's a little bit bigger. You still skip. It's a little bit bigger. You still skip a majority of them. You skip 24,000 out of 25,000. It's pretty solid. And you find 64 extra minimal generators. So, of course, there are 64 variables. So in this computation, we are purposely skipping the ones that we already know. So if you take those 12 generators from before, multiply them by any one of these 64 q's, that would give you something of degree three that's in the ideal, obviously. Degree three, that's in the NPL, obviously. Um, but we're we're making sure we're filtering those out. So there you get 64 new things, and then you can try to do the same thing for degree five. So in degree five, or not degree five, but when the network has five leaves, so it'll be a five cycle instead of a four cycle. Obviously, it just gets much bigger. So now, in degree two, now there's 32,000 monomials. You skip 19,000 of them, and you find six. Skip 19,000 of them, and you find 648 that are all quadratics. And in degree three, I ran this on the cluster last night for Notre Dame's cluster, and I thought it would be done, but because this is something I have computed, but I apparently didn't write it down or save it. So I was hoping to fill out the rest of this table before this morning, but it's still running. It's unfortunate. But it's exciting. Yeah, so that was that one. Yeah, so that was that one example. And so at this point, is it first? You don't know how many components you'll skip either because you're doing the thing. Yeah, you're not going to know that anybody. I mean, it could be that this is faster if you speed up the work on each individual component. Yeah, for sure. Also, one thing I wanted to mention was: so, sort of, like when you're looking at this degree two, let's say you're looking Like when you're looking at this degree two piece, let's say you're looking at this degree three piece, right? So when you're doing the calculation, all the multi-degrees that are of like total degree three, those can all come out. So it. Yeah, yes. So, but I guess also that the expensive part is the, is you doing, is you doing some linear algebra? Or is it just the number of terms you? I think the, I mean, part of the expensive part is just expanding out, you know, these huge polynomials. Oh, I mean, like, um, like, just into sort of like, uh, I mean, yeah, that like, I think building this matrix and saying, like, hey, these are all the things that appear, I think that actually takes a second because when you plug this through phi, you have to do lots of, you know, you have to do lots of multiplications. And Macaulay too gets angry. Obviously, this is built in, but it's slow. Maybe that's not what you meant though. Maybe that's not what you meant, though. But I guess there's a lot of opportunity for Ron for speeding this up. Oh, yeah, absolutely. 100%. 100%. And again, this is, you know, for me, this is like sort of, this is very much just like a toy that you can say, like, hey, I have some new big polynomial app. And for whatever reason, I want to know what the kernel is. I think that there's a lot of structure in it. Think that there's a lot of structure in it, then you can try to do this if chrome rebases aren't working. So, the last thing I wanted to do is: so, here are some references. So, we have a paper about it. And then, obviously, Andrews Jensen, thanks for doing what you do. And this is the paper where they were looking at the K3P network and they tried that robot computation. But we haven't had three both age. So, this is publicly available. You can check it out, download all our code. So, Macaulay, the Macaulay 2 implementation will be available with the next iteration of Macaulay 2. So, if we download the newest version, that'll just come for free. If you can't wait for that, you can download it now. And in the paper, we did lots of examples. So, we looked at So we looked at the general Markov model coming from phylogenics. So this is really pretty silly. So you look at a tree that looks like this, and you have labeled leaves. You want to know there's some phylogenetic ideal associated to that. And you don't make any restrictions. So I swept a lot of things under the rug when I was here looking at this K3P model. There was tons of symmetry. Model, there was tons of symmetry. But this is sort of like the roots here, you can have any root distribution, and there's no restriction on the conditional probabilities to go from the root to any of the leaves. And so when you do this, it actually is the same thing as finding relations for the fourth secant of P3 cross P3 cross P3, which is a hard object to compute. Object to compute, um, and in fact, nobody's done it. So, if you can find that ideal and prove that you have the correct ideal, um, then John Rhodes will go into his backyard and catch you with salmon in Alaska. So if you're hungry, that's a good one to work on. Um, I think that that's their challenge has existed now for 15 years, yeah, exactly. They're a partial result. Yeah, I think Luke Oding has the current record. But he doesn't get the stamp. Yeah, he didn't get the stamina. But he's been like up to degree 11, I believe. And then they start in degree five. So it's already very hard. So we were pretty gung-ho at the beginning. And we're like, we're going to find some salmon generators. And we proved that there was nothing up to degree four, which is already known. Already known. But it's kind of sad when you plug it in, you wait three hours, and you're like, oh, I got sad. So, but that's okay. So, we did a slightly easier problem and looked at this model, but with three states, and then it's only a third secant, P2, press B2. We called it Sashimi. Same. This is the K3P stuff that we were just talking about. If you want to run this yourself, it takes, you know, it takes like 20 minutes. You know, it takes like 20 minutes. And then we also looked at this. I like this model a lot because this TM93 model, because it's not root-based. And so the coefficient ring is actually this C adjoined pi one through pi four, because you don't make any, it's very similar to a group-based model, but you don't make any restrictions on the root distribution. It doesn't have to be uniformly. Distribution. It doesn't have to be uniform like your data. So you do, you can, you don't have to do your computations over together. And I believe, oh, I guess I'm going to show you one more thing. I know we're ending early, but just in Macaulay 2, I want to show you sort of how this works. So let's look at. Yeah, let's look at this one. So here's a brand new parameterization. So this is where X, Y, Z, and W are joined. And that is in the correct alphabetical order. We conform the map. The first thing we can ask is: well, what is the grading for this range? For this random polynomial parameterization that I gave you. And it turns out that you can give x, y, z, and w these degrees. So 15, 10, 6, and 0. Does anybody have a guess for what it is? No, no, what the kernel is going to be. It's a famous Polyhot model. Anyway, it's okay. Very significant. He's hit by. Bearistine polynomial? Is it by? It ends up, it's not a bursting polynomial. It ends up, so this 15, 10, and 6. So if you multiply, I'll just show you. So it gives you a hypersurface. So I'm going to do not the breakdown. So let's say you, if you want to know, if you want to employ this thing, I called MaxGradian to find this, but you don't have to do that. Let's just say I want to find things up to degree five. I want to find things up to degree five and I do f. So I didn't have to call max grading or anything, it'll set that up for us. It's ecs. I don't have v6. So you do this and it returns a hash table. And so we can get the hash table. And there's this one polynomial. So you'll see for all these degrees, there's stuff under them. And then there's one minimal generator of degrees. One minimal generator of degree 30 file 5, and this is the E8 singularity. And obviously, like this is a small example, but if you do like 10, obviously it takes a little bit longer. So yeah, it's super easy to use. You just set up your map and then you call components of kernel up to a given degree and it'll just do it for you. So also, I want to mention that. Also, I want to mention that I mentioned it was parallel. Macaulay 2 does not have a parallelized version implemented yet. The parallelized version will be coming soon in Oscar, thanks to Anthony, who's helped Ben and I tremendously setting that up. So it'll be soon. Cool. And that's all I have for you guys today. So thanks for listening. Thank you. I have questions? Yeah. So I think the applicability of it should be a little bit more in a few programmes. If you, first, look in MAPMA, you can be image of a rational map there in a sort of a naive way. Also, Also, just you can do it with ductor bases. But if you're interested in just the generators up to a certain degree, stuff like the keyboard that just uses interpolation, that's often much, much faster and it doesn't do anything intelligent. Than do anything intelligent to cut down the space industry. Okay. Yeah, I mean, I think. Yeah, so I think the thing is, is that like for these problems that I've shown you, which I guess is only one really, with this phylogenetics problem, it works really well on those. And that's because there were things of low degree. Things of low degree. If you have things where, like, if you have like 20 variables and you want to find something of degree five, it's going to take like a long time. It's going to be pretty slow. So, like, this is, I think it's, I think it's best use case if you want it to be fast is if there are really low gigs though. But that's often the case. That's often the case. And I mean, I agree. So, for these phylogenet models, they're great. The quadratics are. They're great. The quadratics are often enough to tell you a lot of information. Other questions? Yes. So the columns in this matrix are all the monomials of a certain multi-degree set of it. Yeah. The columns are indexed by the monomials. By the monomials in a certain fault degree. So, what are you doing to compute those monomials? Yeah, so okay, so that's like it's so that this is equivalent to finding like lattice points on a polytope. So you're using a poly2's polydrip package? No. We're saying find all the degree D monomials, so and then sort them based on their multiple free. Okay, so if you had some like lattice point degree. Some like lattice points. It actually is like it takes you basically have to iterate for all those and keep track of you take the image on your map and keep track of everything that has that image. I guess you build a hash table for that. Yeah. And if you Macaulay do it, it's bad when you make mutable hash tables of size a million. Yeah. A million. Yeah. Doesn't like that very much. So. Well, I mean, if in general, when you bring up a working package like this, if it has a bottleneck because of the existing system, you find a way to call something that's more efficient for that calculation. I mean, in the end, somebody has to go outside of the system you're working in because it has. Yeah, I mean, I agree. I think. Yeah, this could be improved a lot, especially. Like, there are lots of places where Macaulay 2 is not the best option to use. Like, for example, this, you know, if we could use Polymake to find the monomials if this one would be great, that would be great. If this is a task you needed to do, you could write some software and just call the pieces you need when you need them. That's true. And then parallelize the box. Yeah, or you might paste to another platform that does it for you, but whatever change. Yeah, yeah, yeah. Apparently, I'm experienced, but I simply, yeah, yeah. Hey, but Macaulay 2. I like Macaulay 2. Feels like all. Yeah, feels like we even make Macaulay 2 so they can be easily called one of those character companies. Questions? Okay, well, let's thank Joe again. We have a very long break in 17 minutes or so. They'll put up coffee and have a cup after that. We'll start. And a half hour after then we'll start again at about 10:30. I guess if people haven't checked out yet, yeah, now you can check out, you can check out.