Yeah, thanks, Ren√©, for introduction. And thanks, organizers, for organizing such a wonderful workshop and having me here. And today, I would like to share some of my recent work on learning transition operators from fast space-time samples. And this project is originated from my PhD thesis. From my PhD thesis on so-called dynamic signal problems. And it is modeled by pioneer work of many engineers like Mark Mitanni, Yuan Lu, John Marie Bruce, and Piel Jokaki. They consider the system problem in the physical field. PDE, this physical field modeled by simple PDEs, linear PDEs, like heat equation or wave equation, and try to model, for example, try to model, for example, the variation of temperatures, like pollutions and pressure. And the place sensors at the physical field and collect data. And from the data, they consider both simplification problems and also trying to learn the government dynamics. And there's a lot of work scattered. And almost like 10 years ago, my advisor, Adubi, tried to come up with a unifying mathematical experiment. With your unifying mathematics framework to summarize this problem and put it in a more abstract reading. And since then, this has been a continuing interest following up dynamic slimming problem. And here is my group of me of collaborators I've worked with on the variance of dynamic stemming problem. And today, I will share my work with Jia Kuicheng, who was an IU student, and Christian Kumiril. And Christian Pamirio and my former colleague, and also Marma Jani, who is there. All right, now let me begin with introduction, like about some background on sampling, which is a classic topic in single processing. So the classic setup is that we have some annual function f, but we only have dispute samples, and we want to reconstruct f from the samples. So this is a From the sample. So, this is a year-post problem without putting constraints on F. And so, but in fact, it's mainly single-long interest actually has low-dimensional structure, for example, smoothness. Our classical setting is like F is in the primary space, for example, final width one. Then, if we take the samples of F on the integers, and we have a perfect reconstruction formula. So, this is called Hauchy-Shannon sampling theory. So, in the literature, for So in the literature for total function classes, one has to sample sufficiently in order to recover the function f. But what if f is unsampled, especially in the physical field, like in Hawkins talk, like in social applications, the sampling rate may be insufficient due to the constraints. But the one dynamic problem trying to work is that we want to leverage the temporal dynamics to compensate. Dynamics to compensate the insufficient damping density at a single time. So, suppose this dynamics is modeled by the solution can be represented in this alphabet form, and we can observe this evolving signal at multiple times. So, we asked the question that when do the core samplings taken at the various times contain the same information as the final sampling at the earliest time? At the earliest time. Okay, so we could have like very general formulation about a problem. And here is one: like we consider the initial value problem, like mu is the physical quantity of interest, and it's governed by the A, which is called the transition operator, which includes the physical law, and F is the external potential. And you can add more terms when the physical field has more complex physics. Complex physics. And so there's two types of problems here. One is that if we know the physics very well, like A is known, and we want to do time space trade-off in sampling. For example, a simple case, like you assume F equal to zero, this is simple linear dynamics, and we want to recover the initial value, u0, from a set of space-time samples. Say we sampled some certain space-time mesh, XI. Of course, we can consider. Of course, we can consider more sampling modalities. And the other problem is called system identification, where the A is also unknown and also the initial condition mu zero is maybe unknown. We want to collect a space time sample and we want to recover both A and Mu zero. So this is very related to the blank deconvolution, but we have a continuous time version. And also when he's And also when he's like example heat equation, so this is related to Professor Bona's talk on Monday. So, but we want to consider variant fitting. So, the research goals for us is for a theoretical side. We want to say, categorize the space-time samples for an idea such that this numerous problem can be solved. And we want to understand which dynamics are simpler and which dynamics are more difficult for this kind of problem. This kind of problem. And for the aggregate side, we want to given certain like of optimization criteria, we want to find basetime samples, particularly leverage the underlying physical law to optimize the data recovery process. And also when data is imperfect, we want to do estimation error analysis. And we're also interested in applying this model to some real-world scenarios. So these are kind of the framework. Of the framework, and today I'm going to mainly focus on the system education problem. Like, we don't know the end line dynamics and we want to learn them from data. So, I began to work with this problem with my student, with the undergraduate. So, I consider very, very simple settings, like, so we consider this finite dimensional affine dynamic system. dimensional open dynamic system. So it's quick, like xb plus one equals to a xb plus c, and the x not equal to some initial condition b. And we actually take omega t, which is the sampling of location for each time, and we want to this what kind of information about a we can recover from this partial observations and this time slap shop. So this is a problem we began to work on. And even the dynamics And even the dynamic system point, it already has a lot of applications. For example, this can be applied to land walks, right? It's transition and also diffusion process, like what we say in Volascot. And also in infectious disease modeling, many compiler models also allow such data space representation. And in the literature, people are interested in this operated estimation problem because this eigenvalues tell Because this eigenvalues tell information of a dynamic system, for example, about stability. And if the dynamics are, I define a graph, and actually, I mean, there's a special graph theory, this A encodes the structure information of graphs, so we can know like the relation between data pl. And if I wrote the literature, actually, in the methodology, people consider actually how to learn this A from a very long trajectory. And this is done in this paper. Actually, when Actually, when your list condition that you can learn this A from single trajectory is that this A, if acting on B, iteratively, and this should be spinal basis. So you have been, not every A can do this job, right? So it's kind of challenging to recover the operator A from a single trajectory data. But then more closely, we look at the harmonic retrieval problem in single processing. Actually, this is a kind of like a single interest. like a signal of interest, which in the spine of spine of IE to the AGS, those AGS frequency. And in the single processing, and people do the measurements on the integers, and if reforming it back to the dynamic setting, actually that means partial observation at only one node, but we can have a translational feature and acting on it. So this is exactly fitting our problem. So in this setup, like a So in this setup, like people are using so-called pronoun method to recover this E to the frequency because this is our eigenvalues of this A. So in spite of this, we actually consider that eigenvalue estimation should be feasible for almost all like all operators. So that's what we do. So this is the summary of our algorithm. So given a given like Here, like a dynamic system is evolving and a single trajectory. So, here is the eighth dimension. We only observe at the four coordinates and the consecutive incline. So, what we do is that we're mapping this data using a Hanko matrix and it shows up here. So, when this Hanko system can allow us to get in the analytic polynomial of A. And from this annating polynomial of A, we're able to recover the egg values of this matrix. Matrix A. So, our work is first we categorize the relationship between the sample location with the recoverable eigenvalues for arbitrary matrix A. We don't need any like constraints on the diagnostability or anything. And based on the Jordan decomposition, basically study how the omega zero interact with the invariant subspace of A, and so we can recover the corresponding values. Venues and after this research, I tweeted my students sent me a message because he was an engineering student. He only knows SDT. He said, Thank you for letting me know the Jordan ignoration will be useful sometimes. And then, for the algorithms, like, so they use the idea of crowning method to recover, to solve that system. And this idea has pursued by this. By this group of researchers here, but I found that cloning matter is not stable, especially when the system with the matrix is large and it's quite sensitive to noise. So in the literature, there are many kind of stabilized variants of the planning method and trying to increase the robustness. I believe maybe on the next two days, there are speakers talking about this issue. And we actually generalize the matrix candle. Generalize the matrix principle and each method and to this one, and it improves Prowning method a bit. But my observation is that, yeah, so this type of method is sensitive to noise. You can have even the numerical perturbations and the eigenvalue recovery could be very, very challenging. And especially, this is a kind of the similar phenomenon asolution. If the eigenvalues are very close, this problem is intrinsically difficult. And this algebraic method don't do a good job in separating these eigenvalues. And so we continue. So what is there? We have more data, right? We have like data from the multiple trajectories. So what we consider like now, we have maybe like M trajectories, XP1, XPM, and they corresponding to like multiple trajectories of the symphony and dynamic system, like X1. system like XP plus 1GA equals to AXPGA. And now we begin to observe, have a partial observations at this M trajectories. And a step further, so without loss of generality, this initial state for this M trajectories look like the egg vectors like EGA, the standard basis. And then this baseline observation actually, it says that we are actually observing the partial Actually, observing the partial observations of A square A cubed with A to quality. Okay, so based on this, we formulate the spatial temporal sensation operator recovery problem as follows. So this is our sampling set of like from omega 1, omega t are the other like the subset of the matrix indicator. And this one and t are kind of like observation time carry. So we introduce the monomial map Qt, like mapping this. map Qt by mapping this a to the direct sum of the powers of the matrix A. Okay, so A to the A to the A to the power T. And now by doing this, we are able to write the observation regime as follows. Like QTA is actually the big matrix we are trying to observe. And why is this? Now the problem is how to recover A from this Y. Okay, so the challenge here is that because the Qt is non-linear A, so this is a non-linear recovery. So this is a non-linear recovery problem. And also, we want to solve it even if A is for rank. For example, when it's a unitary operator where it's forranked. And if you look at literature, if 14 to 1, this reduced to the well-known matrix composition problem. And it is well-known that if observed entries are random and A is incoherent, then with this meaning of observed number of samples, which is, you can see this N is NDP. You can see this n is any dimension, r is the rank and log n, and um with hyperb, and one can recover a by um convex optimization problem. And now we're interested in the regime like t bigger than one. And we say whether we can recover a with possibly high rank, like full rank. From measurements, well, each time this omega t is kind of more than the degree of freedom for that matrix, but we combine them together and But then we combine them together and maybe at the same level of the degree of freedom for damage A, and then we recover on this A. This is something we pursue in dynamic sampling. So in other words, like can space-time samples be as valuable as samples at a single time? So similar to the matrix completion case, we consider two scenarios. One is that we make independent observation uniformly at a random from this set. And another one is consider Boludi distribution. And another one is considerable Ludi distribution is more like sample depthly for each entry, this IgA, which is matrix index and is time indices. Okay, so the idea we try to solve this problem is that I try to linearize. So we introduce this map, like the block ankle operator. So given this matrix direct sum, we use this mapping edge, mapping this edge for very structured block angle matrix. Like here, you can see we Like here, you can see we run for this x1, x2, x3, and you know, soton python. And now, um, observing the entries in the Dirac sum is equivalent to observing the entries of this voltage matrix now in the five-dimensional space. So the next few things to us to think about is that can we actually collectivize the right minimization problem for matrix and trying to do recovery? Actually, this is doable. So it's summarized in this theorem. It says that for any logical matrix H generated from this powers of A, like QTA, and then the rank of H equals to the rank of A. So the rank of A, even is M, which is not a low-dimensional product of object in the space, but now you need it in the high-dimensional space, that becomes low-rank. So that's why we can use the low-ranked matrix recovery problem to solve this non-linear To solve this nonlinear image problem. And another side, we had kind of considered the cover side of this, because the dynamic system can give rise to the low-rank blockchain conviction. What about the other side? Is any low-rank blockchain combat can be generated by dynamic system? So we consider this is a very technical condition, but we thought that, yes, by finding some certain constraints on this block, actually the answer. On this block, actually, the answer is yes, like those low-ranked volcanic matrix can be realized by dynamic system. Now, based on the theorem, it means that now if the d1 d2 bigger than 2 means that if we observe the dynamic system for a certain time, and the rank of this bigger blank hypermatrix is smaller than the maximum rank in the ambient space, and even if this rank of A equals M, we want to use this new rankings to realize the problem. To realize the problem. Okay, now, given the data, we want to find a good algorithm. Because now we need a like n times n dimension problem to a high dimension. So we want this algorithm to be like sample efficient and on the memory efficient. And in literature, there are actually some research on how to do the random minimization for the structure matrix. So what we did here is that we replaced this rank surrogate, which is a non-rogate Ranked surrogate, which is non-convex and non-smooth by a smooth version of the log determinant, because it's kind of empirical to observe that this surrogate is sample efficient. And then this is still non-convex problem. So we actually generalize the iterative reweight mid-square algorithm to solve, to transform this non-convex problem to a series of mid-square problems, and then iteration can work very well. Iteration they can work very well. And in the literature, this IRS can be used for mainly like a classical single processing problem, like here, like from the fast recovery problem and then to matrix completion. And now we generalize to the block Hanko matrix. Okay, so here is the theorem, and we only stated for the uniform sampling, and we also have a theorem for adaptive sampling. And so one of the theorems says that if the xx is mu zero. If the xx is mu zero incoherence and the number of random measurements satisfies this low bound, like here t and t plus one and t one and t two are how we actually reshape the size of the Henkel matrix. And the new and n is the dimension are the right, and we have log n. And with time probability, this IRS algorithm converge directly to the ground truth. But here, I want to say this is the local convergence. Say this is a local convergence result. It's not global convergence. So there are several remarks. First of all, that bound would be time dependent, but then we choose E1, D2, equal to T equal to 2. That constant is mean-mass. So this actually matching the empirical observations when people do Hanko embedding, and the square case actually leaves the best results. So somehow from this perspective, we get some explanation. And also, And also a very crucial thing that how this incoherence for the block handflow matrix is related to the incoherence of the original A. And so we did for that, I mean, for if A is the projection, the example P square A square to A, this is a static practice, and we can show that this actually true coincides. And also, the best practice is that when A is orthogonal matrix, like it's the And then we show that this incoherence can be smaller one. This is kind of the optimal we have achieved. This means that no matter how much P is, this N square, log N P over typical time samples, we can perfectly recover A. And also later, we use the numeric experiments to support our findings. And also, when A is positive data, we also have a close formula for the feasible solution for this interherence. For this incoherence actually is founded by the ratio of this mu zero A and the smallest positive egg value of A. And for the examples we run, we found out that almost like R n log n space time samples, so fast. And actually, we can also implement this algorithm pretty efficiently. And even though the theoretical radius of local conversions, they are floating with NP. In the numerical experiments, we use lateral. Experiments we use lateral initialization basically is like the based on what we observe and make i entries equal to zero and the algorithm actually converge pretty well. So we conjecture that maybe we can achieve global convergence for these cases. All right, so here this is the numerical results. Like we, oh, yes, this is kind of the first example we tried. It is orthogonal transition operator. It is the orthogonal transition operators. And so here we consider a matrix A, size of 15 and 50. And so here, this one, the degree of freedom, which is 250, right, because it's orthogonal. And this row is for the overseam ratio. And so this means that we take this 7,500 the space. 1500 the space time samples in total, but now we actually spread it out across different time scales. You can see in all cases, the recovery is actually close to machinery accuracy, and it actually converts pretty fast. And we also do the phase transition diagram for this one, and it actually matches our theorem pretty well. It has the dependence on the log n. And then also we test our like Our like our algorithm are either like this is a kind of low deflation to say whether this incoherence is a good indicator for the space-time sample complexity of this chemistry data. And here we consider a truncated fusion here. So this is the static case, and this is peak to four and p equal to seven. And this real nine is the degree of freedom, which is now exposed to magical matrix here. And this peak nine is Here and this pink line is uh is 1.5 rm. And we can see that, yeah, the semiconduct is linear in R as predicted by our theorem. And also for smaller rank of A, yeah, we need a few number of samples. And also, in this case, when the AA is low-defect future, it's kind of expected that when he goes and this phase transition boundary actually moves slowly. So still here. Here oh oh oh yeah, here we consider uh another case like we have the fast like the fast diffusion like the spectrum decay very quickly. And here you can see that this is uniform sampling. This is adaptive sampling. You can see in this case the uniform sampling didn't perform that well and the adaptive sampling may perform better. So this actually yeah also kind of predicted by Also, kind of predicted by our theorem that if I have a faster decay spectrum, then the mu zero of this volcanic matrix is bigger. As a result, it requires more number of space-time samples. All right, so here I want to give maybe a quick summary. So here we actually formulate this recovery of transition operator from the spatial camera sample. Actually, this, and we propose this. And we propose this iterative related display algorithm to solve this problem. And so it actually is kind of interesting to ask that whether we can leverage some ideas in classical sound processing, like using the low dimensional structure in the Sangunos, and trying to more like doing regularizations for this non-linear inverse problem. And also we want to understand like what's the intrinsic complexity about Complexity about the time series data generated from differentiation and using the incoherence at the bridge. And in the future, we want to explore the robust case and by finding the good regular results and extension to more generics, including the infinite dimensional case. So that's all about the work, and thank you for your attention. Oh, you mean why this method? So, first of all, this method is a little bit different. You know, in the second case, like when we have considered multiple trajectories, right? And in the first case, I only consider single trajectories. Like single trajectory. So it's kind of different. But here, I think maybe it's like we use in general, we feel like maybe the low rankless helps regularize the inverse problem robustly to the perturbations. That's kind of our observation. Yeah, I think it's transformed from algebraic methods to optimization method can help. Yeah, here, yes, so yes, we have like the dynamics involving like A and A square, like and we make observations at each time step. Yes, yeah, because the problem method, another one is that you need to make. method another one is that you need to make a second time observation so that's why you need can use the invariance structure of this data group and then using the and then that's planning how the planning method can be applied right and here you using cancer maybe you can handle the irregular time observations because it's still low dimensional object Curious about the result of the basement. Wouldn't there be some topological cases depending on the A matrix, like A being the identity? Probably a means of one. Yes. UA's identity, the rank node is going to be the same rank we take by block handle because oh, you mean I hear the the is here like the one all at entities. Ah, yeah. So it's not text. Yeah, it's matrix. Yeah, because we have multiple trajectories. So it's so energy matrix. Oh, I say, now we go to multiple trajectories. So now we can observe a kind of like, you know, it's going to be a matrix case here. Yeah, yeah, that's a very good question. Yeah. Yeah. Yeah. Yeah. All of this, like for this cases, this new zero, the formula is explicit. It depends on the eigenvalues and also the magnitude of the eigenvectors. So in that way, we can make a connection. And so for orthogonal matrix, this one can be taken equal to one. We can compute it. It's one. It can be achieved using one. And for the positive difference, and we also have like explicit formula. And here is a bound, but it gave us some insight. And for other cases, Inside and for other cases, we are not able to compute this new zero. We know it's there, but for certain cases, we can compute, but we don't have formulas. Yeah. And my other question is about: is the literature available in center control? Oh, yeah. So we are in different problem setup actually because in that they have a like the classical status base representation. So they have an input and output map like why see here. So here, because our sampling is different on different steps. So if you go back to the language, it's going to be state E times that unstate P is kind of changing over time. So, but I think the idea of the Hanko invading also appearing there when they're actually trying to. Also, appearing there when they're actually trying to recover the market parameters, they also use this hand-foll matrix stuff. Yeah, so they say it's fixed for each time. Yeah, yeah. So they actually, because in that, they already included this observation model there. So, yeah, so here our, because our sampling is different from that setting, but we kind of use, so we are in the matrix completion setting and there is like anchor matrix setting. Yeah. Like uncommitted cities, yeah, yeah, that's a very good question. Thank you.