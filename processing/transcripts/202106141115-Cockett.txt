has shared his screen. I'll go ahead and introduce him. Robin is the next speaker. He's going to talk about the FOD de Bruno construction, sorry, FOD to Bruno and SKU Enrichment. Right. You can add in construction as well. It was my fault, yeah. So as I said, probably JS should be giving this talk and I should have given a previous talk. So if you ask questions, JS will answer. So, um, oh, how do I get my slide to go? Ah, that's interesting. Since I shared it, it doesn't seem to want to change slides. Let me just yeah, Robin, so I found when I went to share the arrows didn't work, so I had to manually scroll up. Yeah, this doesn't work at all. Yeah, this doesn't work at all. Wait. So let me see whether there's something I can do. This is very embarrassing. First glitch. It had to be one. It's only appropriate that it was you. It's great. Let's see, whether this is very embarrassing. Robin, try page up and page down. Well, oh, there we go. There we go. We got something going. Okay, that's good. Okay, so I'm talking about the Fardo Bruno construction and skew in Richmond. And so here are the references. So it all started with a paper which I wrote with Robert Seely, and Robert will remember this well. But then, well, two things happened. JS actually wrote another paper in between, which we're going to completely ignore, where he made it much more algebraic and less combinatoric. And then he got together with Richard, and they related it to skew enrichment. And so I will talk about that. Talk about that. And in particular, sort of one of the issues that JS raised was the issue of whether a Cartesian differential category could be embedded in the co-Kleisley category of a differential category. And they answered that question, but in a very sort of Very sort of roundabout way in some ways, but and it used Richard's favorite technique, which was to enrich. But a step along the way, an important step along the way, was this paper by Clift and Murphy, where they talked about the co-free algebras and differential linear logic. And that led to a realization of the Realization of the relationship of that to the Far de Bruno construction. Okay, so let's see whether I can. So I always like to mention that Francesco Far de Bruno was an Italian of noble birth, a soldier, a mathematician, and a priest. And I always think that you won't get that combination nowadays. And furthermore, in 1988, he was beatified. 1988, he was beatified by Pope John Paul II, so he's Saint Far de Bruno, for his charitable work teaching young women mathematics. He was actually worked with Cauchy in Paris, and he was a tall man with a solitary disposition, and I really like this next bit, who spoke seldom and when teaching in class, not always. When teaching in class, not always successfully. And perhaps his most significant mathematical contribution concerned the combinatorics of the higher order chain rule. And that's what we're going to sort of start. So, and we're going to start with symmetric trees, because it turns out that symmetric trees are fundamental in the combinatoric. In the combinatorics of the chain rule. So the higher-order chain rule, so normally the chain rule, you just have two functions composed, and then you want to know what the differential of those two functions are. And it's governed by the chain rule. But then the question that people were asking at that time was, well, if you differentiate twice, how do you work out what the differential out what the differential of the composite is and if you differentiate n times how do you work out what the differential of the composite is and symmetric trees played an important role in in this so a symmetric tree is a tree but the order of the arguments of the tree doesn't matter so so you can flip the elements of the tree so so these Of the tree. So these two trees are equal as symmetric trees. And maybe what I can do is, yes, can you actually see my pointer? Yeah. So for instance, this thing I flipped into the middle here. Right. And then these three, well, are actually at the end here. And I flipped. Did I flip? Yes, I flipped X3 to the end. So this is the tree of... This is the tree of height 3. So if you start at the root and you just count the number of blobs going upwards, that gives you the height. And width 7. So that's the number of variables that are there. Okay, so if you want to construct a symmetric tree of height n and width r, so one little clever little technique of doing that is you start with the height n tree of Up with the height end tree of width one, and that's this thing. Oops, sorry. After not being able to wait, which way am I going? Help. Okay, hold on. A few more. Hold on a few more steps back and I should okay sorry about that. I touched something which obviously caused things to go forward rather fast. So what you can do is to build a symmetric tree of height n or width r, you can start with the one of width one and then what you can do is you can What you can do is you can add so here, you can add a second argument. And there's two places where you can do it, either in the upper blob or the lower blob. So you get these two trees. And then you can take those two trees. And so here's one of the trees. And then you can add a third thing. Well, if you add something to the first block. Into the first blob. Well, to make this height two, you have to add a blob onto here. So you get a string of things going down there. And then you'll add in X3 there. And then you'll take the second shape here. And this has now three blobs on it. And then you add to each of the blobs in turn. And that's the way, so you can build these trees, and it's a nice little A nice little construction to do this. And there are common authoric numbers associated with how many trees there are of a given width and height. And you can build a little table of these things. Okay, so the question is: what does this have to do with differentiation? And so we're going to build a category. So I'm going to start with a Cartesian left additive category. And actually, I'm going to write my maps in diagrammatic order. Not that you're going to be confronted with many compositions. But then I'm going to build a new category. And the new category And the new category is going to have, well, the objects the same as the old category, but the maps are going to be infinite sequences of maps. And I'm going to distinguish one particular map as peculiar. And this is the map which just corresponds to a map from A to B in the old category, in the left adjective category. And then Category. And then each Fn is going to be a map which in the first argument is not linear or additive. But in all the other arguments, you've got to be multi-additive. So you can think of this as and of course symmetric, right? And you can think of this as the tensor product of the A's. A product of the A's quotiented by the symmetric group, and I just indicated this by doing a slash n shriek. And you can think of this map as the nth higher order derivative, right? Because the higher order derivatives actually are multilinear in their arguments, but not in the position way of taking that. In the position way of taking that derivative. And that's okay, so we're going to look at symmetric trees of height 2 and width r. And then given two of these functions, we're going to describe what the composition of these two functions are. So, and to do that, I've got to tell you what the composition is that. What the composition is at star, and what the composition is at R at a particular index. Remember that the functions are sequences, the sequences of infinite sequences of functions. And so at the star, of course, what we're going to do is we're just going to compose. That's easy. But when you have our arguments, you're going to do this weird composition, and this is the Farde Bruno. Composition, and this is the Farde Bruno higher order chain rule that we're using. And it's a sum over all trees of width r and height 2. And what you do is you take a tree of height 2 and width, well in this case 3, so we're looking at Fg3, and you clothe it. And this is what this thing is. And this is what this thing is. And so here's the symmetric tree. And here are the functions. And you label each node by the function. So these are f's. And the number of arguments determines, so this would be f3 and this is f1. And because this has two arguments, this is g2. Now, g2 actually has three arguments, and one of them was not the non-symmetric. One of them was not the non-symmetric ones. And so, what you do is you take your x-star, you act by f star, and then you feed that into g2. And then here you feed the x-star into the f2 and g1. And you can think of this as telling you where you're taking the derivative, right? And so F2, you tell it where it's taking the derivative. You tell it where it's taking the derivative. So, and this turns out to be a nice associative composition. And when you put it all together, this gives you a category. And in fact, this is a functor on the category of Cartesian left additive categories, and we call it far. So, given a left additive category, you can build another left additive category called far. And furthermore, it's a comonad. So that means you've got something which takes you from far to far compose with far. And of course, you've got something which, given the Far de Bruno map, by taking actually just the first coordinate, Taking actually just the first coordinate drops you back to the original category. But it's also a differential category. And the differential is got by following with doing the delta. Now, when you do the delta, you see to F, well, you had an infinite sequence there. And when you delta it, you're going to far-far. And that gives you a square infinite. gives you a square, infinite square of functions, right? And to get the differential, well, you just look at the first column, and that's the differential. So then the question is, well, what does this delta look like? And it's not very easy. Very easy. It turns out to be very, a very sort of combinatoric thing. So I'm going to sort of just, I just want to give you the flavor of it because, oops, sorry, to give you the flavor of it. So in order to get the delta, what you've got to do is you've got to get something Got to do is you've got to get something which has two coordinates, right? So here in the sequence, you're getting each x is now labeled by two coordinates, and then you're going to apply an f to it to get the function which you want to get down to the b so and what what you do is you sum over all What you do is you sum over all the partial isomorphisms. So here's the FNM that you're constructing. And you're going to look at all partial isomorphisms from N to M and then you're going to do some huge, great, big sum where you're going to form these functions. And I'm just going to give you one more little insight into how. Into how these things are built. So, what I'm doing is I've got the x j's along here, the j coordinate along here, and the i coordinate along here. And where you get off the stars, you're just going to put the partial isomorphism between these two things. But the thing is, the partial isomorphism being partial. Being partial does not take up the whole of the set 1 through n. And so, what you're going to do is you're actually going to put in the elements that don't occur in the partial isomorphism with a star in here. So, when you go across to here and you see what the arguments of F and M are, they They they uh so you'll take the F which has um uh right arity this number of things where here you just have the partial isomorphism and then here you have all the things on the J row which weren't in the partial isomorphism and here or on the I row which weren't in the partial isomorphism. And then you apply F. And then you apply f of that arity to that, and that gives you the function at nm. So, right, this is just a very combinatoric sort of construction, and it gives you a Cartesian differential category. Uh right, so it turns out that every Cartesian differential category is a co-algebra for this comonad. And to actually see what it is, you know, with an f in this x, you've got to associate, well, a sequence of f's, and what you do is you just differentiate. Is you just differentiate repeatedly. And here I've written down in the term logic what that differential is. So F1 is the first differential, F2 is the second differential, where it turns out, of course, this turns out to be bilinear in the two vectors that you are going to take the second differential with respect to. Differential with respect to the directional vectors. Okay, so that gives you the Fardegruno construction that Robert and I developed. And I have to say, it was a big surprise to us at that time that actually every Cartesian differential category was actually just the co-algebra. Actually, just the co-algebra for this co-monad. Okay, so I'm going to flip now and we're going to look at the category of commutative monoids. So the objects are just commutative monoids. The maps are just morphisms of commutative monoids. And we're going to look at a co-management. A co-monad on this, and this is the Clift-Murphitt comonad, where you take an M, and what you do is you form the symmetric algebra on M and then you take the number of co-products of, and Rick described this, which has a component for each element in the underlying set of the model. In the underlying set of the monoid. And that's what you call Qm. And if you think about this a little bit, you realize that the elements of Qm can be written as pairs where this is in the symmetric product of the M. So here, this is a sort of word in the Sort of word in the n-fold tensor of M's, symmetrized. And this just tells you which component you're in. Okay, so the beautiful thing was that I guess Richard realized that there was a relationship between this and. This and the Far de Bruno construction. And I've sort of given it away by giving you this notation for what's happening. But what I've done here is just written out the co-monode structure for this particular functor. So in the epsilon, In the epsilon case, if you've got the empty string here, then you're just going to take the component, right? But if you've got just a one-element string there, you take the one-element string, and then otherwise you take a zero, and that gives you the epsilon. And for the delta, what you're going to do is you're going to take the sum over all the disjoint unions, non-empty partitions. Non-empty partitions of n. And then that takes you up. I mean, you're going to go up into the you've got to get a sequence of things in the Q of Q of M. So, and that's going to be an element of that. Okay, so Okay, so and the whole point is that you notice that if you want to describe a map, this is a great big co-product, you see, from this thing to F. Well, what does it look like? Well, for a particular component element and a particular sequence, well, it's going to give you an element of this. And the point is, it's going to be symmetric in the x1 through xn. X1 through Xn and non-and additive in each. So it's multi-additive here and symmetric. And then this doesn't have to be anything in particular. And the whole point is that these are exactly the elements that you need for the Far de Bruno construction. And so after a certain amount of calculation, one has Amount of calculation one has that the Fardo Bruno construction applied to, well, now I've got to say what this is. This is a very basic example of the left additive category. It's the community of monoids, where the maps are actually arbitrary set maps between the underlying elements. And that, of course, is a left additive category. And it turns out that the Fardebrunner construction there Fader Bruno construction there is the same as the Koch Lisley category on Q. So then the question is: well, okay, can we, so I've got the Farder-Bruno construction for a very particular left additive category, and I've got it as the result of a coma node. And so the question is, well, can I obtain far x for an arbitrary left additive category in a similar way? In a similar way? And the answer is: well, sort of. So, given the Cartesian left-handed category, you can consider the pre-sheaves, so functors from XOP into commutative monoids. And there's a Yaneda functor. There's a Yaneda functor, which, given the left additive thing, just carries an A in the left additive category to the homset of all the maps into A. And the left additivity ensures that this map down here is additive. And so it is actually of this form. I mean, it ends. Form. I mean, it ends in commutative monoids. That's the point. So each homset is a commutative monoid. And if you post-compose, well, that preserves additivity. So now, what you can do, of course, is you've got a queue on Cmon, so you can actually just compose your functor into Cmon. So I've done it here, I've got it Q hat. It here, I'll call it q hat f where you take f and you post-compose with the co-monad q. And of course, actually that gives you a co-monad on the pre-sheaf category. And then you can get far x out of that by taking the cottalized category on q, on q hat, but just Q hat, but just restricting to the Yanada objects, to the representables. And so now we have far as a subcategory of the Coche Kleisi category. And so that's great. And actually, that gives you a first embedding theorem. So if x So if x is a Cartesian differential category, there are faithful embeddings from x into phar of x, right? So because far is the co-free thing, that means that x actually embeds in there. And if you do an epsilon down, you just get x back. So that's just a faithful embedding. And then you can embed that into the code leisure capital. That into the code Kleislick category of Q hat. And so, and the whole point is this code Kleislick category is a differential category, right? And so in this way, you've got an embedding of X into a co-clystic category. So, but So, but it's not a perfect embedding because actually it's not a full embedding. It is faithful, but it's not full. And so the question that is rather obvious after that is you want to try and get a full and faithful embedding. And you have to work a little bit more for that. And that's what Richard and JS did. Richard and JS did. Okay, so I'm going to introduce you to skew categories, as often called skew monoidal categories, and in fact, I'm giving you a left skew tensor category or left skew monoidal category. And sort of amusingly, Sort of amusingly, when I was developing these slides, I was following the paper and I realized that I was writing composition differently to them and they used a right skew tensor category to do this. And of course, the terminology left additive and right skew sort of clash. Sort of clash, and if you turn around the composition, of course, that is the result of not using diagrammatic order. Because when we originally developed left additive categories, we were using diagrammatic order. So, but anyway, so I unscrambled it again for you, I'm afraid. Okay, so what is a skew category? Well, it's got a bifurcation. Well, it's got a bifunctor, and there are natural transformations. So it's a bit like a monoidal category, except these natural transformations are not isomorphisms. And then you have to ask, well, they satisfy various coherences, and they're going to be a bit like the monoidal coherences, except you had to worry about the directionality of all the transformations. Transformations. And right, and I've just written them out for you, so I'll give you a chance to just stare at them. But I mean, here's a typical one where you want, I mean, this is the point is that if you demand everything to be an isomorphism, you just get a monoidal category back. Back. And this is the Kelly MacLean. Oops, I've gone on. Sorry. Let me go back. Yeah, that's the Kelly-MacLean diagram. But here you need three diagrams in order to describe what the unit requirements are. And then, of course, you have the usual Maclean pentagon. Maclean Pentagon, except now everything is carefully directed. Okay, so every monoidal category is a skew category, but there's one important way in which skew categories arise, and that is if you have a monoidal category with a monoidal comonad, S, epsilon delta, you can give a skew structure by By defining x skew y to be SX tensor y. And then to get the associativity, well, what do you do? Well, you take this thing, you see, and then you translate it. And then you have to bump up the first thing, and then you use the monoidal map to encompass the tensor, and then you translate back, and that gives you the associativity, and then the units. Sensitivity, and then the units are similarly given. And this is what people call a warped tensor, because what you've done is you've sort of modified it by the monoidal comonad. Right, so here's an example, and I'm just going to sort of unwind it from. I'm going to sort of unwind it for you. So if you have a monoidal category with copars, which are preserved in the second argument by the tensor, then this induces a monoidal comonad on your category. So you see, if you start with V and you just take maps from I to V. From i to v, well, that gives you a set, and then what you can do is you can go back by taking the copar. So, what is the copar? So, it's here's you take a set, and this is the unit object, and all you're doing is you're taking a great big coproduct of the unit object as many times as indicated by that set. Okay, so let's just specialize this down. So, when you So, when you take cumulative monoids and you look at what this map is, well, this is maps from the natural numbers into your monoid. And that just corresponds to the elements of your monoid, the underlying set of your monoid. So this thing is the underlying map. And then this thing, you just take co-products of the natural numbers up. Of the natural numbers up. And that gives you a co-monad, a monoidal comonad on commutative monoids. And then, okay, so now you're going to look at the warp sensor for that thing. And what does it look like? Well, so just translating it out, what you get is the unit. The unit coproduct as many times as there are elements in X. But then the coproduct and the tensor commute, so you can pull the coproduct out. But then you can get rid of the I because tensoring up with the natural numbers doesn't do anything. So you just get x many copies of y. As the results. Okay, so as for monoidal categories, one can enrich in skew categories. So an A enriched, so if A is a skew category, an A enriched category X, where A is skew, consists of Where A is skewed, consists of, well, a bunch of objects. And for each pair of objects, you're going to assign an object in A. And for each map, sorry, for each object X, you're going to associate a map. I goes to the set of maps from X or the object of maps from X to X, which just picks out the. X, which just picks out the unit or the identity map. And for each X, Y, and Z, you're going to give a multiplication or a composition map. And the point is that you're using the skew rather than a tensor here. And then you've got to demand a whole bunch of coherences. Well, actually, there aren't very many. There aren't very many. But the main one, of course, is to say that the multiplication is associative. And I mean, the point is, you can still do it all with a directed associativity and with directed units. And so that just gets the idea of a skew-enriched category going. So I should say that if you have That if you have a skew-enriched category, you can always extract an ordinary category by considering the maps from the unit object into an arbitrary object. And that gets you an underlying ordinary category. Okay, so let's go back to that where we did a warped tensor on CMO. We did a warp tensor on C monoid and try and work out what actually an enriched category enriched with respect to that warped tensor actually is. And the point is, just looking at this, you're looking at the warp tensor for this. And what is it? Well, it's what you do is you take an arbitrary element of the first homset, and then what Homset, and then what you do is you take a co-product of the second hom set, and then you're going to take a map in there. But if you think about this, you see, it's going to be additive in the second argument, but it's not additive in the first argument because you sort of, this was just a set of things, right? And so actually, this ends up being just left additive maps. Maps. And so left additive categories are skew-enriched, and JS actually mentioned this in his talk. And here's how the skew enrichment actually works. Well, okay, so what about, you know, how do you get differentiation? Well, the answer is what you do is you look at the warped tensor. What you do is you look at the warp tensor with respect to Q, and I'm not going to go through this, I don't think I have time. But to give a Cartesian differential category is precisely to give a Q Simon skew-enriched category. And you have to say that it's got products, right? And in fact, I mean, it's still a little bit interesting because you don't have to have products now to define what a Products now to define what a Cartesian or differential category is. So, okay, so why is this relevant? Well, because for skewing, I'm not, I'm afraid at this stage I'm just going to tell you the story, but for skew enriched categories, there's a UNA dilemma into enriched pre-sheep categories. Into enriched pre-sheep categories. And if you go through that, carefully go through the Yonay dilemma and what's going on, you get out that every small Cartesian differential category admits a full structure-preserving embedding into the co-Kleisli category of a tensor differential category. And you're using all this enriched technology in all. In order to get that. Okay, so what did we do? Well, very quickly, I introduce you to the Farde-Bruner construction. And the whole point of the Farde-Bruner construction is it uses all the higher-order derivatives, but you could use it just formally on a left-added category. And the And then we flip to a view of the Fardebrunner construction as actually the co-classier category for a comonad, which was Q. And then that gave you actually the first indication, the sort of first embedding theorem, where you could embed a Cartesian differential category into. Cartesian differential category into a Cotelisley category. And then we looked at skew enrichment. And the whole point of the skew enriched story was eventually if you looked at what the Yanada embedding was in that situation, it gave you exactly the embedding that you were wanting back of a Cartesian differential category into the Kleisley category of a differential category. Category of a differential category. And right. And unfortunately, there are lots of details that you had to chase out behind this story. But the point is, I mean, the reason why we felt this was an important part of the story was because it was possible that there were Cartesian differential categories. Cartesian differential categories that couldn't be obtained or viewed as the codeleisle category of a differential category. And this closes that gap and says, no, actually, everything could be obtained that way. Okay, so that's the end of my talk, essentially. And the sort of slogan is. Sort of slogan is that all embedding theorems are actually UNADA embeddings, but you just have to find out where with respect to what sort of enrichments they are UNADA embeddings. And I'm just going to put up the references into the skew monoidal world. And probably the main paper is... Probably the main paper is this Campbell's paper on skew enriched categories. Okay. So how am I doing on timing? I'm really confused. You have two minutes left. So if you have something to say, please feel free. I didn't ask for any questions. questions so well so how about we'll thank you first and then we'll take questions so thank you robin for giving that introduction um there were some questions in the chat so i'm going to start there and i i want to apologize because uh alexandra asked this question and by the time that i read it you had moved on so i didn't interrupt you way back at the beginning of your talk when you're talking about um the trees used to index the pha construction you use Construction. You use a, when you're building new trees out of old ones, you use this dashed red line. And the question is about the significance of the dashed red line. It'll take a moment to get back, I'm sure. And I want to ask a related question. So right. Dash trendline. Yes. I've probably gone too far. I'm just flushing the buffer here. We'll go. Right. Yeah. So, so uh, yeah. So so I I oh gosh, damn it. Sorry. Sorry, if I touch the wrong thing, I go all over the place. But the idea with this way of constructing things is you start with the width one tree of height two. And then you want to build width two things. So what you're going to do is you're going to add this. Oh no. Sorry. You're going to add this variable. Sorry. Sorry, I'm just very sensitive to what I wanted to do was to allow myself to annotate. Let's see if I can do it. Okay. Yeah, so yeah, so the idea is you're going to add You're going to add x2 to these trees. And then the question is: well, how can you add that x2? Well, there are just two places where you can join. So the dotted red line is just indicating the two places you can join. But notice when you join this lower thing, well, actually, you've got to sort of extend this to have the same height. So you've got to add in another. So, you've got to add in another bullet to make it a reason in that one that you added in the second time that it's not dotted red? Oh, why didn't I dot this red? I could have indeed dotted in red to indicate what was being added. But I guess I was dotting just the thing where you joined into the first tree. Sorry. Could you say again? do say say again uh how does this these trees how does it relate with the uh the file construction or the this composition yeah sure we can we can still do that okay so you're all happy with these trees let's see oh no i've gotten got stuck okay see if i can unstuck myself If I can unstop myself. I'm wanting to change, to move my slides here. And oh, ah, I know what I've got to do. Sorry. I've got to stop. I've got to stop annotating. So myself back to the mouse. Let's see what I. Let's see whether that's better. I'm getting a sound. I tell you what, I'm going to do. I'm going to stop sharing and come back. I'm going to answer your question, I swear. Right. But it seems I'm having quite a lot of difficulty trying to Why? Okay, let me try again once more. I mean, it's weird. I can do it very easily. Yeah, I think. Okay, so this is the key step of getting the relation to the Fardo-Bruno thing and the higher order differentials. So each f is actually a sequence of functions. Let's see whether I can. Okay, good. So the point is F actually equals, well, F star, so that just tells you what it does as an ordinary map. But then you get all the So if you're starting with the differential category, so this is just the Differential category. So, this is just the ordinary map F. This is the first derivative, the second derivative, the third derivative, etc. But every time you take a derivative, you're adding an extra argument. So, if you have one of your symmetric trees and you want to do the composite of F and G, well, what is it? Well, what you do is you take Well, what you do is you take the sum over all trees of height 2 and width r, and I just had this notation f star g, it means that you're going to clothe this with the functions from f and the functions from g. But the point is with these trees, they have different arities. So when here you have an arity 3. An Arisi 3 thing, you put in F3, and here you've only got one, so you put in F1, and then G, you put in F2. But these things have one extra argument, which tells you the position where you're taking the derivative, right? And so the position plays in too. So you've got to use that. So the dotted red line indicates the position. Indicates the position where you're taking F1. So this is the first derivative, and it takes one linear argument. And this tells you the position, and this tells you the position. Here you have, this is the third derivative, so there are three linear arguments, right? But also there's a position. And then to get the position of G, what you've got to do is you act by F and then take the position on G. On G like that. Does that help? It does. I do wonder why you need higher height trees. It seems you only need height two trees. Oh, I see. Ah, right. Yeah, yeah. Lafar de Bruno does only use height two, right? Height n would be an n composition, right? But it's enough, obviously, just But it's enough, obviously, just to do binary composition to get everything going. Okay, right. But yeah, and trees are such fun. That's why, I mean, you can see the combinatorics very clearly of this. So I just went ahead and described trees of height, an arbitrary height, right? But it's sufficient to do height two to get the composition going. To get the composition going, and indeed, this is what Father Bruno actually did. So he did high two. So there's all kinds of things happening in the chat. I'm losing track a little bit. So I'm just going to open the floor to other questions for Robin. Oh. Well, I think it's the chat has gone off on its own, taken on a life of its own. So I wouldn't necessarily recommend trying to review those to look for. Necessarily recommend trying to review those to look for questions. But if somebody is involved in that that has a question directly for Robin, that would be great. Either that or someone could explain what's happening in the chat. I think the chat has been long enough that that might be a pub conversation at some point. Okay. The RAM cohomology, gosh. Yes. So, while we're waiting for maybe any other questions that are coming directly to you for that, you talked about, in the next slide, you talk about partial isomorphism of trees. And these height two trees, I like to think of as partitions. And I didn't understand your partial isomorphism, the partial isomorphisms. Is there a way to think about it in terms of partitions? Can you translate for me? Yes, yes. No, I understand the question. Now, I understand the question and I'm struggling to answer the question. I'll go to the slide by this trick of unsharing. Yeah. Yeah, it's this slide which is really wait. Let me share it. Yeah. Yeah, this is where I would like to ask Robert Seeley to explain how partial isomorphism suddenly popped out of the... Sorry, they... I mean, I was rereading this and I was thinking, how on earth did we work out that you had to use the partial sum overall partial isomorphisms? I think we barely figured it out at the time, Robin. Barely figured it out at the time, Robin. Yeah, and it hasn't got better. Yeah. It worked, just works out to be what it is, I guess, is the answer.