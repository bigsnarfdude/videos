I also want to say a little bit about my host institution, ICTS, which is located in Bangalore. Like BAM, we also organize a lot of programs and it is open to the international community to put together proposals. And there is a call on the deadline in mid-December. So I encourage you to propose them if you're interested. So let me now go to my active talk. As I heard, I'm going to talk about how. I'm going to talk about how one could possibly use the future observations of gravitationally lensed gravitational waves as a new tool for cosmology. And it's a work done by a number of people, several graduate students, Ankur, Shashwat, Somiti, Shauvik, Kausa, and many of my senior collaborators also. So let me give a couple of intro slides to the gravitational lens. Intra-slides, which is a gravitational lensing in a nutshell. As you know, general electricity predicts that any intervening objects between a source and an observer would curve the space-time around it. So we work in the so-called thin lens approximation. So we have a lens like a galaxy or a compact object, and its size is small as compared to the distances between typical distance between source and the observer. So you can treat these as like sheets of mass. Sheets of mass. And instead of going through this undeflected path, because some of these light rays or gravitational waves that would have gone this way would be bent by the gravitational potential of the lensing object. And introduce a couple of interesting things. One is that introduce a time delay. Basically, all the wave friends would observe a time delay as compared to an unlensed source. And this time delay has two contributions. One, you can obviously see because of the One, you can obviously see because of the larger geometrical length passed by this gravitational wave or light. There's a geometric contribution of that. It's as simple as to that. And apart from that, more importantly, there is a Shapiro delay just because of the mass of the lensing object. Light rays would take a longer time to reach here, and that is this contribution. And there is a whole theory of gravitational lensing, and you can think of these in two different regimes. In the most of the time, Regimes. Most of the time, and in the lensing of, in particular, in the lensing of light, basically so-called geometric optics approximation holes, where the lens mass is, the gravitational scale of the lens is very large as compared to the wavelength of a gravitational wave. So you can treat these basically as rays, that's why you get the name geometric optics or ray optics. And this can have interesting. And this can have interesting phenomenology. For example, you could get either a magnified or demagnified single image, which is called the weak lensing. In some very interesting configurations, you could have multiple paths connecting the source and the observer. It's called gravitational strong lensing. And here, the important thing is that while these multiple paths get produces images that are magnified or demagnified because they're each there. Magnified because they're going to be there. The waveforms are completely unaffected, just like geometric optics, geometric optics. And very uniquely, in gravitational waves, which is not observable in the lensing of light, you can have a situation in which a lens, the characteristic lens scale of a lens, is comparable to the gravitational wavelength. So, for example, if you have a black hole of a few hundred solar masses, its short-shared radius is a few hundred solar masses, which is comparable to the Mass which is comparable to the gravitational wavelength. And then you can have very interesting wave optics effects. So you have to sort the entire diffraction integral to compute the observed gravitational signal. So what you observe is a single diffracted image and in effect what will happen is that the waveforms are distorted. And you can compute this distortion effect, and these waveforms would look very different from the usual chirping signals that you would see from, for example, binaries. For example, binaries. And here is again a comparison of the waveforms in the geometric object regime. Here, for example, this particular lensing configuration, it generates two images. So there are two differently magnified images. You don't see any difference. They are basically the same waveforms except for relative magnification between them. And there's also a time delay. The time delay. And there's also a time delay. The time delay could be from minutes to months, depending on the mass scale or the lens that there are galaxies or clusters, etc. On the other hand, here is a gravitational wave that is lensed by a black hole, a point mass lens. And depending on the mass of the lensing object, you can get very different beating effects. For example, the black diffractive waveform looks very different from a usual chirping signal. And one could search for these effects also. These effects also, and this has been done. So, basically, one has to, in the case of the strong lensing, the geometric optics effect, if you take a, you know, the prediction that it will create multiple copies of the same signal that are the same shape and coming from the same location in the sky, but they look very different, they look the same except for relative magnification. So, given this data, G1 and D2, you can basically ask the question whether this, you can compute a likelihood ratio into. This you can compute a likelihood ratio in two hypotheses. One is that these pairs of events are produced by the lensing of a single merger, or these two pairs of events are produced by two independent mergers that are unrelated. So you can compute a Bayesian likelihood ratio between them. In the case of these wave optics lensing, which we've also got microlensing, basically you have only one single image, but you have two hypotheses again. One is that it's a deformed signal, because the microlensing by intervening compact object and a black hole. Compact object like a black hole, or other hypothesis is an unlensed image. And again, these searches have been done in the LIGO Brico data or the past several observing months. And in summary, we have no significant evidence of lensing either in the geometric optics strong lensing regime or in the wave optics microlensing regime. And even in this non-detection of lensed gravitational waves, only Are already producing some interesting or borderline interesting results in cosmology. One is, for example, the existence of primordial black holes or what fraction of primordial, the dark matter could be in the form of primordial black holes. And here are two constraints. This is basically the y-axis here shows the fraction of dark matter in the form of primordial black holes of different masses. As I said, the geometric optics lensing or strong lensing requires. Optics lensing or strong lensing requires very large massive lenses. So these are sensitive to primordial backwards of a million soil mass to a billion follow mass. The known observation of basically strongly dens events with small time delays, the absence of these things can be used to constrain the fraction of dark matter in the form of primordial black hole. So, for example, we can say that even if dark matter is in the form of primordial black holes of supermassive scale, they could not have formed more than They could not have formed more than half dark matter. Similarly, if the dark matter is formed of primordial black holes of mass hundreds of solar masses to 10 to the 5 solar mass, they could not be more than about 60% of the total of the dark matter. So these constraints are, of course, very modest. We have better constraints, better astronomical observations. But with time, with more and more events, these constraints get better and better. So this shows, for example, how these expected constraints. How these expected constraints would get better and better with the larger and larger number of events, assuming that we have not observed the lensed event. There are some carriages that if you're interested, I can talk about that later. So, this is what had been happening in the now, the non-observation of lensed capital base, but now we can get our imagination a bit wilder. So, we know that there has been very active proposals and ongoing programs to build the next generation of crowd. To build the next generation of gravitational detectors in Europe and North America. And these detectors would detect primary black hole mergers up to cosmological distances, detectors of several decades, even going up to 100. And they would be detecting millions of mergers over the operation time of about 10 years or so. And if you even factor in the modest expectation of the lensing optical depth. Expectation of the lensing optical depth. We expect thousands and possibly tens of thousands of these mergers to be strongly lensed by intervening objects, such as galaxies and clusters. And the detected, the precise number of detected lensed gravitation waves, as well as the distribution of the lens and time delays between these and scopes, contain imprints of cosmological parameters, as well as other things like the details of the dark matter, particle dark matter. So, this provides a new So, this provides a new way of doing cosmography by measuring distances and redshifts in the intermediate range, between the CMB of very, very high redshifts as well as supernovae at very, very low redshifts. So, redshifts of about 10 seconds through 10. These also provide new probes of particle diaphragm. So, I'll mention a few things as we go. So, the idea. So, the idea is the following. So, how does, for example, the expected number of lensed gravitational data? And this is the redshift distribution of mergers. And each redshift has a particular lensing probability, depends on the lensing optical depth. So, for example, this is one theoretical model, one population model of. One population model of how the binary black hole sources are distributed in redshift. And of course, depending on the cosmological parameters, these redshift distributions would change. And it shows, for example, how... Okay, okay about that. And this is the red line here shows our standard calculation of the lensing optical depth assuming a distribution of galaxies and clusters based on a halo mass function model. At this point, Model. At this point, we do not have a very good understanding of what is the rational distribution of binary black holes, but we hope that in the future, with a very large number of unlensed events, we'll be able to measure these things accurately. So that is sort of something that we would know from the large number of unlensed events that would dominate the data. The lensing probability again has a larger uncertainty because one has to either rely on cosmological simulations of how the back matter has. Of how the dark matter halves evolved with the cosmological redshift combined with other red magnetic observations. But this shows, for example, one results from one particular cosmological simulations of how the dispersion velocity of these dark matter halos, which is a proxy of the mass of dark matter halo, evolves as a function of cosmological redshift. So, for example, at very large redshifts, according to the hierarchical structure formation, there are only very light dark males. There are only very light dark matter hollows. At time, these hollows merge to form more and more passive hollows like clusters, and at lower entrance, it has a larger span of the split velocity or the mass of the hardware. And so that's how, and the way the cosmological parameters would come in, the picture is that both the redshift distribution of binary black holes as well as the cosmology. As well as the cosmological, so the lensing probability or lensing optical depth depends on cosmological parameters because this depends on the spectral formation in the universe, which depends on cosmological parameters, and the eventual astrophysics that would determine the spectral distribution of finite black holes also. So, they have an imprint of cosmological parameters in this way. And you have to also factor in the finite observing time, which would mean that you only determine if it uh you know basically all the kind of uh detection selection effects that would Detection selection effects that would determine an effective observation time. In the same way, the redshift distribution of, sorry, the time dail distribution of strongly lensed events also would have an impact on cosmological parameters. So basically, given a set of source parameters and lens parameters, if I vector lambda, you can compute anthropological parameters. You can compute the length in time delay, the time delay. Lensing time delay, the time delay between the two copies of the lensed events. For example, in the case of a simple lens model called a singular isothermal sphere, this is how the time delay looks like. The exact form doesn't matter, but essentially you can note that basically there are various distance measures, like these are the angular diameter distance, the lengths and the source, and between the length and the source. And between the length and the source, as well as there are also the redshift to the lens, also. And the relation between the distance measures and redshift depends on cosmologic parameters. So, and that's how by measuring the time delay, you are able to reconstrain the cosmological parameters. Same way, you can constrain the cosmological parameters in a standard Hubble diagram, where if you measure the redshift and luminosity distance, you can constrain cosmological parameters. So, given a set of astrophysical parameters like various lens parameters and source parameters and cosmopolitan parameters, you can compute the corresponding lensing time delay. And basically, now we have to marginalize these over these astrophysical parameters to predict the expected time deal distribution as a function of cosmological parameters. So, you don't marginalize over your astrophysical parameters, just basically induce this parameter this instead. As a parameter in this system. So, in summary, you can compute the expected number of strongly lensed events in an observing for time years using 3G detectors as a function of the Hubble constant as well as the matter fraction. So, you can see that if you increase both Hubble constant and matter fraction, basically that increases the number of Lenn stevens. And this shows the distribution of the time delay in log hours between Lenn Stevens. Big P length stevens for different choices of cosmological parameters. So, this is the actual distribution, and because you are assuming an observing time of 10 years, you only have access to a smaller fraction of these events. This is your detection of the selection effect. So, basically, you have some templates for the number of lensed events as well as their time value distribution as a function of cosmological parameters. And one can do a Bayesian inference to Bayesian inference to reconstruct the cosmological parameters given we have observed n number of length stevens and their time. And this shows, for example, how the constraints on the cosmological parameters, the omega m and the x0, from the observed number of benefit events and from the observed time-field distribution. By combining these two, you can measure these combined posteriors much more precisely, which is shown by these orange ellipses. LFS, these are 50% and 95% concurrence regions of these posteriors. And you can see this is comparable to the current constraints from the CMP measurement, but probing a different epoch in the cosmological evolution. So this is rather optimistic, assuming 10 years of observation of next-generation detectors. You could also, and it's sort of, so you could So, you could compare this with other gravitational wave-based measurements, like the binary black hole dark sirens and spectral sirens. And this is a paper by Shin Yu Chen and colleagues, and it shows that basically you can see that the constraints are comparable to what you expect from the binary black hole standard science. We also are exploring what is the prospect for near future. The prospects for near future, for example, the upgrades of LIGO and Virgo and the proposed Voyager concept. Of course, the experience concerns are much more modest because your event rates are much lower. But with about five years of observations of the A-sharp detectors and Voyager, you expect over 100 strong events, gravitational wave events. And with these observations, we can get some more. Observations: We can get some modest constraint on cross-possible parameters from these from lens sequence. The next aspect of this I want to cover is the ability of these lens observations to probe the nature of dark matter. And as you know, really the standard model of the dark matter is a cold dark matter, and there has been a large number of Labor experiments plus. Experiments plus astrophysical tests to look for evidence of these cold dark matter models without any success so far. So, people have proposed alternative models of dark matter particles. One is the so-called warm dark matter. These are lighter than the cold dark matter, but they also have a larger dispersion velocity. They are free streaming with a larger velocity, not exactly totalistic neutrinos, but somewhere in the tree. Relativistic like neutrinos, but somewhere in between cold dark matter and these highly relativistic candidates, like neutrinos. And what happens is that during this structure formation, because of these free streaming of these dark matter particles, it seems that they cannot form very light dark matter hollows. So it suppresses the formation of light dark matter hollows because they would just free stream and would not collapse to form these dark matter hollows. And this is shown, for example. And this is shown, for example, in this plot. This is the abundance of Dachmatter halos as a function of the Dachmatter mass. So the halo mass. And these solid lines show the prediction by the cold dark matter model, while these dashed lines show the prediction by the blue line is the cold matter model, and these red, green, and yellow are the prediction by the warm black wire model. By the barn-black matter model. And if you have a very light dark matter particle, then it basically suppresses the formation of these small mass hollows. So that basically changes the abundance of low mass dark matter hollows in the universe, which means that it suppresses the lower part of the timely distribution of the lensed events. So basically, if you compute, if you model the expected distribution of lensing time delays as a function of And lensing time delays as a function of the warm dark matter particles mass. If you go to Lorelei particles, basically, it suppresses the formation of these separation of these lower time delay events in our data. So this is the cold dark weather prediction. For example, if you go to the lowest warm back pattern model, it basically suppresses this lower time delay part in our. Part in our observable lensed time-devil data. And from this, one can actually constrain, put an upper limit or a lower limit on the mass of the warm dark matter particle or an upper limit on the inverse of this mass, the shown right here. These plots, this shows the expected constraints on the mass of the warm black matter particle as a function of the observing period from two years to ten years. From two years to ten years, and it so to give you a reference, the current constraints are actually much worse. It's about the current constraints on the warm, the mass of the warm black matter is about like six KB or so. I'm well about this one. So the future gravitational observations will be able to put a very significant stringent constraint on the mass of the warm back of my particle from. From these time delay and the number of lensed observations. There are, of course, several challenges. For example, we need to confidently identify this large number of strongly lensed events in the data. At least we need to model the contamination effect in our lens data sets very accurately. If we're able to do that, there is a possibility of marginalizing over these errors and to get robust constraint, the cost of Robust constraint, the cost of making statistical errors a bit wider. There are uncertainties in measuring the source population properties which would affect our inference. This is likely to be negligible in the X-ray detectors because of the large number of events that you would detect. We also need to model selection effects accurately. Some of these things are studied already in a recent paper by us, and some of these things are being studied in upcoming papers. The biggest challenge is going. The biggest challenge is going to be an accurate modeling of the population of lenses in the universe. And not only do you need to model the population like the Halloween function, etc., but also to get accurate models of the individual lenses also. And this, we will have to rely on cosmological and body simulations as well as take input from the electromagnetic observations also. This is going to be the biggest challenge in this observation program. Program. So let me summarize. I did not tell you the expected rates, but the expected rates of strongly lensed events is order 0.1% to 1%, depending on the horizon distance of these detectors. So the first observation of lensed gravitational waves will happen in the next few years, five years or so. And the first lenses are likely to be galaxies or clusters. And the next generation detectors would observe. And the next generation detectors would observe tens of dozens of strongly lensed gravitational waves. Their exact number of lensed events, as well as their time-buried distributions, depend on cosmological parameters as well as range of dark matter particles, which will give us an opportunity to measure these things from the observed number of tensed events and their time-believed distributions. And this is a new way of robbing cosmology. Let me stop here. Thank you very much. I just had a quick question. So you said in the next five years you will detect the red touring. So are we assuming that this are we going to do like a similar thing which we do using current dimensional detectors? Like do a mass filtering approach where we do parameter estimations and all sorts of things. So are you going to experience the same kind of challenges which you're experiencing currently or are you experiencing Currently, or are you expecting some other new ones to yeah, so essentially one has to compute these likelihood ratios of lens hypothesis and there are several challenges. It's being done also. One big challenge is that the number of your foreground would scale as a you know proportional to the observing time, while your background would go scale the square of the observing time because. Of those over time, because as we have a number of n events, the number of pairs could go as n squared. So, your false alarms could grow much more rapidly than your foreground. So, this is actually a big challenge in the lensing identification. There are different methods being developed, including Bayesian friends to better astrophysical priors of time delay, correlating with the electromagnetic observations, etc. It is an important challenge, it's being addressed at different levels. When you're getting down to sort of precision topology, you have to worry about assumptions like using SIS throughout. Yeah, so the SIS, again, it's a storm model. One really have to do the lens modeling much more accurately for doing a production analysis, absolutely. Leave them up for somebody else. No, no, no, no. So, one of my students actually One of my students is actually working on a project to improve the lens modeling. It's again not going to be the final answer, but it's the next step from the ASIS. There's a question on lens. Yep. Henriko? Hi, thank you for the very nice talk. And I have a two-fold question because you've shown estimates based on On next observing grants. So we expect from those a large number of lensed events. One asks, how does the base pattern or the lens versus unlensed case is affected by, for example, super important signals? Because those impacts are specifically, but then those signals are those signals are related with like independent signals that made up. And the second one is current generation detectors are like edge maintenance or scenarios in which meaning with these low probability of the latest events, you can detect some bias induced by those events or something like that. So it's either do we see something in the encouraging Do we see something with the current generation effect or the effect generation factors? Are those estimates optimistic? And there are also some challenges with superimposed signals and stuff like that. Thank you. Okay, I didn't fully hear your question, but I think the first question was about superimposed signals. I think the superimposition itself is probably not a big deal because as long as the time delay between the two preimposed events is The 2-3 post event is larger than the autocorrelation time of your template, they can be resolved as individual events. However, the very fact that you have a larger number of events would be a difficult, it would add to the complexity because I said the background would grow as n squared while the foreground would grow as only proportion to n. So this makes it harder to do the lensing identification. There are ways of Identification. There are ways of improving it. For example, the fact that you have a model for the expected time distribution of the length events means that you don't have to consider all the pair that exists in the data. You have to do a sliding window that would reduce the number of pairs that you compare against, et cetera. But admittedly, it is a challenge. One hasn't solved it, but there is ongoing work to address this. We don't have an answer to that yet. The second question I didn't quite hear. The second question I didn't quite hear. Did anyone spoke question? With generated packets, so with a current packet, so closer like the data release, I expect to see some effects of lensing even very small data sets, or edge cases, edge scenarios in which Wish perhaps you could type perhaps you could type the question. Take the other question. I was just curious, when you're doing your 3G forecasting, are you doing two 3G, 3-3G? 3-3G. And is that really kind of required to get the science through the site? Not really. So, because even you have single detector, you are not relying on sky location, et cetera. Just assuming, well, effectively, but because you're relying that all these lens events are correctly identified. That all his Ven sevens are correctly identified, or a good fraction of that.