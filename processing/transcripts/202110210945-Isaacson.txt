And hopefully, you can see my slides okay? Yes. Okay, cool. All right. Well, thank you to the organizers for inviting me. This has really been a fantastic conference. It's got all my favorite stuff, spatial, stochastic, modeling, and even cooler stuff that we're not doing yet, but I hope to do someday on changing geometries. And so it's really been fantastic seeing what everybody's working on. This is going to be more of a kind of methods talk about. Methods talk about kind of our program that we've been working on for a while to get convergent particle methods for doing reaction drift diffusion modeling in general unstructured type domains. So 2D domains, 3D domains, which we've had in the past, and surface domains is one of the things I'll talk about today. And in particular, the work that I'm going to mention was done by a bunch of very good graduate students who've worked with me. So the surface convergent reaction. The surface convergent reaction diffusion master equation method I'll talk to you about was by Jingwei Ma, who recently graduated and is now applying those skills at TikTok. So I really hope she's doing some particle simulation there, but my guess is it's more machine learning. Ying Zhang, who's now a postdoc at Brandeis University, has worked on the Drift Diffusion CRDME that I'll tell you about. And then at the very end, kind of in the spirit that Padmini was mentioning in her talk, I thought I'd show you something that we're working on now. I thought I'd show you something that we're working on now that's still very much work in progress, which is an interaction potential version that lets us do two-body interactions. And I should just say that all the data that I'll show in this talk, the imaging data, came from the lab of Carolyn Larabel at UCSF. Okay, so why I just wanted to tell you at kind of a high level why we're interested in these kinds of methods, even though I'm not going to be going into any specific modeling today, the kinds of models we're interested in. I mean, at a high level, We're interested in. I mean, at a high level, we've been working in recent years with Omer Dushik's lab at Oxford to try to understand how T cells integrate signals coming from foreign antigens and how that guides their response. And if you try to think about T cell signaling and other types of cellular signaling, there's all kinds of components, of course, that come into play. So for example, here on the right, I show a little picture of a recent paper of ours where we were modeling CE28 and PE1. And so in these systems, you've got protein receptors that can move and interact, react. Move and interact, react in the cell membrane, but then you also have enzymes that are diffusing and reacting in the cytosol, and they can attach the tails on the receptors and interact. So, we have mixes of 2D and 3D interactions. We may need to account for drift because there may be actin flows and other forces that actually can bias the motion of proteins inside the membrane. And of course, inside the cell, we've seen lots of talks on microtubule transport and actin transport this week. And of course, when you start looking at these things at this single When you start looking at these things at a single cell scale, one thing that you wonder about is the effect of geometry. So, if you look at the surface of a T cell that's trying to form a synapse with a B cell, which is what I'm showing in EM imaging here, you see you have all kinds of these long micro-VI type structures that come out and form contacts between the cell, and you have kind of a very complicated membrane surface geometry. And so these kinds of signaling processes, at least at the early stages of the signaling, are taking place in membrane geometries with very high curvature. Membrane geometries with very high curvatures. And so the question is: what kinds of impacts could that have on the chemical kinetics and dynamics? And of course, if you go inside the cell, someone was asking, I think, earlier in the week about, you know, what about organelles? And of course, organelles are also very complicated, and that's something I've worked on on several projects in recent years. This is just an example of a human B cell showing a reconstruction of X-ray CT imaging showing stuff in it. So here you see the nucleus with different Nucleus with different types of compaction states of DNA. In gold, you see mitochondria. In purple, you see Golgi. And if we come back, you'll see the ER added in too. And so kind of getting to that question that someone, maybe it was William, had asked earlier in the week, you know, of course, there's all kinds of complicated structure inside the cell too. And so if you're trying to do complicated 3D modeling of things inside the cell, you have to wonder about what effects these structures might also have on the signaling dynamic. Have on the signaling dynamics. So we've been trying to work on developing what I call spatially continuous particle-based stochastic reaction diffusion models that can kind of let us account for these geometries and study signaling in these types of geometries. And today's talk is going to focus on how we get numerical methods to approximate these models. But just to kind of start with, the basic model that we work with usually is what's called the volume reactivity model. And it's a very simple Model. And it's a very simple model. Particles will be represented as point particles. So we'll represent proteins and molecules as point particles. We'll assume they move by Brownian, continuous Brownian motion, or later in the talk, drift diffusion processes. First-order reactions will just be that molecules have some internal probability per time, so they have their own Poisson clocks. And the place, the thing that really distinguishes the volume reactivity model as opposed to Smolachovsky or Colin Kimball's models is that we'll assume that two molecules are reacting. That we'll assume that two molecules or reactants can react with some fixed probability per time based on their current positions in space. And special cases of this are the DOI model, where molecules react with a fixed probability per time when they're within some fixed reaction radius. But we've also derived and used Gaussian rates for interactions between membrane-bound tethered signaling processes, where you can kind of justify that from coarse-grained polymer models. So, just for those, probably all of So, just for those, probably all of you are familiar with some of these types of models, but just to show you what these kinds of simulations look like, don't really worry about the reactions. Here, I've got blue particles that can on their own spontaneously convert into red particles, and red particles can convert into blue particles. And red and blue particles that are sufficiently close have some probability per time, in this case it's based on a Gaussian in their separation, that they can annihilate essentially. And so you'll see blue and reds that get close enough, often one of them will disappear. Enough, often one of them will disappear. I guess I picked a bad one there, but if you focus in up there, you just saw one disappear. So, this is just to kind of give you a feel. These are the kinds of models we're looking at and trying to develop simulation methods for, but ones that can work in more general geometries with more physics than what's present here. And this is just, I should say, this is a movie that Ying made. I think she's watching today for our earlier biophysical journal paper. Okay, so that's the kind of basic. Okay, so that's the kind of basic volume reactivity model: molecules diffusing around and reacting with some probability per time based on their positions. I just wanted to show you our numerical approach for approximating them because that'll get into what I want to talk about today. So our basic approach is to discretize these kinds of continuous volume reactivity models and space onto a mesh, like I'm showing here on the right. And so for an unstructured grid case, usually we would use triangulated meshes, but you can do these things using Cartesian grid meshes too. Things using Cartesian grid meshes, too. And then we approximate the diffusion or drift diffusion of the molecule by continuous time random walks of the molecule hopping on the mesh. And in our CRDME or convergent RDME approach, bimolecular reactions can occur with some probability per time based on the position of the molecules. So here I'm showing A plus B goes to C reversibly. And based on the positions of the A and B molecules, there's some probability per time that the red and the blue can re-associate to form. And the blue can re-associate to form the purple. And then there's some probability over time it can split apart into them. And so, this is something we've been working on for a number of years. And we have several existing papers on how to do this on unstructured planar domains and unstructured volume domains. So, you can see those earlier works. This movie of Ying's is, I think, based on stuff we showed in this 2018 paper. But I did want to show we can also do this in 3D using real geometry. So, this is here is a geometry. So, this is here is a geometry of that B cell that I showed you earlier, where we've now mapped it onto a set of voxels. So, this is actually a Cartesian grid simulation where the geometry labels excluded voxels, but you could do an unstructured grid version too. I'm not showing the cell membrane, but I'm basically running a simple annihilation reaction inside the cell. So, everything is in the cell, and you can see the molecules moving in and out around the organelles and reacting. I'll let this one just go for a minute just to hopefully. I'll let this one just go for a minute just to hopefully convince those. I know a lot of you have seen these movies before, but just for those that haven't, I wanted to kind of give a flavor of what we worked on previously. So, okay, and so as time goes on, you can see we have fewer and fewer molecules inside the cell as they find each other and annihilate. Okay, so before I tell you about what we've been working on, I just wanted to, I often get asked, well, why not use Brownian dynamics or why not use the reaction diffusion master equation to do these kinds of models? And so I wanted to tell you. Models. And so I wanted to tell you at least briefly why we choose this approach. So with regards to Brownian dynamics methods, they can approximate the same exact spatially continuous models. They just differ by discretizing in time instead of in space. And they're very easy to code up in simple cases if you don't try to start making more complicated data structures to optimize, say, interactions. And they work really well if you have very large open regions because you can take very large time steps and still simulate the diffusion exactly. Still simulate the diffusion exactly. Where they get more complicated is if you have complex geometries, then the geometry is naturally going to limit the size of the time step you can take because you don't want your molecule to diffuse out of the domain. And so at that point, it's not clear they'll be more efficient than a lattice model, just in terms of the constraint that's introduced by the geometry itself. And it can be challenging if you want to, say, preserve detailed balance of reaction fluxes when you have drift, something we'll show occurs relatively naturally in these lattice models. It can be challenging. In these lattice models, it can be challenging to do that in Brownian dynamics methods without introducing kind of extensive, or sorry, not extensive, expensive equilibrium sampling procedures to kind of preserve these kind of physical properties. And similarly, if you try to use them on surfaces, very often the standard approaches, not all approaches, might require you to diffuse in the plane of a triangle and then project back once you've gone out of that plane. So you might have to introduce kind of expensive projection techniques. Techniques. In terms of the RDME, the RDME is actually completely the same, the reaction-diffusion master equation, to the approach I'm going to show you today for diffusion and linear reactions. The only place it differs is in bimolecular reactions. In the RDME, they only occur for molecules at the same site. So that makes it a bit more efficient to simulate than our C-RDME approach we take. But as I've talked about many times before and told many of you about before, it means that you lose biomolecular reactions if you try to take a continuum limit, and that can make it hard. Take a continuum limit, and that can make it hard to figure out what size mesh to pick to have confidence in your simulation results. That is, to know your model is resolved. And I just wanted to kind of point that out in one last slide before I go on to the new stuff, because I think especially in 2D, this effect becomes very apparent. So this is just kind of from older work of ours where we're looking at one A and one B molecule diffusing around in a periodic square until they annihilate. So they have some probability per time they react once. Some probability per time they react once they're close enough. It's a delay model. And I'm plotting the survival probability, so the probability they haven't reacted, sorry, the probability the reaction time is bigger than t versus t. And the only thing I'm changing between the curves is the mesh size. So here in the legend, larger values are smaller meshes because the legend is inverse mesh size. And so what you see in these RDME type models is as you change the mesh size, there's absolutely no mesh size where the simulation stabilizes. The simulation stabilizes. This survival probability just keeps shifting, kind of almost by a roughly constant amount. And so, for this type of problem, it would be very hard to say that there's any particular mesh size that makes sense to use in an RDME because your result never really seems stabilized. This is not as apparent when you look at things in 3D. And I've myself used the RDME successfully in modeling many times, but it's always something to be aware of. And it's why we use the CRDME approach, which you can see on the right, does converge. Which you can see on the right does converge as we change the mesh size. It gets around this kind of difficulty. Okay, so that's it for my introduction. So, what I wanted to show you, I guess, in the remaining 15 to 20 minutes is how we can actually now try to build a surface CRDME for reaction diffusion processes. And we're going to basically follow the approach we used in our 2018 Journal of Chem computational physics paper to do planar and volume domains, but we're going to do it on a surface now. In terms of how we Now. In terms of how we handle the diffusion on the surface, it kind of adapts the unstructured grid planar or volume method that goes back to work by Stefan Engblum and collaborators at Uppsala. And so the idea is to focus in on first, suppose we just have one molecule diffusing on a surface, and we want to try to approximate its continuous diffusion by a jump process. How can we do that? So let's write down the diffusion equation for our particle on the surface. So here at P is the probability density, it's at a point X. Density, it's at a point x at time t. And this is our Laplace-Beltrami operator. And D is just throughout this talk, the particle's diffusivity. So the basic approach we'll take is we'll assume that our surface approximation or mesh is a triangulated approximation. That's often how we get our imaging data from our collaborators. And what we're going to approximate our random walk on is that we're going to construct a dual mesh, or what's called a dual mesh to the triangulated surface by connecting up for each triangle here in red. Up for each triangle here in red, the centroids of the triangles to the center of the edges of the triangle and forming a collection of polygons. So this gives us in black a new polygonal mesh. And we're going to try to now define a random walk, continuous time random walk of our molecule hopping between these black voxels across the various faces. And just notationally, in the remainder of this talk, anytime I write vi, I'll mean a given voxel of this dual mesh, and xi, I'll mean Of this dual mesh, and XI, I'll mean the centroid of that voxel. So, how are we going to approximate the continuous diffusion by a continuous time random walk? Well, we're going to use just a standard piecewise linear surface finite element method discretization. So, we use piecewise linear elements and we do the finite element method on our surface. Then we get a set of ODEs of this form. This should be a regular derivative instead of a partial derivative. So, here pH of xit is the probability density. XIT is the probability density of the particles at point Xi at time t. M is the normal symmetric positive definite mass matrix. S is your normal symmetric negative semi-definite stiffness matrix. And the one problem you immediately run into is that this won't define or describe a continuous time random walk on our mesh. And I'll talk about why maybe in a minute. But there's an easy way to get over to overcome that difficulty, which is to use what's called mass lumping. And so the basic Called mass lumping. And so the basic idea is to replace this mass matrix by a matrix where you sum across each row, and you make a diagonal matrix whose values on the diagonal are the row sum of m. And it turns out that doesn't impact or change the order of accuracy of the approximation, but that makes everything work, basically. And there's a nice geometric property that the diagonals now just become the areas of those dual mesh elements. So they have a nice kind of geometric interpretation. So in the remainder now, So, in the remainder now, absolute VI will be the area of the voxel. We'll convert from the probability density, my particles at the center of a voxel, to it being anywhere in the voxel by just taking the probability density, it's at the center and multiplying by the area. And we can write an equation for those probabilities, and it has this basic form once we replace m with the mass lump m. And the convenient thing here is that this S m bar inverse matrix defines a transition rate matrix. Defines a transition rate matrix, or at least I'll tell you, it generally defines a transition rate matrix. So we can interpret this as describing a continuous time random walk where the ijth entry just gives us the probability per time the hop from voxel j to hop voxel i. And so we can kind of write this as a chemical reaction with this rate. So it's a relatively straightforward procedure, and this works on planar domains, it works on surface domain, sorry, on volume domains, and as I'll show you in a minute, it works on surface domain. And as I'll show you in a minute, it works on surface domains too. Okay, so that's how we handle the diffusion. How do we handle the reaction? So, really, it just comes down to how you handle bimolecular reactions. So, if you just consider a Siri always likes to interrupt me while I'm giving a lecture. So sorry about that. But suppose we consider the simplest case of just one A molecule and one B molecule that can annihilate. If you can do this reaction, you can do any bimolecular reaction. Do any biomolecular reaction, the approach is the same. And non-zero or first-order reactions are kind of trivial. So we can write down an equation for the probability density in our continuous model, the volume reactivity model, that the A molecules at X and the B molecules at Y at time t. We just get diffusion of the two molecules, and we get this sinc term where this kappa plus function is the probability per time the molecules react given their positions. So in the doi model, it would be an indicator function when they're close enough. Function when they're close enough. So now we have to discretize in two-particle space. So I take the voxel that the A particle's in, the voxel the B particle's in, and I defined a two-particle voxel, Vij. So I can calculate an approximation to the probability the particles are in those two voxels respectively by just making this kind of piecewise constant approximation. And if you plug that in and integrate both sides of this equation, ignoring the diffusion term, you just get a simple ODE for the change in the probability that. For the change in the probability that one particle is in voxel I and the other particle is in voxel J due to the reaction. Where here, this kappa plus ij is the probability per time the molecules react when the A is in voxel I and the B is in voxel J. And it's just a kind of nice average over the two-particle voxel of this rate function. So now putting this all together, what we've derived on our surface is a method for simulating the particles hopping around and reacting with some probability per time based on their position. So we can lump all that together. If I have many particles, if I let AI be the number of A in the I-thoxal and BJ the number of B in the j-th voxel, then I could simulate this simple example of many particles diffusing around and reacting through this set of reactions. So I have a reaction that a molecule of type A at location J hops to location I with this transition rate, type B hops from location to J to location I with this transition rate, and an A at location I and a B. Transition rate and an A at location I and a B at location J react with this transition rate. And like I said, you can extend this to general second, first, second, first and second order reaction systems. And with a little work, you can get it to do work for reversible reactions. There you have to think about where products get placed, but it's not too significant of a complication. So before I show you just a movie, what one of these simulations looks like, I just wanted to tell you a few properties. So the first thing is you might wonder, you know, why do we know or when You might wonder, you know, why do we know or when do we know that this discretization of the diffusion equation on the surface will actually give us a continuous time random walk? And it turns out actually that as long as your surface satisfies something like a Delaunay condition on the mesh, so in the planar method developed by Engloom, as long as you had a triangulated mesh that was Delaunay, then you would get a transition rate matrix. It turns out that on the surface, it holds two. You just need kind of an analogous. Holds too. You just need kind of an analogous Delaney-like condition. So basically, under the same types of meshes where it works in the plane or in 3D volumes, it works on surfaces. In our numerical testing, the method looks second order as you shrink the mesh size. And you can do this in a way that you preserve detailed balance for reversible reactions by appropriate choice of unbinding reaction rates, essentially. So let me just kind of show you one example of what this looks like. So this is actually a T cell. This is actually a T cell that our collaborator, Carolyn Larabel, did an X-ray CT imaging of. So, this is a surface mesh reconstructed from it. And I'm going to just show you a simple pattern formation example because, honestly, just because it makes a cool movie, this is not a model for any real T cell signaling process that occurs on the surface. It was kind of just something we did for testing. And so, in this kind of Boras, Pearson, Mansur pattern formation system. Pearson-Mansoor pattern formation system. I'll show you the, I believe it's the concentration of U inside each voxel on the surface of the mesh as time goes on. And keep in mind, this is a full particle stochastic simulation using the method I just showed you. And so you see, initially, it kind of oscillates from a low to high state. I believe blue is low and yellow is high concentration. But after a little bit of time, you see a pattern will kind of lock in, and you kind of get the formation, this pattern formation kicks in. And I thought this was kind of a cool example. And I thought this was kind of a cool example because these things are often studied in the plane and in 3D domains. And if you put it on the sphere, it works. But what this example shows is actually, no, it works on a T cell too. And I'll just go to a longer time just to show you. If I kind of zoom it out quite a lot, because it's a long movie, this really, this pattern is quite robust here. It persists for quite a long time. Okay, so that was how we could adapt these CRDME types. So that was how we could adapt these CRDME type approaches to let us do surface reaction diffusion. What I wanted to show you next is how we could include drift due to a potential field, as might be needed to model the effective actin flows or microtubule transport or so on and so forth. And the idea here is to get an unstructured mesh method that would hopefully work in planes and volumes and surfaces in the same way for reaction drift diffusion. So, here again, we'll take the same perspective as if we can work it out for one. Perspective is if we can work it out for one particle, that'll basically tell us how to do it for general multiparticle systems. So now we start with a Fokker-Planck equation for our particle where phi is our potential that's driving the particle's drift. And we ask, how can we discretize this to get an unstructured grid CRDME? And the first problem we run into is that if we reuse kind of just a simple piecewise linear finite element matrix, finite element method approach with mass lumping, the stiffness matrices you get may have. The stiffness matrices you get may have negative entries and off-diagonal entries, which means you would have negative transition rates, which of course makes no sense for defining a jump process. And that ends up being an issue even on very nice meshes. So it's not something that says if your mesh is nice enough, this won't happen. Even on nice meshes, you can get negative transition rates. So the question becomes then, how can we get around this? And in the lower corner here, I've kind of given the hint away. But what we're going to do is use a method that I guess one would say is now classical. One would say is now classical, but isn't very widely used, I think, in this context, which is called the edge-average finite element method. So it's kind of a hybrid finite element, maybe finite volume type approach that, as I'll show you in a minute, involves averaging over edges. And as we'll see, this actually works quite well at giving us transition rate matrices and very nice expressions for what the hopping rates should be. So if we use this edge average finite element method just to show Finite element method, just to show you what you end up with for your new stiffness matrix, it's actually kind of a very, I think, elegant formula. So if here Sij is our stiffness matrix we would get for a purely diffusive problem, so what we would have got by the Engblum method or what I showed you on the surface before, the drift diffusion one that you end up getting has this basic form, where here EIJ is the edge connecting node I and node J. And so what you end up getting is the diffusion hopping rate. Up getting is the diffusion hopping rate essentially modified by this inverse edge average that involves these kind of Gibbs-Boltzmann type distributions. And so that defines an effective hopping rate for molecules that are undergoing drift diffusion to hop around on our mesh using this new matrix. And it has a lot of nice properties. So it's provably first order in the H1 norm. In practice, we see second order convergence for the examples we've looked at. But I will say that I don't know if we've as thoroughly explored the drift domination. As thoroughly explored the drift-dominated regime as one can. So it may be if you have really strong drift that it's only first order. If you're familiar with kind of some older methods by Wang, Elston, and Peskin, or that Charlie Peskin and I used in our PNAS paper, this basically looks like an unstructured grid version of them. So if you just discretize this integral with, say, trapezoidal rule, you recover the cart, you get something like an unstructured grid version of a Cartesian grid method that Charlie and Version of a Cartesian grid method that Charlie and I used in earlier work. And one nice thing about this is that the method preserves detailed balance of drift diffusion rates in the absence of any chemical reactions, which means you get a discrete Gibbs-Boltzmann distribution for the equilibrium distribution of the particle. You can extend this to include reactions in a similar way to what I just showed you. The formulas are basically the same, and you can do a little bit of work and modify things so that you can actually. Modify things so that you can actually, even with the potential, still preserve detailed balance of reaction fluxes for reversible reactions, which is something that my understanding is much harder to do in Brownian dynamics type simulations. But it kind of comes out just naturally from the discretization for this type of method. So just to kind of show you an example of what one of these simulations looks like, I know there's a lot of experts on T cell signaling in the audience. So please don't take this as a realistic model. Please don't take this as a realistic model for T-cell synapse signaling. This is just the toy model to show off the method. It's missing a lot of features. We adapted it from this paper from a few years ago, but we just kind of took some drift diffusion and reaction components. It doesn't have everything there. But the basic idea is we're going to have a circular patch of membrane. And then outside a central circle will be T cell receptors that can bind to major histone compatibility complexes and get activated. And so the color you'll And get activated. And so the color you'll see will be the concentration, the amount of activated TCR receptors. The receptors can diffuse and then also will experience a centripetal flow that can come from actin, remodeled modeling, and other types of effects to kind of drive them into the center of the synapse. And the movie is a little confusing because these are very short time scales. So now you're seeing the activation of the T cell receptors and very little diffusion. And now we're taking much larger snapshots and time scales. Taking much larger snapshots in time where you see the drift that moves them in kick in. Okay, so now they're actually kind of getting transported in using this method. And in the more detailed model, say from this paper, there's actually volume exclusion effects that we're not capturing that kind of causes them to form a ring instead of get into the middle. And that's what I'll mention in the last little bit of my talk right now. I have a couple more minutes, right? Okay, cool. All right, good. Good. So that's kind of quickly our drift diffusion CRDME that we set up. So the last thing I wanted to mention, again, like I said, kind of in the spirit of what Pad Meany was showing, is I wanted to show you some work in progress where we've done a fair amount of kind of simple tests to show this method works, but we're still scaling it up. But I think it's kind of a cool thing, so I wanted to show it off, which is now to go and model actual interaction potentials. So now we want to allow two-body interactions between particles in the system. Between particles in the system, so two body potentials. One reason you might want to do that is to model things like volume exclusion between molecules. And so we've got a method that kind of extends the approaches I've shown you before. It should work in planar domains, volume domains, and surface domains. It still preserves Gibbs-Boltzmann distribution if you have no reactions. It preserves detailed balance of drift diffusion fluxes and of reaction fluxes. And we've tested it on a variety of reactions. We've tested it on a variety of smooth potentials and harmonic potentials, and most recently, Leonard-Jones-type potentials. But we still have to explore strongly drift-dominated regimes, as I said. But in our initial testing with nice potentials, it shows second-order convergence. So I just wanted to kind of end with a little toy example of showing you this method. So if we look at just a reversible A plus B goes to C reaction, and I'll use a harmonic potential. So my molecules will move around when they get within. Around. When they get within three nanometers, they'll start feeling a repulsive potential to kind of push them apart. So it's kind of a weak volume exclusion potential. And then we'll also have a doy bonding model that says for molecules within three nanometers of each other, they'll react with some probability per time, A's and B's, to make a C. And the reaction will be treated reversibly. So here's kind of some movies showing you that. So here's two movies. The one on the left, there's no potential. So it's just reality. Potential, so it's just reaction diffusion. The one on the right, there's this harmonic volume exclusion potential, though that potential was not used in the initial placement. So you notice at time zero, many of the molecules, the red and blues, are overlapping. As time goes on, hopefully you'll see the picture on the right, there's much more volume exclusion. They're much more less likely to overlap. And then these purple molecules are the C molecules. So you'll see kind of reds and blues sometimes disappear. Reds and blues sometimes disappear and turn into a purple, and vice versa. And I think you can kind of see that there's much more overlap of the molecules on the left than on the right. The ones on the right tend to much more kind of stay excluded. Of course, they can overlap because it's only a soft potential, but the potential works quite well, it seems like here, even in the simple case of kind of keeping them apart more. Okay, and maybe I'll just zoom that up. Okay, so just to kind of summarize, I don't want to keep everyone from the coffee break. So, what I showed you is that we've kind of now put together kind of a collection of methods that let us do reaction drift diffusion and pretty much now also, though I don't, you know, there's still work to do on optimizing and scaling up, interaction potentials. And these methods all work on unstructured grids in 2D, 3D, and now surfaces. Surfaces. So, kind of, we can do a variety of modeling contexts with them. A bunch of students have worked on this. Like I said, Jing Wei did the surface CRDME. Ying did a lot of work on the drift diffusion and our earlier unstructured grid CRDMEs. And Max worked on the more recent CRDME with interaction potentials. Some of these are published. Some of these are in preparation. Though the surface CRDME and the drift diffusion. The surface CRDME and the drift diffusion CRDME are pretty well along. So, if someone's interested in those, just feel free to email me and we could potentially share drafts of the manuscripts. All our imaging data comes from Carolyn Larabell's lab, and we've been funded by NSF. I just wanted to thank the organizers again for inviting me and everyone for listening to the talk. It's really been an awesome conference so far, and I'm looking forward to the remainder of the talks. And I really hope I can see you all in person next time. And before I go, I wanted to leave up if there's any questions. Up, if there's any questions, a little advertisement slide. So, completely unrelated to this talk, I've been working with a group of people on a package called catalyst.jl for simulating chemical reaction systems. This is a Julia package, but I think it's a really cool approach. It lets you very simply specify reaction networks, and then it generates a symbolic model of them. You can then translate that symbolic model completely behind the scenes into mass action ODEs, chemical ongoing SDEs. Chemical onjovin, SDEs, Gillespie junk processes, and then you can use what I would say is the most comprehensive set of solvers for all of those types of models: hundreds of ODE solvers, tens of SDE solvers, order 10 jump process solvers to actually then simulate optimized versions of those. And all the conversion is done for you. And in particular, we're starting to get some traction. We've had some cool projects built by people working with Ramon Grima recently to do chemical master equations. To do chemical master equation approximations with finite state projection methods and moment truncation methods, where you can just take your catalyst reaction network and they'll do all the work from it for you. So let me leave that up as an advertisement and I'll stop there. And thank you again for the invitation. Thank you very much, Sam. Rick Pete has a question. Hi, Sam. So, yeah, I was intrigued by this. By this conversion from the finite element mass matrix into a scheme that's interpretable as a continuous time random walk. You did this by mass lumping. I mean, so the thing that, you know, I usually think about when you start with a finite difference scheme, you know, there's often an interpretation of it as just having discrete fluxes. So then it's pretty easy to transfer into a probability flux for the continuous time random wall. Flux for the continuous time random walk. So, I guess the reason you start with a finite element procedure is because of the complicated geometry. That's the more natural computational framework. But I'm wondering, is there a way to somehow, almost like with the Wang, Pestigan, Elston method, to start when you're formulating the numerical method, the finite element method, to somehow impose some kind of interpretability in terms of flux presentation, discrete flux preservation, so you don't have to do. Flux preservation, so you don't have to do some ad hoc procedure to go into the continuous time random walk formulation? So the answer is there's various approaches that mix things. I don't know one that does exactly. I mean, obviously, like discontinuous Galerican methods are flux-based. So maybe, but when you try to apply diffusion, they typically involve things that make it not really continuous time random walks when you start trying to look at how people do diffusion, which doesn't mean it can't be done. I just haven't seen a method that. I just haven't seen a method that lets you do it. For example, though, one can interpret finite volume methods as approximations to finite element methods when you rewrite things as first-order systems. So that's maybe one way to see more of the flux-based approach where you're using an indicator function basis function, essentially. But this edge-average finite element method kind of forgetting the name now, but it actually ends up. I'm forgetting the name now, but it actually ends up being consistent with some methods that are used in electrical engineering that are even older. I just am forgetting the name, but I can look it up and tell you, which are maybe more kind of a physical interpretation for working out the connectivity and the rates. And so maybe they have a little more of a physical interpretation that you would like. And the mass slumping, did you say it creates no error or it creates negligible error? It certainly introduces another error, but it's an error that at least for these kinds of diffusion. It's an error that, at least for these kinds of diffusion problems, seems to be at the same order because it's like applying a quadrature rule to the inner product. It's like applying a trapezoidal rule on your triangle, essentially. And that is a consistent error in terms of the order. So it doesn't really mess up the order of accuracy in practice for diffusion type problems. Cool. All right. Any questions from the room? Hi, Sam. This is Bill Holmes. I've got a question for you. Yeah. So, when you, when you're, it's related to the very end when you're talking about the addition of volume exclusion. When you're using these potentials, I don't tend to think of them as probabilistic entities. I don't know if that's right or not, but is this something you're adding on top of the probabilistic stepping rates that you derived for the random book, or is it something that's being integrated? Or is it something that's being integrated into the formalism and then kind of being dragged out? And then the follow-on is: what does that do to the stiffness of the system? Right. So we're putting it into the beginning spatially continuous model. We're saying instead of point, we're still going to stick with point particles, but we're going to assume now that there's some kind of potential, which the stiffer, you know, the steeper that potential is, that's going to add stiffness in some sense. And that will give us a And that will give us a way to effectively have some type of volume exclusion. So it's like smoothing out what would be called a hardcore potential that would give you true volume exclusion between, say, spheres or something. And yes, like if you're trying to integrate these equations numerically from the perspective of PDEs, they're generally going to get stiffer when you have steeper and steeper potentials. It's also another short-range effect you have to resolve, and so it's going to also put more limitations on your mesh space. Put more limitations on your mesch spacing. But anytime you want to do volume exclusion at the scale of particles, that's an issue. If you do it in Brownian dynamics with hard spheres, you've got to take very, very small time steps, right, to properly enforce it. Here, you end up having to have a smaller mesh. But it's built in in the underlying continuous model we approximate. It's not something we're putting into the discrete model. So it shows up in the Fokker-Planck equation from the beginning, basically. Yeah, we're introducing a soft potential in the Fokker-Planck equation to model volume exclusion type. Equation to model volume exclusion type effects. Thanks. I think we're good in the room. All right. Well, thank you again, Sam, and thank you, both speakers, this morning.