Um and I'll do it I do that very ki kind of carefully, geometrically carefully. Uh and then in towards the end depending on how much time I have I will give you some examples of how this can be used and one of the I guess this was Otto's first example of how he used this was to study various types of gradient flows. So I want to show you some gradient flows related to this. And I'm going to work as I said Work, as I said, things will be in infinite dimensions, but I will focus just on the geometry. I will also give you examples in finite dimensions. So let's start with the Riemannian principal bundle. So I have a principal bundle, some manifold V, and then I have some action, and I take, in this case, the action is from the right. Okay. Um and and then I have a projection like this. Okay. And what what do I want to what do I do want to do with this structure? Well I want a little bit more. I want some Riemannian structure to this as well. So I assume that this manifold E is equipped with some Riemannian structure and I want this Riemannian structure to be compact. And I want this Riemannian structure to be compatible with the principal bundle structure. So it means here I want a Riemannian metric which is invariant to the action, on the right action. And one thing that this Riemannian structure gives me is I have the fibers, its principal bundles, the space is fibrated. And that means I have the vertical directions, but just taking the tangent spaces. Just taking the tangent spaces to these fibers. Once I have a Riemann structure, I also have horizontal directions. And because this is an invariant structure, these horizontal directions corresponds to tangent spaces on the base space down here. So this is the structure that I'm interested in. In fact, I want a special case of this. The special case I want is when the entire manifold here, E, A manifold here, E, is itself a group, and H is just a subgroup. I'm sorry, maybe discussing a con question about like, I think this horizontal direction dependent matrix of connection from Yeah, and that will come from the, in this case that will come from a matrix. But it's not from the Riemannian structure. Sorry? It's not from the Riemannian structure. It's from the Riemannian structure, yes, which is compatible. Structure, yes, which is compatible with this whole principal bonding structure. Okay, we will be more explicit in this case. So I have this sub-manifold here, and then I have a base space here, which is just a quotient. Think of this as just a transitive action. And the right way maybe to think about this is that I have some base point here in the manifold downstairs in the quotient B. We call it V naught, and then the fiber here going. And then the f the fiber here going through the identity is just the isotropy group with respect to the action uh on on on uh okay. So that's the structure and then the projection is just you take uh you take the left action on B naught. So this is how it works out in this set. Um and And I told you, like, I want the metric to be invariant, but it only has to be invariant with respect to this isotropy sub. So I call these metrics semi-invariant, semi-invariant Riemannian metric on G. And Cornelia was talking, when she was talking about fluids, she was talking about a fully right-invariant metric, but that's because she was talking about the metric restricted to just this fiber. I will give you more details on the metric later on, but I'm talking actually about the same metric that Cornelia was talking about, just that my group will be a little bit larger. Cornelia was restricting just to this subgroup, which turned out to be just a volume-preserving diffuse in this case. So, and then I will be interested in various types of flows. And one type of flow that you can get from this is. Flow that you can get from this is that you can take something here, like a flow down here, and you can lift it to a horizontal flow here. So it means it's going to integrate the horizontal distribution you get. By the way, the horizontal distribution is not integrable in general. So the fibers uh I mean the the vertical distribution of course is integrable because it corresponds to the fibers. Because it corresponds to the fibers. But the horizontal one is in general not. That, well, you will see no one. So that's one type of flows we consider, these horizontal ones. But you can also consider flows along the fibers, and those will be vertical flows. And the Euler flow on the identity fiber here is an example. In that case, it's a geodesic flow, right? Flow, right? So I will be maybe later on more interested in not Hamiltonian flows and geodesic flows, but gradient-type flows, either vertical or horizontal. So this is my structure. And another cool thing that comes out of this is that it gives you a kind of recipe for. You kind of recipe for factorizations, for how to make factorizations of elements in the large group. And if you've done optimal transport, if you studied optimal transport, you know that January came up with this polar factorization of maps. And I'm going to tell you how that is related to factorizations you get in this way. So, what do I mean with this? Well, let's start. mean with this? Well let's let's start with any element G in the large group and then it will be on some fiber so there will be a corresponding point B1 below that fiber. And what do I do next? Well next I construct some way, I haven't said how yet, but I construct some way to Move horizontally, some sort of horizontal motion, right? So, and I start from the identity and I move horizontally to the same fiber. This is, you can do this, for example, by geodesics, because there is this point that if you start with a geodesic that is horizontal, and your metric conforms to all this structure here, it's going to remain horizontal. Remain horizontal because it corresponds to geodesics down here. So, think of this maybe as a geese curve and assume that we can find such a geodesic curve that starts there and ends up on the same fiber. And what does this give us? Well, once you have that, so now we have this point and we have this point, and they are both on the same fiber. What does that mean? It means that means that you take the end point and take its inverse and multiply it with the original g, you get something on the identity fiber. And this is in fact your factorization. It's as trivial as that. Because then you say, okay, I can in fact, if somehow I can classify what these kind of horizontal orbits are. Orbits are, and you can do that, and we call it a polar cone, and we'll get into this later on. So I can say that, well, then I can write any element as an element in a polar cone combined, composed with an element from the right that belongs to the identity file. Factorization. But one has to make sure that it is a unique point on the fiber. Of course, yes. So there are many questions that need to be solved, but this is the basic geometry. But this is the basic geometry behind those types of factorizations. And you can get, if we talk about, when we talk about factorizations, we usually don't talk about maps, but we talk about matrices. So which matrix factorizations do you know? At least all the ones that I know, you can get them like this, by different choices of metrics and different choices of Yeah, essentially, different types of groups and metrics, essentially. So, QR factorizations, polar factorizations, LU factorizations, all these are examples of this structure. And some have more structures than the other, but the basic example is this. So, what happened there? I guess sometimes this. Sometimes this shifts order. That's more related to. Maybe I'll say something about this as well. When you talk about not Russianstein metrics, but the Fischer-Rau metric, which is another kind of interesting metric, then normally you switch the action, you take the action from the other side. But I don't want to do that here, so let me actually now say something about the connection drop the modransport. I'm not going to talk much about the So, I'm not going to talk much about this. The main point is: we have two densities on some domain, let's say, or manifold. And the main point is: I'm interested now in the Munch version of optimal transport. And I'm interested in the one for the L2 distance. Then you have, what is the problem? The problem is you have two densities. You want to find a map that pushes one to the other. And assume now that you can do everything. And assume now that you can do everything in a smooth setting. So both densities are smooth somehow with respect to some background measure, maybe. And then you want to find some sort of smooth map that takes one to the other by push-forward action. But, well, if you're in one dimension, it's almost unique. But if you're in higher dimensions, this is not going to give you a unique map, obviously. If you find such a map and you compose it from... Such a map, and you compose it from the right with something that preserves the first measure, you get another one. You want to find an optimal one. You want to find the one that does this, that's the constraint, and also minimize this function. Optimal. That's the original, well not the original, because the original was, I think, L1, but it's the Mars formulation of the problem, transport problem. So, yeah, so that's that. And how is it related to all these things I was talking about? Well, let me just say, so I said m here for a manifold. In the case when this manifold is Rn, then what we're interested in is something like this. Okay? So just Euclidean distance, L2, or L2. Or help to normal. Okay, so that's that. And throughout this talk, I will call this functional J. So when you see this J of phi, phi is the map and J is the functional, but we want to minimize. So now let's see how this works out. Our big group is the group, some group of diffeomorphisms. Diffumorphism, because we're in the smooth category. Okay, let's assume everything works out nicely. We have somewhere an identity fiber, and how do I get this base downstairs? Well, I take the action of my diffeod group on some fixed mu naught, which is assumed to be a nice distribution here, a nice density. Density, and that gives me an isotropy subgroup, the diffeods that preserve this measure. And then there is this nice result here, which goes back to Mauser, which is that you can identify densities with exactly this point. In fact, if you want to do this rigorously, there is an infinite dimensional principal bundle behind this in this model. Bundle behind this in the smooth category. That's the result, I believe, of Hamilton. So, if you like, that's an extension of Moses. So, that's the structure that we are interested in. But I haven't said anything about the metric. And the whole thing comes because of the metric. So, I I will specify that now. And the metric is the the simplest possible one you can come up with. possible one you can come up with. So we run some background manifold that has a Riemannian structure. So we have a finite dimension of Riemannian metric. And then I just take, so I need to specify the Riemannian metric on the diffeo group. So I have some base point where I'm sitting and I take a tangent vector about that base point. So I can think of this as a mapping going from Mapping going from the manifold to the tangent bundle, the manifold. It's not a section, it's not a vector field. But then I can write down this thing here, it's just complete, you know, quadratic thing, and that is going to give me my Riemannian fracture on the space. You see, this is just, if you like, this is just kinetic energy of each of the Kinetic energy of each of the particles, I take its kinetic energy and then I sum them up via this integral here. With respect to this mu naught, then that's important. That's one of the input data I had for my optimal transport problem. Okay, so what do we need from this metric? It's not right invariant. Okay, so it's sometimes called the non-invariant. Sometimes called the non-invariant L2 metric, because you can, of course, construct a fully right invariant L2 metric on the Different group as well. But that metric is not so nice to work with. It doesn't have good properties. And also, it's maybe not so physical. This is what comes directly from physics. But if you restrict the metric to just this isotropy subgroup, the volume-preserving diffeomorphism, then it's invariant. Invariant. How do you see this? Well, this means exactly it preserves this thing, so you can precompete everything here with the phi, with the eta, as I did here, and then you get this identity, this exactly right invariance with respect to this subgroup. Okay. Good. So we have, at least formally, we have sort of the structure that we need. So this should descend to some metric down here on the density space. What is it? Well, if you think about it, what do we expect? It's L2 up here. Okay? We take, how do we project? Well, by project. How do we project? Well, by push forward, when we apply push forward, we're taking one derivative, right? So we lose one derivative when we go down. So it's going to be something like h minus one metric down here. And it's given like this. So if I take a base, I call it g bar here. So if I take a base point mu, and then I take two tangent vectors, mu dot, and then it's given like the integral of the mu. Given, like, the integral over some gradient vector field theta squared with respect to the integrated with respect to the base point, and what is this theta? This theta I get by solving some elliptic equation like this. It's like a modified Laplacian, if you like. That depends on the row, on the base points. Okay? So that's the metric. So that's the metric that you get from this sort of kinetic energy, non-invariant kinetic energy metric. You get this one downstairs. So it's an H minus 1 metric. And now let's talk about geodesics. And it looks, one point here is that you see it's kind of complicated. This metric looks complicated down here, but it's very simple up here. Here. Okay? And if you like, that is kind of optimal transport. Everything is easy when you lift it to the maps, but it's kind of more complicated when you look at it here. Okay, so now we can talk about Eurasics. And I won't be too fast there, I think. So, and I do it just when the manifold is R and just to, because it's a little bit easier. And you see what happens then, if you look at the Lagrangian corresponding to your metric tensor, you see that it's not, if you have a flat manifold, it's not going to depend on the base point at all. It's completely flat. So the geodesic equations are just given by this here. It means it's trivial. It means it's trivial to integrate these equations, and you get the geodesics like this. So, now the next question: what are we interested in? Well, we are actually interested in geodesics down here. So, that means, but remember, there is this correspondence. If we can find horizontal geodesics in this space upstairs, those will project to geodesics down here. And we have explicit formulas for geodesics. Explicit formulas for geodesics here upstairs. So we just need to figure out what is the horizontal directions or the horizontal distribution in this phase. And we do it first at the identity. And what is the identity of the group? Well, it's the smooth vector field. It corresponds to the algebra of the group, which which of uh is the uh so I'm talk the the tangent space, sorry, at the identity, corresponds to the algebra of the group, which are smooth vector fields. The group, which are smooth vector fields. And so I need to, if I should find the horizontal directions, it's a splitting, if you like, of the vector field, and it's exactly the Helmholtz decomposition. And you can think of this as the linearization of the factorizations I was showing you before. And then, so it's the gradient part. So, it's the gradient part in the Helmholtz decomposition. Why? Because we know already what the vertical directions are. Those are the divergence-free vector fields. And then Helmholtz tells us that the orthogonal to that should be the gradient vector fields. And if I should be careful, then I should say that this is slightly generalized version of Helvolts, because I'm now taking a density here, which is not necessarily. A density here, which is not necessarily the density coming from the Riemannian structure of my background manifold. It still works. And it's kind of interesting because you see the horizontal distribution is independent of this. It's because the metric depends on rho naught or mu naught. So it sort of cancels out in exactly such a way that the horizontal distribution is always going to give you this at the identity. And then you translate this and you get the full horizontal distribution. And as you know, Distribution, and as you know, if you take gradient vector fields, they don't form an algebra, so this is not integral. So, what can we do? Well, we can look at this polar cone. So, we look at just geodesics originating from the identity. And we know the formulas for that. So, we look at those kinds of geodesics. It means we plug in V0 here, which is the gradient of something. Something. And that gives us what's called the polar cone. So you start from the identity, you move up like this, gives us a cone. And here is something that is fairly easy to prove, that this polar cone K is isomorphic to strictly convex smooth functions. Here is something that is not easy to prove. The polar cone is a section of this principle one. Is a section of this principal bundle. It's kind of not so hard to prove in finite dimensions, I will show you this, but in infinite dimensions, these are really hard results on regularity for the optimal transport mark. And what about the connection to polar vectorizations? Well, if you do this, you take some element here and then you construct. You take some element here and then you construct exactly the factorizations as I told you before. You get Venus decomposition of the transport maps into the gradient of some convex function composed with something which preserves volume with respect to knots. And the point of Grenier was exactly that you can start from anywhere, you know, in particular you can start from anywhere above the fiber of mu1, and then you make the positive. 1 and then you make the polar composition, and then this is going to give you the optimal 1. Why? Well, because this is the shortest, you know, if you take all the paths from the identity to this fiber here, the shortest one is the one that is on the polar core. This is kind of geometric understanding of this. And let's just now work out what the Work out what the distance is. Once we have this explicit geodesics, we can write, you know, geodesics curves like this. We can work out what is the geodesic distance between these two curves in this simple case. And then we plug in what is the geodesic distance. Well, you take your curve and you take an integral from 0 to 1 and you take the metric tensor and you differentiate your curve and you integrate this. And what do we get? Well, just differentiate this with respect to time, that's easy. We get this thing here, and in the end, we recover exactly the J that we have. So, what does this mean? It means optimal transport is just minimizing the square of the geodesic distance with respect to this Riemannian structure. Polar cone, uh does does that mean that it's convex? Yeah, so the polar cone, you start you start from uh here is the identity fiber, okay? And then you take you take your horizontal space, just at the identity here, and then you take the desix originating from here. So these are all gradient vector fields. So on. And for a short time, at least, it's going to stay in this, as long as it's a uodesic, sorry, as long as it's a diffeomorphism, it's going to be this convex function. That's the easy part of the proof. The hard one is to actually show that you can recover. Actually, show that you can recover the entire intersection. I hadn't understood where the convexity of the potential comes in. How do you show? I mean, how does the condition on the convexity of the potential come up? Because maybe it will come up on the next slide. Something will come up here, right? Because essentially you take variations of this. That's that's how you you see it. And you want this to be you you you require this to be a diffeomorphism. Require this to be a diffeomorphism. But this is already specific to that theory. It would not be generic. Okay, sorry. Yeah, on manifolds, it's a little bit more tricky, yes, of course. But in fact, the Macan theory on Shubi Manian manifold follows the same scale stress. It's kind of the same strength. But potentially in this case, yeah, yeah. So, okay, so here is the point. Okay, so here is the point. If you take a geodesic like this and you just continue, continue, continue, sooner or later you go, it's not going to be convex anymore. But that's not what we're asking. We're not asking that. We're solving a boundary, a two-point boundary value problem. And we know that we're connecting two geodesic. That's what you're using. So this has, if you start with two some if you start with two points in this folder column, you connect them with the geodesic, it's going to stay convex all the time. To stay convex all the time. So, you know, when you talk about optimal transport, sooner or later you come to the Martian-Par equation. So, how does it appear here? Well, it appears like this. You can write your geodesic curves on this polar cone. You can take the identity and write that as the gradient of just x squared. I'm still on Rn here, just to make it quite. Rn here, just to make simplified things. Okay. And then the condition, the condition for this to be a transport map is exactly this. And if you write this, just write it down, this is the one way of formulating the Montreal-Pair equation. So you see that the Montreal-Par equation comes from the fact that you need to match mu0 with mu1. Mu0 when mu1, and the fact that you know that you're on the polar colour. So now we have also some, if you add downstairs some constraints like you only that you move downstairs with in a horizontal distribution itself, like you have some premium optimal transport you get for upstairs also called a con? Oh, you mean you make some restrictions? You mean you make some restrictions here? I mean, then this whole picture changes, right? In fact, I have an example a little bit like that. I hope I will get to that. Let's see. Can I suggest we save some of the questions? Yeah, we are eating into precious time. Right. So, yeah, let's see how I will continue with it. But, you know, I want to do something that is in a way trivial, but it's funny. So I'll now do a linear option. So I'll now do a linear optimal transport. What do I mean with this? Well, I take my densities to be multivariate Gaussians with mean zero, on Rn. So then they are determined by this covariance matrix sigma, positive definite matrix. And then I take my maps to be just linear maps. And it turns out that this whole picture. And it turns out that this whole picture lives as a sort of totally geodesic example in the infinite-dimensional example. So, this is a finite-dimensional setting that fits perfectly inside the infinite-dimensional one. So, it's more than just an analogy, it's actually contained in this infinite-dimensional picture. So, let's see what happens in this case. So, the first thing is you can identify you can identify, of course, your You can identify, of course, your Gaussians with just your covariance matrices. So now instead of having downstairs the densities, I want to replace this with positive definite symmetric real matrices. Okay. And then what is the bundle structure? Well, I had linear mappings, and they need to be invertible. Again, should be diffeomorphic. So I'm on GLN now. So now the bundle picture looks like this. So now the bundle picture looks like this. My group is GLN. My sort of base space down here are exactly the positive definite matrices. And you can think of this as GLN quotient out by the orthogonal group with respect to this point, this first covariance here. And then this is the projection, this is the action of GLN on, and all this fits exactly with. And all this fits exactly with what I showed you before. So, this is how the push-forward action would look like for linear maps applied to Gaussians. And what is the metric? Well, the metric is something else. It's like the Frobenius inner product, but a little bit shifted by the sigma naught here. And then remember, we also had the mu naught in our definition of the metric on the diffeos. On the Diffios, so that's consistent. And what is the metric downstairs? Again, it's a little bit more complicated, but it looks like this. So you're on a base point, sigma, and then you take two tangent vectors, sigma dot. Those are just now symmetric matrices. And then it's the trace of sigma s s, and what is s s is the solution to this equation. This reflects the fact that somehow we need to invert an operator to go back to this S here corresponds to the, I think I called it theta before. But that the thing that I took the gradient from. Okay. And now we can do the same thing. We can start from the identity here and move. Identity here and move. And what is the polar cone? Well, it turns out that the polar cone itself actually corresponds to just the positive definite matrices again. No, it's not. Why do you say that? I mean, you go out and then you over a short. And then you go horizontally, you stay that if it's not integral, you go out. Yeah, but if you take two symmetric matrices and take their brackets, it's not symmetric anymore. So it's not a symmetric. No, it's a polar distribution, the distribution in G and that's the horizontal distribution. Yeah. Yeah. No, it's not. Can I? I can answer. The fiber consists of two symmetric matrices. So the horizontal part is symmetric matrices, but they do not form a sub-algebra. Yeah, not a sub-algebra, but they form a manifold. This is a sub-manifold. The horizontal distribution is a sub-manifold. Maybe it's enough to say that. Yeah. So, but locally, that's true also in the infinite dimension case. You take, okay, let's do stuff for this. So, anyway, so I just show you this example and now what is the Montan-Per equation? Well, the Moham-Pair equation is just like computing the square root. Okay? So I saw, well, it's not exactly, but if sigma naught is the identity, then it's just like. Is the identity, then it's just like computing the square root. Kind of cool, right? That Marshall Paris is like taking the square root of a matrix. Okay, and the factorization we get like this is just the usual polar factorization of matrices. So now I have Q, which is orthogonal, and P, which is positive definite. And what can I do with this? Let's see. Not so much time, I will not be able to talk much about gradient flows, but anyway, so I can construct gradient flows like this, and I've done that. Now I'm going to do it faster, because now you've seen the geometry, right? So you can do this numerically also, and that will this is a way actually to compute the polar factorization. It's not an efficient way of doing it, but it is possible to compute it by looking at. By looking at these types of either vertical or horizontal flows. So the one I show you is like a vertical version, but you can also look at a horizontal version. And here it's interesting to look at relative entropy. So you take your Gaussians, right? Your multivariate Gaussians, and then you can write relative entropy. Relative entropy, so entropy of one with respect to the other, okay? And it's going to give you something like this, and you can try to minimize this as a function on here. You can lift that, take the gradient flow, then you're going to move in horizontal direction. So this would be another way to compute this parameterization. And in fact, in this case, you can prove that this is going to converge in this finite-dimensional case. You essentially just compute the Okay, so you essentially just compute the Hessian, you see it's strictly positive on this polar cone. It's important actually that you stay on the polar cone, but if you start there, it's going to be strictly positive, so you will have all the here the link and so on. So in this final dimension case, the theory works out brilliantly. And we have also tried this numerically and you get to just for fun. What do I want to do next? Well, I'm going to give you now some examples. I'm going to give you now some examples. And I didn't say that, but the the this distance that you get from the geometry that was induced on the base space, that's exactly the Wascherstein distance, Wascherstein L2 distance. So I guess that's autocalculus. It kind of gives you a Riemannian structure to the Wasserstein distance. But this is, in a way, it's not the kind. But this is, in a way, it's not the canonical choice of a metric on the space of densities. There is a, if you like, a better one, and that's the Fischer-Rau metric that's fully invariant with respect to whatever action is left when you're downstairs. And this is in fact the only Riemannian structure that has this invariance. So Peter knows much about this, and Martin as well, who proved it some years ago in the infinite dimensional. Proved it some years ago in the infinite-dimensional case. It was known since before in finite dimensions. And I think the key for this was the Petriga theorem to prove this. That was kind of the one that this is an interesting story. And so you can compare, you can now look at gradient flows with respect to this structure and you can compare it with other metrics, for example, Fischer-Raw. So, okay. So, okay. Um yeah, that's what I just said. So now we can look at this uh again restricted to just the Gaussians and see what we get. And if we take Fischer-Waal and we take as functional, we can take again this relative entropy and we look at the flow, what we get is something called the brocket flow. Okay, so that's a way to a flow that was designed to. Designed to sort of diagonalize matrices. And so, this is an interesting way of obtaining the rocket flow. Just there, you get the full connection to optimal transport. And if you do this, it's in infinite dimensions. Now, I did it on a small toy problem, but you do it on the in the infinite dimensions. And now, let's see, that's not exactly true because the brocket flow is a vertical flow. Flow is a vertical flow, but now I'm looking at the horizontal flow. Okay, so you can do it two ways. But anyway, so now I take the Wascherstein auto metric and I'm interested in looking at gradient flows on the base base, on the densities. I assume everything is smooth. So I have some functional f, and what is the gradient of f with respect to this Riemannian structure? That's what I'm asking. And then it's a little calculation. It's a little calculation to show that the gradient, so this is now like an infinite dimensional gradient. But it's a little calculation to show that, in fact, the flow is going to look like this, where this is some sort of, where this now is finite dimensional gradient. And this, of course, is just a transport term. So and here we have the derivative or functional. So what happens if we take f to be now entropy function? F to be now entropy fun, the entropy functional. Well, it's easy to see that the derivative of this, it's just pointwise, is going to be given by log plus 1. The 1 doesn't matter because it goes away in the gradient here. So, what do we get? Yeah, we just get uh the heat equation. So, it's beautiful. You get from well, in a way, it's kind of a bit It's kind of a bit because it's a linear equation, right? Everything here was non-linear. So we start with something non-linear and we get the linear equation, but it gives us a kind of physical understanding of the original. That's anyway how I like to view it. Okay, I still have some time. So I'm going to talk about a little more advanced topic now, and this is some work that I did recently with Boris Kessin. And this is the incompressible porous medium. Incompressible porous medium equation and its connection to the total flow. And I'll tell you the result before I show you anything. The result is that essentially these are gradient flows, right? You can think of both of these as gradient flows on densities with respect to the same functional but different metrics. IPM. IPM is when you consider this Paschal Stein also metric. Whereas TOLA is a weaker metric. So it's a weaker one. So it depends. So IPM is L two on the vel on on velocity, so it's the kinetic energy metric, whereas tota would be H minus one on velocity. So it's a it's a weaker one. It's a weaker one. And this does not exactly fit to what I was showing you before, because what we're doing now is we're looking at, if you think of this as a flow on the diffeomorphism group, we're not looking at all diffeo, so we restrict to the ones that preserve some reference density. This is typically the volume form that comes with your Riemannian structure. But then the point is that rho notes. But then the point is that rho0 is not one. So it's not the one that corresponds to mu here. So you have like two fibers. You have one fibers in this direction coming from the projection with respect to mu, but then you also have these fibers coming from the projection with respect to rho knots. And what we're doing is we're looking at gradient flows, but we restrict them to these fibers. So then you get something which is still non-trivial down here. So it's a little bit like the Trivial down here. So it's a little bit like the question you asked, because now we have some constraints down here. We're moving on orbits, and orbits with respect to disc group. And actually, there's a nice story behind this, that what is happening actually is that you're moving on the co-adjoint orbits, at least in two dimensions. If this is a two-manifold like here, you're moving on co-adjoint orbits like you do in the vorticity formation of the Euler equations. So That's the point. And maybe I still have three minutes or something. I can show you a little bit more about this. So, what is the porous medium, the incompressible porous medium equation? Well, imagine you have some particles with different densities, and you have Different densities, and you have and then you have some gravity or something defined by a potential function. And then what will happen when you sort of let these diffuse into each other? Well, what will happen is that the heavier particle will move down and the lighter ones will be on the top. And then you do it in such a way that you always preserve the volume. So it's the volume-preserving fluid. This is the incompressible porous medium equation. If you write it down, If you write it down in terms of the row and the velocity, it looks like this. Okay, we're interested now in these equations in the two-dimensional case. If you're in a two-dimensional molecule, what is the Toga-Lattis? Well, it's something else. It concerns interaction of particles on R with neighboring interactions. It's a Hamiltonian system, in fact. And I don't have time to discuss this so much, but what I want to get to is that Want to get to is that the point is in two dimensions the IPM equation can be written like this and I want to tell you now that this Laplacian that appears here comes from the Riemannian structure. So it's exactly the Laplacian that appeared in, if you remember, when you go to the Wascherstein optile metric. Okay? Okay, and this other structure here is what comes from the fact that you have this bracket here, comes from the fact that this is a gradient flow. So, this is an example of a double bracket flow, if you like. So, I don't have time to get into that. But the point is that the continuous version of the TODA flow can be written in the same way, but there you don't have this Laplace inverse. You don't have this Laplace inverse. And because you don't have this Laplace inverse, it's a rougher version. You don't have as much smoothing. Now, the final thing I want to do, just show you some simulations of this. So, this is now on the sphere, so incompressible. Sphere, so incompressible porous medium equation, you have this kind of weird because you have a sphere, but then you have gravity going down. And then red, I think, means light particles. Dark blue are heavy particles. So if we look at this, what do we get? And remember, everything is now volume-preserving. So the action on the density here is just by composition. But I've quantized this with Cyclin's approach, so I actually get something finite-dimensional. Something finite dimensional, and then this is what you get. You see, it's trying to push everything up here, but then again, because it's volume-preserving, you have some obstacles to that. The interesting thing, but it still kind of looks nice. The interesting thing is what happens with the tota in the tota case. Because there, you also see the same trend with the red part moving up, but it's much rougher. And this, in fact, if you look at the equations and do some analysis, If you look at the equations and do some analysis, you can say that this Eben-Marston framework can be applied to the first one, but not to this one. It's not possible. So you lose derivatives, so you cannot apply these trees. We think this is the reason that you see this result. And if you look carefully, you see that in the IPM case, you get very nice, smooth behavior, whereas in TODA, you get this oscillatory behavior. Okay, that's it for me. I think that's it for me. Thank you. After a couple of questions, further questions? There was a slide that you skipped on Todd where you had a double bracket. I saw you were very. Can you upload this slide? Yeah, I'll see. You mean this one? The previous one. But it was this handwritten once or yes. This is an infinite dimensional version, right? Yes, okay. Here, yes, yes. Can you say a bit more about this double bracket? When theoretically you have some yeah, so for me now the double bracket. So let's see. Theoretically, let me show you. Theoretically, let me show you the structure. This appears when you're interested in gradient flows on groups, on Lie groups, with respect to right invariant metrics. So then in general, if you have a gradient, okay, so let's see, what am I how is this formulating now? So if you're interested in gradient flows on Lie groups, okay, with right invariant metrics, then you can sort of use the right invariance, just the same trick that you do in the Euler equations, to express, instead of writing these equations in terms of the digital. In terms of the diffeomorphisma, you express it in terms of the vector field generating the diffeomorphism. If you do it on a two-dimensional manifold and everything is volume-preserving, and you assume that the cohomology is simple, trivial, sorry, Albert. Then you can write this in terms of just a function. And that's what we're doing here, essentially. And in general, this looks like this. You have an add star operator just in Euler. add star operator just in Euler, but then you have another add star operator here, and this is the double gradient, the double bracket structure. So this structure appears whenever you're looking at these kind of gradient flows on Libraries. And one way to understand it is that these are, and this connects to your talk actually, because these are gradient flows that evolves on co-adjoint orbits. These are, you know, it's a gradient structure, but it will still stay. But it will still stay on the co-agent orbit. That's what you saw before. It's like we're trying to minimize or maximize something, but we're restricted to stay on the co-junct orbit. I'm a bit confused because Toda is a discrete case, and you are studying. So you need some hydrodynamic to limit V. Yes, so what we are studying is what is called So, what we are studying is what is called the dispersion-less TODA equation. So, that's essentially if you think of the toda as particles interacting, and then you let the particles go to infinity, so you have infinitely many particles, and you make a continuum out of that, then you get these equations. But you can also think of it, you remember I told you when I showed you the IPM results, I said this is a quantized version. This is a quantized version, and it's true, and then you get back to matrices. So you can go from continuous IPM to a matrix equation. You can also think of the TODA as a quantization of the continuous version. With the TODA, the discrete TODA realizes the QR photonization on large time. Yeah, that's exactly because it's one of these flows, right? Of these flows, right? So it's going to give you a factorization in the end. And in this case, it's the case, it's the QR. Does you see it in your picture? Yes. Yes, we see it in the picture. And I didn't show it, but it's kind of similar to what happens in the brocket flow. Which is, yeah, that's the same process. Thanks again, Classify Chris. So that's it for today. Tomorrow morning we start at 9 a.m. of Gerrard Michael. If you come a few minutes early, we're going to do something special for Gerrard tomorrow morning. Just a few minutes early. But 9 o'clock is when we finish this up. Yeah, I think you'd have to hear yours.  Oh yeah, they are. Yes, there's also a 15-minute gap, so that's a as a birthday present we'll let everybody know where you are. He gets five minutes extra. He gets five minutes extra.