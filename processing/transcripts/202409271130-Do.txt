Okay, so here's the classic story. So it starts with, I guess, this notion of integration on things like unitary groups. I'm going to put a little etc. here because you can take this sort of theory in many different directions. Can take the sort of theory in many different directions. This was sort of motivated from Lattice Gauge Theory back in the, I guess, 70s. And then, sort of, with this seminal work of Vine Garden was sort of some particular instances were calculated. But it wasn't until this millennium that people sort of realized there was a sort of deeper structure, algebraic structure going on, and that there's some sort of very natural relation to representation theory, in particular. Theory, in particular when it comes to the unitary groups, its representation theory on symmetric groups. And in some sense, one can think of this as some sort of like incarnation of survival duality, relating the representation theory of U of N to representation theory of symmetric groups. About 12 or so years ago, this sort of idea led to the definition as The definition, as Matrik said in the previous talk, of monitoring Hellitz numbers. And again, what we'll be doing today is looking at what this, et cetera, can possibly be. Okay. So I want to tell you about integration on Grassmanions. To do some integration, I need to tell you firstly about what spaces we'll be integrating over. So the space I'm interested in, I'm going to call S, and it takes some two positive integers, m less than n, and I'll define it to be this. So you take your favorite Matrix U that is M by N. Well, it can't just be your favorite, it has to satisfy this semi-unitary condition. And then we just do this multiplication in the other way, and so we get this sort of projection operator and this self-space. And so it turns out that this And so it turns out that this is the Graph Mannion of complex M-dimensional planes in an n-dimensional vector space. Okay, so we've got our space. What we need now is some sort of measure. This particular space S of n, n has the action of the unitary group U of n by conjugation. And it's sort of a nice transitive action. So this inherits. Action so this inherits some nice measured called ds, which is just the HA measure. So it's u of n invariant and normalized. And then we need some functions that we're interested in. Defining the space in this particular way, there are some natural functions that come to mind. Way there are some natural functions that come to mind, and they're just the matrix entries. So you pick your favorite i and j from one up to capital N. And in fact, it'll be more general and we'll look at polynomials in these matrix entries. Okay, so with that set up, I think you understand what. With that set up, I think you understand what the goal is. We want to compute all possible integrals of the form integral over s of m, n. You're sticking your favorite matrix entries here. Let's say we have k of them integrated next to this high measure. Okay, so that's the setup. Let's actually get to work. So you could just try and do this, right, sort of naively. This, right, sort of naively, but of course, what you want to use is this sort of nice sort of algebraic structure we have here. And as I sort of mentioned, this Schervile duality sort of gives you some relation between the representation, the integration on U of N with representation theory of the symmetric groups. What that sort of allows us to do is simplify this problem a little bit. And the first step of this simplification process is that we obtain something called a convolution formula. Convolution formula. Loosely speaking, this is that any arbitrary integral of the one I've just written down come from a certain sort of elementary type of these integrals. The actual formula looks as follows. So if we want this integral What we do is we basically end up writing this as a sum of some simpler looking integrals. So here it's a sum over the symmetric group, SK, which will be sort of acting on these indices. And then we need some sort of like a pairing of these indices. It's somehow, you would think of it as some like analog of a wick rule. Analog of a wick rule. So we have had delta i sigma one j one up to delta i sigma k j k and that's next to one of these so-called elementary integrals. It's an integral in the same space now, but where we arrange the indices as follows: s1, sigma 1. S1 sigma one, S2 sigma two, up to Sk sigma k. Okay, so that simplifies a little bit, means that we don't have to concentrate on all sorts of integrals of this nature, but just on this particular type. So the next obvious question, of course, is how do we calculate this? Before I calculate it, I'm going to give it a name. I'll call it W of sigma. It's sometimes called W s s s s s s s s s s s s s s Of sigma. It's sometimes called WG, something like that, in honor of Weingarten. Okay. So rather than actually sort of trying to work it out directly, it turns out that what we can do is relate various values of these integrals with each other. And these are called orthogonality relations. Okay, so you're going to give me some permutation in the symmetric group. In the symmetric group, SK. And we're going to write down a few related values of this Weingarten function. So maybe before I do that, I should just say a couple of words. So I'm sort of hiding in this notation, of course, on dependence on capital M and capital N. A priori, of course, maybe there's no expectation that it's going to have some nice dependence on M and N, but I think you all know that we're going to expect something nice. In particular, we can do things like In particular, we can do things like take large n limits and so forth. And it turns out that we're going to get a sort of very nice dependence on capital N, which is essentially polynomial. Okay, so what does this look like? So I have n times, so imagine you're into some value w of sigma. The relations are the following: n times this capital W of sigma plus, I'm going to have some sum. And then I have some sum here, y equals 1 to k minus 1 of w of its sigma, where you've hit it with the transposition i k. This is equal to okay, and then we have a few cases on the right-hand side. So if it happens that That K is a fixed point of your permutation sigma, then what you get to write down here is M times W of, and I'll write sigma down, what this means is this means remove K from your permutation. So, sigma down in particular is a permutation in the symmetric group S K minus one. K minus one. And then we have this extra term. Okay, so again, it's a sum. I might run out of space. Let me put it down here. So it's a sum. I equals 1 to k minus 1 of w odd. And then sort of looks a little bit like this term over here, except we'd like to, okay. We'd like to be able to send this down to the symmetric group SK minus one, but that's only going to be possible if it's I that maps decay under this permutation. Okay, so this is the relation. As you can see, it's sort of linear in these values of the Weingarten function. And I guess the hope is that this is some sort of like full rank linear system, and we're going to be able to calculate everything. And we're going to be able to calculate everything at least from some small number of values. In particular, we're going to be able to calculate everything just from some sort of trivial value in the symmetric group S1 or S0. Okay. Maybe I'll show you a little bit about what the proof looks like. I think it's sort of quite cute. And I'll just do the case when k is a fixed point. When k is a fixed point of this permutation sigma, in which case we sort of lose this last term on the right side. Okay, so what do I do? I'm going to consider some particular sum of integrals, some from i equals one to capital N. The integral over my space S1 sigma one up to S k sigma, sorry, up to S K minus one sigma k minus one. Recall that K is a fixed point here. I'm not going to write S K K. I'm going to write S I. And what we're going to do is try and use a conversation. And what we're going to do is try and use a convolution formula to deduce what this is. Okay, but before we use a convolution formula, the easiest thing to do, of course, is to just pull the summation in and notice that you're summing over the diagonal element. So this is just some sort of trace. Times the Weingarten function applied to now this sigma down. But because these matrices are essentially Because these matrices are essentially projections onto some m-dimensional vector space, this is just going to be m times w sigma down. Okay, so already we have sort of this element in there, and what we're trying to recover is this other side now. So this is the left-hand side of quite one. Okay, and so let's try again. Let's try again. Okay, now let me just not look at the whole sum. Let me just peel off my favorite part. Let's look at xi. And what I'd like to do is like to apply this convolution formula. It's going to tell me that I should get a sum of some sort of Weingarten functions. And in particular, to make To make my life easy, I'll start off by looking at when i is less than k. So when i is less than k, you sort of think, what property do I need of my permutation here? It's going to sort of permute these sort of first indices, and I'd like it to sort of match the second indices, is essentially what's going to happen. So certainly I'm going to get w of sigma here. W sigma here. But it turns out that when i is less than k, this sort of first index i here actually already occurs somewhere earlier, and the second index i here also occurs somewhere earlier. And so if you sort of think about what you need, and I hope I've got this right, it's this. And they're the only two contributions. In the case where your eye is Your I is at least K. You don't have the second option here because this index i that you see hasn't occurred earlier. And so this is only just W sigma. So now what we can do is we can add everything up. What do we get? We get n copies of w sigma, and then we get this sum. And then we get this sum of W sigma composed with this transposition I think. And that's precisely the left hand. Oh, sorry. Sorry. This was the right-hand side. I guess in Australia we call that the left-hand side. Okay. And then this one is the left-hand side. And then we're done. Exercise, you can try the other case when significant. Can try the other case when sigma of k when k is not a fixed point of the permutation. Okay, so essentially I've told you about integration on some space, and now I'd like to sort of use that to take us to representation theory of something. It's going to turn out to be actually the symmetric group again. Let's try and calculate this fine-guard function via representation theory. Okay, so maybe just looking at this, I'm not sure sort of, we have a diverse set of mathematicians here, but I think at least for some of you looking at this, when I say I'm going to sort of sum up from i equals one up to k minus one, this Minus one, this transposition i k, some of you might be thinking that this is going to involve the so-called user-Smurphy elements in the symmetric group algebra. That is indeed the case. So what I'd like to is define here j i. This is the sum of the transposition 1i plus the transposition 2i plus the transposition i minus 1i. In the symmetric group algebra CSK. Like many of you know, I mean, we sort of have maybe obliquely seen the sort of used Smurphy elements. I can't remember. Maybe someone mentioned explicitly, and they've been sort of obliquely involved in some of the previous talks. So these are sort of quite sort of beautiful elements of the symmetric group algebra. In particular, they generate a maximal commutative sub-algebra. Commutative subalgebra, and they somehow you can sort of develop the representation theory of symmetric groups with really the use of Smurphy elements as some sort of fundamental basic ingredient. Okay, so how do I come up here? Well, all I'm going to do is I'm going to take my orthogonality relation, and I'm going to think of this as, well, some element of the symmetric group algebra. How do I do that? I just sort of sum both sides. Sides with the permutation sigma sitting next to it. Okay, so let's have a think about this. I'm going to sum over all the permutations in the symmetric group, SK, and then I've got this. Sorry. Okay, so I've got this on the left sitting next to sigma, but now an element of the group algebra, and this equates to some other expression here on the right. Okay, Okay, so all of this sitting next to sigma. Okay, and I think hopefully you can see what's going to happen. On the left, what do we see? Essentially, this is a sum over all the sigma NSK of this class function, W sigma. And I can put this N out the front. I can pull this n out the front, we get this n factor, and this is really just hitting this element of the group algebra by the use of Murphy element jk. Okay, so that's the left side. What happens to the right side? You'll see it sort of looks very similar. For this first term, we're really sort of reducing to a sum over the permutations with living in SK minus one. And we get this pre-factor of capital M coming from this first term and from the second term, again, the users Murphy element jk. Okay, so what are we saying? We're really saying if you're interested in this element of the group algebra, which is indeed a sort of a class function, you sort of move down to the Uh, you sort of move down to the next symmetric group down, but you pick up this factor of m plus jk on n plus jk. So, sort of iterating over this, what we see is that this particular element here has this beautiful expression. It's a product i equals one to k of m plus j k all over All over n plus jk. And so that's actually quite sort of explicit. I guess much like in the previous talk mentioned sort of things like character formulas. So you can sort of use some sort of these sort of burnside for Beni's plate character formulas just to calculate sort of any value of this that you like. And then you'll, from this particular structure, you see the sort of polynomiality in M and the sort of behavior of capital N as a rational function of capital N. It's capital N. Is it clear that these are invertible? Well, we'll see in a second they are. I guess another question you might have is, well, I already said that they commute, so that's sort of well defined. Okay, so what might one be interested in? I guess I think almost independent of Almost independent of what sort of mathematician you are in this room, I think a lot of people here are used to sort of looking at large and expansions of whatever in whatever world that they live in. So let's have a look at the larger net expansion here. Okay, Okay. Okay, so I'm going to introduce some parameters. I'll introduce an H bar, which hopefully is rather suggestive, sort of not so dissimilar from H bars we've seen in other talks. And then some new parameter here, T. So T needs, something needs to keep track of the capital M for me, and it's T. For me, and it's t, and for some reason, I'd like to sort of keep track of the capital M in this particular manner: t equals one minus capital N on M. And what I'm going to do now is I'm going to try and sort of extract from this representation theoretic expression here. Some large n expansion for m sorry, sorry, some large expansion Sorry, sorry, some large n expansion where we're thinking of this ratio m over n as being fixed. Okay. Okay, so what does this look like? So I've got this here. I'm going to sort of try and write this out using my new parameters. Looks like the following. So we get one plus t h bar to the H bar to the i j i, sorry, sorry. One plus T H bar J I all on one minus H bar times the C Smurfy L and J I. And then I want to think about what this looks like in this large n, hence small h bar expansion. So you see at one plus plus t h bar j plus t h bar squared j squared plus t h bar cubed j cubed etc and then you just think what happens uh if I want to sort of peel off the coefficient of sigma from this expression okay well what am I doing I'm multiplying these sort of k-big brackets together And what am I going to get? So, I can record what I get by how many h bars I meet along the way. But it's going to be some sort of big quantity here sitting in front of h bar to the m. And then what goes in front? It's going to be some sort of enumeration of transpositions. So, what I'm doing is I'm sort of trying. So, what I'm doing is, I'm sort of trying to take a transposition from the first bracket. Sorry, some bunch of transpositions from J1, which happens to be empty. And then I'm going to take some sort of bunch of transpositions from J2. So some transpositions of the form 1, 2. And then I'm going to take some transpositions of the form 1, 3, or 2, 3. And we're going to take some transpositions of the form 1, 4, 2, 4, or 3, 4, et cetera. And I keep going. And so at the end of the day, every time I use a transposition, I pick up a factor of h bar. So what this m is recording for me is how many. So what this M is recording for me is how many transpositions I see. So that's what we'll be counting. When I do this, at the end of the day, I'd like to see this sigma come out. Okay, so this product here should be sigma. Recall though that when I'm talking Recall though that when I'm sort of writing down my transpositions, this B1 here, you've got to start off with something like two or whatever you start off with. The next one has to be at least as big, and the next B has to be at least as big again, and this is the notion of monotonicity. Okay, so that's all well and good. Except now I haven't said anything about the T's, and so to sort of talk about the T's, I'm going to put this So, to sort of talk about the T's, I'm going to put these little quotation marks around here. I'm not going to sort of count in the sort of baby way. Of course, the baby way is to put a one next to every element and add up all those numbers. I'm going to put something in my sort of favorite abelian group. It's actually going to be a monomial in T. So what this means here is that we're going to count with a particular weight. The weight is equal to what? Well, when do you? Weight is equal to what? Well, when do you see a T? Okay, so you only see a T if you happen to use something in a particular bracket. If you didn't, you just got the number one. So you see a T for every different B here. So this is T to the number of B's, but we don't count with multiplicity, the number of distinct B's. Or in our paper, we call this T to the hive number. Oops, T to the hive number. For reasons which I hope are obvious. Okay, so that's all I'll say about the representation theory side of things. And then the last sort of piece of the puzzle is to consider the monotone Howitz numbers. What happens in this case? Well, you can essentially see in the previous talk, we were introduced to monotone-herwitz numbers, but it's sort of. Introduced to monitoring how much numbers, but sort of purely from the representation theoretic perspective. But setting t equals one here essentially produces here, these numbers would be called something like disconnected monotone Herwitz numbers, and we want to look at the connected analogs. Okay. So, as I said at the top of the lecture, you know, this is a classic sort of tale, classic meaning I guess it's been around for 10 to 20-ish years, being sort of retold. But I think at the end, what I want to say is that there's some sort of new phenomenon that we see, which could not have been seen, which could not have been seen without this sort of introduction of this. Not have been seen without this sort of introduction of this M and N. Okay, so what do I want to do? I sort of just want to take this as my inspiration. I'm going to define Montevideo and Hurwitz numbers. I say numbers when really there's a T here, so they're going to be polynomials. So I'll use this sort of genus language. We have H, we have a G, and we have an N. I'll put a T to remind us that there's a T parameter. And it's going to take in n positive integers. And I think of these n positive integers as my cycle type, my permutation. And then I do the same thing. With one extra caveat, which is a connectedness or transitivity condition, we want These transpositions to multiply to give this permutation sigma. We want this monotonicity, but we also want the transpositions to generate a transitive subgroup of the symmetric group. The transpositions act transitively. Okay, so I do this and then I. Okay, so I do this and then I want to, I think, do some normalization here. So times one over the product of the mu's. Okay, so typically these might be, I guess, rational numbers or rational polynomials. Sorry, not rational. Rational coefficient polynomials is what I mean. Okay, and so Sorry, that's what, and this means deform. So same weight, so yeah, with the T weight, or with this hive weight. I guess the only other thing I need to tell you is you might be wondering what's M, what's G. Many of you will know this is just the sort of usual relation by the Riemann-Herwitz formula. Okay, so it's just a slight repackaging of these numbers, and then I would say, well, let's say I gave you something like H11T of 4,1. I say, go ahead and calculate. So I'll give you all a few seconds, but I think we're going to beat you to this if I can get this right. So I believe this is 35. So I believe this is 35 t to the 4 plus 175 t cubed plus 175 t squared plus 35 t. Okay, so okay, I don't know if anyone beat me to calculate that, but I'll give you another shot. Here's H11 of T. Let's put it 51. Okay, so what does this look like? This is 70. like this is um 70 t to the four plus um 560 sorry 70 t to the five plus 560 t to the four plus uh 1050 t cubed plus um 560 t squared plus 70. okay so just a couple of examples to stare at So, for the remainder of the talk, I just want to mainly tell you what we can say about these numbers, some things that we can't say about these numbers, at least with certainty, and then tell you maybe about some other versions of this story. So what might you be wondering about these numbers? I mean, I think they're something that should immediately come to mind. Can you calculate them? And if so, can you calculate them recursively? These numbers or polynomials. These sort of numbers or polynomials. And of course, yes, you can. And all the sort of usual sort of things that you expect from monotone Herbert's numbers or even Herbert's numbers more generally sort of persist into this deformation. And not for sort of, I mean, usually for not too difficult reasons. So we have some sort of cut join recursion, or you could think of them as something like Virasaura. Of course, you might be wondering, is there a topological recursion? Of course, there is. Is there a topological recursion? Of course, there is. You also might be wondering what the spectral curve looks like, and it's this: xy squared plus b minus 1xy minus y plus 1 equals 0. I guess you can come up with your own rational parameterization of that. What else do we have? Of course, if you like quantum curves, there's going to be a quantum curve here. The one I sort of want to sort of stress more than the others, and not because they're more important, but maybe just a little bit less known, is that there is a Just a little bit less known is that there is a one-point recursion here. So, just to write this down, let me just, so what do I mean? So, what I want to do is I want to concentrate on the numbers where there's only sort of one entry, so where n equals one. Okay, and I'm going to sort of just re-normalize this, but it'll make my life easier. Let's put a d here and I'll notate this by some curly d of g, d. Okay, and it turns out that we have this nice, rather effective recursion here. So d times Hgd equals. I don't think I'm going to remember this. Let me quickly check. Okay, so we get t plus one, two d minus three. T d minus 3 times h of g d minus 1 minus t minus 1 squared d minus 3 h of g d minus 2. So this is all sort of genus g contributions so far, but then we have to include something that takes us down to genus G minus 1. These are sorry, sorry, thank you, thank you. These are n equals two, thanks. I was too busy counting these to count these. So I should have said, I guess a lot of this, all of what I'm saying is. This, uh, all of what I'm saying is joint work with uh previous uh PhD student, Elena Moskowski, and current PhD student, Xavier Bolter. Um, the reason why I want to sort of stress this is that I'm not sure if people sort of understand these one-point recursions very well yet. With another sort of previous PhD student, Anapan Chaudhary, we proved that for all rationally weighted, rationally weighted g of z, rationally g weighted Howitz numbers, you're going to have some. You're going to have some one-point recursion like this, but possibly with many, many terms, which we can't really predict very easily in advance, but also more than this. Finding these one-point recursions is entirely algorithmic. You give me the weight. Theoretically, I should be able to crank a handle and come up with this. So this was actually sort of not produced by me, but produced by the computer. Okay, so I've told you about the recursion. About the recursions, what you can do is you stick this on your computer and you see just thousands of polynomials come up, and you can ask, well, is there anything interesting about these polynomials? Possibly you can already guess some interesting things about these polynomials. What could you ask about? You could ask about coefficients. And have a look at these coefficients. What do you observe? Well, firstly, they're palindromic. Okay. And they also sort of go up. Okay, and they also sort of go up and they come back down. So, this is a sort of general phenomenon. So, I guess we would say that they're sort of palindromic and unimodal. The people in the business, people who work with Polynesian unimodal polynomials often call them very suggestively like Lambda polynomials, which I think is sort of a cute name. Okay, so that's what we can say about the Okay, so that's what we can say about the coefficients. And I guess this is where, when we're dealing with polynomials, we're sort of going beyond what you can observe with just numbers. Possibly more interesting, you might say, well, what can we say about the roots? So I'll let you quickly do a calculate. You calculate the four roots of this first polynomial here. I'll give you one of them for free, t equals zero. You can calculate the five roots of this one. You can calculate the five roots of this one here. What do you observe? What could you observe? I mean, what are roots? I mean, they're just going to be sort of a bunch of complex numbers, except for the fact that they're not. Okay, so it turns out that these are real. Sorry. Um but sorry okay so they're real uh rooted uh but even more than that they satisfy some property uh called interlacing and actually I don't even know if they do this what I can say the theorem is that they're real rooted and interlacing for um about 2,000 examples and so conjecture they're real rooted and interlacing for all examples. All examples. Okay. So, what does insulating mean? It means if you take the roots of your favorite example of one of these polynomials, but then you also take the roots for another example where you've bumped up one of these parameters by one. Right, they interlace, which means that they do this. Okay, so they alternate along the real line. Okay, so. Okay, so the claim here is that, you know, this is this sort of notion of this integration on these graphs mains has introduced us to a def a natural, I claim, deformation of the monotone-Hurwitz numbers into polynomials, and that once you're in the world of polynomials, you can start observing new phenomena. The sort of claim, which I don't make particularly boldly, is that this is not an isolated sort of incident and that you might be able to find other examples. And so let me tell you another example where all of this holds. Another example where all of this holds. So this all holds for the enumeration of Dessen-Lafant. In that case, you might be wondering what this T parameter record is. So this is a weighted. So, this is a weighted enumeration now because I want polynomials, but weighted by t to the number of, let's say, black vertices. So, descent enfant, I guess you could also call them bipartite maps, bipartite ribbon graphs, the sort of very sort of rich sort of enumeration of Dessente L'Fant was performed by, I guess, could be various people, but I sort of learned it from Maxim Cassarian here and Peter Zograf, who records. And Peter Zograf, who records sort of many, many sort of parameters here. But if you just sort of restrict to just a recording number of black vertices, you observe essentially everything I said. Yeah, but in particular, this sort of the fact that you get palindromic unimodal polynomials and that the roots are real-rooted interlacing for about 2,000 cases, conjecture they'll be real-rooted and interlacing for all cases. So I guess maybe. So, I guess maybe, I don't know if this is really the moral of my story. Maybe it's something that, but maybe it's suggestive of something someone can try. A lot of sort of this sort of study was actually motivated by a sort of basic question that was asked by Max Karov to do with topological recursion. And then I guess one thing you might learn from this is that maybe topological recursion is a Um is a is a source of interesting polynomials, let's say. So here in the disk case, the sequence of polynomials we get are essentially very well studied. These are the Narayana polynomials. They're known to be sort of real-rooted and interlacing. Topologizing this sort of sequence for the higher G's and N's somehow still gives you real-rooted and interlacing for reasons that I have Placing for reasons that I have no idea about yet. Is this related to GUE? Oh, how is this T definition related to GUE? I'm not sure exactly it should be. In what sense? Do you from the spectral curve perspective or? Perspective for the company is a very important part of the company. Sure. I don't think it's sort of GUE, sort of, on the nose. What you would get there is, so as I said, sort of in the disk case, you're seeing these sort of Narayana polynomials. We stored them in a very sort of particular way to recover these default monotone Hurls numbers. One could ask, you know, what if I stored them in a sort of slightly different way? What you would get is this sort of descendant problem. And then you could ask again, you know, what if I stored it another way, which is closer to G? If I sort it another way, which is closer to GUE, I think what happens is you get a spectral curve that I haven't sort of studied very well, which I believe in that case is a genus one spectral curve. Not sure. In the deformation, but I could be saying this incorrectly. I think this one is not GUE just exactly, but there are other versions that you could do. Anyway, we can have a chat about it. About it. Okay, so I just wanted to finish with maybe a little sort of bonus section. Okay. Okay, so I guess I can. Yeah, of course. In genus zero, do you mean the disc? In Genus Zero? Do you mean the disk case or Genus Zero in general? Yes, yeah. I mean, so, I mean, it's essentially the, I mean, you're looking at the disk case. Yeah, so, you know, you could just run this recursion, and what you're getting is Narayana polynomials, I guess, which Narayana polynomials. So it's a refinement of the Catalan numbers. Refinement of the Catalan numbers. So it feels like GUE, but I think that's right. Yes, yeah. Narayana polynomials. Yes, that I think is correct what you're saying. Yeah. Maybe we can chat about it afterwards. That's right. Yeah. So these are somehow, these numbers are dual. So in that case, what you would recover. So, in that case, what you would recover is this sort of Da Saint-Dauphin. This is somehow like the dual picture of that. So, you still can recover it from LUE, I think. Okay, so I guess one thing here is for this sort of type of phenomena in other places, there's a sort of very natural sort of place you could look. You could sort of ask for sort of integration on now real grass manions. Now, real Grassmannions. And you would sort of guess that this whole story should carry through. I guess now the representation theory sort of gets slightly more complicated rather than looking at just the symmetric group. You're looking at this hyperoctahedral Gelfund pair. And then you could sort of talk about what's the sort of analogous sort of numbers. And I guess you could call this something like orthogonal. You would call this something like orthogonal monotone hero summons. And after you've done that, you could say, well, you know, we've learned from Machek's talk that there's a very natural sort of B deformation that you have in many problems. And here the B deformation should take you between the complex Grossmanian case and the real Grossmanian case. Okay, so we. So we, I guess, for want of a better name, I'll just call them deformed B-monitone helitz numbers. Sorry, but deformed B-monitone helitz numbers. And what should happen is that at sort of b equals 0 and b equals 1, you should recover this sort of known cases. Here's the complex case, and here's the real case. But unfortunately, here, you know, we can try and observe all the sort of properties. We do observe essentially interlacing for not quite general B, B above some certain bound. I think it's B above negative one. Be above negative one, you observe real rooted there as well. But somehow, yeah, so but we're sort of still missing here the rest of the story, which is some sort of notion of integration or some notion of the representation theory aspects. My understanding is that this sort of B deformation parameter, there's some things understood about the representation theory, but not there's not really a But there's not really a complete picture here. So it's not sort of some sort of obvious sort of construction that would sort of generalize this to all B. Matchak's nodding, so I think that confirms. Okay, and I think with that, I'll finish my talk. Thank you.