Thank you. Thanks for being and Helen Xiao for the invitation. Today I will talk about some very classic work in machine learning like clustering we always use. One of the issue related themes is when you obtain a clustering result, how robust and stable the results are, or reliable you call it a cluster. So this So this is line work we have been doing for many years and some old and the new work have talked about a bit. Okay, so you're all familiar like heat maps, so we can plot a data matrix. Like here in gene expression data, without ordering, it's just random coloring we can see, right? And then often we can try to use, say, hierarchical clustering by ranges. Say hieratic clustering by arranging the rows and columns to identify some block of similar color. So we can say this group of patients together with this subgroup of genes, they may have similar behavior. So they may corresponding to subtype of disease and the potential treatment are similar for this patient as well. So this is very common techniques in machine learning. People have used it often in many scientific studies, especially in medical study. Study. So this has been used, say, in many cancer research. One of the earlier work, I think around 2000, there was a science paper identify breast cancer subtype. And then this subtype turns out to be requiring very different treatment. Some chemotherapy doesn't work, but some other chemotherapy works. So obviously that's very important for the treatment of these patients by knowing which type of subtype they belong to. Type of subtype they belong to. So, one of the earlier work, they essentially really just use the inspection data, cluster them, and then say these are subtypes. That was published like in Nature and Science. And then people doing similar thing, and a lot of people identify subtypes, say all sorts of cancers. So, we are also involved with some of that. But then later, the reviewer said to us, you can keep doing this, keep calling a subtype. But at the end of the day, there got to be The end of the day, there got to be some kind of artifact rather than something real. So that's called for the issue of how significant these classrooms are, or if we think about robustness of machine learning, how stable the results are. Are they something real or just being random? Happened by chance. So this is the outline of what I'm going to talk about today. So I will talk a bit more on clustering and then discuss the issue. And then discuss the issue of significant classrooms, some of the earlier version work we did for C-class to assess significance. And then some issues involved the earlier version and some recent improvement we have done and the connection with very recent work by others as well. So as I said earlier, clustering is one of the very traditional machine learning technique to be used for unsupervised learning. There are different versions. There are different versions, but the standard version is just to identify structure in the data. And it's very useful, and there are many different algorithms in the literature, such as k-means, hierarchical, and so on. I'm sure you have used some of that in your own data analysis. So as I said earlier, when you have done some clustering on your data, then the natural question is, are they real? Is are they real or just being artifacts? So that's caused for the issue of assessing the significance. Then, this issue can be more challenging if the data dimension is really high relative to the sample size. Like, this is often in many genomic study and many medical study as well. So, then we know that a lot of conventional methods may have some problems, such as given your estimate covariance matrix, it's not going to be full-ranked, et cetera. The front, etc. So, assessing significance is more challenging, as I will show you a little bit. But before we move on, just the issue of what do we mean by significance, and then in fact, that's cost what do we mean as a cluster to begin with. So, before I move on, just to show you a very simple toy example. So, here we just similar data from standard normal 01, and then we plot it as. And then we plot it, and then the height is random, just for better visualization. And in this case, if we apply, say, two means is essentially extreme labeling, that the small one is one cluster, the large one is another cluster. If you apply, say, t-test, it's really significant. But if I call normal data as one cluster, this is obviously not quite right, right? So because I really just simulate data from standard normal zero one. From standard number 0, 1. So you shouldn't just call it a two-cluster by simply calling small one and large one as each one as a cluster. So in this setting, if you think about Gaussian cluster definition is reasonable, then obviously this should be just one cluster rather than two clusters. So if we do like t-test, it's very significant, but that's not quite to be the significant clustering. That's more like subgroup differences or sub-populated differences. Or subpopulated differences. So, in this case, conclude the subpopulation difference is not quite the same thing as calling significant clustering. Okay, so as I said earlier, in terms of clustering, we need to decide what do we mean by definition cluster. There are some existing work to say determine how many clusters and it's related, but here our goal is to detect. Goal is to detect the significance of clustering. And there's some also other work in validation clustering, such as using resampling techniques for prediction strengths and so on. And I guess we are related to the Gaussian mixture for the earlier part of the talk, in the sense if we think about each Gaussian component as a cluster, then the Gaussian mixture is really fine with different components of different clusters. But in that case, if you try to In that case, if you try to fix a Gaussian mixture distribution, you need to estimate the entire parameters involved. That is, all the means and covariance, etc. That's a very challenging problem. So there are works you can do by imposing sparsity and so on to estimate that, but that's not quite the same thing if I just need to assume, assess the significance. I don't necessarily need to ask me all these parameters in the end. Yeah. Okay, so in terms of the definition classes, it's in fact there's no universal definition. What do we mean by a cluster? Although we all know that within the cluster, they should be more similar between they are more different, right? But what do we mean by a cluster? I don't think there's a definition. Everybody agrees. Although, a common definition, perhaps, is Gaussian distribution can be a cluster if you use a Gaussian mixture type of notion. Type of notion, and that's also limited, right? If your data are skewed and so on, you may not necessarily just only call the Gaussian data as one cluster. So I will talk about that later as well. But for now, think about as Gaussian to be a cluster definition, and that we will generalize it later. The main difference for C class, what I will talk about first, is that we do not really need to estimate entire parameters in order to do testing problem. To do testing problem. Because in Gaussian mixture, you need to estimate everything. But here, I just want to see if it's really significant to classify the data or not, that you don't necessarily need to estimate everything. Okay, so if I use a Gaussian definition, one of the simplest testing we can start with, is the data come from a single Gaussian distribution or not. If it's single Gaussian, I say, well, you shouldn't divide, otherwise, you should divide. So if that's the Gaussian distribution, So, if that's the Gaussian definition to start with. Then, the test statistic we use is to quantify in terms of classical operation, we use the classical index. It's really essentially assessing the results variation over the total variation. So, what the procedure I'm going to present first is we use estimate a null distribution as assuming it's a Gaussian distribution, and then we Distribution and then we use the test statistic class index that gets the p-value. So, some of the trick we use here is: I do not really want to estimate everything, so we use rotation invariant of the test statistics and the factor model to simplify a lot of framter estimation. So this is very simple test statistics. You can see the numerator is just really within cluster variation divided by total variation, which is really some number between zero. Which is really some number between 0 and 1, right? So, in the extreme case, if I have, say, very little variation within, then I have very tight clustering, so the clustering index should be close to 0. Otherwise, it should be close to 1 if it's very poor clustering. So, that's very simple thing, it's between 0 and 1 in terms of proportion on how much variation you left within the cluster compared to the total variation. Variation. So I need a null distribution to do this. So the null is a normal distribution with, you know, think about it as a high-dimensional. So you have a long vector mean and a large dimension covariance matrix. So because I do not really care about the mixture type of estimation, I just want to do testing. The mean doesn't matter because the variation explained doesn't depend on where it's located. So I just set a mean to. Located. So I just set the mean to be zero. And then the covariance matrix, because the test statistics is rotation invariant, we can assume it's diagonal as well. So that doesn't lose anything. Essentially, it's just a property of the test statistics. But even diagonal is still too much because the dimension is high. So we can use it sort of the factor model by assuming that I have relative low dimensional at. Relative low-dimensional as a signal, and then the background noise. So, this you can think about a few large ones, and then the rest is very low background noise level. So, in the end, I need to estimate these large lambdas, and then the background noise, we can use some robust estimator like mad mean absolute division of median. So, that gave us sort of the non-distribution, which is really univariate because I'm assuming it's that. Univariate because I'm assuming it's diagonal for covariance. So mean is zero, variance is really look at the maximum between the sample eigenvalue and the background noise. So this is really the whole idea of the earlier version many years ago. Now it's more than two years ago. In 2008 in JASA, we have this version as the first version of C class. So just put together what we did. So we first So, we first calculate the class index for the original data if you perform some sort of clustering. And then you can use original data to estimate robust version background noise and get the sample adding value so that then we get the null distribution, which we can use to simulate the data under null. So, for simulated data, I can then perform the same class 3 operation, like here, two means, and then we get the test statistics. Test statistics for each simulation, then we do it many times, we can get p-value comparing the observed test statistics. Is that clear? The procedure? Okay, so if we go back to the toy example, the normal data I simulated earlier, by doing this, each blue dot is by simulation the class index. Now, this particular value here for the Particular value here for the green bar is the observed one or the original data. So in this case, the p-value is like 0.4, which means it's not significant to divide data at all, which is the right thing to do if we're assuming Gaussian is the right thing to not divide. Alright, so there's some the rather probably said there are some weaknesses as well. Internal property, if the not hypothesis is true, meaning it's normal, the p-value can be shown to follow uniform, so that means I can preserve the test level, right? And then if the data come from a mixture of two normal as long as two normal are recently separate, the p-value is essentially going to be zero. So that means it's going to be powerful to detecting if there's multiple clusters. But there's some problem. In fact, if your dimension gets really In fact, if your dimension gets really high, sample size is limited, this relates to a challenging issue about eigenvalue estimation. In fact, in high dimensional eigenvalue, we know it's not going to convert to the true eigenvalue, and this is going to have some problem for our test as well. So this related to the random matrix theory as well. So the result actually show that if it's more spherical type of situation, the test is relatively conservative, which is fine. Which is fine, but if it's extremely spiked, meaning you have some really large eigenvalue compared to the rest, the result can be anti-conservative, which is problematic, meaning we may get some faucy discovery. So just briefly talk about the issue. So the original version is we ask the sample eigenvalue. If it's greater than background noise, we keep it. Otherwise, we repeat. We keep it, otherwise we replace by background noise. But this large eigenvalue estimation is actually not going to be consistent. So we have a soft threshold coding method try to address this issue by bringing the down by the amount of tau. I will show you a picture. Maybe it's easier to show this picture first. So suppose this is the eigenvalue for the covariance matrix. We have a bunch of large ones, and that's Have a bunch of large ones and then the rest are small. If you have dimension high, sample size low, what happens with the sample covariance estimation is that this is going to be a bunch of really large one. You can see it's overall larger. The reason because if the round is bounded by your sample size, right? If your dimension is say a thousand, your sample size a hundred, you essentially pack all the variance to Essentially, patch all the variance to the 100th eigenvalue. The rest is going to be zero. So that makes it over-inflated. It's larger than it's supposed to be, which is not quite to understand because I packing everything to the earlier large eigenvalue, sample eigenvalue dimension. So that's what we call the hard thresholding. So the soft threshold holding is we try to bring it down to keep the total variation to be the same. Because the original version, we keep. Because the original version, we keep this sample eigenvalue, the rest we replay the background noise, then the total variance is going to be larger. Because this part, it avoids zero for sample eigenvalue, right? And now I keep the total variance, then bring it down by the amount of tau, that would be called soft thresholding, essentially bring down those large eigenvalues. That helps somewhat. So let me go back here. So let me go back here. So this is what I mean by the soft thresholding by amount of tau and then preserving the total variation. So this soft thresholding reduce the bias of large added values and then we have a version called the combo method, take the largest p-value of between the two. So try to make the conservative. So we have an R package called SIG class and that's the version of the hard and soft built-in in this The hard and soft building in this package, it actually was very successful. And last time I searched, many nature science papers use our method to do the significant clustering. But actually, it's still not good enough in many ways. So that's some recent work I will talk about, which is just published last year. So the issue about the C-class is in high-dimension data, the sample Dimension data, the sample eigenvalue estimation is problematic. In fact, Larry Wassman had a paper in 2019. They showed some cases our methods fail. They quickly asserted that they have a eigenvalue, variance really large in one direction, but the signal is on the small variant direction. And that is actually our method is going to look at this large variant direction and then lose the signal. So that's the setting the method is going to fail. Setting the message is going to fail. And in general, it's actually very difficult to estimate the eigenvalue because the dimension gets so high. So the intrinsic difficulty is the high-dimensional eigenvalue estimation. There's also some setting our method doesn't work as right now we need our original data to do everything. If you don't have our original data in some application, we only have the various, only have dissimilarity matrix. Only have a dissimilarity matrix to begin with for clustering, and that is not possible to do sync class. For example, natural language data sets, you may only have the dissimilarity matrix to begin with. That is not possible to do our method. And then the third issue is, what if I don't like Gaussian assumption for a cluster, right? For example, we may think about each mode as a cluster, then we need to extend the assumption as well. So before I move on, just to mention a tool we use is using multi-dimensional scaling, which you're probably familiar already, which is really just to project data from the high dimension in low dimension space by preserving pairwise dissimilarity. So this method was very popular, and it's actually quite a number, I at least find two science papers just on this particular method. So it's pretty. So it's pretty popular. It's really just preserving in low-dimensional the relative location of your points. So you're preserving their pairwise distances, which is really perfect for clustering purpose, right? For clustering, we have this dissimilarity to use to do clustering. So our ideas can we utilize MDS and then in the sense I can bypass a high-dimensional estimation issue. Dimensional estimation issue. And in this paper, Lido et al., this is one of my early previous students, they had a paper showing MDS can preserve in the classroom structure. And then, in the sense that if I project my data in the low dimensional using MDS, your classroom structure is mostly still there. So then the idea is: can we utilize that to do the testing? Testing. So, in the very recent work, we have done this, we can use the multi-dimensional scaling to help us to do C-class in the sense that I project the data in MDS space, and then we can do C-class there, and not only two clusters, we can also do any number of clusters as well. Another extension we consider is beyond the Gaussian definition. So, we don't have to only consider Gaussian cluster. Dozen cluster can be arbitrary type cluster in the sense that as long as each mode is clustered and then multiple cluster setting as well. So for the interest of time, I won't get to all the detail, but here in terms of beyond the Gaussian case, the null could be the data come from a single D-dimensional unimodal distribution. And then the alternative, a mixture of several unimodal distribution, then you can Unimodal distribution, then you can think about each cluster corresponding to sort of a mode type of setting. So that's Carver Gaussian as a special case. So the main idea is I project the data in the MDS space. So now it's dimension R. So R can be really low, like 2 or 3. And the cluster structure preserved in the MDS space, either single unimodal or mixture unimodal distributions, and Distributions, and we can still capture clustering using class index and apply C class on the projected space. Okay, so the methodology is that we could do k minus one test. So each one consider two clusters, three and so on. And each one we can calculate p-value, and we do multiple comparison adjustments by looking at the minimum p-value. Look at the minimum p-value, and then we do that for the simulation as well. So, essentially, you want to identify: is it three the best clusters, or four is the best number of clusters? So, some theoretical results, still under null is uniform. And an alternative, we can control the type 2 error in the sense that if your signal is reasonably large, we can Reasonably large, we can come from mixture of Gaussian, then the p-value converges to zero as sample size grows. So it's essentially saying if there is no separation, we can do it, detect it using C class. Okay, so earlier I talked about the Gaussian type of clusters. So now our test statistic itself doesn't really depend on Gaussian. In fact, the test statistic In fact, the test procedure can work for a number of non-Gaussian types of data as well. And essentially, even you have non-Gaussian, the class index still indicates whether it's reasonable to call it multiple cluster or single cluster. And the Gaussian null distribution is still needed. If I need to simulate the data, right now, you know, what's the null distribution should you simulate if you don't go beyond, if we If you don't go beyond, if we don't use Gaussian definition, it's not clear because that requires what type of data you have on the original data. And we show that Gaussian is a relative conservative reference distribution. So in the sense that even the cluster is say chi-square or some other distribution per sign, even, I use Gaussian to simulate now, it still works. It's actually relatively conservative. Okay, so. Okay, so for the interest of time, let me just briefly mention the numeric example. We used a bunch of numeric examples on cancer subtype, such as pairwise type of comparison, joint comparison. We also did some null experiments. This is based by reviewer in the sense that if it's just supposed to be one cluster, we do not want to divide a bunch of clusters. So just want to show that in terms of pairwise comparison, this new method can work better than the original method. In some cases, the original method fail. And yeah, let me skip this. And then for multi-cancer as well, the new version does a better job in the original version as well. So finally, let me summarize what we did and discuss. Summarize what we did and discuss some extension. So, the origin method we talked about can be viewed in the likelihood point view as hard thresholding. And the soft thresholding is try to correct the inflated eigenvalue estimation, but still the difficulty of the eigenvalue estimation makes the test problem still difficult. So, the MDS version essentially are bypassing sort of cheese a little bit. We do not deal with that eigenvalue estimation anymore. With that eigenvalue estimation anymore, we do it on the low-dimensional space. There's some extension we have done in hieratic clustering, and that version recently extended by Eurozaris group in Harvard for RN-SIG data. They actually published in Nature Methods for extending our hierarchical version for RN-Seq data, which is extremely skilled, and then a lot of zero inflation. Zero inflation. And so they are extended using our original version, which is still requiring the high-dimensional eigenvalue estimation. So I have a student working on this version of using MTS for RN-SIC data. There are some other lines of research related to this. For example, Larry Wassman's work using the relative test. There are some selective inference literature as well. All right, so I think that's all. Thank you very much. Thank you very much. Thank you, Jisol, for the nice talk. Any questions? So, we'll ask. Yeah, I think, as you mentioned, I think there are certainly some actual uniqueness in your first version. And the second version, I think, is more flexible. But a potential bottom is that I think that approach is more similar to just the selection, for example, number of clusters. You think about it, you're real with. Right, you think about it, you're real with so. I don't know whether people are going to ask you to compare with that, just uh, in comparison, yeah, you're fitting the model now, right? You said the first advantage, I don't need to fit the full model, right? It just compares the model with whatever authority. But now you're now thinking about the Gaussian mixture, you're trying to figure out what's the yeah. So, here I still don't estimate the different component either. It's just two versus uh, two versus uh not two, three. Not two, as three versus, well, essentially, you are talking about the multiple one we are working on. It's really using the cluster index, it's using two clusters, three clusters, and so on. But I still don't estimate them. So you're testing two versus three, three versus four, right? No, it just two are not two. Three are not three, that kind of thing. So I don't ask me about the component either. Okay, okay. So it just, once I apply the Just once I apply the cluster, and then I have the class index, and that's all. We don't ask me the mutual components. Right, right, right. That's right. But we do this for the NOW as well. So that adjusts the multiple comparison issue. Okay. Thank you. Do you assign actually individuals into different classes in NATO? So we need to apply classing algorithm first in order to calculate. First, in order to calculate the class index. So that by classing an operation, you need to assign which one is a cluster, right? So, for example, if for this, you are testing, for example, three versus now three, and then so the group membership will be estimated using some existing algorithm, right? So, for example, if the null hypothesis is being accepted. Is being accepted. If you accepted the non-hypothesis, then you can use the class membership assign using the existing algorithm to assign the individuals into the classes. So you can also accept the group membership assignments as well by the existing methods. Right? So if you are. Right, so here you could use any classroom method to assign a membership. Once you have The membership. Once you have the membership, I just need to calculate the class index, the test statistic. And then the null part, we just use k-means. Because the null is simulated from normal, so we just use k-means. Since this is a hypothesizing problem, then at the end you accept null hypothesis, you can also accept the class membership assignments. So if we do not reject null hypothesis, right? Right, that's just saying, well, I shouldn't divide into three or I shouldn't divide the four. But then the membership, we can use whatever every song. For the sake of time, I think we can leave more questions to the brief. Let's thank the speaker again. So I'm going to go to the next one.