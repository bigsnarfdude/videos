From the University of Michigan, who's going to tell us about concert groups, root systems, and week work. Okay, thank you so much for organizing this. I'm really looking forward to this week. And what I thought I would do is talk about one of the sources of lattices, which is certainly very central to the things Grant and I want to work on, and I think is also relevant to other projects, which is coxchener groups and the weak water on cockschucker groups. So, absolutely, nothing here is new. A lot of it goes back to board. Is new. A lot of it goes back to Bourbaki. It's just meant to be a fast introduction to what are Coxlet groups, what is weak order. Okay, so a Coxlander group is a group which has a certain sort of presentation. The generators are called S1, S2, Sn. And the relations are that each of the Si squares to the identity, and that each product Si, Sj. Each product Si Sj has some prescribed order Mij. So a basic example is the symmetric group generated by the adjacent transpositions, the transpositions which switch i and i plus 1. And so, of course, they square the identity. And if I multiply Si times Si plus 1, that gives me a 3 cycle, so that's what a 3. But if I multiply Si times Sj, where I and J are more than Sj, where i and j are more than one apart, then s i and s j commute, and it swears to the identity. It's not obvious, but it's true, I think it goes back to teths, that this is a presentation of the symmetric group, that the abstract group with these generators and relations is the symmetric group SA. And so in our notation, m1, 2, m2, 3, etc., are going to be 3, and all the other mij's are going to be 2. Already be two. There are two other examples that I'm going to go back to in this talk, so I'll often use the symmetric group as an example, but there are two others I'm going to use. I'm also going to talk about the affine symmetric group. So this has a very similar presentation. We again have n generators, and the adjacent ones have order 3, but this time it wraps around. So Sn S1 also has order 3. So if you think in terms of the Coxner diagram, the Cox. So, if you think in terms of the Cochrane diagram, the Cochrane diagram is a cycle rather than a path. And all of the other SISJs, and they're more than one apart on the diagram, have order two. And I'll tell you how to think about this as sort of an infinite symmetric group later in the talk. And then finally, we can just have the free constituency group. We can just set all the m by j's to be infinity, and then we have the group where we have generators si, and the relations are si squared is 1, and no other relations. Is one and no other relations. Okay. Now, I gave this to you as a group of generators of relations, and in general, such a thing is sort of pretty intractable. But what makes Coxer groups very tractable is that they have nice linear representations. And so I'm going to tell you how to build that linear representation now. So, I'm going to go to a video representation on a vector space equipped with a symmetric bilateral form where each SI is going to act by an orthogonal reflection. So, here's the vector alpha i. Here is the hyperplane. Alpha i perpendicular. Alpha i is not an isotropic vector as I've thrown it. It's non-zero to output itself. And by simply And my simple reflection Si is going to negate alpha i and is going to fix alpha i prime. And it's convenient to normalize that each of i alpha i's has life square root of 2, in which case the formula for a reflection is what's given there. Let's give in there. If you've taught linear algebra recently, you've probably received that. And so a key piece of data will be the dot product of the alpha i's with each other. We're going to write Aij for the pairing of alpha i with alpha j. And A gets called the Cartad matrix. And then there's a small print note for people who have seen this before and are wondering where this fits in. So to abbreviate that, it's a little bit small. To abbreviate that, it's a little bit small because if you haven't seen it before, you're not going to read it. I am using symmetric cartile matrices. If you read more of the literature, you will run into symmetrizable cartile matrices and maybe even more general cartile matrices than that. There's lots of good uses for them. I would not be at all surprised to see them show up in the working group. But every Hochli group can be presented using a symmetric cartile matrix, and they're good enough for everything in the stock. Okay, so because I made SI act by a reflection, it squares to the identity. The other thing I want is I want Si Sj to have a prescribed order. So let's think about how Si Sj is going to act. Si Sj is going to fix alpha I perp intersect alpha j perp, because those have to be fixed by both of them. And it's going to take span of alpha i alpha j to itself. Look at that formula, you'll see that's the span of alpha i. where you'll see that the spare of alpha i alpha j is an invariance of the s minus j. We want it to act with order mij on spared alpha i alpha j, but actually we want something more specific. We want it to act by a rotation by 2 pi over m i j. So the determinant should be 1 and the trace should be 2 cosine of that. And a basic computation is that this is going to happen exactly if we make Aij, the top product between Ai and Aj. The top product Ai and Aj be negative 2 cosine pi over mij. You should think about that as the angle between alpha i and alpha j is pi minus pi over m i j. And so the angle between the Lithogo hyperplane, between alpha i group and alpha j group, is pi over m i j. Notice I'm not saying my dot product is positive definite. So all these times when I'm talking about angles, there's a, oh, you know, not positive definite, but still keeps sticking. You know, not positive definite, but still keep sticking with this theory of motivation. And so then, theorem, the religious computation, and this is an exercise, if we have a bunch of vectors with these specific dot products, then these SI give a representation of w. This is a representation of the vector space. This is a representation of the vector space V. I'm also often going to want to introduce the reaction of the dual space V dual. Now, very often I can identify V with V dual in such a way that alpha I becomes the inner product, and then you can think of them as both living in the same space, but not always. And specifically, the criterion where things go wrong is when the determinant of the Carton matrix vanishes, is we need to actually keep track of the difference between V and V to R. So, warning that's coming. Okay, I've been going pretty fast. Let me pause for questions, and then I'm going to run through how this all works with my channel. Questions, comments, thoughts, problems so far? Yeah. MIJ is intense. Is there a... What are you doing like, please? Okay, you couldn't. I'll give two answers. If you just want me to give you one formula which is correct and will work, then Corrected will work. Then pi divided by infinity is 0, and negative 2 cosine 0 is negative 2, and that will work. In fact, there is more flexibility. Actually, we'll take AIJ to be anything which is negative 2 or more negative than that. And there are times when there are advantages to that. But if you want the shortest presentation, I'll say thank you. Thanks. Other questions, comments, corrections, clarifications? Okay, let's check if this all works for the symmetric group. So I'm going to take my vector space V to be Rn, just the standard dot product. I'm going to take alpha I to be EI plus 1 minus EI. So then the inner product between alpha i and alpha j, alpha i dot alpha i is 2. Alpha i dot alpha i plus 1 is negative 1, which is negative 2 cosine i over 3, like I wanted. And alpha i dot alpha And alpha i dot alpha j when i j go further point is 0, which is negative 2 cosine i over 2 minus 1 terms. And what is the actual action on Rn? You work out Si, I actually got the vector x1 through Xn, is the vector where I have switched the ith value and the i plus first value. So this is the standard action of Sn on Rn. Okay. Um so so the symmetric group works like we want and now we're going to go back to generalities. Go back to generalities. If you start out as a group theorist, if you say I really like this group and I want to study it, then you can always take your alpha i to be a basis of v and the phi v by what you're forming v. And define the biodiverse form on V by those cosine formulas, and that's a formula that works. On the other hand, if you're coming from representation theory or coming from geometry, someone might give you a vector space with a bunch of vectors in it, and you might want to know if they work. So it's convenient to have more generalities where we don't need to assume that the alpha i specifically are a basis. And so here is the hypothesis that we'll make everything work. What we want to assume is that there is some vector which pairs in a dual space. Vector which pairs in a dual space which pairs positively with every alpha, which is the same as saying that there shouldn't be any positive linear relations between them. And that will be enough to make everything I'm about to say for the next 20 minutes true. And with that assumption, the actions of W on V and V dual are faithful actions. There's no kernel. So a specific case of that is to think of, due back to tits, that Due back to tits, that you saw that when I took this matrix for the symmetric group, the group that I got out was the symmetric group. And so, a special case of this theorem is that the symmetric group really does have that presentation. And say, the abstract group of this presentation really is the subgroup of GLV that you get. Sam, I see your hand up a bit ago. No, but I can ask a question. So, is the row the bio vector in case of the file vector? That's what I have. That's fine. So I'm calling it Moe because I am thinking of the vowel micron. Okay, so I've told you about the group, I've told you about its representation, and now I want to talk about the geometry of all these reflections and their reflecting hyperplanes. Excuse me. Okay, each SI actually. Okay, each SI acts by a reflection, so it fixes a hyperplane. Anything conjugate to SI also fixes a hyperplane. And we will call an element of the cocksure group a reflection if it is conjugate to an SI. And traditionally, it's going to be more like key, instead of reflections, it's denoted by T. Each reflection fixes some specific hyperplane HT, and for the cases where I'm distinguishing V and V dual, my H T lives in V dual. lives in the dual. And if t is u s i u inverse, that's conjugate to si, then h t is the orthogonal to u alpha i. This says vectors the form u alpha i should be important. Those vectors are called roots. In some contexts, if you're coming out of quiver theory, they're called real roots. I'm going to go just roots. So each hyperplane is beta perp for Hyperplane is beta perp for beta some root. And sort of the first big non-trivial remote you prove in this area is that every root is either a positive root, meaning a positive combination of the alpha i, or a negative root, meaning a negative combination of the alpha i. And notation, phi will be the set of roots, phi plus or the positive roots, phi minus are the negative roots. And then using the fact that I'm working with symmetric cartography. Using the fact that I'm working with symmetric Cartab matrices and using the positivity assumption, the hyperplanes, the reflections, and the positive roots are all in bijection by the obvious things. Like you might worry about something like, could one positive root be a scalar multiple of another? And no, that is symmetric or not matrices. There's no issues there. And other things like that you might worry about don't have. Or you might worry, could it be two reflections that fix the same hyperplane but have a different negative one? I could say, the answer is no. Negative one I could say is the answer is no, that doesn't happen anything. Okay, let's again see how this works very symmetrically. So the reflections, so the simple generators, switch i and i plus 1, which elements of that standard conjugate to switch i and i plus 1, the transpositions switch i and j. The hyperplane is fixed by switching i and j is the hyperplane xi equals xj. And the root normal to that is EI. The root normal to that is Ei minus Ej. And with the side convections I've chosen, Ei minus Ej is positive for i greater than j and negative phi less than j. Here I've drawn a picture of the S3 root system and the S3 hyperpoint arrangement. This is a picture in R3, but it's invariant under translation by the vector 111. So I've quotiented by that in order to draw a picture on this two-dimensional screen. There are six roots, three positive roots which I've drawn solid, and three negative roots which I've drawn dashed. Alpha 1 is E2 by the C1, alpha 2 is E3 by the C2, and alpha 1 plus alpha 2 is E3 by the C1. Notice the angle between these is 2 pi over 3, or 120. And here I've drawn the orthogonal. And here I've drawn the orthogonal hyperpoint arrangement. Okay. Let me pause again. We're trying to wrap it before I keep going, because I know I'm going fast. You had said that it was okay to not have the alpha IP that makes it. You're not allowing them to be linearly deep-handed. You're just allowing them not to spend the space. No, um, okay. No, um, okay. This is a figure that I was this is a figure wasn't going, but I do want to allow linear dependency because I want to allow subroot systems. So Nathan Turkey knows what I'm talking about. If you're thinking sign E3 tilde, then again, I do want to say that alpha one plus alpha two, alpha three plus alpha four, alpha one plus alpha four. Alpha 1 plus alpha 4 and alpha 2 plus alpha 3 already use this stuff of type A working show in a cross-sector. So I'm not quite able to focus on that issue in this talk, but I've anticipated it showing up in a working group this week, so I sign up for Problem Wizard to deal with it tonight. However, every example I have is slides, they will be video reindependent because that's the point I didn't want to get it to today. Good? Okay, a little more generality. D is going to be a set of those rows that pair positively with all the alpha i's. So the positivity subject is that d is non-empty. And interesting theorem, it's basically the same as the fact that if the void is positive or negative, D is a region of the hyperplane arrangement copy. If you take V dual and yike out all the hyperplanes, then D is one of the connected components of what's left. One of the connected components of what's left. It translates W D O R disjoint, and there are bijection with W. And I have an example of this soon where we toss out some more vocabulary, but I want to toss out now. So each hyperplane, D is on one side of it or the other. The hyperplane doesn't pass through. And I'm going to say that D is the positive side of each hyperplane holder. And the tips cone is the closure of the union of all these W T. The closure of the union of all these W D's. And theorem, the tits cone is exactly those points that are on the positive side of all but finitely many of the hyperlights. Okay, let's see what this looks like for a symmetric group, and then let's finally see some examples that are not the symmetric group. So, but here again is the symmetric group. So, once again, that's the S3 hyperpoint arrangement. There are six regions in the complement. Actually, the fact that the size is. The complement, matching the fact that the size of S3 is 6. On the left, I have labeled D down at the bottom, and I've labeled which element of the symmetric group translates D into each of these positions. On the right, I have labeled which explicit inequalities they correspond to. And in this example, if I take the union of all of those open cones and close it up, I get the whole space. It up, I get the whole space. The tits code is everything. And that's what happens for all finite coxoder groups. If you're used to finite coxcheter groups, but you're not used to saying the words the tits code because it's everything. Sorry, will you go back and say what the tits cone is? Can I just do the double check? The tits cone is the closure of the union of all W D's. Okay. And theorem is those points which are on the positive side of all the finitely medi hydroglades. So when we are finitely medi hydroglades, So when we are finally hydroblades, it's actually hydrogen. Okay. I'm about to finally get to our second example of a concert group. Excuse me. Okay, this is the affine symmetric group. So I already told you what it is as an abstract group. It adds relations in that M one two, M two, three, all the way around to M one Maps cycle are three, and the other J's are two. The other MIJs are two, but I want to show you how to think about a geometric way as a reflection representation and how to think about a common destroyer way as an infinite permutation. So I'm going to work with a vector space, which is finite-dimensional. It'll be n plus one-dimensional, but I'm going to describe it to you as the span of a bi-infinite set of vectors, e sub i for every integer i, module all the relations that e n possible. That E n plus I minus Ei is independent of I. Work it out, you'll see that gives you an n plus one-dimensional object. And my inner product is going to be that EIEJ is 1 if I is congruent to j by n and otherwise 0. So you can check that that product does descend to that quotient. And then alpha i is going to be what we're used to, it's going to be y. Alpha i is going to be what we're used to. It's going to be EI minus EI plus 1. And when we do the computation, what is alpha i dot alpha j? Well, just before, alpha i dot alpha i is 2, alpha i dot alpha i plus 1 is negative 1, including this time the wraparound. And all the other alpha i, alpha j's are 0. So this is a reflection of the this. So the Si is going to give me a representation of the affides electric group. So this is a completely computed. This is a completely computable, reasonable thing to give a computer. Like, this is a list of n explicit n plus 1 by n plus 1 matrices. You could give it to a computer and say, I'd like you to multiply these. You can definitely work with this thing. I'd like to make it something which you can not only work with, but think about by telling you how to think about it as a permutation group and how to think about it geometrically. Okay, the way to see what it is as a permutation group is to see how we act on my space. How we act on my spanning vectors Ei. So a computation for the reflection formula is Sj acting on Ei because Ei plus 1 for I equals J mod M, Ei minus 1 for I equals J plus 1 mod M, and Ei otherwise. So EJ and EJ plus 1 switch, EJ plus N and EJ plus N plus 1 switch, Ej plus 2N and EJ plus 2n plus 1 switch, and also in 2n plus 1 switch, and also in the negative fraction, the platinum set. So we can think about this as a group of permutations of Z, Z indexing by spanning set, where Sj is not a transposition, but it's a transposition and all of its shifts by N. And so what this is, is it's the group of all permutations of Z generated by these N permutations. Like to know what that is explicitly, Like to know what that is explicitly, there's an explicit description. It's those bijections from Z to Z. Oh my gosh, typo. Fi plus N is Fi plus N. It's a bijection. Sorry. And the average displacement of Fi over any n consecutive thing is zero. Okay. Okay, so that's how you can think about this as a combinatorial group reading the integers. About to talk about. Okay. Think about it geometrically. Okay, V, I told it to you by taking an infinite-dimensional vector space and quotienting it, which means the dual should be a subspace of infinite dimensional space. So the dual. So, the dual is bi-infinite sequences, zi, which obey certain relations. So, the dual is going to be bi-infinite sequences, starting with z-negative, you know, z-negative 2, z-negative 1, z0, z1, z2, etc., which obey these woody relations on them. So that's an n plus one-dimensional vector space. And the action on this n plus one-dimensional vector space is by permuting disease. It's by computing disease. Let me put up the picture on the next page so I can point to it. So the left-hand side is drawn in V, and the right-hand side is drawn in V2. In the left-hand, so let me show you that these are not isomorphic representations. Isomorphic representations. In the in less n equals 2 is too small. Let me do n equals 3, then I'm sure I might. The analogous picture of n equals 3, which is hard to draw, which I didn't draw it, has in V we have a vector delta, alpha 1 plus alpha 2 plus alpha 3. Plus alpha 2 plus alpha 3, which is fixed by all aspects. And so if you draw the matrices on the V side, and if delta is one of our basis vectors, then let's make it be the first one, then all of those matrices look like this. There's a vector which has three. There's a vector which has slope one on that left-hand picture, which is fixed by everything, and everything is invariant under that. On the right-hand side, in V2 all, what that means is that delta per is fixed. So there's a fixed hyperplane, and all of our matrices look like the transpose matrices like this. This. So if you think about this, this is something I'm going to say a bit later. If you think about these as acting on vectors patterned with an extra one, these are affine linear transformations preserving this one-padding coordinate. Excuse me. Whereas these are not. These are the dual of that. The dual of that. Yeah, Grant? I mean, there's another way to see it in this picture. So in this picture, if you look at the fixed points of S1 and S2, on the left-hand side, the fixed points of S1 and S2 is just the line of slope 1. On the right-hand side, the fixed points of S1 is the vertical line, and the fixed points of S2 is the horizontal line. Oh, that's very good. Yes, I should have said that. Maps that. Okay. So, somebody's telling you there's a switch on the Viper note, that switches volume. Ah, this one. There's another possible laser pointer. Let's see if this one works. Yes. Okay. Yes. Okay. S1 on the V side, negates alpha 1, that's here, and fixes alpha 1 perp, which is this, is slope 1 line. S2 negates alpha 2 and fixes this alpha 2 purple, which is also this line. So they have the same one eigenspace, but different negative 1 eigenspace. Space but different negative one eigen spaces. On this side, S1 fixes this line and negates this line of slope negative 1. And S2 fixes this line and negates the same line of slope negative 1. So here, the 1 eigenspaces are different and the negative 1 eigenspaces. Spaces are different and the negative one-eigen spaces are the same. So I mean, we showed you a different representations. And that's why when I said before, reflections, positive roots, and hyperplanes are in bijection. I specified the hyperplanes are on the V-dual side. Because on the V side, alpha 1 probe and alpha 2 probe are the same. This is a degenerate by when you're 4. Is that helpful? And so we can see why we're distinguished via d-dual here? Yes. They are very much the same. Yeah, but I didn't see the difference between the two systems. Okay. So V is spanned by these EIs, and we permute the EIs like this. So an element of V is a finite videocopy. So, an element of V is a finite radio combination of the EIs. Of course, you could select the subset of them with your basis. The dual space is going to be bi-infinite functions, right? If you have a vector space and a countably infinite basis, the dual is all sequences. So these are not finite combinations, they're infinite sequences, but they're infinite sequences which obey conditions, namely they obey these conditions. Good? Yeah, they are very similar because dual things are very similar. But I do want to distinguish them because of this issue we were just talking about. That is because of this issue we were just talking about, that they are not isomorphic representations. And you said it's only when they're taken together that this is faithful. Is that correct? Nope, I said both of those are faithful. Oh, each one's equivalent to okay. Yep. So, I won't try to find it out now. So, the two representations are dual to each other. So, in matrices, if I have a matrix G for one, it's G inverse transpose for the other. So, faithfulness is the same value for the one. Good? Okay, so I already started talking about this picture, but let's say a little bit more about it. So, again, the simple generators switch i and i plus 1, and all n translates to that. What's conjugate to that, conjugate to that is switching i and j. For i not conjugate to j mod n, I meant to put that in there. And I'll translate to that. And then to the roots of ej minus ei. And then to the roots of EJ minus EI and not J minus M, I didn't say it in second sentence. So when N equals 2, the roots look like this. And notice E4 minus E1, that's the same thing as E3. E3 plus E3 minus E2 plus E2 minus E1, which is alpha 1 plus alpha 2 plus alpha 1, because E4 minus E3 and E2 minus E1 are the same thing, which is why I've written that's 2 alpha 1 plus alpha 2. And so your roots look like alpha 2, 2 plus multiples of alpha 1 plus alpha 2, and say with alpha 1. And say with alpha 1, they start accumulating along this line of slope 1, and the orthogonal hyperplanes start accumulating along this hyperplane of slope negative 1. The fundamental domain is where the sequences are increasing. And this time the tits code is only a half plane. So here's the fundamental region D. Here's the images under S1 and S2. I didn't label the rest because it would be so small, but here's S1, S2. Because it would be so small, but here's S1S2D, here's S1S2S1D. But all of the W translates of D all stay on this side of the big pile up. The stuff over here that's not blue is not in the tits code and is on the negative side of each of the different big egg points. Excuse me. Okay, I will skip that slide. And let me very briefly do the Free Cockscher group. So we had the question earlier: what do I do when m is infinity? When m is infinity, I will take Aij to be negative 2. So I'm going to take AR3 with this bioinear form. This bioinear form is non-degenerate, so I do get to identify V dual now, but it has signature plus plus minus, so we're doing in space-time geometry. In space-time geometry, alpha i is the standard basis in these coordinates. So when you have a plus plus minus signature form, then you get a two-sheeted hyperboloid where the form takes the negative value. And you can think of one of those sheets as the hyperbolic disk. And all of our reflections are isometries of the hyperbolic disk. They are reflections over the edges of the isometries. Reflections over the edges of the ideal triangles in this hyperbolic triangle tile. So you can think about this reflection group as a group of hyperbolic symmetries. But drawing this hyperbolic picture only sees some of the picture. So here is the double cone where that quadratic form vanishes. The tits cone is one of the two components, so a single cone. Of the two components, so a single cone, and that hyperbole is sitting inside there. The negative tips cone is sitting down here, and all of this space in between, all of the space-like vectors, we don't know what they are. We don't know what's going on there. We have very poor controller understanding of the geometry and anything like that. Because I want to make sure that I get some latter steering before I end the talk. Talk. Okay, I'm supposed to be talking about lattice theory, and all I'm supposed to talk about coxular groups. Coxider groups are lattices. Or more precisely, finite coxswain groups are lattices, and infinite coxswain groups are very related to lattices. Okay, so starting with some basic group theory language, for an element in the cocksure group, its length is the length of the shortest word for it. A word of that shortest length is called reduced. And weak or And weak order is the partial order on the Hochschler group where u is less than v if lv is lu plus lu inverse v, or in other words, if there's a reduced word for v, whose prefix is a reduced word for u, and whose suffix is a reduced word for u inverse v. So on the left here, I've drawn a Greek order on S3. The only maybe not obvious thing is that S3. only maybe not obvious thing is that s1s2s1 is above s2s1 that's because s1s2s1 also equals s2s1s2 so it has s2s1 as a prefix and on the right hand side you can either think about this as the affine symmetric group of rank 2 which it is or you can also think about it as the three hatchery group of rank 2 which is also i see so here's what weak order on those guys looks like what do When W is finite, weak order is a lattice. And when W is infinite, it is a complete semi-lattice. That means every non-empty subset has a meet, and every bounded upon subset has a joy. I think of semi-vadas as, yes, meets and joins are unique, but maybe they don't exist. And for experts who know lots of, lottis vocabulary. Bots of waters vocabulary. This is a semi-distributive wattis in which every element is a canonical joint decomposition. The majority of the use of elements have been extensively studied by Nathan Green. And generalizations of these ideas, the infinite type are ongoing. Like there's lots I could say, but there isn't one sentence that I'm saying. Okay, let me tie the lattice theory back to the geometry. So So, on the left-hand side, I've drawn a finite case where everything is really nice. So, the regions of the hyperplane arrangement complement correspond to W, and the adjacencies between those regions are of a Haas diagram of Water. And going towards the positive side of the hyperpoint is going down the W quarter. So, length is the number of hyperpoints. So, length is the number of hyperplanes that you cross going from D up to W D. Just the same in the infinite case, except I need to remember to say the word tits cone, because only the regions of the complement and in the titzcone are the form W D. But then, with that proviso, the same is true there. The Haas diagram is of weak order, is the adjacency, and going towards the positive side of the hyperpoint, it's going down. Hyperplane is going down. Okay, so I said, you know, oh, the length is how many hyperplanes separate you from the identity. It seems like it would be really natural to think, what can we say about the set of hyperplanes that separate you from the identity? So the inversions of a group R with W are those positive roots so that W D lies on the negative side of V perpendicular. DUIs on the negative side of B perpendicular. And a theorem is that dominance in weak order is containment of inversion sets. So here I've taken S3 and I've labeled each of these cones by their inversions. This cone is on the negative side of alpha 1 perp over here and alpha 1 plus alpha 2 perp over here. You can see that the weak order is the containment between these. Between these. That's a geometric definition, and it's the one that I like. But if you really like our group theory and words and group theory, here's a purely group theoretic definition. The corresponding reflections, which are bijection with the positive roots. You take any reduced word for w, and you write down each of its prefixes and turn them into a power draw. So S1, S1, S2, S1, S1, S2, S3, S2, S1, etc. And those L. And those L reflections aren't just against each other, so that's the L. Okay, so we have a way of thinking about weak order as containment between these inversion sets. And that will lead to a question of which sets of roots are inversion sets. Okay, there's a geometric description of this, and there's also a pure group theory description of this if you want that. Group theory description about that. I'm going to do the geometric one. A let x be some subset of the positive roots. We'll say x is closed if whenever we have two roots in the set and some other root that's a positive radio combination of them, that root gamma is also in the set. We say that a set of roots is co-closed if its complement is closed. We say it's bi-closed if it's closed and co-closed. Closed if it's closed and co-closed. And I'm pretty sure Matthew Dyer is the right person to credit for this. X is a set of inversions if and only if it is a finite biclosed set. You should notice this condition of biclosure is very motivated by convex geometry. If I choose the vector V dual and say, okay, which roots to that pair positively with, or which roots to that pair negatively with, then I get a biclosed set. Then I get a biclosed set out of it, and those biclosed sets are called separable. And whoops, I definitely should have choked the negative sign. Finite biclosed sets come from taking points if it did so. Okay. An ultimate slide. And basically ultimate. This suggests it would make sense to think about all biclosed sets. This is a program Matthew Dyer originated, one of the papers about, and then Greg Barakri and I have. About, and then Greg Barclay and I have really been thinking about this for the last five minutes or so. And we'd like to think about it with you. Is if we take the poset of all biclos sets, what can we say about it? Well, very little that we can prove, but a lot of things that we believe, we believe it's a complete lattice, we believe that it's completely semi-distributive, and I should say complete rejoiner reducible elements, but people check details. That the complete rejoiner reusable elements are bijection. Completely joined images well, which are byjection of the shards. In rank three, we think we know what all biclosed sets look like, and this is literally false. As far as morally right, we think they all set work. Ask us if you want the correction to why that's not exactly the right statement. And if you have a crystallographic root system, then there is an associated quiver and an associated pre-projective algebra and an associated modus of torsion classes. Classes. The finite type the latest quotient classes is weak order. In general, it's much, much bigger, but it looks like this is a quotient of that. An ultimate slide. I would love to talk about all these things with all of you. Thank you. Do we have any questions? Yeah. Any conjecture about cover relations in the that's a good question, Drell. Conjecture about cover relations. Let me just draw you the simplest picture just to show you the sort of things going on here. So I drew you weak order on either the free rank two coxider group or the affine group rank two. What the extended weak order looks like. The extended recorder looks like it has that at the bottom but it has an element here which is the joint of this chain, be better, but it has an element here which is the joining. It has an element here, which is the joint of this chain, and then it has a second upside-down copy on top of that. So one thing to understand is that these guys are not connected by cover relations. I can't get from here up to there through covers. That said, you totally can ask me about covers. It's a reasonable thing to ask. I. I well, one thing I guess I want to conjecture is that things work the way the shard theory says they should work. That is to say, if you make an equivalence relation between covers using like two subgroups, those equivalence classes should correspond to shards. But I don't know if you were looking for some other conjecture and I don't know what the conjecture you might be looking for in the idea. Yes, the question would be like can we say anything about like are the colours of physical?