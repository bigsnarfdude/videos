I'm a PhD student under the supervision of Zach at the University of Alberta and today I'm going to talk about the joint work that we did during my master's at the Club. So the title of the talk is to a new method on approximation algorithms for a problem called meaning software. But before we dive into that, let's just have this quick review of clustering. So first of all, what is clustering? We have some inputs. First set of inputs is a First set of inputs is a set of items, a set of data points that want to cluster. Now I'm going to talk about the classification. And then we want to group them such that items in one cluster are similar to each other, are close to each other. And so what does this closeness mean? Researchers have defined objective functions or closeness metrics to describe this similarity. I'm just going to go over some. I'm just going to go over some examples of this objective function. So, first of all, I'm going to talk about k-center problems. So, what's the problem? We are given some data points, we are given a value integer k, and we wish to pick k clusters, we wish to form k clusters of the data points, for example like this, such that we minimize the maximum distance of a point, of data points, these red dots to be five data points, to the center of the cluster that we form. Center of the cluster that we form. That would be the case of the problem. So the objective function is to maximize, the objective function is to minimize the maximum results of a pointer itself. If you have another objective function that is well studied to work, one of them would be the k-means problem. So again, set of data points and we wish to pick k-centers such that the sum of this, this word, sum of the distances of each point to each center is minimized. You may have heard of this problem, especially in the cases of the AI or data science and as such. Another problem, so this is another problem that's similar to the k-means problem, is the k-media problem. Again, k, data points. We want to pick clusters. Each cluster has a center, and we wish to minimize some of the distances to the center of the cluster. And this one has more purposes in the OR. The OR version is clustering. Data points, integer k, we wish to pick group data points and then optimize some objective function. So I'm just going to introduce another objective function. But before I do that, I want to instantly. So let's just say we have in houses in an area. And we wish to set up antennas throughout this area that provide service to those houses. Provide service to those houses. So, what I'm going to do is that I pick a location, I put an enter, and I assign a radius to the bandwagon. I wish to cover every house. And then I want to minimize the coverage cost or the cover cost of this assignment. Because we know the coverage cost is proportional or exactly just the radius of the antenna. What I'm going to do, I'm going to pick locations for antennas, I'm going to assign the radius, and I want Antennas, I'm going to assign the radius, and I want to minimize the sum of all of these radius that are assigned to these antennas. That would introduce the problem that I'm going to talk about today, which is the minimum sound threat. What do I have again? It's a clustering problem. I have n data points, 12, but it's like a scripture, so they're like third data points. I have an input integer k. I wish to pick k locations and assign a radius to each of these k locations, such that I'm, first of all, I want to cover every point. I want to cover every point and I pick k-centers and also minimize the sum of rate of these assignments. As you can see, one of the use cases of this objective function is the coverage problem. And researchers have noticed that using the other objective functions, such as case center, had some disparity. That is, one day. That is, one data point had to be, it would have been better, it would have made more sense, any center that you would have thought about it, to another center. And that's why we introduced this problem. So why is this a problem at all? So, first of all, this sentence, there is a 3.5 approximation algorithm. What does it mean? This problem, the MSR problem, which I'm going to. The MSR problem, which I'm going to run now on, is an NP-hard problem. What does that mean? That means there are no efficient algorithms and researchers do not expect to find one for this problem. By efficient algorithms, I mean that they run in polynomial time. So we don't have any, we don't expect to find any. So we have to work with something. We have to find something to solve this problem, even not exactly, but approximately. So when I say there is an alpha approximately, There is an alpha approximation of this problem. That means I have an efficient algorithm, I have an algorithm that runs in polynomial time, though the solution of the algorithm is at most 3.5 times the optimal solution of that instance. So if I had an instance of the MSR problem, I use this algorithm. It might not be the exact optimal solution that the algorithm gives me back, but it's at most some value of the optimal solution. Some value of the optimal source. So, the previous work on this problem showed that OGIPRI is the 3.5 over approximation for the gas algorithm. Well, it's notable to say that there is an exact algorithm, near exact algorithm, for the problem that runs in this time. And that's a straight solve. And also, there's an exact algorithm. By exact, I mean we solve it, there's an efficient algorithm. We solve it, there's a po there's an efficient algorithm, we solve it and we get the exact solution that we want for some cases. So for this case, if the metric that the data points are in is a Euclidean metric, then quit. Oh no, we can't do it. So what did we do? So the work that we did is a refinement and simplification of the previous work of Sherry Karat Piagram. And before I go to that, let me just Go to that. Let me just talk about a modification of this problem. We just got the minimum sum of diameters. Again, a clustering problem. And we wish to find k clusters throughout these data points. And we wish to minimize some of the arrows of each cluster. So what's the diameter of the cluster? It's just the maximum distance of between any two points in the cluster. So if you take a moment to think about it, any alpha approximation to the MSR problem. To the MSR problem would be a two-alpha approximation to the MST problem because a diameter of a cluster is at most two times its radius if we were to pick a virtual center for the cluster and assign a radius to it. So naturally, the previous work would imply a 7.008 approximation for the MSD bar. However, using a machinery, using a process using what we did here, we can get better than Here we can get better than two times the MSR algorithm for the MSR. I want to mention another problem, M-modification, modification, which is similar to the MSR, it's just minimum sum squared of radar. That is, the objective function is to the sum of squared radius. So that's why I assign the center, I assign a radius to that center, but I take some over this field. And it's V, it's the first time I explain this. First time. I experience. How can the diameter be different from two times the radius? Radius times two. But you said the diameter can be smaller than that. Okay. So for example, in this cluster that they form here, the diameter of the cluster would be the distance, the maximum distance between any two pairs of the data points in the cluster. So for example, this would be here. Though if I were to pick If I were to pick a center for this cluster, the radius of this cluster would be at, but this diameter is at most two times this radius. So if you could choose the centers, it would be the same. In abstract metric space, it doesn't work like that. When you draw pictures on the plane, you can view them like that, but if we're just talking about subsets of what it's the diameters, it's the distance, maximum distance between any pair, which is Maximum distance between any pair, which is different than two times the optical center for the reference. So the same. So, um. Even no, I would actually say not the diameter of the ball, but not the distance between breaking points. So if this is our question, the answer is this, but the center would be this, which is yes. Okay. Um so yeah. So yeah, the MSSR problem was explicitly solved for the first time. Solve for the first time. So, what should we do? What's the approach here? Let me talk about, talk this truth. So, first of all, let us pick a value length. And I'm going to solve a little different problem. Instead of just counting how many clusters I have opened up by now, I'm just going to take a penalty for each cluster that I open up by the value of number. What does that mean? If I have a set of data points, for every cluster that I open up and cover some Cluster that I open up and cover some data points, I will penalize my solution by lambda. So it's like it's sort of a facility location problem. I penalize opening a center and then cost naturally calculate the cost of the solution, which is the sum of radar. So in this problem, the addition we have sum of radar, which is the sum of the radius of the spot solution. Which is the sum of the radius of this cluster of IP, plus this lambdas, which is the lambdas times the number of clusters of IP power. If I find a lambda that I just conveniently open k clusters with that value lambda because that was good lambda value, then the optimal solution. Perfect. I don't need to do anything else. Note that the constraint that I have to open k clusters does not exist. Open K clusters, it does not exist. I just penalize opening clusters by the value lambda. We couldn't do that. If I didn't find just a good lambda to do that, I use approximation. Can I find a solution that covers K Center approximately? I will talk about what does that mean in this sense. But still, it's K center. And if yes, again, perfect. It's approximation of work. But I did find. But I did find case centers, which was the thing that I want. So, sometimes I may not. Sometimes I have this value lambda, one single value lambda, and two solutions. One of the solutions has too few clusters open, less than k, and one of the solutions has a lot of clusters open, more than k open. So that was what it would look like. So everything is covered in these three cases. So, everything is covered in these two cases. And if k, for example, is 2, this one has one bulb, which is costly because the radius is increased. And this one has four bulbs. So remember that I'm penalizing the solution four times by the battle length. So I want something in between. I don't want this, I don't want that, but I want to find a feasible solution, a solution with K clusters and a good And a good cost value using these two solutions. So I will combine them. What does that mean? I don't get to that. But what we will show that is that when we combine this solution, we build down the cost of this combination just enough so that we have a good approximation algorithm. And then how do I do that? I will show that there wouldn't be too many balls. Be too many balls. So let's just think about it intuitively. I have a solution with a lot of balls, a lot of clusters, and I have a solution with too few. If I keep looking at them, I will think that what if I will merge some from this solution with a lot of bots, I will merge some of them into one bot, but carefully. And I will talk about that carefully, just in a moment. But before we get to that combination, let's just talk about how do we get those two solutions. How do we input the lambda value to the problem and get the solution of the balls that we want? First off, let's just model the problem as an integer program. So in this integer program, the objective function, so the variables are corresponding to a center and To the center and assign radius. So if x of i and r is equal to 1, I opened its center at location i with radius r. I want every location in this data set x to be covered, so that's why it's the constraint name. And I want to have at most k clusters. And so this is an integer program. Unfortunately, integer programs are also an hard efficient algorithm to solve them. Although, I can't rely. Although I can relax this integer program to a linear program. That is, I will relax the constraint of the variable being exactly 0 or 1. The problem would be if variable is not 0, not 1, just something between them. How do I make sense of that? Do I open the center? Do I not open the center? That's a problem. Remember the London idea that I described that would penalize opening too much clusters. Okay, so how do I do that? Okay, so how do I do that? Let's get rid of this constraint of atomos k clusters, and then I add this penalty to the objective function, the objective function that wanted to minimize some of relevant. And this new LP, this new linear program that has lambda value as its input is what we are going to work with. And this whole process is called Lagrangian preserving approximation process. Preservable approximation problem. Lagrange and multiply a preserved network sensitive for us. The good thing about this one is, as opposed to our previous course, is that this one is much simpler. I'm going to show how this is simpler. So remember that. We have an LP, we solve it. We solve this little... We can't do that in problem of time. And each of these XIR values have something, have something assigned to them. 0 of 1 between 0 and 1. 0 of 1 between 0 and 1. So we solve that, we get the solution, call the solution x prime, and then we round the solution. I will talk about that now. And the important thing is this algorithm, I give Blando value to it, and I give the solution of this LP to it, and I get a set of bots that will cover everything, that is a feasible solution, except for the K clusters part. The k clusters part. And as I mentioned before, if I change the value of lambda as the input to this problem, the number of the output clusters will change. So let's talk about this algorithm. Let's think about the support of the LP. What does the support of DLP means? It means all the values of DLP that are non-zero. And let's sort them by the non-increasing order of the radius corresponding to that. And I have an empty set here. I look at this ordering. I will put one of the balls with this empty set if it does not intersect with any of the balls that is already in this. So naturally, I will put the first ball in it, and then I will go through the list. If this one intersects with this one, I will play it. So that's how the solution will work. I will add this ball to the set, and for example, if I check this ball, I add this ball to the set because it does not intersect the... The set because it does not intersect all the balls that are already in the set. And when I get to this ball, I cannot add it to the set because it already intersects with one of the other balls. Is the algorithm clear? So this picture is what the solution could look like and what problems that are not picked. Let's zoom in on that and see what properties we have that we can use later on. Later on. Okay, so let the black ball be the one that we picked and its underset. There are a couple of intersecting balls that we didn't pick because it was under set. There's something here. Since we ordered them by the radius, these blue walls will not have a radius greater than the black ball. So every R2, this one R2, this one R2, are at most R1. So if I were to open the center at To open the center at it here and assign a radius three times R1, I will cover everything. It's like I merged this ball into one, for a sense, and I have found a good radius. It's not too large. It's just three times the first radius that I picked here. And use LP duality and copy slightness to show the optimal. So the optimal solution is also not that bad. That's what is important. Okay, use grounding and you want to use binary stitch. What's that? Why would you want to use binary stitch? Remember the value lambda. Think about it. If the value lambda is zero, it's like we do not plan on opening clusters at all. I can open the center at each of the collient locations and assign a radius zero to the cluster. And assign a radius zero to that, I have the best solution ever. It's zero. Though it has too many locations, it's not at most k. And if I as I increase it, the number of clusters will also decrease. So it's like if it's too small, I have some number of clusters. If it's t big enough, I have another number of clusters. So I use binary switch to snap to a proper location that I get that I can get my two solutions that I already mentioned. Two solutions that I already mentioned. So, yeah, that's what would happen if I picked the planner to be zero, I still open the center at each of the locations, and that's here. So that's the next step. I already described rounding, and this is the next step to the wire. So, again, as I described, this is the language of landmass, if I start here, it turns If I start here, it might be a lot. If I end a few, it might be too much. It might be too few. And then I will close this gap until I snap to a length of A that is just good enough for me to have my two solutions. One more than K and one less than. So let's just continue with that. Remember that I said I have two solutions, which I already have here, so let one of them be one, let one of them be. Let one of them e1, let one of them be two, e2. One of them has two match clusters. So, if the k value is three, this one has four clusters. I don't want that. This one has two clusters, it might be too costly. I want to get close to k as much as possible. And that's where we get to the emerging section. Okay, I will look at my two solutions. One of them has too few and one of them has too too much, of too many number of balls. Let's just Let's just the process that I'm going to describe here is going to be called the grouping procedure. So I will pick one of the balls from one of the smaller solution and all the balls that are intersecting the solution. I want to merge balls from the larger solution, the one with more balls, so these dashed balls around the solution that had too many balls, I want to merge them into one because they're too many. I want to decrease the number of clusters. I want to decrease the number of clusters. I want to pick a center, I want to assign a radius to that center such that it covers everything. That's what we're going to do. So our improvement in the algorithm came from the flexibility that we had in PHIM Center. So for example, this is one of the candidates that I can use in this tool sheet to cover everything that is in the tripled radius of this dashed board. three-point radius of this dashed ball, it would be the center eigen. I don't care about anything else that is here, I just want to cover everything that is into three times the radius of this dashboard. That's one of the candidates. One of the other candidates would be this one. And another candidate attributed to be justly essential between these two. By thinking about that, By thinking about that, by bounding the cost of picking, by cost, what I mean by cost, I mean the radius that I have to assign to each SD sender to cover everything. By bounding that, I will find a good enough solution. That's what this slide means. Nobody cares about that. It's good enough. So my candidates are good enough. Now, where are we now? So, if I were to merge every ball from solution two, we should have more balls. More balls to corresponding with respect to a solution, I will again have too few clusters. It's like nothing at it. So, what do I do? For every group that I interact with, I have a choice. Do I merge this group or do I not merge this group? I use the original solution. And we can use a greedy-like approach to decide for each of these groups whether or not I merge them. And this slide also shows that at the final step, our solution would be 3.389 times the optimal solution, which is the approximation factor I mentioned. All I'm trying to say as this first talk is that the algorithm that we described here is a 3.289 approximation algorithm for the NSR, which is better than 3.54. So I have it here. I have it here. We gave a solution using this machinery that was the 3.289 approximation algorithm for MSR. Major components, LPs, Lagrangian relaxation, which was penalizing the opening of the cluster instead of having a constraint for that. Then round the solution, do binary search to find the proper lambda value. And I think that's just it. So if something can tell me. The MST solution, the MST problem. The MST solution, the MST problem, the minimum sum of diameters problem, can be solved using this machinery to get 6.5 approximation. Though not that this is less than 2 times 3.38, because we use another machinery and this is not naturally coming in from 2 times the radius. And for the MSSR, we use the same machinery to get a little bit scroll 7.8 approximation. Thank you for listening to me. I have any idea. Is it to me? How many questions? Questions for Myanmar? Um, sorry? The lower model for the opportunity? MSD case no we cannot do better at than a two at once. For the MSO no MSD case no we don't know for fantastic anybody else lunch is in the business. So in the business diagram, where we're