So thanks everyone for coming at this late hour. Yeah, so I'll tell you about robust recovery for a statistic clock model. This is joint work with Saddam and Hossad. Okay, so you've seen this picture like a million times by now. But okay, we care about statistical inference. There's this like phase diagram that we care about. Whereas we vary the SNR, things go from being information theoretically impossible to maybe computationally hard and then computationally easy. And I guess the motivating question. Easy. And I guess the motivating question that I want to predicate this entire talk in is basically understanding: like, how brittle are all of these thresholds? How much do they change if you sort of slightly perturb the modeling assumptions? Okay, so hey, we're going to focus on SBM. Also, pretty familiar for this community, but let me just quickly go over it. So, we're just going to talk, for simplicity, we're just going to talk about two-community SBM in this talk, but our results will generalize to arbitrary constant number of communities. But we have, again, the two community. But we have the two-community SPM, we sample some Boolean vector, and then we observe this graph condition on the random graph condition on the vector. So in this case, we're thinking of the case where within communities, you're more likely to see an edge, and within communities, you're less likely to see an edge. Okay, and the goal in general is, okay, condition on seeing this realization of the graph, I want to recover what the communities, the true communities are. And we're going to be thinking in general about this. And we're going to be thinking in general about this low SNAR regime where we can't quite hope to perfectly recover the actual community assignment. But maybe we settle for this slightly more relaxed goal where we output an x hat, which is non-trivially correlated with the true community. So we just want to find some x hat such that the inner product is omega n. Okay, and one thing I want to point out here is that it's not really clear if the best way to do this type of weak recovery is to do optimization because it turns Optimization because it turns out that the min-bisection, which you might think is intuitively the right thing to output for the communities, might not be correlated with the true communities. And yeah, so there's a lot of interesting work on this. So first, this prediction by physicists like Florent and Chris about when it's actually possible to do this recovery problem based on some notion of SNR. On some notion of SNR. So, this is like the chaos threshold. Above this threshold, it's easy and then impossible below. And this was rigorously confirmed by Moso Ni Mun Sai and Maso Yie. And one interesting aspect of this problem is that, okay, if you, not just two communities, but if you look at a larger number of communities, then there's potentially this information computation gap that seems to arise. And that computational threshold is known as a chaos threshold, the plus second threshold. Threshold. Okay, and like I said, we're just going to focus on the two community case for this talk, Focus and Flisten. And in this, we're generally going to be thinking about this sparse regime, so we have constant average degree. And in this case, it's actually easy to see that you can't really hope to get perfect recovery because there will be isolated nodes. And we're going to be really close to this KS threshold. So in this low SNR regime, you can only really get like slightly better than random guessing accuracy for each node. Accuracy for each number. Okay, so let me first review some of, or just introduce and remind people about some of the algorithms that people have come up with to recover down to this cancer. So actually, yeah, a lot of these algorithms are based on these Brown walk statistics. Okay, so there's a bunch of these works. And at a high level, what are they doing? Okay, basically, I look at every pair of vertices in my graph, and I count some notion of And I count some notion of walks, like maybe non-backtracking walks or something like that. Count how many of them there are of length, roughly log n. And one aspect of this is that, okay, this particular statistic is pretty sensitive to perturbations in the graph. If I add some small cliques or cycles, this is going to really affect the actual values of these statistics or the distributions of these statistics. And so, a pretty natural question then is: you know, as I foreshadowed in the beginning of the talk, like, how specialized is this chaos threshold actually to the exact model of like SVM that we're looking at? Like, if we add some corruptions, is that going to shift the chaos threshold? So, that makes sense? Just here real quick. Yeah. So, the issue is that if I add a small clique or a bunch of small cycles, then this greatly increases the number. This greatly increases the number of long walks because things can just go around a lot. It just confuses those algorithms. Yeah, exactly. Okay, so we're going to talk about corruption models for the SBM. And so we're going to try to relax the model a bit by allowing an adversary to actually corrupt the graph that we see. And in this talk, we're talking about the edge corruption model. So, how should I think about this? So, how should I think about this? Okay, I give the adversary some budget delta, and then in the sparse regime, this kind of makes sense. Like, the adversary can only arbitrarily add or remove delta and edges. So they can actually corrupt up to a constant fraction of the edges in the graph. So, just pictorially, like I have my original graph, which is similar from the real SVM. Like, some adversary comes in and adds and deletes some edges. Maybe I get some new graph, g tilde, and this is the actual only graph that my algorithm is able to see. Graph that my algorithm is able to see. So I don't know what the original graph is. Okay, and I think it's important to also mention what other notions of corruption models I haven't been studying in the literature. So there's this notion of node corruption where I'm able to arbitrarily modify the neighborhood of delta n vertices. I think this is mentioned in David's talk a couple days ago. It turns out in the sparse regime that we care about, one of David's papers actually shows that you can. One of Debut's papers actually shows that you can reduce this node corruption model to the edge corruption model. So, sort of our results will apply, but maybe with a slightly constant factor, smaller budget for the adversary and the edge corruptions. And another interesting corruption model, which has also been studied by people at this talk, is this notion of a monitoring adversary for SPM. So, in this two-community setting, this adversary can corrupt the graph, but sort of in a way that should only help your algorithms. Should only help your algorithms. It'll only be able to add edges between your communities, like within the community, and delete edges in between. Okay, so intuitively, maybe you would expect this to only make the problem easier. But it turns out that this actually does change the chaos threshold. So this was established by Alex and Anker in their paper in 2016. And one interesting thing about, like, if you shift the chaos threshold, is, well, actually, it may Threshold is, well, actually, in that regime, maybe the planted bisection is really the right thing to do. Like doing optimization is sort of the right thing to do, because it'll be smaller than the min bisection of a random erosion graph. So maybe we're actually getting some signal there. But again, since we're going to be concerned about doing everything close to this conjectured KS threshold, these random graphs will actually have smaller bisections in the planet one. So it's not really clear if that's the right approach for the edge corruption model that we care about. But I just wanted to point out, this is. But I just wanted to point out this is like sort of a semi-random model, but it's definitely very interesting, and there are lots of cool techniques that come out. Okay, so what's our main algorithmic result? So let me just first formally define what the task that we're trying to solve is. So we have this sample from SVM, and the adversary has some budget delta, and you have some target correlation rho. Your input is just this corrupted version, g utility. Input is just this corrupted version, g tilde, that can be corrupted up to delta edges. And the goal is just going to be to output this x hat, this binary vector, such that the expected correlation is at least row. Okay, so the main result in this line of work is actually that also due to David and his students. So they show an efficient STP-based algorithm which blocks heroes succeed at robust recovery for the two-community block model. For the two-community block model. And so, our result, it's not an STP algorithm, it's actually a spectral-based algorithm. And this will succeed at actually in a slightly more general setting where you consider allow any constant number of communities. And let me tell you a little bit about how we managed to do that. But first, let me just stop here if there are any questions. Probably your row depends on delta. Yeah, yeah. So this isn't okay. You're just like obfuscating the parameters for the correct. Yeah, yeah, yeah. Just some correlation. You want to do better than manual guessing, some constant over. Okay, so let me just give like a high-level overview of the rest of the talk. So first, I'm going to talk about a new spectro algorithm that we came up with that actually works for non-robust recovery. And to motivate it, I'm going to first go through some of the prior approaches that achieved recovery. That achieved recovery depth of the chaos special. Then I'm going to talk about robustifying the algorithm. And along the way, it'll turn out to be nice to talk about this other self-contained problem of robustly recovering a subspace with finite sigma. And maybe this is of independent interest. And finally, I'll mention some open problems at the end. Okay, so there are tons and tons of algorithms for read recovery, actually. For weapon covery, actually. So, this is definitely not an exhaustive list, but let's just try to go through some broad classes of approaches. So, first, there are these spectral algorithms that use the spectrum of certain graph-defined matrices. There are also these SDPs, which also naturally come with some type of robustness guarantee, so it makes sense why it would come up in this type of setting. But actually, our starting point will actually be the spectral. Will actually be the spectral result by Bordenov, the Larger and Maussulier, and their really nice results on the non-bacterial matrix, which sort of was one of the first major results or successes in this area. Okay, so let me just remind people what the result says. So it's a spectral algorithm that actually achieves recovery down to this chaos threshold based on the so-called non-batch of matrix. Okay, so let's just remind everyone what this is. It's this asymmetric. This is. It's this asymmetric real matrix, or 0-1 matrix, which is indexed by pairs of directed edges. Okay, so I have edge u1v1 and u2v2, and it's one if and only if these two edges come in, like sort of one goes into the other, but it's not a self-suite. Okay, and what BLM were able to prove is actually this really nice picture on the spectrum of this, you know, of this AC. Of this asymmetric matrix. So they actually showed that, so this is in the complex plane. They showed that most of the eigenvalues, which some of them are going to be complex, are contained in this wall of radius root d. And then there's also these two other outliers. So we're just talking about two community SPN. So there's first this trivial parent eigenvalue that'll always be there. That's not going to tell you anything. That's not going to tell you anything about the presence of a community or not. But it turns out that there's also this other non-trivial outlier at lambda d. Okay, and it turns out that you can actually round the eigenvector corresponding to this eigenvalue to a good community assignment. So sort of the spectrum of the non-vector k-matrix is actually telling you some interesting information about the true communities and your network. I'd be happy to send you a nice picture of the spectrum and the complex. Spectrum in the complex plane, if you want, for your talk. So, you were saying this is not nice. No, no, you should see the remote. I'm sure that Chris is special. Okay, so the non-back tracking matrix is really nice, but there are some slight annoyances. Is really nice, but there are some slight annoyances to like sort of analyzing it directly. So it's a little bit unreality to work with. And one thing is just right off the bat, it's like this asymmetric matrix. So that already makes it a little annoying to work with in terms of spectral properties. It's not diagonalizable. It has these complex eigenvalues. And these eigenvectors aren't necessarily nice either. They might not be orthogonal. Another thing is that, for example, for asymmetric matrices, this nice variational characterization of eigenvalues might not hold. Eigenvalues might not hold. Okay, so like looking at the quadratic form for the non-bactracking matrix might not really tell you much about the actual spectrum of B. But one nice observation by Florat and Tsadorova was actually that there's this related real symmetric matrix, this Betha-Hesh matrix, which seems to also succeed empirically for like sort of detecting communities in these SPLs. Okay, so let me show. Okay, so let me shift now to talking about the Bethahessian. So the Bethe-Hessian is this real symmetric matrix, which is actually turns out to be formally related to the non-backtracking matrix. And it's defined as follows. So I take my graph, I look at the degree matrix minus identity times T squared. So T here is a parameter between 0 and 1. So it's just this graph matrix, B minus identity times T squared minus AT plus high. And okay, maybe this is. And, okay, maybe this is a little bit mysterious. I haven't really motivated why it comes up. But one way you can think about this Bethahessian is some regularized form of the normal graph Laplacian. So you can check that if you set t equals 1, this just reduces to the standard graph Laplacian, and then if t is 0, this is just the identity. Okay, and as you change t, you sort of interpolate between these two extremes. Okay, so I have this figure that sort of. So, I have this figure that sort of shows what was known previously about the non-backtracking matrix and the related Bethahessian, and sort of what we were able to prove in our work. Okay, so the non-backtracking matrix, so BLM showed that there is one non-trivial outlier, which shows where the, like, tells you about the community. And the Bethahessian was empirically observed to also sort of work for community recovery. Turns out that this Ihar-Bas formula, which I'm not going to spell out, what that Formula, which I'm not going to spell out what that is, but there's a formal way of relating the spectra of these two matrices. And it demonstrates that you can actually use an argument due to Fan Montanari that shows that the Becca Hessian will actually have at most two of these negative outlier eigenvectors. And so what this work shows is that actually it proves the existence of this non-trivial outlier that says something about the communities. So this was actually not previously known rigorously, well, to the best of my knowledge. Well, to the best of my knowledge, but so this is new. Any questions about this? Maybe I should ask Florantis, but I've never understood. So I know that there's this one parameter family of matrices, and it has several names. But like the beta-Hessian, in principle, doesn't have a free parameter. It's so what is this T actually in terms of the Hessian of the beta free energy? Of the beta free energy to the total energy. I think T is like beta, yeah. Then you will put in it as the critical value to get the number. The wood of C something. But if you know the parameters by which the provisional block model was created, I mean, by which the graph was actually generated, then you know the value of t that you should use. Yeah, but I mean, if I remember well, like the square root of C or what's a very good idea. Yeah, yeah, we'll talk about it. There are a lot of way to think about the selection as a random walk. As a random walk. Yeah, I'm not sure. Maybe. You mean like on the underlying graph? Yes, yeah, the same somehow. I mean, you know, it seems like it. somehow I mean you know it seems like you may be a bit lazy and stay on the single extension probability and maybe I guess supposed to be like a luckless shadow but it when you first come it's probably luckless yeah okay so uh I guess completely we construct some symmetric matrix M. Symmetric matrix M, I'm going to slightly like you, you can just pretend like M is exactly this Bethahessian, but it's not quite that. It's like something very close to the Bethahessian, but to pretend like M is a Bethahessian. So what we show is that this matrix M that we constructed has M of two negative eigenvalues, as promised, and also one of the negative eigenvectors for M is correlated with the hidden communities. So it satisfies this type of weak recovery guarantee. And then like a pretty simple And then, like, a pretty simple rounding argument or rounding algorithm will actually be able to also achieve weak recovery in the sense that we originally requested. Can you tell us how you're aligned to this? Yeah, so we're not quite able to show. So, property one is true for the Bethahessian, but for property two, we're not quite able to show that for the Bethahessian, but you take the Bethahessian and you sandwich it with some constant non- It with some constant non-backtracking power of your graph. So this is kind of a technical thing, but yeah, we don't actually know how to prove explicitly that the Betha-Hessian zigenvectors can be rounded. But like experimentally, it seems like it's the case. Yeah, I think that's what Florant's work shows. Yeah, yeah, exactly. Yeah. Okay, so let me. Okay, so let me just show this picture again. Okay, I'll have a nicer picture in the future from next to Chris. So we have this non-trivial outlier at lambda d. And so it turns out if you pick this set r to be like very close to lambda d, like it depends on lambda d, but just some epsilon, you're really close to it, then it turns out that the spectrum of h of 1 over r will actually satisfy this nice property that, okay, it has at most two negative eigenvectors, and one of these is. Negative eigenvectors, then one of these is going to have information about the kinetics. Okay, so that's sort of all I want to say about the spectral side of things. Now I'm going to talk about robustification. So let's first remind ourselves what the corruption model is. So we have these edge corruptions. We can arbitrarily add or remove delta and edges. So this picture that I just flashed on the slides a couple seconds ago was a spectrum, this nice spectrum of. Go was this nice spectrum of the Bethahessian. Okay, but now that you have this adversary, it can sort of ruin this nice picture. Okay, so it turns out that the spectrum of this corrupted Bethahessian could have up to omega n negative eigenvalues. So this really nice picture that we had before is now going to be sort of muddled up by all these other negative eigenvalues. This is only for 1 over R or do I get every point in T? That's a good question. I think you can make this true for any T. You can always add. Like, you can always add negative eigenvalues. Yeah. But every t is equitable. Well, okay, if t is zero, then you literally have the identity, so you're not. Yeah. But yeah, for reasonable t, yeah. Sorry if I like missed the threads in that, but like I guess a different thing you could worry about is you could worry about the adversary making there be no negative eigenvalues. And then you wouldn't be able to solve like detection either. Be able to solve like each action either? Yeah, yeah, yeah. So there's definitely some question of how large can delta be. But I guess the thing that we're showing is that this negative outlier, which contains information about the communities, is pretty far out. You have some buffer. So if delta is a small constant, you're definitely not going to be able to be able to fully delete this. Also, the quantifier is presumably when you're above the chaos bound by some amount, that constrains what the delta. Strains with the delta exactly, yeah, yeah. Right, because if you like are epsilon above, then you can like add or delete epsilon nine edges and just get to an original graph. So it's not interesting past that threshold. Yeah. Okay, so now you're only concerned about the recovery questionnaire. Yeah, we're just concerned about the recovery question. Theoretic procedure wouldn't I mean in the sense that you would try to um remove as many eigenvalues as as possible by uh you know like It's possible by just getting a few edges. Yeah, yeah. You mean like, for example, like deleting like high-degree vertices or something like that? Or just like information theoretically. You know that the supposed to have like a matrix that is small like this. Yeah. And now you can only ask you I don't know. It's possible to ensure this problem. It's possible to infer this problem. Yeah, yeah. I think that's pretty close in spirit to what we end up doing to deal with these annoying negative eigenvalues. So it turns out that for the rest of this talk, I'm just going to talk about this self-contained linear algebra problem, which might be interesting to people in this audience. Okay, so the setting is: I have this symmetric real matrix M, we can think of it as being the same matrix that I said, phrased the That I said, phrased the, or that I stated about the non-robust recovery. Okay, so this matrix M, it's normalized, okay, whatever. It has a few negative eigenvalues, and I guess most importantly, like, there is some Boolean witness, right? Because we're in two community SVM, such that this quadratic form is negative. Okay, so basically, this Boolean witness is actually telling you that there is, it's certifying that there's a negative eigenvalue. And as the algorithm designer, like you're given this. Algorithm designer, like you're given this corrupted version of M, where you have this corruption, which is actually a localized principal submatrix. It's pretty important that this is localized. Okay, and the goal is to output some low-dimensional subspace U, which is actually correlated with X. But sorry, what do you mean by localized view? So here I can say, like, okay, delta is bounded in norm, and it's only supported on delta and coordinates. So, like, it can only affect a few vertices at a time. Yeah. Okay, so a first natural approach to movement. Yeah. Yeah. So the number of edges you touch is also under control. Yeah, yeah. Okay, so like a first natural approach is like trying to throw an SCP at this, because SCPs are used all the time to robust apply spectral algorithms. And yeah, some previous approaches did try to use SCPs to instill this robustness into. Instill this robustness into this recovery problem. But turns out that, okay, like while this basically works for us, for some very annoying technical reasons, this doesn't actually always give us a good subspace that we want it to. In particular, it turns out that when the average degree is between 1 and 1.5, this for some reason doesn't actually work, or at least we weren't able to show that it works. So I can talk a little bit more about this offline, but yeah. Bit more about this offline, but yeah, we tried this initially and we had to come up with something else. And so we actually give a different spectral approach, which is subjectively maybe a little bit simpler and also has a bit better parameter dependencies in terms of robustness. Okay, so here's a picture of this corrupted spectrum. And our high-level algorithm is we're just going to try to click Our high-level algorithm is we're just going to try to clean up this corrupted matrix into some other matrix m prime, and then at the end we're just going to output some bottom subspace of m prime. And you know, okay, ideally, maybe we can't actually delete all of these bad eigenvalues, but we can delete most of them. So sort of get to this constant number of negative outliers. And like I said, the important thing is, like, you know, in this problem, is that the corruptions are very localized. So there's this There's this intuition that actually the new eigenvectors that the adversary introduces are also going to be super localized. So maybe you can use this structure to hope to detect and delete these localized eigenvectors. Okay, so it actually turns out this intuition turns out to be essentially correct. So it turns out that you're either in this like win-win scenario where maybe your matrix M, like M tilde already has few negative eigenvalues. Already has few negative eigenvalues, or it's the case that, okay, so let's look at, let S be the set of corrupted coordinates in M tilde. If I look at this matrix M tilde with these corruptions on S, this is like some principal submatrix on delta M coordinates, then if I look at the corresponding projection onto the negative eigenspace of M tilde, like actually most of the mass is going to lie along these diagonal entries. These diagonal entries. Okay, because I put a lot of corruption budget on those few coordinates. So I think this is the most... So coordinates here means edges, not just vertices. So this is an n squared by n squared matrix we're looking at? Oh no, this is vertices. Still vertices, actually. Oh, okay. Because I guess s here is the nodes that touch a corrupted edge. Okay. And you still have the delta n bound. Exactly. You're also like you're using localizing to reflect. Also, like you're using localized in two different sensors, right? The corruption matrix is localized in the sense that it touches very few edges or whatever, but you also mean that the eigenvectors of S are sparse, basically. They cannot be like flat. Yeah, yeah, yeah, yeah. Yeah. Okay. So yeah, I can answer another question. Later. Okay, okay. Yeah, so it turns out that you can run the following cleanup trimming procedure. Following cleanup, like trimming procedure, like cleanup procedure. So, I'm just going to repeat this process until I only have a few negative eigenvalues left. I want to sample index i with probability proportional to like the diagonal entry for this negative projector, and I'm just going to delete that realm column. And because of this observation that I outlined in the previous slide, it turns out like deleting this, like doing this process at each step will delete a corrupted index with constant probability. So, like at the end, So, like at the end, you're going to obtain some principal submatrix m prime with only a few negative eigenvalues because you terminated. And with high probability, the number of rounds you go until you terminate is like, oh, delta n. So you didn't like delete the whole matrix. And okay, because of this assumption of localized corruption, and I have this Boolean witness vector, which is actually very delocalized, this quadratic form is pretty stable to the localized corruption. Pretty stable to the localized corruption. And it turns out that you can show that this quadratic form is still going to be negative. So, in particular, you're still going to be able to certify the presence of a negative eigenvalue in your matrix. So, this is what I meant by you're not going to be able to mess up the informative eigenvalue. Okay, and then output is just going to be the negative eigen space of m prime. Any questions about this? Yeah. So, this certainly. So, this certainly agrees with the picture that we had, like when we were writing about the non-backtracking matrix before Bordenov, Massa, Vasuel, and Lamarge. But I'm curious how you prove that the corrupt eigenvectors are in fact localized. I mean, that totally fits with experiment and intuition, but I didn't realize that that had been proved. In fact, when we first defined the non-backtrack, well, we were not the first to define it. Well, we were not the first to define it. We first proposed it for this problem. It was precisely to delocalize the eigenvectors around, for instance, the outlier high-degree nodes. And so I also wonder if your approach could work directly using the adjacency matrix, just hunting down those localized eigenvectors and tossing them out. But anyway, getting back to the initial question: is it easy to say? I mean, we can take this offline. Easy to just say, I mean, we can take this offline, but how does one prove that the adversarial eigenvectors are localized around the adversarially tweaked nodes? I mean, are you using anything more than just the sparsity of the things to argue about it? Like, aren't they kind of so sparse that there's no hope but for them to be localized? And then you sort of bound how much the eigenspaces tilt when you add the two matrices together. Yeah, yeah. Yeah, yeah. So, I mean, I think like literally, like, if you like, the formal assumption is going to be something like the L1 norm of the corruption is going to be bounded. So, by holder, like, we know that it has to be delocalized in the other direction by just unfolding there. And, like, because it's a bounded and L2 norm, you get holder, you get the delocalization, the localization. Yeah, so maybe it's the following reason. If I look at the graph S of the edges corrupted, either edge looks randomish, then the eigenvectors will be delocalized, but the eigenvectors will be. Will be delocalized, but the ideal will be smaller, so it doesn't matter. Or we have to somehow have the graph with like Boolean objects, which will be delocalized, sorry, which will be localized, but larger. In some sense, this is everything that can happen. Exactly, yeah, yeah, yeah. And the click situation, you can basically see a new frame with really good probability. I just, it might be worth mentioning, like, in the physics community, people like Panzhang have experimental results where they look at the eigenvectors and sort of. They look at the eigenvectors and sort of ignore the ones with a, I forget, low or high participation ratio or some measure of localization. And this also works well experimentally. Yeah, yeah, yeah, exactly. Yeah, I think this is like where they add some like type of L4 regularization or something like that, right? Yeah, something like that. Yeah. Yeah, okay, so let me just talk about open problems and then I'll finish. Okay, so this entire talk was sort of motivated on the The this entire talk was sort of motivated on the first slide about talking about like how KS specials change when you like change the modeling assumptions. So one interesting direction would be like okay in these more semi-random settings you know does a chaos special change? We talked about the monotone adversary where it does change but there maybe there are other semi-random models where it doesn't change or maybe it changes in an interesting way. So for example like instead of looking at the complete graph with these random observations maybe you look at some family of worst case graphs and looking at what you can achieve there. A lot of probation is sort of a Although, you know, probation is suggested. And then another problem which is slightly different, so it's Swil and Misha, they have this paper in stock where they show that AMP can be made robust by using SOS with large constant degree. What we showed was some lightweight spectral algorithm that succeeds after you do some type of pruning procedure. It'd be interesting to see if some variant of pruning and AMP can give a faster, robust AMP algorithm. Um that's right. At the end of the day, what's the runtime of your algorithms? Yeah, so it's going to be like, I think, so I think it's going to be like order end. Order n to the fourth or n cubed. So you run like n rounds of this deletion thing, and at each round you have to like compute a special decomposition. That's like the main thing that's going to problem I use. Yeah. Probably the fastest, like even if you just put restricted two communities, this is probably the fastest algorithm. Like this year it says do not have a fast algorithm, right? Yeah, okay. Just check me out. Thanks for watching. Asking questions where you know the answers to. I don't know what's special about the, like, what's special about the beta hush compared to the non-factor matrix? I mean, like, could you have done a similar analysis for my favorite matrix? Or, like, is there something really special with this? Or even the original adjacency matrix. Yeah, yeah, yeah. Yeah, that's a good question. That's fine. Yeah, so I think, okay, so the not so. I think, okay, so then maybe for the original adjacency, it's a little bit hard because of this annoyance about high-degree vertices. So I think that's what the original intuition behind using non-factor. I see, I see. Okay, that's why you can't just weed out the delocalized high-defectries and then it just like some of the original approaches really just tossed nodes that are at a high degree. And then you could get like a constant factor of KS, but like you really do give up something once you once you toss it. Okay, but then there are other matrices that have like you know that correct for this. For example, I guess the Laplacian, just like at t equals. So I think the Laplacian is going to run into the same issues as the OJS is integrated. If you take the normalized Laplacian print, yeah, so you like the she's she wants to be made by the degree maker. Oh, yeah, like the normalized Lazy. Yeah, yeah, yeah. So like one of the other things is sort of Montana sent. One of the other things is the Montanari-Sen paper handles high-degree nodes in a very different way because it only looks at inner products of the matrix, like the adjacency matrix, against things which are PSD and have ones along the diagonal. So in effect, prevents how much contribution you can get from binary nodes. But that doesn't actually reach the KS. But is there anything special about the is there anything special about the like other than this, right? Like I guess there are other matrices that you're trying to recognize. Presumably, they also work. Is this one nice to analyze with the trace method or something? Yeah, yeah. So, like, the trace method is, it's like a lot simpler in this case, because, first of all, because you want to get robustness, you only look at constant length logs. And so, like, you don't actually end up needing, you only need to compute the first and second moment of this quadratic form. And because of the non-backdracking structure, it's actually going to play really nicely with the definition, because there's this nice identity. Methahessen, because there's this nice identity that if you invert the methahessing, you get some power expansion in terms of non-backtracking powers. So that's really the intuition behind why that's the correct way to amplify signal. Yeah, yeah, in the in the random walk matrix, so the stochastic version of the adjacency matrix, you get a different kind of localized eigenvector, namely when you go down a little, when you go out into a little tree that's hanging off the giant component. That's hanging off the giant component, at the edge of the giant component, you can wander inside that for quite a long time. And this creates a different, you know, it's kind of funny. It's a different problem of localized things in a high degree, but it also, you know, non-backtracking also gets rid of that. But it's pretty cool. Yeah, I think I found the plot the drawing but case. The plot, the drawing but the case and that's excellent. Yeah, that's nice to find. Excellent. One more quick comment while time is setting up. So for regular graphs, we have all regular graphs. There's a version of the one called regular press. You know, even the original adjacency went shoulder worked well. Yeah, but you could still