Thank you, thank you Valeria and thank you to all the organizers for setting this up. I mean wonderful place, great people, good food. I'm happy. Thank you. Thank you very much. So today I'm going to talk about kind of a hot topic that is sketching. We heard about it earlier this week. It's a kind of like well known topic in theoretical computer science. Maybe a little less in numerical algebra, but it's getting popular. But it's getting popular. And in particular, I'd like to talk about how we can combine scratching with a more traditional American algebra tool that is piddle method. Okay, so I'd like to talk, I'd like to start with a gentle introduction about this sketch piddle method, and then towards the end of my presentation, I will present some recent results coming from a joint work with Varelia and Marcel Schweitzer from Ubertalkin. Upper Tal in Germany. Okay, so I'd like to start with this picture here. This is a picture from this paper by Eugene Katsukaza and Joel Trough that, to the best of my knowledge, that was kind of like the first attempt to combine sketching and clothing method. In particular, they presented sketch GMRS there. So I think this picture is quite important to motivate why we are interested in sketch git method. So we have a little Sketch bit of method. So we have a linear system, AS equals B. A is pretty large. It's this matrix here coming from the suit sparse matrix collection. B is a random factor. And here, this black solid line, you see the residual norm history, relative residual of GEMRAS. This is plain GEMRAS, no preconditioning, so we need a lot of iterations to get to the manual 13 or something. To 10 to the minus 13 or something. And we know that since we are constructing such a large space, the cost of the opticalization within GMBRAS is pretty large. Indeed, if we look at the computational timing here, we see that the cost of GMBRAS grows pretty fast. So, one of the remedy for this issue is the restarting paradigm. So, restarted GMRAS. So, here you see So, here you see three instances of restarted GMRAS: GMRES 10, 30, and 100. So, that means that we restart GMRAS every 10 or 30 or 100 iterations. And you can see here that this technique is pretty good at reducing the computational cost, the computational timing of TFRES. But on the other hand, we know that it often or leads to a delay. Lead to a delay in the competence of the method. So, why are we interested in sketch Jim Ras? Because if we look at the residual history, you can see this a solid red line that is pretty close, it follows pretty closely the black solid line. So, we have a convergent history that is pretty similar to the one of standard GM brass. And then you will say, okay, well, the two lines are close. Well, the two lines are close, that means also the computational cost probably is going to be close to each other as well, but this is not the case. So, Sketch GM REST is able to basically have the same convergence history of plain GM REST, but at a much lower cost. So, I think this really motivates why we should look into this kind of method. So, let's see what sketch gemras here is. So, let's start from like plain gemras. I don't think I Like plain gem rats, I don't think I need to go too into details here. We all know this. So we have Ax equal to B, we have initial gas X0, initial received wall are 0 here. And what we want to do in GMRS is like to look for a solution Xn of this form where Vm, the columns of Vm are an orthonormal basis for our kiddos subspace, and this vector Ym. This vector ym that contains the coefficients of the linear combination in terms of the basis factors, it's computed by solving this is force problem here. And we know that computing y amp in this way is equivalent to imposing a type of altering condition on the receiver act. This is plain Jim Rasp, but when we move to sketch GMRASP, things are pretty similar. Again, our solution has the same. Solution has the same exact form as before. Um is again a basis for our clear satisfaction method, and what changes is how we compute this factor Ym. So now Ym is the solution of this sketch, this space problem. Okay, where we have this sketching matrix S here. This is a rectangular matrix, it's S times N, where S is the sketching dimension that was supposed to be much smaller than. That was supposed to be much smaller than m. And again, by computing ym in this way, we can show that this is equivalent to imposing this sketched petrogalakin condition here. So we want the sketched perceived vector to be orthogonal to this sketched space A times theta. Okay, so the first question that it could come into your mind is: how do we choose S? What is S? S. What is S? Donimir talked about this property a little in his talk. What we need to have here is this epsilon subspace embedding property. So given a subspace V, of course going to be our kiddo subspace, we want S to be such that when we apply S to a vector in our subspace, this doesn't really change much the length of our vector. Okay, so the norm squared of Okay, so the norm squared of S times V is bounded from below and above in this way here. Okay, moreover we want S such that A times X is cheap to compute. Of course we want S to be an external subspace embedding for our closed subspace method, but we do not know our Kloss space method a priori, right? We do not know how it is and the vectors that are in that space. In that space. So that's why we need to use so-called oblivious subspace embeddings that basically construct it, can be constructed by only knowing the dimension of our subspace. Again, we don't know in principle at the very beginning of the process what's going to be the dimension of our credo subspace, but in general we have an upper bound on it. We fix the maximum dimension. And so we can use this end mass to construct our oblivious subspace in the. Construct our obligation space embedding. And also, SpongeBob has this kind of like rule where this catching dimension s must be of the order of this dimension m max times epsilon squared. Okay, so this shows you that epsilon cannot be tiny because otherwise the sketching dimension would be huge and then we would not have any gain from a computational point of view because S would be extremely large. So that's why you resume. Extremely large. So that's why you usually use values of action like 1 over square root of 2. So that means that S, the sketching dimension is about 2 times the maximum dimension of the phyllosa space that we allow. Okay? There are very many different sketching matrices that we can choose from. This is just an example. Whatever we do is kind of like independent of this selection. Independent of this selection. Here, again, just an example. We have this factor here, this normalization factor square root of n over s. And then we have these matrices here. We start from the right. F is just a discrete Fourier transform. E is a diagonal matrix whose diagonal entries are independent stand-house random variables. I don't really know what that means, but you can look at this paper here from Mattison and Schop. This paper here from Mattison and Chaub. And V is an S times N matrix that randomly picks S components. Okay, so that means that the action of S on a vector V, it's very cheap to compute. Just analog N. Okay, so I'd like to mention here that for me, sketch kilo methods are not randomized in your algebra. Because even though we may have this. Even though we may have a catching matrix here that has indeed a random nature, as this example here, this S is fixed at the very beginning of the iterative method, and then it is used throughout the iteration. So for me, randomized linear algebra is something where, for instance, you have an iterative method and you inject randomness at each iteration by for instance sentence, something like in randomized SVD, for instance. This is more similar to, I don't know, the power method. Similar to the power method, where you choose an initial vector that is random, and that's it. So, S is chosen at the very beginning, and you use that. So, going back to GMRAS and sketch GMRES, so this is the general framework. Again, our solution X is chosen in XG is chosen in that way, where YMG is the solution of this. Mg is the solution of this least square problem, and it's very similar to the scat TMRS framework. Again, the only difference is in how we compute YM as G. So, thanks to the epsilon embedding property that I showed you before, if we assume Um, so the basis that we use in SketchGMRES to be orthogonal, then we have that the received one norm computed by SketchGMRES is bounded from below. Bounded from below by the GM rest residual, and from above again we have the GM rest residual multiplied by the scalar here. So this means that scat GM rest cannot do better than GMRS, but also not much worse. Because again, for instance, for epsilon equal to 1 over square root of 2, this value here is about 6. So the receivable norm in Scatter GM mass is about 6. Norm in scatter GMS is about the same order of the one achieved by plain GMRS. Okay, so in exact arithmetic, again, if UM is orthogonal, sketchy mRES cannot do better than plain GMREST, but also much worse. However, we know that in GMRES, again, the bottleneck, computational bottleneck, is Bottleneck, computational bottleneck, is in the basis construction because of this full orthogonalization. And both, as I showed you, both GMRAS and SCATGMRAS achieve the solution from a basis of the Ploxus space. And that means that if we use an orthogonal basis in Sketch GMRES, since we're just changing how we solve this problem here, probably we're not gonna get. Probably we're not gonna gain much because, again, the main computational cost of this geographization of the business is still there. Okay, so that's why I think that the setting where sketch credit method are extremely powerful is that we use not a fully orthogonal basis, but a locally orthogonal basis. Okay, so we're not gonna We're not going to perform a full orthogonalization, but just a truncated orthogonalization. That means Um is not orthogonal, but just locally orthogonal, meaning in our branch meet loop, we just orthogonalize the new basis vector with respect to, let's say, the last k basis vector. Okay. So this means that we're using a truncated pillar method. This is pretty well. Criddle method is a pretty well-known method, and again, since we do not have any more orthogonal basis, this truncation in the orthogonalization may lead, most likely leads to a delay in the convergence of our method. Um, and 2 is also A times UM, and A is S times A times UM can get very conditioned because, again, we do not force an explicit. Force an explicit orthogonalization. But in the sketching literature, we have a remedy to this that is called widening. It took a bit for us to understand what that is, but that's just a QR virtualization. And so, of course, we cannot compute a QR virtualization of our basis UM, because that would be equivalent to performing a full orthonormalization in Rm. Full of factorization in Rn. But what we can do is to perform a QR, a scheme QR factorization of this catch basis. So this is now an S times M matrix. It's small. We can compute a QR factorization of this guy here. And then we can use R in this way to define this new basis Um tilde. And it can be shown that this Um tilde is very well to reach. In particular, we In particular, we can also show that this basis here is now orthogonal with respect to the innet product induced by S transpose S. Of course, that's only a semi-definite matrix. But again, thanks to the epsilon embedding property, you can show that on the space where this matrix is in epsilon subspace embedding, that induces an actual unique one. Okay, so that's why we. Okay, so that's why we can say that this basis is orthogonal with respect to this inner product. Okay, so this widening fixes that problem with the conditioning of the basis. But if we go back to the actual solution process, how we can implement this algorithm, again, we need to solve this sketched least request problem. And if we didn't have this. And if we didn't have this S here, like in GMRS, then at this point we would take advantage of the annual evaluation and we will show that solving this least press problem is equivalent to solving a much smaller least pressure problem with the anode match, let's say this HM. But at this point here, we do not have any unloading derivation. So what we need Derivation. So, what we need to do here is to compute again a QR factorization of this guy here, and then we can solve ym. We can compute ym in this way here. And by using w, this is the orthogonal matrix coming from the draw factorization, we can also compute the norm of the sketch residual or the sketch norm of the residual in this way here. Okay, and again. Okay. And again, we know that in the standard case without S, the amount of relation also helps in deriving an easy and cheap way to compute the receivable norm. And we do not have that here. So that's why we thought that that was the first point to be addressed. And we did that. So even the standard unloading. Even the standard amode relation with UM, so um may be not fully orthogonal, but the Arnodi relation holds also for truncated method. So we have this and then if Qm plus 1, Tm plus 1 denotes a QR factorization of the sketched basis here, then we do have this, we call it sketched Arnoldi relation. Relation. What is important here is that here we have exactly the same HM that we had before plus a rank one matrix here. So sketching basically leads to this rank one modification to our HM and also here we have this F Qm plus one, that is the M plus first column of Q and so indeed these guys And so, indeed, this guy here is orthogonal to this SUM, and that's exactly what happens also in standard and ALDI, right? So, here we have UM and UM plus 1. These two guys are orthogonal with respect to each other. The same happens here. Okay. So, having this sketch on all the relation has many consequences. First of all, from a computational point of view, we can Point of view, we can actually rewrite our scapped least squares problem in terms of the HM matrix coming from branch mean. That is the one that contains the orthogonalization coefficients. We just have a rank one multiplication to this matrix here. But this is not just, this is not the most important part, I think, because I think what is really important is that now we have a less scary picture of what's going on. Of what's going on, especially when we need to analyze this kind of method. Because before, when we had this definition of ym, we didn't do anything with it in the analysis of the method. And we know that basically when we talk about Krilov method, basically any results about the properties of Krylov method relies on the unloaded relation. On the anode derivation. So, not having that anode derivation in this framework was a real issue. But here, now we do have that anode delivery. And what I'd like to stress here is that, again, here we have a rank one modification to our HM, but it's not just a rank one modification, it's a rank one modification that changes and acts only on the last column of our HM. Last column of our HMAP. And that's extremely important because, for instance, it allows us to maintain the moment matching property of the Kilo method, but also polynomial exactness of the Trinov method. Meaning, for any polynomial of degree less or equal than n minus 1, P A of A times P can indeed be written in this form. So it can be exactly represented on our side space here. Here. Okay. Another important aspect that we analyze thanks to this catch-on-org relation is this here. So, okay, we have a rank one modification to our matrix HM, but having this rank one modification can really changes the, can really change the spectral properties of HM. Properties of HM. We don't really have a control on this aspect. So, for instance, HM could be stable, for instance, or definite, and HM plus rank 1 can be indefinite or even singular, and that would slow down the rest. This is a pretty common situation, for instance. So, we have these red crosses here at the eigenvalues of Hm. The blue line is its field of values. Its field of values. And then you see the black circles are the eigenvalues of Hm plus frank 1. So they're mostly close to the field of values of Hm, but some of them are far away. So these eigenvalues here could be problematic. But in practice, it turns out that they are not. So we wanted to understand why this is not the case. So we did is uh this is not the case because um if you if you look at the entries of the related eigenvectors you can see a behavior like that. Okay, so if we look at the eigenvalue of uh h m plus rank one that is far from the field of values of h m, Values of Hm, then the corresponding Eigen vector, the entries of the corresponding eigenvector have a trend like this here. So you see that the first components are basically zero and then they grow to have nor equal to one of these factors here. Okay, and this trend depends on how far the n value is from the field of values of HM. From the fiddle values of HM. Having these guys here very small, it's crucial, it's been crucial for us to be able to derive sensible bounds between the sketch solution and the real solution. I'm not going into details because it will be very technical. But what I'd like to stress here is that this kind of analysis can be done thanks to our sketch and all the relation. The relation. Okay, because now we know that we have a ranked one modification. Okay, so as I said at the beginning, I think that sketch GMRS should be used only when and together with the truncated construction of the basis. So you could say, well, why then you can use just a truncated gene rest scheme. Scheme. In this case, the solution of this catch scheme and the Tunkirk scheme would be retrieved by using the same exact basis. And again, what changes here is just how we compute ym. And the least square problem here, again, differs only from this rank one term here. Exactly. Yeah, but rank some question. It's EM. So yeah, you're right. And last vector. Yeah, yeah, absolutely. So you would say, okay, if we do not have this rank one, this is exactly the same thing. Okay? We said that truncated gym rest often leads to a deal. REST often leads to a delay in the convergence of GMRES. And it seems that by adding just this rank one term here, it fixes that program. Here in the image. So here we're using sketch GMRS, dotted red line, dashed line, and shunketed GMRAS, these flat points here. Points here with the same truncation parameter. So the basis in both methods is such that we enforce orthogonality only with respect to the last 10 basis vectors. And you can see the sketch GMRES, even though we're using the same exact basis, sketch GMRES follows GMRES precisely. While truncated GMRS has this delay. I think this is crazy. It's pretty impressive, honestly. Honestly. Because again, the construction of the bases is basically extremely cheap, and again, we're able to maintain the same performance of Jim REST in terms of convergence. Okay, so to conclude, sketching can be a very, very powerful method to speed up basically any, I think, any numerical algorithm. When we combine sketching with pre-look methods, When we combined sketching with preload method, we saw that we managed to maintain the computational efficiency of truncated method, but also the convergence of the full Jim RAS method. We don't really understand yet fully what's going on with sketching and pillow method, but we think that the sketch and load i relation is a first step in this direction, understanding. In this direction, understanding better this method. And of course, since the anode relation is the base of any closed method for a different algebraic problem like matrix functions, matrix equations, and other problems, this technique can be extended to that to close setting as well. So if you are interested in more details, this is our reference. Please have a look at it or come and talk to us. Thank you. Questions? Please? I have two questions. Just on the previous slide, the dimension of UM for these two curves, is it equal to this number of steps chosen to produce a basis? So the basis here, yeah, it's going to be the number of columns, yeah, in the sketch. Yeah, in this catch the MRES is going to be like I don't know 250 and here it's going to be 550. So you're saving work by not forcing these to be orthogonal, but there's still a fair amount of work on this step. I think if I have it right. Is that true? No, I didn't get the question then. You're still using a linear combination of a basis of size, whatever that is. Yeah. Whatever that is. Yeah. That's still large. We don't store the true finger. I don't understand. I think that number is. But even if you store the vector, I think the authorization cost is the same because it just authorizes a new basis vector with respect to the previous ten. The thing is that here we do that like many, many more times. Many, many more times. Okay. Because we need 550 iterations to converge. But here we just need 200. One other quick question. You mentioned the sketching matrix at the beginning on Hodinson and that didn't seem to have anything to do with the actual problem. It's just some special technique. Absolutely. As long as this. Absolutely. As long as this action embedding property holds, you can use whatever you want. You said that you need to fix this. I'm just thinking, you know, can you talk about that? I'm just thinking you can kind of restart once you have a rich enough space, maybe solve smaller least phase problems. Maybe you lose some theoretical properties, but then accelerate computing. Is that a legitimate thing to try? Legitimate thing to try? Could be. Again, we do not really form S explicitly. So even though we have S little large, it's action, it's cheap. So even though we have, for instance, here, S equal to 1200, that doesn't really increase the computational cost of the overall procedure much. But the number of columns would directly relate it to the overall. Directly related to the overall computational cost. The number of costs of what? The dimension of S, right? So. Comment on this? May I make a comment? Sure. So the presentation here is for the inverse system. But this is mainly relevant for lattice function evaluations, these equations. I can buy problems where it's not over the star as easy as you can. As easy as Jim Pax. I see. Functional evaluation isn't really nice than starting. So this is really a breakthrough actually. I find it really impressive. And of course, as someone else asked, S is extremely obscure. Why it's working? You don't understand. Let you see here, for instance, the main potential of cost in applying this kind of S is uh poly logarithmic in N. Theoretical. That means very good implementations, particularly this particular one. Well, FFT method. So this is not gonna be, I don't think you're gonna get huge gains from the size of matrix your time. Gaussians could be quite fast because matrix matrix for applications is. Yeah, again, but I think the application, yeah, the application of S, again, if you look at the organization. So the SGMRS here is a very simple thing. The SGMRS here is mathematically equivalent to the original SGMRS by Nakasikas and Trop, right? So do you h notice any differences in numerical behavior of using one formula for the solution? Oh yeah, absolutely. That was also an act better because, you know, the air conditioning of the basis is inherited here in this vector R. So how you implement that, this is a formulation that is very convenient from a theoretical point of view, but from a practical point of view, we need But from a practical point of view, we needed that brightening and stuff like that. So you need to be very careful with that. That's my experience as well. So, in the end, which implementation would you recommend in your so you can use lightning and then you would have like the same exact structure with a H that is similar to this HM here and then you have like an extra inverse here and stuff like that. You need this You need this T from the QR factorization of this cached basis. So what's it? Yeah, R, this guy here. And that will be conditioned. Yeah, exactly. But that's crucial to have reliable results. So, can I ask Hilaria's question from the first day, which was: maybe you have another slide about work, and can we compare your method with ICG stab2 for the same problem? We do not have that. I mean, with the short-term recurrence method, you know, Spicy Jeb Stab two will converge in twice as many iterations. Yeah. GM res maybe look the same in terms of work. Maybe look the same in terms of work, but that's what we care about in the end. Yeah, I mean, that would be interesting for sure. We didn't do that because, again, our main focus in this work was a Matthew's function, for which the universe, of course, is a special case. So, I want to talk about linear system here because I thought it was like the most appropriate way to start looking at this topic. So, we didn't really work on linear system directly, but that could be. But that could be something to look at. And we didn't do that yet. Any more questions? Thank you, speakers, again. Are there additional things that make it all the most important? My only selection is like a tape. I thought it's that space. Yeah, I'll be in the middle. But one of the activities that we can say is market time sessions. In which it should get done again, the director's. I just wasn't saying I'm saying that. Well no, but I think it works, you know. Started in function sort of defined as far apart. This epsilon and middle? There are several kinds of little things that are interesting at budget. But you know, one question I have, which is there a way to connect the sketching matrix to the map? The scheduled matrix and how that can be accountable. That's what I was asking, Dave. We haven't promised that we would start looking at it with the L sent yet. And I think the function would be like a transform, like a consigned transform that we asked him to make us more function. That's right, and I'm thinking maybe naively about all these finite elements. I think almost almost kind of projection and so maybe say more to like a signal process you're able to crash or point wait. Yeah. But I haven't seen it before in my life. It's really, really interesting, I think. It's a reduction, you have probabilities for things to work. You see a real I was intrigued by your multi-bed stuff. He showed the orphan high-pressure. Most of the time. You know, th they talk about bicycle style. It's by choice itself because you also like it to be a little bit more. The only issues that cities in the same basis are. That I'm not trying to oh, yeah, you need to rebuild the whole thing. Uh, you build it run that default y, uh-huh. And then, once you have y, yeah, you rerun that whole thing, which is very cheap because you need to combine. So, let me ask, so does it make sense to do that only for VMware as at all? I guess, so let me, I mean, CG mean results. I mean C G mean result of those are short components. But the problem is symmetric do we believe that is because there are some situations where they're running alpha or first you use generator. Yeah, I mean I talked about Jim Rest because I wanted to start from. Rest because I wanted to start from that paper, but for minus functions, they're okay because you have the fascinating