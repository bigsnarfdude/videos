But here they're sort of underwhelming. But we're close to the ocean, if that's the sorry do I do this? Oh, I don't know if I connect it. I'll just press it. So I'm a physical activity epidemiologist, and I do work in both interventions and observational studies to try and understand how different activities relate to health outcomes. But I also did my PhD with John and Patty Friedson. With John and Patty Friedson in a lab that was primarily focused on developing machine learning methods to apply to sensor data. And I think where I'm coming from in terms of presenting this talk is that the ultimate goal is to use these wearables. We want to develop algorithms that mean something and produce metrics that are meaningful to epidemiologists and interventionists, like how much time is spent in different intensity categories or different types of activities, because we want to better understand behavior and disease relationships. Relationships. So, I'm really coming at this from a signal processing perspective. We've seen a lot of work over the course of this week on total activity counts and how there's interesting patterns and descriptors. But since 2006, when John first published a paper applying a hidden Markov model to try and understand lab-based signals and predict activity type, there's been hundreds and hundreds of different studies that have applied different machine learning approaches to acceleration signals to try and improve prediction. From what Jennifer showed, Improved prediction from what Jennifer showed of this traditional threshold approach of if you're higher than this count, that's MVPA or not. As far as I know, there are less than five studies that have actually ever used one of these approaches in a study that is an intervention or a prospective cohort to try to relate to health outcomes. And that's a problem. This has led to a lot of confusion. So these are data that we collected just on 25 people with seven days of data. So each bar is a different. So, each bar is a different method. If it's in red, it's a hip-worn method. If it's green, it's an active pal. And then the blue is different risk processing methods. You can see there's 111% difference in the same people over the same week in terms of how much moderate to vigorous activity they do, depending on your processing approach. There's a paper that came out that showed that depending on which approach you choose, the prevalence of people meeting guidelines is either 3 or 97%. And for Kelly Evanson, published a paper looking at NHS. Kelly Evanson published a paper looking at NHANES data and showed that depending on the threshold you choose, you either need 25 minutes or 128 minutes to get a 40% reduction in mortality risk. So this is a problem. So is this obtained for specific thresholds for each device? So they wore the hip-worn monitor, they wore the active pal, and they worn the wrist, and then we just processed the data with the Frietzen method, the Sasaki cut points, the Sojourn method. The Sasaki cut points, the Sojourn method, the Crowder method. So it's the same data just being processed differently to estimate a VPA. Yeah. So I think that two of the main challenges and barriers to progressing are that we need to collect and share the amount and quality of data that is needed to improve these signal processing approaches. And then we need to open the black box. And I'm not a computational scientist. I know you can't actually get into the black box that is these machine learning methods. Box that is these machine learning methods, but we need to give applied researchers the information, the tools, and to better implement these algorithms into their studies. So, we published a paper back in October that lays out a framework to think about the stages of evaluating devices that assess physical behavior. So, we tried to make it parallel, for terminology's sake, to the drug development trials. Although, to be clear, we don't view this as trying to get FDA approval for a wearable device. Approval for a wearable device. That's a very different thing that others have talked about in terms of digital biomarkers. This is just saying, from the point that a device is manufactured, we want to know its reliably producing device output that we can predict. So that's testing it in a mechanical shaker. We can do lab-based calibration studies where you have people do 10 minutes of continuous walking on a treadmill. They can do vacuuming for seven consecutive minutes with set, start, and stop times. And then as we progress forward, the main things are that there's increasing variability. Are that there's increasing variability and transitions between activities as we progress along this? The vast majority of our data falls in this category. So most people collect this lab-based data or some sort of semi-structured data that says do these 10 activities in a row. But I think the main, the useful data falls in this category: these phase three, what we call naturalistic validation studies, where we have a ground truth measure. And then finally, we talk about adoption. And finally, we talk about adoption. So, we define characteristics of these naturalistic validation studies. They should be done in real-world conditions. So, you're not telling the person when and to start and stop an activity. They should be informed by time use data. And I think this is true across the whole spectrum, that there's studies where people have calibrated monitors using horseback riding as an activity or a frisbee golf, which are activities that people do. But on average, we know from time use data what people do most of the time. And those activities should be the bulk of what's included in. Should be the bulk of what's included in our calibration studies. We should try and strive for diverse and large participant samples. You know, I was guilty of this. We did a validation study observing people at work, and it turns out that sitting during driving is different than sitting at work, but we just defaulted to a convenient location to observe people. So we need to make sure we're getting different types of jobs, different types of activities in these studies. I think we need simultaneously to look at a variety of sensors and attachment sites. There's now about And attachment sites. There's now about 500,000 people or studies, people in studies around the world that have wearable sensor data, but it's the wrist, it's the hip, it's the thigh. And ultimately, we're going to want to pool these data and make public health recommendations. So I think studies using, even if you prefer the wrist, you know, adding a hip monitor so that we can begin to get comparability becomes really important. Obviously, appropriate statistical evaluation. I don't think we told this group that. And then using gold standard ground truth. Gold standard ground truth measures. So, I'm going to quickly walk through a study that we did. It's a pilot study looking with 25 participants that each had two two-hour sessions. They wore an actograph on their hip, both wrists, an active PAL, and then a monitor called a biostamp, which could be worn at the chest and the thigh and gives acceleration signals as well. We structured it so that the two-hour sessions, they were free to do whatever they want, but we gave them some parameters around. Want, but we gave them some parameters around what we wanted them to do within it. So they were observed, some people were observed at work. So it was whatever you normally do at work, you have to get up at least once. You can't just sit at your computer for two consecutive hours because that doesn't help us too much. Household activities, we wanted at least some of the session to be meal prep, cleaning, caring for children, sedentary leisure, active leisure, so they had to do some sort of exercise or recreational activity, and then transportation and errands. They had to take either a bus or a car to a public. Bus or a car to a public place, and we followed them around Costco and Target and all of these things with their GoPro cameras. People are surprisingly unfazed. We then went through and coded those videos. So the first pass we do is for the American time use broad category. So that's a large survey that the Bureau of Labor and Statistics does that asks people about 24-hour recalls. So that gives us some idea of how sort of representative the behaviors are. So we code for things like personal care, eating and drinking, are they work, if they're out in the community? If they're out in the community, socializing and then exercise. So that's the behavior. And then we go through and code for the posture. So you could be, you know, caring for others and lying down, if your kids let you lie down. Or you could be caring for others and standing and moving and puttering around. So the behavior is distinct from the posture in terms of how it's coded. We had 20% of our videos dual coded, and we were getting ICCs of 0.97, so really good. When we look at, in terms of When we look at in terms of how intensity classification, so these are different methods. So, ActivePAL is the thigh-worn monitor. These are two algorithms for the hip, hip-hip, wrist-worn monitor, and the hip. They should have been ordered better, but we finished these last night. So these are the F1 statistic, and we're looking at the accuracy of classifying sedentary, light, and moderate to vigorous activity. The colors represent the type of activity that's Colors represent the type of activity that's being performed. So if it's green, it's exercise. If it's this yellowish color, it's the sentient or leisure behavior that's not exercise. Work is the pink, and the household activity is this other color. So we see a couple patterns. The active pal consistently, the thigh-worn one does the best. And then there's a spread in terms of the types of activity. Household activity is the hardest. We think that's because people tend to stand the most. So for the hip-borne monitor and the The most. So, for the hip-horne monitor and the wrist, it's the hardest to distinguish between sitting and standing. I'm going to present some preliminary signal processing results, which I want to put a few caveats. So, I've worked for the past three years with data science students at Cal Poly. We don't have a big biostats group there, but we have these students who do a capstone project and they've been working with these data. So, I'm sure that all of you know much more both than I do and than they do. Both than I do and than they do, about these types of signal processing approaches. But they applied neural networks and then they applied a data segmentation approach to try and detect change points within the data. So I'm just going to show you these results to give you some idea of the types of things we can try and predict and then how they differ across sensors. So here's what our data look like. We just started with classifying these five things, sitting and lying, standing and moving, walking, running, and bicycling. And I know others have interest in this. We code at a fine Have interest in this. We code at a finer level than this, but we decided to call walking a one-minute belt for the purposes of this, and then lump together this stand and move where people are sort of walking back and forth. We have the belts categorized in 10-second, 20-second, up to a minute belts of walking, but this is just for this analysis. So here are some confusion matrices. This is the chest data. This is the wrist data. And so we see the overall. And so we see the overall accuracy of 72 for the wrist, 79 for the chest. We see some things like bicycling are just not predicted at all with the wrist data, which kind of makes sense. And then with the chest monitor, we're doing a little better with that. We move on to the hip data. We get a little bit better. We're still not good at bicycling, which again, I think physiologically makes sense. And then with the thigh, we're doing really well. So the prediction of sitting versus lying, this is from the bio standpoint. Sitting versus lying. This is from the biostamp, not the Actophel. Sitting and lying is 99%, and then overall accuracy of 95%. I think there's still lots of room for improvement in this. They were just predicting sort of on a second-by-second basement, second-by-second basis, using the segmentation as an input into their model, but not sort of doing more complicated things. And so moving on to kind of the adoption phase, so how do we get people to actually use more sophisticated models in Use more sophisticated models in their study. And I think that the things are that they need the tools, they need confidence, and then they need the information to make an informed decision because they don't have the expertise in signal processing and they all, all these words just sound the same. So people want to know if their intervention worked. So if they're trying to get people to walk more, they want to know did people walk more. If they're trying to get people to bicycle to work, they want to know did people bicycle more. And there's also the applied questions of how much activity reduces. Of how much activity reduces risk of disease. In my experience in working with people who are interventionists, who have risk-warned data, they have no idea what to do with it or how to process it, I think they are willing to invest the time or the cost in implementing a new method if somebody could say, this is definitely better. So, this is the approach that's going to be better. They want to be able to publish their data without someone saying, why did you choose the Friedzen cut point over the NHANES cut point? They want to be able to say, this is sort of the best. They want to be able to say, this is sort of the best method. It's been shown and it's been compared across a big enough data set to be able to make those decisions. I should put that up earlier. So I think it has to be meaningful to their question. It has to be convinced it's better than other methods. And then there has to be some tools available. You know, obviously they're going to need someone with statistical expertise to implement an R package or whatever. But I think someone mentioned the GGIR package, which is an R package that takes risk-warrant data, and people are using. Risk-worn data, and people are using it now not because the signal processing method is the best, but just because there is a package to actually process a week's worth of risk data. That approach is actually just a cut point for the wrist, which makes even less sense than a cut point for the hip in terms of how accurate that's going to be. So, I think we should be working towards user-friendly software that can process a full week's worth of data. So, people don't want to get, you know, process their data and just know was it driving or not. They want to know. Driving or not. They want to know the full package of the weir time estimation, the other metrics, and be able to get a complete analysis out of their data. I think we need to do a good job of giving within that succinct information on the description of a method so someone can just take that and put it into the methods section of their paper. They don't have to go try and read the full machine learning paper and understand exactly what was going on, but we can give them enough information that they can understand and talk about in a presentation. Understand and talk about in a presentation what they did. And then information of the validity on a large data set. I think this is what John's going to talk about as well. But my dream is to have this big data set that we've trained and tested models and we're combining things and we know that method A has been tested on this standard data set and gives you accuracies or R-squareds or whatever you want to look at. And we can sort of pool across and understand what are the strengths and weaknesses of different approaches because there isn't going to be a single sensor that's the best for. Going to be a single sensor that's the best for everything, right? There's trade-offs between the wrist and the thigh and the hip in terms of data compliance and all these things, but being able to make that informed choice becomes really important. So things that I'm interested in collaborating on. So we have that 100 hours of annotated data, and we're doing another 150 hours. So people are wearing five to six sensors. We follow them around the camera, we code exactly what they're doing over that time. I think there's a few of us in our little working group. There's a few of us in our little working group that are really interested in obtaining funding and/or just figuring out how to scrape together methods to share large amounts of labeled data and then use that data to develop new processing methods, which I won't be doing, but I would love to see happen. I have one more. And then the second opportunity is not related to the signal processing, but some of the other pattern-based approaches. So Ray presented our physical behavior score approach of trying to think about how we look at different types of activity together. Activity together. A lot of the metrics we've talked about are sort of time-dependent and are really interesting, these diurnal patterns. But I think there's also this is a paper we looked at trying to extract different features from Phi Warren data that were representative of, it's similar to the fragmentation index, but how are people accumulating their activity or sedentary time over the day? And then I think there's a whole area looking at these different compositional, isotemporal. Compositional, isotemporal, and other approaches that I know John's been working on to try and model these 24-hour because ultimately, if you decrease TV time, you're increasing exercise. And how do we sort of model and understand how those behaviors relate to each other? So this is my team at Cal Poly. And I thought I'd just stick this up because these are not new problems. This is one of my favorite quotes. This is when the early pedometers first came out. And I think it still all applies today. I think that still all applies today. Thank you. As we transition to our next speaker, please. Any questions? So I'm intrigued by your study where you're having people do these different tasks. Do you have different shapes, sizes, ages of people? Yes. Like child care for a 30-year-old and child care for an 80-year-old are going to be very different in terms of intensity and what. Different in terms of intensity and what it requires. Yeah, so this study is adults age 18 up to 75. We don't have older than that. We tried to get it's only 25 people. So I think that's one of the big things is that we need to build up these repositories and then start to be able to pool data across studies. But yeah, so we have we try to get different body sizes, we try to get different race and ethnicity. One thing I think is missing in all of our data is true of physical activities that we mostly have work in white. Physical activities that we mostly have work in white-collar jobs. There's a lot of, you know, it's harder to get people who are doing agricultural jobs or mechanics to participate in studies, and I think that's a big gap that we have. Do you think about relative intensity at all? So like what you're saying should be moderate or greater may be worse or easier depending on somebody's physical conditioning? Yeah, for sure. I think that I would like to see the field move towards Like to see the field move towards classifying activity type more so than strict intensity thresholds. So, these because I think that does give us, I think we can objectively distinguish walking slow from walking fast on the videos, right? And that's how we distinguish intensity of walking. But I don't think that the sensors, unless you have a measure of someone's functional capacity, you can't scale and we're not going to get VOT max on most people. So I think we have to think somewhat in absolute. I think we have to think somewhat in absolute, but understand it's going to vary across the aging threshold. So, in terms of thresholds, and in terms of, I think that the train just left without us. You know, so people just use thresholds left and right, and that is terrible. And somebody's MVPA is not somebody else's MVPA. And you show that. I think that that's. But that's what people are using. Like, that's the polar mess. Yeah, no, I. Yeah, no, I completely agree, but I also think even as we've incorporated more machine learning approaches that we think are better, we're not, it's like 15 people doing these set activities. There's still no clear consensus on like what the best way is, and so people are just defaulting to what's easy. But I'm not sure even if we ask people around whether we agree what is the right strategy. I think we should first agree as a group what is the right strategy. For example, my proposal. But for example, my proposal is to actually have, go back to the raw data, open source code that leads to a number, which could be memes or NMO or something else, and then have a definition of something that is in VPA, but include in it what is the actual processing pipeline, what is the actual threshold, and what if you don't include that, and If you don't include that and if it's not explicit, I think this is not going to have a solution. So I think that's really valuable to get these overall metrics and come up with a standard metric for that volume. I think there's so much more we can do with that data though in terms of identifying the type of activity, those types of things. And that's where I think that this idea of pooling data across people with free living ground truth data and look at estimating type of activity is really what we have to come to agreement on. What we have to come to agreement on, and I think that's what our liberal group talked about yesterday: is how could we actually do this, potentially have a data challenge? I don't think anyone's going to say this one method is the best way and everybody should do it in terms of activity type, because it's going to vary by sensor as well. It's not like this machine learning approach is going to be the best for the wrist. It may not be the best for the thigh, but we need to get that data so we can figure that out. Yeah, we in general starting as MDPA with four With four indices, which is location, so it will be W, H, or, and it will be whether or not it uses raw data and the type of aggregation, for example, ENMO or memes or something else. But is it a threshold or how are, like, what are you? The threshold would be part of that. So you will have one of the So, you will have one of the indices would be the threshold. But it's all based on defining MVPA. It's not even MVPA. You get the data, and you now say what is it that you've done to it. But have everything explicit there, because I think that when somebody says MVPA, when somebody says MBPA, and hey, it's totally different from when somebody says MDPA in PLSA. Totally different. And I think we should move away from that. And I would start to push back and say, you probably are not measuring MVPA. But we know that, because we have the ground truth, right? So we have the data and we can begin to take these different approaches and say, if we agree that this is MVPA, then we can estimate it across these different things. So I think, I mean, I agree that we should be transparent about what we're doing, but I think I'm not. But I think I'm not fully. Okay, we can talk more about that. Thank you. Thanks.