For a bit of model individual level data. Okay, so this is an overview of my talk. So I will first motivate our work and then I will mention some of the methods that are currently used within the context that they will describe. And then I will show you our approach to the problem. I will then apply the methods to Bayesian inference for multi-strain epidemics with applications. Multi-strain epidemics with application to Scheria Goli in cat tolls, and then I will conclude with some discussion and some summary of our findings. Okay, so infectious diseases remain one of the major causes of human mortality and suffering, and COVID-19 is an example of an infectious disease that we all are fortunately experienced and we still do. And we saw the traumatic impact of the coronavirus out. Impact of the coronavirus outbreak. So, our applications are on the field of statistical epidemic modeling. And in this area, the main objective is basically to understand the transmission dynamics of an infectious disease, or in other words, how the disease is spread within a population. And this process can help us to act against the disease, and for example, can be used to create some prevention strategies and other ways. Strategies and other ways to control it before it can reach larger populations. But statistically, this type of problem is also very interesting. And one reason is that we typically have a lot of missing data in the sense that we don't know when individuals got infected by the disease or even the times that they clear from it. And another reason is that we have a lot of information, our observed data might not be perfect. Observed data might not be perfect since they base on the agnostic test that can be wrong, for example. So, these issues complicate the statistical inference, and therefore there has been a significant amount of work of how we can perform parameter estimation for partially observed epidemic data. However, there are still some open challenges, some of which emerges with the fact that we have a lot of big data in the field in epidemiology, and this is where our work lies too. Is where our work lies too, is trying to address some of these challenges. Okay, so this is how is the structure of our data. So, I'm going to talk about the particular type of data that we have. So, we start with a population of individuals, which is divided into different groups. So, in this picture here, we have four different groups, and these groups think about them as households within a community, or for example, if we Community, or for example, if we're talking about animal diseases, this can be pens within a farm. And here we have a population of individuals and that is divided into four groups. And each group can have different number of individuals who are those within the big circles. So now, in our case, what happens is that we longitudinally gather data from this individual, and so we pick certain dates and we test the population. Dates and we test the population to see if the disease is present or not. So, for example, here the black dots are the non-infected individuals, and the red dots are the affected individuals. And as you can see here, it's only one group. And we take samples on specific dates, and then it is possible for an individual to recover and then reinfect again, such as in this example here. So, unfortunately, our data. Our data, what I mentioned before is like we have a lot of missing data because we don't take samples every day, such as day two and day three in this picture. But unfortunately, even on the dates that we have some observations, we only receive the outcome of the diagnostic test, and therefore we cannot be certain for the true infection status. And an interesting thing to notice here is that our missing data can be as large as our observed data and can be Observed data and can be several times larger if the sampling is sparse. So, because of the presence of these missing data, inferences for such problems become computationally involved. And the main difficulty is the fact that our likelihood is intractable because we need to sum over all these possible missing states. So, what we usually do is that we add these states as parameters into the model, and we can use some data augmentation, MCMC methods to impute them. On MCMC methods to impute them. And this is what our work has been focused on: on how to estimate this, how to simulate these additional parameters within the MCMC data augmentation technique. So the solution is given once we see that this epidemic model can actually be represented by this diagram here. So here I presented only for one group, where it's a graphical representation with only three individuals. So each line So each line corresponds to the different individuals, each column corresponds to different time points, and the arrows indicate interactions between the individuals. So if I focus here, for example, for each individual, the circles correspond to the true infection status. We denote those by X, but we don't really observe them. So those are what we say the unobserved or the hidden data. Unobserved or the hidden data, and the squares represent the observed data. So these are the results from the diagnostic tests, and those are not taken every day but only on particular time points. And in the literature, this construction is known as a couple-hidden Markov model, because basically it's a collection of hidden Markov models that interact one with another. And here I'm focusing on a Markov model. I will present the methodology for the Markov model. Methodology for the Markov model. And why is Markov? Because the infection status of the individuals, we assume that they only depend on the infection status of all the individuals in the previous time point and not in the history. But this can be extended later on. So what we can think about is if we put it back on the epidemic model, we can assume that we have one individual-based model. So we have information for each individual, where each individual can be. Where each individual can be either susceptible or infected at each time point. And we have some transition probabilities of them jumping between these two states. And this is a Markov model since the probabilities, they only depend on the infection states of the previous time point. Okay, so this is just a general MCMC scheme for the data augmentation. What I want to focus here is like To focus here is like that, just want to say that I'm not going to focus on the rest of the algorithms, how to update the model parameters, but I will only mention how we update the hidden states, this matrix with all the hidden states that I mentioned, because it's the most computationally involving part. Okay, so before I discuss some of the details of our approach, I will first briefly describe some of the existing approaches that there are on within the epidemic model that I describe. Model that I describe updating the hidden states within this framework. So the first method is called the block updates, and basically what it does, it propose to change blocks of state components within a single individual. So within a single chain, we only propose to change a block of these states. And this method builds on the fact that the individuals, they remain in the same epidemic state. They remain in the same epidemic state for a long time. And the idea is that for each individual or for each chain, one block of states that has a maximum length of, example, four or three is chosen. And then one of the possible changes is proposed, either to add or remove a block of infection or clearance or to move an endpoint of such infection. So and then the change is accepted. The change is accepted or rejected based on the usual metropolis hasting acceptance probability. And the main advantage of this method is that it's very computationally fast because most of the hidden state are not chosen to be updated. However, this results to very slow mixing of the MCMC change. And therefore, we need to run the MCMC algorithm for longer. So, okay, so another way to update the states is to draw each one of the States is to draw each one of the components from their full conditional distribution, and this is called the single-side method. So, if I go back to this diagram that I have here, basically, what it proposes is updating each of these red circles one at a time. So, at the end of the, from the full condition distribution, which actually can be found in a closed form. So, at the end of the MCMC, all of the nodes, all of the X's will be updated. However, the way that they are updated, because we update one at the time conditional or the rest, it results to very high temporal, sorry, it results to very slow mixing again of the MCMC change because it doesn't explore very well this high temporal dependence that we have within our epidemic process, the hidden states process. And another way, which is basically Basically, it's an alternative approach which improves the mixing of the MCMC chains and is well established. Is what is called the forward filtering backward sampling, or for simplicity, I will call it FFBS from now on. And the idea is that the FFBL side, we have a Gibbs step where the whole infection status, the whole infection matrix is sampled from its foot conditional distribution, which this can be found in a closed form. So this method has a very So, this method has a very good mixing properties. However, it's very, very computationally intensive and therefore cannot really be applied when we have a lot of chains, a lot of individuals, let's say, or if we have a lot of number of possible states. So there are algorithms. However, those don't scale well with larger populations. And what we did is that we proposed a method that actually tried to achieve a good balance between the computation. A good balance between the computational complexity and the mixing properties. But before I mention our approach, because it's highly based on the Vanilla FFPS algorithm, I will briefly explain this method in a bit more detail. So this method, what it does is that it groups the states within particular of the individuals within a particular time point, as shown in this figure here, and then it samples those jointly. So basically, what it does is that it's converting. Basically, what it does is that it's converting the couple-Hider Markov model into an equivalent Hidenmarkov model. But now, our state, instead of being the X only, it's basically the X for all the individuals in a particular time point. So we have N to the power of C. This is our hidden state each time. And in this case, we can update the whole hidden state process once we convert it to the coupled hidden Markov model once converted to a hidden Markov model. Converted to a hidden Markov model, we can actually just apply the standard forward filtering backward algorithm, where we update the whole hidden process from their full conditional distribution. And this involves the algorithm of finding like we have a forward recursion, which calculates some filter probabilities from time t equal to one up to capital T, which is the last time point. And then we have this backward simulation step where first we generate the last time point and then we go. Last time point, and then we go up to the t equal to one. And the computational complexity of this algorithm is given here. So, basically, you can see it's like exponential to the number of individuals. So, if c is the number of individuals that we have within a group, and n is the number of infection states, which in our case can be either susceptible or infected. So, n is equal to 2 in the simplest case that I describe. You can see that this is becoming very computationally expensive quite quickly. So, now we'll describe our approach, which is the main idea is to combine some of the advantages of the previous methods that I mentioned. And so in this figure, I have simply reformulated our initial diagram to explain briefly how the method works. Explain briefly how the method works. So, as in the block method, what we do is that we update the hidden states for each individual separately, conditional on the rest. But this time, instead of choosing a block of states, what we do is that we actually update the whole states for each particular individual. And the way this is done is by modifying the original FFBS algorithm. Original FFBS algorithm, and then we sample the hidden state individual per subject conditional on the remaining from their full conditional distribution, which actually can be found in closed form. So we actually have a Gibbs step. And this is different from what the standard FFB has done, is doing, where the sampling is done for all the individuals jointly. And actually, this modification reduced the computational complexity of the algorithm, and now we have. And now we have is like linear on the number of individuals rather than exponential. However, now we have n to the power of three. So, but it's much, much faster compared to the other one. And why is linear to the number of individuals? Because in each iteration of the MCMC, we only update one individual at the time. So we have like a for loop of the individuals conditional on the remaining. Okay, so um So now, what I will do, I will just show you some, apply the methodology to some real data. Let me see how it goes with the time. Okay, so our data set consists of observations from 160 cattles that were randomly assigned into 20 pens. So we have 20 groups and we have eight animals or eight individuals, let's say, per group. And our longitudinal data. And our longitudinal data comprises of fecal and rectonal musical swaps, which I will call RAMS for simplicity, that they are tested for the presence of E. coli O157H7 to the catals. So they were collected for each individual approximately twice per week. So we have 27 samples from each individual. And in each sample, we have the results from the two diagnostic tests. And the study. Diagnostic test. And the study period is like 99 days. So now the difference that we have with the previous approach that I described before is that in this particular example, the diagnostic test not only formats whether an individual carries the disease or not, so if not is infected or susceptible, but we have an extra information for some isolates where for those samples, we actually Samples, we actually have information about the strain of the pathogen that the animals are infected. And in particular, only 12 positive samples were selected from each of the pen to be typed. And here, what you can see is like a map with the location of all the pens that they have been using in our study. And as you can see, we have like these 20 pens. And there is always, so the colored one are the pens that they have like individuals in it, and the white ones are empty. And the white ones are empty. And as you can see, there is always an empty pen between them, ensuring that there was no possible direct contact between individuals in different groups. So what we do at that particular, for this particular study is that we assume that the pens are independent in that particular case. And this is a graphical representation of the data in animals in four different groups. So on the x-axis, we have the number of Have the number of time points, so we have 99 days. On the y-axis, we have the animal index, so we have eight individuals per group. And for each individual, the top row corresponds to the results from the RAMS test, and the bottom row corresponds to the results from the FECAL test. So the dots shows that the results was negative. The pluses indicate that the result was positive, but not chosen for genotyping. Chosen for genotyping. And otherwise, we have the name of the genotype. That, for example, here, where both of them were sampled, we have D and D, which is then what has been identified, the strain that was identified. And as you can see, in the most of the cases, when both results have been chosen for genotyping, they retain the same result. However, there are some cases for However, there are some cases, for example, here, if you see that one test indicates the results C and genotype C, and the other test results identify in genotype D. And this can be attributed either to misclassification error of our genotyping procedure, or because an individual may carry more than two genotypes. But how we tackle this problem is basically to assume that we have a misclassification of our genotyping procedure. And also, the data here, we can comment on a microepidemic of a genotype X. So, for example, individual 5 was the first that was identifying carrying genotype X at the beginning of the study. And then you can see there are another, almost all of the rest of the individuals that were also carrying this genotype later on. So, you can see that there's this microepidemic within a particular group. Okay, so overall, we have like 48 different genotypes that they were identified. Different genotypes that they were identified in our study populations, which we assign arbitrary labels according to the order in which they appear in the PFG typing. 24 they appear only once, and we have these seven major types where we have at least 10 RAMs and the fecal samples. So what we did, because we don't have information, we have like very little information for some of the genotypes, is that for the most common genotypes, we assume that we have a different We assume that we have a different car age. We have this, we call them, we gave them a name from one up to seven for these seven major genotypes. And the remaining genotypes, what we did is that we put them all together into a single group and we assumed that they have the same genotype, which we call genotype 8. So what we ended up to fit is a model, a multi-state model, which has nine possible states, including the susceptible states, the courage of one of the seven Courage of one of the seven most common genotypes. And then we have the courage of all the remaining genotypes. And also, we have accounted for misclassification, both in the sense that an individual can be miscoded as not career, or also in the sense that the genotyping procedure may indicate the courage from the wrong genotype. And this is what I described before: basically, that we might have some. That we might have some samples that one test gave a different result and the other test gave a different result. And this is a graphical representation of the model with only three genotypes. Remember, in our case, we have nine, but just like for illustration, but the most important thing to notice here is that an individual can jump between any of the two states with rates given at the site of the arrow. So we have this acquisition rate. So we have these acquisition rates, which is the probability, it's the rate that an individual is becoming infected, which depends on alpha, which is the community rate. Beta is the within pen transmission, within group, let's say, transmission rate. And this acquisition rate depends on how many individuals they have been infected in the previous time point. And then we have this clearance rates, which are constant. However, they're genotype-specific. So this alpha. Genotype specific. So, this alpha, beta, and mu, they depend on the different genotypes. So, we have different rates for each of the genotypes. Okay, so this is a graphical representation of some of the results where we fitted the IFFPS method, and it was not really possible to fit any of the other methods within a reasonable time, especially the standard FFPS method was not possible to be applied. So, here what we show. So, here what we show is that we plot the posterior probability of infection by a specific genotype for four individuals in the same group, in group three. And the different colors corresponds to different genotype. So we have nine different colors, including the susceptible states, which is the gray. And then we have different colors for each of the genotypes. And here for the reference, we also put the results from the diagnostic. Put the results from the diagnostic test. So, again, the top row is for the RAC test and the bottom row is for the fecal test. And what is interesting to see is that even though samples were not taken every day, but only twice per week, the algorithm provides the probability of infection for every day of the study period. Also, the algorithm can predict the genotype of an infection for an individual who is not chosen to be genotype. And this is a gene. Genotype. And this is achieved by borrowing information from other individuals or from the individual itself on a closed time point. So, for example, here we can see that the individual was identified carrying genotype 4 in this time point and at that time point, and then it has some pluses that were not genotype. However, based on this information, the alcohol can actually provide a probability of more than 90% that this individual was carrying genotype. That this individual was carrying genotype for this whole period here. Other thing that we can see is, for example, this individual, if we focus on individual six, what we can see that the method assigns a non-zero probability of the individual being colonized by genotype six, which is this yellow color here. Even though, if we look at the results, I'm not sure if you can see here, but basically, You can see here, but basically, the individual was sampled in this particular time point, and the fecal test was identified in genotype 5. However, the method allows for the possibility that this individual was actually carrying genotype 6 almost the whole period of the study. And this is because other individuals within the same group, for example, this individual here, was also identified. Was also identified carrying genotype 6 in a closed time point. But the individual itself at the last day of the study, so here, it was identified by carrying genotype C. So the method allows for the possibility that this individual was actually carrying genotype 6 all over the time period. Okay, so here is like just like as a simulation study, we wanted to investigate the effect of the total number of samples that there are genotype per. Number of samples that there are genotype per group. So, if you remember at the beginning, I said that only 12 samples were chosen to be genotype per group, so not all possible samples. Is that enough, or do we need more information in order to be able to reconstruct the infections? And we can see actually that the optimal one is somewhere between 10 and 15, so we're actually quite quite likely. I think even even ten sh should have been okay. Even 10 should have been okay. The 12 was actually randomly selected. So we're quite relieved to see that actually the method is working well. So this is just showing the ROP curves, the false positive rates and the true positive rates. And you can see that everything that is like up on this corner here, it seems that is working very well in identifying, recovering back the unobserved hidden states. And here we just have like. And here we just have like the estimates of the parameters and for each of the different, if we have only five selected types to sorry, only five positive samples to be genotype from five to 20 up to all of them. And what we can see is that here we have like the true value. So the red line is the true value of the parameter that we choose in our simulations. Our simulations. And here are the results that we get from each, when we apply to each of the data sets. And we can see that when we have only five samples, we can see that there is a huge uncertainty. So as we increase the number of samples that we genotype, we can see that we get less and less uncertainty. But even like with 15 samples, it's still okay. It can recover back the parameters. Okay, so I think that's all that I wanted to mention today. So I have shown you how we can use the IFFBS algorithm to exploit the dependent structures in epidemic data and achieve scalable inference. Here, I need to mention also that the IFFBS algorithm is working quite well for the epidemic data, where actually there is more dependency within each individual rather than between individuals. In a general case, Individuals. In a general case, when we have more dependencies between individuals, the R code might not work as well as in this particular case. Also, it can, so it allows much more complex model to be fitted. So we have these EPIPPOMS art packages if you're interested to apply, if you have like a similar data set and can construct the genetic type of every function for surprisingly very few type observations, for example, 12 in our case, is quite expensive. It's quite expensive to type every observation. So, if you have the chance not to type all of them, that's an advantage. And actually, we can use the metropolis hasting to feed a semi-Markov epidemic models. So, as I mentioned, I show you how we can do that in Markov model, but we can extend this methodology by using this method as a metropolis-hasting proposal. And then can also be used for scalable model selection. Used for scalable model selection, so we can use for model selection. And Jay Carson and Simon Spencer from Warwick have like a paper of how we can do that, but I think it's not still available, but it's coming soon. Okay, so thank you, everyone. I think I will stop here. So yeah, I'm happy to receive any questions. Thank you. Oh, sorry, I didn't notice that my, I'm really sorry, I didn't notice that my video was off. Sorry, I didn't want to interrupt you. I noticed you probably didn't read it. Any comment, reactions? Any onliners? Actually, time for one quick comment. We are running a bit late. This was like an impressive case study of how you can push the boundaries of computational base. Glad to see like FFP aspect now hidden in this big model. There's no comment, next talk. Okay, thank you. So thank you so much. Thank you. Let's thank again.