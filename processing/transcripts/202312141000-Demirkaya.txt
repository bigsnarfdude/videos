Today I will indeed present joint work with Lan and Ying Jing Pen, Jing Chili, Patrick Bosner and Jing Povank. Let me start. This is the kind of the overview of the talk. First, I will make the introduction and I will directly dive into the R method PDMN and then I will discuss. And then I will discuss the theory behind it and finish with the simulation studies. First of all, let me lay down the general setting. So we are interested in a non-parametric regression model. So we have a output y and we have inputs x that are randomly generated. And the link between them is given by the function u and we have the random errors. We assume that observations are. We assume that observations are IIT, and since this is our method is about nearest neighbor methods, it is good to have some sort of notation here. So we have a new test point X. So we order our sample, reorder our sample based on inputs distance to the new test point. So the closest. New test point. So the closest point in our sample to the new test point is called x1, then the second closest to the test point is x2, etc. And I will, of course, order the output y values accordingly. As I mentioned, this is a non-parametric regression model. So what does this mean? So we can kind of divide the learning algorithms into two broad categories. Broad categories: one of these model-based learning when we have a relation between x and y, we can assume certain structure about the link function, for instance, a linear regression. Then we just need to learn the parameters. In the non-parametric regression or in instant-based learning, we don't assume any new relation, and all we have is a new X point. And based on our sample, we can And based on our sample, we can decide, looking at maybe the closest points, we can decide or we can estimate y-hat directly without any underlying structure. So what is our method? So what's our title? At the core, we have a nearest neighbors method, and we look at some version of a nearest neighbor method, which is which we call distributional. Which we call distributional nearest neighbor, which was already in the literature. What we do is we combine two of the two distributional nearest neighbors, we call two-scale DNN. So this is the T-BNN method, and we studied its theoretical behaviors, and we come up with an optimum non-parametric inference for our method TDNN. And let me quickly describe what is TDNNN. Quickly describe what PDNN does. What is this method? So, first of all, we started the KNM method. In the KNM method, what we do, we look at the X values and we have a new test point, which is this red dot. In the KNM with K equals 3, we look at three closest points, then we take simply the average. This method is one of the most widely used nearest neighbor method. One trivial question or one, if you look at this form, you directly question why do we choose just these three points? Why don't we choose all of them? Or in other words, if you look at this, this is a kind of the weighted summation of all the Of all the y's. So 1 over k is kind of an arbitrary choice. As long, we can generalize directly by replacing 1 over k's with kind of arbitrary rates as long as they sum up to one. But once we do that, there is one immediate question. So, how do we choose rates? One weight. Way to answer this is to use the begging idea or begging technique and apply it on the one apply it to the one nearest neighbors. So we have one nearest, we sub-sample the data over and over again, and we look at one nearest neighbor methods and we average them out. So in this form expression, it doesn't look like it doesn't look like It doesn't look like a weighted nearest neighbor map because we are just looking at the first nearest neighbor on the subsample. But if we do some sort of combinatoric analysis, if we recount how many times the fourth nearest neighbor, overall fourth nearest neighbor appears as the first nearest neighbor in the sub-sample, we can reorder or we can recalculate. It's kind of a simple one calculation. Communatorics 101 exercise to get to this form. So this form essentially tells us what are the weights as long as S is given, we can easily compute these coefficients and create this DNM method. And this is already in the literature and it has nice behaviors. And let me reiterate what how does DNM Reiterate what how does DNN work in using fading idea? So we have again our sample and new test point x, and we sub-sample and choose the blue points. And now we apply the one nearest neighbors in this blue point. So we look at the distances, choose the closest one, and look at its y value. We do it for all possible sub-samples and average them. And average them. As I mentioned, PNN is a nice tool, already a nice method. It has an optimal convergence rate under Lipschitz-Continent assumptions. However, if it is under smoother assumption condition, the DNM fails to achieve the non-parametric optimum rate of convergence. Optimal rate of convergence. And the reason behind that, DNN has a bias, and this causes the non-optimolerate. In this study, we study the expansion of the bias and we have a way to cancel the leading order term. And the idea is that the The leading order term, there is a constant which is independent of sub-sampling scale, as we will see next. Indeed, if we look at the DNN estimator's expected value, if we have our goal, which is mu of x, we are trying to estimate this, but it has a bias. The next two terms are the bias. If the leading order term is c times s to the power. C times s to the power minus two over d. C is unknown to us, but we also know that c is independent of s. This constant doesn't contain any dependent information or any form of s inside. So we will use this fact to construct the NN estimate, our estimate, to scale the NN estimator. How do we do that? We choose two. How do we do that? We choose two different sub-sample scales and then we want to weigh them to get the TDNN estimator. So we still want to estimate mu of x, so we want the weights to sum up to one. The next thing that we want to do is cancel these two terms, the c times s1 to minus 2 over d and c times s2 to minus. times S2 to minus 2 over D. And how can I do that if W1 times this and W2 times that sums up to 0, then the second term will be killed, essentially. And if we look at these two equations equations in terms of W's, they are just two linear equations with two unknowns. Again, as introductory course in mathematics, Introductory course in mathematics: linear algebra 101 tells us how to get those weights. We can easily compute those weights, and finally, we can create our TBNN estimator by combining these two different scale TNN estimators. And if we take the expectation, the second we are we will be estimating mu of x, but the leading order term in this expansion. Term in this expansion will be will no longer be there. So I will describe next our theoretical findings and this is kind of a sneak peek of the results, the spoiler. So first, we have the TDNN as an easy computation thanks to different ways of representing it. We have a reduced pi SI. We have a reduced bias. I already mentioned it a little bit. And because of that, we have an optimal convergence rate. And finally, we study DNN's behavior asymptotically and we can see that it's asymptotically normal. And finally, we can estimate its variance. Last two items helps us or makes the practitioner to use this as an inference tool. What is What is the theory? So let me dive into the theoretical properties of PDNN and PNN and tell you the theory behind this. First of all, I want to go back to the definition and to representation. This is how we define the BNN estimator. So it is the again Sub-sampled one nearest neighbor methods combination of sub-sampled one nearest neighbors. So, this has known this is known as the use statistics representation of the DNN estimator. And this is nice because use statistics is well studied in literature, in statistical literature, and there are many theoretical properties of use statistics. Use statistics, and that is what we use to make our ground our theoretical work. However, this representation uses enumerates all the subsamples and computationally when n and s is large, these are too many different operations. So this is computationally expensive. But there is a silver lining. So as I mentioned before. As I mentioned before, there is another representation of the same statistics. Basically, by recounting, we can form the same estimator in this form. And in this form, it doesn't use any sub-samples. Once you have the sub-sampling scales, the weights are given automatically by this combinatorial factors, and it is very easy to compute. So we have To compute, so we have directly we directly assign weights to observation, and it's very easy to compute. So, to sum up, we have two different representation. One is good for theoretical analysis, the other is good for computational purposes. Next, I will go to the biaster and let me give you Let me give you some idea of the assumptions we have. We assume that x is randomly distributed with a fixed density function. We assume that x is kind of radially the distribution of x kind of is radially sub-exponential kind of Distribution. It doesn't have high chaos. We assume that the density of f is bound away from zero and infinity and we have some smoothness conditions on both f and the link function. And finally, we assume that the sample is IID. Under these assumptions, we can show that We can show that the expected value of the NN estimator is times Ps. This is kind of the first expression I have showed to you, but now we see the details. Now, indeed, this is the constant that I mentioned before. And it depends on many things. We have the gamma function, we have the underlying dimension of the x, we have the volume of the. The volume of the d-dimensional sphere, and there is the relation between the density of x-variable, and finally, we have the link function appears in this constant. And what I want to point out here is that, so we can once when I show you this expansion, so one immediate question, why don't we cancel this first order term? Cancel this first-order term directly without using two-scale DNN. The reason that we cannot do this is that we don't know this constant. Even though this doesn't depend on s, it depends on some unknown things that we don't know. We don't know the distribution of x variable or the f function. Even if we have some idea about the distribution of the inputs, we definitely do not know. We definitely do not know the link function. The whole purpose of the non-parametric inference is to find out what is. So it is, I mean, very, it would be a very strong assumption to say that we have what mu double prime is or what mu prime is. So we don't know these two terms for sure. So that's why it is not possible to de-bias DNN estimator diagram. Biased DNN estimator directly. I will just want to give you kind of the overview of the proof. So to prove this, the idea is to look at the expected value of y given x, but project it on the kind of the post path line given by x minus x. Here, the small x is the new. Small x is the new test point. Capital X is the arbitrary point in the distributed by F. And we define MR function. So this is the projection. And then we observe, at the end, we will expand MR and we observe that M0 is expected value of X or Y given X. mu y x or y given x minus x is x or so m zero is mu x so if we expand mr we can indeed have a form for expected value of the n and x matrix and in in this expansion of course we will have the the powers of x minus x so we also need to extend we have also need to We have also need a form of this expected volume. But by combining these portions and with those extensions, we can drive the DNN estimator. There's expected volume or the bias. And after we have this bias term, or the next thing that we studied is the normality of. Studied is the normality of the DNN estimator and the t-DNN estimator. We use the Hofin canonical decompression thanks to use statistics representation of the DNN estimator to show that the DNN estimator is asymptotically normal under certain conditions when sub-sampling scales diverge with a proper rate. The main challenge in this proof is that diverging sub-sampling scales, so when we Sub-sampling scale. So, when we have the use statistics with a fixed S, Hafting canonical decomposition, using Haftin decomposition, we can always show that use statistics is asymptotical normal, but challenge in this one is that S also diverge and we need to come up with different techniques to solve that issue. And next, we look at the normal. Look at the asymptotic normality of the two-scale DNN estimator, which is not, which is unfortunately not a direct consequence of the previous theorem because two-scale DNN estimator uses the same, I mean, even though we are combining two different estimators, they are not independent. So we need to redo the proof for the two-scale DNN estimator. Scale the NN estimator, even though they rely on the similar techniques. But once we have these two theorems, now we know how to do inference with this, because now we know that our estimator is asymptotical normal. The last ingredient to make this inference is the sigma n. So if we know sigma n, then we are done, we can. done we can create confidence intervals for for our s for for the our estimator using our estimator and sigma n is unknown and but we can estimate sigma n using jackknife estimator what we need to do is we need to rewrite our dnn estimator one more time so after we re-express our Re-express our DNN estimator, TDNN estimator, in this form with this kernel function. Then we create Liu1 out like estimators using the same kernel and UI, and then combining them, we can create the Jacknaf estimator for sigma F. For sigma n. And once we have the Jackknife estimator for the sigma n, we can show that it is consistent in this sense, sigma n square of sigma, our estimator divided by the true sigma n converges to one in probable. And hence we have now used using JackNap estimator for the variance. For the variance, and using the asymptotic normality, we can in practice create confidence intervals. Finally, I want to finish with our simulation studies. In the simulation studies, we choose one link function. We have a fixed test point, and we vary S and then we have a version. Version for TDNM, we choose the S1 and S2 in this manner. Of course, here we have two for DNM, we have the kind of a hyperparameter S. For T DNM, we have two hyperparameters, and we choose them by tuning using a version of cross-validation. And if we look at If we look at the bias of the estimators, if we compare DNN's bias and the TBNM bias, as subsampling gets higher and higher, both of them are the bias, both of them will be unbiased or approaches to an unbiased estimator. So bias will shrink. What we observe is that bias of TDNN converges to zero less. Our bias of TDNN approaches to zero faster. We see, as we can see, the both x-axis and y-axis, the labels are, even though the shapes are similar, the axis labels are different. And this is understandable. This is aligned with our theory because we showed that TDNN gets rid of the first order bias term of the TDNN estimator. The DNN estimator. And because of that, we also have an improved mean squared error. If we look at the mean squared error, both of the mean squared errors of TBNN and DNN estimators are U-shaped, but minimum of TDNN estimator is 0.11 our estimator. First of all, drops faster than the DNN estimator. Faster than the DNN estimator when the sub-sampling scale is between 5 and 10, it's already dropped, whereas the DNN estimator drops, their minimum is around 100. And moreover, the minimum value is also lower for the TBNN estimator. And we can also see the results in numbers. What we observe is that Is that the bias of the TDNN estimator is significantly smaller than DNN estimator, and the variance of the TDNN estimator is significantly smaller compared to KNN estimator. Therefore, the MSE, I mean, TBNN kind of wins the bias variance trade-off and have the lowest. And have the lowest mean squared error. I guess that's all for my talk. Yeah, thank you for listening and thanks for the invitation again.