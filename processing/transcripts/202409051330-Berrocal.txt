So this session is about applications. Our first speaker, our third speaker is an online speaker, Veronica Verocal from UCI, University of California, Irvine. And she's going to be talking to us about patient-spatial modeling for geophysical science applications. Thank you very much. Whenever you want, the crowd is. Thank you so much for inviting us. Thank you so much for inviting me to give a talk. I am sorry that I cannot be there, but unfortunately, I am trying to get basic statistics in the head of undergrads here at Irvine. So I have this mission to accomplish and I can't leave. So that was part of the reason, but I would have rather be there with you. I also wanted to make a disclaimer that when Peter invited me to give a talk, Me to give a talk at this workshop. He told me, Oh, it would be great if you can talk about different applications of spatial modeling. And so I think I was exaggerating. And I put in the abstract I was going to talk about three applications and then I realized that maybe three is too much. I should go down to two. And now I don't even know if I would be able to talk about the second one fully. Talk about the second one fully. I will probably just do a quick, just give you quick highlights about what the second application is. But what I'm presenting is based on work with two PhD students here at UCI, 111 and Romain Brain, Francesco Denti, whom I believe you might know, and then Michela Figerin and Alessandro Guillami, who are at Politecnico di Milano, and they are. Tecnico Di Midano, and they are the co-authors on the second project. So, the first project is about soil. For much of my career, I have worked on air pollution modeling, and then I started to feel that I wanted to explore more than air pollution. So, lately, I've been very interested in soil. And I think there is a lot of work to be done in soil, soil dietum. Soil data and soil is actually not as much studied by statisticians lately, but it is very important from a climate perspective. The variable that we are going to be focused on this talk is soil moisture, which is a parameter that is very important and plays the role in many geophysical processes, societal processes, and also hydrological phenomena. Soil moisture. Soil moisture refers to the amount of water that is in the soil pores, and so it is a number that goes from zero to 100. And as I mentioned, it is involving several land surface processes that have important societal implications. So soil moisture is fundamental for crops to grow and flourish. It is also very important in wildfire dynamics if the soil is dry, actually. Is dry, actually, that is conducive to rapid spread of wildfire. It's also very important in hydrological extremes, floods, droughts. So there is many variables that are cascading from soil moisture. In terms of data availability for soil moisture, we don't have many direct measurements of soil moisture, at least not that are publicly available. That are publicly available. Soil moisture is measured in many different ways. The most reliable and probably desirable type of data would be the measurements that are done in situ by probes that basically measure the quantity of water in soil pores at different depths. So here you see this cartoon that shows from 15 up to 90 centimeters. So those are the in-situ problems. And those are, as I said, the most. And those are, as I said, the most high-quality data that we would like to have. But other information about soy motion is available from instruments that are aboard on planes or remote sensing. The remote sensing in particular is typically what is widely used to generate maps of soil moisture. NOAA, for example, has a drought index that is based on remote sensing data. But one thing that is But one thing that is important to note with remote sensing data, and it is, I think, my one of the crusades I'm trying to battle when I am in meetings with other scientists and not just statisticians. It is important to understand that remote sensing data is actually not providing a measurement of the variable that the remote sensing data is solved for. So, for example, remote For. So, for example, remote sensing measurements, quote-unquote, of air pollution, they're not really measures of air pollution concentration because remote sensing does not measure air pollution the same way as remote sensing is not measuring soil moisture. What the instrument on board of satellites do is they basically measure some property of light reflectance, and that light reflectance gets transformed through some algorithm. Through some algorithm or statistical model into an estimate of a variable that is related to soil moisture. And so there is a lot of uncertainty that has to be taken into consideration when we deal with remote sensing estimates of soil moisture. Another problem with remote sensing data, particularly for soil moisture, is that the instrument that is providing an estimate of soil moisture is a board of a satellite that Is aboard of a satellite that is orbiting. And so it's not always taking measurement at the same location that it is moving over time. So here you see basically a snapshot of soil moisture for February 22, 2016, that it was taken down by this. This is the major mission from NASA that provides estimate of soil moisture for the entire world. And as you can see, the instrument gives estimate of soil moisture. The instrument gives an estimate of soil moisture over vertical sweats that basically change over time. So, if we look at the Nagderday, we have three different bands that provide soil moisture over the United States. And typically, in a week, an area in the United States has two overpaths. So, we have two snapshots of what is the soil moisture at a given location over the United States at a given time in a week. Given time in a week. So, this is clearly showing that we don't have greater temporal coverage with remote sensing data on top of the fact that remote sensing data is also highly statistically processed. What you're seeing here is what is called level three soil moisture estimate. So, this is the soil moisture that gets estimated by the statistical algorithm or the Algorithm or the algorithm that is used at one of the NASA lab, probably JPL, that takes the measurements of light reflectance and then through some geophysical relationship transforms it into soil moisture. Very likely the instrument does not see where there is cloud, so there is an additional step of post-processing that is done to cover the empty spots that are covered by cloud. Empty spots that are covered by cloud. So the white spots that you see here are basically regions where probably there are bodies of water, and so the remote sensing cannot give an estimate of soil moisture. So hopefully with this, I have convinced you that we are in a situation where we need more data about soil moisture from in-situ proves. And we would like to basically understand where. Understand where to collect additional measurements of soil moisture. And so, for this, it would be great to understand the spatial dependent structure of soil moisture. It is very well known because soil moisture is affected by many land and ecological variables. The soil moisture is not stationary and is actually highly heterogeneous in space with potential non-stationarity. So, our goal is to develop a model that would allow us to basically Would allow us to basically understand the connection. I think we're having a connection problem. Okay. Sorry? Yeah. Maybe the connection will improve. Okay, so you're not disarming. Okay. Um yeah, it seems that we're having a connection here. Can you hear us? Okay, yeah, well, good. I can hear and also see the room. Okay. So, I don't know what should I do. Okay. Okay, it seems everything has been stable. So, should I continue? So, should I continue? Yes, please. Sorry. Oh, okay. Okay. So, as I said, we would like to understand where we should, we would like to understand and characterize the constitutionality in soil moisture. Oops, we would also would like to know where we should intensify sampling of soil moisture. And also, we would like to potentially generate maps of. Generate maps of daily soil moisture because, as I said, right now, the maps that are generated of soil moisture are mostly at the weekly level and it's based on the remote sensing data. So, to accomplish this goal, we start with the classical spatial statistical models that basically model soil moisture at a location S or some transformation of soil moisture using, as I said, a classical. Using, as I said, a classical spatial statistical model that decomposes y of s in the sum of three terms. There is the mean term Î¼ of s that represents large-scale spatial variation and typically is modeled to depend on some covariates that are assumed to also vary spatially. Then we have WS, which in spatial statistics we call the spatial random effect, and it's basically capturing fine-scale spatial dependence. And then we have a measurement errors term. Measurement error term epsilon s that is modeled to be IID with a mean of zero and a variance of tau square. So, WS for a special statistician is where all the interest is. This is typically modeled to be a Gaussian process with a mean and zero and then a covariance function that typically we take to be parametric and depends on some parameter theta. Many times, because we most of the parametric covariance functions are still. Metric covariance functions are stationary and isotropic. This covariance function CW is taken to be stationary and isotropic, even though you might be in a situation where the special process YS is not stationary, even after you have accounted for potential non-stationarity through the mean functions. And so the statistical literature, the spatial statistical literature has been very much focused on developing. Focused on developing models for neostationary processes for a long time since probably the publication of the deformation paper by Guthorp and Samson in JASAM. But since then, there have been several ideas that have been proposed to account for non-stationarity. And one of those is the idea of imagining that your process is globally non-stationary, but locally stationary, which means that it's made of many. Means that it's made of many local stationary processes. And the question here many times is: where are these regions that have local stationarity? There is not a very easy to implement and clear way to determine these regions of local stationarity. Grammar C and Li use basically a three Gaussian process idea and generate partition of the Generate partition of the latitude and longitude to come up with these regions of local stationality, but those are probably not realistic. And any other method that has been proposed to identify these regions of local stationarity has never really had a major goal to identify the regions of local stationarity. And if not, they have used methods that computationally are pretty cumbersome. So we would like to. So, we would like to identify regions of local stationary, if you exist, through an approach that is computationally more feasible. And so, for this, we took an idea that was proposed in the literature to solve a completely different problem, which was the problem of computational burden when you're trying to perform inference on a spatial process. And we revisited that. And we revisited that idea to serve our purpose. So, in 2008, Balar G. Edol proposed the predictive process, as I said, as a solution to solve this problem of handling computation when you have great larger and larger spatial data sets. And the idea basically consists in two introducing a set of nodes R, which in number is much less than the Number is much less than the number of locations with observation and project the original process WS on the space spanned by the process Ws at the notch location. And so if we call Q the set of notes location, we're basically approximating Ws with the conditional expectation of Ws given W at the notch location. This W tilde S is what we call the predictive process approximation, and it can be And it can be shown here in figure. Basically, this is what the idea is. You have this spatial domain with the black locations. If this number of location is quite large, performing likelihood evaluation or doing fitting any sort of MCMC algorithm becomes computationally very cumbersome. And so the idea is to introduce a set of knots, these are the red dots here. Red dots here. And instead of dealing with the n-dimensional vector W at the gray dots, you're basically dealing with the projection of that vector onto the R-dimensional space of the WS and the NOT location. And that simplifies and alleviates computation quite a bit. It is also easy to show that this predictive process approximation. Process approximation WS is actually expressible as a basis function expansion where the basis functions have a specific definition and the vector of basis function weights eta has a specific distribution. And so this W tilde S is basically a non-station approximation of the original process WS. So this idea of predictive process was very successful. Process was very successful, it was used very widely. But what people notice is that the predictive process approximation actually does not capture a small-scale spatial correlation. And so a remedy to that would be to look at the discrepancy between the original process and the predictive process approximation, call this a remainder term, and then do a second predictive process approximation that we can call. Process approximation that we can call tau1s. And you can repeat this over and over. And if you do this, you're basically decomposing your original process Ws into a sum of orthogonal processes, tau 0s, tau1s, tau, tau ms, that are all predicted process approximation of the various remainder obtaining at the previous stage. And then there is the remainder term delta ms at the mth level. So, this decomposition is always existing, and that this is basically the equivalent of the Karunan-Love expansion of the process WS. Katskus in 2017 basically found a way to use this decomposition to obtain a better approximation of the original process WS that is also computational advantageous because basically he Because basically he selected the knots and made some additional assumptions that made things go much faster. And so he introduced what is called the MRA, the multi-resolution approximation idea, which I'm going to explain through figures. So basically, we have our spatial process, our spatial domain. We introduce a set of knots and we approximate the original process in the spatial domain with the predicted process approximation that is the Predictive process approximation that is defined by these 16 knots, and we call that tau 0s. Then, at level one, we're basically partitioning the domain in four sub-region. We are going to calculate the difference between the original process and the predictive process approximation, imposing the additional assumption that we have independence across those four regions. And then we repeat the predictive process approximation. We introduce. Approximation. We introduce the same number of nodes in each of the sub-regions and then we perform a predictive process approximation in each region separately. And this leads to our new process tau1s. And you can imagine that we can continue doing this. So we now partition again each region in additional JSON region. We introduce new sets of knots. We calculate the difference between the remainder. The remainder and the predictive process approximation are summing independence and so forth. So, if we do these many, many, many times, at the end, basically, we have what is called an M-level approximation of the original process that we indicate with WMS, which can be expressed as a basis function expansion with basis function coefficients and basis function weights that are now indexed by the. Now indexed by the resolution level and the region where we're doing the approximation, and then we have a remainder term. So, if you're trying to think about what this multi-resolution approximation does, you can basically think about, you know, figuratively, when I was trying to explain this to my student, it's basically the same as you are passing. As you are passing some water through a sieve, looking for nuggets, and these nuggets for us would be basically the spatial correlation. So you first go with wider, with a wider net, with a with a coarser grid, and this will basically capture the large scale, the larger scale spatial dependence. That's what we are capturing with the first predictive process approximation. We then Approximation. We then pass the remainder to the second set of sieve, and we are trying to capture whatever spatial dependence at lower scale is left that we didn't capture with the previous set of sieve and so forth. And so you can think about that at the end, when you repeat this process over and over, what you are left is basically just very tiny nuggets. And so basically, what is left, this remainder term, can almost. This remainder term can almost be considered to be independent because we have removed all the spatial correlation through this successive filtering. I hope that made it a little bit more clear. And so this understanding was actually very important for us to move forward and extend this multi-resolution approximation for our goal. So just before I move forward, let me again give you some additional insight about what you Insight about what this multi-resolution approximation does. As I said, it basically expresses a special process as this combination of basis function and basis function weights. The basis function basically depends on what was the original covariance function that you put for the special process WS. So, this can also be a stationary covariance function. It doesn't matter for us, for our goals of finding regions of local stationary functions. Final regions of local stationarity. The basis function weights for a multivariate normal distribution with a covariance matrix that is defined as k and j and it depends on again the original covariance function that you have placed on the original process. But what we notice is that how many levels you need to go deep in this multi-resolution approximation is really linked to the strength of the spatial dependence. Strength of the spatial dependence, or the scale, this is how typically scientists like to talk about the scale, the range at which the spatial dependence persists. And so we decided to use that to our advantage and basically do inference on the basis function weights and look at these basis function weights and see when they are basically almost zero versus where they're not. Versus where they're not. And instead of just looking at when they're almost close to zero, we decided to shrink them to zero. When they're shrunk to zero, this will tell us that those are the regions where the spatial correlation dies off slowly. And so we don't need to add additional levels to the multi-resolution approximation because the spatial scale operates on longer scale and it's already captured by the first low level of the multi-resolution approximation. Approximation. Instead, in regions where the spatial correlation operates at verified spatial distances, we need to add more levels and so we don't shrink the basis function weights to zero. And so we basically change the MRA and introduce what is called the mixture MRA, where instead of specifying this multivariate normal prior on the basis function, On the basis function weights, we change it to a spike and slab prior, where this spike was defined to be, instead of being a point mass, it was defined to be the K and J divided by a constant N, so that we could retain the correlation among the R element of this vector. And these basis function, these basis function weights are such that as we go low, as we increase less. Low as we increase levels in the MRA, we want to be able to shrink them much more aggressively. And so we accomplish that by basically saying that this probability of shrinkage pi of m increases as m grows. And we express it as 1 minus rho to the m, where rho was provided with a beta prior. And so that was our mixture and ray. We actually were able, we used this model. We used this model to on simulation data and on actual data, and we were able to identify different regions with different levels of spatial dependence. But there was something that we didn't like about, oh, I went backwards, that we didn't like about this model. It was, first of all, to apply this model, you have to determine the number of levels a priori. And then also, there was basically this probability of shrinkage. probability of shrinkage was forced to follow a specific law and we want didn't want that we just wanted the probability of shrinkage to grow with level but we didn't want to grow it geometrically and so we changed the prior and the basis function weights using the cumulative shrinkage process prior which i think for this audience probably i don't need to explain very much in details um so if you have an order sequence of parameters and for which you Sequence of parameters and for which you want increasing shrinkage, you can basically accomplish that by defining this probability of shrinkage using the cumulative sum of weights that are constructed using the stick baking process construction. And so that's basically how much I'm going to say on this. Also, I'm realizing I'm running out of time. So here is just a plot that shows you the difference behavior for the shrintage probabilities. This is a plot. Shrinkage probabilities. This is a plot that Francesco created and I stone. So you see the different behavior of the shrinkage profiles for our mixture MRA versus the cusp. So you see much more flexibility in the profiles that we get with our cusp specification that if we use our model. In the cusp, there is a parameter alpha that controls the shrinkage. And so basically, we use the cusp and connected with the Cuspen connected with the MRA. And we said that a process WS follows a cusp MRA prior if it can be expressed as this infinite sum of basis function and basis function weights, where the basis function are constructed using the MRA construction. And so they're determined based on a certain covariance function and a covariance function parameter theta. And the basis function weights are provided with this. Provided with this spike and slab prior where the shrinkage probabilities are defined as in the cusp. And so, this is what we call the cusp MRA model. So, as I said, this cusp-married depends on many things, the covariance function that we choose for the original process WS, the covariance function parameter, the number of regions in which we divide the spatial domain, the number of knots that we introduce in each region, this constant L that is for the shrink. This constant L that is for the shrinkage, and this alpha that is the global shrinkage parameter. And if you were to choose alpha as a practitioner, how would you choose alpha? So we have provided some indication to people about how to choose alpha by defining what we call the local activation level. So the number that we of a multi-resolution approximation that we Multi-resolution approximation that we expect is needed at location S, and so we can derive what is the local activation level a priori that depends on alpha if it is known. And if an alpha is not known, we can provide a prior on alpha, probably a gamma prior following Poval and Canalen's recent paper. And we can determine what is the expected value of the local activation level and also the variance of this local activation level. Local activation level. So, here is the realization of processes from this prior. And so, you see these three panels show basically realization of a process that has a cusping array where the original covariance function is exponential, where we have using four regions, 16 nodes, L is 6 set to 15, and we have different values for the global shape parameter, 0.1. Single parameter 0.1 and 10. So for 0.1, this is basically a process for which we don't expect a priority to have many needs for many level of the MRA. In fact, we see a process that is that vary very smoothly in space. When alpha gets larger, we have a process that has many more local features. And this is the comparison is basically what you would get if you were generating a radiation from the M. Generating irradiation from the MRA with the same setting as ours, but with three levels of MRA resolutions. So does our model work to identify if a process is stationary and if it's not stationary, does it work to understand where the regions of local stationarity are? So we generated simulation. So here we have a simulation of the spatial process on a unit squared domain. unit square domain and this was a process that we generated with an exponential covariance function where the marginal variance was set to be one and the range parameter so this is telling you how fast the spatial correlation decays is set to one and so what you see here for this particular simulation realization we basically estimate a pretty constant number of multi-resolution Number of multi-resolution levels everywhere. So, this is telling you that because this is done very spatially, that we're probably dealing with a stationary process. And the same with the next set of simulation, where the only thing we change is the extent of the magnitude of the range parameters. So, what you see here is that we have a process that is less, that has many more local features, but is still stationary. And so, our model understands that, and it just says that we need. And it just says that we need more levels to be able to capture the local features. What happens if we still have a process that is non-stationary? So here we did a simulation of a process that is non-stationary. It is made of two stationary process. One that operates outside of this amethyster. This is the mascot for UCI. And then there is another process that is operating inside. Process that is operating inside the antieter, and so we wanted to see whether our model was able to identify that there are two regions and basically was also able to identify this irregular boundary. And what you can see here is that, in fact, our model does understand that, that we have two different levels of spatial correlation operating inside and outside, and we correctly see that we need more levels inside the anterior rather than outside. Rather than outside. So that was pretty, we repeated this for multipolarization. So that was pretty reassuring. And now let's look to the application that I mentioned in the beginning, soil moisture. So we looked, we took data of soil moisture from In Situ Probes for seven days in May 2016. We also had data from the remote sensing. This is nine kilometers by nine kilometer. Is nine kilometer by nine kilometer grids. As I mentioned, the data is not for the remote sensor, it is not available on the daily level. So we basically average it for the entire week. And so that's what this X bar bar T is. And then we also add information on precipitation at the same nine kilometer grid resolution. We fit our model that we indicated at the beginning, the classical spatial regression model, where we specify the cousper merit radio for our The custom rate radio for our WTS. We fit this model independently for each day. And then this is basically an illustration. This is just to show you the data. So the remote sensing data and the observational data. So you see that the remote sensing data is actually not that well calibrated. So for example, you can see in California, there is the land is much drier than water. Is much drier than what remote sensing is saying, and vice versa in other locations. Here you have the rainfall amount for May 1st. We have visible data for every day. These are our daily prediction of soil moisture for two days of the week, May 1st and May 2nd, and with the corresponding uncertainty quantification. But maybe what we're most interested in is identifying. We're most interested in identifying these different regions. So, for the seven days that we ran our model, we actually obtained that the W surface basically had the same activation level across the space for four of the seven days. However, for May 1st, May 2nd, and there is another day, I think May 7th, we have a different behavior. So, what this is telling us is that. So, what this is telling us is basically telling us that precipitation and remote sensing data do not explain all the spatial correlation that is in soil moisture, and that the residuals are basically non-stationary. And what we see is that there is a residual that is much more that is a little larger with a longer range of special dependence in the Pacific Northwest, but definitely the residual. But definitely, the residuals in the Midwest have either a larger special variability or a shorter range of special dependence. So, if we were to indicate a region where we would suggest that more intensive measurements of soil moisture are taken, we would say that based on this, we would concentrate in the Midwest. Basically, Midwest here, I also consider includes the Northeast. Northeast. And I think, do I have time to quickly say what this air pollution project is, or maybe better not? You have about 10 minutes, including. Oh, okay. Okay, so this is basically talking about. So, in conclusion, we have presented this method that we think has a lot of application because it can be used for monitoring to determine which region will need increased monitoring. Need increased monitoring. It also is very important for other purposes that I'm not going to elaborate here, but basically, understanding what's the level of spatial dependence, the strength of the spatial dependence, and how it changes over space can have many applications. We would like to extend this to space-time processing and maybe to multivariate spatial processes and think about more flexible ways to do this shrinkage. So, the other problem is sort of related to air pollution. Air pollution. So I am going to skip this. We all know air pollution is bad for us. And we are going to focus on PM 2.5. So PM 2.5 is the small particles of pollution that are in the air. But PM2.5 is made of many things. It's made of sulfates, nitrates, dust, pollen. So when we talk about PM2.5, there is a lot of variability in space about Of variability in space about what is the composition of PM2.5, and this is understanding what is the composition of PM2.5 is very important because different components of PM2.5 have been found to have different effects on morbidity and mortality. And different components of PM2.5 are mostly generated by different sources. So, what we would like to do here is we would like to understand what Is we would like to understand what sources contribute to the different components of PM2.5 so that in the future, if we know that sulfates, for example, are very bad for a specific health outcome endpoint, we focus our efforts into reducing the sources that generate sulfate pollution rather than just doing a general control of air pollution. We can do more targeted intervention. Targeted intervention. And so, what we would like to know is we would like to know what sources contribute to what components of PM2M5 and by how much. And this is the project of source apportionment. That is a problem that actually does not only limit itself to a pollution. And so, basically, if what is the classical formulation of the source apportionment problem is the following. We have an observed matrix Y of multiple pollutants. Of multiple pollutants over multiple time points. And we want to write this Y as the product of two matrices. A matrix G that basically is the matrix that tells us the amount of pollution generated by a number K of sources over the time interval. And basically, this matrix H, that is the matrix that tells us how much a source contributes to each pollutant. When this This mathematical formulation was developed. It was developed under the idea of mass balance, which means that no pollution disappears. Okay, so the pollution that gets generated by a source is just there. It doesn't disappear. And that's very important. It's a very important assumption. So basically, we can rewrite this equation in this way. And there are a couple of Couple of um of course, this problem is quite hard because identifying both G and H is basically impossible, and so we need to impose certain constraints besides the mass balance. And one constraint is that all the elements of GNH need to be positive. Of course, we are referring to pollution and to contribution. And then, if we consider a given pollutants, basically, we can say that if we if we fix k. And if we fix K, then we're saying that the sum of the contribution of the K sources to the CF pollutant has to be less or equal than 100%. And then once we look at this contribution, we can actually identify the actual pollution source. So typically people have solved this problem either assuming the number of sources is known, or if the number of sources was assumed unknown, typically people have looked at one. People have looked at one pollutant at a time collected over multiple years or multiple days, or looking at multiple pollutants, but one location at a time. So, basically, there has never been any previous work that has modeled data collected in space and time for multiple pollutants and take into account both the spatial and temporal dependence. So, that's what we try to do this. And additionally, we also want. We also want to keep the number of pollution sources unknown. So, for people that do Bayesian parametric, this seems to be the perfect problem to work on. So, basically, we work with the concentration on the log scale, and we then rewrote this log concentration for the seed pollutant allocation I as this linear combination of. Linear combination of different sources that pollution vary by location and in time. And the contribution of each sources to pollutant, however, is assumed to be constant in space and time. And we want this number K star to be basically also inferred from our model. And so the GIKT are this local source profile. Local source profile and the HKC are the contribution of each source to a pollutant. And so we want this local source profile to be spatially correlated because we assume that locations that are nearby are going to have similar concentration generated by a source. And we also want to have the impact of the characteristic of the location to play a role in this GAIK team. But of course, we have so many, so many Many, so many functions to estimate, so we need to try to borrow information across them. And so, basically, we are assuming that each of these local source profiles are written as a global source profile, so the pollution emitted by source K, that is modulated by this quantity gamma K that depends on the characteristic of the location. And then we include this special random effect to induce correlation among the different locations. Among the different local source profiles. And then we are going to, I'm going to go very fast here, we're going to write each of these FKT as using QB splines with coefficient lambda Km. And then you can imagine that we are going to use the multiple DV gamma process Schimpech Praio, Bata Sharaya, and Danson to basically try to estimate the number. Estimate the number of components that we need. And I think this is pretty much, I can skip the prior specification and show you that we apply this model to data from six different components of PM2.5 that was observed in California during year 2021. We had data from 32 monitoring stations, and we actually were able to identify four different sources. Four different source profiles that we're now trying to basically name based on the way these different sources contribute to the six different pollutants and also looking at the spatial distribution of these locations. So, I think I'm just gonna go to the end. So, basically, this is just a This is just a first attempt that we have developed to try to answer this question of identifying what sources are creating pollution and how much these different sources contribute to the pollution to the pollution. We had to make some simplified assumptions to be able to perform estimation, but now we're actually working to extend this model to Extend this model to remove some of these assumptions. So, for example, we don't want this idea of having a global source that is the same for all the locations, but it's just modulated by this gamma k-time. So, we want to add some warping. We want to also make sure that the source contribution is not the same across locations. We would like also to do some clustering of the location using BMP factor models. And that's it. And that's it. Thank you very much. Right, so I think we have time for maybe one question. Anybody has a question? Okay, so I have one in can you hear me? Yeah, I can hear you. Okay. So can you do this simulation study that you're gonna Simulation study that you got simulated like stationary process for the soil? Yes. Okay. So I'm wondering if you try to simulate, let's say, other stationary or non-stationary processes, like different types for the simulated data set. For the soil moisture or for in general for the simulation? For the simulations. For the simulations. Yeah, for the simulation, we looked at different things. So we looked at stationary processes where we change the level of spatial variability. So is it sigma square one versus sigma square ten? Does it make a difference in terms of the estimation of the activation level? And we did actually see that the more variable your process is. Variable your process is, the more levels you need. So that's one thing that we found. So it's not just how many levels in the multi-resolution approximation you need, that's not only depends on the scale of the spatial correlation, but it also depends on the variance, which is something that we didn't know when we were doing the mixture MRA. And then we did for a situation where we kept this variance fixed. Then we change the length of the range. I'm hearing a lot of noise now. Okay, noise disappeared. So we basically played with this. So we played either with sigma square and phi or both. And we saw that this affects the number. That this affects the number of levels, but not whether you're gonna get a known spatially varying activation number. But we do see that instead when we have processes that are made of regions with different characteristics of the spatial correlation. So, I don't know if that was the question you were asking. Yeah, yeah, yeah, perfect. Thank you. Okay. Thank you. If there are no more questions for the interest of time, let's thank Veronica again.