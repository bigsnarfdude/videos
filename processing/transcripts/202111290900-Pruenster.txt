In presence, and that's really nice. Also, nice to be back in Oaxaca. And so, today I want to talk about non-parametric variables within the context of partial exchangeability and look a little bit at the dependent structure and borrowing of information. So, mostly on dependent structure in this talk. Talk and also nice to talk at a workshop on foundations of objective Bayesian methodology. And I still remember ages ago, I was at an objective base conference in Rome, and I was discussing of Mike Jordan. And I remember he started his talk saying, I don't really exactly know what objective base is, but the closest thing I can think of objective base is non-parametric. So I stick to his point of view, and I also see. A point of view, and I also see non-parametrics as, in a certain sense, to be objective. But I think I've been trying a lot since many years is actually since in based on parametrics you have to deal a lot with infinite-dimensional objects, it's important to understand what implications they have. And understanding of the implications is, in a certain sense, I would consider them also objective, that you understand what your model is doing, why it's doing, or what it's doing. Doing why it's doing what it's doing, and so on and so forth. Uh, so quickly, a bit of uh, I need to go through first to set a little bit the notation and the things I want to talk about. Obviously, go to the exchangeability part very quickly. Ah, so that comes with the hand. It's nice. Exchangeability introduced prediction, which is natural in a Bayesian framework. Then I introduced the discrete non-programmatic. Then I introduced the discrete non-parametric bias and species sampling models. So, the species sampling models for the exchangeable framework, which go back to the famous paper of Pittman of 96, that have a bit of a difficult relationship with species sampling models, and I'll later explain you why. And then I move on to the dependent context of partial exchangeability, and I introduce in particular the most of the talk will be about multivariate species sampling models, and then I will also try. Models and then I will also try to go a little bit beyond. Okay, and in doing this, I try to answer some open questions. At least for me, they were open questions, were questions I was interested in since many years. And I mean, with this kind of work, I'm able to, we are able actually to answer some of them. Okay, I don't know whether they're interesting for all of you, but for me, they were a lot. And I forgot this is actually joint work. I mean, different parts of a joint work with different people, but mostly with Philippa. People, but mostly with Filippo Ascolani, Bertrica Franzolini is going to talk right after my talk. Then it's always Antonio, as usual, and then it's Giovanni Rebaldo, who is here. And actually, some comments of Giovanni actually sport the work on multivariate species sampling models. And I'll explain you why. So that's more or less the plan. Will be just a purely theoretical talk, no plots, no data, no nothing. No plots, no data, no nothing. Okay, but I think I have a story to tell. I don't know whether interesting or not interesting, but I have something to say which I at least find interesting. Good. So, as usual, exchangeability, no, I don't need to repeat this here. Invariance with respect to permutation of the observables or latent variable, whatever they are. Okay, so the order of the observation does. So, the order of the observation doesn't matter. And so, that's if you want, it's a symmetry, you know, it's symmetry, an analogy between past and future, and so on. Because, at the end of the day, I mean, I'm pretty convinced of that, that statistical inference is nothing else than some sort of symmetries. No, you have to find some symmetries in the exchangeable setup. It's straightforward because you have the symmetry directly on your data. If you have a more complex model, then it's all about kind of splitting up your model in a smart. Model in a smart way, and then find the symmetries within blocks of the model, and that's the way you learn. No, it's that's an analogy between past and future, can be a spatial analogy, whatever, but you need to find symmetries somewhere in your model, split it up in a way that you can make use of symmetries and symmetries. That's what allows you to learn. Good. That's exchangeability. Why it's important? It's also important because you have the infinite representation theorem which links you an exchangeable sequence. So the law of an infinite. Exchangeable sequence. So, the law of an infinite exchangeable sequence, when I speak about exchangeability, I always mean infinite exchangeable, links it to a non-parametric prior q, which is the law of a random probability measure. Okay, in the baseline parametrics context, the random probability measure is not indexed by a finite dimensional parameter, so it's really an infinite-dimensional object. I like a lot always the integral representation because it shows you nicely that the definitive measure lives on the space of all probability measures, but more standard way of writing it. Standard way of writing it is in this hierarchical form. Good, then a thing which kind of links a little bit that the Italian community, if you want, with the machine learning community, the emphasis on prediction, which really goes back to definetti. Here I always like to quote Definetti in a way or another. That's one I like a lot: that science cannot limit itself to theorize, but accomplish facts, what must foresee. Okay, and so prediction. And so, prediction, well, we all know what the one step ahead prediction is, okay, which at the end of today is nothing else than the posterior expected value. And I always try to emphasize this one as well, which is the M step ahead prediction, which is really important. So, if you want to understand the finite sample properties, if you have a non-parametric model, that's a good quantity, okay? Because there you really mix the data with the model properties. If you just look at the one step-ahead prediction, they tend to look all a little bit similar. Prediction, they tend to look all a little bit similar, but if you go M steps ahead, and the characteristics of your model come out, and you really see how the data combine with your model. Okay, so M step ahead prediction, which is actually, if you speak about prediction, that you should think about this one, okay, not just the one step ahead. Good, so that's the really basic stuff. And then, based on parametrics, I mean, if you take Gaussian processes away, it's mostly to deal with discrete non-parametric bios, which is not. Non-parametric bios, which is nice nowadays, that also in maths, there's a lot of study about discrete structure. So, disk structure is really popular, and so based on parametrics fits in very well and has nice connections with a lot of different branches in maths. So, a discrete non-parametric prior is nothing else than a definitive measure, so a non-parametric prior who selects almost surely discrete distributions. And then you can represent always a discrete distribution in this way, you know, as a series of probability weights. Of probability weights at random locations. And obviously, since it's discrete, then with positive probability, your vector of observations will exhibit ties. And I denote those with x tas from 1 to kn because you will have a random number of distinct values in the ton. That will see then the corresponding frequencies, which sum up to n. A little bit more formally, what you can say is that your vector of observation induces Vector of observation induces a random partition, okay. A random partition of the integers from one to n. And basically, two elements belong to the same set if the corresponding observation values coincide. And if you look at the distribution of the partition, you end up with this exchangeable partition probability function, which is nowadays kind of a standard and based on parametrics. If you want, you can see it as the probability of observing a specific sample. A specific sample featuring k distinct values, the sample is of size n with corresponding frequencies and one up to n k. So you essentially forget about the labels. Okay, that's what from a statistical point of view, the PPF tells you. And it's nice because the PPF completely characterizes as n diverges the random partition, the infinite exchangeable random partition. And also very nicely, which for us, it's very natural that we tend to construct an exchange. We tend to construct an exchangeable sequence and look at the induced random partition, which is then automatically exchangeable with respect to probabilists, which typically try to define the random partition and then go and check that it satisfies the condition for exchangeability. And fortunately, all exchangeable random partitions can be generated by an exchangeable sequence. So the statistical point of view of looking at those things is actually natural and good. Okay, so it's nice. We can always look at the induced random partition by our. At the induced random partition by our exchangeable sequence of observations or whatever. And so that's a key concept, not APPF, which is the exchangeable partition probability function, which everybody's used by now. Okay. Good. Let me look from a predictive point of view a little bit at the Dirichlet process. We all know this stuff. But it's a nice way of introducing it. I also to students, I often introduce it this way. Okay, you have an exchangeable sequence, you have a prior guest at the low, which is your P note. The law, which is your p-node, then you have the strength of your prior belief, which you measure by theta. Okay, and then you say, I want to predict. And how do I want to predict? Well, a natural way of predicting is using a linear combination between your prior guess, the empirical measure, and give weights to those. Weights to those, which depend then on the sample size n, on your strength of your prior belief theta. And you have that linear combination. It turns out that it's an if and only if. Okay, if you predict through a linear combination between the empirical method. A linear combination between the empirical measure and your prior guess, then it is the Dirichlet process. Okay, so that's a simple way of introducing the Dirichlet process, just kind of as a learning function, right? You learn about the following data points by using this kind of learning mechanism, which is nice and natural, linear combination, empirical measure, which is what a frequencies would use. Your prior guess, you bring your Bayesian point of view into it, and then obviously the weight attached to the prior. And then, obviously, the weight attached to the progress decreases as the sample size increases. Okay, so that's a nice way of introducing the Dirichlet process from a predictive point of view. You could do it also in terms of APPFs, and it's a very elegant construction. And for instance, the APPF of the Dirichlet process, which in terms expressed in terms of frequencies of frequencies, coincides with the so-called UN sampling formula, which is kind of a cornerstone of population genetics. Of population genetics has the following form, and you see it's pretty simple. And you can obtain then the predictive distributions by looking at the probability of sampling something new or sampling one of the old ones as the ratios of the PPFs. So the new ones essentially corresponds to the PPF. So the probability of the sample you have observed plus one, so a new category, a category one, k plus one, divided by the PPF. The PPF, so it's essentially a conditional probability, and in that way, you obtain the weights of the linear combination. Okay, it's just as a range of the PPF, whereas to re-observe a value you have already observed, what you do is you add just a plus one to the j category if the value have to observe is j. Okay, so that's another way of obtaining the predictive distribution of the Dirichlet process, starting from the partition distribution, which is characterized by the PPS. Distribution, which is characterized by the PPF. Just to have another example, you can do the same, for instance, with the normalized stable process, right? And in that case, your APPF has this form here. It doesn't matter here now how you derive that, but that's the form it has. And again, you take the ratios of the PPF with the same logic. At the denominator, you always have the PPF. At the numerator, you have the PPF, either with a K plus one block. So you open a new block and you have one observation into it, or otherwise have the in the J block, you add one. In the j block, you add one. You take those ratios and then you obtain the predictive distribution of the normalized table, which in this case, obviously, it's now a linear combination between p naught and not an empirical distribution, because otherwise it would be the Dirichlet process, but like a weighted empirical awaited by this parameter sigma. And then, importantly, here also the number of distinct observations appears, but that would be then a different story. Okay, so the idea of Peter. Okay, so the idea of pitmore was essentially: let's introduce the species sampling models exactly with that particular logic. Okay, that you obtain the predictive distributions as ratios of the PPFs. And you can do that in general. So you define what he calls the prediction probability functions as ratios of PPF, exactly with the logic I had before. You add a k plus one category to generate a new value. To generate a new value, you add one to the j category for having the probability of observing that particular j distinct value, okay, or j partition block, because the PPF actually doesn't look at the labels. If those are then defined as the prediction probability functions, then you can define the so-called species sampling sequence, which is an exchangeable sequence, an infinite exchangeable sequence, such that the prediction rule is as follows, that the first one you sample it from your pinot. That the first one is sample it from your p-naut, and then the exam plus one is sampled to essentially a similar prediction rule to the one of the Dirichlet process as a linear combination of your p-naut and a weighted empirical. And the weights are given precisely by the prediction probability functions. So that's the generalization of Pittman of the learning mechanism of the Dirichlet process using the prediction probability functions defined in terms of ratios of. Defined in terms of ratios of APPFs. So, what is that? Okay. Okay, so that's a species sampling sequence. Corresponding characterization, no, because species sampling, the prediction rules, the PPS are all marginal properties, right? So I look at the probability distributions of the exchangeable sequence. Distributions of the exchangeable sequence. And obviously, by definitive representation theorem, we know there's an underlying non-parametric prior corresponding to those. So the question obviously is what's down there. And down there, that's a nice result of Pittman. He characterizes the species sampling sequences. So Xine is a species sampling sequence, so exchangeable plus that type of prediction rule, if and only if the underlying priority measure is of this form, which doesn't tell much. So the key assumption here is key assumption here is this one okay that the well it's uh it's an earthquake or now so the key assumption is this one the fact that the label so the the locations of my random probability measures are is independent the locations are independent of the probability weights so that's the structural thing in the in the corresponding species sampling process which is this quantity here okay and then the the the locations are iid from p naught Are IID from P0, and typically we're interested in the cases where this is proper, so where the sum of the probability weights is exactly equal to one, and so this deterministic part falls away. Okay, but so the key assumption is you get a species sampling sequence, so predictive distributions of that particular form if underlying there is a discrete random operating measure with independent weights and locations. So that's the assumption, right? Good. So, summing up, and then I move to the dependent case. A species sampling model, you can actually identify it in three different ways. One, through the species sampling process, and then you need the probability weights and your p-note. Okay, so I have the species sampling process, and I know that there is a corresponding species sampling sequence and a species sampling model, or otherwise, I can characterize it through the species. I can characterize it through the species sampling sequence to the predictive probability functions and the pinode. Or if I like more the partitions than the predictions, I can just characterize it in terms of the PPFs and so the partition distribution and p-note. In the way I can think of it just as I sample a partition and then I assign according to p-note the distinct values of that but the values, the labels of the block, blocks of that partition. Blocks of that partition. So, as I said, I have a little bit of a difficult relationship with species sampling models. Well, not with the paper itself, because I love this paper, it's really a great paper. The point is that I got a lot of rejection saying that the models I was proposing were special cases of species sampling models. And on that, I kind of disagree, no, because the beauty of species sampling models is precisely that it gives structure. Precisely that it gives structure, it gives structural properties. Okay, it makes you understand the deep mechanism of things, but actually, they are not a model in themselves because the key thing, they don't specify a law for the PIs, right? So if I don't specify a law for the PIs, it's just a structure. So it's unfair to say that a model for which I specify a law for the PI is a special case of species sampling modes. Species sample mode gives you structure, tells you the deep structure. Structure tells you the deep structure of things, but it's unfair to say that a model would specify law for the p for the p t lies, or for the which is the difficult part at the end of the day if we want to implement things, right? If you want to implement things, giving a tractable law for the probability weights is something not too easy, okay? Because it's a random object, non-parametric, which has to sum up to one almost surely. Okay, so it's not fair to say that our specific model normalized random measures. There are no normalized random measures, neutral to the right are non-neutral to the right are not a special case, but homogeneous normalized random measures, stick-breaking priors are a special case of this, okay? Because it's just that there's it's a proper model because the way it's specified. Whereas this is structure, it's beautiful, no, because it gives you a deep structure, but it's not a little bit yeah. So maybe you would like to call these species something models. Because this is where the Because this is where the ambiguity is, that the the term model is yeah but that's how pittman called it so it's difficult to repeat the question because otherwise the mind is also you'll repeat this question and then also is eco in this place yes so the comment was that the ambig that the ambiguity is due to the fact that they are called species sampling models but that's the name uh pitman choose and so it's difficult to change but It's difficult to change, but in a sense, it's correct, but it's structural models, no, because they don't specify the law of the PIs is not specified, and so it's just structure. But it's true, most of the things you can say are special cases of species amplimote. So essentially, what happened then is that I got so fed up and I said, let's do the same in the multivariate case. So everybody's a special case of us now, no? And that's what we did. And that's what we did. So, everybody who's now going to try to define a dependent non-planetary flight is a special case. No, I'm just joking, but again, it's interesting because it shows you the deep structure. That's the whole idea. So, let me pass to the dependent case. Dependent case, and here again, let me quote the fineti. You need analogies. You can see the exchangeable case as a limiting case, but still you will have analogies. The whole idea. You will have analogies. The whole idea is split up your model in a nice way, such that you have analogy symmetries and exploit that for making inference. And what we use here is partial exchangeability, which is pretty general. Okay, and ideas, you have J population, but potentially they could go even to infinity. And with multivariate species sampling models, you could do that. Where you have homogeneity, so exchangeability within each population, but heterogeneity. But heterogeneity across. So you cannot put everything in the same pot and say they are exchangeable. No, but you have exchangeability within each population. Well, obvious example is that of topic modeling, where you have exchangeability within each document, but then you obviously don't have it across, but you have still dependence. You borrow information, you learn across meta-analysis or whatever you want to sample problems. You can think of many applications of partial exchangeability. But so, what does it mean? No, it just means that you have exchange. It means no, it just means that you have exchangeability within each group. Here, I do everything just for two groups, but you can do as many groups as you want without it. Just a notation becomes a little bit more complicated. So, I focus on two groups. So, they are partial exchangeable if they are invariant within each group. So, here I have a little example just to show. So, you need homogeneity within each sample. You have equality distribution. If you swap the elements, here you see the first at the first at the bottom. At the first at the bottom, it indicates the sample or the group. So, if you swap within a group, you preserve a quality distribution. If otherwise, you swap across groups, so you bring this one here of the first group in the second group, and the third one of the second group in the first, then you don't have the equality distribution. So, I really have exchangeability, so invariance with respect to permutation in each of the groups. And that's partial exchangeability. And that's partial exchangeability. And so the phinetic representation theorem holds also here. The only difference is that now you have, if you have two populations, you have two random priority measures. And the key thing here is that you have a vector of random priority measures, which are dependent. If you want with all the exchangeable things, you solve the problem of defining nice classes of random priority measures, work out all the properties, posterior, so on and so forth. But here now we have the dependent vector of random priority measures. Of random priority measures. And in a sense, defining this one is done much harder. And we are back to square one. Okay, if the problem was solved by Ferguson in 73 and then we have a lot of extensions, we're back to square one, but because now we need to define a vector of dependent random pretty measures and most of all, understand the dependent structure. Okay, the dependent structure, the corresponding body of information, and so on. Okay, because we don't want to be black box people like People like in machine learning want to understand what's going on. Okay, that's a difference between us and them, if we want. And so that's really important. It affects a lot the inferences you make. And so obviously the pioneering works were, as usual, one in Italian by Gifrella and Tregazzini and then the papers of Steve McKikon, which confirmed my idea that when you have always very innovative work, it's always difficult to publish it and that witnesses it. That witnesses it. I mean, the Baron technical report of 88 was another instance. I mean, there are a lot more. So, when you deviate a lot from the standard, it's always difficult to publish. But that was really pioneering work of Steve McKikum. And as I said, we want really to understand the dependent structure, okay? And possibly to introduce a measure of dependence. So, obviously, corresponding to the APPF in the exchangeable case, you can define the partial exchangeable. Case, you can define the partial exchangeable partition probability function. And especially in this case, having a partial exchangeable random vector makes things easier because if you don't have that, then it becomes quite cumbersome. You can still do it. I mean, we are trying to do it, but it's more complicated. Whereas with a partial exchangeable array, it's very natural to define the corresponding partial exchangeable priority partition function. Okay, the key thing is that you have ties within each sample and also across. Within each sample and also across. And the across the ties across sample is what essentially regulates the borrowing of information. So, just to set notation, I now have two samples because I said I focus just on two populations. I have two samples with K distinct overall values across both samples. Okay, and the overall sample, if you want, across two of them, I denote it by capital N. And then you have your frequency vectors. And the frequency vector runs from one to k, the overall number. Runs from one to k, the overall number of distinct values. This means that some of the entries of that vector could now be zero. Okay, and if both for the j element both are strictly bigger than zero, it means that that value is shared by the two. Okay, that's kind of an important notational thing. So you have those two vectors of frequencies, which go from one to k, which is the overall number of distinct values you have across both the samples. And so you can have zeros inside them. If both are strictly bigger than. Them. If both are strictly bigger than zero, then it means you have a value which appears in both samples, in both populations. And that's what makes the Borough free information and the dependence as I will show. And then you can define the partial exchangeable partition probability function in the same way. Okay, so that's the probability for observing on an overall sample of size n1 plus n2k distinct values with the corresponding frequency vectors. Okay, so I'll just ignore the label. So I just ignore the labels, and that's the object I need to compute, which I can compute in many specific instances, but now I'm not interested. No, I played a Pitman game and now I'm structural. I just know that this object exists, might be difficult, might be easy to compute, but it exists, it's well defined, and that's what I'm interested in. Okay, so that's the partial exchange of partition probability function over two populations. Good. So, what's the dependence? Then I pick up the The dependence then I pick up the partial exchangeable probability partition function. What's the dependence? So I can have maximal dependence. Maximal dependence in this case, it means that I have full exchangeability. So if you want on this plot, I am the diagonal. This means that the two random priorities measures coincide almost surely. And so what I can do is the observation I have in one and in the second group, I put them together and then treat them as a single exchangeable sample. them as a single exchangeable sample. So that's one extreme situation which corresponds to maximum dependence. The other case is obviously independence, okay? That the two random measures are unconditionally independent. So I can actually analyze the two data sets independently, and there's no borrowing of information or gain if I analyze them jointly. So those are my two extreme situations. situations. So what's the how can I measure then the intermediate cases? I understand what extremes are, which is always important. If you want to understand something, I first look at extremes, but what happens in between? And the most popular measure of dependence is the following, which, I mean, for quite some time was a little bit mysterious to me. That it's the correlation of the random operator measure. Relation of the random parity measures evaluated on a set A. And it turns out that typically, and I don't like that typically, that correlation, the value does not depend on the set A on which I'm computing it. I still remember once Ad asking me about why doesn't it depend on A? Okay, so that's something I was asking myself for quite a long time. And now I try to give an answer for this. But that's typically the method. But that's typically the measure of dependence which is used. And precisely because of the fact that even though you compute it on a set A, it doesn't depend on A, it's used. That's an overall measure of dependence. Good. Then you have also in terms of extreme cases, you have that if you have exchangeability, then you actually get a correlation equal to one, but there's no guarantee of having the vice versa. Correlation equal to one that you have full exchangeability is not guaranteed. Is not guaranteed. The same thing, no, if you have independence of the random property measures, then obviously we will have correlation equal to zero, but vice versa is not necessarily true. Okay, so in a sense, as it stands, it has some problems. And on top of that, I mean, I'm not completely convinced correlation is a good measure to measure the overall dependence. One typically would need with random operating measures to take the whole infinite dimensionality of the random operating measure into account. But still, so. Into account, but still, so it kind of works, but it has some issues. The nice thing about this correlation is that it's very, very simple. In many cases, you obtain simple expressions. So, for instance, in the hierarchical Dirichlet process, where you take a Dirichlet process and you take another Dirichlet process, the base measure, essentially, the model by EWATA, Mike, and Mike Jordan, you get the correlation, that quantity here, where C0 is the theta parameter of the Dirac process on top. Parameter of the Dirschle process on top, the one which is joining the populations, and the C is the parameter at the bottom. Okay, and just to make the parallel with the example I had before with the normalized table, in that case, you obtain very simple expressions. And here you see it, if you have that the C0 theta parameter on top goes to infinity, it means that the Dirichlet process on top degenerates on its Process on top degenerates on its base measure, and the two and the two Dirichlet process at the bottom they become independent, so the correlation goes to zero. On the other hand, if C, so the theta parameter at the bottom goes to infinity, it means that the two Dirichlet processes at the bottom go to, they generate on their base measure, which is exactly the Dirichlet process on top. And so they will coincide almost surely, and you got full exchangeability. So you see, you have nice interpretations and on top of that. Have nice interpretations, and on top of that, you can compute it quite easily. But still, I'm not quite satisfied with this structure. So, the open problems I'm trying to answer, at least in part today. So, why does that correlation not depend on A? Okay, and can we be a little bit more precise than saying typically? Typically doesn't mean anything, no? Then, a thing which is a lot overlooked in the literature about the pendant process, what are the implications on the What are the implications on the observables? If I have a certain dependent structure on the underlying non-parametric prior, what happens on the observables? What happens to those? So that's another question I was asking myself. Then on the third question I'm asking, are there situations where it's an if and only if? So full exchangeability if and only if the correlation is equal to one or independence if and only if correlation. Or independence if and only if correlation is equal to zero. That will would give a bit of support to using correlation as a measure of dependence. So if I understand why that does not depend on A, and then I'm able to show, at least for a large class that that's true, then it makes much more sense to me to use correlation as a measure of dependence. So if I have the extreme situations and it's an if and only if that correlation zero is independence, correlation one is exchangeability. And then on top of that, I mean another big problem. Can I measure dependence in another way? Can I measure dependence in another way? And that Marta will talk about that in her talk, to use the infinite-dimensional nature of the run operating measure and make a measure based on that. So that would be more kind of a principled approach to quantify dependence. Notable classes of dependent players, so we need to become a little bit more specific. For instance, additive structure first proposed by Peter and typically First, proposed by Peter. And typically, for all, you have kind of one approach based on stick-breaking priors or constructions, the other one on random measures. There are pros and cons. With stick-breaking representations, typically computationally, things are easier. With random measure approaches, typically it's easier to derive distributional properties. Okay, so I mean, it depends what you want to do with things, but those are typically three very popular strategies to construct dependent on parametric paths. So, additive structures. Non-parametric paths, so additive structures, hierarchical structures, nested structures. Good. So let me introduce multivariate species sampling models. And as you can see, special cases, all. Good. The ideas are the same. I mean, technically, it's not so easy as I present it here, but the results are. Okay, so I'm hiding here a lot of things which are which are Things which are kind of below, so it's technically not so easy to, but the results are neat. Okay, so I can actually define a multivariate species sampling model just by generating the partial exchangeable partition probability function, so instead of the APPF, the partial version of it, and then independently sample the unique values from a non-atomic measure p naught. Okay, so I can do that. Then I can also define the corresponding Find the corresponding species sampling sequence for the partial exchangeable setup, things become a little bit more complicated, but not too much. Conceptually, they're exactly the same. So you need to define now the multivariate prediction probability functions, again, as ratios now of partial exchangeable partition, probability, partition, partition, probability functions. And the idea is the same, no? If I want the probability of getting something new in population. Getting something new in population one, what I need to do is I need to add a k plus one category on the frequencies of the first group. Okay, and the same if I want to re-observe a certain j value, then I need to add a plus one to the j value in the corresponding, say, group one or group two, depends on which I am. Okay, so I have this now I have that vector of prediction probability functions, which in the logic, the logic is exactly the same as the exchangeable. The same as the exchangeable one, and so I can define the multivariate species sampling sequences precisely in this way: it's a if it's a partial exchangeable sequence, and then the prediction rule is as follows, that the starting value is generated by p naught, and then you have this prediction rule, which is in this case you have one for each of the populations, which follows the same type of structure, okay? Linear combination between p-naught, weighted empirical measure. Empirical measure, weighted, and the weights are. Sorry, the weights are precisely given in terms of the prediction probability functions. Okay. Good. You can also show that marginally, if you start from a multivariate species sampling sequence, you get a species sampling sequence, a traditional one. And you can also give this characterization, that the fact that if you have a multivariate species sampling model, and it's an if and only if with the multivariate species sampling sequence. The multivariate species sampling sequence. So you can just again, like in the exchangeable case, you can go from one to the other. You can work with the partitions, you can work with the corresponding prediction rules. Again, question, what's underlying to those marginal properties of the partition of the prediction rules? Underlying is the multivariate species sampling process, which is again pretty similar to the exchangeable case. Obviously, here we have a vector. Obviously, here you have a vector. And the key assumption underlying is similar to the one in the exchangeable case: that you need the locations to be independent now of the vector of the random break measures. And that's your multivariate species sampling process. And again, if the weights sum up to one for both sequences, then it's proper. Okay, so you have a nice characterization of the underlying random quality measure. Again, it's not explicit, it's structural. Not explicit, it's structural in the sense that the law of those is still missing, okay? But it's structural, so I um that's what I have, and that's which is not so easy to prove that's that's quite quite quite more complex than in the exchangeable case. It's actually linked a map, okay? That if x, if the partial exchangeable vector is a multivariate species sampling model, it's an if and only if with the corresponding multivariate species sampling process, okay? So you have the link between all of You have the link between all of them: multivariate species sampling model, multivariate species sampling sequence, and multivariate species sampling process. So the theory glues nicely together, and you have a very similar picture to the exchangeable case. You can also define that in terms of a sequence of partitions, which is a bit complicated, but it has advantages maybe in a future talk that you can easily go to infinite populations. Yeah. Yeah. Um so here you have the for both PT1 and PP2 you have a seven distribution from the Z yeah that's the species sampling that's the ID of the species sampling sequence of the Z are I ID from Pinot I'll do it at the end yeah that's important actually you repeat the question ah sorry uh whether here it's important Whether here it's important that the Z's are all the same old generators from P0. Yeah, that's the spirit of the species sampling models from the univariate case. And so we carried it over. Okay, if you want to see that's a little bit of a restriction in the sense that it implies that either the atoms are shared or they're not shared. Okay, so that's the two situations you can encounter. And I'll get to that. This is an important slide. The correlation for multivariate species sampling. For multivariate species sampling models, and I don't need to specify any law for the weights for saying this one. And that was actually an observation Giovanni made linking up the correlation with the probabilities of ties. And everything started from there. And that result tells you that the correlation between two dependents, or in this case, multivariate species sampling processes, is equal to the probability, look at the The probability, look at the situation down here where it's uh with the same marginals: the probabilities of having a tie across samples divided by the probability of having a tie within a sample. And that explains you now why it doesn't depend on A, because the correlation of the underlying probability measures, random probability measures, depends just on the probability flow of ties. So that gives you the reason why it doesn't depend on A, okay, which is pretty. Depends on A, okay, which is pretty nice. We've been able with this also to replace that typically by saying this holds for every member of the multivariate species sampling models, okay, which is something a little bit more precise. And it's a nice structure, no, it's just in terms of probabilities of dice. On top of that, I mean, it also tells you that the whole dependence is exactly related to the probability of sharing atoms. Of sharing atoms. Okay, so the whole dependent structure boils down to that. Important observations to make, and that's precisely the reason related to Judith's question, why you have to leave that assumption that the ZI's are generated from P0, is that for all multivariate species sampling model, I wrote it actually up there, the correlation is positive. The correlation between two random operating measures is positive. And this implies also that from the observables, On the observables across samples, the correlation will be positive. Okay, that's quite the restrictions in many situations. Okay, but so all these popular models they have this assumption underlying, and many people probably don't realize when using them, including ourselves up to recently, okay, that the correlation is always positive. Then obviously you have moments, if you want, you have all the special cases now of us, which I listed before. Which I listed before, everybody's special case. On top of that, if I define a regular multivariate species sampling model, I just skip this because I'm out of time essentially. I just need a small independence assumption. With that regularity assumption, I'm able also to answer that question, and in the sense that within regular multivariate species sampling models, and regularity in a nutshell means that. In a nutshell, it means that the weights for atoms which are not shared by the two random probability measures, the corresponding probability weights have to be independent. So dependence can exist only for the weights which are possibly shared. So that's the regularity assumption. If that's the case, then you have an if and only if on the correlation equal one, exchangeability, correlation equal zero independence. Okay, and that in a sense justifies correlation as a measure of dependence. Correlation as a measure of dependence, at least for multi-bargeted species sampling models. So I have three minutes, so let me try to go a little beyond, but then Beatrice will pick up on this topic in her talk. So actually, by discovering that the correlation for all this model is positive, both for the random operating measures and observables across sample, the question was, so is negative correlation possible or incompatible. Negative correlation possible or incompatible with partial exchangeability? And it turns out the answer is yes, it's compatible. Okay, it's compatible and you can achieve it. But you have to play it smart on the locations of your discrete run operating measures. So I just have three minutes, so probably I don't have time to go into this, but let me just give you the construction and that Rachel will talk about this as well. So I start with a species sampling model. So I start with a species sampling model, multivariate species sampling model. So it's a vector. And in this case, I take it over a product space x times x. And so I will have them that the locations is a pair. And then I take the projections. I take the projections for one and the other coordinate. And this is now my vector of random operating measures. And that's not a multivariate species sampling model anymore. Okay, because I've taken the projections. Okay, because I've taken the projections down, and it's a multivariate species sampling model only if my G0 there, which generates the pair of locations of my original multivariate species sampling model on the product space, is either independent or they coincide. Okay, so essentially here what I have is that I have two distinct values which are generated from the G0. And this implies, under this assumption, I can get Assumption: I can get a negative correlation among the two samples. I also discover that to get negative correlation, I need to work on that G0. And so I think I'll stop it here. Just one sentence to conclude. So if I play the game with that model, which doesn't fall into the category of multivariate species sampling model using a projection of them and playing around. A projection of them and playing around with the G0, what I obtain is that actually the correlation across samples is a constant gamma times rho zero, which is the correlation coefficient associated to G0. And that's the key of this type of study. That gamma here is the so-called probability of a hypertie, which is the probability of a tie, which is not a tie in terms of values. Okay, that's a new. Of values, okay, that's a new concept we've introduced, but Richard will discuss it. It's the fact that you loosen the requirement that observations either coincide or they are independent, and you create this kind of hypertie on the product space, which corresponds to the fact that two values belong to the same location. They are not the same value, but they have the same label in my series representation. If I go back here. I go back here. So basically, a hypertie would be an X and Y, which come from the same location here. Okay, so it's not a tie because it's not, but it's a hypertide because it's up there. And so, and that's the key for giving the correlation structure in this kind of more general models, which become the probability of a hypertype times the correlation of gamma zero. And I can get negative correlation. I can even get correlation between zero. Correlation between zero minus one and one. So, as I said, here I was just structural in everything I said. So, again, also for this type of situation where I want negative correlation, then the question is one is giving nice structural properties, the other one is giving tractable classes, something which you can implement. And that's more or less what Bertrich will try to talk about. So, that's what I wanted to say. I tried to answer some questions I had for a long time. Well, let's thank Iber. I think the more microphone, I think. But there's a microphone here. It's open. Okay. Okay, no, no, I don't need to put this. Let's well, for the sake of time, I think we have to move on to the next speakers, but we have probably time for one question for the sake of the super. For the sake of the super lecture that Igor gave. Nori, do you have one question last year? You are trying to characterize in which situation correlation can actually characterize the dependence. So, can some connection be drawn with the multivariate Daussian distribution type where Daussian distribution type where in Gaussian we know that correlation is the so for multiplied Daussian distribution correlation characterizes the dependence so can some some similarities be drawn with that I understand these are very different models but you could use the mic uh no what we did actually was uh in this case it's a uh correlation we we looked whether correlation was able to characterize the extreme situations and it so Extreme situations and it so independence and exchangeability. And that was able to characterize. The things in between the correlation can be at most a proxy, because at the end of the day, it's really reductive to encapsulate an infinite-dimensional object like a random paradigm measure into just correlation. So you need something more sophisticated, like for instance, Marta will talk about it, index of dependence based on the Baselstrain distance, which takes the whole distribution into account. Whole distribution into account. So, in between, it can be a good proxy, but in it's not. But on the extreme situations, yes, we've been able to show that it's in a way, in spirit, similar to the Gaussian case. Well, thanks, Igor. Our next speaker is Beatrice Franzozo.