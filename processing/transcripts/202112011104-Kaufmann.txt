Dorian Boudry, who started just before the pandemic, so you might not have met him in conference, but he did already a few cool works on this thematics of non-parametric exploration in a multi-armed bandit. So I start with recalling the setting that we consider, which is a stochastic version of a multi-armed bandit, in which you have an agent f facing a bunch of unknown distributions denoted by nu one up to nu k. By μ1 up to νk, and in each time t he may select one arm and observe a reward that is drawn from the associated distribution. And his goal is to find a nice collecting strategy that would maximize the expected sum of reward, which is equivalent to minimizing the regrets, which is the difference between what you would get if you knew in inside the arm with largest expectation a new star. Expectation new star minus what he gets by just collecting reward with these adaptive strategies. So there has been a lot of research on this topic in the machine learning literature and there is a very nice survey written by Tor and Chaba on the topic. So I will not motivate further this problem. So in order to position our work, I will start by reviewing a bit By reviewing a bit existing algorithm and what is missing and why we want to go further into this non-parametric method. So I'll start with a very stupid with telling you what not to do that I guess you all know here. So a very simple algorithm and quite natural if you think of trying to maximize reward is to try to just exploit the current knowledge you have gathered up to a certain round T. Up to a certain round t by just selecting the arm and observing the reward. So, an index of the quality of the arm might just be the empirical mean of all the history of reward we have gathered so far from that arm. Of course, if we do that, the algorithm is very simple. And indeed, I call it non-parametric because it doesn't rely on any underlying assumption on the distribution u1 up to nu2. Distribution ν up to νk. However, it is well known that even for simple distribution, it achieves linear regret because if you start, if you have barely arm and the best arm gives you zero in the first round, of course you will never go back to it. So a smarter algorithm instead of just exploiting, have to do a smarter exploration, exploitation trade-off. And this is those are, I would say, the Those are, I would say, the two typical algorithms to achieve this exploration, exploitation trade-off in stochastic bandits. So the first type of algorithm are based on upper confidence bound. So compared to the greedy strategies, I replace the empirical mean by an upper confidence bound on the value of the unknown mean mu A. So this will ensure more exploration. More exploration. So, here we use the illustration on the five arm bandit problems. So, this algorithm will typically align upper confidence bound on all arms, and this will balance in the right way the number of selection. We see here that the arm with largest mean has been selected a lot. So, the other family of alberism is randomized and is follow an old principle introduced by Thomson in the 1930s. By Thomson in the 1930s. And the idea is now to replace the empirical mean by a noisy version of it, which is obtained by assuming a prior distribution over the unknown mu A and sampling the posterior distribution. So this will naturally lead to a noisy version of the empirical mean and this will randomization will give us the right amount of exploration. Exploration. But what we can reproach to these methods is that to build the UCB, you have, of course, to make some underlying statistical assumption on the distribution that generated the reward to do it in a proper way. And for Thompson sampling, you have to choose the prior distribution that will lead to, for example, an easy posterior update. And you do that usually knowing what is your likelihood. So knowing what distribution you Knowing what distribution your arm comes from. So, what I mean is that in order to have good performance with this kind of algorithm, you have to tune the UCB or the prior intervention sampling in order to be optimal. And what my notion of optimality is in this talk, it's one possible, of course. I consider problem-dependent asymptotic optimality, meaning I'm trying to matching these two. Trying to matching this to match this lower bound given by Lyle Robbins in 1985 on the regret that says, for example, that for simple parametric distribution, the regret is at least log T multiplied by a sum over the arm of a gap divided here by the Kulbert-Liberal divergence between the mean of arm A and the mean of the best arm. And if you want using And if you want UCB or Tomson sampling to match this lower bound, for example, if you have Bernoulli reward, you should put a uniform prior in Thomson sampling, or you should design confidence interval based on knowing the KL divergence in the family. Whereas if you have a Gaussian reward, you will assume, for example, a Gaussian prior in Thompson sampling. So in order to reach this bound for a given family of arms distribution, you will actually have a different algorithm. You will actually have a different algorithm. And what we want here is a single algorithm that, when we instantiate it, we try it on Dernoulli Gaussian or other kinds of rewards, we reach optimality in each of these families. So this is what we put. So the sub-sampling dualing algorithms that I will introduce fit in a more broader line of work that emerged recently on a resampling approach. Indeed, if you want. Indeed, if you want something fully non-parametric, a natural idea would be to induce some exploration would be to replace the empirical mean, for example, by a non-parametric bootstrap estimate. So you would draw in the history without with replacements or without, I don't remember, possible samples, and then you average them. And then you average them. So, this could induce some exploration, but actually, it was shown that this doesn't work. And in this work on this line of work on the partial history exploration, Quetan et al. proposed to fix this by adding a few fake samples in the history in order to skew a bit the distribution towards uniform. And so, this type of algorithm can also be instantiated in many more general bandits. A more general bandit, it's quite cool. But in our use case, like trying to match the Lyon-Robin lower bound for different families of parametric distribution, it fails to match the lower bound. So we have to find a different approach. One possible such approach is this very nice non-parametric Thomson sampling idea that was proposed last year by Ryu and Onda. So the idea here is to replace the empirical mean with a The empirical mean with like uniform weight on all observations by a completely random re-weighting of all the observations in the history. Plus, you need to put our weight on the upper bound of the support that you have to know. And this algorithm is proved to be optimal for bounded distribution. But in both cases, you have to know that distributions are bounded. And with our method, we will be able to tackle both. Be we will be able to tackle both bounded and unbounded distribution. And our algorithm comes from compared to this resampling approaches that can be seen as fixing the simple follow-the-leader strategy by changing things or rewriting the history. So, our approach will rely on sub-samples. Will rely on sub-sampling, like trying to ignore some parts of the history of the arms that has been pulled a lot. And the idea of sub-sampling has been introduced before in other works, and we will later explain how the SDA family of algorithms relates to these two papers. So, what is a subsampling dualing algorithm? So, first, it departs So first, it departs from the standard index policy type of algorithm I presented before. So indeed all the algorithm I discussed were of the form for each arm compute a quantity which depends on the history and pick the one with largest UCB or Thompson sample or empirical. Here what we will do is actually pairwise comparison between arms and for this we propose a round We propose a round-based structure. So, in each round, we will draw several arms and we will start by trying to find the leader, which is in our setting, the arm that has been sampled the most until now. So, it's a natural guess for the optimal arm in a regret setting. And then we will organize k-1 duals to compare this leader to all other possible arms that we call challenge. Possible arms that we call challenger, and what is crucial here is how the duals work, and this is why where the sub-sampling idea kicks in. So, the idea of a dual is to perform a fair comparison of two arms based on empirical means, but the thing is, the arms have different history size. So, for this, we will try to equalize. So for this, we will try to equalize the history by sub-sampling the arms that have been selected the most. So more precisely, for the challenger, which has been drawn the least, we will just compute its regular empirical mean, like in the greedy strategy. And for the leader, we replace its empirical mean by the mean of a sub-sample of the same size as the history of the challenger. So now we compute two empirical means based on the same number of On the same number of observations, and then we add the challenger in the list of arms that should be drawn if its empirical mean is larger than the sub-sample mean of the leader. So let's see to make it clear that you all understood. I have here a simple example with three arms with a binary reward. So the first arm in blue has been drawn four times, the second arm in green has been drawn two times. Arm in green has been done two times, and the third arm in red has been done three times. So the leader is blue. And then we will perform a duo between blue and green, and between blue and red. So for the dual between green and blue, blue has been drawn two times, so we need a subsample of size two of the leader. So for example, let it be this one. So now the leader has an empirical mean of zero, whereas the challenger has 0.5. So then 0.5. So then we decide that this arm needs to be drawn. And for the other draw, for example, I assume that this is the subsample of size 3 that was computed by the algorithm. And then we see that the arm in red has a smaller empirical mean. So the leader is winning. So in the end, in this round, we will select just one arm, the arm in green. And in case the two challengers would lose their duration. Two challengers would lose their duels, then we would select the leader. This is the only way that we select the leader in a given way. So, this is a simple illustration of an SDA algorithm. So, this algorithm is again a family of algorithms which depend on how we will draw the subsamples. And so, the question we have to answer in order to precise our sub-sample. Precise or subsampling dualing algorithm is how we will subsample n elements out of the capital N elements in the leader. So the leader has been draw more and so we need to subsample these small n elements. So the first and most natural idea maybe is to use sampling without replacement, so which boils down to picking a random subset of size n in the Is n in the history 1n. So this was actually what the first algorithm based on subsampling was doing. So this is the BESA algorithm that was proposed in 2014 by Odeleric and some co-authors. But this algorithm was only analyzed for two arms and bounded rewards, and it was not proved to be optimal. So what we proposed in our work is also we propose several Work is also we propose several sub-samplers in the paper, but today I will focus on these two. So, first is a random block sampling. So, it is a bit more efficient numerically than sampling without replacement. We will decide if we have to subsample a history of size capital N to always output a block of consecutive observation, but any possible block. So we just pick at random the starting position like. The starting position like uniformly over one n minus n, and then we have put the subsequent block of reward as our subsample. And last block sampling is pushing this block sample ID to an extreme by just using the last block, so n minus small m to n in the leader's history. So I mentioned before another paper that was Before another paper that was pushing for this sub-sampling idea. So, this is the SSMC algorithm that was proposed by Chan in 2020. So, in their work, what they do is even they try to make the leader look bad by even choosing the block that has the smallest empirical need. So, it's even and then this is data-rependent subsampling, whereas what we promote in this work is really. Whereas what we promote in this work is really a randomized sub-sampling scheme. In particular, what I will do in the rest of the talk is to explain why this random block sub-sampling algorithm is achieving a very good performance in terms of request. So, as I told you, this SDA types of algorithms, they are based on ROM, where They are based on rounds where in each round there is a set of arm AR that is sampled. So, usually we define the regret to be the regret that we get when we collect a total number of sample t. So, to achieve this total number of sample t, we will use a random number of rounds. However, when we write up the regret just by upper bounding the total number of rounds by Total number of rounds by the worst case, which is the T, in case we always draw the leader in each round, which, by the way, will never happen. By doing that, we end up with a usual regret decomposition, which sums over the arm the sub-optimality gap times the expected number of times a given arm k has been selected in T rounds. So here T rounds. So here now T becomes the index of the rounds. So in the end we have also to just to count the suboptimal selection in order to propose a regret analysis. So the key tool that we need to analyze the regret of random block sampling and potentially also other block sampler like this less block ID that I also Last block idea that I also mentioned is a concentration property for the subsample. So we have to say that the probability that an arm whose tromin is smaller than another arm, the probability that it wins the duo has to be small at some point. So to present the result, I need to introduce some notation. So I will denote by ykn the nth observation. And the nth observation from arm k. Then, from a subset of observation, like of index of observation S, I will denote the empirical average computed on the subset S in this way. And then what is this SK RMN? So, this is the subsample that would be used in round R if somehow. R if some RK is a look suboptimal, so it's a challenger, and the leader has been drawn n time. So this is a subset of size n of history which is larger. So this just allows us to define this quantity, which is the sub-sampled average of the leader, which is computed in the algorithm. In the algorithm. So, what we are able to prove is that if we have an arm B which has been drawn less than some arm A, even if arm A is actually smaller than arm B, then the probability that arms A loses a dual against B. So here we see we subsample the history of arm B, taking just N A samples. NA samples, this probability is actually connected to properties of the distribution, the underlying distribution. So here on the right-hand side, we have just the probability that an empirical average, so y bar ij is the empirical average of the first j observation from arm A, exceed a threshold, and here the probability that this is lower than a threshold. Than a threshold. So basically, this result is just telling us that in order to prove that you cannot lose too many duals if you are a good arm, we just need concentration property for the underlying distribution. And indeed, we analyze the algorithm under like a simple assumption of the distribution that there is some exponential concentration. So typically for exponential families, we have this type. Families, we have these types of inequality with Chernoff inequality, which gives a rate function, which is exactly the Cuba-Kleibler divergence function, which appears in the Lawopa. So, the first step of our analysis only relies on this arm concentration assumption, and we are able to upper bound the number of times we select an arm k by a leading term in log t divided by. In log t divided by this weight function, and this will exactly give the leading term in the line-rolling slower bound for exponential families. And then here we have a second term that might be annoying to control, which is the sum of probabilities that the optimal arm, so here arm one is the optimal arm, has been sampled less. Has been sampled less than log R squared times. So, this in the analysis of Thompson sampling, we might be able to analyze this type of algorithm by also proving similar things, like proving that the probability to draw the optimal arm too little is small. And so, here in our analysis, we will control a similar term for this subsample. Subsampling dueling algorithm. So now we will need something else and concentration to show that this probability is small. And more precisely, I will try to give you an idea of what we need. You can forget the technicality here. So if you have the events that the arm one has been drawn less than log R square, then there must exist a sequence of Of consecutive time step of lengths at least r over log square in which arm one was never put. So arm one was always losing its duals. And among these R over log R square duals, the hope is that the sub sampling schemes will sometimes output independent subsample. By independent, I mean we By independent, I mean with non-overlapping supports, because then they will become statistically independent and we will count like it will be not likely to lose a lot of duals based on independent samples. So, this is the idea underlying this diversity property that we need to establish for the sub-sampler and counting the mutually non-overlapping set that we can get when. That we can get when self-sampling using several calls to the sampler, we were able to prove using some combinatorial arguments, specifically for this random block sampling, that we have this sum of probability that is of order small O of 40. So, this is just telling us that we will have like a large number of independent subsamples. Independent subsamples during the sequence of pools of rounds where the leader is not winning duo. And the second property that we will need is saying that, okay, if my leader is losing a lot of duo based on independent subsampler, then at some point it cannot be very likely. Cannot be very likely, but it turned out that there might be still some problematic distribution where these probabilities are not small enough. And in the end, we needed to analyze this sub-sampling dualing algorithm under an assumption on what we call the balance function. So this balance function was already introduced in the first analysis of this BESA algorithm that I was mentioning. That I was mentioning. Yeah, okay, this is a table, but anyway, and it's really the so here a new 1j is the distribution of the sum of j sample from arm 1 and ν kj is the sum of k j sample from arm k. And here we have this one minus f to the m due to the independence of the g subsamples that we are looking at. Subsamples that we are looking at. So, again, this might look like a very technical condition, but it is just to say that in order to control this probability, we needed first a property of the subsampler that we conjecture is also true for something without replacement, but that we were only able to prove for this random block sampling scheme. And then we need something that is not super explicit, I I admit. Super explicit, I admit, on the distribution of the arm. So we managed somehow to decouple the two. And yeah, the algorithm could also, you can come up with a relaxed version of this balance condition if you allow the algorithm to add some forced exploration. So forced exploration is just if you have some arm in one R that has been drawn less than F R times, then you just draw them. It just forces all arm to be drawn at least. Be drawn at least FR times in run R. So what we prove in this paper is like we give an analysis first of this RB SDA algorithm under concentration assumption for the arm and then under the assumption of the balance condition. So we managed to prove logarithmic regret in that case. And then when we particularize this result to exponential Particularize this result to exponential family. As I said before, this leading term will be exactly what you expect in this line-Robin lower bound. And what we managed to prove with specific arguments in each case is that the only Gaussian and Poisson distribution satisfy the balance condition. And for any exponential family, like exponentials did not work, but if you have a little amount of forced exploration, then you can be more robust. More robust. So, in the end, to conclude, this RBSDA algorithm is achieving what we wanted. It is the same algorithm that will be asymptotically optimal for Gaussian, Bernoulli, Poisson exponential distribution. And this did not, what is interesting is that here we catch distribution with possibly bounded or unbounded supports. So, yeah, I think in the interest of time, I might skip that, but I just I might skip that, but I just added one slide with numbers saying that interestingly, RBSDA is even having the same performance as Thomson sampling almost when you run it on different problems, which is quite cool because the algorithm doesn't need to know the distribution unlike Thomson sampling. And I'll jump to the conclusion. So we think this sub-sampling during algorithm framework is a nice alternative to more standard UCB or Thomson sampling. More standard UCB or Thomson sampling exploration in stochastic bandits. And since this paper, which was published at Norips last year, Dorian has also proposed some very nice extension of this work. And maybe a current limitation is that this balance condition is quite not explicit. So we still need a more simple characterization of the distribution for which SDA can have logarithmic regret or can have optimal guarantee. Can have optimal guarantees. And the extension to more structure models, like cleaner bandits, like how you extend this fair pairwise comparison in this setting, is a bit less clear than the other work on resampling that I have talked about. So, yeah, this concludes the presentation with some references. Thank you. Yep, there's a question. You must have thought about the extension to the linear bandits. Did you see any difficulty there? So actually in linear bandits, a fair comparison would not be in terms of the number of observations, but maybe would be to try to reduce like the information that you gather in some direction X would be like In some direction, x would be like this norm of x in the design matrix. So, if you want to equalize for two arm x and x prime this quantity, you would need to like take your least square estimate, like your database with which you have computed the least square estimate and just forget the right observation in order to equalize these two quantities. But this does not seem so easy to do, and if it is doable, I guess it would. Is doable, I guess. It would still be numerically not very appealing compared to online squares that you do usually in linear bounties. So far, it's still on our to-do list to really know how to deal with this case, like to come up with this notion of fair wire, like fair comparison between two arms. Is there any chance that this can be extended to heavy tail distribution? Can be extended to heavy tail distributions if you use some fancier instead of the empirical mean, use some fancier mean estimators. Yes, it could be actually recently we have proposed a follow-up where we are dealing with extreme bandits. So in extreme bandits, what you want to do is to find the bandit model with heaviest tails, and there we combine the median of means. combine the median of means with LBSDA type of approaches. So we have batches and we have also the history size that we forget. So yes, there might be some application to handle heavy tails. This might be doable. So yeah, so there's something I'm not understanding about the dueling mechanisms. Maybe you could The dueling mechanisms, maybe you could clarify. So, what prevents the best? Having n1 of t equal to one? So, it's had one sample. It's a terrible sample. As a challenger, it's always going to lose. Why do you sample from that one again? So, you mean if the leader has a zero, for example, in the Baroli case? Right. Case. Right. Why do you sample again? Because sometimes your challenger, if you keep playing, there is an arm that you will keep playing. And if it's Bernoulli, this arm will give you also a zero and will give you a lot of zeros. And at some point, the subsample of size one of the leader, even if it's not the true best arm, will also be zero. So then at some point, you will compare zero against zero and you will get unstuck. I see. So the condition on the leader. I see. So the condition on the leader, like being in an exponential family or something, excludes the support being bigger than the others or something of that flavour, right? Yeah, balance this condition there. Forced inspiration quite a bit. So maybe a follow-up question to this, whether because of this issue, can you still get worst case optimal balance? Optimal balance, or is it like this algorithm has a true asymptotic-like nature? Works well also for like moderately large horizon, like it's outperforming Thompson sampling for all the T's. But indeed, to get the optimal square root KT bounds, at least it's not doable from our current shape of open. Of upper bound, which is quite asymptotic with some second-order terms that have bad scaling with the gaps. So I don't know if we could prove, I guess the same question is whether Thompson sampling is minimax optimal. And I see. I don't care about log factors. Yeah, okay. Okay, you're okay with the log factor. So maybe. Okay, you're okay with the log f factor, so maybe we could get that, but uh um we would need to develop new tools to be able to prove this uh square root kt log k or log t even. I did four six. Okay, okay, thanks so much. Thank you.