Introduce Andrea Nahmod from the University of Massachusetts at Amherst. Andrea will talk about invariant Gibbs measures and global strong solutions for the 2D NLS. The floor is for you, Andrea. Thank you, Sakhar. And thank you very much, Jalal and Pierre and Tristan and Sacher for the opportunity to talk. This is actually joint work with Yu Deng and Haitian Yue, both at the University of South. UA both at the University of Southern California and so both the work and the and the talk is joint so and I hope that they are I hope that they are connected so let me actually start very easily so just move oops don't move it there so we have the the periodic NLS power P on the torus T D and so we have the usual Hamiltonian and the mass which are conserved quantities and and every time I talk about for for now until a little For now, until a little bit later, I will talk about the critical in the sense of scaling and in the sense of what I call the deterministic scaling, which is the usual for the equation. And in the case of the periodic equation, so remember that if you are, if S is the maximum between zero and the critical, then you have local world poseness and below you have imposed. And actually, you know, it's interesting because, for example, if you take the cubic equation in 2D, which The cubic equation in 2D, which is L2 critical, that's an oh, the local world process is open, right? And the same with the 1D quintic, the local world process is open, and there is this derivative lost in the strigher. So these are open problems deterministically. So we are mostly interested in sort of like the NLS with random data. And in fact, I'm going to, I will explain this better later, but for now, let's just say that I consider what is called the weak order. Consider what is called the weak order equation or renormalization, in which I will explain what this means a bit later. With random data, which is simply data of these forms, where these are the GK omegas, are ID complex random variables, which have been normalized, who are center, have mean zero, and the law of the GKs is rotationally symmetric. And this renormalization is needed when the solution. Needed when the solution has, so for example, dimensions two, when the solution this alpha is one and the solution has infinite mass, and also to make sense of the nonlinearity and in particular of the Hamiltonian. So you need to renormalize this to actually make sense of that. I'll say more in a minute. So in this work, the dimension that I'm going to fix is two, but a lot of the things in our work are for general B and general. Are for general B and general T, but let's just say that we're going to be in dimensions too. And so the behavior of generic initial data is actually, as everybody knows, physically meaningful. And a way of describing these genericities through some canonically defined measure. And so this is also linked to the notion of a statistical ensemble point of view, which says that instead of actually studying an individual solution, you study a family of solutions that are distributed. Family of solutions that are distributed according to some canonical law. And so the key point of this randomization is that actually you can prove better estimates than you would otherwise thanks to the large deviations, you know, the usual Kinchin inequality that we all know, or multilinear versions of these inequalities. And so this is key in actually improving results that would be otherwise in improving the deterministic scenario. Improving the deterministic scenario. So, what is it that we are interested in in general? So, from the point of view of PPE, the fundamental question is, of course, well, positive: do solutions exist almost surely locally? And we mean existence in the strong sense, that means with some suitable form of uniqueness. And do they have some sort of like a global, do we know, do we understand any sort of local long-time behavior of the solutions? Are they global? And that, and from the And that, and from the statistical physics point of view, then one is interested in the behavior of the statistical ensemble as a whole. And so, you want to know how does the distribution law evolve under the flow? Are the invariant measures? How do the ensemble averages evolve? Much harder is there, look at this CPU mixing. These are like harder questions. So, the concept of the Gibbs measure lies sort of like nicely in the intersection between. Of, like, nicely in the intersection between these two points of view, the PDE and the statistical mechanics point of view. And so, you know, formally, I can think of this measure as being defined as the exponential of sort of the Hamiltonian times, say, the Lebesgue measure. And of course, this is total nonsense, what I wrote here. You have to make sense of this. Beta here is the inverse of the temperature, is one of the temperature. And for every beta, you have a different measure, and they're all mutually singular with respect to each other. They're all mutually singular with respect to each other, but here we're going to just normalize the beta to be one. And so the Hamiltonian, this the H, the Hamiltonian gives two parts to this measure, which is the part which is the exponential of the kinetic energy, which is times the Lebesgue measure, which is the Gaussian part, and then another part which you think as a weight. So this is a Gibbs measure, it was given by Hamiltonian, is actually a weighted. Is actually a weighted Binner measure, a weighted Gaussian measure. And as I said, nothing here makes sense. This is purely formal, right? All these terms here, I mean, you cannot define the Lebesgue measure in infinite dimensions. But formally, if you could, Liu-Bill's theorem will tell you that this is volume-preserving and then the Hamiltonian is a conserved quantity. So you expect this measure to be invariant, right? I mean, the question is: how do you judge? I mean, the question is: how do you justify that? And in many cases, this has been justified rigorously. And so let's actually see a little bit of that. What is it? So in what cases this justification has been done? Well, first, let me mention that the construction of the measure is intimately connected to what is called the P plus 1D or Phi4 model. So dimension, if P3, we are with a cube. If P3, we are with the cubic equation. This is the Phi4 model in different dimensions. And this goes back to seminal work of, sorry, I need to be tracked of the time. Seminal work of William Jaffe, Leborino, Spear, Simon, Nelson, Einsmann, Sogan, and a lot of other people. And so in dimensions one and two, you can construct these measures for any piece. Okay, so uh for the qubit in dimensions two, this is the force the piece. Two, this is the course, the key for two models. In dimension three, you can construct sorry, and I'm losing your voice. I don't know what happened. I don't know if you were everyone or yeah. Let me actually see if I put headphones or is the connection. I think you were closing the microphone with your hand or something. Ah, so now it's very good. Is it good now? Yeah, yeah, okay. If not, I'll put the headphones. Yeah. So, uh, so this, so as I said, in that patient one and So this so as I said, in dimensions one and two you can construct this measure for NEP and in fact we are only in the defocusing case but in dimension one Lebois and Rosen Spear did it even in the focusing provided that you have some control on the mass. In dimension three however this is only done for the cubic equation this is the celebrated Fi43 model and it's expected I mean there is a very nice book by Bridges Frodek and so By Bridges, Frodek, and Sokal, in which you can actually read a little bit that is not expected to hold in dimension three for any other P than the cubic. And actually, his very recent work, very exciting recent work of Einsmann and Dominique Copen, who actually show that you have no phi4 theory in dimensions four. So in other words, if you are in dimension bigger equal than four, this cannot be done. This cannot be done for any p. Okay. And okay, so in the case of dimension two, let's actually focus on dimension two, the first line here. Then in dimension two, then the measure, sort of the Gibbs measure, is absolutely continuous with respect to the Gaussian measure, which is the distribution law of the random initial. The distribution law of the random initial data that corresponds to the one that I show you when alpha is one. So, in dimensions two, then this is the random data that you consider. So, this is essentially dividing by a mod k, you know, when k is not zero. And so alpha is one. Okay, so this is the random variables that we're going to see as the, we're going to consider in the background of this of this case. Background of this case in dimension two. And so, after the construction of the measure, of course, the next goal is to understand the invariance of this measure, the invariance under the dynamics of the NLS, and to prove that actually solutions exist globally for data in the statistical ensemble of this measure. And what is the main difficulty here? The main difficulty is that the statistical ensemble of this measure, in other words, Statistical ensemble of this measure, in other words, the support of this measure lies in very rough space. And so in dimensions D, the support is in H1 minus D, the dimension over 2 minus. So essentially, for each dimension, you peel off half a derivative. And so this only makes in one dimension you have functions, but in dimensions higher than one, you have distributions. And what determines the support of this measure is actually. Of this measure is actually a trace theorem that tells you when the Gaussian measure is countably additive. And so there is this trace theorem that tells you that this Gaussian measure is countably, the Gaussian part is countably additive, provided that you lie in this H1 minus D over 2 minus, which as I said is negative, the dimension is B or equal to 2. And so if the dimension is 1, you're still in HR. If the dimension is one, you're still in h and so you can actually have a deterministic local reposness. But if the dimension, sorry, if the dimension is bigger than one, and so actually in dimension one, this was done by Bougain for all p, the invariance of the measure. But if the dimension is bigger than one, then the only result that is known is up to date is the cubic case, which also was done by Bourne. Which also was done by Bourguen in 1996. Okay, but then you are already, the support of the measure is already just below L2, it's in H minus epsilon, if you wish. And so there has been essentially no progress in this particular problem since 1996, since Borgen work. And the almost sure global well poses, I mean, the global well poses for random data of the form that I show you. Show you and the invariance of the measure has been open for all other nonlinearities, not even for the quintic. Not even in the quintic case, people have been able to show that you have global strong solutions for data in the statistical ensemble of the measure. And so, this is actually the result that we were able to solve. We solved this problem by showing actually that indeed Showing actually that indeed in dimensions two on any odd power, so the cubic is Bourguen, but any odd power, the renormalized non-earwave NLS is almost surely global well posed in support of the Guys measure. And there exists the Borel set sigma of full measure, such that the global flow maps this set sigma into itself, is a one parameter group, and keeps the Gibbs measure invariant under the flow. Measure invariant under the flow. And so, so this is exactly saying that we have global strong solutions and the statistical example of the measure, and that the measure is invariant under the flow. And our point here is the uniqueness. I mean, that these solutions are indeed unique in the canonical sense that they are the limit of smooth solutions coming from canonical truncations, smooth or sharp truncations, which is the usual sense of. Is the usual sense of strong unions? And what I would like to say is that in dimension, when the P is 3, so the problem in the cubic, the problem is L2 critical, and the support of the measure is in H minus epsilon. So Bourguin's work was actually to try to bridge, was actually the first supercritical result ever proved, because he actually had to prove that if you take data below L2, Tell data below L2, right? That is supercritical data in H minus epsilon, then that you actually have solutions, right? And so he had to bridge this sort of like epsilon, go epsilon below the critical scaling. But if you take, for example, something like the quintic, actually, what we are saying is much stronger because we have to bridge the quintic equation in 2D scales like half, and so the support of the measure is still. The support of the measure is still in h minus epsilon, so we need to actually bridge, we need to gain more than half a derivative. And if the power is higher than five, then the scaling is going to be closer to h1, and so we have to breach almost like one derivative. And we are in Schrodinger. Okay, we have no smoothing. We're in the Schrodinger case. So let me just make some remarks. Some of them I just made. So weak solutions with So, weak solutions with sort of no uniqueness for this case and with some sort of like preservation of the measure in some other sense were obtained by Owen Toman. So, these are like weak solutions. And as I said, the only other case was Bourguin's case. And so, the reason why we expect Reason why we expect this to be much harder than, so sorry, and the other, sorry, what I was trying to say here is the only other case where the measure is constructed but not known besides the dimension two is the cubic in 3D. That's what I said before. And we expect that in the cubic in 3D, that problem is much harder than the powers, any other power in 2D, because actually the cubic in 3D is actually In 3D is actually critical, and something that we define that I'm going to explain at the end of the talk, which is a notion of probabilistic scaling. I mean, we're going to sort of posit that something, what matters here is a probabilistic scaling, not a deterministic scaling. And so the cubic in 3D is still open and is the only case of the measure in which is known to be to exist, but that invariance is not known. This is after our theorem and Our theorem, and of course, Bourguians. And the other problem in terms of invariance of measures that is open is in dimensions one is the cubic, which is also critical in our sense, a probabilistic sense, but actually here this is a completely integrable system, so it's a little bit of a different animal than 3D. And I should also say that there are cases, of course, there are results for other dispersive equations like. For other dispersive equations like the wave equation, but that's more advantageous because then you have some smoothing. In the wave equation, you have some smoothing, so I'm not going to discuss this. So I'm going to focus on Schrodinger. So let me talk a little bit about Burgundy's invariance argument. As I said, the lack of local well poses is due to the graph support of the measure. And so the problem that you have. The problem that you have really, the only problem that you have, is to establish the local theory. Because once after you have the local theory, then to prove the global theory, to prove that you have global solutions and that the measure is invariant, it's actually something that is a classical argument of Bourguin that roughly goes as follows. You take a parameter n and you sort of consider the truncated equation. So pi n here is the projection to frequencies less or equal than n. You truncate the or equal than n, you truncate the data. So this is the finite dimensional approximation of your system. And this is a finite dimensional Hamiltonian ODE that itself has a Gibbs measure, a finite dimensional measure, which itself is invariant. And so this finite dimensional measure is actually well suited to study global in-time arguments for the truncated system. And so if you can prove the If you can prove the almost sure local well-pulseness, then with high probability, you can bound the solutions to the truncated system, to this system, uniformly non-short intervals. And then what you do is you use the invariance of the finite dimensional measure to continue these local in-time solutions for the truncated system to global one. So this measure is invariant, so you you use it you use the invariance of this mu n as a consideration. Variance of this as a conservation law, if you wish, to extend to global solutions, the solutions to the finite-dimensional approximation. And then this long-time bounds, which are still uniform in n, there is sort of an approximation argument, will allow you then to define global strong solutions to the infinite-dimensional system by taking limits in two dimensions, say in H minus epsilon. So, so, and then once you So, and then once you have these, then you take limits in the measure, sort of weak limits in the measure, and then you get from the invariance of the mu n, you get the invariance of the mu. So, in other words, the key is actually this, is to actually get this local velposeness. And once you have that, more or less the argument would follow. That's the key and the core, the hardest point. This point. And what I would like to remark is in terms of the uniqueness is that, okay, we use these projections, but you can replace that by any sort of smooth approximation or sharp truncation. And the limit U does not depend on the choice of the truncation. U is the unique limit of any canonical approximation. So that's the strong uniqueness. All right, so now how do we study the almost sure? Now, how do we study the almost sure? Okay, I need to hurry up. How do we study the almost sure local well-poseness? So, okay, let me actually talk a little bit about this with ordering point. So, as I say, we need to prove, how do you prove this? Well, the first thing is that this is our random data, which is supported in H minus epsilon. I mean dimensions two. And you have first two difficulties. One is that you have infinite mass because you are below L2, and the other one is You are below L2, and the other one is that you are relative to the deterministic scaling, you are supercritical, you are below L2. And so, the infinite mass implies that the expectation of the potential energy is infinity. So you have a little bit of a problem here. And also, you cannot make sense in the sense of distributions as it is of the nonlinearity. It doesn't make sense almost sure as distributions. Make sense almost sure as distributions. And so, in some sense, you need to remove, you need to re-normalize the non-linearity to remove these infinities. And this is what is done even for the construction of the measure through this weak ordering process, which is actually quite common in quantum field, constructive quantum field theory. So, I put a slide here, which I don't want to spend a lot of time with, but essentially it explains how is this done. Done. I mean, you know, the point is that if you take the expectation of the L2 norm of the truncated mass, then this is kind of logarithmic in n. And so you sort of reorder these powers accordingly. So for example, I give you some examples. So the reorder cubic will look like the cubic minus two times the log n times u. And And in the Hamiltonian, you will have a power 4, so you will reorder the Hamiltonian according to this, where sigma n is always this log n. And actually, what you can prove is in two dimensions, you can prove that the limit of this finite dimensional order nonlinearities converge almost surely in the sense of distributions. So, let me actually move along and say, okay, so what I would like to do is I would like to tell you how we prove. Do this, I would like to tell you how we prove our theorem. But before that, I want to take a little bit of a trip, a detour, into tell you what were the cases that how people did it before, how Bourguéne did it, and what were the cases available, and so that you see why sort of we could not proceed in the same way and we had to do something different. So, the main idea of Bourguen for the cubic one, this is a paper of 19. This is a paper of 96, and also there is something of a similar nature in the stochastic context, which is called due to La Prato de Busch and is from the early 2000s, is that what you want to do is you want to do a linear, nonlinear decomposition, where the linear part is rough but is random. Okay, that's what's coming from your initial data. And then the nonlinear part is smoother. And so, in both cases, in the Bourguéne or the Prato de Bouche, the method. Of the Prato de Bouche, the method can be understood as constructing solutions that have a particular structure, a particular affine structure, the line in this sub-manifold, which is center in the random evolution of the linear, random evolution of the random data, the u-linear, that's what I call u-linear, plus a ball that sits in a smoother space. And so this is exactly So, this is exactly what Bourguin did. What he said was, well, even though the data is in H minus epsilon and it's supercritical, he was able to construct solutions that have this particular form, this particular form, where this is actually the center and this V lies on a ball in a smoother space. So, this is very rough, but it's actually random. And actually, if you subtract. And actually, if you subtract from U this linear evolution, then you can place, you can think of this V as being, you gain regularity. This V is more regular and therefore puts you in a sub-critical regime and you can treat it somehow by deterministic methods. And so this is what allows him to do is actually the multilinear latch. Uh, this the multilinear large deviation estimates that I mentioned before, then you allow you to recenter the solution around this linear evolution of random data. You can also separate more than just the linear evolution of random data. You can separate higher order iterates of the random data and then put a smoother piece. And then conclude that if you take the difference between the solution and this linear evolution or the linear evolution plus higher order either. Or the evolution plus higher iterates, then you get something that is more regular than that would be originally dictated by the weaker regularity of the initial data. And so essentially, this is what I'm saying here, which is that the exact Gaussian structure of the linear evolution of random tells you that interactions of this with itself or interactions. This with itself or interactions of this with V are actually much better than if this was not running. Okay, but the point is that it doesn't matter how many Picard iterations, I mean, even if you separate more than just the linear evolution of random, if you say want to separate some higher order Picard iterations of interactions of the U-linear with itself, then there is a point in. Then there is a point in that it doesn't matter how much you separate, you will stop gaining regularity. I mean, it will take you up to a certain point of regularity for B, but then you will not gain anymore. And so, if you want to attempt this, say, for the equintic into D, it's not enough. You cannot grab what you need in D to be able to close your estimates. So, in general, it will not give you optimal results except in the cubic into D. So, beyond the method of Bourguard, there is this other. So, beyond the method of Bourguin, there is these other two methods that were actually done recently, which were the method of regularity structures of Martin-Herr and the method of para-control calculus of Mubinelli, Keller, and Perkovsky. And so this goes back to stochastic parabolic equations, forced equations, like the KPZ or the parabolic 543 model. And the regular destruction method is based on the local property. Is based on the local properties in space of solutions at fine scales, which is very suitable for parabolic equations. And what it does is it builds this general theory of distributions, which include the profiles that come from the noise. And they allow you to perform multiplications in this, it allows you to perform multiplications that otherwise wouldn't make any sense in the sense of distributions and therefore analyze the nonlinearity. Therefore, analyze the non-linearity. So, I don't want to say too much about this because what is closer in spirit or what is the precursor in spirit to what we are doing is more the paracontrol calculus of Vinelli and Keller and Perkovsky. And so, in this theory of paracontrol calculus, the idea is to go a little bit, is to go back to the method of Bourguin and Daprato de Bush and identify. So, you said you have the linear evolution of random plus a smoother term, and what you want to do is you want to. And what you want to do is you want to identify in this term that has a higher regularity what is actually the term that is the worst. I mean what inside this V has the lowest possible regularity? And what you would like is you would like to separate that and move that to the center. Now, what you can see is that such terms only come from interactions between the high frequencies of the random evolution of the data. Random evolution of the data, right? The high frequencies of this, that is very rough, and the low frequencies of D. So those are the terms that give you the lowest regularity. And so, roughly speaking, then what you want to do is in addition to the random linear evolution and some higher order of that, what you would like to do is you would like to. So, these kind of things, when you take the linear evolution of random data and higher order iterations of the linear. Iterations of the linear data. This is what formally we call the trees. What you do is, in this paracontrol theory, is you want to move to the center one more term, a new center, which is coming from what I describe here, these interactions, and it's a term that is essentially para-controlled by this Gaussian term. So these linear evolutions of random data and higher order ones. Random data and higher-order ones have a particular Gaussian structure. So, what you will want to do is you want to identify the terms here that have the lowest regularity and also that can be somehow para-controlled by similar expressions as you already had in the center. And I will explain this in a moment. So, what this means is that what this is saying is that in order to actually have a decomposition, you don't need to have Position, you don't need to have a specific structure in the center, which is a multilinear expression of the Gaussians. That's what the linear evolution of random and the higher order expressions give you. It gives you a particular explicit multilinear expression. But actually, you can also throw into the center terms which can be paracontrolled by multilinear Gaussians. And so, a quick definition of paracontrolling, this goes back to the Bonit-Para product. Bonnie para product. And so what it says is that you say that a function f is paracontrolled by g, so this function f is para-controlled by g, if up to some sort of like a smooth remainder, f looks like the para-product between the high frequencies of H and some auxiliary function, sorry, the high frequencies of G plus some auxiliary function H. So in some sense, So, in some sense, it's saying that you could think all these terms essentially behaving like the G. And okay, so this is the usual decomposition of paraproducts, and usually you see this is the worst term. Okay, so this is what you want. And so, here is a quick example in the stochastic, which is for the cubic, the 543 model, in which you have the parabolic semi-group, you have a cubic equation, and then you have a renormalization. And then you have a renormalization plus xc, which is the space-time noise, is your forcing term. And I'm going to ignore here. The renormalization, of course, is very important, but just schematically, I'm going to ignore it. So what is the Duhamel here? The I, I'm going to call this curly I, is going to be the Duhamel. In this case, it's going to be the Duhamel for the heat kernel. And Z is the solution to the problem in which you are applying the heat kernel to the node. The heat kernel to the noise, sorry, to the to the z, and then you get the noise. So, z is the duhamel of the noise. And so, modulo, some details, essentially what the paracontrol theory tells you is that you can write it, you can do this decomposition, you can look for solutions that have the following structure. You put, okay, this is where it corresponds to the linear evolution, so you put the z or any higher order, it iterates orders. This is kind of like the center of Bourguin, but you can also put here. Borgen, but you can also put here this term, which is the para-product between some higher-order iterates of the z plus the remainder. And the point of this term is that if you were to try to do the product, just the regular product between these two terms, that would make no sense. But because you have this para-product, this makes perfect sense. But not only that, it behaves essentially as the Z. It behaves like it has a Gaussian structure. It has a Gaussian structure, and then you and then immediately you obtain a smooth remainder. And so, in terms of regularity, in this case, the Z since they use Helder classes is in Z minus a half, the Duhamell of the, so three, this is cubic, so you have three Z is in C one half. So, this is the same as Bourienne de Bouche and the Bourienne and the Prato de Bouche, and then. Bourrienne and the Prato de Bouche. And then this term, the green term, is the paracontrol one. As I say, the point is that this is not just a product. But I'm repeating what I said before, that this term actually makes sense. You can actually put it in C1. Well, if you were just having a product, then this P2Z has regularity below C minus C. Below C minus one, and this has regularity below C1. And so this product does not make sense in the sense of distributions, right? Because you need the sum to be bigger than zero. But if you take this term, this paraproduct term, then it makes sense and it's actually much better. It's in C1 and it behaves essentially like it has a Gaussian. Essentially, like it has a Gaussian structure. This term behaves like this, which is very important. And once you have done that, then immediately you use the strong smoothing of the heat kernel, and you can put the remainder in some very smooth space, like C5 quarters. But you already can see that in terms of Bourguin, the solution belongs to a random sub-manifold, which is much more non-linear than Bourguin, right? Linear than Bourguin, right? Bourguin has this first term and the last one, and here you have actually more sophisticated decomposition. Okay, but let's go back to the 2D Schrodinger and the 2D Schrodinger problem that we are interested in with powers bigger than five. Compared to the heat equations, first of all, we can only use sovereign spaces because Holder spaces make no sense. The Schrodinger kernels are not bound. The Schrodinger kernels are not bounded there. And the other problem that we have is that, unlike the wave equation or the heat equation, the Duhamel evolution for Schrodinger has no smoothing whatsoever. So how are we going to go in the quinty between being sitting at critical h a half to trying to reach h minus epsilon? So we have to bridge this gap that is larger than half a derivative in the quintic and is almost. Quintic and is almost one full derivative if p is larger. And so if you were to follow Bourguin's approach and write it in this form, like linear evolution plus V, then what happens is that then this V, as I said, is not regular enough. The best that you can do is put this V, this is below L2, the best that you can do is you put this V in a space that is below H. A space that is below half. So this space is not regular enough because your scaling is h half. So this space is still supercritical. And so you cannot close the estimates using Boudin. But what you can do is you can show that the poor regularity of V actually comes only from, again, the high-low interactions, like the ones that we discussed in the paracontrol theory. And so what you will do is you say, And so, what you will do is you say, well, let's try to proceed like in the para-control theory. Let's try to identify a term x, which behaves like a para-product, the duhamele of a paraproduct, between the linear evolution of u and the powers p minus 1 here of this, the low frequencies of this. So, the high frequencies of this and the low frequencies of that. But now the Duhamel, the I that we have is the one for Schrodinger. Finger. And what you want is to hope that this x has a, you say, can I find this x so that this x has a structure, a Gaussian structure than the linear. And then you want to say, well, I want to identify such an x, hope that it has a structure than the linear, and that if I subtract from the solution, not just the linear evolution of higher order iterates, but I also subtract this term, then what I remain, what I get is smoother. Smoother. But the problem that you have is that in order for x to be para-controlled by the linear, then you need to have reasonable bounds in the low frequency components of u to the p minus 1. But the low frequency components of u to the p minus 1 contain themselves the components of the x to the p minus 1. dx to the p minus one because you are doing an iteration so if you want to understand this you have to understand dx and the regularity of x is is is still as i said approximately h below h half which is supercritical and so there is no way of controlling this powers p of x to the p minus one assuming only the regularity you cannot control that and this is in contrast with the heat equation and wave equations where you can do that you indeed can do You can do that. You indeed can do it because you have smoothing. Here, you cannot. And so you're stuck. You cannot do what people in paracontrol theory do. And the main difference is that actually what you have to do is, I cannot just start placing norms here like they do here. What you have to do is you have to put your hands into the X and actually zoom in and unveil in a much more refined way what is actually the structure of this X. What is actually the structure of this x? What is the structure of this term? So, how can this be done? So, one way is to do sort of like inductional frequency. And what you would like to do is you would like to say that in order to validate the behavior of x, of this x, and by validate, I mean that this x behaves like, has a Gaussian structure, has behaves like this, and also has some. And also has some approach estimates that I will prove below, some estimates I will prove below. So, in order to validate the behavior of the low frequency parts of this X, you want to do it so that you validate the low frequency components before you go into the high frequency components. So, for example, let me rewrite the para product in this way. So, the projection into frequencies n of the x that I show you is just the sum over the frequencies l less than n of the Duhame. Frequencies L less than N of the Duhamel of this. This is the high frequency and this is the low frequency, high and low. But in order to validate the behavior of this term, the main difficulty comes from understanding the low frequencies here. You need to actually really understand what's going on with the low frequencies of this term. So in other words, I need to validate what is the behavior of P L of U. So in order to understand Pn of X, I need to understand P L and Need to understand P L and P L of U contains itself a piece, a P L of X. So, in order to understand Pn of X, I need to understand P L of X. So, I need to first validate that. Now, there is one thing that I say that I want to make a point is that in all these things, of course, there is a lot of multilinear estimates, like large deviation estimates that you want to make. So there is a little bit of a trick here because you need this piece to be independent of this piece. This piece to be independent of this piece. You need some independence between these two terms. And so there is a little trick in which you replace, you replace here, let me explain here. So you want independence between this high frequency and the low. And so what you do is you replace the low frequencies, the p minus one low frequencies of u by actually the solution, the p minus one power of the solution. Power of the solution where you take initial data that is very, very small frequency. So you do, there is a commutator estimate there, and that allows you to gain this independence that you want to be able to use the large deviation estimates. And the reason is because the key point is that when you use this term these solutions, these are measurable in functions of the Gaussian of K much less than n, and so they're automatically in. And so there are automatically independents with the Gaussian at order n. And this is actually an idea that goes back to the work of Brinkman on the derivative in LS. He used it as well. So anyway, let me go back to what I was talking here before. And so at this point, what you would like to do is you would like to do a recursive reduction, namely to write P and X as a multilinear expression of the low frequency component. Of the low frequency components of X up to some remainder that presumably you can control, and then iterate and write the low frequencies of X as an expression of even the lower frequencies of X until you reach one. However, if you do this iteration process, you will have these very complicated tree expansions, which are a crazy amount of combinatorics, which actually are quite overwhelming. And this is something that maybe you can try by hand. Maybe you can try by hand to do in the quintic case, but if you want to do it for powers higher than five, then it's just completely out of control. So, what we do is we sought a different method, sort of like instead of doing this by hand, we sought sort of like a unified method to do this. And so, how do we capture the implicit randomness? I mean, so our goal is to capture the implicit randomness in this term P and X. In some nodes, In some norm that propagates, and by propagate, we mean that if you can prove bounds for the low frequency part, then you can prove bounds for the high frequency part. That's what I said before. And that in terms that it will allows us to do an induction in frequencies from the low frequencies to the high frequencies. So, clearly, as I said many times, the fact that this term is in H minus is not enough to do the job. Job and so, what we have to do is we have to actually change the paradigm, change the point of view. And instead of thinking of this p and x as a random variable and trying to put a norm in it like they do in the parabolic case, we actually think of this not as a random variable but as an operator. We take this term and we think of that as an operator which is multilinear in the low frequencies and is acting And it is acting on the linear evolution of the random. Okay, so we think of this as an operator. And as I said before, in the paracontrol theory, thanks to the smoothing of a thick kernel, they don't need to do this. They just put the norm in this and they are done. But for us, treating this as an operator acting on the PNL will allow us to study this and understand the structure of this as a multilinear operator. As a multilinear operator on the low frequencies, will exactly allow us to run the ANZAT, the inductive ANZAT that I sort of explained before. So this operator PNL is actually depends on, as I said, on multilinear expressions of P of the low frequency, and that's why we view it as a random operator. On the other hand, this is basically a paraproduct structure. It has a paraproduct structure. So, the effect on the projections to frequency n. So, you could think of this operator, if you wish, as a weighted average by the low frequencies of the high frequencies of this. So, in other words, this operator is like the weighted average of these by the low frequencies. And so, and so And so, and so, and this is key, right? I mean, this is key to propagate the probabilistic bounds. So, the question now is: what are the right norms in which you want to study these random averaging operators? And so you can do many candidates, but actually, there is two very simple and beautiful norms, which is just simply the operator norm as an operator, as a PL operator from L2 into L2, and the Hilbert-Schmilt norm. And the Hilbert-Schmidt norm, which is more like the matrix norm. And of course, we don't treat them as operators from L2 into L2, but from operators in sort of the Fourier restriction spaces, the XSP. And so there is two a priori bounds that you want to propagate, the operator norm, essentially, which is this, a priori bound, and then the Hilbert-Schneid norm. And the Hilbert-Schneid norm just tells you that you are in H1 minus, which you cannot use directly. But the operator norm is actually the key that would allow us to propagate this in here. And why is these two norms are important? Because if you were to try to use general functions instead of h half minus, so in the x one half minus one half in the x sv as the low inputs in the in the p and L, then you will never be able to prove this. able to prove this uh to prove these two estimates so so so in some sense this this operator and this bounds really capture the randomness structure of the low part which is actually the the new q ingredient here is understanding exactly the randomness structures of the low frequency part and these two norms are actually very easy to use and so here is our answer our answer is that now we write u as the linear evolution of u plus the linear evolution of U plus the sum over L n of the sum over L less than n of these random averaging operators acting on the linear evolution plus a W and the coefficients here of these operators are independent of this so you can use all the multilinear Gaussian estimates all the large deviation estimates and then the remainder actually you can actually put in a space which is actually almost h1 Which is actually almost H1. Okay, so when you separate this, this is kind of like order H minus, but then the remainder you gain before in Bourguin, remember that I said that this is still in H half minus, but now this is almost in H1. Okay, and so schematically then the proof goes as follows. We induct on frequency and we show that the P and L satisfied our approach bounds and the W is bounded in H almost in H one. Bounded in H almost in H1, and then you do an induction. You suppose that for freedom. Can I uh sorry, stop you for one second? So the P and L the way you defined it, it is not only defined in terms of the initial data on your previous slide, right? No, I'll fix it in a moment. I'm putting a white lie. Okay, okay. I'll come to that in the next two slides. Yes, you're right. So, but let me let's stay with this. Let's stay with this. So, then you suppose that this is true for frequencies less than n, and that the P L can be written as the sum of the linear evolution of the linear plus the lower term frequencies lower, r lower than l of this, which now you know that this behaves like the, has a Gaussian structure and behaves at the u-linear, plus the term remainder that is in H1. And so then the coefficient. And so then the coefficients of this operator, as I said, they are going to be multilinear expressions of this. And so you can prove those bounds by using the multilinear Gaussian estimates, counting lemmas, and the ideas of Bourguin that go back to our like TD star type arguments, which will tell you that then the bounds, assuming that you have the bounds below n, then the bounds are true at n. And then for the P and L, you solve this by a fixed point argument. This by a fixed-point argument. And essentially, this completes the proof. And so, let me actually now address a little bit what I think Sakhari is asking. So, I said that this was the answer, but let me actually, let me give a name. Let me call this interior sum. I'm going to call this interior sum Ln applied to u linear. So, so let me give you a little bit more detail. So, let me call this interior sum. So, here L is kind of something that goes to Is kind of something that goes to so the L the low frequency goes from one up to some power of n, one minus delta, and this delta depends on the power p of your NLS. Okay, so suppose that you take that integer sum and I call it ln of w, and this is essentially the du Hamel of the paraproduct, right, of the linear evolution of the this is the this is what what I said was our random averaging operator, right, with the low frequencies here. With the low frequencies here. But the true answer is not really this. What you do is the true answer is you write your solution as an infinite iteration, as an infinite iteration of this PNL. So if Ln is this, you actually do an infinite series of these operators applied to the PNL, and then you have a remainder. And so if I call this sum, this sum and This sum, this sum on here, if I call that psi n and one minus delta, then this is like, you know, like a Taylor sum in ln. So essentially, this psi n delta can also be written as the inverse of the identity minus ln, okay, applied to this operator. So this psi n is essentially the linear evolution of random data plus the actual random average operator is Ln applied to this. Is ln applied to this, psi n and one half? So it's an infinite iteration of this, not this. Okay, I think this is exactly what you were asking, right? And so the answer of u, the answer really, is the linear evolution of the random data plus this random average operator plus something smooth. So let me tell you something more about this in a picture, which you will relate better. So what is dip ψ n? What is it that I just said below? Psi n, what is it that I just said below here in this line? Is that this psi n is essentially the blue here is the linear evolution of the random data plus the Duham L of the Ln in which you have in one entry you have the same psi n L and then all of the other ones have the low frequency. So this is like the Quinty case. You have low frequencies Pn 1 minus delta of U. And so, but this is the same by this Taylor expansion as By this Taylor expansion, as putting the linear evolution, and then you take the first iterate, the first term is the Duhamel of putting the linear evolution here and then all the low frequencies. And then you put the Duhamel of the Duhamel. You replace again in the first component the same term in which you put the linear evolution and the lower order terms. And then you do it again and again and again. So this is the infinite iteration. Infinite iteration. And so, and these two things are the same. So, it's what I'm saying here: that this and this are the same. And so, this is the picture. Sakhar, is this what you asked me? Yes, yes, in some sense. But I have then just a clarification. So, is there some sort of contraction on W that is the variable that you want to write? The contraction will go on W. That's essentially I went too quickly, but I said that in the W since it will contain. The W since it will contain only the high interactions because you somehow put all the bad high low interactions in the random average operator, then you apply contraction mapping to prove that it's in H1. It's kind of like pretty smooth. But Pn of P and L includes W in it. It also depends. W, no, W is outside. Ah. W is outside. So W would be the review. So you see, Bourguin had linear evolution plus V, and what we are doing is we so V and what we are doing is we so paracontrol peel from the V the paracontrol term and what we are doing is we are peeling from V this random average operator and then the remainder we call it W. I see. So I don't understand the definition of ln of w on the first line there is a u in it and the u depends on w. This is kind of like the low frequency these are actually the low frequency scheme so it's not the you have to get all the right estimates before you can contract this is you do this at the Can contract this you do this at the very end. Let me say that you the u contains w, but this is like low frequency of w. So you already have like the low frequency w is already under control. It's under control. Okay, okay, I see, I see. So really the contraction is on the high frequency of W. Exactly. In some sense. Okay. Yes, just the high interactions. Just the high, high interactions. Yeah. Yeah, that's what we need to control. Okay, thank you. So, um You so, um, so let me say one more thing about this, which is actually what I want to emphasize. The beauty of this, what we think is beautiful, is that we are telling you, we are giving you information about how randomness has transported. So you start with random data, then you have the linear evolution of random data, and then what is the next term? What we are telling you is that the next term, what is this psi n1 minus delta, is actually a twisted gauge. It's actually a twisted Gaussian. What we are telling you is that this random operator, so if you want to re-understand these random operators, they are like actually a matrix acting on a Gaussian, which is what we call this color-color-Gaussian. So essentially, here I explain you how to find this matrix. But essentially, what we are telling you is that after the linear evolution, the next term is a twisted Gaussian. And it's a twisted Gaussian by a matrix that if you look at the estimate. That if you look at the estimates that you prove, and I want you to focus on this blue one, what we are saying is that the Hilbert-Schmidt norm of this weighted Hilbert-Schmidt norm tells you think of L for a moment as being like one. And K is of order N. And kappa is a very large number. So what we are telling you is that it's a twist of the Gaussian, but with a by an affected by a matrix that is kind of like almost diagonal. So we're getting. So we're giving you a description to first order, say, after the linear evolution to the first order of how the randomness has propagated. Okay, so and it's a very precise description of how randomness has propagated beyond the linear evolution to the next term. So let me actually say something else about this, but before that, Something else about this, but before that, let me actually tell you in the last, I'm running out of time, but let me tell you this idea of the probabilistic scaling, which is very, very important into where are we going with all of this beyond proving the invariance of the measure and solving this open problem in dimensions too. Where are we going with this? So, in the background. This. So, in the back of our heads, is always this thing: how does randomness propagate? So, what we want to posit is that the actual scaling lenses to use are what we call the probabilistic scaling and other deterministic scaling. And so let's just explain this quickly. So, I said many times in dimensions 2, the critical threshold is a half. And if p is large, the criticality is 1 minus 2 over p minus 1. is 1 minus 2 over t minus 1. And so in our result that I just said, we have gained in dimensions 2, we have gained at least in the quintic case more than half a derivative, which is amazing because Schrodinger has no smoothing. And if p is very large, we have essentially gained a full derivative, which is again amazing because Schrodinger has no smoothing. And so why is this possible? So here is a heuristic argument that tells you that what we did is not just possible. You that what we did is not just possible, but it's actually can be improved. And so consider the equation with, let's say, the deterministic setting, and suppose that you take data with a single mode, single scale n, and bounded in HS. So where S is say alpha minus d over 2. The alpha is the alpha malph. And so if I would like to have any sort of local position, To have any sort of local world poses, then you expect that the guiding principle is that the second iterate of this, namely this formula here, should stay bounded in the same subler space. So what does it mean that this is bounded in the same subler space? Well, you take the Fourier transform, you write what it is, and then essentially, you know, what you need to bound, the size of the Fourier transform of the second iterative is like n to the minus p alpha times the size of this set, of the set of Of this set, of the set of the k1, kp that I'm summing here, such that they are in the ball of radius n, and such that this quadratic expression is like m. And so if you do it in dimension counting, the size of this, the carbonality of this set is essentially like n to the p minus 1 times d minus 2. So if you want, that's what you put in here. So if you want to stay in HS, essentially, if you want these two to stay bounded, essentially what you need, okay, you do your calculation. Okay, you do your calculation, tells you that your s has to be bigger than the deterministic scaling. But what's the point of the random data? If instead of taking this data, I put some random variables here, then the point is that then instead of having here the supremum of Sm, you have the supremum of Z to the one-half. It's kind of like you gain half, you gain a square root, right? So what I'm saying is that if you put n to the minus alpha, That if you put n to the minus alpha, I'm almost done, okay, to the to the random variable, then and you do the same calculation, then you have a square root gain, which is kind of like what happens in the central limit theorem, which says that then in here, instead of having this, you have this to the one-half. And so when you say, well, I want this to be bounded in the HS norm, then the calculation tells you that instead of being bigger than the critical scaling, that says that I have to stop. That says that I have to stop. Then you need to be bigger than minus one over p minus one. And this is what we call minus one to over p minus one is what we call the probabilistic scaling. And notice that this is independent of the dimension. But in particular for the quintic, you see for the quintic we went from one half, the criticality one half, we went all the way to h minus epsilon. But what this is saying is that we should be able to actually to go to h minus equal. To go to h minus a quarter, okay, which is which is the scaling. So there is room to improve this. And so we have this conjecture that says that actually we can do this, that actually in the local world process, we should be able to go to any sort of scaling that is above the probabilistic scaling. And this probabilistic scaling, when p is very large, right, the probabilistic scaling approaches h minus epsilon. Is H minus epsilon, so the theorem that we prove in 2D is asymptotically sharp in this limit. But the other thing that I should say is that it fails to reach still in 3D. We cannot solve steel with this, the measure in three dimensions, because we are right at the edge. So in 3D, in the cubic, the probabilistic scaling is minus one half, but the measure is supported in minus one half minus epsilon. In minus one-half minus epsilon. So we are still epsilon away. So even if we can prove this conjecture, we are epsilon away. So what's the point of the conjecture? It's not that we can go so lower regularities, that in order to prove the conjecture, we need to actually take now the w and understand the random structure of the next term in this w. So in other words, okay, we understand evolution of random. I just told you how the next term looks like, which is these twisted Gaussians, this These twisted Gaussians, these colour Gaussians. And then, if I want to prove this, if I want to prove this conjecture, we have to understand what happens in the W. We need to peel off from the W the next worst regularity and tell you exactly how randomness propagates next. Okay, to have a description of how randomness propagates to higher orders. That's what it takes. And that's why this is interesting and what's the beauty of this. And so, let me finish with the following, which is. And so let me finish with the following, which is a byproduct. If you could prove this conjecture. So if you could prove this conjecture, then as a byproduct of the proof, what you can prove is this is a question that both Einstein and Tristan asked. So as a byproduct of the proof, what you will have is that for smooth and well-prepared random data, for example, the random data that arises in the derivation of the wave kinetic equation. Of the wave kinetic equation, the time of existence is actually longer than the deterministic one. In other words, there is no energy cascade until a very long time, longer than the deterministic one. So what that means is that randomization effectively extends the time of the perturbative regime. So more precisely, here is an example. Suppose that you take the cubic equation with, and your prepared data is that you take a single frequency. Is that you take a single frequency, a single scale, and then you take Fourier modes that are uniformly distributed in the ball of radius n. So you take a well-prepared data of frequency n, and you say that the HS norm is of order one. You're in the qubit case. Then in the deterministic case, the first energy cascade happens at a time scale of the continuous resonance equation of the work, as in the work of Faux, Germain and Hani, and Batmaster, Germain, Hani, and Chateau. Master Germain, Hagyen Chateau, which shows that it occurs at a time that is like n to the 2 times s minus the deterministic scaling. On the other hand, if the conjecture is true, we are saying that in the randomized case, the first energetic scale is going to happen at a much later time scale, like n to the 2 times s minus sp. And remember that this sp is minus 1 over p, minus 1 over p minus 1. So this is this. Over p minus one, so this is this is larger than this, okay. And with that, let me stop here. Thank you very much for your patience and attention. Thank you, thank you very much, Andrea, for telling us about this very nice and impressive result. So now I'll open the chat for questions. Yeah, Alex, I see you all. Yeah, so in this last slide, I actually have two questions. This last slide, what happens if they take Questions. This last slide: What happens if you take S to be large? You cannot improve the time of existence just by taking S large. Larger than S P. Well, let's say that S is 10. How does this claim work? So let's say the test was just large. You're not worrying about... You asked me to tell you how we would prove this from the conjecture if we prove the conjecture. No, but what I mean is, let's say you're looking at, let's say you're not worrying about the low regularity. You just think that you have. No, it's not a low regularity. Here, S is. Here, S is bigger than is. I'm not the S here is any S bigger. It could be as large as you want. At least this S is smooth, is a smooth data. I see. So if S is 10, then you get a really long time of existence. That's right. If S is 10, you get much larger. I see. And also, my second question about this concept about the probabilistic scaling, which feels quite right to me. If you compare now, there are lots of results that. Compare now with all there are lots of results that are of the type that, let's say, an equation is critical in L2 and we prove local regularity in a supercritical regime, let's say H minus one, H minus a little bit. If you look at them from the point of view of probabilistic scaling, they become all sub-critical, I imagine, right? They become, that's right. That's the idea that somehow the deterministic scaling is not the right scale. Scaling is not the right scaling to measure up the notion of criticality. This is the right threshold. Other questions? May I ask a question? Yes, there are two questions actually. And the first is the connection with the approach by Burk and Svetkov on the wave equation. I mean, is there a connection? Wave equation. I mean, is there a connection or the result is comparable? The original Berkenzelkov or the more recent one with the Guvineli, which with the original. I am thinking of the old one. The old one, they just do Bourguin method, is Bourgen's method. But the one that shows that if you slightly change, then you can go beyond the critical. No, no, but it's the same. Critical, uh, no, no, but it's the same as Bourguard, okay. So, so, so, so, so they're doing there, they're in the wave equation, right? And they actually go supercritical relative to the deterministic scaling, which is exactly what Bourguin had done before. Bourguin, that's what I said. Bourguin was in the cubic and the lesson 2D. The deterministic scaling is L2, and what he proved is that you have local velposeness in the probabilistic sense below L2 in H minus epsilon. Do in H minus epsilon. He went supercritical, and he was the first one to do that. And what in the papers of Perkins had covered for the wave, essentially they also go supercritical in the same sense using the same method. I see. Thank you. Other questions? Can I ask a question? A question? Yes. Yes, please. Go ahead. Yes. Yes, the estimate that is here on the slide is Estimate that is here on the slide 36, it seems to be to have a strong probabilistic scaling in some sense, no? Yes, it's very strong and it's actually very strong in the sense, it's strong in the sense that, I mean, it's what you get from the just basic, this is not from just this basic estimate here, right? I mean, it's just a root. So it's not, we are not doing anything than just the classical large deviation. Large deviation estimate. And you immediately get this scaling here. So if you're interested just in the heuristic argument, of course, you have to prove the conjecture. In order for this to be settled, we need to prove the conjecture here. Yes, but I mean, if you are interested just in observables, like actions, probably you can obtain a more optimistic bound or something. Obtain a more optimistic bound or something. You see what I mean? In many situations, just observables. So it's a weak. In a sense, I think that what you're telling me is that you might not want to care about this very rough data, right? Going so rough. First, this is not, I mean, it's not that rough for low frequencies, for low piece, right? Sorry, for high piece. But the point of this. The point of this is that actually, how would you actually try to, what would you like to, what is that is going to force you to do this? What is more interesting is, okay, not that you can get rougher, but actually that in order to understand, it's more the method than the result, is that in order to be able to do that, if you follow what I did, what I was saying is that now we have to take the next step, which is to have tell you what happened next with random. What happened next with randomness? I thought, you know, we went one term and now we have to go and put our hands in the remainder, and actually, that remainder is still