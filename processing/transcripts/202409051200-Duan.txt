The data without any supervision of labels. The representations learned by self-supervised learning could be useful for many downstream tasks, actually any downstream task. For example, unsupervised clustering or linear classification, etc. Here I'm showing a very simple example in two-dimensional so that it's easy to visualize. Here we have visualize uh here we have the no i'll go the traditional way okay so here we have the input data x in a two-dimensional space that has the structure of this noisy two circles and we will be very happy to learn some representations that look like this i'm still plotting all the experimental units with respect to the original space of the input but colored by the Input, but colored by the one-dimensional representation we learned, we can see that on this inner circle, we have this representation close to zero, and we have the outer circle with a larger representation learned. And using these one-dimensional representation demonstrated in the colors, we can do downstream tasks. For example, clustering, maybe we can get something look like this, which looks pretty well. And we know that. Looks pretty well. And we know that if we do some simple k-means on our original data set, we could not expect to get such things that look so good. So in self-supervised learning literature, one of the widely used kind of approach is contrastive learning. For example, this paper in 2020, in this method, we need to first specify positive pairs and negative pairs of the Pairs and negative pairs of the samples, where the positive pairs are basically those objects that's supposed to be similar. Taking image data as an example, looking at these cute images, a cat image could be similar to another cat image, so these could be a positive pair. And a cat image is very different from a dog image, so these could be a negative pair. And in image data applications, the positive pairs are the positive pairs are typically obtained by data augmentation, like rotating or cropping the image. And the negative pairs are typically generated by randomly pairing from the full data set. The contrastive learning methods learns the representations for these experimental units by explicitly minimizing a loss that encourage those positive pairs to have similar representations. To have similar representations and active pairs to have different representations. Another more recent type of approach in self-supervised learning is the non-contrastive learning. For example, these several papers from 2020 and 2021 compared to the contrastive learning, instead of pushing the representations for the negative samples away, they choose to control the overall diversity of the representations across the whole. Of the representations across the whole data set, which does a similar thing to the contrastive learning, but it still used the generation of the positive pairs that we explicitly pull the representations of the similar objects to be similar. However, this generation of the pairs of data could be challenging. Thinking about the image data, it's quite straightforward by just cropping images, but looking at other types. But looking at other types of data, especially those with some structures, it could be difficult. Therefore, our goal here is to propose some novel methods that avoid generating those positive or negative pairs of the samples. Additionally, these self-supervised learning I mentioned are optimization-based methods that specifically interest in finding a point estimate. Finding a point estimate for the representations. So, we would like to have some full probability inference so that we can propagate the uncertainty in representation learning into the downstream task. And it could possibly have a better out-of-sample predictions also. So, before I introduce our proposed method, I would like to go through this V-correct method, which is one of the most recent non-contrastive learning self-supervised learning. contrastive learning self-supervised learning method this is inspiring our proposed model and closely related to what we are proposing i use the notation for input data x x1 to xn without any labels we aim to learn the representations for all of these i from one to n data points denoted as z1 to zn each of those z i's in the rj Those di's in the Rj space. For V correct, we first generate positive pairs. That's supposed to be similar. For each Xi, we have a positive pair Xi prime, and I use the DI primes for those representations for XI prime. Yeah. You could see it as like, for example, if we For example, if we learn a lower-dimensional representation, that would be like dimension reduction. In other ways, we are learning a long linear projection of the original data to the RJ space so that it demonstrates the structure of the data. Yeah. So the VCRAC method minimizes the loss function specified here composed of three. I composed of three different terms: invariance laws, variance laws, and the covariance laws. The invariance laws is specified by the squared exponential distance between each pair of the positive samples. So it is saying that we would like those zi and zi primes we learned to be similar. This forced the similar samples to have similar representations, and this is a And this is in the loss function where the positive samples are required. And the variance loss is here specified to maintain the variance of the overall representation C to be above a given threshold. Approximately, I can write it as the negative sum of the standard deviations. Sorry. Is standard deviations. Sorry, I missed the square root here: standard deviations of all the DIs. This term forced the representations to be having an overall diversity across all the data sets. So especially those different samples could have different representations. The third term is the covariance loss term that shrinks the covariance between each dimension of the data specified by the off-diagonal elements in the covariance matrix. Elements in the covariance matrix of Z. This prevents the different dimensions of the representation we learn to be highly correlated. So inspired by the spread method, we would like to propose some full probability inference that's not required generation of the positive pairs. So we propose to use this Gaussian process to help us. Suppose still we have to input data. still we have to input data x1 to xn and we generalize the aim of learning representation a little bit say that we aim to learn a distribution over the representations for any arbitrary point in the input space x or in other words we want to learn a distribution over the function z that maps the original data space to rj and here we use a gaussian process prior to Gaussian process prior to enforce the function z to be smooth over the space with respect to x. So we see that z has this Gaussian process prior with mean function m and a kernel k. And for example, in examples, we would use a constant mean function m and a squared exponential kernel look like this with the parameters else that basically control the smoothness across this. Across this function generated by Gautam process. With this Gaussian process prior for the function z, if we consider some traditional Bayesian methods using Gaussian process priors, we would proceed with follows. We have some observed data or labels y and we can specify a likelihood for it. And then we can derive. And then we can derive the posterior for the function z that looks like this if we write the prior and the likelihood like this with this L denoting the negative log likelihood. However, in our case of purpose in self-test supervised learning, we do not have such observed data y as labels. And what we do here is we want to evaluate this generalized basis posterior. This generalized phase in posterior, which is defined based on a prior distribution P and some arbitrary loss L. So this generalized posterior P tilde is proportional to Pz times the exponential negative L. And the loss function L we use here is actually taking the variance and the covariance loss term in VCRAG. In VCRAG. In the implementations, this generalized posterior is approximated by the generalized variational inference. So I would like to recap a little bit to make it more clear how our GPSSL works. So we learn a distribution over this function D that maps the input space to Rj. We put a Gaussian process prior on the function. process prior on the function z, this encourages similar samples to have similar z. And we evaluate this generalized posterior for some loss x. Here, the loss function is specified like this with the weighted sum of those two terms. The variance loss is given by approximately the sum of the standard deviation of all the dj's. Of all the DJs. This encourages different samples to be different. And the covariance loss is defined by the sum of optimal elements in the covariance matrix C that decorrelates the variables. And a footnote here is we use this term in the variance loss function that was this gamma saying that we penalize this learning when the sum of standard deviation sum of standard deviation of dj's does not exceed alpha so we want it to be at least gamma at least gamma but when it exceeds gamma we do not penalize it anymore and this epsilon is like a relatively small value to avoid computational issues so this is our proposed gps l and i would like to talk about some properties this About some properties. This proposed method is closely related to both the self-loop supervised learning method and also the kernel method. First, looking at the connection to VCRAC, if instead of considering only the input data X, we take the positive pairs, all the data we generated using VCRAC, and consider the same generalized posterior P tilde here. Posterior P tilde here, we can rewrite this in this form by putting this prior into the exponential term so that this term could be also seen as a part of the loss function approximately. So based on this Gaussian process prior here, we can write it out explicitly. It has this form of D transpose Kz, where K is the covariance matrix. The covariance matrix. So we conclude that if this k has this form of kij equal to one if and only if the xi and xj's are positive pairs, then this recovers self-supervised learning invariance loss. It is actually exactly the same as the Euclidean distance between the di and dy primes. So that this is saying that energy. saying that under this specific covariance matrix, our method is equivalent to the log function used in the VCorag. And this is also closely connected to kernel PCA, which is one of the most traditional kernel methods. We all know that PCA learns some principal components. Say we consider these Say we consider these J principal components ZI to ZJ as the representations we learned here as our goal, so that the PCA is learning a linear projection that's maximizing the sample variance of the projected data, which could be right in this form. And I guess everyone is familiar with it. And kernel PCA, instead of defining this in the original space with the inner product of X prime. inner product of x prime x, we define the inner product on the Hilbert space defined by a pre-specified kernel K. So if we take this pre-specified kernel K, kernel PCA could be written in this form with Z transpose KZ and K is defined by the kernel with all the X. And you will definitely see like this looks familiar from some previous slides. From some previous slides, and we proved that for our GPSSL, if we take j equal to one and these parameters ought to be zero, maximizing this generalized posterior p tilde is equivalent to finding the solution of kernel PCA. In other words, it's saying that in GPSSL, we aim to find this distribution of representations. Distribution of representations, which actually has the maximum point located as the solution of kernel PCA under some conditions. Okay, and now we can look at some examples. This is actually the expected thing I show at the very beginning, which is actually our result. For here, we look at a slightly different scenario. Had a slightly different scenario to denote the properties of our proposed method. We consider a classification task as a downstream task. So we have the labels on the training data set and a testing data set, which all look like this two noisy circles. And on the training data set, in order to better illustrate the uncertainty quantification in this scenario, I generate In this scenario, I generate data in those four different quadrants have different sample size, saying that we have very dense data here and less than less and nothing in this quadrant, just to illustrate this uncertain quantification. How we do this is we take all the input data X in the both training and testing data set, just ignore the labels here for now, and then we can put all of these into our GPSSL to look. Into our GPSSL to learn some representations to learn the structure of this data. So, this is actually a summary of the representation we learned. In order to better visualize it, we did some summary for this multi-dimensional D we learned. This is a different distance into a reference point to show it in one dimension. So, seeing these colors that we still have this inner circle similar to each other. This inner circle is similar to each other, and outer circle different from them. And taking the in a training set and testing set, and also the y labels here we have, we can do our downstream task. For example, for classification, just a simple logistic regression, and we get the estimates for the class for the testing set, which look like this, which looks pretty well. And we can take a And we can take a closer look at our results for the ancillary quantification. The first row shows the clusters we estimated and the standard deviation of getting those two binary clusters. Second row is showing similar thing, but the probability of clustering into the two binary clusters. And we can see that in both of these cases, looking at a standard deviation. At a standard deviation, we have like a larger uncertainty when the data points are in between those two circles, and also it get the accidentally get larger and larger when the training data set becomes less and less. So, this kind of experiment is conducted on probably six other different two-dimensional and relatively higher-dimensional data sets from the UCI machine. From the UCI machine learning repository. But I guess those visualizations is less exciting. So I'm not showing it here, but our method is showing variable performance. And I would like to show some beautiful real data applications, which actually partially motivated this project. So we are looking at a spatial transcriptomics data where we have this tumor tissue and we split it into And we split it into grids. And each of the spots in the grid, we put it in an RNA sequencing machine so that we have the RNA gene expression for each of the spots. And we also know the location of these spots on the actual tissue. So we have the facial coordinates and the histology information for each of those spots. And these three things is the composed of our input data X. Composed of our input data X. Sorry. For spatial transcriptomics, there's an issue that in each of spots, it contains multiple single cells from probably about 10 to 100, and it could be from different cell types. And we are interested in what those cell types are in each of those spots. So our aim here is to know the cell type composition for. The cell type composition for each of the spots. This is the pipeline how we are going to do this. Taking these three different things as input data X, we can put it in our GPSSL and learn some representation Z out of it. So we have the Z for each of the bots. And then in this proposed method, Starfish, in this paper in 2024, it says that we can use some biomarkers and this visual transcription. markers and the spatial transcriptomics data to annotate partially of these spots that's considered to be pure in cell types. So we can take these annotated cell types for the part of the data as labels and we can put this representation learned and labels in a classifier to get the cell type composition for all of the rest of the spot. This is the This is the result. On the left-hand side is the data we have. After annotating the spots considered pure, with this labels being negative one, these are the spots. We do not know the cell type decomposition. And with these colors, with nine different cell types, these are the annotated spots that's considered to be pure. And we have And we have this point estimate of this classification for all of the spots. This is showing a summary that we actually have the proportion of those nine different classes on each of the spots, but I am just showing the dominant cell type here for visualization. We can see that it's making a lot of sense, and we are definitely working on. And we are definitely working on some more downstream tasks to answer the scientific questions. And I would like to mention some benefits of using GPSSL on such kind of data. First, we have this multiple types of data inputs, being the spatial coordinates and gene expressions. There has been paper showing that for representation learning, Fausion process has been doing great. Doing great than the neural network-based methods on tabular data kinds. So we have this gene expression also being like a multivariate observed data for each of the experimental units, which could be used fit in this kind of framework. And also, we have this spatial information of the histology and also the coordinates, which naturally aligns with the use of Aligns with the use of a Gaussian process to smooth over it. In summary, we propose this novel self-supervised learning method using Gaussian process to learn the representations that demonstrate the structure of the data. This Gaussian process prior is used to encourage the smoothness of the representations for similar input data x. And we obtain a generalized position. And we obtain a generalized posterior, which is a distribution over these representations that minimize a loss function with those terms for a good representation we want to learn. And this is closely related to both self-supervised learning and kernel methods, which seems to be quite different used in the field and avoids the generating of the sample pairs, which is very favorable in the structured data. The structured data, and we have full probability inference. That's some selected references, and thank you for your attention. Thank you so much, Yushan. I want to open the floor up for any questions from the audience. Hi, Jushan. Thank you. I'm happy I'm not to see what this is about. Yeah. So, I'm So, how do you say you generalize variational inference? Yes. So, what's the, I guess, can you just explain a bit like how what's the exact like, you know, like what's the variational time linguist, like kind of like what do you do? That's a great question. And it's an important point. So, for the typical like Bayesian posterior, we can do vibrational inference. And it's actually very similar. You can see that we just You can see that we just replaced this likelihood term with this loss term so we can derive all the things with this similar in a variational inference for Bayesian posterior. And intuitively, it's just we define a virational family Q and we find the distribution Q that is like minimizing the callback like divergence between this T tilde and Q. And we are using a very simple variational family being the like the Family being the like the multivariate caution. But it's showing that it's doing good enough for our current examples here. So, yes. Thank you. Any other questions from on-site or online? So, you know, you're defining these Gaussian process priors. And this would, and you're using the squared exponential kernel. And you're using the sort exponential kernel. I'm just curious, when X is sort of non-Euclidean, you've got an image or whatnot. Like, do you have any recommendations for what we should be what we should be using here? Sure. So although I introduced the square exponential kernel at the very beginning, it's just the example of the kernel we use in these examples. We could actually apply any kernel we would like to use in the Gaussian process priors. We did not specifically look at We did not specifically look at any image data, so I'm not very sure for image data what would be appropriate. And one of the reasons we did not look is the traditional self-supervised learning method like BCRAC does very well on those image data and it's like designed for it. It's easy to generally read those samples with like cropping images, which is like intuitively natural. So that's the reason we didn't consider. The question is about the application where you predict the composition of the spots that are not annotated. So, since your training, your training label is all just pure cell types, right? So, how do you do the cell type composition? Is this just the probability of each label? That's the composition you're capable of. Yes, that's what we're. Yes, that's what we are getting currently. But that's a great question. I'm also thinking about more complicated downstream tasks with these representations because, in principle, we can do any downstream tasks. But this is a very simple example for class. About the computation. So, so probably it doesn't matter because in this case, you take the exponential. In this case, you take the exponential to the negative loss functions. Does that have to be a proper probability distribution, or it doesn't matter? So if we do not look at any consistency asymptotic theory, it doesn't matter. We can do it with anything. And so far, we haven't met any problem here. But I know there have been literature doing research about what kind of criteria. About what kind of criteria we need to meet to have these good theoretical properties, but it's very interesting to look if we can meet those. Any questions from people online? Okay, great. Yeah, let's thank Yushan again for the great presentation. Now we can probably adjourn any good questions. So thank you all. Thank you all. I have a question about special touching topics. Also, what happens to X-ray? 