Well, thank you very much for the invitation to this great workshop. I will be talking about community detection, which of course is familiar to most, if not all, members of the audience. So, of course, it's the problem of given a network, can we identify its community structure? This is work that was done with my collaborators. I started it when I was a postdoc at MIT working in the group of Elfman Mussel. At the time, I met Shobik Dara and Colin Sam. Time I met Shobik Dara and Colin Sandon, who were postdocs but have since moved on. And then I continued this work at Northwestern with a PhD student, Jeremy Joshi, who's in the CS department. All right, so for those of us who kind of eat, sleep, and breathe stochastic block models, sometimes it's easy to forget about motivation and application. So I probably don't need to convince you that this is an interesting problem, but let's maybe take a step back and like think about why we're studying this in the first place. Clustering is a common phenomenon. Clustering is a common phenomenon in networks. Perhaps the most famous example is the Belgian phone network. You may know that Belgium has three official languages. This network pictures two of them, French and Dutch. It was formed by analyzing mobile phone call records. So it like based on the frequency and kind of duration of phone calls, this network was formed. And what you see, of course, is clustering, as you might expect, because French-speaking people call other French-speaking people rather than Dutch-speaking people, and vice versa. Although People and vice versa, although there are some people who speak both, and so there still are some connections going across. Hyperlink networks can also exhibit clustering. So here is a political blog network of US political blogs, most of them leaning either left or right rather than being in the center. And here, each vertex represents a blog, and two vertices are connected by an edge if one links to the other or vice versa via hyperlink. Do you know what's from? Do you know where that's from? Oh, I don't, I don't remember. Sorry. Probably long ago. I just wonder what it would look like today. I mean, it was certainly, so I cited it, you know, I think you're right that it's 2004. It was certainly before 2017 because that's how I cited. Probably, I wonder if the structure has since changed. But like, unsurprisingly, this network also exhibits clustering. In biology, networks are used to study proteins and their interactions in a protein-protein interaction network. In a protein-protein interaction network, each vertex represents a protein, and two proteins are connected by an edge if they form some biological function together. And so a cluster represents kind of like a complex of proteins. The common theme is that somehow clustering is driven by latent features of the nodes. And so if we are able to cluster network, then we can infer those latent features. I gave this talk to more like a business audience at one point. So I wanted to, like, who was probably completely unfamiliar with the stochastic block model. So I wanted to give them. So, I wanted to give them some of these examples and they stayed for this talk. So, you may know that Uber Eats actually uses some graph analysis, including community detection, in order to uncover similarities between their users in order to drive their food recommendation platform. LinkedIn is, of course, a network by itself, and it uses community detection and other graph inference tools in order to suggest new connections, as you may have seen with the people you. Seen with the people you may know feature. Community detection is extremely valuable for advertisers. Actually, Google Ads has a feature called Customer Match, which allows companies that use the Google Ads platform to target customers that are similar to ones they've targeted in the past. And to uncover that similarity, some kind of learning and networks is used. All right, so just to kind of set notations, of course, probably familiar with the stochastic block model. Probably smaller with the stochastic block model was used to mimic networks with community structure, and it's like very striking similarity. So, qualitatively, if you take a typical sample from the stochastic block model, it really does look similar to a real-world network. Of course, there are variations to capture the variations that we see in real-world networks, so like multiple communities, different sizes, degree correction, et cetera. And let's think, why should we study the stochastic block model rather than developing algorithms for specific applications? For specific applications. Well, I mean, pictorially, you'll already see that the SBM captures the structural properties of real-world networks very well. You can think of it as a source of synthetic data because if you're trying to test the performance of your algorithm, but you don't have the underlying labels, it's very hard to verify whether your algorithm is performing correctly. And it has been observed that algorithms that have been developed for the stochastic block model actually transfer very well. So, if you like, actually, the political block network has a 95% accuracy because you can actually like read the blogs later. Accuracy because you can actually like read the blogs later and verify that you correctly labeled the blogs. And because a lot of people work on the stochastic block model, it serves as a common testbed. And of course, a well-defined model imposes statistical limits, so you kind of know where to, when to stop trying to develop algorithms. All right, so of course there are variations and generalizations of the SBM, but here I'm going to consider the case where the community labels are either plus one or minus one with probability one half each, and the probability of connecting. And the probability of connecting to nodes is P within either community, so same probability within each community, and Q between communities. And for the whole talk, the goal is exact recovery, so recovering all the community labels. But of course, there are other notions of recovery, almost exact partial and weak, but I won't touch on those at all in this talk. Of course, for those who have worked on this, there's a notion of recovering up to global flip because there's like a symmetry in the problem. There's like a symmetry in the problem. So, when I say recover all the community labels, it's recovery up to global flip is deemed a successful recovery. All right, so the central questions when studying the stochastic block model are of two flavors. One is statistical limits, that's identifying possible parameter regimes, and then an algorithmic question, which is to design efficient polynomial time algorithms that ideally match those statistical limits. Okay, I've worked on several community detection problems. This talk will discuss two of them. We'll discuss two of them. One is on sensor networks. That's when some of the connectivity information is missing. And the other is on higher-order networks. So that would be hypergraph, random hypergraph models. And that goes beyond pairwise relationships that we kind of limit ourselves when we look at graphs. The first topic will kind of take up the bulk of the time. So let's go into the first topic. Well, there's several modern contexts in which we simply don't observe the whole O network. For the whole network. That can arise for various reasons, maybe privacy. Maybe we don't know whether Alice and Bob are connected simply because Alice and Bob's information is private, or maybe noise, or maybe there are some sampling constraints. Maybe it's expensive to uncover information about the network, and so we only see part of it. So yeah, so one example would be like Linton. Maybe we simply don't know whether two people are connected. And two high-level questions for the censored model is the statistical question of when can Question: When can we and can we not recover the clusters? And also, can we find scalable and optimal algorithms, meaning those that achieve those statistical limits? So, let's describe the model. It starts from the standard SBM. So again, we find the community labels by labeling each node either plus one or minus one, with probably one half each. And then the probability of connecting two nodes is as before. We have two probabilities, P and Q, describing those connection probabilities. Describing those connection probabilities. And then on top of that, we reveal or not reveal information independently. So for any particular U and V, we know whether or not they are connected with probability alpha. So the ground truth is over here on the left, what we observe is a network where every pair of vertices has one of three possible observations, present, absent, or censored, which is like described pictorially by three possible connections or lack thereof. Or lack of any questions about this model? All right, and the goal is exact recovery. So just again, like we want to find the statistical limits and find a polynomial time algorithm. This is something that my collaborators and I explored in more and more generality. First, we started from the SBM and then the PDS and then like more general but still too community version of this program. General, but still too community version of this problem. All right, so let's describe the site. We have n nodes, and the connection probabilities are assumed to be constants. The reveal probability alpha is parametrized as t log n over n, where t is a positive constant. And that's because this is the first relevant regime. Like if you fix p and q to be constants, then log n over n ensures connectivity. Like if you have t large enough, that'll ensure connectivity of the graph. So kind of some motivation for why. Graph. So, kind of some motivation for why the reveal probability is of this order. And so, you should think of T as measuring the amount of revealed information. The larger T is, the more information we see. So, larger T means easier to identify communities. This model has been studied by the workshop organizers in the case where people's Q equals one. And they showed that a semi-definite programming relaxation of the maximum likelihood estimator is statistically optimal. So, we found the statistical. Is statistically optimal. So we found the statistical threshold and showed that the SDP matches this threshold. And that leaves open the question: well, what if people's q does not equal one? And also, is there a more efficient algorithm than the SDP that is also optimal? To state our results, define this threshold function. So as a function of p and q, this is sort of, this is meant to be the critical value of t. And the result is that if t is smaller than this critical value, then the problem is. This critical value, then the problem is impossible. No algorithm can succeed in exact recovery. Versus, if t is above that threshold, not only can we succeed, but it's a spectral algorithm that succeeds in exact recovery. And I'll come back and kind of describe what the spectral algorithm is. Yes. Is there like some intuition for why P plus Q equals one is like natural somehow? Yeah, so there was a dual certificate argument used to establish optimality of the SDP, and at some point I wonder. And at some point, I wonder if that argument can be generalized to p plus q not equals one, and maybe the authors have explored that. No, I feel like this formulation is slightly different from the original. Oh, I mean, we don't think about chips of y on p right. So I think you, yeah, if I remember correctly, yeah. So, I mean, I think you did, you considered multiple versions of this, including the one with people p and q equal to constants. Constants, all right. I don't remember P plus Q equals, yeah. Oh, okay. So, this is from like a 2016 paper. Yeah, I mean, there's like several kind of close together. So, because you had a parameter epsilon that was actually, yeah, okay, now I think I remember. So, the model that we study is like edge, and then on the edge, you have a regional condition, like it's plus or minus. Plus and minus. So plus typically it's like taking the model of the space to one point. So you've got the same, then you've got a plus, you've got different or minus. So it's like an edge and all the edge are plus a minus. But I guess maybe we have some covariance to use. Yeah, so we did reformulate what you wrote into kind of our model, but yeah, it would be good to look at the details again as well. Yeah. Yeah. Okay. Sorry, Julia, just a question. I think I mean. Julia, just a question. I think I missed something. So, so called Eugene, so P and Q are probabilities of inside and across the, yeah, so it's this picture. Yeah, so this is this is the meaning of P and Q, and then you get across, and then you reveal. So, like, so like you have, so like the observations here are either present or absent, right? And then, and then, if, if like a present observation is revealed, you see it as present. If an absent observation is revealed, you see it as absent. If it's not revealed, It is absent, and if it's not revealed, you see sensor. That's why you have one of three possible observations. But the degree of the actual ground, there's no flipping, it's just like, do you see it or not? So sensor means you do not see the sorry, do not show you the you just don't know if it's present or absent in the original graph. It's a question mark, yeah. Okay, yeah, so let's let's visualize this threshold. Um, yeah, so if you there's a question, sorry, oh, sorry, yeah, so if I don't tell you what. That's your question. Yeah, so if I don't tell you whether it's not there or sensitive and just versus those two glasses or keep paper. Then, yeah, then that would be equivalent to a stochastic block model. Yeah, so you do need to leverage the fact that you have three different observations. So like a first attempt at finding an algorithm would be to kind of just merge two of the observations and get an SPM, but that won't achieve the information theoretic threshold. You actually do need to separate those observations. So we can visualize, of course, there are three parameters, P, Q, and T, and so we can. Parameters p q and t and so we can visualize the possible and impossible regimes by fixing one of the parameters. And so here we've fixed t to be 50. And of course, and maybe you can't see, but this is p on the x-axis, q on the y-axis. Of course, if p equals q, then you can't hope to like find the two communities. And so the impossible region is some region containing the diagonal. And it like the shape of the region is described by the function tc of p and q. Okay, so let's talk about the simple spectral algorithm. A spectral algorithm in general works with a matrix representation of the graph. So, in the usual case, when observations can either be present or absent, then what we would do is encode a present observation as a one in the matrix and an absent observation as a zero. And so, of course, in an undirected graph, this would produce a symmetric matrix. But in our case, we have three possible observations. We have to make some kind of encoding from three observations to three. Encoding from three observations to three numeric values. And the way we do it is for a present edge, we put a one as before. For an absent edge, we actually put some like mysterious value that I'll get to later, some negative number. And for a censored edge, we put a zero. It turns out that if p plus q equals one, then this y should be one. And so maybe like your first guess as to how to make an encoding would be kind of symmetric around zero. Be kind of symmetric around zero, and that turns out to only achieve the threshold when people's q equals one. So, somehow hinting at like the role of y, but it's somehow balancing relative information in an appropriate way. So, the whole algorithm is just a couple steps. So, first we represent the network by a matrix. Okay, so if ij is present, record a one, absent, record the mysterious value, sensor, record a zero. Then you find the eigenpairs of the matrix. The eigenpairs of the matrix A, where the convention is that we're ordering them in proper order rather than by magnitude. And then to cluster for each i, which is a vertex that we're trying to determine, whose community we're trying to determine, we only need to look at the top eigenvector, and then the ith entry determines whether we put i in community plus or minus depending on the sign. So all we do is, so it's called spectral clustering because we use an eigenvector and cluster according to signs. Eigenvector and cluster according to science. Any questions about the algorithms? This is exactly color. This is for exact recovery, and there's no cleanup. So that's in that sense is a simple spectral algorithm. There's no, yes, yeah. Yeah, that's a bit surprising because it was only known until, it wasn't known until very recently that the regular SBM doesn't require cleanup. But that is known. That is known. Yeah, so we'll get there. Yeah, okay. So I'm actually like part of the Okay, so I'm actually like part of this talk was just to make you aware of a cool paper that other people wrote that we're using. All right, okay, so let's. Um, so the intuition behind why the spectral algorithm works is in two parts. So the first kind of basic intuition is just like why on earth we would expect the leading eigenvector to recover the communities. And like the second part is more like a statistical intuition for why we need a particular encoding. All right, so again, like we're determining communities based on the signs of the leading eigenvector of the signed adjacency matrix. matrix. And so let's consider its expected matrix where here I'm conditioning on the community assignments. And so for shorthand, I just called it a star. And that's going to be a matrix that only has two possible values. And it is of rank two. And let's denote its eigenpairs by these two ranks. Then, if you appropriately rescale the first eigenvector by root n, then its values are going to tell you exactly the community values. You exactly the community value. So, if i is in community one, there's a one in that position minus one if i is in community minus. Um, okay, so like if we had u one star, we would easily be able to recover the communities. However, of course, we don't know a star. We only know it kind of up to permutation. And so, if our goal is to show that the leading eigenvector of A correctly classifies, a good like meta strategy might be to try to compare the leading eigenvector of A to some other vector that we know how to analyze. Some other vector that we know how to analyze. So, like, right, so compare. So, strategy number one: well, if we already know that u1 star is successful at exact recovery, can we compare u1 to u1 star? Where this really should be entry-wise, right? Because for every entry, we need to be able to transfer the signs. This doesn't work simply because the entry-wise gap is too large in order to transfer the signs over. Strategy number two is to compare U1 to a Two is to compare u1 to a different vector, which looks a bit odd. So it's this, I'll call it the ratio vector. It's the random matrix A multiplied by the deterministic vector E1 star and then divided by its corresponding eigenvalue. And so here we're comparing random to random versus here we're comparing random to deterministic. You can also kind of have like a power method interpretation of like why you would expect you want to be close to this ratio. You want to be close to this ratio vector. So it turns out that this strategy works. These two vectors are close entry-wise. Okay, so in order to show that these two vectors are close entry-wise, we use a relatively recent technique that's very powerful of Abe Sen Wang and Zhang in 2020. That I'll come back to exactly to their technique, but first I'd like to show how we use it in order to prove our exact recovery results. Exact recovery results. So we managed to show that using their main theorem, that our vector that we want to analyze is close to this ratio vector entry-wise. More specifically, when we look at the infinity norm, it's going to be sufficiently small to transfer signs, basically. And then once we have this claim in hand, then we just use standard probabilistic tools, just like a basic turnoff bound argument to show that the signs of the ratio vector are able to. Of the ratio vector are able to recover the communities. So, like, if we take the ith entry of this vector, it's going to be positive if i is in community plus and negative if i is in community minus. So, this that means that the ratio vector is successful at doing spectral clustering. But since we want to show that our vector U1 is successful at spectral clustering, we also need to show that those entries of the ratio vector are not too close to zero, because then once we account for the error in the approximation, we'll be able to transfer the signs over. We'll be able to transfer the signs over. Any questions about that strategy? Okay. Yeah, so then the conclusion will be that our vector is successful at exact recovery. Okay, so now let's give intuition part two, more like statistical intuition. So we know that our vector will be compared to this product, AU1 star, of course, divided by the corresponding I. star of course divided by the corresponding eigenvalue but let's let's just look at that product of the matrix with a vector for now and so let's fix a particular vertex i well if we want to find the ith entry of that vector well that's just the inner product of the ith row of a with this vector u1 star and i for convenience i've drawn these in block form right where like one community is represented by the first n over two or so vertices and the second community by the rest um so let's just So let's just bookkeep and see what that inner product gives us. Recalling that a sensored observation is represented by zero, present by one, and absent by mysterious value. So for that given node I, let's introduce some notation in order to help us with bookkeeping. So that we'll call that the degree profile. It'll be represented by four numbers, which pally up the number of present and absent connections to communities plus and minus. Connections to communities plus and minus. So we get four numbers. Then, if you kind of just add things up, this ratio vector is going to be proportional to a weighted sum of these four numbers, where these first two numbers are coming from the blue part, and the second two numbers are coming from the red part. The minus out front is because there's a minus entry over here. Okay. Okay. And so that means that U1i itself is approximately this value, which is proportional to this quantity. So I just kind of repeated that observation over here. But let me emphasize that these four numbers are unknown. So it's not like you are trying to classify I and you know these four numbers. Actually, the spectral algorithm implicitly weights these four numbers. Okay, and why is that so powerful? Well, let's think about like the best possible estimator. We'll call it the Gini estimator. Its task is to label vertex I knowing the degree profile of I. So like knowing all of the labels of the other vertices. So it's kind of like a cheating estimator. Let's think, like, let's kind of figure out what does the best possible estimator do? If you work it out, it actually thresholds this. Thresholds, this same expression. Okay, and that's where we see why the encoding is important because, in order to have the same expression, we needed to have the same encoding. Okay, so the behavior of the genie estimator is informing our choice of encoding. What's the genie mean? Oh, the genie has one task, which is to label vertex i, either plus one or minus one, knowing the labels of all the other communities. But remember that community assignments are IID one half. Of course, it would be like trivial if we had a fixed size, but so that's that's actually why we study the IID, isn't it? And then we said what optimal wise is solved by solving its optimal. Yeah, so this becomes a statistical problem. Like what's the function that you're supposed to threshold in order to determine? Yeah. And it kind of just pops out. But I guess you want to show these the optimal product is the optimal testing product. To call it spoiler, yeah. So, we also, yeah. Um, so in order to show optimality, uh, we also derive the lower bound, yeah, so that's kind of separate, yeah. And the optimality, the way it pops out is when you actually do that turnoff bound analysis, like when the parameters are in the feasible regime is when a certain turn-off bound is like strong enough, correctly. So, the genius 30 is without all y, not just anyone. Yeah, yeah, yeah. So, that's it's that same why that's a hunch-fg in key. It's that same y that's a function of p and q. That's right. Okay, and so the takeaway here is that the spectral estimator mimics the genie estimator. Clearly, the genie estimator is statistically optimal because it has extra information and it's using that extra information in an optimal way. All right, so I said I would come back to the technique and I encourage you to look at the paper because I think it's very powerful. Actually, that paper was the one that showed that spectral clustering for the usual SVM works without need for a cleanup. Without need for a cleanup. And so when I saw that, well, okay, maybe that same technique will be useful for variants of the SBM. But they also have other examples in their paper that are not community detection problems. Okay, so I like on purpose did not give all the notation. I just want to give you some kind of idea of the assumptions that are needed to verify in order to apply their entry-wise bound. So the first one is like pretty easy, right? So it's just some like two to infinity norm verification for a star, which is the expected matrix. A star, which is the expected matrix. This is a deterministic matrix, so like it's like super easy to check this. Like, don't worry so much about what's on the right-hand side. Assumption two is like a type of independence that holds very trivially for stochastic block models conditioned on community assignments. So first two are just done. The third one, the main thing is verifying a spectral norm concentration. So taking the difference of a random matrix A and its expected. Of a random matrix A and its expected value, and then that random matrices spectral norm. But there is like already existing results in random matrix theory to like help you out with these kinds of tailbounds. And then the fourth one is kind of unusual. It's called a row concentration assignment. Yeah, so if your goal is to prove an entry-wise guarantee for a single vector, you can think of this W actually just become. You can think of this w actually just becomes a single vector. So the requirement is for a fixed vector w, if you take a certain like the mth row inner product with w, then that needs to concentrate like sufficiently as a function of w. So like, I'm not telling you the exact details, but this is maybe like a little bit more specialized versus like the first here trivial, the third one, there's some tools already, but the fourth one is like maybe more problem specific. This is like concentration and mainly direct. This is like concentration in many directions simultaneously. It's for you have to show concentration for any fixed W, not simultaneously. No, instead, because W is not a vector. So, right, so they have a stronger result than the one that we use. So, they have two results. One, that's just for comparing a single vector to its ratio quantity. And they also have more like an eigenspace kind of perturbation. An eigenspace kind of perturbation where they take multiple vectors at a time. We just didn't need to use that. But this is, these are the assumptions that are needed for their more general results where, yeah, you have to verify row concentration, not with a vector, but with a matrix. But we just never had to do that for our purposes. What is R? I'll use it to n by R. Oh, so R is the number of vectors that you would like to recover. R was always one for us. Okay, cool. So it is close to n. Okay, so yeah, I encourage you to go. Okay, so yeah, I encourage you to look at this paper. It's very neat. Okay, yeah, I'll just mention a few words about the impossibility results. So we showed that if t is sufficiently small, then no algorithm succeeds. And so we need to show that the maximum made posterior estimator fails because that's the best estimator. But yeah, so that's the one I'm computing like most likely sigma given the observation. But because the labeling is uniform, the Labeling is uniform, the MAP is equivalent to the MLE. And so we actually argue through that. And so yeah, so we just design kind of a confusion for the MLE. So like if sigma naught is a correct assignment, then this swap notation means that we swap the labels of U and V. So we should think of U and V as having opposite labels. And so like imagining that the realization G had some pair UV with opposite communities. With opposite communities, such that, like, the probability of observing that graph is the same under these two, like, different labelings, then we would fail, right? We would not be able to determine which one is the true labeling. And so that's just like one G. So we need to find like many G's for which that confusion occurs. And so the idea is to find a bunch of G's. Like a bunch of G's so that we are sufficiently likely to observe an element from this set. And also, each of those G's on its own confuses the MLE. That's kind of a high-level idea. All right. So, all right, so that was encouraging. We showed that spectral algorithms are optimal for the sensored stochastic block model, but that was like a little, that was kind of specific, right? It was balanced, same probability within each community. And so, like, the next thing we moved on to was the planted dense subgraph model. To was the planted dense subgraph model, which is like the same thing that the guy was talking about, but in a different parameter regime. So, here there is a subset S star that is unknown to us. Inside S star, edges are formed with probability P, and all other edges are formed with probability Q independently. Here, to decide the community labels, there is a random set of row times n nodes that are belonging to S star, where row is a constant. So, like very different regime than what Guy was presenting on. Was presenting that. All right, so like one example application, actually, this is the planted and subgraph type analysis is used for anomaly detection and cybersecurity and threat assessment. Okay, so there has been prior work in the case where the size of the planet densgraph is a constant fraction of the vertices by in a stream of work by the workshop organizers. So they considered the uncensored model where uncensored model where p and q were like order log n over n where a and b were parameterizing these connection probabilities and they defined this function f in terms of these two parameters a and b and they showed that if the product of rho and fab is greater than one then this is the achievable regime that the mlray is able to recover ester but conversely if rho fab is less than one then no algorithm works Is less than one, then no algorithm works. That is the threshold. And here's just a graphical representation, fixing a row of like what values of A and B are realizable. And of course, it's going to be some region containing the diagram. All right. And they also showed that a semi-definite programming relaxation of the MLE is optimal, but also a simple algorithm they also showed is optimal where you do like degree thresholding and voting on top of that. So actually this problem. This problem, you don't even have to go to the SDP route. They already showed an arguably simpler argument, algorithms here. All right. So, to say our results, we showed using similar techniques that a spectral algorithm is optimal for the Lapland-Den subgraph model and for its censored variant. And it's also true that a spectral algorithm works for submatrix localization, which is like a Gaussian version of the problem where the entries of a matrix are Gaussian rather than Bernoulli. Are Gaussian rather than Bernoulli. So that hints at some generality of the optimality of spectral algorithms. There was a slight twist in the first two problems. We actually needed to use a weighted combination of the leading eigenvectors in order to be able to mimic the Gini estimator. So now that we knew that we should mimic the Gini estimator, we could kind of like go backwards and think, okay, what is a Gini estimator doing? And how can we mimic that with an appropriate choice of ways? All right, so again, just to visualize our results. Yeah, so the spectral algorithm recovers a star with high probability in that same regime. So that it's still that row times FAB product. And then to state the result for sensor PDS, it's stated in terms of Chernoff-Hollinger divergence. So if P and Q are constants and A, as before, is parameterized as T log N over. As before, is parametrized as t log n over n, then there's a condition in terms of t, rho, and then the turnoff-Helmer divergence between p and q. If that product is greater than one, then a spectral algorithm is successful. And then here's kind of the corresponding regime for some, I guess this is some fixed value of t and rho. So if you fix those and then vary p and q, here's the phase diagram. All right, so I wanted to give some idea as to how we designed this algorithm. So like knowing that Design this algorithm so, like, knowing that we should mimic the genie helps, right? Um, the idea is, um, there's a helpful portion of the matrix and an unhelpful portion, right? So, if you're, you know, if you're looking at this row i versus another row that's that's supposed to be outside of s star, um, then where they're differing is kind of in the first chunk of this matrix, right? Because here you have one distribution, here you have another, and kind of the right side of the Another and kind of the right side of a matrix is actually going to throw you off. It's not giving you any kind of helpful information. And so, what you would like to do is be able to multiply this matrix by the vector that has a constant value in the left part, and it just zeroes out the rest. And so that requires like calculating parameters C1 and C2 so that, well, just from the entry-wise guarantee, we'll know that that sum is going to be. Entrywise guarantee, we'll know that that sum is going to be approximately equal to the appropriate sum of ratio vectors. And we kind of cook up C1 and C2 so that this right-hand side is going to play the role of multiplying by this like dream vector, as I call it. All right, so I did gloss over the fact that there are sine ambiguities when recovering eigenvectors. So you actually know the eigenvectors up to sine. And so that And so that so because of assignment ambiguities on both u1 and u2, that leads to four possible candidate vectors. And so we end up just checking all four and evaluating them in our algorithm. And actually, so there's a concentration like lemma that we need to use in order to show that once we multiply A by this dream vector, then we get the appropriate signs depending on if a vertex is in. Depending on if a vertex is in S star versus not, that concentration was already done by workshop organizers, so that part was easy just to find this one. Okay, so the overall algorithm, we take A to be a sign adjacency matrix, and we encode in the same way as before, and then compute the top two eigenpairs, and then now we form a set of candidates. So we allow a sign S1 and S2 on each of these, and that gives us four candidates. That gives us four candidates. And then for each candidate, we do spectral clustering and then just return one of the four candidates, depending on which one maximizes the likelihood of observing the matrix or the graph under that community assignment. I think I missed it. Like what the YouTube factor, or sorry, not YouTube, but YouTube stuff. Or, sorry, not U2, but U2 star is so U1 star is like some value on the planet soft and zero otherwise. And what's the U2 star? Oh, like I actually didn't tell you what the what U1 star and U2 star. Yeah, so they're actually, so actually neither of them is going to be the vector that places that is a zero outside. And so that's why we need to take a linear combination in order to get back a vector that is zero outside. Oh, so your ideal. Oh, so you're identifying. Yeah, we can do that because these are both both U1 and U2 are block vectors, and we want a block vector in return. And so we're able to take linear combinations of block vectors to do that. Thanks, Amit. All right. Okay. All right. So that's encouraging, right? So that suggests maybe spectral algorithms are always optimal. So let's consider the case where, right, we have two communities, community plus and community minus, with connection probabilities. With connection probabilities that might be different within each community. So they may be also unbalanced. And as before, we reveal the connectivity with a probability alpha independently through all pairs of vertices. Now let's consider a class of spectra algorithms. I'll call it spectral one because it uses one matrix. Okay, but first we create an encoding matrix where we encode our observations by three values and the absent one can be anything. And the absent one can be anything. It doesn't have to be the function of P and Q. And actually, this is arbitrary. We could have shifted and scaled things. And this is already as expressive as you could possibly make it. Because you also have freedom in how you weight the two eigenvectors and the threshold. So like once you give yourself freedom there, it's like sufficient to consider encodings where you fix two values and vary the third. Okay, so I'd say like this is the class of reasonable algorithms. Like a class of reasonable algorithms that you might want to evaluate for this problem because, well, the expectation matrix is ranked two, so that's why it makes sense to consider only the top two eigenvectors as the leading eigenvectors. And so I think this is the appropriate generalization. Okay, so let's see. Is that class always optimal? Well, some bad news. If P1, P2, and Q are distinct, then actually that class is suboptimal, meaning that there is some delta such that if T is the same, Some delta such that if t is sufficiently close to the threshold but still feasible, then for any choice of these parameters, then spectral one will fail to perform exact recovery with high probability. Okay, so is that maybe a sign of hardness? No, actually there is an optimal spectral algorithm, but you need to encode your observations using multiple encoding matrices for the same data. So it's not like you have to like resample the graph, but you just take your same data and record it. Same data and record it in two different ways. So the spectral two class is, so we actually just need two, not more, adjacency matrices, A of y and A of Y prime, where we record absent observations as either negative y or negative y prime for two distinct values. And then we compute the leading eigenvectors, obtaining four eigenvectors in total, and then classify depending on some weighted combination and like a certain Weighted combination and like a certain threshold. And this class always succeeds for any t that is feasible. And a bonus, so like this class is actually not sensitive to the choice of y and y prime, unlike what I presented for the like basic model, where that value of y had to be a specific value in order to mimic the Gini estimator. Here, because you have enough degrees of freedom, you only have to make sure that they're distinct. To make sure that they're distinct and there's some finite exclusion set, and so if you just sample these independently and uniformly on zero to one, that's like with probability one, you'll have y and y prime that worked for the algorithm. Okay, and so I'll just emphasize that you don't need to like resample anything, you just take your existing data and write it down in two different ways. Okay, so let's give some geometric intuition as to why this class works, but also why the class with But also why the class with one matrix fails. All right, so remember that, like, I'm just kind of conceptually writing down that a random vector can be approximated by the appropriate ratio quantity. We've already seen that a spectral algorithm classifies a given vertex i by thresholding a linear combination of the four degree profile values, where these coefficients beta one through beta four were like specific when I first presented this. Specific when I first presented this concept. But now, like, you might imagine that any spectral algorithm will be implicitly thresholding some more generic looking linear combination. Okay, so like, what does that look like geometrically? Well, a degree profile is four numbers, and so we can interpret that as a vector in R4. So these are clouds here of degree profiles where the degree. Where the degree profiles of one community are kind of clustering in R4, and similarly, the other community. And so, what does a spectral algorithm look like? Well, it's thresholding a linear combination and at some prescribed value r, meaning that for these points that are on like bigger than r, that will the spectral algorithm will classify them as being in one community versus those smaller than r get classified. Smaller than R get classified as being in the other community. So, like implicitly, the spectral algorithm is kind of cutting degree profiles, even though we don't know the degree profiles ourselves as an algorithm designer. Okay, and so that means that a successful spectral algorithm corresponds to a separating hyperplane, where here I've just conceptually represented the hyperplane by a line. Here, the spectral algorithm would succeed if the parameters beta one through beta four and R. Parameters beta one through beta four and R were such that they generated a separating hyperplane. So now let's like think about why a one matrix algorithm would fail while a two matrix algorithm would succeed. Well, it turns out you can check that a one matrix algorithm is constrained. Its coefficients are limited to satisfy like one of these three constraints, which worked out just fine for the SBM and the PDS, but not the generic case. Okay, so now if T is feasible. Okay, so now if T is feasible but very close to the threshold, that represents the challenging case. Intuitively, the clouds should be close. And so the choice of hyperplane is restricted. Some hyperplanes succeed in separating, while others don't. Here you can see a little bit of crossover. And so spectral one cannot produce a separating hyperplane unless the parameters P1, P2, and Q are like very specific. Versus spectral two can produce Versus spectral two can produce any hyperplane. And so two matrix algorithms are the ones that are able to mimic the Gini estimator, while one matrix algorithms are overly constrained and so cannot achieve the information theoretic thresholds except for some certain special cases. Any questions? Can you say a bit more about the y and y prime? So they only determine the threshold r, but not the coefficient? Correlation, yeah. So, like, um, you can so if someone gave you y and y prime outside a certain exclusion set, then you can from those you can like cook up what should be the coefficients and the threshold. Actually, thresholding at zero is already sufficient, so you can just set that to be zero. Yeah. Yeah, I also want to clarify. So, in your model, you're unwearing and custom, right? So, so you don't need to know the size. You don't need to know, okay. So, all this assumes that you know the model. Okay, so all this assumes that you know the model parameters, but you don't know the realization of the size. That's true. So you just, um, yeah, so you don't, but yeah, you, you, no, no, so you do need to know, like, okay, so I assume that you know the parameters, and like you could ask, like, what if you don't know the parameters, but you, like, you know, it's from this family, then could you maybe estimate them by some like kind of subgraph counting? So if you have, yeah, so if you have a sufficiently good estimate of the probabilities, then like this should go through. This should go through. Okay, then we are like, where do those factors appear in our algorithm? Oh, okay. So it's to determine what's the correct values of the weighting parameters. So those are sensitive to like which block model you're studying. Yeah, so I didn't go into the calculation of how you find these, but it's the same type of thing where you just write down what the genie estimator would do and just like match coefficients and like solve a system. That's the type of thing you need to do to like. Type of thing you need to do to figure out what the coefficients are. Is there a setting maybe with multiple blocks where spectral two does not work, but spectral three works? Yeah, yeah, yeah. So we're kind of in the middle of that. So actually, the showing impossibility of spectral one was like extremely difficult. And so our current goal is to show just the achievability portion for multiple communities. Yeah. But I also want to clarify that what you're doing is for these very specific choice of special public populations. So now I can do an exhaustion for the choice of positive construction. Yeah, so I think like our class, I think, is arguably a reasonable class of spectral algorithms. Like if you think of any spectral algorithm that has been proposed for, you know, that that's that's just that's not doing any kind of cleanup, for example. Are you thinking about maybe you should uh join the elements of the interest based upon this price? The matrix based before this has a story. So, that encoding, so it allowed for three different values. And we argue in the paper that like locking in two of them is actually arbitrary. We don't need to do that necessarily. So, we're already like, whatever like three values you have in mind, you can put that in into our framework. Okay, so just to summarize this portion, we considered general. Portion. We considered a general two-community-censored SBM and identified the statistical threshold, showed that spectral algorithms are optimal, but we might need to represent your data twice. So, a natural direction is to consider more than two communities and also more general distributions. So, yes, I'll get into the briefer second part, which has to do with higher order networks. So a canonical example is the academic collaboration network, where authors of a paper are connected by a hyperedge. Of a paper are connected by a hyperedge. Actually, this is hyper-edge, or sorry, hypergraph modeling has been used successfully to identify COVID-19 cases from other respiratory infections by connecting patients together who have similar like CT scan. And in shipping networks, actually, hypergraphs are used rather than graphs attempts in order to connect multiple ports together according to their routes. So, motivation to this. Because I haven't seen this before. All right, so the hypergraph has to be on, like, there are many generalizations, but one accepted generalization is, you know, for fixed order D, where in this picture D is three, after labeling the communities, we connect any set of D three nodes, either with probability P or Q. So P if those three nodes are fully within a community and Q otherwise. So like over here is an example where we have two and one, that happens with. That happens with probability Q. Okay, the regime we consider is this one, which is like the one that makes sense for exact recovery. Where this model statistical threshold is known from a recent paper, and they also give a polynomial time algorithm. So it seems like that's it, we're done, but it's a bit complicated. And so, what about this simpler strategy? Let W be what's called a similarity matrix, which means that the entry w ij is a number of height. entry wij is a number of hyperedges that contain both i and j. So it's like the number of papers that two authors have co-authored. And so can we use W for community recovery? It's the question. Of course, you're wiping away information, so the threshold changes, but like let's see what we can do. So this specific model was already considered by Kim Bandera and Gohans, who proposed an SDP relaxation. And they showed that for some parameters A and B, which are governing the edge probability. A and B, which are governing the edge probabilities, that SDP was successful. And they also conjectured that the SDP is statistically optimal, of course, for the altered threshold where you're only given W. And so that left open the question of, well, does the STP actually achieve that threshold? But also, is there a simpler optimal algorithm? I think you know where I'm going with this. Yeah, so we showed, yes, we did show that the SDP is optimal via a different dual certificate argument than the one they used and also a simple. And also, a simple spectral algorithm is optimal. And that, in order to show optimality of the spectral algorithm, we need to show this kind of entry-wise guarantee as we were doing before. But the results of Abe, Fan, Wang, and Zhang did not apply out of the box because like assumption two, which is trivial for the previous cases, simply doesn't hold because there's a complex dependency structure. And so that was something that the student put a lot of effort in. Okay, so the message here is. Okay, so the message here is that spectral outcomes can be optimal even if you have some kind of complex dependency. All right. Yeah, I just wanted to, yes. Like revealing W, you said is strictly less information than actually the whole hypergraph. Right, because there are multiple hypergraphs that map to the same W. Yeah, so you could also like, you think, like, let's say you're given the full hypergraph, then you could yourself make W in polynomial time and then run this thing. And maybe I should, but that. And maybe I shouldn't mention this, but that is faster than what the optimal algorithm is doing. And so, if there's some kind of runtime consideration, then you may want to use our algorithm as long as the parameters are within the achievable regime. And that would be like better than the complicated one, even though the complicated one achieves the citizens. Okay, yeah, I just want to give an aside. So, the entry-wise eigenvector technique. Eigenvector technique that we saw for the more usual, for the graph-based problems, required a bound on this spectral norm. And that's already a known result. So in the case where A is a symmetric matrix whose top half has IID entries, there's a classical result of Hagen-Ovic from 2005, which is like a pretty involved combinatorial argument. But again, the organizers improved on this quite a bit by a nice Improved on this quite a bit by a nice symmetrization argument plus concentration on top of that in order to argue about like a tailbound for this spectral norm. And so yeah, we also when we adapted the entry-wise technique, we also needed a tailbound on this spectral norm, but of course that doesn't fall into the symmetric IID construction. So we couldn't just apply what was already known in that case. But thankfully there was already a result that actually covered this case. And so we could just use that out of the box. There was nothing. And so we could just use that out of the box. There was nothing to do. But before we found this paper, we tried to do it ourselves. And so we tried to adapt the symmetrization argument in order to control the expectation of this random quantity. And so, and then we could just like, if we didn't even have this sharp result, like a Markov inequality was already fine for our purposes. But it would still be interesting to see if we could recover the sharp concentration result through some kind of concentration argument. Result through some kind of concentration argument, but we haven't been able to do so yet. Okay. So, yeah, just to conclude some future directions. So, like one I already kind of alluded to is the case of more than two communities. That's something I'm working on with Tobi Khan and Akman. And okay, we're avoiding cases with zero eigenvalues and multiplicity. So that we actually avoid some like important cases, which are like symmetric models, because we want to be able to apply the entry-wise results just to like. Just like approximating one eigenvector at a time. And so we are in the process of showing that in the sensored SPM, multi-matrix generalization is optimal as long as you kind of, if you have K communities, then like K choose two matrices are going to be sufficient. And then we would like to also study community detection with like more general planted distributions. And we're hoping to find like mild conditions for which. Find like mild conditions for which spectral algorithms are again. Often, we already saw this was the case in the submatrix localization problem. Sorry, what is general finite distribution? Oh, we don't know yet. Just like more than just like, like just like give conditions. Oh, the like rather than being just Bernoulli, like specific cases like Bernoulli or Gaussian, we'd like to give conditions on the distributions for which we can apply that entry-wise eigenvector result and therefore like prove optimality of spectral algorithms. Okay, so you're saying the observed like edge values are yeah, so I mean. Edge values here. Yeah. So, I mean, you could think of this as a graph problem or a matrix problem, either way, but like, can we just give like conditions rather than studying one distribution at a time? Okay, yeah. Okay. All right. And then something I've started recently is community detection in the semi-supervised model. That's something I'm working on with Nerman, who's a PhD student, and actually Rogovson, who's a high school student who I'm mentoring. He's doing more like on the computational front. So here we are given some of the community labels and somehow can we leverage this extra information. Somehow, can we leverage this extra information also in a simple spectral algorithm? That's something that we're just now starting to explore. And yeah, some longer-term directions that I think are interesting are just kind of various new directions for community detection because it kind of seems like the basic models are solved by the community. But ones that have not been explored quite as much are the semi-supervised case, a hierarchical version, overlapping communities. I think the field is moving in this direction with like where we're at the variation stage. We're at the variation stage, kind of the basics are already done. And another area I'll mention that I'm hoping to get into, maybe some of you have input, is this idea of optimal compression for downstream inference. So the motivation is, let's say you store some data, but there are memory or privacy considerations that force you to like modify or wipe your data. Well, but if you know that in the future you have some inference task, how do you optimally compress your data to satisfy the memory or privacy considerations? By the like memory or privacy considerations, but like still do the best you can on the inference task. So, there's some like future directions I'd like to highlight, but thank you very much to the organizers and for your questions. Any questions? Okay, go ahead. Yeah, so one thing that I guess immediately comes to mind is: what about the statistical and additional gaps for finite density cell graph in a censored case? Do you know if that's referenced there or not? Oh, I don't think that has been studied by anyone. Okay, because yeah, one okay, if we can discuss, yeah, there's some natural things one might try to map up with those diagrams. But maybe I'm wrong, maybe someone else knows. Yeah, I don't, I think, I think the centered version has not really been studied much. And then another discussion about this, the more general distribution thing for the matrix case. I mean, I guess one thought is just there's probably some classic. Thought is just there's probably some classes P and Q where you can threshold the likelihood ratio of each of the entries. That would do something for that class. Are you thinking to go for, I mean, you could threshold this and get a graph and then run your method on that. Yeah, we're hoping to accommodate continuous distributions also. So you could do that. You could compute the likelihood ratio between the Q and then threshold. So, between the Q and then threshold this at some value. So, it might be that, yeah, thresholding methods are often more sometimes than not other times. It'd be interesting to explore that. Okay, cool. I had a question about this concentration result for the year minus one was the extent the error bound that's showing. Okay, so what I wrote was like little little O of one over root n, but really. little o of one over root n, but really it was like c over root n and then log log n also in the denominator. So like sufficiently small that signs transferred. Yeah. My question is if you were to put in like a squared instead of a, does that actually include um i i don't see immediately how that theorem could apply to a squared look what would be the motivation for using a squared i just like if i think it's I guess, like, if I think about the reading item vector as how you should make, then the comparison that you had to v star directly, I guess what you're saying is it doesn't make that constitution too bad and you're applying one power method in which you're still around A to get a show for power. So I'm just wondering if you do constantly. Oh, oh, I see what you're saying. I see. So, so, okay, so you're comparing like U1 to A squared. Yeah. Oh, I don't know, but that's still something that can be explored empirically. That would be nice to see that. So I haven't worked on this for a while. I'm curious about the state of the art. For example, I remember around the time of the paper Abi fan one way agreed, there was the one thing left open was without sensoring, just classic block model with three communities. You run a vanilla spectral method and then just cluster based on, for example, the second and third pipeline. Based on, for example, the second and third Python vectors, without cleanup, does it achieve the corresponding sharp threshold? I remember at the time it was left open. But now, is it already shown? So as far as I know, that it's still open because there is this rotational ambiguity. Yeah, I remember that. And that's exactly the issue we're trying to avoid when we're generalizing because we're more communities. So we're actually like, yeah, so that has not been resolved. Even without censoring, this is still working for us. But we're avoiding the cases. But we're avoiding the cases where there is a rotational ambiguity. So the case they considered was every community has an inter-community probability that's P, and all like between communities are P. Yeah, yeah. So that's the case where you run into rotational problems. So it's still unresolved. It's still unresolved. Yeah, thanks. So you took P and Q constant and T to P like login, right? And it's what you could have P and Q smaller with T larger with like probably the product. Would like probably the product be the same quarter? So, so we did that sort of analysis for the PDFs only, but yeah, it is possible to like tweak the regimes. Oh, I just wanted to ask, going back to this like hypergraph thing with the W matrix, might there be like a different matrix instead of W that's pretty simple that would achieve the optimal threshold? Like W is like for every two vertices. W is like for every two vertices, see count in the hybrid that's continuing forward. I mean, I have a feeling that as long as you have like a many-to-one mapping, you probably don't have optimality, but there might be some matrix that achieves a better like feasible region. Okay. Yeah. What about a bigger matrix? Oh, oh, yeah, yeah, yeah. Because you could do like some Kikuchi, like I mean. Yeah, maybe. Like, yeah. So maybe if you like double the size of your matrix or something. Like double the size of your matrix or something, somehow you can encode enough to like appreciably change the framework. Yeah, or like if you allow a clean up to first use w to assign the boss under cover and then use the cleanup to get onto the stretch board. Yeah, quite possibly. There's also like a nasty distribution you can put on the edges where you say probably P and Q, but depending on if the intersection is even or odd with one of the parts. Odd with one of the parts, then I think there's a huge computational gap. Okay. So, but I wonder if you've thought about that at all. No, we haven't. So, just a parity distribution, and then it's like n to the d over two development algorithms, we know. Okay. So, I just wondered, so is this, is there, does anybody study like the regime of not exact, but overlap, you know, usual and partial recovery in this? A partial recovery in this model for which process? Yeah, I mean it's for this like a Xander's model. Like, let's say I take alpha of the rest. Yeah, yeah, that has been said. I can't remember the reference off the top of my head, but yeah. But everything is wrong except short thresholds. And sorry, I'm forgetting, to be honest. Yeah, that's interesting. I also wanted to mention, I actually implicitly thought about this in the context of recommendation systems. The context of recommendation systems, because in recommendation systems, right? I mean, you constantly have the situation that the majority of the observations are question marks, right? You don't like you don't know, right? This, for example, this community are similar or not, but they were never shown to anyone, right? And you know, for some like small subsets, right, you know that they're either non-similar or very similar. Yeah, so it'd be interesting to move to like the revealed graph being that not random, but like absolutely. Not random, but like that's something just a week. I mean, we were, I was specifically looking in the context of actual, you know, practical work. Uh, so this minus white week is amazing. Yeah, I'm going to tell you more. But we actually do spectral decomposition of these graphs with a lot of question marks. We just have white v zero. So that's what we did. Uh, and uh, yeah, so I can immediately apply this. Surprisingly, I didn't expect to find any applications in this control, but this is. Yeah, I guess we asked earlier what was Bruce on this recommendation of plus one minus one question one. I see like plus minus one just needs to write at this time. And you did the same like, I mean this is just very doable, right? Yeah, yeah, but I think in that setting we just we don't consider this formulation, so so you don't need to have like a minus one. So there's no special encoding, just plus one, minus one, and question one. And then for question. Question mark. And then follow a question mark, just to easy answer questions. Okay, there's no question. I think to the speaker here.