Thank you very much. So I created those slides on relatively short notice, but they are supposed to be self-consistent. If they're not, just let me know. So it's also my pleasure to be here, of course, in person and to hide a little bit behind those screens. So that's pretty nice. I can only see about three people of you. Like, hello, hello. Nice to see you. And it's also my pleasure to kick off the topology part here. This is inadvertent a little bit, but it also was pointed out to me that. But it also was pointed out to me that I'm the only one of the organizers that hasn't got a talk in the schedule. So, um, that's oh, yeah, yeah, maybe. Oh, yeah, I could try this. Yeah. Ah, this, okay, okay. Whoa. Wait, this is super, is this, isn't this super loud for you guys? Okay, okay, okay, okay, okay. So now I can at least stand. So, yeah, pleasure to kick off the topology part of this section. So, Liz and Sarah, please don't hate me too much for it. Don't hate me too much for it. I'm trying to give an intuitive overview. So, let's first get acquainted a little bit with topology. And I'm sure that Liz and Sarah can do this better than I can, but here we go. So, let me tell you about computation topology and why it's important. Fundamentally, it's about trying to figure out why shapes are different or why shapes are similar to each other. So, for instance, this cube is a little bit different from a sphere, primarily because, well, it's not smooth. Primarily because, well, it's not smooth, but maybe it's more similar to the sphere than to the torous, because the torus has at least this one hole that we can see, the one that we can put our finger through. Like if you're eating a donut, this is like a great way to store the donut, for instance. So in some sense, it's fundamentally different from the sphere and from the cube. And computational topology tries to answer such questions, namely, well, why is the sphere not the same as a torus? As a Taurus, the answer to this question lies in some interesting algebra because maybe this is the time for me to make a remark about the difference between algebraic and differential topology. In algebraic topology, we are fundamentally interested in actually calculating stuff. So it needs to be something that, well, that is amenable to calculations, to approximations, and so on and so forth. Most of the things that I'll mention in this talk boil down to This talk boils down to linear algebra in the end. Not going into the gory details here because I want to rather build the bridge to machine learning and show that this is useful. But one very useful concept in computational topology or topological data analysis are the so-called Betty numbers or Betty. Let's see how we pronounce this. And the Betty number in dimension D is the count of the number of D-dimensional holes. So this already tells you that it's what we call an isomorphism invariant because We call an isomorphism invariant because that number should not change under certain transformations. So, and it can be used to distinguish between spaces. In low dimensions, you have a very nice kind of intuition behind that. So, you have the number of connect components in dimension zero, the number of tunnels in dimension one, and the number of voids in dimension two. So, now you could take a look at this small table here on the right-hand side where you can see the signatures of different spaces. So, the point, this yellow. Spaces. So the point, this yellow thing on the left-hand side, it has almost nothing. It's just the smallest thing you could think about. It has a connected component, but nothing else. The cube is a little bit more complicated, has the same signature as the sphere. And then, of course, the torus is even more complicated with its two loops. Not going into the details how that works, but it gives you a rough idea of how you could distinguish between these very platonic, in the true word of the sense, in the true sense of the word. In the true sense of the word, objects. Now, but how do we handle real-world data actually? This is a little bit more complicated because real life is messy or complex if you want to, because it has real components and imaginary components. But anyway, so the fundamental thing that we're doing when we're dealing with real world data is we're calculating what is known as a simplicial complex from the data set. This is also known as a Viatorus-Rips complex, dates back to the beginning of the 20th century. The beginning of the 20th century, actually. There's a paper in, ah, don't want to lie, 1913, 1918, I don't know, by Leopold Biotoris, and he came up with this construction. And essentially, it boils down to if you have a bunch of data, like a point cloud, and you have a metric, so something like the Euclidean distance, then you can build this complex V epsilon here, which consists of all the subsets of all the points that have a pairwise distance of less than or equal. Distance of less than or equal to epsilon. And what this amounts to is you are growing kind of these Euclidean spheres, Euclidean balls, I should say, around every point, and you connect things that fall in this condition. At the beginning, you can see that not a lot of things are being connected, but then as the complex grows, you add more and more things. You also add triangles, of course. So it's not only edges, but you also add triangles because notice that this definition. Notice that this definition here, just showing you the pointer, is encompassing actual subsets. So you're also calculating, well, triangles and tetrahedra and so on and so forth. And at some point, you have reached a very, very high level and then everything is connected to everything else and you're done. Now, why is this useful? The first question we have to ask when we see this is: how can we pick epsilon for this? So, is there like a proper way to do this? The answer is. To do this, the answer is in most of the cases, no, there is not. So it makes much more sense to not think about this as a single scale that you pick and that you have to get right at all costs, but rather to take this as some process that given a set of scales gives you a way to associate topological features with a specific point cloud. And this is where persistent homology comes in. And I mean, for those of you that have more of a bio background, there's More of a bio background, there's an unfortunate clash of terminologies. So, the last time I was talking to biologists, they had different things in mind when I was talking about homology, but of course, they are wrong and we are right. So, this is persistent homology. And persistent homology works by calculating simplicial complexes for literally every value of epsilon. It's still a finite number of values because, if you think about it, it's a finite number of data points that you have, so stuff can only change at finite points. Change at finite points. And you do, and while you do this, you watch how the topological features of the associated simplicial complex change. So, whether you get any holes in there, whether you get any connected components that merge, and so on and so forth. And the important thing to do is while you're doing this, you assign each of these topological features, what is known as a duration or as a prominence value or something like this, depending on when during this process it was created and when it was destroyed. And these features are stored in a Destroyed, and these features are stored in a persistence diagram. So, let me illustrate this to you. We do this for point cloud. We grow, we grow, we grow. We see that, for instance, this hole in the middle closes up. At some point, it's gone again. And then we have this lovely persistence diagram on the right-hand side with one axis denoting the creation and the other axis denoting the destruction. It's called persistence because the idea is that features that you have for a long time in this process of growing an epsilon, so holes that are born. Holes that are born very early on and that get destroyed very, very late, they persist for a long time. So, in some sense, this is the origin of that word. At least that's how Herbert Edelsprunner tells the story. You can think of persistence as some kind of prominence value for your topological feature. High persistence means that it's very relevant. And now, that's an interesting paradigm shift that I should maybe briefly mention here. So, people used to think that low persistence is always indicative of noise, but Is always indicative of noise, but some folks like Henry Adams and his colleagues have been changing this, and they showed that while low persistence can, in some applications, also be indicative of relevant features, it just means that they exist at different scales. And anyway, so without going into the details of how this works, there is a process for this, there is even code for this, there's some nice toolkits for this, it's all linear algebra. Whether it scales very well, that's a different question. Don't ask me that right now. Don't ask me that right now. So, we're just going to ignore this. And I'm going to talk to you about what we can actually do with these descriptors. One fundamental thing that we have a descriptor that we like to do always is we want to have distances between them. So we want to figure out what makes one descriptor more similar to another one. There's two distances that are particularly nice. One is the bottleneck distance, the second one is the Wasserstein distance. Both are kind of similar, but well, also kind of different, of course. Also, kind of different, of course, in what they are capturing. So, oh, wait, I'm seeing the mouse pointer here. This is not good. Anyway, the bottleneck distance is defined as like an infimum of the supremum of some kind of matching. So you go over all the projections that you can have between two diagrams, between the point sets of these diagrams, and you try to find the one that minimizes a certain functional. The Wassershine distance is then a little bit more elaborate in that it takes. Elaborate in that it takes all the points into account. We'll see an example of this, but it's literally the same thing. So you have to solve optimal transport or optimal matching problems between the two diagrams to make this work. Now, one thing that you might be wondering about or that you might look quizzically now in my direction is how does that work if you don't have the same number of points in the diagrams? Well, there's a neat trick for this. You just augment each of the diagrams with projections of the points to the diagonal of the respective other diagram. Respective other diagram. So, point being here that it doesn't matter, it works out even if the diagrams have different bijections. The Wasserstein distance is also kind of nice because it falls into this optimal transport realm of distances, which are still all the rage in machine learning. So if you're, for instance, familiar, there is something called a Wasserstein Gahn, where they use a similar type of loss. And this is kind of a distributional matching that is going on. Now, to illustrate. Now, to illustrate you, maybe the difference between the two distances, if you have these blue diagram and the red diagram, you can also see the cardinality differences. The bottleneck distance is only defined by actually one pair of points, whereas the Wasserstein distance takes all the pairs of points into account. And the gray points, they are kind of ignored by the matching process. They get a cost assigned to this nonetheless, but they are not relevant for all intents and purposes. Now, okay, let. Now, okay, let me show you what we can actually do with this. So, I have one nice application prepared for you, which is also illustrating why geometry and topology can work best in tandem. And this is an application for the prediction of the shape of cells. So bear with me for a second here. This, by the way, perfectly captures what I know about the application, more or less. So, I'm not a biologist, but this is what I understand here. What I understand here. So, pathologists use confocal fluorescence microscopy to obtain images of cells. You can see some of those on the right-hand side. I think this looks pretty cool. They're interested in predicting the 3D shape of a cell from these 2D images. Why would you want to do this? Well, this is what they call morphological analysis, and it's crucial to detect certain pathologies in the blood. So, to quote from a very nice paper. From a very nice paper, when used properly, red blood cell morphology can be a key tool for laboratory hematology professionals to recommend appropriate clinical and laboratory follow-up and to select the best tests for definitive diagnosis. So this is really interesting to figure out what is wrong with people if you have access to their blood. And maybe small tangent here, you might be familiar with this Theranos startup, or hopefully not. So anyway, so this technology in contrast. So, this technology, in contrast to Theranos, this technology actually works. And blood is, for various reasons, is a good substance to have in the clinic because literally every patient that you get, unless they are like extremely in a critical state, they can probably afford to lose like a few drops of blood. So if you have a little bit of their blood and you can do some predictions about what is wrong with them or predict certain things earlier, then this is a kind of nice thing to have. And it also doesn't hurt too much. So we're not talking about pines or Too much. So, we're not talking about pints or gallons of blood, we're talking about a few drops of blood to analyze certain red blood cell anomalies. And then you can detect things like sickle cell anemes and so on and so forth. Now, how can we actually do this? How can we make this work? Here's a super simplified overview of the model. So, we're taking a 2D input image, we're putting it through, of course, deep learning model, because that's how you do it. Because that's how you do it. And we obtain an output image that now lives in 3D. What this is, mathematically speaking, is it's a complicated inverse problem because you're going from 2D to 3D. And so it's also kind of ill-defined because there's a large number of potential solutions and you need to kind of like regularize that and make sure that you get something for your money here. Now, the architecture that we've been using, or rather, that my PhD student has been developing before. Has been developing before I joined the project, some remarks about this later on, potentially. It's called Shaper. And it's essentially like a convolutional architecture that takes the image, scales it down a little bit, and puts it through various convolutional layers and then some fully connected layers to then later on blow it up again. So you can also see our dimensions here. What this does essentially is it's learning a likelihood function from the three-dimensional space. From the three-dimensional space to the real numbers. So, for every voxel in this grid on the right-hand side here on this slide, we have a value that indicates the likelihood of that voxel being part of the true volume or the true shape. So that's what we're learning. That was developed without me doing anything. I have nothing to do with this, but it's kind of cool. Now, the loss function that was used for this, this is the geometry part of the talk, is a combination. The of the talk is a combination of a so-called dice loss and a binary cross-entropy. So, the dice loss is something that compares the two volumes. So, the intersection of the volume divided by the sum of the volume. So, you can also represent it as kind of a true positive divided by true positives, false positives, and false negatives. And the binary cross-entropy loss is, of course, the binary cross-entropy loss. All of this is done on the level of a per voxel basis. And the idea is that you want to figure. And the idea is that you want to figure out whether the reconstructed volume is well aligned with the ground truth one. So that's the geometry part. But now let's see whether we can make it topological, because it turns out that in the geometry-based part, training this model is super hard to do and the results are also not super nice. We have a bunch of examples in the companion publication to this talk. And I'm going to show this on the last slide. You can also check them out on our website. We have some kind of 3D models that you can rotate. Some kind of 3D models that you can rotate. I had some fun implementing this. But to make this topological, we added a few additional ingredients. Namely, we added a sum of Wasserstein losses between the respective persistence diagrams and a regularization term based on the total persistence. So let me walk you through this. The part in red is essentially a metric. It's supposed to align the ground truth likelihood function with the predicted likelihood function of the model. Likelihood function of the model. So we have a ground truth function that some nice people have measured for us. So we know what the shape of the cell should look like. And we have a predicted likelihood function f prime by the model that, well, that is just our prediction. And with this Wassenstein loss, shown in red, we try to align the two. So ideally, this would be exactly the same, topologically speaking. The second part of this loss is more like a regularization term in the sense that it is supposed to reduce the geometrical and topological variation. Use the geometrical and topological variations of the predicted likelihood function. So, this you can see that the term in blue only contains the predicted likelihood function f prime. And so, to minimize this, you need to minimize this overall quantity, which doesn't involve the ground truth. And the idea is here that you want the model to give you nice surfaces that are kind of smooth and that are not like super, super jacked. And this is what the second term does for us. Now, we can combine this and obtain. We can combine this and obtain a joint loss, a geometry-based topology-based loss by choosing some kind of lambda parameter for scaling and then just calculating the sum over this geometry-based loss and the topology-based loss. And that actually seems to work. So, okay, well, this slide is maybe not super appropriate for this audience, but if you take a look at the respective second row of every one of these partitions, you will find that in almost all the cases, except for the last one, The cases except for the last one, the errors that we show on the metrics that we show on the left-hand side, the errors go actually down. So, just by virtue of adding this small loss term here, this topology-based loss, we end up improving the quality of the reconstructions substantially. We obtain better results in terms of an IOU error, so an intersection over union error, in terms of volume, in terms of surface area, and in terms of surface roughness. And all that, and that's kind of the cool. And all that, and that's kind of the cool part, at virtually no additional computational cost because we can calculate these topology-based loss terms even on smaller, downscaled variants of our data, because topology is relatively robust to this. And so it doesn't cost us a lot to do this computational speaking, and it integrates well. Now, that's all I have for you. This is like a very, very short run through this type of project, but I want you to take away a few things at least. Want you to take away a few things at least here, so some key messages. So, first of all, I hope that this at least started to convince you on a path that topology can provide some useful inductive biases for these shape reconstruction tasks. I also want to stress that these persistence diagrams, and that's kind of a recent finding as well, they encode geometrical and topological properties of data. So it's not only about the topology, even though. So, actually, maybe our whole field is misnamed, and maybe this is something we should discuss. Misnamed, and maybe this is something we should discuss. And I don't want to vex on this too much. But the fact that, for instance, persistence diagrams can capture some kind of curvature properties of a space is really, really surprising when you come from this, from first principles and when you say, oh, it's topology, so it doesn't know about this, right? But so anyway, these persistence diagrams are useful descriptors here. And the last point that I want to make, and this is maybe my major point, is that the integration into, well, let's say standard in air quotes here into standard machine learning models is actually. Here into standard machine learning models is actually possible and feasible nowadays. So, this really works. It's like the model doesn't care that topology is fundamentally computed in a different fashion than most of the deep learning things that we're saying. So this kind of works. And this is what I want you to take away. So there's two key publications that I'm shamelessly promoting here. The first one is a survey of topological machine learning methods by Felix, Michael, and myself. This has been a while. And myself, this has been a while, but this is maybe a nice way to go into this because we're looking at what people are doing in this field and how are they doing it. And the second one is the publication that belongs to the results that I showed you in this talk. It's with a bunch of folks from Helmholtz. And the first author is Dominic, my PhD student, or rather the PhD student that I sort of inherited from one of the PIs there and that was kind enough to work with me. Was kind enough to work with me on this topology-based project. It's supposed to appear somewhere in Mikai, but since we don't know how this goes, and since Springer is not super fast in typesetting the books, and I just realized that this is recorded and I have jeopardized my career. But anyway, there I said it. So yeah, so they are not super reactive. So I don't know whether it will be published. But in the meantime, you can find it for free on archive. And now the last point that I want to make, and again, continuing with the shameless self-promotion, I'm really sorry. Continuing with the shameless self-promotion, I'm really sorry about this. Someone has to do it. And ain't nobody going to do it for me. So I want you to point to PyTorch Topological. If you like PyTorch, like this deep learning framework, then maybe you can guess what PyTorch Topological should do. It's a shameless and uncreative variant of PyTorch Geometric, of course. But if you have a better idea, we could also call it PyTorch Donut or whatever. But we've been developing this now. And by we, I mean, I mean, well, a bunch of my colleagues. I mean, well, a bunch of my colleagues and myself. And this provides a few things that you can throw on your network to make it topology aware. And yeah, let us know if you like this or if you don't like it. Also, let us know, right? Yeah, thanks for your attention. And also, thank you very much for my co-authors who can't be here and who might not be looking right now, might not be watching, but let's thank them anyway. Of course, so Carson, Dominic, Felix, and Michael, thank you very much for this. It was great fun. And I hope. This was great fun, and I hope that these projects continue in the future. Thank you.