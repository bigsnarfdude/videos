Okay, thanks a lot for the organizers. So the tutorial lectures, I guess, can be more informal. So just feel free to stop me or you can even say, oh, I don't want to listen to this. You want to listen to something else. Just tell me and I can always adjust my sort of general picture. You know, some sort of general picture of what kind of questions we are working on. So it's it's even not maybe directly related with my own research. So this tutorial series is about stochastic colonization. The chalk is much much better than Chinese brush for me. So the so what's the what's the So the so what's the what's the case quantization? Just like you study PDE from uh physics problem like uh general relativity or something. Here we study stochastic PDE uh from quantum field theory. So it's about the relation between uh stochastic PDE and quantum field theory. So here the stochastic basically comes from the quantum sort of random quantum fluctuation. Fluctuation. So maybe part one of the part one for today is what is quantum field theory. There are all different formulations of quantum field theory, and for us it means some sort of infinite dimensional measures. So usually in probability the measures just in or maybe measures on Rn or something, but here we are going to have measures on function spaces. So for example, So for example, it can be, in general, okay, in general this sort of measure look like, you know, in classical mechanics, physical model is defined by some action functional. And you want to minimize this action functional, you get Euler Lagrange equation, which P D E. Here we want to look at, so we want to quantize them, means we want to look at exponential of some functional. times the form of the big measure. And so in general, the sort of form of the measure we consider look like that. For example, the simplest action function all would be the direct late energy. So it's e to the integral of maybe over R D gradient phi Gradient phi square d phi. So this would be an infinite-dimensional Gaussian measure because this is like e to some quadratic functional, at least formally it's Gaussian. This is called Gaussian free field and uh uh actually this can be rigorously defined uh because Gaussian you know infinite dimensional Gaussian measure is uh now standard theory in probability. Now, standard theory in probability. But before that, let me just give you more examples. So actually, more interesting models in physics are not Gaussian. To make it not Gaussian, you just put some non-quadratic terms here. For example, so this is so-called Phi 4 model, where you have a so-called mass parameter. So-called a mass parameter, and then you have a quartile term. That's why it's called a Phi 4 model. This is called a Phi 4 model. And another interesting example, essentially coming from condensed matter, is so here we focus on 2D and And then we put a cosine term in the functional. Okay, so this is also a non-Gaussian measure. And this model is very interesting when people study a lot of two-dimensional models. And there's a famous so-called costless. And there's a famous so-called uh costlis-Taulis phase transition. Uh the phase transition depends on this uh parameter beta. So at beta square equal to eight pi, there's uh this phase transition. Um and there are a lot of story about this, but maybe give you more examples. So all these models have some sort of symmetry. For example, here you can shift phi by a constant, you know, this function. Constant, you know, this functional is invariant, right? And here it's also periodic. So if you shift phi by 2Ï€ over beta, integer multiplied by 2 pi over beta is also invariant. So all this model has some sort of symmetry. But a more interesting sort of symmetry is so-called a gauge symmetry. So the following model would have a gauge symmetry. So here, all these models, the five So, all these models, the phi is this function, but here the A is a is a vector field. So, here A is a has D components, and the notation F A, okay, F A is a so-called two-form, it just means it has. form, it just means it has two indices here instead of one. So A has one index, F has two indices, and F of ij for ij between y and d is defined to be essentially the curl of A. So this is also quadratic. And this has a nice symmetry, which is if you shift A by gradient of any function. Gradient of any function, so we know the curve of gradient is zero, so this is invariant. So, this is called a gauge symmetry, it's an infinite-dimensional symmetry. Okay, um you can also make this non-quadratic by coupling with another field, which is called Higgs field. So you can add another term. Um uh now you have Now you have two fields, A and Phi. So this is a model with two fields, where the D phi is so-called a gauge covariant derivative, and this is the so-called Higgs model. You might have heard about Higgs mechanism and you know Higgs Bosong and so on. So this is the model. The model. So this is defined to be where the phi take a value in complex numbers. So this is a non-quadratic interaction because it has a times phi and then you square that. Okay. And then there's a very important number. And then there's a very important non-abelian generalization of this. So here the symmetry is abelian because you shift by f or f prime is the same as the other way around. So by Young and Miels, so this is called a Yang-Miuse model, which is a non-ability angularization of that. So here the model looks the same. Looks the same, but now each AI take a value in the Lie algebra. So each AI is a is a matrix value field. And now the F is a nonlinear in A. So there is a Lie bracket of A i and A J. Ai and Aj. So this is called a Yami Os model, and it has a non-abelian symmetry. So the symmetry is I can write it down. So, but it's not, okay, I just give you some feeling how these symmetries look like. And g is a okay, g is a function taking value in a Lie group. But this is not important, just some examples to give you some impression what our concept. Some impression what our quantum field theories. And the point is that these are all formal. These are all formal measures. Formal, okay, I said the Gaussian free field can be rigorously defined, but these guys can, in general, they are only formal because of singularity problem. So that means Problem. So that means the Gaussian free field actually is supported on so the almost surely the Gaussian free field has regularity minus d minus 2 over 2. So when dimension is 2 or higher, this is a negative number, which means the phi is not a function. Phi is like a generalized distribution. So So, if phi is a generalized distribution, then all these powers or functions phi, and here various nonlinear terms, would not have a classical meaning. Because this is a sign-order model. Okay. Uh so So they are just formal measures for different reasons. So as I already mentioned, we have a small scale problem. Small scale means the small scale the field fluctuation is very very large and that's why the symptom the field is almost surely very singular. And also large Large scale problem because now, okay, so basically, how to make sense of such infinite dimensional measures? You want to maybe first put first find a finite dimensional approximation, right? You just replace R D by some lattice and replace the integral by a sum and this by a finite difference. Okay, then you have a finite dimensional well-defined measure. But then you want to send the lattice spacing to zero and also the lattice to the whole. The lattice lattice to the whole lattice, right, to the whole Z D or R D. Um, but it's not clear whether you can always uh uh send the volume to infinity. So whether this sort of infinite volume limit exists or unique is a large scale question. And then of course you want to normalize right, so you want to define this to be probability measure. So the question is whether this can be normalized to to be Can be normalized to be a probability measure. A related question is a large field problem. So large field problem means for these to be a good measure, they have to have a good tail, right? So here you see when phi is large, the probability is very small because, okay, I forgot to say important lamb. Okay, I forgot to say important lambda is positive. So this is important when you want to integrate something against measure. So for phi, four model, this is the probability indeed is small when phi is large, which is sort of obvious. But for Yamios model, this is not clear at all. And then in this circular singular setting, what's the meaning of all the symmetries? So here I define the gauge transformation and so on, pretending that the field is smooth, but when the field is a generalized function, a distribution, how to make sense of such symmetries. Okay. That's called the field theory. And And part two, stochastic quantization means I want to introduce some sort of PDE or stochastic PD to study the measures. So here, you know, when you have some sort of function now, you can study the gradient flow, right? And now we have such measures, and we're going to look at the stochastic gradient flow. flow. So here my S is this action functional. And I look at this, this would be a gradient flow for the action functional. But then I add a so-called space-time Y noise. So space-time Y noise just uh is just a uh mean zero Gaussian field, right, um with uh with a Dirac correlation. So Correlation. So, this is a formal way of writing this covariance. And because this is a generalized function, so you need to integrate against test function to rigorously write a definition. But the point is that, okay, so first of all, this is also a formal dynamic at this level for the same reason. For the same reason in quantum field theory. And why do we study this? So the relation of this equation and these models is that these formal measures are formally invariant measure for this dynamic. Okay, so invariant measure. In case your deterministic In case you are a deterministic person, maybe you maybe so environment measure just means when we say mu, a measure mu is an environment measure for dynamic X, right? So X is some dynamic, for example, this dynamic. And this just means the following. So so basically it just means if the probability law of x zero is mu, Is mu, then, well, the probability law of x and any t is also mu. So suppose at time zero, this is a random function, and the distribution of random function is mu, for example, mu is one of these measures, then the law is not changed over time. Or you can also Or you can also think of environment measure in the following way. So for any initial condition, right, so might be deterministic or random or whatever, then, okay, it's up to some technical condition like uniqueness, environment measure, and so on. But heuristically speaking, just means the x when t goes to infinity. Infinity, the law of this is going to converge to mu. So just like gradient flow eventually converge to a critical point or minimizer of the action functional, and for a stochastic gradient flow, the property law is going to converge to that environment measure. So in this stochastic quantization program, This stochastic quantization program, we care about two questions. The first question is: you can just study the stochastic D, where S is one of those examples. The second question is, can you say something about the quantum field theory measures by studying the property of this equation? Okay, so we focus. So, we focus on the second question. So, how do we use such dynamics, such equations, to study the quantum field theory? Roughly speaking, there are several ways to do that. So, one way is, as I said, these are infinite-dimensional measures. We start with a finite-dimensional approximation. We started some finite-dimensional approximations. In that case, we're going to have a lattice, like a finite lattice, and we discretize the equation on the lattice. So it becomes like ODE. And then we pass the continuum. Okay, so we take the maybe the lattice facing to zero and so on. So we get infinite-dimensional. Infinite-dimensional limit. And of course, the question is: why can you pass a limit? So as lattice spacing go to zero, can you get some uniform bound? And in fact, we obtain such infinite such a priori bound using the equation. So it's not so easy to get some bound on the measure, but maybe we can get some. Um maybe we can get some bound on the measure by proving some bound uh of the solution of this equation. And since the probability law of the solution is just mu, right, so this stationary, then uh once you have some bound on the on the x, maybe you can you can bound the law. Maybe you can get some uh limit, like an infinite dimensional measure mu. So that would be one strategy. Another strategy is a Another strategy is uh you can also do the following. So just don't uh if you forget about the lattice, you just think of this equation in continuum. Um you first prove some sort of um well postness, right? So the first question is of course well postness. So uh short time exists and uniqueness. And then try to show some sort of bound independent time. Once you have a bound on a solution independent time, there's something called a Krilov for Goliubov argument. So it's called a Kriloff. Well, roughly speaking, it just means that you can so you just solve your equation for time t, and then you send t to infinity, right? So you average the law of your solution over time, and then send time to infinity. And this theorem tells you that as t goes to infinity, that measure is going to converge. Uh that measure is going to convert to environment measure. Okay, so that's another way to obtain uh this measure mu. Okay, um so actually the class quantization can be more general. Uh this is just one way to link a uh quantum field theory with uh uh with the analysis problem, right? So this environment measure. But there are also different ways such as a renormalization group. As a renormalization group or a variational approach, maybe Wei Jin will tell you tomorrow. So I can show you some examples how to get some control, get some limit of the measure by working with this equation. Okay, so how to Um how to obtain some limited point of filter measures by studying this equation. So I'll just uh um focus on the five four model. So for five four model the the example here The example here becomes the equation which is la plus phi, right? You just take the gradient of this phi 4 action. So here I have this m square and then lambda phi cube plus space-time y noise. So there's a famous argument of the Prato de Bush. Of the plateau, the bush, which says the following. So actually, maybe I should have said this earlier. So for this Gaussian measure, the corresponding equation is a linear equation, right? So for the Gaussian measure, you just have a heat equation without these two terms. So heat equation plus a noise. And it's And it's very well understood. So, heat equation, the solution is Gaussian. You can just explicitly write down the solution. But when you have a nonlinear term, it's not so clear. Not so clear again because all those problems. So, small scale, so the solution is a generalized function. And large scale, can you solve the equation? Large scale, can you solve the equation for a long time or entire space and so on? So the problem the Bush said since the linear equation, so without this term, is well understood, in particular you know this is a Gaussian solution is a Gaussian random field. The Gaussian random field, then why not just subtract the Gaussian from that equation? So it says I can, in order to understand that equation, I just write a phi into y plus z, where y solved the linear equation. And then you can just use the two equations to write down the equation for z. So So right, so y plus z cube is equal to this. Okay, once you have this, okay, basically. Once you have this, okay, basically the idea is that y I know it's a Gaussian process. I can compute all the moments of y and so on. And z is not a Gaussian, but you hope that the z has better analytical probability, maybe. Okay, so and then you can do whatever, you know, all sort of PD techniques apply to this Z equation. For example, you can do L2. For example, you can do an L2 estimate, right? Energy estimate. You just multiply Z and integrate. You just get something like this. So this is standard L2 estimate and then And then I have this, okay, so uh the point is that I want to use the z cube as a strong coercive term, right? So lambda is is is positive. So that's why when I multiply z on the two sides and I move the z to the power 4 on the left side. So the right hand side I still have these terms. These three terms basically. So Okay, so this is a sort of basic prototype energy estimate. Um you can apply like a Young inequality and so on to um to the right hand side and uh you can get some bound like this. You can get some bound like this. So say you can get 1 over 10, maybe left hand side plus some powers of y. So each term, maybe you use Young inequality, and then the important thing here is that the The powers of z here are all lower than 4. So you can bound them by z to the power of 4 times a small constant and plus maybe some power of y. So the power z can be absorbed. So these terms are basically those terms on the left-hand side. You can absorb them to the left-hand side. So you can move these terms to left-hand side and right-hand side only depends on y. And the y is this Gaussian process. And you can do many things about the Gaussian. For example, this can allow you to take the infinite volume limit. As I said, you can start with a finite dimensional. You can start with a finite dimensional approximation, right? Put the measure on the lattice. And now the question is: can you pass the so you put the measure on the lattice? This will give you a finite dimensional measure, and dimension depends on the number of lattice sides. And then you want to pass to the infinite volume limit, so get a measure on Z D. Well, how do you do that? You can just use this energy bound. So basically, you can use So basically you can show that the um to get infinite volume limit, uh you can you need some sort of weight, right, as usual, um some weight which tell you how bad the behavior at infinity. By the behavior at infinity. So here you can use some weight like this. So here L is some parameter. So L and sigma just parameters. Then you can prove this bounds where these bounds where the L2 L4 are weighted Lp, weighted L2, L4. So here L2 means L2 on Zd with some sort of polynomial weight and L4 on Zd with that weight. Right, so here this just means the weighted LP space. means the weighted LP spaces. And since I know that the dynamic is the measure is invariant, right? So this means the property law doesn't depend, right? So I so here when I write Y and Z, I just mean they are stationary, so their probability law doesn't depend on time. In that case, so now what you can do, now So s so now what you can do, now now probability comes in. So you just apply expectation over both sides, for example. Okay, so you compute expectation of both hand sides. And then the expectation of the norm of z doesn't depend on time. So the time derivative of this is zero. And then the Then the right hand side, so I already moved this term to the left-hand side, right? So left-hand side comes to 9 over 10 maybe. And then the right-hand side, there are powers of the Gaussian. You can always bound the excitation of powers of Gaussian. So this means that I have the following thing, which is excitation of Z at some time t, actually doesn't depend on t of L2 norm. So on the left-hand side, I have that L2 norm is bounded by say weighted L4 norm of y, for example. Of Y, for example. So then, okay, maybe I can skip some details. Basically, because y is Gaussian, right? So y is Gaussian, you can just compute the the our four norm, our p norm of the y. And then if you choose the parameters in this weight function. In this weight function properly, then you can prove that this is bounded. So by using this sort of PD estimate, I get a moment estimate for Z. And in probability is a standard factor. Once you have a moment estimate of some random field, then you know tightness. Okay, so this means that when my lattice goes to infinite volume, Goes to infinite volume, right? So we get an infinite-dimensional measure. In this limit, I have a tightness, so I know my field is tight. So just like in analysis, you if so here the D is arbitrary. So when the when we take the small scale limit, the D is important. But this argument for infinite volume, the Infra volume, the D is not so important. And so this means as some tightness, just like in analysis, if limit is hard to prove, you prove some compactness. In probability, you prove tightness, right? So tightness of so another So another example is the so tightness of Z as the lattice goes to entire lattice C D. So the uh no the the law doesn't what is translated? So the law doesn't uh The law doesn't uh invariant on the translation. Yeah. I'm not doing any renormalization. So yeah, so I haven't done renormalization. Um so yeah, so maybe this this related with the next question, small scale problem. Small scale limit, right? So now I take Now uh I take a lattice with lattice spacing epsilon and uh I want to pass the continuum limit. So pass to Z D. And now the dimension is very actually very important. Now let's just consider 2D in fact this quantum field series when you want to take continuum limit, they need to be renormalized. Continuum limit, they need to be renormalized. So renormalization means the following. You need to introduce some renormalization constant here, which depends on the lattice basic epsilon. So this is called C epsilon phi square. So again, this integral means the sum and the gradient means finite difference on the left. Difference and all that. So in 2D, the C epsilon is divergent. Okay, so when dimension 2 or 3, the C epsilon is divergent. So in 2D, it diverges like a log. And the idea of renormalization, so this is called a renormalization constant. This is maybe the most important idea in quantum field theory. Idea in quantum theory theory. So it says that although the constant is divergent, when you have this constant, the measure will converge to a non-trivial measure. So at the level of the equation, now it's also re-normalized. So here we have a Have a we have a like a discretized y noise on the lattice, also write as C. And then the equation is also renormalized. The point is that you must have this renormalization constant for you to get a non-trivial limiting solution. Okay, so with this. So, with this renormalization constant, here the c will enter this z equation. For example, the y square will become y square minus c. And one can prove that the limit, as if some go to zero, this has a has a non-trivial limit. This is called a weak power. Power, usually write this, okay. This is called a weak power, weak renormalization. And similarly for y cube, y cube is, y cube becomes y cube minus 3c y. So, of course, the question is: how did we find this constant? Y diverge like a log epsilon. Basically, Uh basically uh it boils down to Gaussian moments calculation. So here again y is a Gaussian process, right? So you can compute the moments of y square. In fact uh the excitation of y square in 2D excitation of y square is divergent. So basically the covariance of y, so if you compute the covariance of this Gaussian field y, the covariance This Gaussian field y, the covariance is just a Grande function of Laplacian. So the excitation of y squared is just the Grand function at zero. So in 2D, the Grain's function at 0 is logarithmic divergent. So the C epsilon is just defined to be the excitation of y squared. So by subtracting this C, you make the whole thing mean zero. And then And then you can prove that any moments of this guy is bounded. So without a C, the moments of y square square will be divergent. But after a subset of C, the moment of this is finite. Okay, so that's how we find the C. Okay, so in other words, my so back to the energy estimate, right? So we can still use the energy estimate to take continuum limit. But now the y cube and y squared become the weak powers. In this case, one can again prove some boundaries like this. So Some bound like this. So it's not simply young inequality. You also need some inequality by interpolation or embedding or so on. But basically you can show you can still separate the separate the Z and the Y, right? So so h these are these are terms about Z, these are terms about Y. Z, these are terms about Y. So now, but you need to be careful what kind of norms Y. So here, so powers of Y. So as I said, in 2D, the regularity is slightly below 0, right? So like a 0 minus. So here, it's a 0 minus normal y. This is zero minus normal y and okay, so so the right-hand side can be bounded by these powers, powers of this norm, so y. Okay, so you can prove the limit actually holds in this spaces in 2D. So now with this boundary you can prove something like this anymore because it's finite volume. finite volume. Uh so um so you can you can show that so again this the same sort of uh idea. So apply the excitation on both sides and then the the moments of the Gaussian or Gaussian square and so on they are finite. So I get the excitation of the left-hand side. So I get the excitation of the left-hand side in that energy estimate is bounded by some uniform constant. So as epsilon goes to zero. Okay, so in this way you can also prove the tightness of this guy. So when when you pass the continuum limit. Pass the continuum limit. Okay, so here the tightness implies the tightness of the measure because the measure will already be composed in two parts, y and z, and y is Gaussian. So y always has limit, and z now by tightness you can pass a sub-sequential limit. So So, these are some examples. So, actually, you can apply various PD techniques to this equation. Here is energy estimate. You can also do some maximal principle and also prove such continuum limit or infinite volume limit. For example, So uh this is a paper by okay, maybe you can Google Weber and other people. So Weber, Mura, and the Weber students, Muna they can also prove a very strong version of maximum principle on that equation Z. Principle on that equation Z. It goes like this. Here, okay. Basically, you want to use this sort of argument to construct the measure. So, here the crucial thing is, the crucial step is that we want to prove. The cruel step is that you want to prove some bound independent of time so that you can send time to infinity and therefore use the Krilov-Bogolubov argument. So in their case, they can prove the following kind of boundary. So here is a space-time. So this is, so maybe you start with a finite domain and then finite time. So So they can prove, okay, first of all, you can prove a usual maximal principle over this domain, okay, over this space-time domain. But they can prove something stronger, which is if you're away from the boundary and also from the initial condition in time zero plane by some distance, say R here. Then Then the maximum principle tells the supremum of the z, where the z solves that equation. So supremum over that shaded domain can be bounded by okay, basically just as in the energy. Just as in the energy estimate, here you also get powers of y on the right-hand side. Okay, so just like these powers of y, powers of Gaussian. And the important thing is that the right-hand side here, in this sort of strong version of maximum principle, the right-hand side does not depend on any information about the initial condition and the boundary condition. And a boundary condition. So they all they call this uh coming down from infinity, which means at time zero, if your solution is huge, then after short time, if you look at the maximum of your solution here, the maximum doesn't doesn't feel anything about a very, very large initial condition. So this is very important because, you know, if you You know, if you can, you can first prove the short time well postness, right? So run the solution for some time, t, then the question is whether you can solve again from t to 2t. Since you have this sort of bound, so basically you can forget about the size at time zero. So that's somehow. Time zero. So that's somehow how you can sort of glue together the solutions over all the time intervals and get a global bound, or get some bound independent of time. In 2D, this also in 2D and 3D. So maybe I think that's a good idea. Um so maybe I conclude by uh by remark. So the dimension here is very important because if dimension is three, so in 3D the the the you can still do this renormalization and the limit um you can prove this as a limit, but the limit is not in C0 minus. So here uh this would be c minus 1 minus and And when the regularity of this is below zero, then for example, you look at this term, right? So this term. If this guy is below zero, then your analysis, you know, to deal with this product, the regularity of z square should be above positive one. If this is below minus one, then this should be above positive one. So if this positive one, So if it's positive one above positive one, then actually the left hand side doesn't have any, right? So here this is H1 of Z. So the left-hand side doesn't have anything to control the like C1 plus norm of Z. So that's somehow the that's the reason that this argument, this this simple argument is decomposed into two Simple argument, you decompose into two equations and do this for the maximum principle and so on. This would fail in 3D. So in 3D, because this is more singular. So that's why Martin Hayer introduced regular structure. And the first application of regularity structure is a short time well-postness for this equation in 3D. So regular structure would be a structure would be a much more complicated decomposition in some sense rather than this. So you need to have a good answer for the local behavior of your solution Z and then solve the equation in the space where you have this sort of short time behavior. But I guess for today, just maybe some Yeah, for today I just maybe some um give you some general feeling what kind of uh uh question we we care about. So basically first for such singular SPD we need to renormalize um and do some decomposition and you can apply some uh standard P D arguments. Okay, I'll stop here. Thank you.