Tom Carol, all the way from Imperial College, who will talk to us about the path characteristic function, free probability and signature curves. So, over to you, Tom. Okay, thank you very much, James, for the introduction. Thank you to the organisers as well for the invitation to speak. I'm sorry I couldn't be there in person, but I hope you're all enjoying yourselves with what looks like a very exciting program and also enjoying the fresh air and mountain scenery in Banff. All right, okay, so what I'm going to talk about today. All right, okay, so what I'm going to talk about today, I think, are some initial results really with a graduate student of mine, William Turner, at Imperial. It very much fits into the theme of this data SIG project, which you can see here advertised on the first slide. I think that there are potentially interesting connections with the theme of this workshop, which is why I've selected it as a talk. But it definitely comes for reasons I'll explain from or is inspired by. From or is inspired by some practical applied mathematics. What I want to do is, first of all, give some introduction and background to members of the audience perhaps who are not so familiar with the signature, the signature transform, as it's sometimes called, which is going to be a recurring object throughout this talk. So, I'll give a few words of just definition. I'll give a few words of just definition, background, motivating properties, and drawing your attention as well to some key results that have been proved in this area as well. Once I've completed that introductory setup, I'll explain in a bit more detail the sorts of results that we've been able to prove so far. And also, perhaps to highlight where some of the shortcomings of these results are and where there's Are and where there's potential need for some new insights to make more progress. There will be a bit of geometry in this talk, but not very much. But I think there's the potential for there to be more as the subject develops. All right, okay. So perhaps this definition will already be familiar to some members of the audience. The signature transform or the signature, and I'll typically be dealing here with continuous bounded variation paths. I'll assume that they start at the origin. They take values in some finite dimensional vector space V, which I'll later on take to be the Euclidean space. You can view the signature in a number of different ways. Essentially, what it is, is an element of the extended tensor algebra, so a hierarchy of tensors, a sequence of tensors. A hierarchy of tensors, a sequence of tensors. You can view it as a kind of non-commutative exponential, a solution to a specific controlled differential equation. And there are theorems which allow you to understand how the collection of the coordinate iterated integrals somehow provide an analogue of the monomials on the space of unparameterized paths. And I want to sort of unpack a few of those definitions before we get into the core subject. Before we get into the core substance of the talk. So, as I said, you can interpret this as a kind of non-commutative exponential. The product here is the product of the tensor algebra product. There's a long history, and going back at least to Chen, I won't really explore many of these antecedents too heavily, but primarily I'll be basing my talk on the material. The material in the 2010 paper of Hamley Lyons and a lot of the work that's been done subsequently on this thing. So you can realize this as iterated integrals, at least if gamma is smooth enough. So in the bounded variation case, this is certainly something that can be done. And then the k-th order term in this signature, so the element of the kth tensor product space of v with itself, is just going to be the iterated integral of gamma against itself. And this is going to be the same. And this is going to be a k-tensor of a v. And the signature is just a collection of all of these objects. So the signature has nice properties which make it allow one to understand it as providing a summary of the underlying path gamma. One of these features is that it's invariant. Is that it's invariant under reparameterization. So, whatever this object is recording about the path, it's not sensitive to how quickly you're moving along it. If you take your path gamma, reparameterize it, and compute the signature. You don't change the variation regularity by doing that, so it's still compute its signature, and the signature is unchanged. It also interacts very nicely with natural binary operations on path space. So, if you take two paths. Path space. So if you take two paths, one natural thing that you might want to do with them is to concatenate them. And the multiplicativity identity, which I've written here, Chen's identity expresses the fact that the signature of the concatenation will be the tensor product of the signature. So another way of saying this actually is that the range of the signature map is closed under the tensor product. And so you can always find. And so you can always find a new path, which is given by the concatenation of the two, which is the tensor product of the signatures. Okay, so this is a non-commutative product, so the order of the product has to reflect the order of the concatenation. Equally, one can identify inverses by taking a path and running backwards here. So here I'm translating the path so that it starts at one. It starts at one. Okay, so what you can learn from this last property in particular, or through other examples as well, is that since any constant path will have signature that coincides with the unit in the tensor algebra, but there may be different paths which are different as path and in classical path space, but which have the same signature. But which have the same signature because you can take any non-trivial path, you can concatenate it with self-run backwards, and you'll get the same signature as if you computed the signature of the constant path. And so, this is what motivates the notion of unparameterized path space. So, clearly, trivially, you can define an equivalence relation on path space by saying that two paths are equivalent if they have the same signature. What is non-trivial is that it turns out that the significance is the same. Non-trivial is that it turns out that this coincides with a purely path-based notion, and by that I mean a notion that has nothing to do with signatures a priori, called tree-like equivalent. So proving that this is an equivalence relation is itself a non-trivial task, and that's one of the key contributions of the Hamley and Lyons paper that I mentioned earlier. I won't explain what this is, but the Just to capture this definition of what I mean by unparameterized path space, it is the set of equivalence classes on classical path space that you obtain through either one of these equivalent notions of equivalence. Okay, so the unparameterized path space is simply the set of equivalence class tree-like equivalence classes and the tree-like. And the tree-like equivalence classes, any two distinct elements have the property that their signatures coincide. Okay. What has prompted some of the interest in the practical use of the signature recently is the following observation, really, and its extensions that That if you consider the coordinate iterated integrals, I mean, well, more generally, if you consider the class of functions on unparameterized path space that are induced by the linear functionals over the tensor algebra, then these form an algebra. And this comes about from the shuffle product relation. And you'll have to excuse me, I've just noticed that there should be a shuffle product in between the F and the G here, and equally in this final equality. In this final equality. But nevertheless, don't think too much about that. You can easily see that something like this has to hold by going back to the definition of the signature in terms of iterated integrals. But the basic message here is that this class of functions forms an algebra. So the pointwise product, they're clearly well defined on unparameterized path space because of the way in which it's been defined, the pointwise product. Has been defined, the pointwise product can be reassembled and expressed as another one of these functions associated to some induced by some linear function in the way that each of the terms in the product were. All right, okay, so the reason this is important then is that you can show that this, if you're given a topology on the space of unparameterized paths. Space of unparameterized paths, then you can show that this class of functions is dense in the uniform topology on compact sets. So one has to be a bit careful when talking about topologies on unparameterized path space because you've introduced this quotienting. And there's a recent paper different addressing this topic, which my student Will Turner and I wrote a couple of years ago, appeared in JFA. Wrote a couple of years ago, appeared in JFA earlier this year. Okay, so one can deal with this canonical algebra of functions associated to linear functionals in this way. What's become of interest in recent years has been to study functions on unparameterized path space that are induced by kernels. And this class of signature kernels, I'll talk more about in the subsequent slides. Subsequent slides, but here you essentially introduce an inner product on the tensor algebra, the extended tensor algebra, and then this induces a kernel, so a function on pairs of unparameterized paths that you can obtain through computing signatures and then taking the inner product of these signatures. And I'll give concrete examples of this later on, but this has proved to be a very useful tool when. Useful tool when you want, for example, to describe a sequence of data using the signature. You want to be able to have access to tools that will allow you to compute useful quantities which involve the signature without actually ever having to compute signatures or iterated integrals or anything like that, because these potentially involve very high-order objects and can become computationally quite involved. And there are such tools for these signatures. And there are such tools for these signature kernels in some cases, and I'll describe these later. All right, so okay. Okay, so let me get then a bit more down to the detail of the objects that I'll be working with. As I mentioned, V, you can think of as being R D equipped with the standard Euclidean inner product. Delta S T is going to be the two simplex. I'll mostly be dealing with bounded variations. I'll mostly be dealing with bounded variation paths. For those who are interested, I think there are potentially interesting extensions to rough paths, but I won't really address these here. X, curly X, is going to be the space of finite length paths, continuous, starting at the origin. I'll be dealing with spaces of matrices, n by n matrices, over some field F. And in this talk, F will be. In this talk, F will be either the real number or the complex numbers. WNK is going to denote the words in n letters of length k. I've already mentioned the signature. It's useful for some of the subsequent discussion to allow a slightly different perspective where I can actually vary the interval over which I compute the signature. And from this point of view, I can regard it as a two-parameter path taking values in the tensor algebra. But otherwise, Algebra, but otherwise, the definition is unchanged. I've already mentioned coordinate iterated integrals. So, if I fix some, let's say, the canonical basis, I have a word of length m, then s i st is going to denote the coordinate iterated integral of the nth order term in the signature. All right. Okay. Okay, so signature kernels were originally introduced in a paper by Franz Corally and Harold Oberhauser, which was published in 2019. So they've been studied extensively recently. I'm just going to move, I don't know, right, okay. These have been studied extensively recently by a number of people. And so I've already motivated this idea in the early I've already motivated this idea in the earlier discussion. I motivated the idea that a kernel is somehow an object that on unparameterized path space is a function which will return a value when you give it a pair of unparameterized paths. And the ordinary signature kernel, as I refer to it, is a particular instance of this where the function is obtained by this canonical. By this canonical Hilbert-Schmidt inner product on the tensor algebra. So you have the Hilbert-Schmidt inner product on the nth tensor product spaces, and then you can just impose this inner product then on the tensor algebra. The properties of the signature will guarantee that the norm of any signature of the bounded variation path is finite under this Hilbert Schmidt norm. Hilbert Schmidt's norm. And so this is a well-defined quantity. And with this particular choice of inner product, it's nice to work with in a practical point of view because of this feature that I mentioned earlier, that you can compute this quantity without ever having to compute signatures. And this is a fairly simple but useful observation. And I'll write it here in the form of a functional equation. In the form of a functional equation, so an integral equation, because later on we'll introduce a different kernel which can only be expressed in this way. And I want to draw a comparison between the two of them. So it solves this functional equation. Now, if gamma and sigma were differentiable, you could write this as a second-order PDE. And then you could use something like a PDE solver to actually get your hands on the solution numerically. And this is. The solution numerically, and this is one of the reasons it's important without ever having to compute signatures. So there's a labor-saving, computationally efficient approach to computing this type of signature kernel. And of course, this structure comes from the particular choices in a product that we've imposed. All right, okay, so. Okay, so I want to take this in a slightly different direction then. And instead of looking at the signature features, one, and this goes back really to a paper of Chevirev and Lyons in 2016, one can think about developing the path onto some matrix lead group. I want to consider some simple examples of this now and use this to prompt the question. Prompt the questions that I'll study later in the talk. So, the general setup here is that we have some matrix Lie group, n-dimensional, and we want to study the development of our path gamma onto g n. So, by studying an evolution of this form. So, m here just denotes a linear map. Just denotes a linear map from V, that's the space where gamma lives, into the Lie algebra of this Lie group. And then here we have matrix multiplication. We solve this evolution. This is our development. And two examples that I want to study carefully. The first is where, as I mentioned before, the field is taken to be the real number or complex numbers, and I consider the general. Members and I consider the general linear group with its corresponding Lie algebra. So this is a very simple case. And then I want to consider the compact case. So where F is taken to be, where the group is taken to be the unitary group with the algebra of anti-emission matrices. And in this case, because I will look at this object later on, I'll introduce the notation script u gamma. Notation script u gamma m to denote the terminal time solution of this evolution associated to the development map m where the input is the path gamma. So the basic question I want to address is what happens when you randomize this development map and then consider suitably normalized limits. Limits as the dimension of the lead group tends to infinity. And of course, this is quite this and questions like it have been well studied in random matrix theory. And part of the contribution here is to repurpose some of those techniques so that we get a better understanding of this signature kernel in the case of the general linear group as being represented as the asymptotic limit. And in the case of the unitary group, we recover a different. Group, we recover a different kernel, which has some features in common with the signature kernel. In particular, we can still identify a closed-form functional equation which it solves. And then the natural question, which we haven't really addressed yet, is whether you can use the structure of this equation in this kernel in some practical setting to improve the results that have been obtained using the ordinary signature kernel. All right. All right, so let me talk about the simple case of the general linear group first of all and discuss its relationship with the signature kernel. So this map M, which induces the development by choosing a basis for V, you can write it in this form. So 1 over N, sum over I, I goes from 1 to D, AI, VI, VI are just the entries of this. Just the entries of this vector v in the chosen basis, and AI are just matrices in your Lie algebra. And we first started thinking about these problems because of a paper that appeared last year, was written by Muka Cirone and Venercier and Salvi. And this paper drew interesting connections between. Drew interesting connections between these sort of randomized limits and the signature kernel. And so this is what they're able to prove. But in the case where the entries of the matrices AI are IID and standard normal, then you can take a pair of development maps and you can look at the Hilbert Schmidt inner product of the pair. Of the pair, suitably normalized and taking the expectation, you recover the signature kernel in the limit. So I want to perhaps discuss the proof of this result and some of its limitations, first of all. The proof that they gave was quite long and also specialized to the case of the Gaussian setting. And so a natural question is. And so a natural question is to what extent is this limiting object robust to changes in these assumptions? And just as a warm-up for the subsequent result, I'll walk through the proof of this in some detail now. So, as I said before, So, as I said before, the objective here is to use the framework of random matrix theory, free probability, to understand where these limits come from and then hope to sort of extend these limits to the case of other matrix leaders. So, we need a bunch of assumptions in order for to obtain these results, but they're much weaker and less confining. They're much weaker and less confining than the assumptions that I showed on the previous slide. So, these are the sorts of standard assumptions really that arise. It might be possible to weaken some of them, but broadly speaking, I think these are quite a common set of assumptions in obtaining these limit theorems involving random matrices. So, we need uniformly bounded moments for the entries of the matrix. For the entries of the matrices, uniform over the dimensions as well. We need some uniform control. We need the operator norms of the matrices to be uniformly sub-Gaussian. So that's captured by that assumption there. This factor of one over n featured in the way I presented the development map earlier. The development map earlier, and that was essentially to ensure that the entries of our matrix can be normalized to have unit variance, centered in unit variance. So we make that assumption as well. We need some independent assumptions. We need that different entries of the matrices are independent, but the same entry of the different matrices are only required. Are only required to have zero correlation. So, of course, in the Gaussian setting, this would imply independence, but you can have this, you can get away with this weaker assumption for more general distributions. All right, okay, so under these assumptions, then one has the same conclusions before that the limit of this expectation gives you back the signature kernel. And in fact, the convergence also happens. Kernel and in fact, the convergence also happens in L2. That was also observed in the Gaussian case. All right, so how does the proof of this go? Will be quite familiar, I think, to anybody who's studied the way in which these arguments normally go through in random matrix theory setting. So the first thing that you do is just re-express the Hilbert-Schmidt inner product of these pairs of developed paths in terms of the power of the power of the path Developed paths in terms of the signature. If you do that, then you get a summation that looks of the that has the following form. So the summands here involve this normalized expected trace, which I'll explain in a moment, contracted against products of terms in the respective signatures of gamma and sigma. This normalized expected trace involves the trace of this matrix Ai star J. Star j, i here denotes a word of length, and j denotes a word of length. Well, I hadn't said that this notation gives you the word of the length of the word j. What this notation AI means then is you just take the matrix product of the A matrices, AI1, AIN. AI star then means. Ai star then means you take the conjugate transpose of this matrix and you multiply it by the matrix Aj. The exchange of limits here does need this uniform sub-Gaussian assumption that I mentioned before. So for convenience, I'll write k to be the sum of the length of the word i and the length of the word j. And this normalized expected trace. Normalize expected trace can be written as a sum of some quantities C W, which I'll write in a moment. And the summation here is over words, closed words in n letters of length k plus 1. And essentially, here you can capture all of the terms that compose the trace. The terms that compose the trace is a product of matrices of the AI matrices, which has this form. Which has this form. And then you can do the usual thing, which is associated to this word, an undirected graph, whose vertices are given by the support of the word. So that's the set of letters in the word. And the edges are given by the pairs wi, wi plus one. Plus one. The relatively simple argument, just using the independence and zero-mean assumption, shows that unless every edge is traversed at least twice, then CW will vanish. You can also discard the number of terms. You can also discard from their making a contribution in the limit the words here. The words here for which the support or the cardinality of the support is less than or equal to k over two. So if the number of distinct letters in the word is less than or equal to k over two, then you essentially get a contribution from these terms, which you don't see in the limit because you can count the number of terms that you see and it's bounded by n to the k over 2. By n to the k over 2. So one's left just having to look at this summation over words for which the number of distinct letters is equal to k over 2 plus 1 and for which cw is non-zero. And an argument shows that this implies that the undirected graph that's associated with this word is a tree. And in fact, it just has a single branch. It goes out and then it comes back down and that's capped. then it comes back down and that's captured by the fact that this expression here so wn plus one minus r is equal to wn plus one plus r you can count the total number of such words and then finally you can see that when you come back down this tree d you can use the um the the The uncorrelated assumption to see that CW must equal delta ij. So the word i must coincide with the word j if you get a non-zero contribution. And if you do, then you get that this is equal to one. If you assemble those arguments and consider the influence of this factor one pre-factor in the summation. Pre-factor in the summation, then you recover exactly delta ij, which is what you need here in order to recover the signature kernel. Okay. So in the last 15 minutes or so, maybe I'll just talk about how this idea can be extended to consider the unitary developments. So we were, in the title of my talk, I have this object called the path characteristic. I have this object called the path characteristic function. This is inspired by a paper of Haoni and her collaborators that was written last year. And in this paper, they propose a metric for probability measures on path space, which is based upon the unitary development. So I'll introduce a few definitions here to explain how this comes about. So suppose mu and mu are two probability measures on this space X. Probability measures on this space X that I introduced before. You first of all fix M. So M, remember, was this linear map that defined the development map. And you can take the expected value of the terminal value of the development when mu is realized according to the probability measure mu. This then allows them to define this notion of distance between mu Between and another measure of where I take the phi mu m map and I take the phi mu m map, I consider the difference and then compute the Hilbert-Schmidt norm. Randomizing the choice of development then, so considering this L2 norm type quantity, gives us the notion of the path characteristic function distance, which they studied. Distance which they studied. And this is motivated by some of the theoretical results in the earlier paper of Chevy Revan Lines that I mentioned earlier. And roughly speaking, this paper shows that if you have two probability measures on path space, then for n large enough, remember n is the dimension of the Lie group, and for a For M, a fully supported measure on the Lie algebra, you can always arrange for the distance between two distinct probability measures to be non-zero, strictly positive. So in other words, you can separate two distinct probability measures in this way, provided that you work in a high enough dimensional regroup. So Li, Lu, and me were interested in using this fact in what they call PCF GAN. So these are somehow generative models, where you use this idea to try to differentiate or act as a discriminator between two probability measures. So, in their case, they don't take a fully supported measure on the Lie algebra, but they take some Take some measure of this form, where the atoms of this measure are somehow learnt adversarially. I won't explain too much about that, but this is somehow the basis of our interest in this object stems from this application. So, there are some problems here if you want to implement this idea. First of all, it's not clear. First of all, it's not clear how large a dimension n, how large a dimensional d group you need to work in in order for the discrimination to work properly. Strictly speaking, one needs a measure here that's fully supported. And so potentially there you have the problem of doing the Monte Carlo simulation over a large dimensional space. And also there can be complications if the dimension of the underlying space V, where the party is the same. V where the paths themselves are living is too high. So, the basic question is: is there a way to compute the path characteristic function in the asymptotic regime as n tends to infinity? And then, what does the resulting object look like? Does it share any of the features that in common with the signature kernel? What can we say about it? Are we able to compute it in some sense? Does it satisfy a P V or something like it? Satisfy a PVE or something like it. Well, in order to make this connection a bit more tangible, this was an observation made in the paper of Houni and her co-authors. You can compute this path characteristic function as an MMD distance associated to a kernel, so a function on pairs of paths. And you can write this thing then as the expectation where m is distributed according to some measure ψ of the Hilbert-Schmidt inner product of the development maps associated to, well, that emerge from gamma and the developments of gamma and sigma. So using properties of unitary matrices, you can rewrite this as the expected value of the trace of the unitary depth. Of the unitary development of gamma composed with sigma run backwards. All right, so there are some issues with actually computing this that I won't mention, but we're now in a setting where we can apply the theory that we developed before and take limits as n tends to infinity. All right, so here these things have been well understood. Well understood if you just dig around a bit in the literature. I think many of the members of the audience will already be aware of the theory of free probability introduced by Wojelescu back in the 80s. And this provides a framework for computing limits of this type and lots else besides. So the basic framework here is that we have a Is that we have a non-commutative C star probability space. So this consists of a pair, which is a C star algebra, a unital C star algebra, and a state, which you can just think of as being a complex-valued linear functional on this space, which takes the unit to one, satisfying some other restrictions as well, but I won't go into these. So for our purposes, and this is well known, two important examples are the space of n by n matrices. Of n by n matrices over the complex field, let's say, where this state is given by the normalized trace. And one can also work with random matrices. One can restrict the L inf necessary adjustments to get rid of the L infinity assumption here, in which the state is provided by the normalized expected trace. Okay, so what's useful here is that Is that so the random vector? So there is a notion of independence free for the over non-commutative over these non-commutative spaces, C star algebras. And I've captured the definition we need here. So if we have a collection of sub-algebras, each of which contains the unit, then we say that this family is freely independent. This family is freely independent if it satisfies the following conditions. If we have elements aj which belong to the subalgebra AKJ, and if we impose the assumption that Kj does not equal Kj plus one, and if each of these elements evaluates to zero on the state, then the state of the product is also zero. Is also zero. So non-commutative random variables are free if the algebras they generate are free. And this is a truly non-commutative notion, as the following example shows. If we take phi x1, x2, x1, x2, and we assume that x1, x2 are freely independent according to this definition, then phi of this product will be zero. And this will not be the case for non-degenerate random variables, of course. Random variables, of course, in the commutative setting. So, a particularly interesting class of these non-commutative random variables are given by the three semicircular random variables. So, these are those for which in the collection the marginals are just the semicircular random variables. Circular random variables with radius two. And the joint law is characterized by these, or an instance of the Schwinger-Dyson equations. And this is the algebraic structure that we need in order to understand what the appropriate limiting object is for these unitary developments that we want to study. So I let psi i denote the non-commutative monomial. Non-commutative monomial of a collection of these semicircular random variables. And the Schwinger-Dyson equations are captured by the following two expressions, constraints. So psi ij is equal to psi ji. So we can reverse the order of the i and the j and we get the same answer. So this So this behaves as trace the trace of matrices behaves. Similarly, we can append a letter J to the word i, and we have this formula which allows us to compute the expression for the moment of this monomial in terms of lower order terms, lower order moments. All right, so. All right, so this key result of Wojcalescu tells us that when we take a collection of now Hermitian random variables, and essentially with the same moment assumptions and independence assumptions as we had before, just making some allowance for the fact that they're now omission. Then, when we view this collection as elements of this non-conference, Non-commutative C-star algebra, then they're asymptotically free, and we can identify what the limit is. And the limit distribution is just given by these D-free semicircular random variables, whose law is characterized uniquely by the Schwinger-Deissen equations that I wrote on the previous slide. Okay, so let me just. Let me just, perhaps I won't show you the proof of this, but let me just describe what comes out of applying this idea result to the structure that we had before. So we have a collection of matrices, and now we're interested in unitary development of just a single path, because remember, I was able to re-express this kernel as the expected. As the expected value of the unitary development of gamma composed with sigma run backwards. So I only need to consider the development of a single path at this stage. The relevant Lie algebra here are the anti-Hermitian matrices, but I've multiplied everything by I, the square root of minus one. Okay, so here I'm considering the development which starts at the identity matrix at time s. At the identity matrix at time s, and then this is just a development equation from before, where I've now realized this linear map m through these matrices A. Then the result that we're able to prove is that for fixed S and T, there is, well, first of all, the limit exists, but more importantly still, this limit can be is the same. Is the solution of a functional equation, a quadratic functional equation, which we haven't seen really before, couldn't find this in the literature. But you see, it's somewhat reminiscent of the functional equation for the signature kernel, which arose as a comparable limit in the case of the general linear group. So I think I'm running a little bit short on time. So let me just quickly explain. Me, just quickly explain where this equation comes from. It's not too difficult if you just focus on the main aspects of the argument. So, the first thing that we do is just expand the trace exactly as we did before. If you do this, then you get a bunch of terms which involve the moment, just using the Wojaleska result and our moment conditions. You get a bunch of terms which are expressed in terms of the moments of the monomials of. Minomials with respect to this non-collection of semicircular free random variables. At this point, you can apply the Schwinger-Dyson equations to re-express this psi function in terms of lower-order moment and using fairly elementary properties of integration and properties of the signature. You can decompose the signature so that it somehow reflects the terms that appear in this summation. And by combining everything, you end up with an expression for k which has this form. And so this is the functional equation that results. What makes this different from the functional equation that describes? A functional equation that describes a signature kernel is, well, first of all, it's quadratic in k. That presents some difficulties when you want to show that it's some equation of this form has a unique solution. You can do it, but you need a bespoke argument in order to do it. And secondly, you can see that the function u, the dependence on this variable u appears both in the second argument here and in the first argument here. Here and in the first argument, here inside this integral. And this presents some difficulties when it comes to computing this object. But nonetheless, one can show that this class of equations has a unique solution. And of course, the solution is the one that was given by the series expansion on the previous slide. All right, I think I'm running a bit over time. I'm running a bit over time. We're really at the beginning of trying to understand how one can possibly use this Sphinger-Dyson kernel, as we've called it, for want of a better name, to reproduce some of the desirable properties that have been observed for the signature kernel. And quite a lot of this is heavily computational in the end, or will have to be. In the end, or will have to be. And if I had a bit more time, I would have shown you a numerical scheme that we've introduced in order to compute this kernel explicitly and to give you some idea how it compares in terms of its computational complexity with the naive approach, which is based on randomization. And it's not really clear that any one of these. And it's not really clear that any one of these approaches is uniformly better than the other at the moment. It very much depends on the type of curves for which you want to compute these objects in the first place and various other parameters. But I think perhaps that's a good point at which to stop. I'll try to give a list of references to so that you can. So, that you can, if you're interested, you can look at this in more detail. Most of this work that I presented today is contained in this paper, which appeared on the archive earlier this year. And some of the earlier references as well contain useful and relevant material. The paper, particularly worth drawing attention to, I think, is this PCF GAN paper. This PCF GAN paper by Han Liu, Sirenli, and Hao Mi. All right, okay. I apologize, I've gone a little bit over my time, but I think I'll stop there and entertain any questions if there are any. Hi, Tom. Can you give us some insight about the key reason that you obtain a limit that is different from the standard signature kernel in the unitary case? And also the flexibility of obtaining different kinds of uniting kernels. Yeah, okay. So, why do you understood the first question to mean why do you get a I understood the first question to mean why do you get a different limit, and I think that's essentially because you're dealing with, well, you have different Lie algebras, they have different, this imposes different constraints. In the case of the general linear group, you essentially take independence across all entries of the matrix, whereas in the case for the unitary group, it's a more constrained setting. It's a more constrained setting, right? Because your Lie algebra consists of anti-hemission matrices. So the very fact of that constraint imposes some restrictions. So it's natural to expect different limit, I think, if you change the lead group over which you're working. I think we don't, I mean, we've only really considered these two examples so far. The fact is that if you want to be able To, if you want to be able to use the limiting objects that result, then in the end, they have to be amenable to some sort of computation. So, if you apply those two constraints, then sort of these are the only two examples at the moment we've been able to identify that say anything useful. And what was your second question? Sorry, I don't think I quite addressed your second question. I quite addressed your second question. Flexibility of obtaining different types of different sorts of okay, yes, flexibility. So, I mean, the signature kernel, I'm taking people's word for it to some extent, but I think one of the drawbacks of using it for the sorts of approaches that Hao Ni and co-authors were interested in. Nee and co-authors were interested in is that you need some restrictions on the measure in order for the expected signature to actually be well defined. So it's somehow useful to operate in a situation where you don't have to make those assumptions. And so in the case of the unitary group development, I think the compactness essentially gives you the boundedness of the kernel. The bound the boundedness of the kernel. So you can work across a broader class of measures without having to pay attention to any of these sort of somewhat annoying considerations about whether the expected signature exists or not. Is that a satisfactory answer? Well, I guess what I meant is that would you expect different kinds of limit, for example, if you Limited if, for example, if in your assumption, you introduce some sort of dependencies. Yeah, well, oh, you mean within a fixed example? Yes, no, that's a good question. I think you would expect different limits, yes. Yeah. So if you, for example, if you work in the case of the unitary group and you introduce And you introduce dependencies between different entries of the development map, then you'll get a different, I think you'll get a different set of Schwinger-Dyson equations in that case, would be my expectation. But as I said before, you actually, if you want to use the resulting objects, then you actually have to be able to compute them in some sort. There has to be a numerical scheme that will allow you to derive these. Derive these expressions and in a way that is well controlled by the, for example, the dependence on the dimension and the complexity and the length of the path. All of these considerations need to be sort of properly understood. Okay. I have a question. I don't think you can hear me, Tom. I really think you can hear me, Tom. I'll come up to the front. Can you hear and see me, Tom? I can hear you, but I can't see you. Because hearing is the more important of the two. So thank you for a really excellent talk. This is probably more of an over-coffee question, but it's a little hard over Zoom. So back on one of your early slides where you introduced the signature, a lot of visual. There's a lot of visual similarity between the objects you're talking about and the rolling map SDE, which defines the Brownian motion on the groups in question. So I'm just wondering in a sort of general way if there's a way to describe the kernels and limit signatures you've developed here in terms of the non-commutative distribution of the general linear Brownian motion and its free probability limit, or the free unit, at least in the Gaussian case. At least in the Gaussian case. I don't know is the honest answer to that. I mean, I think there's a lot of literature in this area, and I'm familiar with only a very small part of it. I think it's definitely worth asking those questions, right? But the honest answer is I don't even have a sense really at this stage. Yeah, I wish I was a little bit more-I only know a tiny bit about what you've worked on. I only know a tiny bit about what you've worked on, and I'm sure we I should have asked you this question when you were in San Diego a month ago. Oh, is that Todd? Hi. Hi, yeah, yeah. Todd. I couldn't quite tell. I said I could hear you, but I could only just hear you. I definitely couldn't see you. So I hi, Todd. Thanks for the question. Let me think about it. I mean, certainly, so you and Bruce have certainly written papers together that I think go in that direction. I think go in that direction. Yeah, there seem to be tantalizing connections, so I'd love to talk about them with you. But maybe we'll let everyone go to lunch and I'll send you an email. Absolutely. Thanks. Are there more questions? And are there any questions from the Zoom?