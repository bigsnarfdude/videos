Okay, so sorry for the small uh technical delay, all back on track. So we have the second talk of this morning uh with Ava Yoldas about an asymptotic preserving scheme for parabolic equations of Lotka Bottera type. Thank you very much and thanks for the organizers for making such a nice event possible. And it's always a pleasure to come back to Granada and I'm sorry about this delay, I just wanted to show you. Sorry about this delay. I just wanted to show you at some point three videos. That's all for about like this 10 minutes. No, it's not listening to my English. Okay, you stay there. Maybe not more so. Let's see how it will work. So, my talk will be about a little bit of different nature. It will be on numerical analysis. But the equation that I will be talking about is parabolic PD and it has diffusion inside. It's a multi-scale problem, so, in many respects, it fits in the I guess the theme of the workshop. So, let's get So let's get it started. So it's just to summarize. So the equation is coming from population dynamics. It describes the evolution of a population which undergoes rare mutations and it goes on selection process. So it is structured with phenotypic traits, which means a visible trait, like a hair colour, eye colour. So this passes on to generations through the genes inherited from the parents. Inherited from the parents. So it's for you will see the equation soon, but it's for asexual population, meaning that there is one parent. And yeah, so a tool that we are going to use, since it's not a scale-a-scale system, is that asymptotic preserving schemes. And particularly, I'll be talking about this cripple that I think it's this cable. So yeah, but anyway, let's see how much I will be able to talk about this. So this is the equation. So we have a population density and epsilon Tx. Okay, well that. Okay, well, we'll change this. Okay, let's change this. Okay. If it doesn't work with this, then we have to go to the next one. We're going to try this. If it doesn't work, then we stop and we'll let you know. Okay, let's try this as our last chance with this thing. We'll see. So, as I was saying, the equation describes the The equation describes the time evolution of the population density. It is structured with phenotypic traits, and here this Laplace term describes the rare mutations happening in the population. And the epsilon is making these mutations happen in a longer time scale than the evolution of the population. So here we have also a reaction term, which is like a reproduction or growth term. It can be represented as. It can be represented as so it depending on the phenotypic trait, this reaction term can be positive or negative. So, meaning that some, depending on some traits, like the some traits will be will have more advantage in the population. And it also depends on I, which measures the total population density in a weighted way with phi. So, it is somehow the measure of predation, or some traits have more weight. Some traits have more weight in the population. So that makes the one of the non-linearity comes from here, also the non-locality. So I have the initial data, which is positive, and it's L1. It's like a nice smooth initial data. And yeah, I just put it here some true time scales. It's a multi-scale problem. There are two time scales where one of them, the population, evolves, and the other one is that. Other one is that mutations happen rarely in a long time. So there are two different time scales. That's why also we will need to have, we will need to use asymptotic preserving schemes for this equation. So I wrote some names here. These are the people, some of them wrote these biological papers at the end of the 90s deriving these types of models or like modeling the adaptive dynamics of Darwinian evolution of a population. Evolution of a population. So, one remark is that this Laplace term here, modeling the mutations, is not very realistic. There are more realistic kernels, like integral kernels, including also the really inheritance from parents, but they are more difficult to handle. So, I will remark on that later on again, maybe. Yeah. Okay. Okay. So what we are interested in is the asymptotic behavior of this model. What happens if in long time? So what you expect biologically is that like more fittest traits, traits which are adapted to environment better, they become visible. And mathematically, it means that the population density, commercial Density converges to direct measure in some locations or some of some of direct measures. It is something like that. So an epsilon x converges to something which is some of the applications. So here are some grains. Some gates and this only it is determined in this way where I d is the limit of epsilon limit equation. So, what I'm trying to say is that this, so you start. Say, is that this? So, you start with smooth solutions, but as time goes to infinity, your solutions get degenerate, like they go to direct distribution. So, that's like these direct points, they can nicely be approximated by Gaussians with variance 2 epsilon. So, coming from the same idea, how you determine these points is that you consider this type of logarithmic transform called Hop-Call transform. So, when you So, when you do this transformation, you get another equation in U where actually these concentration points, if I try to plot this, one time it could be something like this. If I this is maybe long, and if I look at this equation that I will comment a bit about soon, there you will be. There you will be exactly this minimum points. Okay, here it should be zero and this should be high. So these are concentration points which corresponds to the solution of this second equation. U where yeah, it will be it will be it will attain its minimum. So it will force these solutions to be positive. So, it will force these solutions to be positive. So, here we have what we have is that we have the Laplace term, here gradient square. So, it looks and also still the reaction term, and I is computed in this same way, it's just the integral. And here, it resembles to like actually this gradient square term is Hamiltonian, it resembles to Hamilton Jacobi equations, which I'll be commenting on. So, what means to What means to look at this problem in large time scales, small mutations regime, is actually corresponds to looking at this equation, behavior of this equation when epsilon goes to zero. So it translates into this problem. So from now on, I'll be considering this equivalent problem, not the initial one that I introduced. Yeah. So we need to make some assumptions. Oh, sorry. Yeah. Yeah, first one is only initial data. So we need to have like the first equation I showed you, it was nice advanced solutions. But in order to avoid the initial layer formation in the limit as epsilon goes to zero, I need to make further assumptions on the initial data regarding the second equation that I showed. So, most important one of them is that it should be sufficiently increasing at infinity, and it is. Increasing at infinity and the slipshies. So the minimum of the initial data should be zero as epsilon going to zero. This is the third one. And also more than being liquids, I need these type of bonds in the initial data. So the second one actually is for like a technical one, which is maybe not needed in the analytical setting, but in the numerical setting it is needed. And so this one is the So this one is the lower bound is just to ensure that it increases sufficiently at infinity. So these are the assumptions regarding the initial data. Then what I'm going to ah here in person. So for the psi, which was the, remember it was debate in this integral term, total population. So it should be a smooth function bounded, measure of predation, and important assumptions on R. Important assumptions on R. So it should be decreasing with respect to I, it's the second variable. So remember, it was it depends on right. So this means as the competition increases, the growth rate will decrease. So you can interpret that in that way in terms of modeling. So for this as well, we need some explicit assumptions how it decreases our How it decreases, or what are the bounds, quantifying these bounds. So, these are not very maybe important, but they will appear later in the theorem. So, I put them, they don't look very nice, but the important thing is that it is monotonic and decreasing in I with respect to second variable, this reaction term. So, under these assumptions, first I'll show you the analytical results. I'll show you the analytical results by Bar Patam and Vira Yimi in 2009. So they show that if we have uniformly continuous sequence of initial data, which is locally uniformly converging to some V0, V0, then up to extracting sub-sequence, we will have locally uniformly convergence to nice smooth solutions of same. Of and same in I, which will be also, in this case, it is in the boundaries variation class, which I will comment later. And then these limits, V and J, are the least positive solutions of this Hamilton-Jacobi equation. So here we have this quadratic Hamiltonian and some reaction term. Here you see the condition on J comes as a Lagrange multiple. comes as a Lagrange multiplier related to the constraint. So the constraint forces solution to remain positive at all times. And yeah, so this is the limit of the initial data. But yeah, so just to remark, this result is using compactness. As you see, this up to subsequences. So there is no uniqueness. It only shows the existence of solutions for the Solutions for the viscosity solutions of this constraint Hamilton-Jacobi equation. So, yeah, I just remarked that the Hamiltonian is the quadratic given in this way. So, it's monotone, this convex. And yeah, so the reason, like this important thing, is that this J may have jumps, jump discontinuities at this uh location, so the traits uh Pittus traits may change in that way. In that way, so this J having jumps, like having only this bounded variation class, restricts us not to have the uniqueness. So, there were a couple of results, uniqueness results. They were assuming some special forms of this R reaction term, like it is separable, it's convex, or some are assumptions. But the most recent one is by Vancouver Javes and the collaborator Lam. So they So, they provided the uniqueness results for these types of constraints for Mithron-Jacobi equations, including the case that I'm talking about today. So they have the minimal assumptions compared to the previous references that I did not list, but I should have probably. So, what does it say? So, suppose that it has two parts: this theorem. First part is that if we First part is that if we have two different solutions, V1, J1, V2, J2, with the same initial data, these solutions are again ellipses, locally ellipses, and in J they are locally bounded variation, then they must be equal almost everywhere. And the second one is that if for a given J, the variation solution of this constraint familiar. Variation solution of this constraint Hamilton-Jacobi equations is the unique local ellipsoid solution over this time interval and independent of the choice of J. So just to comment maybe about the proof of this theorem is that they use some sort of decouple the solution of the constraints of miton Jacob equation from its Lagrange multiplier. So, how do they do that? First, they How do they do that? First, they consider j as a source term in the military-Jacobi equation, and independently from this fact, they prove these solutions, minimum solution, will be zero. So the solutions will remain always non-negative. So combining these two approaches, they will get the uniqueness. So this involves a little bit, but I just am not able to comment more than that for now. So it's really nice results. It's really nice results. So, actually, this also because of this, we will be able to complete the numerical results on this equation. So, now I'm going to talk a bit about numerics, but first, maybe some slide about the synthetic preserving schemes. Many of you know what they are, but maybe just to comment. So, here, this picture is summarized as follows. So, P epsilon is my Is my equation on U, which is logarithmic transform, which I didn't write. So here, P0 is the constraint, Hamilton-Jacobi equation, the limiting equation. And S epsilon H is my numerical scheme that I propose for P epsilon for my mass scale problem. So H denotes the discretization parameter. The discretization parameters, so it's I just put them delta t, delta x over there, so I didn't write them separately. So, what I would like to do is that I want to show this convergence, right? And just if I look focus on this part of the picture, what it means, the error I will get will be of order of my scheme, which is, let's say, the injection. Which is the, let's say, in general, the order of the scheme is R, like not in my case, but in general terms, let's say. And it will be H to the power R or epsilon to the power S. So this will create a problem when I take these steeritization parameters very, very small compared to my scaling. So then I will get numerical instability. So I forgot to mention these asymptotic preserving schemes. They are introduced for modest scale. For modest-scale kinetic problems to discretize them efficiently or to solve them numerically, treat them efficiently, also for some hyperbolic problems at the end of 90s again, beginning of 20s, 2000s, sorry. And yeah, so this will lead a problem, some instability issues. And what to do is that the idea is that you fix the discretization problem. That you fix the discretization parameters and you look at your limiting scheme. So, what happens to your scheme as the scaling parameter goes to zero? So, what you obtain is S0H, a limiting scheme. And if this S0H is a good approximation for your limiting problem, limiting PVE, then it means that your original scheme is asymptotic preserving. Asymptotic preserving. So, how do I see this? So, if I have this direction, so if I have this nice limiting scheme, this means I will have an order as epsilon h, this zero h, something order epsilon, right, for fixed discretization parameters. And from this level, I will have. Um, I will have my error of my Linux scheme, my constraint Linux, the equation of order HR, R is the my order of my scheme, order of my problem, let's say. So not in a general case. So, what I how can I use this? So, for this same error, I can use this triangled inequality and I write. And I write the things that I know here coming from scheme being asymptotic preserving. So, meaning that these arrows are whole. So, this means that for fixed disparitization parameters, as epsilon goes to zero, this scheme converges to the meeting scheme. Here, this arrow means that for fixed epsilon, for epsilon zero, fixed epsilon, there is no epsilon anymore. Here, as this critization parameters go to zero, my scheme. This goes to zero, my scheme approximates the solution of the constraint Hamiltonian Covey equation. So this means that I will have a second error for the same term. So if I try to sketch it epsilon over my discretization parameters, my first error is something like this, and my second error is something like this. Then, if my scheme is like, of course, asymptotic preserving, what I can do is, Osymtotic preserving, what I can do is that I can take the minimum always and I will have some sort of limit here for the error bound. So if I'm lucky enough, I can even have this error independent of my scaling parameter. So I can choose my disparitization parameter independently of my scaling parameter. So that's the nice thing about asymptotic preserving schemes. And also, it applies to our case. I don't think there are many examples using this in a parabolic setting. Using this in a parabolic setting, a couple of more papers, I guess. So, this is what I'm trying to do in the remaining part. Yeah, so let's look at the scheme that I propose for my problem. So, here I have a finite difference scheme in time, and I discriticize Laplacian. So, what's important is here to discriticize the Hamiltonian, numerical Hamiltonian, correctly. So, it should be increasing in this first variable, decreasing in this. First variable decreasing in the second variable when it's first variable is negative or second variable is positive, it's zero. So somehow this makes me forced to consider this Hamiltonian as so if I'm in the positive side, I have something like this, something like this, and if I'm in the negative side, I have something like this, and I take the maximum. I take the maximum, so this means my numerical Hamiltonian is always convex. It is monotone. And so this I enters the system implicitly. So this is the stiff term and it is computed with some quadrature rule here. And that is very important. And that's the yeah, that makes us get. Yeah, that makes us getting these stability conditions later subject to CFL conditions that I will mention later. So, this is actually the construction looks very easy. It's a finitely different scheme, like with implicit terms. Implicitness is coming from the stiff term I. Okay, so then the first result. So, I'm going to talk about three results concerning value. Results concerning when epsilon fixed as epsilon bolts to zero, and then epsilon is equal to zero. So first one is when epsilon is fixed. So under these assumptions that I mentioned, I guess in the third slide, that we fix epsilon and under this red Seattle condition, we have the following. We have the following errors for the scheme, so convergence of the scheme to the problem. So you see, we have some order and we have a constant depending on epsilon. As epsilon goes to zero, there is no hope of getting uniform bounds in that con constant, but still later the numerical tests I'm going to show you that they suggest that this scheme is uniformly accurate. Uniformly accurate. So, this CFL condition ensures actually in the proof that I have certain monotonic property of the scheme, which gives some nice inequalities that will lead to stability. So, this is the first result. And how to maybe do the proof as an idea. So, what we do is to we What we do is to reformulate the scheme in the following way. So M denotes this part. And why I separated M like this is that I can show that under the CFL condition that I mentioned, M is monotonic. So this is very important. And this monotonicity gives us to preserve some sort of Lipschitz bounds, Lipschitz inequalities. And most importantly, so in the second equation that on That on U epsilon, we are dealing with unbounded solutions. So, even the solutions are not bounded, I can have this type of inequality, like the difference between this term will be bounded by the difference between the solution. So, as I said, monotonous is very important. It gives the stability estimates, and after the stability estimates, the rest is the proof is taken. The proof is technical, but nothing very elaborate. It's like how you prove an implicit Euler scheme for ODE, that kind of thing. So, how do we implement is that you solve this I enters the problem in an implicit way. So here, here. So, what we do is we solve it with Newton's method. So, we plug U here in the Solution. We write the solution here, u coming from here, and we try to solve a fixed point. We try to find the fixed point of this map. So Newton's method we used here. And that's all about it, but I'm not going to talk more on this. So the second result as epsilon going to zero, again, we have under this CPA. We have under the CFL condition, up to subsequences, we have the convergence of the scheme, the scheme problem, to the scheme limiting scheme. So again, as you can see, it's along sub-sequences, so this uses compactness. There is no rate, it's just the convergence of the limiting scheme which we obtain in this way. So this is all about that. Is all about that. Maybe. Do I need to comment more? You know, this again, what we use more: the monotonicity properties of the skin coming from again from the CFL condition. So maybe the videos that all this caused all the delay. If they don't work, it will be very ironic. Yeah, so here, the solution. If I start this dashed line initial data, as you see, like Data, as you see, like you missed probably, the minimum point was somewhere here and it keeps changing. And so I have a plot maybe. The next one will be better. Here, I plotted the same solution in different time steps. So here you see the minimum is here and it's slowly changing. It already time 0.3, it's it's It moved. So this also in I, if it is interpreted in I, it will have jumps. So there are some more one more with a different initial data. So here if I have, you see some, this is dashed line, it's always the initial data and it always gets smoother faster. Yeah. Okay. Okay. I think. Yeah, I can. So here, as epsilon goes to zero, again, some plot already I showed, it's related to the first initial data simulation that I showed. So here, you start with this type of initial data, and as epsilon gets smaller and smaller, you converge something like this, which is a good candidate for a solution to the human. Candidate for a solution to the Hamilton-Jacobi equation. Okay, so this is for I. You see again, as epsilon goes very small, it reaches to the actual one one jump I. So it has jumps in in the points where the fittest trait is the location changes. The location changes. Okay, so this is about the order and in L infinity norm for U. It seems like the slope is one. And for the same thing holds for L1 norm for I or J, not L infinity. So L infinity norm has some sort of plottle here, which is the point exactly. Which is the point exactly where the scaling parameter delta t is smaller than epsilon. So there is a problem here. So this error is, this convergence rate is in alpha norm for i, so which is also expected. It has jumped. It's founded variation function in the class of boundariation. So yeah, so the last result is the convergence of the limiting. The limiting scheme to the constraint Hamilton-Jacobi equation. So, again, we have two CFL conditions here. Under these conditions, we have the convergence. And normally, actually, in the genetic setting, we have the other two ways that I described, other two theorems. These are more involving parts, but in our case, these were easier due to monotonicity. Easier due to monotonicity properties we have, but this is the one which involves most of the new ideas and computations. So, this is the interesting part, which I'm not going to talk about. But yeah, it mainly uses the ideas coming from the uniqueness paper that I presented by and some more elaborate stuff, but it's nice. So, in this, yeah, this is also again. This is also again the compactness due to some compactness, but we don't have a result. We don't have a rate. So there are some thoughts, but which one I should show. So this is the behavior of the limiting scheme in different times. And yeah, so maybe I will just come to the last one. So what's The last one. So, what's interesting is that even though we cannot prove the uniform accuracy of our scheme, I showed you the first theorem, the constant depends on epsilon, still with the numerical tests we can see that it seems to be uniformly accurate in L infinity norm for U and similarly for L1 norm for I and not true for actually L infinity norm for I or even total variation semi-norm for I. So for I it depends very It for I, it depends very much on the norm you consider. And finally, it, yeah, yeah, maybe I will not show this, but this works for all the proofs that I showed you are for 1D, but they can be generalized for 2D or higher dimensions. It's just that you need to pay attention to indices. So this last simulation was for 2D simulation. And yeah, this is the concluding remark. And thanks. It's just same thing in fooding. Here you see the minimum is in the Z point and it will move to the middle. So it's it's the you look at it from above. So right, the this is the same initial data I took, but it's in the Initial data I took, but it's in the 3D version of it. That's all. It's nothing fancier than this, yeah. Okay, so it was a bit fast. Yeah, one of the things that you mentioned that you know is that people use key access to those things. I mean, I know that your method. I mean, I know that your method of looking for convergence techniques kind of, but I guess you have to check how much time he goes in the beginning. Ah, okay, that's a good thing. That's actually opening even in the continuous setting. What happens? Yeah, yeah, yeah. Yeah, I don't think we tried. Maybe I maybe I, yeah, I guess not. Yeah, yeah, I guess no. I don't think we try. Yeah. And for your difficulties you have in your convergence here and in the end, is there any hope that you could get the right by going to some negative known? Like negative sobre for Basserstein or something. Yeah, I don't know because what we use is like the normal, like the viscosity procedure in discrete setting. So it's normally valid for smooth solutions, right? But even we have this jump and stuff. So even that doesn't work in a standard way. So we construct sub-solutions and super solutions. Normally these are somehow symmetric, right? But in this case, they are not the same function. That is very that is the technical part, I guess. And we somehow set up an argument to sh show that they're almost everywhere the same, but yeah, we need to use this decoupling argument of one sum, second theorem that I mentioned. The ideas coming from there, but I don't know. So you mean you rely on some uniqueness of the lineage? Yes, exactly. We rely on There's no more questions. Let's have a break and since we're a bit late, let's assume I can ask.