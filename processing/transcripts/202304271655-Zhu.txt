Okay, so Richard Juke now, also from CNU, is going to tell us about quantifying systematic uncertainty unfolding for bundles using optimal transport. Hi, thank you. Thank you. Thank you for being on to this meeting. Also, thank you, Mike, for the wonderful introduction for the unfolding problem. So yeah, as he mentioned, my talk is basically concerning another source of systematic uncertainty. Systematic uncertainty in unfolding, which is about the forward model. And we will propose to use optimal transport to address this systematic uncertainty. This is a joint work with Mikael, Larry, and also Andrea from CERN. Okay, so Mike has already mentioned this, but just to give a brief recap for unfolding what we're doing. For unfolding, what we want to do is we are interested in the distribution of some particle of parameter of interest, but what we have is the smear distribution coming out of the detector, and we want to unfold the true distribution. And again, I store the figure from Michael's. So here is another recap, but the It's another recap, but it's important to give another review here. So, how we model the unfolding problem mathematically is given by this integral operator, which relates our smear distribution with the true distribution. And this response kernel K is actually representing a response coming out of the detector. Coming out of the detector, and it's basically a conditional density of the smear observation given the true collision event. So our work is basically to quantify this systematic uncertainty in this response kernel K. In the previous talk by Mike, this response kernel K is assumed to be known. There's no uncertainty regarding that. Uncertainty regarding that. And in this talk, we basically relax this assumption and we try to quantify this systematic uncertainty. So where is this systematic uncertainty coming from? It's because we usually have an imperfect knowledge of the detector alignment and calibration as well as the distribution of certain variables can affect the response kernels in different ways. In different ways. So, if we have two different detector simulations, we might end up getting two response kernels. And how we quantify the systematic is the topic of this talk. So, our method addressing this systematic is by optimal transport. A few days ago, Philip and Tudor has given us a great A great introduction and summary of the optimal transport. And here's another example on how we can use optimal transport in high-energy physics analysis. So just to give a quick introduction, suppose we have two kernels, K1 and K2, and remember that these two kernels are basically the conditional density. So this means we can compute voice resistance distance between these two kernels as follows. These two kernels as follows. And here, F1 inverse and F2 inverse are the point of function of K1, K2 condition on a fixed true value T. So what is this useful? The reason why we want to introduce the open transport and Watson distance is because it has several nice properties, and one of these is the And one of these is the Watson barrier center. So, given these two kernels K1 and K2, we can define a West-Stone barrier center with weight vector T as the following weighted out mean of the weighted mean, weighted sum of these two Watson distance. And specifically, in one-dimensional, the quantile function of this. function of this Watson's embarrassment kt satisfy this weighted mean of the Pontile function of K1 and K2 separately. And then if you vary in the weight t, then this will define a geodesic that morphing between the K1 and Kt. So if you look at this figure here, we have the K1. Here, we have the K1, which is the red curve on the left, and K2, red curve on the right. And any green curve between is a point on the geodesic that morphs between K1 and K2. And this is a very nice, there's a very nice property about open transport: that when we do the morphing, it will preserve its geometry and shape of the distribution. So if we know that K1 and K2 are Know that K1 and K2 are our two possible kernels we have, then these green kernels in the between might also be a possible candidate of kernels that we might get. So this might be well known to the audience here, but as I mentioned, the open transport, when we do that, we do it on the on-bing space. Space, but in actual analysis, we need to do the binning. So, here is basically how we do the bin, and this is exactly the same as Mike introduced before. So, we have the bin on the true space and the meaning on the experience space. And we have the particle over and detector level histogram, which follows a Poisson point process. And Point process and we steer a Poisson mean separately. And we have a linear system that connects the Poisson mean on the two space with this response matrix K that we can calculate from our response kernel K. And by this distribution setup, our goal in the end is to do inference on the Turkey scramming lambda. So now I'm going to basically introduce basic machinery on how we do the how we can compute the confidence interval for the true history mean lambda while accounting for the systematic uncertainty in the response kernels. So here we still start with two kernels, K1 and K2. Then we can compute their geodesic. We can compute their geodesic. After that, we aggregate this into the response matrix K1, K2, and KT. After that, we unfold these kernels and our data with the one identifying strict bound intervals, which might describe. Describe. And in the end, we obtain a collection of confidence intervals for the lambda. So to illustrate the idea, we introduce a simple simulation study. So here we simulate the inclusive jet transverse momentum spectrum. We specifically simulate the particle level data using this following intensity function. We choose the number. We choose the number of beans in the detector to be 40 and the number of fine beans in the particle level as 40 as well. But in the end, the number of white beans in the particle level is 10. So basically, we will do some aggregation here by aggregating these 40 beans to the 10 beans. So here's the intensity function, and here's the true histogram. And here is the true historical mean lambda. And this is the parameter of interest basically we want to do the inference on. And we model the detector response using a crisp function. Again, we need two base kernels to start with. Here we basically choose two kernels that have different means. That have different means and also a slightly different virus. And you can see this is the two basic kernels that we are considering right now. And these two basic kernels is what we might actually obtain from the detector simulation. And as we discussed, then we can compute the geo-what's the geodesic between these two kernels? Any yellow line between here is Yellow line here is one point on the geodesic. And now, suppose for now, if we know what is the correct kernel, then this is the blue line here is what we assume that now is the actual unknown detector response that generates the smear observation. In the real analysis, we won't know this one, but for the sake of study, we For the sake of study, we first assume this. And next, we can basically do the unfolding using the OSB intervals. And the plot here I'm showing is the confidence interval for one of the bins, one of the 10 bins in the particle level. As you can see, each bin here has exactly the same height. So these are all one bin. Same height. So these are all one things. What changes here is our confidence interval, which, if you remember, we have a collection of confidence intervals that is indexed by the point on the geodesic of the kernels. So if we have, say, pick ten points on the geodesic, then we will have ten confidence intervals in the end. And this is how this confidence interval change. How does this confidence interval change across the geodesic? And in total, we have 10 bins, so this is what we get for all of the bins. Interesting here is the train of this solution is quite linear, but Quite linear, but it's not clear why it is because it's the actual like the there's no linear functional between the kernels and our unfolding solution. But here's quite interesting phenomenon happening here. And here's another aspect of how we can view our solution. So instead of So, instead of looking at one bin at one time, you look at a pair of beans at one time. So, here, each plot, this plot, we have having pin 4 and pin 9. And each box is here is the two-dimensional confidence stats that is resulting by our unfolding. And the red one and And the red one and the green one is unfolded using the base kernels K1, K2, and the yellow one between is unfolded using the geodesic of the kernels. And we define the confidence lab to be the collection of these two-dimensional confidence labs unfolded by the geodesic of the kernels defined by K1 and K2. And this black dot lambda is the edge. This black dot lambda is the actual true historian mean. And as you can see in this case, confidence step has coverage for our lambda. The plot on the right side is another way of view the systematic uncertainty. It's by basically Honsinity is by basically interpolating the unfolding solution. So here is basically we take the black yellow box and then we interpolate to this green box. And as you can see here is that the confidence that we get from the geodesics is actually different from what we get from the interpolation of these two boxes. And again, I just show a pair of beans, but in this study we have 10 beans, so in the end we have a 10 by 10 by 10 matrix of plots, basically. And as you can see, some of the cases we can see some nonlinear patterns present. Presents in the talk. We did a quick cover study for this confidence lab and it has proper coverage, which is not surprising. Based on our construction, it should follow its step. And also Yeah, I think that's it. And so far I make two assumptions here is that we only have two base kernel and when we define the geodesic, we need to constrain the weak parameter t1, t2 to be greater than zero. And actually, we can define a notion of Actually, we can define a notion of extrapolation by allowing this weight to be negative. Then, what we get is basically having an extrapolation of kernels indicated by this side line here. So, if you are given this to your base kernel, and if you think some form of, and like if you take the difference of these kernels as like a sigma, and if you think As like a sigma, and if you think there's, you need to account for more systematic outside of these two kernels, then basically we can do this notion of extrapolation. And we can follow the same process before, except we need to also consider these kernels as well. And another assumption we made is basically we have two base kernels, but this process can be generalized to three. To three kernels or more kernels in general, by basically considering the Barsonstein barycenter of these M kernels followed naturally by this definition. And in the winding case, the quantal function is basically like a weighted average of the individual quantile function. And then if you vary this weighted vector t, Vary this vector t, it will define a Western stand hole of kernels defined by our base kernels. And in this case, we have three base kernels. And here we basically compute the wiseness and hold of these three kernels while also considering some some kind of Some kind of extrapolation. One note about the extrapolation is that this extravagation does not always work. It has some limits where if you go bypass that limit, then exflation will fail. But the detail about that, I'm happy to answer in the discussion. So this is a summary and some open problems of my talk. So we address the unfolding problem. Specifically, we study the systematic uncertainty in the forward model, specifically the response kernel K. And we propose to use optimal transport to quantify the uncertainty in the response kernel. The response kernel. And the result is that we can basically have a confidence lab with proper coverage of the true histogramming lambda when the correct kernel is on or at least close to the geodesic of the base kernels. And here are some open problems. So through a So for a given kernel KT on a geodesic, we can view this weight parameter t as a nuisance parameter. And so far we basically have a collection of interval that is indexed by this weight t. But um is there or how can we summarize this uh collection of confidence intervals into a single confidence interval? Single confidence interval. For example, can we do some form of profile like people? Or can we actually learn this T from the data? So we can have a better sense of where our correct kernel is on the geodesic. And in the end, so far I only talk about the simulation study, but we haven't really worked on any real data. Work on any real data, and it will be interesting to see how well it does work on the real high-energy physics analysis. And this is the end of my talk, and thank you for your participation. All right, thank you, Patrick. Can we have now, just to make it all fair, take a few questions? I'll take a few questions specifically for Richard before we open up the internal discussion. So, Igor. My confidence slabs and not confidence ellipsoids? Yeah, I think this is just some name we make. I think conference apps always a good name. Right now, they don't have a shape behind it, actually. I mean, right now, we have basically produced in Dinovis and Tuolos. So, really. They are not in this method. There's no point estimator, and there's no kind of a Gaussian distribution for a point estimator. So it's actually kind of similar to what we had in this Monte Carlo talk of this boxes, like using boxes as your confidence sets. That's kind of what this metal is right now. So there's no heavy episodes in there right now. When you're producing open source, point estimates you are producing point estimate because when you do this inversion as you said without regularization so that's a point estimate yeah so that's the part that might even have time to talk about so so in the actual method so so if you are in a case where you have a full rank matrix with no with no constraints then you have a point estimated you can invert and so forth but everything here we are doing in a case where we assume that that matrix is not the invertible we is the we are doing it the We are doing this in a case where we have more true pins than smear pins, in which case there doesn't exist a list pressure estimator, and then moreover, we also put the constraints in there. So the way it is actually done, it's done without the point estimator. That's the stuff that we regard after kind of at least half an hour talk to explain what is really done there. But the construction does anywhere. Thanks, Olaf. Yeah, I think I just have to say what we already discussed, but I think that is nice. But I think that is I think that this concept of it's like you know it's like a you assume like that it's a Bayesian analysis like a flat prior for the camera between so this between some minimum and a maximum resolution, right? I mean you assume I mean normally okay let me say it more directly you would assume the Gaussian uncertainty of the resolution kernel for instance you assume some You assume some of the resolution to be between some some some very smaller value better resolution than just wider resolution, right? Two specific K1K. I thought this is like a it is like a length error or antoose error error. It's like a seen as a flat uh flat PDF for the he's just saying he has you answered. I presume what you're what you have is are I presume what you have are two response kernels that could represent, say, the nominal one and one that's shifted, or perhaps one that's good resolution and bad resolution. But you talked also about the way of extending that to this kernel, like one of them is nominal one and the other one is the alternative one, then you can basically construct a whole candidate of these kernels between them. Yeah, I'm just saying normally I think we uh let's say for to prepare LHC analytics uh we have like let's say for example for chat PT resolution we have something let's say uh 10% longer than resolution and then we assume this has a relative uncertainty, this has an uncertainty of relative uncertainty of 20%. So the resolution uh varies uh from from 16 to 24 percent absolute, but this we would assume this uncertainty or the resolution to be Gaussian. This uncertainty or the resolution to be Gaussian. I think that's the probability thing we're doing with most of all this problem. And why do we assume something which is like flat PDF? Just to be clear, the problem we are addressing here is the two-point system I get unfolding. So I think of the two kernels as like PTI and Hurry. And then the question is: what do you do in that case? And this is the electronic proportion of what you do that. In that instance, I prefer that certainly is one of those that you can treat it kind of on a traditional way. But really, the point we are addressing here is that you can't do quite a bit of money. Yeah, but I agree, but I think it's like if you use PCR, how do you not change thermodynamic resolution? I don't think so. They're talking about the detector resolution. The colour is dominated by the detector resolution. Dominated by the particular information. So that's the capitalist. So I have a different. I don't think Mikael men had litter at all. I think maybe my question is no longer relevant, but you're just talking about two points of schematic. So this is not, for instance, if I think there's some detector effect that I'm sh like easily shift around. That's not what this is. No, we have start with two point systematic. But it is detector resolution. It is detector resolution. Yeah, I'm sorry. I'm getting very confused now. I mean, like the uncertainty on the detector resolution? Or we're talking about two points on the uncertain resolution. Right now you actually get sets of confidence intervals, and that set is, well, it's infinite dimensional because you get one for every possible value of t. Possible value of t. Yes. And you're saying you still have yet to define a procedure for determining what the ultimately rephrased. That is, if the user is providing base kernels, K1 and K2, and it's not a clear-cut case where one and two are two different discrete things. It's just I have what Olaf says we have, and now should I give you a 68%? A 68% square, a 95% square, 100% square. What endpoints should I use for K1 and K2 if what I really want to represent is a Gaussian 20% uncertainty? Yeah, I think that's a good point. And this is also something I'm thinking about. So, say, if we have these two kernels, and if you look at these two endpoints, these are Two endpoints, these are actually the confidence intervals unfolded by these two base kernels. Then, how do we say, like, say, for example, considering the 68% confidence interval, then maybe one way is basically to look at the range in between that consists of 68% of the kernels. And this serves as the up-polly component. Can I propose an announcement? Yes. An answer? Yes. So basically, you ask a theorist, right? For example, if you unfold the P T spectrum of a jet or something, you ask for some kind of a lower bound curve or an upper bound curve. If there is no series nearby, you take a function which gives you a delta function at the left part of the bin and a delta function at the right part of the bin. Okay. That's really, really concerned. Okay. That's really, really conservative. I since we're going into a broader discussion here. So I'm going to press the button now. And so, Richard, thank you very much.