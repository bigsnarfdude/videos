To present this work in this venue to this audience. It's a different audience for me, and I hope that you'll bring some fresh perspectives on something I'll highlight that are needed. So I'll explain in a couple of slides what I mean by localization of eigenvectors. This is joint work with my student, PhD student Robin Reed. I'm going to use a mechanical pointer. So I'm looking at kind of a. What's that? Portland, Oregon. Yeah. Portland, Oregon. Yeah, in fact I probably had the shortest trip here. The flight was about the same duration as the shuttle ride. Two hours, roughly. So I'm considering an eigenvalue problem for a second-order linear elliptic operator. And you can assume these are L infinity coefficients. In fact, in practice, they're piecewise smooth. In 2D, even the leading one, the diffusion term is what I would call that, could be elementary. Would call that. Could be L infinity in 3B, probably Lipschitz, something that ensures unique continuation and that eigenvectors cannot vanish identically on a non-trivial open set. But having said that, the coefficients or domain geometry or boundary conditions can cause some eigenvectors to almost vanish outside of a pretty small set. And this is what I mean by localization. Really be concentrated spatially. Be concentrated spatially in some small portion of the subdomain. For most of the talk, I actually will be considering self-adjoint operators, but there'll be a point where non-self-adjointness enters in in an essential way. So I'm not really cheating to be here. I have something that I'm calling morally self-adjoint. It's when you have an appropriate relationship between what I'll call the convection term and diffusion one that you can do a simple conjugation and come up with a self-adjoint operator. Just one example I'll show like that where some. Just one example I'll show like that where something interesting pops up. There are other things. What I really want, at least for now, is that the eigenvalues are real. This is what I want. So I'll jump right into a simple example of what we've seen before, a Schrodinger type operator, Laplace plus potential, negative Laplace plus potential. And the potential here is piecewise constant. I took the unit interval and split it into four chunks, even sub-intervals. And on the first one, the potential, first and third, the potential is zero. The potential, first and third, the potential is zero and then 80 squared and 400 squared, or the other two, the non-zero bits. And it might not be shocking that some eigenvector could localize like this, be largely concentrated in the third sub-interval where the potential is zero. You might have to travel higher up in the spectrum to get some non-trivial behavior where the potential is large. But what you might not have expected is that all of the first 12 here, 12 here. I went ahead and computed the first 130. What are you looking for at infinity? It's on the real line. No, no, it's not on the real line. This is a bounded domain, Dirichlet boundary conditions. I'm just looking for something where the eigenvector really is nearly zero outside of a small subdomain. Potential is a small function. Potential is a step function. It's zero from zero to it's zero here, it's zero here, it's eighty squared here, and four hundred squared there. Here and a 400 square there. That's the potential. So among the first 12, we'll get localization kind of dancing back and forth between the first and third subinterval. And then right when your eigenvalues jump past the first potential barrier, you get some other kind of behavior. A different sort of behavior kicks in. In this community, there actually is a community that's kind of built around this over the past decade or so, they call both of these. So they call both of these ground states. And you might say, it's abuse of language a bit, but you can understand why you might call that a ground state, even though it changes sign. It's like a ground state. So I was curious. You see a sharp transition when the eigenvalue breaks this first barrier. What happens when it breaks the next one? So I explore around there. And here is the last eigenvalue, an eigenvector that's smaller than 400 squared. And here is the very next one. Squared, and here is the very next one. We get localization where no one would have expected. I would not have anticipated this in advance. A different sort of localization, small amplitude, high oscillation here as opposed to exponential decay, but localization nonetheless. And then it, you know, gets a little bit worse, a little bit worse, disappears, sort of reappears. It's a murkier picture out here. So, what I was interested in is developing numerical methods that would. That would basically not even observe anything like this and just observe the kinds of things that are localized in the region you care about. That's the idea of the talk. Here's a different example, kind of a toy one. Here's again a step potential. I chose it at random. Pick numbers between 0 and 100,000 for this C. I throw on just this kind of constant convection term where it's that. Term where it's that morally self-adjoint version I spoke about. I could solve a source problem. And that's this function u, and its graph is the top bit, and there's the negative of u. And there's a theorem. If you become unamused with what I'm saying, you could try to prove this theorem. It's not so bad to prove. That every eigenfunction of this operator, its graph has to lie in here if you scale it appropriately, between these two curves. Between these two curves. And here's the appropriate scaling. You scale by the eigenvalue, you scale by the infinity norm of this kind of shifted thing that I used in the conjugation. So there's this one, there's this version here where I solve this problem with a 1 there and the conjugated operator. And the relation between these two graphs is that this one is e to the 2x times that one. And you can see the first eigenvector, it'll Vector, it localizes here. The next one, which I didn't anticipate, localized in this smaller hump. I didn't understand that until I actually looked at this picture, which is telling me the order in which things will localize. That kind of convection term shifted my, or it screwed up my intuition a bit until I figured out what was going on. But again, you get localization like this. And maybe what you'll care about is localization in some sub-interval, and you only want to. In some sub-interval, and you only want to find this one, this one, these goes to this one, like that kind of thing. So here's a 2D example. I have some of my own 2D examples later on. This is from a paper of Philosh and Maibaroda in Proceedings of the National Academy of Sciences. You know, a plus-plus potential again. They have what in the business is called the land. In the business is called the landscape function, and they look at this to get some intuition about where things might localize and what would be the ground state eigenvalues. Ground state in the broader sense that I mentioned. And here are five. C in this case. C is this. This is C. They'll just piecewise constant. And these are large, I suppose, values. Yeah, there's no scale here. I could look in the paper and get a sense of how large they are, but in higher dimensions, I don't think they have to be as large as they need to be. They have to be as large as they need to be in one dimension before you really see this behavior. And they can be really random and surprise you where the localization is going to occur. Could be surprising. Here is a superposition of the lowest five eigenvoltes. And where they peak is related to where the landscape function peaks, just like those pictures I was showing earlier. And there are theorems, but Theorems, but I don't think they're fully explanatory. So, okay, I care about this for different reasons. I like a mathematical challenge, but a lot of scientists and engineers are going to care about it for, this is a kind of a vague broad statement, don't read it literally. But if you have localization to localization of eigenvectors, then you'll have localization of waves, electromagnetic waves, acoustic waves, at least at certain frequencies. And if you're designing material where you want to exploit that, Where you want to exploit that, it's nice to understand it. So, this comes up in semiconductor design or designing solar cells and things like this. This is probably the easiest for me to understand, is sound mitigation. And here's an example of a wall made of concrete, and you wouldn't think of concrete as, well, it's reflective material. But because of the shape of it, it actually traps a lot of the sound waves that hit it. And I've been told that this works well until it doesn't. That this works well until it doesn't. And it stops working well when people start sticking trash in here, or birds start sticking sticks in there. Because it's kind of sensitive. There's the shape here. And here's a patent for exactly this kind of thing. This is a U.S. patent. There's also a French one. With at least one of them is related. It's a mathematician. Marcel Philoche. So where did the story start? In the 50s. In the 50s, on the physics side of things, with Philip Anderson here, who was working at Bell Laboratories, there was something going on experimentally, and he trusted the results enough to think that the current models for how electrons would diffuse through a lattice could not explain that. So he came up with something, wrote a paper. In 57, when it was published, it took about 10 years to gain traction, and then it really took off. 10 years later, he, this is part of his Nobel Prize. This is part of his Nobel Prize speech for this sort of work. And I've underlined just this part. It's the only part I want to read. You love it? I love it. One has to resort to the indignity of numerical simulations to settle even the simplest questions about localization. If my wife permits it, this is on my tombstone. So I I take that as a delightful provocation. There is a saying here, a gentleman doesn't compute. A gentleman doesn't compute. Yes, yes. Okay. And this is exactly that sentiment. I'm not a gentleman. So I'll tell you just on the math side of things, more in the computational direction, because the physics literature is vast, like thousands and thousands of papers, at least since the 60s. Computational and theoretical physics. This is the big team. This is the Marcel Feloche that. This is the Marcel Felosche that was on that patent. Stitlana Maibaroda, who's Minnesota. David Jarison, probably this community would know. Kennig and Jerison, for sure. MIT, right? Chicago, I think. But maybe MIT. Jerison to MIT. Jerison to MIT. Kennig is Chicago? Okay. Whatever. Okay. You know these names. Doug Arnold is big in numerical analysis. He's the one that I would have known out of this group. This is the big group. Known out of this group. This is the big group that has the $10 million Simons Foundation grant and a few Nobel laureates on their team. And I've listed in the order of the first paper that they came in. Stefan Steinerberger's at the University of Washington. He was at Yale until a couple years ago. And both of these methods aren't really trying to give you eigenvalues directly or eigenvectors. They're trying to identify regions of likely localization and give you a Likely localization and give you an estimate of the ground state without actually solving an eigenvalue problem. That's kind of cute. They solve a source problem. This is it. In fact, he doesn't even solve a source problem. He just takes the potential and does some targeted smoothing of it. This group here is closer to me in the sense that they're numerical analysts and they're really trying to compute things. But all of these works are really focused on negative plus potential and the lower part of the spectrum. And the lower part of the spectrum, and I wanted a different approach. So I think our approach is quite different. The paper, oh, here's a story. This is why this might be an open problem. The paper was accepted without revision, the first time that's ever happened with me. And then about two weeks before this conference, I found an error. Not a trivial one. The first proof was wrong. And I scrambled to try to fix it before this meeting. And I did, but I'm not happy with the fix. So I'll explain that in a few slides. So, I'll explain that in a few slides. So, what do we do? Trying to keep an eye on the time here. So, again, this is a reminder, some boundary conditions, which will actually be pure Dirichlet for me. So, I'm sort of pretending by saying there's maybe mixed boundary conditions. They'll just be pure Dirichlet in the examples. So, I started with the targeted task. If someone were to give me a subdomain and ask, is there anything left? And ask, is there anything localized here? I'm not saying how we would find such a subdomain. I might say that later in conversation. I won't have time during the talk. And some small tolerance, which is going to govern just how localized do I have to be to get that title. And also on a large interval of eigenvalues where I'm searching for things like this. So given these things, I'd like to find every single eigenpair of this self-adjoint operator where the eigenvalue Operator where the eigenvalue is in there and I'm localized enough. So here are how I'm measuring localization. Two complementary measures that I think are kind of natural. There are others that you could find on the market. You could just measure the L2 norm of a function on the region you care about, normalized by its L2 norm of the whole space. Or you can do it in the complementary region. So you would be localized enough if this is really close to one or if this is really close to zero. This is really close to zero. Because the sum of squares is one. And these are scale invariants. They're the right kind of thing, you know, to do this. I guess that's scale invariants, just there. So here's the task. I just recopied what I had on the old slide and highlighted two bits here. I want to have an eigenvalue solver that really tries to find eigenvalues and eigenvectors with the eigenvalues in here, and that's a big interval. Essentially. A big interval, potentially quite large. But also, I have this sort of geometric constraint. And if you were to look on the market, and I know what the market's like, for eigenvalue solvers, they do not naturally incorporate this kind of condition. They're great for this, but not for this. But, if I'm willing to go in the complex direction, and here's where the non-self-adjointness is going to come in, in fact, non-normality, I might be able to encode this kind of thing in the complex direction. That's the idea. Direction. That's the idea. I think it's kind of a simple idea, inspired by some work of Marco Maletta from a little over a decade ago. So I'll change the problem. I change the operator. Here's the self-adjoint part, and then it is a cellular bit. Let's do a shift. That's the characteristic function on the region I care about. And the high-level intuition is that if I had an eigenvector that was almost zero outside of here, Outside of here, then that will almost be an eigenvector of this new operator. Almost, you know. And the eigenvalue that goes with it will almost be lambda plus is. That's kind of a loose statement, but that's the intuition. So I would just push it high up. And if I wasn't very localized there, it wouldn't push as far up. So I'll start searching for things that are near the line where lambda plus IS would be. Lambda plus IS would be. The imaginary part should be close to IS. That's the idea. It's pretty true when S is large. Ah, yes, so we'll get there. That's one of my sufficiently small that I'm not satisfied about. So one of the ideas is that instead of searching for eigenvalues for the original operator that are in here, that have a geometric condition, I will search for eigenvalues of this shifted operator where the eigenvalues are in here. This is what we, I don't even know if I need to use the language. I don't know if I need to use the language. This would be the lower half of a bunimovic, if that means anything to a curve. The lower half of a hockey rink, basically. So I'm going to be searching just in here for eigenvalues of this operator, and that will rule out all kinds of things that could not possibly be localized in the region. It'll act as a first, I'll call a first filter. So here's some numerical or empirical support for this. This is a repeat of what we saw earlier, shrunk down. Repeat of what we saw earlier, shrunk down. And if I only cared about things that were localized in the first subinterval, which is all of the odd ones until here, until number 11, what I do is just go ahead and compute the eigenvalues of the shifted operator with a shift that's 1, and then for you, a shift that's 100, just to see what happens. And these pictures look almost identical. The scaling is the only thing that's different, and it really highlights exactly the And it really highlights exactly the ones that I would have expected. And these ones here that were really localized elsewhere have almost zero imaginary part. And then there's these ones, these ones here, where something small happens in the imaginary area. They're not identical. The scale here is such that you really can't see the shift in the real direction. There is some shift. Not very much here. To eight digits, you wouldn't tell a difference between the eigenvalues, the real parts. Here, there's a little bit more. There's a little bit more. And then you can see there is some kind of difference going on here. So that's lower in the spectrum. The picture gets a little muddier higher in the spectrum, but still can be used. These ones right here, you might be able to buy looking at the pictures to see what they are. But the rest of it, it's not so dramatic as it was in the earlier slides. But still, if I was going to search, I would somehow draw a line. Somehow, I draw a line, and I'd only be looking for these eigenvalues up here as candidates. And that's a shift of one and a shift of a hundred. But now I'm only looking in the fourth sub-interval. Only the fourth sub-interval. So here's the theoretical support, and here's where, if there's an open problem, you can decide if it's worthy. But it's here. This is where I got the proof roll before. Okay, so there's the theorem. If I have an eigenpair and its concentration is below this threshold, so I say it's localized enough, for S sufficiently small, and this is the one that sticks in my throat a bit. It was unfair when I asked Francis about it, but I think it's completely fair and desirable to give a sense of what S acceptable S is or it could be. Then the distance between lambda plus IS and the spectrum of the shifted operator is bounded. But the shifted operator is bounded like this: s times delta star. And that gives me precisely this kind of figure as a hunting ground. Hunt for things in here. So here is my current. What's that? You don't want to have 2 times s times delta star, something like that? I don't want to. I mean, I could. If I have to, I'll be forced to, but the empirical evidence is so strong that I don't. Empirical evidence is so strong that I don't need that, that I'm unwilling yet to even concede it on theoretical grounds. So here's my proof as it spans. I have a shifted operator. Well, now I have, I guess, a holomorphic family. Katu would call this a holomorphic self-adjoint family of type A, where the domain doesn't change as the parameter Z changes. And it's called self-adjoint because when Z is real, Joint because when Z is real, it is a self-adjoint operator. But if Z is not real, it's not even normal. And that was where I screwed up before I declared it to be normal in a fit of, I don't know what. Just declared it to be normal, and the proof was easy. So I let this be the case, and I assume that this eigenvector is scaled and unit-length. And it's just a direct computation to see that how far is the original eigenvector of L close to being. vector of L close to being, how close is this to being an eigenpair of the shifted operator? It's just a straight-up computation. Like that, simple computation. And the norm of this, the norm of that is delta, is the concentration measure. So I have an actual target. I have something whose concentration is hopefully close to zero, if I really want to accept it. And here is the reciprocal of the norm of the resolvent. And in my earlier not proof, In my earlier not proof, when I assume that this was normal incorrectly, well, that right there is the distance between lambda and the spectrum of LZ. But when it's not normal, it's not the distance anymore. There's something else. I could say something about epsilon pseudo-spectrum, where this is epsilon, but I haven't figured out how to make that useful. But if it is real, then this is the distance. This is the distance here. The distance here. So then I say for sufficiently small z, if I fatten this delta up a little bit, I still get the distance to the spectrum being controlled in this way, just using continuity of the spectrum. This is what I'm not satisfied about, is that right there, sufficiently small. And okay, so I can pick a particular S, or Z, I can pick it to be IS, which is related to my operator. In fact, my operator is. In fact, my operator is L of IS plus IS. It's that thing. So I get closeness of the spectrum here, and then if I just push them both up by constants, it's still the same distance. Push them both up by constants. So there's my argument, and this is the bit that I don't like at all. Because in practice, it does not even seem to matter that S times delta is smaller than the gap between two consecutive eigenvalues. It doesn't seem to matter at all. It doesn't seem to matter at all what S is from experiments, just experiments. It seems that I could pick any S, and this ought to be true, replacing delta star with this actual measure here. That's what I've observed. I just don't know how to prove it. But this is why I think it was unfair when I asked you, Francis, but I think it's fair here because this is such a special type of perturbation. Something ought to be said that maybe I haven't read far enough in chapter 7. I haven't read far enough in chapter 7 of Kapto or exponential decay doesn't depend on S. The exponential decay of the eigenvectors? No. But some of mine don't decay. Okay. Well, maybe an answer is there. But when I'm standing here, I don't see it yet. Almost out of time. So I do have a So, I do have a proper theorem. Well, I guess the other one is a theorem, but it's not satisfying to me. To go back from finding an eigenpair of this operator to finding an approximate eigenpair of this one. Okay, there I'm on solid ground. Things are normal in just the right way. And the proof is that if you did have an eigenpair of the shifted operator, and you just look at the imaginary part of the eigenvalue, it is telling you directly up to this shifting thing. Up to this shifting thing, which you control, what its concentration measure is. You could just read it right off the eigenvalue. And a question might be: if I just took the real parts of both of these things, is that close to being an eigenpair of the original operator? Just rip off the real parts. That's it. And the answer is, well, yeah, it is. That the distance between the real part and the spectrum of L is truly controlled by S times delta times tau, the two complementary measures here. Here. So this is something a little bit less than S times delta. Truly is controlled like that. And if I were just to take the real parts of both and look at this residual, well, I have an exact formula for it. I could control for this residual by controlling S. And I could read the delta and the tau directly from the imaginary part of this eigenvector directly off. So, okay, there's still a little room to wiggle because now we're in the complex place. Room to wiggle because now we're in the complex plane. And even if phi has unit norm, I can still multiply by any complex number having unit modulus. It don't change a thing. So maybe there is a best normalization. And I would say the best normalization is the one that minimizes this. Minimizes just this bit. And that's actually pretty easy to find, what that constant is. It turns out that you can solve a 2 by 2 eigenvalue problem by hand. By hand if you want, to come up with that. By hand, if you want, to come up with that scaling. And then you just take that as the real part, or yeah, take that as the normalized width and blow off its imaginary part. So hopefully seeing an algorithm is not too off-putting. But if it is, it'll be over quickly. So the idea here is that I hunt for eigenpairs of this operator where the eigenvalues in that region I showed you. And that's my first filter. That's where I want to rule out nearly. That's where I want to rule out nearly all possible candidates, anything that's not possibly a candidate. But it might open the door a little too wide and let a few other things in. But for each one I find, I normalize it in the way I've just said, and then I blow off the imaginary parts and use that as an initial guess and maybe some little iteration to drive it toward an eigenpair of the actual operator. And then once I have that in hand, I can just check this condition directly. That's the second filter. Directly. That's the second filter where I might throw away a couple more things that squeaked in the first filter. And there are a variety of techniques that are designed precisely for hunting for eigenvalues that are inside a contour in the complex plate that are based on contour integral methods. There are slides at the end. I'm not going to get to those, but you can look at them if you like. I use what's called the Feast method, and that screams acronym. It's a failed acronym. It doesn't stand for anything. He tortured words until he gave up and said, I'll call it feast. I've basically run out of time, but I'll at least show this. So I knew, I should say this at least, if this was my interval AB, where I'm going to hunt for eigenvalues that are localized, and that's my tolerance, there are 130 eigenvalues of the original operator there. I computed them all. Do 100 digits of accuracy if you want. Which is strange for a numerical analyst. Which is strange for a numerical analyst to do that. There are only five of them that are localized in this first region for that tolerance: none in the second region, five in the third region, two in the last. You could look at the pictures themselves and see which they were. And when I use the strategy, when S is 100, or 1, or S is 100, and I'm looking at this sub-interval, I end up with six candidates. In fact, these things almost, they agree very strongly. Almost, they agree very strongly with the true eigenvalues. But I had to throw away one of them that just barely failed to make the cut. So it filtered out 124 in the first pass, and then I had to throw away one more with the final check. Higher up in the spectrum, I had to throw away more. I can't tell you why it found precisely six, but it really did. Just six things. And then what I'll do is, I won't talk about this now, maybe at lunch. About this now, maybe at lunch, if you'd like, but I thought if you're going to ask questions, you might as well have a pretty picture to look at. Localization due to geometry. Dr. Grebenkoff would know quite a bit about that. I don't know if he's watching it. He's got a nice Scientology article that talks about geometric things that could cause this. So thank you for your attention. Yes. Sorry, I ran over a little bit. I ran over a little bit. Any question or comment? I'm sort of wondering whether there'd be any interest in looking at C depending upon. I mean, the operator with eigenvalue is zero. Depending on X, depending on X for you as the eigenvalue? Yeah, position coordinate. Okay, the position coordinate. Right. I haven't thought along those lines. I guess I thought that maybe the z could depend on the gap between the eigenvalue you care about and the next one, but I hope it doesn't. I don't think it does. And I was showing you a bunch of 1D examples, but you must have a generalization you have in mind when I'm talking about two or three D. Right. Okay. Well, perhaps you could. Okay. Well perhaps you could we could talk about that. So in in applications it may allow for just placing a uh a piece of rubble inside this reticle that you have. Now you have an indirect that that's the C, you change the C. Yes, I did. I changed the C. In one spot I changed it to be some complex thing. Yeah, when I add an impunity to your original C, that's connected dependency. Yes, yeah, it does, yeah, yeah. Yes, yeah, it does, yeah, yeah. It certainly is a spatially dependent, and that's how I chose it. I chose the characteristic function of the region you care about, but then I push in the complex plane directly and introduce a complex impurity. Okay, thank you. I hit a state that something happened.