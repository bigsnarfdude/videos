Today I will be talking about the different notions of non-commutative independence. So I will start by introducing what are non-commutative probability spaces and how do we define non-commutative distributions. And then I will discuss each notion of independence and give you examples of matrix models where these notions appear in a natural way. Notions appear in a natural way. And then in the second part of my talk, I will tell you a little bit about the operator-valued setting. I will only discuss there the free case and I will tell you about results on the central limit theorem for each notion of independence and show you what happens in the operator-valid setting. So, to start, I'm going to To start, I'm going to tell you what are non-commutative probability spaces. So, a non-commutative probability space consists of a pair A phi, where A is a complex star algebra with unit one, and phi is a linear functional that's unital and that's positive. And we call any element A of the algebra to be in. Of the algebra to be in non-commutative random variables. So, to give you some examples, of course, this includes the commutative setting, the classical probability setting. So, if you consider random variables with moments of or orders, in this case, the linear functional phi, which I'm going to call a state, is nothing but the expectation. As for a non-commuted As for a non-commutative example, we have the n by n complex-valued matrices. And in this case, the state phi is the normalized trace. We normalize the trace so that the trace of the identity is equal to one. And if we put two these frameworks together, we obtain random matrices. So these are matrices whose entries are random variables. And in this case, the state. State is the expectation of the normalized traits. So, how now we have a notion of non-commutative probability spaces, we would like to make sense of non-commutative distributions. So, in the non-commutative setting, distributions are defined in terms of moments. So, if we have a non-commutative random variable x, its distribution is simply the collection of all moments of x. Of all moments of x with respect to phi. And convergence in distribution, we say that xn converges to x if for any k, the kth moment of xn converges to the kth moment of x as n goes to infinity. Now, if we assume that our random variable is a self-adjoint operator, then in this case we can. Operator, then in this case, we can give a description of the non-commutative setting in terms of measure theory. So there exists a unique probability measure, actually a real probability measure, mu x, that has the same moments. So its moments are the same as those of x. And we say that mu x is the spectral measure of x. Now, having this link to Link to measure theory gives us access to lots of tools, analytical tools, that turn out to be very powerful and useful. And among these tools is the Cauchy transform. So along this talk, I will be denoting by gx to be the resolvent of x at z, and the Cauchy transform is phi of gx, which is the integral of g x which is the integral of 1 over z minus t d mu x of t. So having access to the Cauchy transform will help us prove convergence in distribution using analytical tools and not only through the moments. So we have this due to the fact that convergence in distribution of xn to x is equivalent to the pointwise Is equivalent to the pointwise convergence of the Cauchy transform of xn to the Cauchy transform of x for any point z in the upper half plane. So this reduces the study of convergence in distribution to proving the pointwise convergence of nice analytical functions like the Cauchy transform. And another nice property about the Cauchy transform is that it characterizes the measure. So if we know the Cauchy transform of x, Transform of x, we can find the Î¼ of x using the Stilges inversion formula. So knowing the Cauchy transform means that we know the distribution, and this reduces our problem to looking at the point-wise conversions of Cauchy transforms. Now, how about giant non-commutative distributions? So, now assume that I have a family of random variables and not just one, and I would like to look at And I would like to look at the joint non-commutative distribution. So, this is simply given by the collection of all possible mixed moments in these elements. So, I take any word in the xi's of any length. So this can be any word of any length, and I look at phi of this word. And the collection of all these mixed moments is what defines the joint non-commutative distribution. Non-commutative distribution. Unlike the case of one self-adjoint random variable, here we cannot describe this combinatorial object in terms of probability measures. So we cannot find a real-valued or even a complex valued probability measure that encodes all these moments. So, in order to be able to say something analytical, we consider test functions. So, this is one way of doing things. Doing things. As an example of test functions, our non-commutative polynomials, of course, this is not the only example of test functions we have, but I will only consider this for this talk. So non-commutative polynomials are expressions of this form. Here we have words in the x i's and the a i's are complex valued. And we call c of x1 up to xn to be the unit. X1 up to Xn to be the unital complex star algebra of all non-commutative polynomials in the determinates X1 up to Xn. So let's assume we take a self-adjoint polynomial. Now we have a self-adjoint polynomial in our variables. So we have one single self-adjoint operator. This gives us access to measure theory. We can describe the distribution of these polynomials in terms of a probability measure. In terms of a probability measure, and we can do the analytical stuff that we usually like to do. Other examples of test functions are, of course, for example, matrix linear pencils, also rational functions, but I'm not going to go in this direction. It's out of the scope of this talk. Now, having defined non-commutative spaces and non-commutative distributions, it's Distributions, it's time to define the first notion of independence. So, in the non-commutative setting, we have five notions of independence: free boolean, monotone, and antimonoton independence, in addition to the classical independence, which is the tensor independence. So, these are the notions of the independence. I will start by the free case because it's the most known one. So, free independence was So free independence was introduced by Valkulisko in the 80s when he was studying free group factors. And he defined it in this way. So we say that unital sub-algebras a1 up to an are freely independent with respect to phi. If whenever we have a1 up to a k that are centered, so each ai is centered, and the ai's of And the Ai's are alternating, meaning that two consecutive Ai's cannot come from the same sub-algebra. So, whenever we have this setting, then phi of this word is equal to zero. So, you can think of this, I mean, this is from the name it's inspired from free algebras, you can think of it that it translates the fact that if you take two words from two free algebras, Two free algebras, then there's no algebraic relationships between them. You can think of it that way. Now, this definition gives us a rule that allows us to compute moments of independent variables. I mean, to compute phi of words in independent variables. So, if I assume that the AIs are freely independent from the BI's, then phi of words. Then phi of AB is equal to phi A minus phi B. Simply, you can do the following. We know that if we center both of them, then by the definition of free independence, this is zero. And this will directly imply what is phi of AB. So this gives us a recursive way of computing moments, but of course, we have a moment cumulant formula that allows us in terms of Formula that allows us, in terms of the few cumulants, to compute moments directly. But it's coming from this in this recursive way. Now I will elaborate a little bit about the freeness. So just to make a little bit of comparison between the non-commutative setting and the classical setting. So in the free setting, we have the notion of free independence, which is different from the classical. The classical independence. As in the classical setting, what does it mean to have independence between two variables? It means that if we know the distribution of each one of them, if we know the distribution of A and we know the distribution of B, and we know that they are freely independent from each other, then we can compute the distribution of their sum. And this is given through the additive convolution, free additive convolution. Free additive convolution. Similarly, for the product, as the Gaussian distribution plays a central role in classical probability, here we have the semicircular distribution that plays an analog role. We also have a free central limit theorems that I'm going to discuss later on in my talk. And the analog, for example, The analog, for example, of the look-Fourier transform is the R transform. I mean, I can keep, I can make this list longer, but it's just to give you an idea that this is a theory on its own, and many things are parallel to the classical setting. And on top of this scalar non-commutative setting, where we have a functional phi from A to C, so this is complex valued, we have an This is complex valued. We have an operator extension to it, which is the operator-valued free probability that I will discuss in the second part of my talk. So, as I promised you, for each notion of independence, I will be illustrating matrix models. So, I'm going to give you a small reminder from random matrix theory. So, if we consider a GUE matrix, which is GUE matrix, which is a symmetric or Hermitian matrix whose entries are IID Gaussians, assume that they are centered and their variance is sigma squared. And we look at the empirical spectral distribution of this matrix. So we look at 1 over n, the sum of the Dirac measure and the eigenvalues of this matrix. So this is a random probability measure. And one of the most fundamental results. One of the most fundamental results in random matrix theory is the Wigner law. We probably heard most of you have heard of it, which says that almost surely this random probability measure converges weakly or in distribution to a deterministic probability measure whose density is given by the function. So, this is the semicircular distribution. This is the Wigner semicircular distribution. And you can see here that in the limit, what appears What appears is the variance sigma squared. And even here, if I consider different distributions than the Gaussian distributions, then the limiting distribution will remain the same. And what will appear in the limit is just sigma squared. So this reminds us of the law of the large number in the classical setting, because we have a random probability measure converging to something which is deterministic, and also a trip. Is deterministic, and also it reminds us of the central limit theorem in the sense of its universality. Like, no matter what distribution you start with, what will appear in the limit is the semicircle distribution, and what is appearing is only sigma squared, the variance of the inference. So how does free probability come into the picture? Well, it comes into the picture when we consider more than one matrix. Okay, so and this is what Volkolisko realized. And this is what Valkalisko realized in 91: that if you consider D-independent GOE matrices, and then if you look at the, you take any polynomial in them, and you look at its limit of the expectation of the normalized trace of this distribution, then this is nothing but the distribution of the same polynomial, but in three semicircular elements. So these semicircular elements. So, these semicircular elements have the semicircle distribution. So, they share the same moments as the semicircle distribution, which is given in terms of the Catalan numbers. But what's more important, I mean, what's also very important is that, sorry, don't know what happened, is that these semicircular elements, they are freely independent. So, freeness appears in this context. Appears in this context in a natural way in the large n limit of random matrices. And once Mokulisku did this connection to random matrices, I mean, there was lots of interchange between the two domains that led to very nice and important results. Now, let me pass to Boolean independence. Boolean independence is defined as follows. If we consider If we consider sub-algebras a1 up to an that are not necessarily unital in this case, we say that a1 up to a n are Boolean independent if phi of a1 up to an is simply the product of phi of each element. So this is very simple. As long as the AIs are alternating, then we have this factorization. Factorization and this also gives us a rule to compute mixed moments in Boolean-independent elements. So, here is the rule is really very simple. You just, if you look at the last example, you take phi of each. Now, how about a matrix model for the Boolean-independent setting? So, for this, I need to introduce some notation and some definitions. First of all, I'm going to write an n by n matrix in its block 4. Okay, so I'm going to choose n0. I'm going to fix n0. And this block here will be of size n0 times n0. And this one here, it is n minus n 0. So it's a square block. So I'm going to divide my matrix. I would like to write it in this block structure. And I'm going to define t of an Of a n to be the matrix where these block diagonal blocks are replaced by zeros, and I keep everything else as it is. I would also like to define the partial trace of a matrix A n. So the partial trace is the expectation of the trace of the first block A11 of the matrix, and we normalize it by 1 over N0. Okay? Okay, so Lenszewski proved in 2014 that if n0 is fixed and if I choose a n and b n to be independent GUE matrices, then T of A n and T of B n are asymptotically Boolean independent with respect to the partial trace. So here for this model, if A the model if A and B are independent GUEs, but now instead of considering A and B like in the free case, I consider T of A n and T of B n, then what we obtain is asymptotic Boolean independence with respect to the partial trace. And what does this really mean? This means that there is a star probability space, a non-commutative probability space. Space and it has two elements A and B that are Boolean independent with respect to the state of this space, such that T A N and T B N converges in distribution to AB with respect to the partial trace. So if I look at the joint distribution of these matrices with respect to the partial trace, this will be the distribution of two Boolean independent elements A and B. Independent elements a and b. So, in other words, if I take any polynomial in them and look at the partial trace and take n to infinity, then this is simply psi of p of ab. Also, Camille Mal gave in 2014 another example for a matrix model where asymptotic Boolean independence appears, but this time with respect to the antitrace. Now I come to monotone independence. Unlike the free and Boolean notions of independence, monotone independence has an order. So we have an order on the level of the sub-algebras. So we say that A1 is monoton independent of A2, monotone independent of A3, etc., up to a n. And we, in order to have this order. To have this order clear, we use this sign. We say that they are monotonous independent if they satisfy this condition. So maybe this rule is hard to read right away. So I chose to give you an example of only three algebras here, A, less than B, less than C, and that are monoton-independent in this order. And I'm picking elements from A, B, and C, and I would like. From A, B, and C, and I would like to look at phi of this word. So, an easy way to find this is to do this following diagram here. So, we have the AIs on this lower level, the BI's are on the second level, and C on the highest level. And we start by removing the summits. The summits. So the first thing we do is that we remove the summit here and we obtain phi of c. And then what's remaining are the following here. So this is phi of a1, b1, a2, b2, b3, a3. And now I need to remove the other summits. So here I have one summit, and here I have another summit. And this will give me phi of b1 and phi of b2 b3. And what remains at And what remains at the end is phi of A1, A2, A3. So maybe with this example, it's easier for you to see how monotonous independence is defined. And now here's an example. I'm choosing, as before, technically the same examples to show you how what we obtain. So phi of AB is phi of A times phi of B, and this is the last one that is. And this is the last one that is changing among all the other notions of independence that I showed. Now, for the anti-monotone independence, it's defined like monotone independence, but it is mirror symmetric to it. So it's the opposite order. This is what we call anti-monoton. For the matrix models, so again, I will remind you by the notation I used before, have a matrix AN divided. Have a matrix an divided and write it in this block structure, and I consider t of an with zeros of the block diagonal, diagonal blocks. And again for the partial trace. So I'm going to give you several examples about where monotone independence appears in the limit. The first one was due to Lenszewski in 2014, where he assumed that if AN and BN are independent GUEs, Are independent GOEs, then T of A n and B n, so you take T of the first one and you and B n is asymptotically monoton independent with respect to the partial trace. So for the Boolean independence case, we consider T of A n and T of B n. Now, T of A n and B n, we obtain monotone independence. Recently, there were Recently, there were new results of Mango and Seng that allow us to construct matrix models that are asymptotically monotone independent once we have results on asymptotic infinitesimal frames. So, I'm not going to go into details of this. Go into details of this, but in the literature, there were a few results on asymptotic infinitesimal freeness. So, if we can combine these results with the results of Mango and Seng, we can come up with matrix models where mountain independence appears in the limit. As examples, if you take A n to be GUE matrix and B n to be an entry permutation of A n as an example, you can take B n As an example, you can take Bn to be the transpose of A n. So here you can see that they are no longer independent. And again, if you take T of A n, if you look at the pair T of A n B n, then this is asymptotically monoton independent with respect to the partial trace. Another results by recent results by Benson Ao on asymptotic infinities in Melfreeness, also one combined with the results of Mengo Seng, will give Main god same will tell us that if you take a n and b n to be independent Wigner matrices, so Wigner matrices for me are the symmetric Hermitian matrices that I showed you before, but whose entries can have any distribution, not necessarily Gaussian. So if you take Wigner, independent Wigner matrices A n and B n that satisfy some moment conditions, then you have asymptotic infinitesimal freeness, and if you combine it with the results, And if you combine it with the result of mango, you obtain you obtain asymptotic monotone independence. Maro, did you notice that Gaitan had a question in the chat? No. Okay, I think that there are some conditions on the permutations. So, how you choose your permutation. But, for example, the Permutation. But for example, the transpose works. Right. So there's a, yeah, there's you have to move a certain number of matrix entries away. Yes, yeah, yeah. Yeah, yeah, yeah. So yeah, I should have been more clear, sorry. So what does this mean? This means that, what does it mean to be asymptotically monotone independent? It means that there is a star probability space, an uncommitted Space, a non-commutative probability space that has elements A and B that are monoton independent. And T A N and B N converges to AB and star distribution with respect to the partial trace. So again, if we take a polynomial in these matrices and we look at the large n limit of the partial trace, then this will be the distribution of these polynomials in A. Of these polynomials in A and B, with A and B being monotonous. So having all these notions of independence, as I told you before, it is important because it will allow us, now we know that these A and B are independent, and we know that they are monoton independent, so we have rules that allow us to compute the distributions of sums of their sums of probability. Sums of their sums of products or of polynomials in A and B. So, among the first natural polynomials to look at are the commutator and the anti-commutator. So, the distribution was found by Nikke and Spicer in 2006 for the commutator. However, it was recently that the distribution of the anti-commutator and the free Mutator in the free case found. Also, recently, Mengo and Seng covered the Boolean case. And with Pylun Seng, we fill the gap and complete the story by finding the distribution of the commutator and the anti-commutator whenever A and B are monotone independent. We also actually Give the distribution for more general functions. So, if we consider linear spans in AB and BA, also this is covered. And also, some more general functions we give, we explain the approach to find their distribution. For example, if you have polynomials of this form. And for the particular case of the linear spans. The linear spans of AB and BA. If we assume that A is monoton-independent from B, then this is how the case moment of this polynomial look like. They are given in terms of the case moments of A and the first and second moments of B. And if you pick alpha and beta in a way to obtain Beta in a way to obtain the commutator and anti-commutator, then these are the distributions. So you don't need to remember these details. But if we consider now matrices, for example, AN and BN, that are n by n Wigner matrices that have some moment conditions. Here I'm hiding some hypothesis in my statement. And for example, if we consider this polynomial in T of A. Polynomial in T of A n and b n. I told you from the previous slide that now we know that T of A n and b n are monotone independent. So by our result on monotone independence variables, we can apply it to these matrices. And now we know that the limiting distribution of Pn with respect to the partial trace, we have its moments, and this will be the probability measure. measure given by 1 over 2 the Dirac and omega plus the Dirac and minus omega, where omega depends on the hetalon numbers, okay, in this way. So just to show you that knowing having such results on the level of the operators, it allows us to work on the level of the operators and then deduce them on the level of the matrices once we have these asymptotic independent. Independence result, whether it's for free, Boolean, or not. So, this is another example just to show you that we can consider more general polynomials than just the commutator and anti-commutator and linear spans of AB and BA. For example, we have this polynomial. We can also find the distribution of the limiting distribution with respect to the partial trace. Distribution with respect to the partial trace. Okay, now, as for general polynomials, if we want to consider polynomials that are more general than what I showed you, then of course there's the linearization that is a very well-known technique that allow us to study polynomials in non-commutative elements. In non-commutative elements. So I will go back to the free case. I mean, here it's not important. I'll just give you a few ideas to motivate why we need the operator-valued independence. So let's assume I would like to study the distribution of this polynomial in x1 up to xd, and we know that they are freely independent. So the linearization techniques allow us to consider To consider this linear polynomial in x1 up to xn, but whose coefficients are now matrices instead of being complex numbers. So here the AIs were complex valued. Now I have a linear polynomial, but with matrix-valued coefficients. And this polynomial p hat, which is the linearization of p, we have the following. We have the following interesting relationship. We can find the Cauchy transform of our original polynomial P by looking at the operator-valued Cauchy transform of this matrix-valued polynomial and pick up its one, one entry. So, I will not go into details, just want to tell you that if you would like to study more general polynomials, General polynomials, we have this non-linearity. We lift this polynomial to an operator-valued setting to turn it linear. So we make our polynomial linear. We have now, it's much easier to consider such a linear polynomial. But the price that we pay is that now this polynomial is matrix-valued. But in order to use all the tools that we have from free probability, Free probability, we would need its operator-valued extension. So, here our operator-valued probability space is the matrices whose entries are operators from M. Okay, so these polynomials belong to it. The state of this non-committed probability space is the normalized trace tensor phi, but here we are looking at the idea. But here we are looking at the identity tensor phi, and I'm picking the 1-1 entry of this matrix. So, this here will be the conditional expectation in our operator-valued probability space. So, in order to be able to study more general polynomials, we need more advanced tools. And for this, I'm going to tell you a little bit about operator-valued free probability. So, what is an operator-valued probability space? It consists of A, E, and B, where A again is a complex algebra with unit one. B is a unital complex sub-algebra of A. It's unitly embedded in A. And E, it's a mapping from A to B, which is a unital BB by module map, satisfying these two conditions. So if you look So, if you look closely at these conditions, these are conditions of conditional expectation, and we call E the conditional expectation in this case. So, let's assume now we have independence with respect to E, free independence with respect to E. I'm not going to tell you about the Boolean and Monoton case. They are similar to the free case. So, in the scalar setting, if A and B were free with respect to phi, this is how. Respect to phi, this is how they factorize. Now, if they are free with respect to E, then we have exactly the same decomposition, but it is nested because now E of B, it's not a complex number, it is an operator in B. So it doesn't commute with A, so I need to remember where the operator was. So it is exactly the same rule as in the scalar case, but we need to keep track of where our operators are. So we get this. Our operators are so we get this nested version of the scalar set. This is another example. I assumed phi of b to be equal to zero and e of b to be equal to zero just to simplify the expression, but to show you that it's exactly the same rule, but with this nesting. All right, so now how about b-valued distribution? So, now how about B-valued distributions? How are they defined? Again, they are defined in terms of the moments, but now we would need to consider words with elements B1 up to BK coming from the sub-algebra B that can be between the XI's. So, I need to consider B-valued moments that look this way. So, again, to look at the distribution. Again, to look at the distribution of x, I consider any word of any length for any choice of the bi's. And the collection of all these moments is what we call the b value distribution of x. And now how is convergence in distribution defined in this case? So, just for the sake of clarity, I'm going to define the kth moment of x in this way. So, depending on the choice of the bi. On the choice of the BI's and convergence in distribution over B. We say that Xn converges in distribution over B if for any K and any choice of Bi's, the kth moment of Xn in B1 up to BK converges in norm to that of X. Okay, so if we have, if this holds for any If this holds for any k and any choice of the b i's, then we say that xn converges to x over b. And similar to the scalar case, we have operator-valued Cauchy transforms. So now this is the resolvent, but now it is defined on the upper plane of B. So similar definition, but with respect to the expectation E instead of Expectation E instead of phi of the conditional expectation. But now the equivalence between convergence in terms of the Cauchy transforms and convergence in distribution has to hold for any amplification of the x n's. And why do we need this? We need these amplifications in order to have convergence. To have convergence for any choice of B1 up to BK. If we only have the convergence for B, not the amplified one, this will imply that all the moments will converge, but here if we have the same B. But if I want to allow mixed any choice of the Bi's, I need that this holds for any amplification that I choose of any order K. choose of any order k. Okay, so once we have this, we have equivalence to convergence and distribution over b. And here again, I have the same analogy as before. So we have freeness with respect to the conditional expectation. We have operator value the free convolution that allows us to find the distribution of A and B, the distribution of the sum of A and B. Of the sum of A and B, once we know the distribution of each one of them, we have an operator-valued free central limit theorem, operator-valued R-transform. And here, what plays the role of the semicircular distribution is, or the semicircular elements are operator-valued semicircular elements that are centered, and now their variance eta is no longer. Is no longer a scalar. So, in the scalar setting, the variance was sigma squared. Now, this the variance of the operator-valued semicircular element is no longer a scalar. It is a completely positive mapping from B to B that is defined in this way. And moreover, the B-valued Cauchy transform is the Transform is the unique analytical map that satisfies this equation. So you can see that this equation here depends on eta and it is this variance map that will define the distribution of your operator valued semicircular element. So once you solve this, you would find your operator-valued Cauchy transform, and from it you can find the Can find the distribution of the operated value semicircular L. So now I will start telling you a little bit about the central limit theorems in each setting. I will start by the scalar setting and then I will move to the operator-valued setting. So, the central limit theorem in the classical case, the tensor independence or classical independence. Or classical independence. If we have random variables that are IID centered and with variance one, then we know that by the central limit theorem, the limiting distribution is the Gaussian distribution 0, 1. In the free case, so now if I take operators that are freely independent, again centered of variance 1, then what we obtain at the limit. Then, what we obtain, the limit, is the semicircular element. So, in terms of moments, this would look as follows. So, if I, the kth moment of this of the normalized sum will be the kth moment of the Gaussian distribution and here of the semicircular distribution. Now, how about the Boolean and monoton cases? So, in the Boolean setting, what we obtain is a What we obtain is a Bernoulli element, and this was due to Spicer and Vurungi. And in the monoton case, this was proven by Muraki, and the limiting distribution is the arc sign distribution. So I'm going to write them in terms of moments so that you can see the probability distributions that are appearing in the limit. So here we call it Bernoulli distribution in the Boolean case, but actually what appears at the limit is the Redmacho distribution, but we call B a Berlin. Redmacho distribution, but we call B a Bernoulli element. And here in the monoton case, we obtain the arc sign distribution. People are always interested in finding rates of convergence for the central limit theorems. So I'm going to show you what rates we obtain and what moments do we require. So in the classical setting, we would need moment of third order, and under the colomograph this. And under the Kolmogorov distance, which I'm recalling here at the bottom of my slide, this convergence is of the order one over square root of n. In the free setting, we would require moment of order 4 to be finite. And again, this was proven by Christia Kobe and Goetze. And again, we obtain. And again, we obtained the square root 1 over square root of n as rate of conversions. And in the Boolean setting, this was proven by Erismendi and Salazar. And here we require fourth moment, and the rate is n to the power 1 over 3. And in the monotone setting, this is of the order n minus 1 over 8, and also we require a moment of order 4. Also, we require moment of order four. So, all of these are in the scalar settings. Now, I will tell you what is the operator-valued central limit theorem, and I will show you the results that we obtain in this framework. So, if we have a family of self-adjoint elements that are free with amalgamation or free over B, free with respect to. Over B, free with respect to the conditional expectation E, and we assume that they are centered, then Valkoliscu proved that what we will obtain in the limit is when we consider the normalized sum of the xi's, is this will converge to the operator-valued semicircular distribution whose variance is given by this completely positive map eta. eta so as expected here we what will appear in the limit instead of the semicircular element we will get we will have the operator value semicircular element and again its distribution is determined by this variance map now with to bs my we give quantitative estimate for this result and we prove that And we prove that if you take the difference between the Cauchy transforms, of course, we do it on the level of the Cauchy transforms, and we prove that their difference in norm is upper bounded by one over square root of n, the norm of the inverse of the imaginary part of b times a of x. I will tell you in a second what is this a of x. But the most important thing is that we have this rate one over square root. This rate one over square root of n. And again, of course, the limiting distribution is that of an operator-valued semicircular element with this completely positive variance map. So what is this A of X? This A of X, it depends on the second moment, second operator-valued moment of X and the fourth and the fourth. And the fourth operator-valued moment of the xi's. So, here again, we require moments of order 2 and 4. But of course, here it's in terms of the b-valued moments. And I would like to say that there were other results in the literature, for example, the result of Jeckel and Liu. They get the same rate of convergence, but it is in terms Convergence, but it is in terms of the operator norm of the xi's. So, our approach allows us to pass to the unbounded case, which we tackle in an ongoing work with Nicolae Girier. So, again, remarks about this approach. The bounds we obtain do not depend on the operator norm and are merely. On the operator norm and are merely in terms of the moments. And we also can have the fully matricial extension of these results to have such bounds for any amplification of any order k. So again, if the xi's are freely independent over a, then their amplifications, when we amplify, freeness is We amplify freeness is conserved, but now we have a freeness with respect to the identity tensor E. So there is no need to repeat our to prove it in the amplified setting from the beginning. We just change our operated valued probability space and apply this same result to obtain the following. And here, what changes is that you obtain this k to the power 3, but this is not harmful because we don't want to. Because we don't want to be uniform with respect to k. So, what matters for us is only this one over square root of n. And of course, if we assume some more results, then we recover Valkolisko's, the three-central limit theorem by Valcolisko, but by giving quantifying quantified the version of it. And maybe now I will tell you a few words about the proof. Few words about the proof. So, how to prove this? We have the xi's that are free with amalgamation over B. We're going to choose operators ui to be operator-valided semicircular elements that are centered and that have the same second moments as the xi's. So, we choose operators yi to be semicircular elements that are. Semicircular elements that are free with amalgamation, of course, within each other, and that have the same first and second moments as the xi's. And we set Sn to be 1 over n, their sum. Now, this Sn is itself, for any n in n, is itself an operator-valued semicircular element with this variance map. So, I'm going to write the operator-valued semicircular element under this form. Under this form, okay, similar form to this one, and I'm going to compare their conditional expectations, the difference of their, sorry, the difference of their Cauchy transforms. So in order to do this, we're going to do this using the Lindeberg method. So we're going to choose Zi to be x1 up to xi. up to xi and starting from the i plus 1 position we consider the yi's and we're gonna write this difference as a telescopic sum in this way so note that z0 is equal to sn and zn is equal to xn okay so at each step i'm prepared i'm replacing one yi with the corresponding yi with the corresponding xi. And then I'm going to consider this zi0, replace xi here by 0. And why do we do this? Because notice that zi minus z0i, it is xi. And xi is independent of zi0. So I want to have terms that have independent elements. And then, here, what do I do? So, here I subtract and add this resolvent, but with respect to z i zero. And now I will consider this term alone and this term alone. Why do I do this? Because if I use just the resolvent identity up to order three, Up to order 3, I can write this difference in this way. So this is an algebraic identity, this is the resolvent identity up to order 3. But what happened now? Here I have G of Z0i, G of Z0i, which is freely independent of Xi. Similarly here, they are freely independent of Xi. And here only I have this. And here only I have this zi that contains xi. So here in the last term, I have dependence. But in the first two terms, I have words in freely independent elements. So what happens now if I take the expectation, this will factorize in this way. Okay, so if you look at the first term. So, if you look at the first term, here I have E of xi, but I assume that the xi's are centered, so this term is zero. And here, what do I obtain? I obtain the second moment of xi, but I'm subtracting the same thing from it. From the second term, I will obtain exactly the same relationship, but with the xi replaced by yi. And since I assume that they. I's and since I assume that they have the same variance, then this term will cancel with the corresponding term from the second term here above. So what remains at the end is this third order term that we need to control. So under the hypothesis of matching moments, the first and second moments cancel, and it's enough to control the third order term. Control the third order term to obtain bounds in terms of the fourth moment. Here I will not go too much into the details, but here we need to do some work in order to obtain results on the level of the Levy and Kolmogorov distance. So just a reminder about the Levy and Kolmogorov distance, they are defined as follows, but in order to find bounds of from the operator from the Cauchy transforms on the Levy distance, we use this upper bound from the Levy distance. From the Levy distance, it is a very well-known bound, and we give a proof on it in the appendix of our paper. So, once you have bounds on the Cauchy transform of the difference, and these bounds are integrable, then we can pass these bounds on the level of the Levy distance. And the hard part in the proof was to find bounds that are integrable and in terms of the second and fourth moment. And once we have the And once we have this, we can pass our results to conditions, sorry, to quantitative estimates on the level of the Levy distance and not only on the level of the Cauchy transforms. So these results, we're not claiming that they are optimal. And actually in an ongoing work, we obtained results on the level of the Wesserstein distance and we obtained better rates. But these were the Weights, but these were the first results on the operator-valued cosh on the operator-valued central limit theorem that give quantitative estimates. And if we would like to obtain results on the Kolmogorov distance, we need to know more things about the distribution of the operator-valued semicircular element. So, if we know in addition that it is holder-continuous, That it is holder continuous, then by all the results together again with 2BSMI, we can pass our bounds to the level of the Kolmograph distance. So the advantage of having these bounds on the Levy distance is that they hold for any distribution, whether it has atoms or not. So the distribution of Sn is allowed also to have atoms, and this bond will hold. If we want bonds on the Kolmogorov distance, then we need to. On the Kolmogorov distance, then we need to know that we have some holder continuity kind of regularity on the distribution of S. So similar results to these, I mean, what I showed you are very SC and bounds in terms of the moments. We have them on the level and of the Levy and Kolmogorov distance. We also extend these results to the multivariate setting. I only Multivariate setting. I only showed you the case of one variable. We can consider in the multivariate setting matrices with operator-valued entries. We can consider matrix linear pencils, non-commutative polynomials, but I'm not going to go into details in this. Here are some, I'm going a little bit fast because I'm running out of time. I'm running out of time. Here's some history that deserves to be discussed with more attention, but I will just list it here for the time being. And finally, I would like to tell you that these similar results, we extend them to the operator-valued Boolean and operator-valued monotone-independent case, and also for. And also for the infinitesimal free Boolean and monoton independence. So I think I'm running out of time, so I will stop here, although I was hoping to tell you more about infringees in the freenas. But unless I have more time, I would continue. Otherwise, I can stop here for questions. So it's up to the organizers to decide. What about another five minutes? Could you work with that? Could you work with that? Yeah? Okay, go for it. Okay, so the operator-valued infinitesimal setting, we have also a free central limit for that. I will tell you in a bit what is this setting. And similar results as those that I showed you can also be extended to the infinitesimal free, infinitesimal Boolean, and infinitesimal monotone independent setting. Monotone independent set. So, what is infinitesimal free independence? Just I'm going to show you the free case. So, an infinitesimal non-commutative probability space, it consists of a non-commutative probability space A phi and an additional linear functional phi prime from A to C such that phi prime of one is equal to zero. That phi prime of one is equal to zero. So, this is an infinitesimal non-commuted probability space. And we say that sub-algebras ai of i are said to be infinitesimally freely independent if for any, I mean, if the variables xi are freely independent. So, this first condition is the condition of freely independent. And then, whenever they are centered with Centered with respect to phi and coming from alternating algebras, we also have this condition satisfied for phi prime. So I'm not going to discuss a lot this definition, but I'm going to tell you, to give you an explanation on the level of the matrices, because I think it's more intuitive. So if we consider a tuple of n by n random matrices and we And we assume that M is the algebra of polynomials in K non-commuting indeterminates, then we're going to define phi of an to be from M to C, which is the expectation of the normalized trace of P. So phi A n of P, you take P of these matrices and you look at the expectation of its normalized trace. So what is fun? So, what is phi? You've seen it from before. This is the limiting spectral distribution of phi an. So, when you take limit as n grows to infinity. Now, what is phi prime in this case? It will be the term of the order 1 over n in the expansion. So, this phi of a n here, this is actually when we prove this result, the limiting spectral distribution, we write. Spectral distribution, we write an expansion of this. So we will get a term which is of order one, which will correspond to phi, plus a term of the order one over n, that times something here, plus one over n squared, something, etc. Okay, so of course, when you take the limit as n grows to infinity, all these will go to zero, and what will remain is this phi. So, in the infinitesimal setting, we look at the first order that comes here. So, we consider phi of That comes here. So we consider phi of a n minus phi and we multiply by a to be able to see this. So we're zooming m here. So this phi prime, in the case of the GUE matrices, we know that phi of an minus phi is of the order 1 over n squared. It means that when I did this expansion here, this I did this expansion here. This first term was equal to zero, the one that corresponds to one over n. So the infinitesimal distribution of the GUE matrices is zero because we don't have the term of order one over n. Okay, we directly, the first term, that's non-trivial, it's one over n of the order one over n squared. However, this is not true for GOEs. So if you consider matrices whose entries are real values, Matrices whose entries are real-valued Gaussian variables. So here is some list of some matrix models for asymptotic infinitesimal freeness, where we have this independence appearing at the limit. So if you consider GUE, GUE, or unitary invariant matrices, Unitary invariant matrices, they are asymptotically infinitesimally free from finite rank matrices, and this was proven by Schlachtenko. If you take unitary invariant matrices and finite rank matrices, then we have an almost sure proof of it. So, here this was proven in expectation, here it's almost surely. If you consider GUED matrices. If you consider GUD matrices and bounded deterministic matrices, this was proven by De Laporta and Fevrier, and this is also an expectation. If you consider C V shart and C we shart matrices, this was proven by Mingo and Nika, and the infinitesimal distribution was given by Mingo in 2019. Now, what might be surprising for some of you, and it was also surprising for me at the beginning, that if you consider two GOEs, they are not. GOEs, they are not infinitesimally free, and this was proven by Mingo. And finally, Wigner matrices with finite rank matrices are infinitesimally asymptotically infinitesimally free. And this was proven by recent results by A. So this just to give you an idea about what infinitesimal freeness is, because I mentioned it at the beginning of my talk when I was telling you about how we can find Can find matrix models that are monoton independent. So I mentioned it, and because this for infinitesimal freeness, we also have central limit theorems and also in the operative ratios. So related work in progress. We are now considering the C3 case with Nicolas Gdier. We have improved bounds on the free case as well. Free case as well, and another results that are in progress on the multiplicative free central limits here. So, these are works that are ongoing in this direction, and I think it's time for me to stop. Thank you for your attention.