Welcome to our second session today. Our speaker will be Edward Kennedy from CMU statistics, who will talk about non-parametric estimation of heterogeneous effects. Edward, it's yours. Thanks a lot, Pang. Thanks for the work of the organizers. So yeah, I'll jump right in. So there have been a lot of methods proposed recently. If you look at the archive, you'll see lots of different papers on this topic of estimating how treatment effects. Estimating how treatment effects or causal effects vary across people in terms of their measured characteristics. And the main story of the talk today is that there are some important theoretical gaps here. And this is especially the case when there's some special structure that you could exploit. And I'll talk more about what this means shortly. And so one question is, you know, how do we construct better estimators that improve over current state of For current state-of-the-art methods, and more generally, what's the best possible error that we could hope to achieve in this kind of problem? And so, in this talk, I'll sort of survey two papers, two recent papers on these kinds of problems. And so the main contributions here are from this first paper: two estimators with some error guarantees that are faster than what had appeared previously. What had appeared previously, faster rates of convergence, which means smaller mean squared error. And then this paper from this year with Siva Balakrishnan and Larry Wasserman, where we derive a mini-max lower bound so we understand the minimax rates for this problem and then construct a new estimator, a third estimator that actually attains these rates. So that's the punchline. Okay, so I'll start by just kind of introducing heterogeneous causal effect ideas at a high level. At a high level. So, treatments and policies are often studied at population levels. We often look at, say, average outcomes if everyone versus no one were treated with some, you know, according to some policy. But of course, this can obscure potentially important heterogeneity. So you could have these cases in theory where, you know, maybe an effect is zero on average, but it's just because half the population is benefiting and half is being harmed. And so averages would miss this, of course. Miss this, of course. And so, why should we care about this? So, one is just to understand the scientific system that we're studying, improve our understanding of the variation in the treatment that we're working with or the policy. Another is to actually use this heterogeneity to construct optimal treatment decisions, optimal ways of treating people in practice, or to figure out which parts of the population aren't responding the way maybe we hope. Uh, the way maybe we hope they would, uh, you know, in order to derive new kinds of treatments. So, lots of reasons why you might want to understand this kind of heterogeneity, and of course, this comes up across fields. So, I'll just give you two quick examples to kind of keep in your mind as we go on. So, one, we've all become very familiar with. So, these are the three big trials that were done for the three major COVID vaccines. And each of these looked at effect heterogeneity in one way or another. That affects heterogeneity in one way or another. So, here's the Moderna trial. And here they're looking at vaccine efficacy across different subgroups of patients. And of course, this is really important, right? If there are groups of people for whom the vaccine doesn't work, we need to figure out other ways to protect them against infections. On the other hand, if the vaccine is effective across groups, that's also useful to know. We can make sort of Also, useful to know we can make sort of blanket recommendations. This is a case where the latter, something like the latter, is true. There's some uncertainty in some of these groups, but vaccine efficacy is quite high across all these different kinds of patients and subgroups. This is another different trial that looked at more heterogeneous populations, so people across different countries, for example, and saw more heterogeneity in these cases, both in terms of geography and in terms of the kinds of outcomes that they were looking at. Outcomes that they were looking at. So, another that's one example to keep in mind where heterogeneity is important. Another one is in political science where there's been a lot of work studying ways to improve voter turnout. So, here's a plot of voter turnout across decades in the US, and it's quite low, right? Relatively speaking, it's around 50%, depending on the year, even for major presidential elections, midterm elections. Elections, midterm elections. So, you know, not ideal that half the population almost is not voting. So, there are lots of reasons to care about this because the people that do vote look different in general than the people who don't. They tend to be older and wealthier and whiter. And so, there are all these movements to improve voter turnout, these get out the vote sort of movements. Lots of experiments have been done in political science looking at different. Political science looking at different strategies for improving turnout and how effective they are. This is sort of just a high-level summary of effects that have been found for different kinds of strategies for mobilizing voters. So the sort of gold standard here is face-to-face canvassing, you know, going and talking to someone, reminding them of an upcoming election, you know, to register to vote, things like this. This has a pretty strong effect in these past experiments. Past experiments. Maybe the main idea is that, you know, as you sort of become less personalized, the effect gets smaller and smaller. You know, once you get to sort of robo calls and emails, there are little effects seen here. But you can imagine that these effects probably vary across different kinds of people. Maybe some kinds of people would be more responsive to text messages than others, for example. You might imagine this could vary across age. And so here's a just Age. And so here's a just a plot where Cosmo and I and Aaron Strauss did some analysis of treatment effect heterogeneity. Here, looking at, I think in this case, the treatment was a text prompt. And they found some important heterogeneity that, as you might expect, younger people responded differently than older people in these cases. This can be useful for campaigns that have limited funds. Maybe you want to, texting is much cheaper than face-to-face canvassing. So maybe you want to spend your face-to-face canvassing. Your face-to-face canvassing on you know, funds on people that maybe wouldn't respond to text and try to spend text elsewhere. Okay, so that's just some motivation. So, we'll jump into a few more details about the data structure. So, we're just thinking about kind of the classic causal inference setup where we have covariates X, treatment A, which is discrete here, and an outcome Y. So, in these get-out-the-vote examples, for example, X might be. For example, X might be things like where you live, your affiliation, your party affiliation, how you voted in the past, your age, your family size, your race. Treatment might be whether you were actually contacted by a canvasser, and then the outcome might be whether you ended up voting in some upcoming local election, say. And so we need to think about counterfactuals here. So Y superscript A is going to be the outcome we would have seen under a particular treatment level, little A. So Y1 would be whether I would have voted if I was contacted, and Y0, if not. Contacted and y zero, if not. Okay, so the kind of classic parameter that you know people first tend to think about in causal inference is the average treatment effect. So this tells us what would happen in a world where everyone's treated versus no one's treated on average. And so there are potentially lots of ways to characterize heterogeneous causal effects, but maybe the classic way is with the conditional average treatment effect. So instead of looking at how the mean looking at how the mean looking at just the mean potential outcome you look at the mean in different groups defined by covariates x so instead of just averaging this difference potential outcomes we instead sort of regress it on covariates x and in general this would give us a curve or a surface telling us the effective treatment for people at some level little x so that's a causal parameter parameter we want to study the you know statistical estimation of this parameter so Statistical estimation of this parameter. So, we're going to need some assumptions to link it to the kinds of observational data that we typically have where we don't see these potential outcomes. So, under the kind of standard assumptions of known measured confounding that our treatment is as good as randomized within levels of the covariates. And then as long as treatment is not deterministic and we don't have problems like interference, which you would see in some of these vaccine trials, for example, where if you're vaccinated, you might affect my outcomes. But assuming that away, we can actually. But assuming that away, we can actually identify these counterfactual regressions as just usual regressions among people who happen to receive treatment little A. And so that means we can identify the conditional effect, the KET, as just a difference in regression. So it's a regression of the outcome on covariance among treated people minus the regression among controls. So it's a very simple parameter. It's just a difference between two standard vanilla regression functions. It seems like. Vanilla regression functions, it seems like it should be an easy problem, but there are some interesting nuances that come up, and I'll try and give some intuition about why that is. So, how should we estimate this thing, this difference in regressions? The most natural thing to do would be to use a plug-in style estimator where we just estimate each regression separately. So, we filter out the treated subjects in our data set, fit a regression model, filter out the control subjects, fit a regression model, and then get predicted values for both and take. And then get predicted values for both and take a difference. So that's just a plug-in estimator in the sense that it's just sticking hats on these two regressions that we don't know. And it turns out this approach can be highly suboptimal in terms of mean squared error, for example. And the reason why is because these individual regression functions can be much harder to estimate, much harder to model than the difference itself, the difference between these regression functions, which may be very simple. So these can have very different complexities and Have very different complexities, and one way to think about this is that the individual regressions are kind of like natural outcome processes. You know, maybe just describing, for example, how likely certain people are to vote based on their individual characteristics. These might be very complex and difficult to estimate, but the treatment effect is like an interaction term. It's, you know, in a classic regression model, it could be constant, it could be that. Model. It could be constant. It could be that everyone is affected the same way. It could be that there's no effect if we're studying sort of an ineffective treatment. It could be essentially zero, regardless of how complex these individual regression functions are. And so this is what makes this plug-in estimator sort of break down. So in general, you can think the k tau, this difference in regression functions, it has to be at least as smooth or sparse as the individual regressions, but it could be much more smooth or sparse. Regressions, but it could be much more smooth or sparse. And in general, in practice, we might expect that to be the case. Okay, so here's just a toy example driving this point home a bit more. So here's a regression function that's kind of hard to estimate. It has this jump right here, and it has a kink, so it has some non-smoothness, and it's going to be difficult to estimate in general. But in this case, this is the regression function for both treated and control subjects. So I'm just going to simulate some data and show you how the plug-in estimator behaves here. Estimator behaves here. So, in some sense, this is one of these difficult cases where the individual regression functions are hard to estimate, but the difference is just zero. It's just a straight line, right? It's zero. So, here's some simulated data where there's some confounding so that the treated subjects tend to have larger X's and the control subjects tend to have smaller X's. And then, these are just two standard smoothing spline fits in R during this plug-in procedure that I described earlier. And you can see that these are trying. And you can see that these are trying to capture the complexity of these underlying regression functions. And so they're going to be way too complex when we take their difference to construct the plug-in estimator of the case. So here's what that plug-in estimator looks like. You can say it's way too wiggly. It should be just this gray straight line at zero because we're trying to estimate zero effect, but it's giving us something way too complex. And this is sort of a general phenomenon that this plug-in estimator basically makes the K problem as hard as estimating the regression functions. As hard as estimating the regression functions when in many cases, it can be an easier problem. Okay, so I'm going to tell you now about two estimators that can improve on these plugins in some cases dramatically. So one calling the DR learner and the other is called the R learner. And it's, I'm going to tell you about an undersmoothed version of that. I'll tell you what that means in just a minute. And so before I get into the details of these estimators and the results, Details of these estimators and the results, I'll just tell you about the kind of model that we're thinking about here. So, we're thinking about a smooth non-parametric model. So, kind of the classic model that people think about in non-parametric regression and non-parametrics. So, we're going to imagine that the propensity score function is alpha smooth. So, that means it has essentially alpha minus one bounded partial derivatives in each direction of x, and the highest order derivatives is continuous. So, this is also called. Continuous. So, this is also called a Holder class. It's a way of characterizing the smoothness of this function, sort of how close it is to a Taylor expansion. And intuitively, the smoother a function is, the easier it is to estimate. So, this is a classic non-parametric function class. Yeah, there is a question from Shud: Can we add an additional smoothing step to smooth the plug-in estimator? Estimator. Yeah, there are lots of ways potentially that you could kind of fix this problem with the plugin. These doubly robust estimators I'll tell you about, I think, are very natural ways to do it. But there are potentially other ways. Yeah, the issue basically is it's not super straightforward to just smooth that plugin because you really need to take into account information about the treatment process, which the plugin doesn't do, but there may be a Which the plugin doesn't do, but there may be a way to smooth the plugin using that information. That's a great question. Thanks, Shu. Okay, so we've got a smooth function. So our propensity score is alpha smooth. Our regression function is beta smooth. The kate can have its own smoothness, which it has to be at least beta smooth mathematically, but it can be much more smooth than beta than the regression function. Okay, and so then from this paper, just to give you sort of the punchline, I'll show you two. I'll show you two rates of convergence for these two estimators. So here's a rate of convergence for the doubly robust estimator, this DR learner estimator. And in general, these rates tend to have kind of the same sort of flavor. So there's one piece right here, if you can see my mouse, that is an oracle rate. It's a rate that you would get in this model if you actually saw the potential outcomes and you could take their difference and regress them on covariates. This is just the rate for estimating a gamma-smooth regression. Estimating a gamma smooth regression. So that's going to be the best we could hope for, for sure. We're not going to be able to do better than that because even if we saw the potential outcomes, we would not be able to beat this rate. So this is an oracle rate if you saw the potential outcomes. And then here is sort of a rate that comes from having to estimate nuisance functions, which in this case are regressions and propensity scores. So we don't actually see these potential outcomes, of course. We just see covariance, treatment, and outcome. And so we need some way to sort of construct pseudo-outcomes that look like. Of construct pseudo-outcomes that look like these potential outcomes, and there's going to be some price that we pay for having to estimate these, which depends on how hard it is to estimate the propensity score and regression function. And in this, for this DR learner, you get a product of rates for estimating each. I'll talk more about this in just a bit. And then for the undersmoothed R learner, we get a faster rate. We get the same oracle rate here, which is the rate we would get if we saw the potential outcomes. And then we get some smaller contribution from having the estimate. Some smaller contribution from having to estimate the nuisance stuff. And so, this is the benefit of using this under-smoothing trick, which again, I'll tell you about in just a bit. Okay, so those are just the results for these two estimators. Now I'm going to tell you a little more detail about the two estimators. So, first I'll start with the DR learner, I'll move on to that undersmooth R learner, and then I'll tell you about the minimax piece from this most recent work. Next piece from this most recent work. Okay, so we've seen this plugin. We have some intuition now for why it's kind of deficient. It's not doing a good job capturing the structure in the category, it's really reducing this problem to make it as hard as regression estimation when it shouldn't be necessarily. So how do we exploit structure in the category? So in this problem, I think it can be very useful to take intuition from the average treatment effect problem. And the kind of link is that we know. kind of link is that you know we know a fair amount about how to estimate average treatment effects and we know something about optimality there and you can sort of think about conditional average treatment effects as just being average treatment effects in small bins at least in these smooth models and so for example we know that there are doubly robust methods for estimating average treatment effects which are optimal under some conditions minimax optimal for example and they roughly look like averages of pseudo outcomes that's one way to think about them Outcomes. That's one way to think about them. You know, pseudo outcomes that look like inverse-weighted residual terms, plus some regression prediction, imputation kind of thing. And so a natural kind of guess at a first method for estimating the k would be, okay, instead of averaging these things like I would do for an average statement effect for the k, which is a regression on x, I'll regress these pseudoba outcomes on the covariates. So that's the kind of intuition for this first DR learner estimator. And this actually Estimator. And this actually was proposed a while ago. The first place I saw it was in 2005 in a paper by Mark Vanderland. And so here's a few more specifics about the algorithm. So first we need to estimate these nuisance functions, which again are the propensity score and the two regression functions, the regression under treatment and under control. So we're going to use sample splitting here so that we can analyze this procedure and more generality than had been done before. So we're going to use part of our sample. Done before. So we're going to use part of our sample, or you can use folds, separate folds, to estimate these nuisance functions. And then in another fold or a test sample, we construct this, what I'm calling this pseudo-outcome, but it's really just the thing we average when we estimate the doubly robust, when we construct the doubly robust estimator of the average human effect. It's that inverse weighted residual term plus some plug-in kind of thing. So if you've seen a doubly robust estimator, it's just exactly what you average there. Just exactly what you average there. Double-robust estimate of the average street effect. But now instead of averaging, we're going to regress it on covariates. And in this 2020 paper, we analyze what happens when we don't make strong assumptions about the underlying methods we're using. So we want to try and make some general claims about how this works without committing to particular estimators. And so we've got this E hat n, which just denotes some generic regression procedure that takes in some outcome, which in this case is this. Takes in some outcome, which in this case is this estimated pseudo-outcome, w robust pseudo-outcom, and regresses it on covariates. So that's the DR learner procedure. Here's a picture. So we split our sample. This is our data set where we've got covariance treatment outcome. We split our sample into two, say. We use part of it to estimate nuisance functions, which are the propensity score and regression functions. And then we construct that pseudo pseudo-outcome in the other sample and regress it on covariates x. And you could use random forests or You know, random forests or whatever procedure you like here. Okay, so I actually have an update to this 2020 paper, which should be on archive tomorrow, I think, or tonight, maybe. So here's sort of a master theorem about how this works. So we assume some stability condition about the second stage regression procedure. So there's the, this is kind of a two-stage procedure where in the first stage, you're estimating the nuisance functions. In the second stage, you regress some combination of them onto covariates. Some combination of them onto covariates. So we're going to assume that second stage estimator satisfies some stability property, which I'll just refer to the paper. And we're going to give a result about how this estimator compares to an oracle that actually regresses something like the potential outcomes on X. And so we can think about the risk of that oracle procedure in terms of mean squared error. It's just the usual mean squared error, pointwise, mean squared error. All the results here are pointwise results. Point-wise results. And then this theorem says that under the stability condition on the second stage estimator with zero conditions on the first stage estimators, our DR learner procedure is very close to the oracle up to some bias term. And this bias term has some doubly robust form where it looks like a difference, a product of differences of errors in propensity score and regression estimation, and then some smaller order term. Term. Okay, so that's the general result. It doesn't commit to particular estimators, and it is telling us how we can make Kate estimation easier than regression estimation by getting this product of errors, very similar to what happens for average treatment effects, but now we're doing it for generic regression procedures, not just averages. Okay, so now we're going to dive in a little more detail and look at smooth functions, as I talked about before. So these are this classic infinite dimensional function space. This classic infinite dimensional function space that I mentioned before. Okay, in this case, you can sort of just plug in the kind of smoothness rates and get an interesting result. So we're going to assume that, again, the propensity score has some smoothness, alpha. It's estimated at the minimax rate. So for example, like local polynomials kinds of estimators here would attain that rate. The regression functions are beta smooth. The k is gamma smooth. We're going to assume that second stage estimator is minimax optimal. And then you get a rate that matches what I showed before. Rate that matches what I showed before. So you get the oracle rate here, which is the rate we will always have in some sense. It's the rate we'd get, even if we saw the potential outcomes. And then the second order contribution from having to estimate the nuisance functions, which looks like a product of errors in each of them. And so you can figure out, you know, you can back out what the conditions are for this nuisance contribution term to be smaller than the oracle, in which case you're actually achieving that oracle rate, and the condition looks Rate and the condition looks like something on the geometric mean of the smoothnesses. Okay, so here's a picture. This is just a specific setting where we're just illustrating results. So we're looking at 20-dimensional cogrates where the K-smoothness is twice the dimension. The x-axis here is the nuisance smoothness, which we're just thinking about as being the same here for simplicity for the propensity score and regression function. And then the y-axis is the mean squared error. The mean squared error. So, this gray dotted line here is the oracle rate. That's the rate we would get if we saw the potential outcomes. And of course, that's not going to change depending on nuisance smoothness because we just see the potential outcomes with this oracle. This is the rate for the plug-in estimator. It's larger, it's a slower rate, it's larger mean squared error, and it's just matching the rate that we get for estimating the regression function itself, which is a harder problem. It will only align with the oracle when. Will only align with the oracle when the k doesn't have any extra smoothness. This is the rate you get with the DR learner. And so you can see it actually hits the oracle rate at some non-negligible smoothness amount in this case and improves quite a bit over this plugin estimator. And then of course, you know, as you sort of have less and less smoothness to exploit, you get a worse rate. You get larger and larger mean squared error. Rate, you get larger and larger mean squared error. At some point, if the nuisance functions are just very hard to estimate, very non-smooth, we'll have very little hope to estimate anything at all. Okay. And maybe I should point out that the procedures I'm talking about today don't assume structure indicate. They exploit it when it's there, but they don't like they don't need that structure to be there. If the structure isn't there, then they'll just behave like plug-in estimators because that's the best. Behave like plug-in estimators because that's the best you can do, anyways. Okay, so this is showing how that DR learner procedure can adapt to the smoothness of the conditional effect. This is true even when the propensity score and regression functions are harder to estimate. We can back out these interesting conditions for when the oracle rate is achieved. And then, yeah, in the paper, there's a generic result for when you're doing general regression with estimated outcomes. So just point to there. Point two there. Okay, so we've got some conditions under which this DR learner is optimal. We know when it sort of hits that oracle rate. And so the question now is, you know, what if that condition fails? What if that second term, the contribution from having to estimate the nuisance stuff is actually, you know, not trivial? Then what happens? Is it the case that we could have achieved the Oracle rate with a better, different estimator, a more clever estimator? Clever estimator? Or is it the case that maybe that's just the best you can do in this kind of model? And so, in this second estimator, which I think I'll go through kind of quickly just to make sure I have time, we're going to pursue a different kind of estimator, which uses an under-smoothing trick. So this under smoothing idea has been used in parameter estimation problems before. And the idea is that you, when you have these underlying nuisance functions to estimate, you estimate them with too little bias. You estimate them with too little bias, and this gives you an increase in variance in the parameter estimation problems. And what happens here is that bias is really the issue in all of these problems. And so if you can drive down the bias in your nuisance estimation and you pay something with variance, you know, in parameter estimation problems, when you're estimating averages or in this case, like woke up averages, you're averaging out some of that extra variance that you get and it doesn't end up hurting you. And so you can actually improve estimators with this trick. Improve estimators with this trick. There are some practical difficulties, which I'm happy to talk about at some point. But so that error bound that we gave before, it involved a product of mean squared errors. So that tells us that at least with that bound, this undersmoothing trick won't help at all, right? Because if we drive down the bias, we're going to make the variance bigger and then our mean squared error is going to be larger. So we need a more nuanced bound. For this reason, we are going to actually switch to a For this reason, we are going to actually switch to a different estimator. It's called an R-learner. This R-learner comes from just like the DR-learner is sort of a conditional version of the W robust estimator for the average treatment effect, the R-learner is sort of like a conditional version of the W robust style estimator for a constant conditional treatment effect. So, this was first studied in the 80s by Robinson in this model where the K is. In this model, where the K8 is assumed to be constant. And this estimator, roughly what it looks like, is you do a linear regression of outcome residuals on treatment residuals. So you can see it also depends on estimating propensity scores and regression functions, but it combines them in a different way. So you just do a regression of residuals. And so we're going to do a conditional version of this. There have been several variants of this proposed before, but we're going to use a local point. But we're going to use a local polynomial version of this where instead of doing a single linear regression, we're going to interact these treatment residuals with a bunch of covariates and do a local kernel sort of regression. So yeah, there are some series versions proposed by Jamie Robbins in 2008. Neon and Vager had a nice paper looking at an RKHS kind of version, Viktor Chernuzipkov and Viktor Chernuzakov and others studied some lasso versions of this, but we're going to do a local polynomial one that works well in these smoothness classes. So I'm not going to go through tons of details here, but I'll just show you a brief picture about what this looks like. We actually use this triple sample splitting here. You'll see that as we move through the talk, the estimators become in some ways less general, more finely tuned to smoothness glasses. And so there's a trade-off there. We get better. So there's a trade-off there. We get better, faster rates, better performance, but in some sense, we're really tailoring ourselves to these smoothness classes at the same time. And the procedures become a little more delicate as we go. Okay, so we're going to do triple sample splitting. Now we estimate the propensity score in this first sample. In the second sample, we estimate the propensity score again, and we estimate the regression function. In this case, it's a marginal regression, not the ones separate for treatment and controls. Then we construct some kernel weights. Then we construct some kernel weights multiplied by residuals. We construct residuals from the second sample, both for the treatment and the outcome. And then we do a weighted least squares of outcome residuals on treatment residuals using a basis here interacted with those residuals and with these kernel weights that we constructed. To be a little careful here. And then what we show is that we get, I'm going to bypass details, but we get a faster rate by doing this kind of special. By doing this kind of specialized under-smoothing procedure, we can reduce the contribution from having to estimate nuisance functions from what we had before that looked like a product of usual non-parametric regression rates to something now that looks like n to the minus 2s over d. So again, we get this oracle term here for this undersmoothed R-learner, and then we get some smaller contribution now from having to estimate nuisance stuff. Okay, so we get weaker conditions. I'm going to just go. Weaker conditions, I'm going to just go to a picture. Okay, here's the same picture we had before. So there's the oracle rate, there's the plug-in estimator, there's the DR learner, and here's the R learner. And so we can see it's now hitting that Oracle rate under even weaker restrictions on the nuisance smoothness. So it's doing better than that DR learner. And then we have, again, some decay. You know, as smoothness gets smaller and smaller, the problem becomes harder and harder. And so at some point, we're just not going to be able to estimate things very well. Uh, things very well, and now we still so now we found this sort of clever, finely tuned estimator that does better than that generic, uh, you know, nice general DR learner procedure. But we have the same questions, right? Maybe we could have been more clever. How do we improve this R learner rate? Is it even possible to improve it? What's the best possible error to achieve here in this problem? And so, this is what we studied in this last, this most recent paper from this year. So, I'll spend the last Year. So I'll spend the last 15 minutes talking about this. So first we have to say what it means, what optimality means. And so a natural way to do this in non-parametric estimation problems is with the mini max rate. So this the mini max rate says, okay, look at an estimator and look at its error. In this case, the mean absolute error. The particular loss here doesn't really matter. And we're going to look at the worst case error over our model. And then we're going to try and find. And then we're going to try and find the estimator with the best possible error across all possible estimators. So, this Rn tells us the best possible rate of convergence we could hope to achieve uniformly over the model. So it's the best possible error, where by error, I mean the worst case error. And it's across all estimators. So it doesn't matter what estimator you're using here. Anything that looks at the data and spits out a number is. A number is involved in this inf. So these mini-max rates are well understood in lots of problems and in non-parametrics. I think there's a lot of work to do sort of figuring out what happens in causal inference problems. But for example, in smooth non-parametric regression, we know the rate is n to the minus one over two plus d over s. So the way to think about this is it's like the parametric rate, a root n rate, n to the minus one over two, but with a penalty that you pay for how big the dimension is relative to. For how big the dimension is relative to the smoothness. The rate for parameter estimation, for functional estimation, for smooth functionals, is faster than this. It's n to the minus one over one plus d over 4s. So you pay less of a price, and you can actually achieve root n rates for n functional estimation problems. And so this is a way of showing that in some sense, functional estimation is easier statistically than non-parametric regression. For sparse linear regression, we get S la. Linear regression, we get S log D over N kind of rates. Density estimation with measurement error is a hard problem, so we get slow rates that are logarithmic in the end. So these mini-max rates are really important. They are important both practically and theoretically. They give you a benchmark for the best possible performance. This is a nice practical use. We can know when our estimators are making the most of the data that they see. But they're also interesting theoretically in that they tell you precisely how hard. They tell you precisely how hard a statistical problem is. They tell you what the fundamental limits are to estimation. They allow you to say something about how difficult a problem is compared to another statistical problem. Okay, so the way these mini-max rates work is you try to derive a lower bound on that rate, which tells you that no estimator can do better than that lower bound. And then you try to construct an estimator that matches, that has error equal to that lower bound. And then you know that's also an upper bound and it's actually attaining the lower bound. And it's actually attaining the lower bound. So the lower bound is not, you know, it's actually meaningful. And so for deriving lower bounds, the game that you play is you try to construct, say, two distributions that are very similar. They're so similar that if you see samples from both, you can't distinguish the two apart. You can't say which samples are coming from which distribution. But you construct these distributions in a way so that even though they're very similar. In a way, so that even though they're very similar, sort of in general, the parameter that you're trying to estimate is as separated as it can be, or ideally as separated as it can be. And then this implies that no estimator can have an error that's actually smaller than that separation, because if there was such an estimator, then you'd be able to test between these distributions and distinguish where the samples are coming from. That's the main idea. This can be made formal. So you have to cleverly construct these distributions. So, you have to cleverly construct these distributions, which are close, typically close, but far apart in the parameter you're trying to estimate. And for functional estimation problems, it turns out you can't just pick two distributions. You have to pick lots of distributions and then put a prior over them, construct two different mixture distributions. Okay, so there are three ingredients here in driving these lower bounds. So, one is we need a pair of mixture distributions. So, that means we, again, we need to. So that means we, again, we pick a bunch of distributions that are indexed by something, and then we put a prior over that index. We need to bound the distance between their infold products. Infold product just means I sample my index from the prior, and then I generate a sample of in observations from that particular distribution. And we want that distance again to be small. We want our distributions to be very close so that they're not distinguishable, so that we know that the separation in the parameters. In the parameters is actually the minimax rate. And so that's the third ingredient that we want to try and make the parameter separated. Here's a limma from Sibakov that just formalizes this. So we've got P lambda and Q lambda. These are our lambdas, the index, and these are a bunch of distributions in our model. And then we put a prior over these lambdas. And then we want to bound the Hellinger distance between these two mixtures away. Away from two. And then we want the parameters, for example, under each of these p lambdas and q lambdas to be separated, say by some at least some s. And then if you can show that the distance is bounded in this way and the parameters are separated in this way, then you get a finite sample lower bound on the mini-max rate. And so the trick is to try to construct a hard distribution, hard in the sense that you can't test between them, but the parameters. The but the parameters as far apart as possible and sort of strategically you know put bumps places and control their size so that you can make the distance small but the separation large to get a meaningful bound. And so in all these non-parametric smoothness models, generally the distributions all involve sticking bumps, places, very high level. And so I'll just show you what we did to make this work in this case. And there's something interesting that happened. And there's something interesting that happens, which is our construction here for the minimax lower bound is sort of a combination of constructions for non-parametric regression and for functional estimation. So for non-parametric regression, the way to derive these minimax lower bounds that I told you about before are basically to, so under one distribution, you say your regression function is constant, say, and then for the other distribution, you stick a bump somewhere. So we're going to do something like that with the case. So, we're going to do something like that with the Kate, and then we're also going to combine this functional estimation construction. The functional estimation construction is the one that uses mixture distributions, and there you basically have to put bumps on the nuisance functions in careful ways, where the bumps are either going, say, up or down. And so, up or down is the index, and you put a prior over these directions. And so, we're going to mix these two. So, okay, here's the picture of what these. A picture of what these distributions look like. So here, the black line is the k in this construction. And so under the p lambda distribution, so called a null distribution, we put a bump on the k centered at this x0 point. So it's a small bump. And it's actually important in the calculations that it has this flat top kind of structure. And then we partition the space within this bump into bins, into k-bins. Into bins, into k bins. And then within each bin, we put a bump on, in this case, the regression function. That's what this red line is. And the bumps either going up or down. And that's the, we put a prior over these, the direction of the bumps again in each bin. So it's just a random choice whether it's going up or down. And then, so for this null, we keep the propensity score just constant. So that's now this is. Score just constant. So that's now this is our distribution. We also have to do something careful with the covariate density, which I'm going to not worry about in the talk. Okay, so that's one distribution. Now we can write out what the density is for this distribution. For the other, we want to make one that's close as possible, but that makes the categ separated. So here we're not going to put a bump on the category. We're going to have the cate just equal to zero. So the category separated by whatever the height of that bump is. And we're going to do the same part. bump is. And we're going to do the same partitioning locally around the point x0. And we're going to put bumps again on the regression function, but also on the propensity score here. So that's the second kind of mixture distribution. And now we need to bound the Hellinger distance between these things. In general, this can be a pain. It can be very complicated. There's a nice lemma, which I'll point you to from this nice paper by Jamie and Lingling Li and Eric Chekinchuk and Odd Vandervaard from 2009, where they give a result that. But they give a result that helps you bound these Hellener distances for mixtures in a relatively simple way. And so you can derive a bound here that now will depend on the size of the bumps that you're using. And you can finely tune the size of these bumps so that you get a Hellinger distance that's not too big between these mixtures. And so if you do this, you get the following mini-max rate. So you carefully pick these, the size of the Know carefully pick these the size of the bumps and then you figure out what the separation is in the gate. Okay, so here's what the mini-max rate looks like. So we're going to think again about the model where we've got some conditions on the covariate density, which I won't go into detail about. Again, the propensity score is alpha smooth. The regression function is beta smooth. And in this case, the propensity score has to be at least as smooth as the regression function. It turns out things get complicated in the other setting. And then the kate has its own smoothness. And the minimax rate looks like this. And the minimax rate looks like this. Okay, so here's the minimax rate on the left-hand side. S is the average smoothness of the regression and propensity score. And there are two regimes. So one regime is if you're in the oracle kind of setting, you get the oracle rate. And the condition for getting that oracle rate actually matches what the condition is for that undersmooth R learner. So that verifies that that undersmooth R learner is actually doing as well as possible when it hits the oracle rate. Possible when it hits the oracle rate. But in the regime where we're not able to achieve that oracle rate, then the minimax rate here is faster than what's achieved by the 100-screen-R learner. So you can actually do better. And the rate here is very interesting. It's a mixture of functional estimation rates in blue and non-parametric regression rates here. So let me just talk about this. So. Also, Eddie has three minutes. Great. Okay. Sounds good. Thanks. Great. Okay. Sounds good. Thanks. Yeah. So the way to think about this rate is that it's a mixture between functional estimation or parameter estimation rates and non-parametric regression rates. This is showing how this k quantity is really this sort of weird hybrid creature that somewhere lives in this world that's between regression estimation and parameter estimation. And so you can see this precisely because the non-parametric regression rate scales with. Non-parametric regression rate scales with the dimension over the smoothness or twice the smoothness. The functional estimation rate scales with dimension over four times the smoothness. And then our minimax rate scales with the sum of these two things. So you can really see how it's a hybrid in this sense. And this is something that, you know, kind of a cool fact that you get from this minimax rate. So what happens here is that at one extreme, when you have lots of smoothness in the gate, think of gamma as being like infinity, then you basically are getting functional estimation. Then you basically are getting functional estimation rates, and it's as easy to estimate the Kate as it is a parameter, like an average treatment effect. At the other extreme, when the Kate doesn't have any extra smoothness, you're just getting non-parametric regression rates. Again, this formalizes how when the cate doesn't have any extra structure to exploit, this problem is just as hard as non-parametric regression based on the nuisance smoothness. Here's one more picture, which I'll Here's one more picture, which I'll probably finish up with. And this is just showing, again, the minimax rate as a function of the cake smoothness. So, this red color, dark red means we have more smoothness, and the yellowish means we have less smoothness in the cake. So, when this dark red regime, this is like saying the cake's very smooth, and we're getting close to parameter estimation rates. These are, you know, we're getting close to root n rates. We can never get root n rates exactly here unless you take gamma to be like an infinite. And you see this elbow. And you see this elbow phenomenon where at some point the oracle rate is not achievable and then the rate decays in a particular way. And where this elbow occurs depends on how smooth the k it is. So this is an interesting phenomenon that doesn't come up in standard functional estimation where the elbow changes in this way. And I won't have time to tell you about the details, but in the paper we construct an estimator that achieves this bound. And it looks like a variant of a higher order influence function type estimator. function type estimator looks like a use statistic basically version of the r learner procedure i told you about before and it's uh yeah interesting estimator i'll just point you to the paper there for details and let me just finish up here i'll show you the rate um again it's a plug-in estimator the dr learner that undersmoothed r learner and then here's the mini max rate which we show is attained with this higher order r learner procedure use statistic guide Use statistic guy. Okay, so yeah, again, lots of methods have been developed here in the space recently, but I think many, you know, the operating characteristics are not very well understood, and it's unclear how we should benchmark them, you know, what optimality means. And so here we wanted to provide some more flexible estimators that give faster rates, stronger guarantees on the error, and some resolution of this minimax optimality story. There are still some open pieces here. The role of the co-variation. The role of the covariate density: what happens when the regression function is actually smoother? What happens in other function classes, of course, is a big one. So, lots of cool things to work on here. And yeah, both practically and theoretically, there's still lots to do. So, here are some links to the paper. Again, the second one was with Siva and Larry to work on. So, I'll stop there. Thanks a lot.