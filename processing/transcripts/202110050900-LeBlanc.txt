Perfect. All right. All right. We're going to count till we're getting recorded. All right. So, hello, everyone. Thank you for coming. What I'm going to do in this talk is I'm just going to cover some brief introductory theory to recommender systems. We think recommender systems are important, but they're not particularly well known, at least in academia. So this is an introduction to those. And I am stuck on my title screen. And I am stuck on my title screen. Here we go. So, first of all, what is a recommender system? And David kind of covered this in his talk yesterday. But we've all encountered the problem where, you know, we're sitting and we're scrolling through Netflix and you're scrolling through like dozens or hundreds of movies and you can't find anything to watch. You know, in some sense, this is kind of silly, right? Because Netflix has thousands of titles. There ought to be something somewhere you can find which is enjoyable. But the problem. But the problem, which this scenario illustrates, is that when there's so many movies and you only like a subset of them, it can be really, really hard to find that particular subset of movies that you'll enjoy. And it's even harder to like find the most enjoyable movie in that subset. And this is kind of like an information overload problem. The same situation arises in plenty of other spots. Like Amazon wants to sell you stuff, but they have thousands, hundreds of thousands, millions, I don't know, lots of items. Anytime someone wants to recommend you something, Anytime someone wants to recommend you something, this is where a recommender system comes in. Solve the information overload problem by recommending useful items to users. So that's what a recommender system is. And then what I'm going to cover in this talk is, as I mentioned earlier, an introduction to recommender systems as they exist in the academic literature. So it's not comprehensive. I mean, we only have like 25 minutes. So I'll cover, you know, some of the main sorts of filtering algorithms, which existed at least early on in academia. Algorithms, which existed at least early on in academia. And I will note that we're not covering state-of-the-art methods for industry, at least not necessarily, because we just don't know what they are. So if someone here does, feel free to chime in at any point. So historically, recommender systems, we started seeing papers about these show up in the 1990s, roughly paralleling the growth of the internet. Because there's lots of stuff out there. People need to recommend things to each other. And then I'll cover the approaches up until about 2010 or so. Up until about 2010 or so. So, as I might have mentioned, we have three main approaches, which will be intuitive and easier to understand than some of the more recent ones. And one of the reasons I'm cutting off at about this point is that, well, A, it fits a 25-minute talk nicely. And B, recommender systems research really kind of exploded exponentially. We went from like dozens to hundreds to thousands of papers per year after about 2010 or 2015 or so. And there's a ton of new methods to explore there. And it's all very interesting. But there's also a bit of a reproducibility. But there's also a bit of a reproducibility crisis in the literature. So if you read a lot of academic papers or conference journals on this, you'll also note that like some of them don't necessarily look reliable. There's lots of stuff from somewhat obscure institutions and journals, which isn't necessarily a knock against them, but does raise some flags. And in addition to that, we have had some researchers go through and find evidence of reproducibility crisis. So extra et al. 2011. Just for Just found that it was difficult to reproduce work, whether it was because some algorithms which were proposed were proposed only in some sort of like analytic form and there was no code given. So if you ever want to reproduce it, you have to go code it yourself. And that just introduces new challenges and you may or may not do the same thing which the paper authors did. And also that a lot of new methods when they're proposed are just people just aren't doing good research practices. They're just not comparing the proposed methods to state of the art algorithms or some cases. Methods to state-of-the-art algorithms, or in some cases, they don't even have a comparison at all. So, you have to be a little careful. And this paper was from like 10 years ago, so I found a couple more recent ones that shows that the issues seem to persist because this group analyzed a bunch of different more recent recommender system approaches, which used various machine learning algorithms. I think they actually focused on neural nets. So, they found that a lot of them could not be reproduced, and a bunch of the ones which could be reproduced could be outperformed by some of the older methods. Performed by some of the older methods I'll cover in this talk, which in some cases are even older than I am. So, whenever you're reading through literature, you just need to have one eye open. Okay. So with that out of the way, we're just going to jump into things. So more formally, we're going to define a recommender system. There's some space of users, there's some space of items, and then we have a utility function on the product user item space, which takes a given user item pair and models utility that that user receives. Utility that that user receives from that item. In particular, our goal is to find items which maximize that user's utility, and then we can recommend those to users, whether it be movies and Netflix or items in Amazon or what have you. Now, we need to determine the form of this utility function. This is where the statistical part of the problem comes in. And in order to do this, we need to collect feedback from users. So there's a couple different types of feedback which are collected. One of them is in-person. Which are collected. One of them is implicit feedback, which is kind of what you get when you implicitly observe user behavior. So if we're Netflix, then Netflix might watch you, like might record which trailers you watch, which TV shows you click on. If you watch a movie but don't rate it, or if you like watch a TV show but bail after the first episode and Netflix records that, that's kind of implicit feedback. Explicit feedback, on the other hand, is when the user just directly tells you what they think, either because you asked them to or they just provided it. So, you know, thumbs up. It so, um, you know, thumbs up, thumbs down on Netflix, uh, one to five star rating on Amazon, anything like that. Um, for most of the rest of the talk, I'll be assuming that we're working with explicit feedback in the form of ratings, though I will note that if you're clever, you can kind of come up with mappings between implicit and explicit feedback. And then the fundamental problem that we have with this ratings data, though, is that any given user is only going to rate a small subset of items. Because it'd be nice if they just rated everything. We could recommend them good things. But that's impossible, right? Because like an NFL. But that's impossible, right? Because like Netflix has thousands of movies. I personally have seen at most like 100, 200, and I've probably only rated a subset of that. So, right away, we have a problem there, which we're trying to fill in. So, a little more explicitly, this is like an example ratings matrix we might work with, where rows correspond to users, columns correspond to items, in this case, movies, and excuse me. And users give ratings. Users give ratings to items, and that's the information that's filled in here. Some item user pairs are unrated, and that's the blocks. So, ratings matrix is very important. I'll note that there's also other forms of data you can have here, like you can have demographic information, users, which you either asked or you bought from Facebook or something. You can have information characterizing each item, usually in the form of some sort of content vector. And then, because the next talk is about cross-domain recommender systems, I will note that they exist and give you something to look forward to. Okay, so I was like. So I've alluded to the three sorts of recommender systems I'll be covering here. So I'll go over those now. So there's collaborative filtering systems, which make use only of this ratings matrix. They try to use the filled in spots of this rating matrix to fill in the empty spots. There's content-based filtering systems, and they use the content description of items to predict ratings. And then we'll also cover hybrid filterings, which is kind of just a catch-all term for combining other types of recommender systems together to improve performance. Together to improve performance. So, first, we'll go over collaborative filtering. So, as I mentioned, they use only the ratings matrix. And the intuition here is that you're trying to find users with similar tastes. So, if Alice and Bob have watched the same movies and given them the same ratings, then they have similar tastes in movies. And if it turns out that Alice has watched a movie that Bob hasn't, Alice's rating on that movie is probably a good predictor of how Bob will rate that movie as well. Rate that movie as well. It's like, if you know, if you have friends in real life and you're going for movie recommendations, you might ask someone who likes the same movies you do what they think about a particular movie. It's kind of the same approach here. Further classifying things, there's memory-based methods, model-based methods. First, I'll just go over memory-based methods. These are kind of more like. Sorry, is there a question? Yes, if you don't mind. I can often imagine that, you know. I often imagine that Alice may prefer movies in the same way that Bob does, but she tends to be really generous and give the movie she likes a five, whereas Bob is much more reserved and he rarely gives a five. He tends to go with a four. Is there any way that we can try and adjust the scale to account for how stringent a rater might be? Yes, yes, there are ways of doing it. Yes, yes, there are ways of doing that that is doable. So I will cover, I guess, probabilistic matrix factorization a little bit on, which kind of does that with latent, like user-specific features. I'm less clear off the top of my head if there are memory-based methods which do that explicitly. Okay. Thank you. Yes, that is something which can be done and probably ought to be done, to be honest. Okay, so where was I? So memory-based methods are heuristic. So memory-based methods are heuristics which use the entire ratings matrix or the entire filled in part of the ratings matrix to predict user item ratings on the empty sparts. And they, I'll just note that they use all of the ratings generated before a user interaction. So if David and I are using Netflix at the same time and David rates something, the Netflix, if they're using memory-based collaborative filtering, I should mention, they're probably not doing this exclusively, has to rerun a memory-based method when anytime anyone else. Method when anytime anyone else gives ratings data. So that can be very memory intensive. And you know, there's ways around that if you're really clever, but that's the theory. So here's a specific implementation of a memory-based collaborative filtering method. We have a list of users, we have a list of items, we have a ratings matrix where the users correspond to rows, items correspond to columns, entries are ratings, and we're just going to assume that a rating is zero if an item is unrated. Unrated. And then for each user, which corresponds to a row vector, we can basically just pull out the row vector of this ratings matrix, some subset of the entries, and that row vector will be rated. And that vector can kind of characterize the user. So then our goal is to use a similarity function to find the similarities between users. And there's a lot of similarities functions. It is a source, an open question in some sense, which is the best similarity functions to use. I've just thrown Functions to use. I've just thrown up cosine similarity here. So that's one. And then given that we have similarities between all of the users, you can do something like find for a given active user who you're trying to give a recommendation to, you can find the k nearest users to that specific user and then average over this neighborhood in order to predict that user's ratings on a given item. And in fact, that was a specific memory-based collaborative filtering, but a lot of Memory-based collaborative filtering, but a lot of memory-based collaborative filtering tend to have similar outlines where you use some sort of similarity measure to group things together, then use an aggregation strategy, whether it be just an unweighted average, a weighted sum, or something more complicated to attain weighting predictions. And then you can transform ratings, predictions into recommendation lists, however you see fit. The easiest way is just, you know, grab the top highest predictive ratings. Okay. So that was memory-based collaborative filter. So, that was memory-based collaborative filtering. There's also model-based collaborative filtering. So, as you might suppose, based off the name, model-based methods use the ratings matrix to learn a model, which they then use to predict unknown ratings. So, this has some advantages. One is that you can be computationally more efficient because once you've trained your model, the model is trained, and you don't necessarily need to rerun it every time someone gives you new ratings. So, if you've got a bunch more ratings at some point, you should rerun your model. And you can also. And you can also explicitly capture underlying structure, which you think might exist. And you can design models that will be better able to handle sparsity than, let's say, a memory-based method will. So model-based collaborative filtering tends to be a little more specific and less amenable to like, here's a general outline describing all of them. So I'll just cover one particular relatively popular instance, and this is probabilistic matrix factorization. This was developed for use in the Netflix Prize data set back in 2008. And what it does is it And what it does is, so you have a big ratings matrix, and we factor this ratings matrix into two smaller dimensional matrices. One encodes latent user factors, and one encodes latent item factors. And then the rating of a particular user item pair is just a sum function of the user's latent factors and the item's latent factors. Other methods exist, but this is the one we're covering here. So here it is in a little more detail. You can make this a little more complicated if you prefer. This is about the simplest possible implementation of it. About the simplest possible implementation of it. So we have a normal, everything's normal. Excuse me. We model the ratings data as normal. The mean is a function of user and item latent features. We play spherical Gaussian priorities on those. And then for a given user item pair, which is unobserved, we can predict the ratings using that user's and that item's latent features. This works pretty well. There's a whole class of models based off of it. It's a good thing to know. It's a good thing to know, okay? Um, so collaborative filtering does have some challenges specific to it, and I'll briefly note that um the cold start problem where you have new users and new items is a thing, but I'll cover that later, so don't ask me questions about that quite yet. But challenges which can be specific to collaborative filtering, uh the first two of these are more famous. So data sparsity is a thing, right? And like the intuition here is that, you know, collaborative filtering works by using the filled in parts of a ratings matrix to fill in the empty parts, but the more the empty parts, Parts, but the more the empty parts dominate the ratings matrix, the harder it is to fill them in. And also, scalability can be an issue just in terms of implementation, how long it takes to run your models. Memory-based models, as I mentioned, can be very intensive because you have to rerun them every time. But like in general, you can be working with ratings matrix that are really, really huge. Netflix, for instance, probably has thousands of movies and I don't know, hundreds of millions of users. So you can imagine doing collaborative filtering on something like that is quite intensive. And there's also something which There's also something which I found just trolling around the literature called chilling attacks, which you can kind of think of as like someone coming in and just review bombing a bunch of particular movies. It's less an area of open research and more just a problem that I thought was cool, but there's probably something here to do if you're interested. Okay, so that's collaborative filtering models. In addition, the other sort of famous earliest class of recommender systems, which are still used today as our collaborative filtering, is content-based models. Filtering is content-based models. So these models explicitly question before we move on to the content-based model. Sure thing. I can certainly imagine that you and I might enjoy the same action-adventure movies, but have very different tastes about Westerns. Is that going to be something that can be identified in the collaborative filtering method? Methods? So if you use purely collaborative filtering methods, then you only have the ratings matrix. So you only have a group of items and a group of users, and you have the ratings given to each. So like you're not really able to distinguish between, let's say, movies based off of genre explicitly. Now, if you do something like probabilistic matrix factorization, which postulates latent factors and it works correctly, some of the latent factors might correspond to, oh, this is a Western, this is. This is a Western. This is a. I've forgotten what the other cost of movies you mentioned, but whatever that was. So, stuff like that can happen. But it's something that you would be learning and it's not necessarily guaranteed. Okay. I think that we could use this latent factor model to come up with a representation for me as sharing your taste in action adventures and not sharing your taste in westerns. That's Not sharing your taste in westerns. That's that I think that would be doable. Yes. So a model I'll cover later kind of does that, but in a slightly different field. Okay. Using just the ratings matrix, that ratings matrix is generated by several different mechanisms. If I don't rate a movie, it probably means that I don't like it. It's a genre I don't like. It's something I don't follow. So there's a lot of information in the novel. Follow. So, there's a lot of information in the non-ratings that should be sort of negatively inclined. Similarly, for your example there, it's a nice example, but everybody's seen Casablanca, everybody's seen Dirty Howe, everybody has seen The Godfather. To some extent, we all agree that those are good movies, and there's very little information content in stuff that everybody likes. So, I'm not sure that all the issues in the rating matrix should be weighted equally. Ready matrix should be weighted equally. Yes. Yes, yes. So those are to your later point first, which is that different movies are more or less informative. Yes, that is true. And if you're working in, let's say, the cold start problem where you're trying to onboard users as quickly as possible into one of these systems, generally that'll happen by like asking the user to rate a bunch of movies in sequence. And like popularity versus, I guess, controversy of a movie is something you have to take into account when doing that. Something you have to take into account when doing that. The cold start problem is going to be tricky, though, because you can't guarantee that the set of movies that we've all seen will always get high ratings. We want to ask about sort of obscure movies that, you know, might correspond to particular tastes of individuals. Yes, yes. There is definitely a trade-off between popularity and like the probability that a user can rate a given item, as well as how much information that item is giving you. And threading that balance is kind. Giving you and threading that balance is kind of something that's hard to do, but is needs to be done. And then, towards your first point, where the missingness is informative, I would tend to agree, though, I don't recall having seen any methods which make use of that. It could just be something that I've missed. There's always that possibility. You've done a fantastic literature review, I know. And so I don't think you missed much. Well, thank you for that vote of confidence. At least one of us is confident in me. At least one of us is confident in me. All right, so where was I? Content-based models, yes. Okay, and I guess the 25-minute time limit might be shot at this point, but we'll just carry on. No question. So we're going to, so content-based models explicitly make use of the fact that items have properties. Like as David was saying, you know, we have Westerns on one end and we have sci-fi movies on the other. And people's tastes tend to be consistent. So content-based models. To be consistent, so content-based models work off the assumption that a user will have similar ratings for two items if the content vectors are similar. So, like if you like Star Wars, you probably like Star Trek, if you like Lord of the Rings, you probably like Game of Thrones, stuff like that. I mean, that's not always necessarily true. You have to control for like quality and stuff, but the underlying assumption is logical and it's intuitive and it works. So, there's three main components to content-based filtering algorithms. One is you have to generate the item profiles for each item. This can be really Each item. This can be really tricky to do and really hard to do. Two is you need to be able to compare the item profiles with user preferences, whether it's some sort of you can build a model here or something that's commonly done. And then three, based off of that, you have to be able to recommend items based off of user preferences. And before we go on, I will just note that, so as I said for model-based collaborative filtering, that tends to be rather particular and not very well generalizable. That's even more true for content-based models because it goes. True for content-based models because a good content-based model will be tailored to a specific area of application. So if I build a content-based model for Netflix and movie recommendation, then I'll be making use of things like genres and runtime and script writers and actors. And if I take that same model without modifying it and apply it to like toilet plunger recommendation, we'd be very surprised if it worked. So things can be very particular. And whenever you move between recommendation problems, you kind of need to ask yourself, what's relevant? How do I do this? Do this. So, first of all, we need to extract item profiles. So, it's not necessarily trivial. Sometimes, if you're lucky, you're working in a field where you already have item profiles. So, like if you're recommending academic articles to people, then academic articles tend to come with a certain set of keywords provided by the author providing some sort of description of it. And that can serve as your content-based vector. And that is, in some sense, at least a A characterization. But this can be unreliable because you're relying on other people's work most of the time, which may be of good or bad quality. And as well, it's sometimes not always easy to use these given profiles. Like if you're working with art or picture recommendation and you have pixel data characterizing a picture, that can just be really hard to work with. It's very high-dimensional. It's not necessarily clear how you're going to take that to a rating. So those are easy. So, those are easy to use if they exist, but there can also be problems. And then I'll briefly mention, if only to show why you shouldn't do it, is that you can manually define content vectors for each item. But this can be very labor-intensive and it'd be very inaccurate, and I really wouldn't recommend it. So just don't do that, I guess. But this goes to show how hard it is to do these sort of things, because like the thought of doing it by hand is just terrifying. And then something which is relatively common in the academic literature, a lot of content-based models are built around settings. Are built around settings where each item is characterized by a block of text. And you use some sort of, I don't know, text mining or a topic modeling algorithm to build some sort of latent features like a TF-IDF scheme, a latent direction allocation or things which are commonly used to characterize blocks of text, which then characterize items. That has the advantage of being of, you know, easy to do because you don't easy to do because you don't have to go and do it yourself. So assuming that we've been able to generate our content vectors. That we've been able to generate our content vectors, we then need to recover the user profile. So, the first possible construction I'm mentioning here, I'm not going to spend a lot of time on it because I think it's something that tended to be done more frequently a while ago, and it's not as much in vogue now. And basically, this is just to treat a user vector for each user in the same space as your content vectors. And like each attribute in the user profile is kind of like a weight showing how much interest that user has in the corresponding. That user has and the corresponding entry in the item profile. And then you can do memory-based methods to sort of match things together and recommend items. I haven't really seen anything using that recently. So it's good to know that it exists. But most of the ball game is in user profiles as instances and models. So like the simplest way you could do this is for a given user, you have a bunch of ratings data, and each rating comes with an associated covariate vector, which is just the content vector characterizing the items. Content vector characterizing the items, you could just build like a normal linear regression model, for instance. It's probably the easiest thing you could do characterizing that. And then for a new item, you just feed a new item into your linear model, and then that'll output ratings, predictions everywhere. That being said, that is like the easiest thing you could do or the first try. There's lots of ways you can refine this. For instance, machine learning algorithms in the last five years or so have been very in vogue and these sort of things. So as I mentioned earlier, I guess you need to be careful to make sure your works. Guess, I guess you need to be careful to make sure your work's reproducible when you're doing it. Okay, so here's an example of a specific content-based model and how it was implemented. So Wong and all in 2018, they proposed a content-based model to help computer scientists decide which journal or conference to submit their manuscripts to based off of the abstracts. So the goal here is to recommend manuscripts to publishers. So for feature selection, they use the TF-IDF scheme to characterize abstracts on a train. To characterize abstracts on a training set in terms of important keywords. And if you don't know what the TF-IDF scheme is, it's just a way of picking out important words out of a text document. But the TF-IDF scheme resulted in something that was very, very high-dimensional and too high-dimensional for them to really use. So they decided to use chi-square tests to select the most important keywords for each abstract and just went along with this reduced list of keywords as the content-based vectors. So that was how they did the feature slide. So that was how they did the feature selection. Then, in order to build their, I guess here we call it a journal or a conference profile. I see we have a question in chat, but I'll just address that later, I guess. Just a comment, we can save it for the end. All right, cool. Sounds great. Okay. Yes. So they used a softmax regression with the features of structure variables and the journal conferences as a response in order to sort abstracts into. Abstracts into publications based off the keywords. And they claim to achieve a 61-point-something percent accuracy in matching manuscripts with publications, which, on the one hand, sounds really impressive, but on the other hand, there was no baseline comparison. Though in fairness to them, I don't know what a good baseline comparison would be for this particular setting. But I guess that just goes to show you that the reproducibility crisis is a thing, even in kind of cool applications. Okay. So I just skipped a slide. So, I just skipped a slide. So, content-based models also face specific challenges. A lot of these have to do with the fact that it's just really hard to build content profiles. It's hard to get them. It can be hard to work with them when they exist. And then you need to be worried about how discriminating they are, whether or not they're informative enough to truly discriminate between items. So, there's a whole bunch of hullabaloo which has to go on there, which is really, can be really hard to work with and is always a good area for future research. For future research. On the more model-based side of things, on the recommender system side of things, I guess I should say, instead of the future extraction side. Content-based models, yes, David? I think people organize their ideas in different ways. So the content isn't sort of well-defined. There are people who watch any movie that has Johnny Depp in it. There are people who will watch any movie that Alfred Hitchcock directed. Those are very different ways of trying to organize. Ways of trying to organize one's taste, and it's not clear how the content-based model can address that. Am I missing something? Yes. So, in that case, so for movies, you might have this, I've seen some applications where you have this like super long content vector, which is just a bunch of ones and zeros, where like an entry corresponding to Johnny Depp will be a one. If Johnny Depp is in it, it'll be zero otherwise. So, that's a way of encoding things like actors and producers and whatnot. Actors and producers, and whatnot. And then you can build a model which will take that vector to the ratings data. I mean, you can build that, you can build separate models for each user. You can kind of pool your information together. But if you're building separate models for each user, let's say, because that's easier to think about, then you would hope that, let's say you just decide to do linear regression on it. You would hope that the linear regression in that case, for me, might find that Johnny Depp is very important, but for you, you might find that Steven Spielberg is very important and vice versa. Is very important. And vice versa is not the case. Thank you. That's helpful. Yes. But that actually kind of does speak to a problem that they run into, which is over specialization, which is they can get stuck. Content-based models can get stuck in content space. So if I watch a bunch of Johnny Depp movies and I give them all high ratings, and I watch a bunch of movies which don't have Johnny Depp in them, and I give them middling ratings, the content-based model, there's a risk that it will decide that I really like Johnny Depp and don't like anything else, and then it will forever. Don't like anything else, and then it will forever only recommend Johnny Depp movies to me. So, we need to be careful that we're not just like finding a local or global maximum or something in content space and say that this is the only thing that's worth recommending. Cool. So we've gone over collaborative filtering algorithms and content-based filtering algorithms. So they each have their strengths and weaknesses. And a fairly intuitive idea is just to kind of duct tape them together so we get most of the strengths and not much of the weaknesses. Strengths and not much of the weaknesses. And this main intuition is called hybrid filtering algorithms or hybrid-based models, which combines different types of recommender systems to improve performance. This can go beyond just the couple I've mentioned here. I mean, there's a whole bunch of other recommender systems. You might have like demographic filtering is a thing, time filtering is a thing, spatial filtering is a thing, cross-domain systems are a thing, context-where, I'm not going to list them all out. There's a bunch of other information you can include. And combining multiple approaches into one is generally. Combining multiple approaches into one is generally pretty powerful and a good idea. So, there have been two main types of hybrid-based models, one of which is an ensemble, which blends together the results of many different recommender systems. You can kind of think of it as like model averaging, where we have, I don't know, I guess the easiest way of doing it is we have five recommendation systems and we'll have an unweighted average of them. And then that's our new recommendation. You can improve upon that, but that's the simplest one. And the other option is to build. And the other option is to build a singular model for a specific problem which explicitly uses multiple types of recommender systems. So, first, we'll cover ensemble methods. So, I've mentioned the Netflix prize data before. So it was an open competition between 2006 and 2009 where Netflix was like, we're going to give a million dollars to whoever does the best job on our data set according to root mean square error. And to demonstrate the power of ensembles, this competition was won by a group out of AT ‚Åá T, I believe, which used an ensemble method. Which used an ensemble method, which blended together 107 different types of recommender systems, which included a bunch of things, some of which I've covered, like key nearest neighbor models or factorization models, and most of which I haven't because there's 107 of them. So this was a huge ensemble, and they actually managed to outperform Netflix's algorithm, I believe, by something like 10%. So this goes to show the power of this approach. By taking a bunch of different recommender systems and combining the results, you can do much, much better than you can with any one recommender system. Any one recommender system. The other way of doing hybrid filtering is just building a specific model. Again, these can be kind of particular, so I'm just going to go over one. And this is collaborative topic regression. And the goal is to recommend scientific articles to researchers who are doing a literature review or in the process of writing a paper and need to find something. And I've just realized it's an interesting immersion feature in my talk that I have so many recommender systems and articles, whatever that says about me. Whatever that says about me. But for this, we have ratings data. Heroes in the form of like/dislike, whether the researcher liked or didn't like an article. And you have blocks of text characterizing each article. I forget if they used abstracts or if they used the full text. And I guess for our purposes, it's not too important. So the idea behind this is to use a probabilistic matrix factorization to model the ratings matrix as a function of user and item latent features. But because But because they were working in a setting where they had blocks of text corresponding to each item, they thought they could do better than just learning the item latent features without any structure. And so they used latent, while fitting the probabilistic matrix factorization, they simultaneously fit a latent Dirichlet allocation algorithm on the text documents, which and that out of that pops for each document, among other things, a vector of topics in that document. So that vector of topics per document That vector of topics per document was used as the item latent vectors in the probabilistic matrix factorization model. So, this combined a collaborative filtering approach with the PMF and a content-based model where they extracted item profiles. And this provided an interpretable latent structure and also just outperformed either a pure collaborative filtering or a pure content-based model or filtering algorithm in the same setting. So, as I alluded to earlier, all of these recommender systems face the cold start problem. These recommender systems face the cold start problem. We kind of briefly addressed this earlier, but the cold start problem is when you have something new. So, the item cold start problem is when you have a new item. Netflix has found a new movie. Amazon has found a new, whatever Amazon sells, a new book, I guess. And we need to figure out what to do with it. Content-based filtering algorithms are kind of okay in this setting, because as long as we have item characterizations, we can make recommendations. But collaborative filtering algorithm is running the problem. Algorithm is running the problems. Because we have no data for new items, it's hard for a collaborative filtering algorithm to recommend it. And one possible solution is just kind of force recommend it to a certain group of users. In addition to the new item problem, we also have the new user problem, which is also very problematic. It describes a situation where, you know, we have a new user joining our system. I'm using Netflix for the first time. They have absolutely no idea what I do or do not like. And so this is a problem because if you're running a business, Because if you're running a business and you're trying to sell people things or get them to watch things and your recommender system is really terrible until you have a whole bunch of data on them, they're not going to be willing to put up with that. So you're trying to get them onboarded into your system with as little effort as possible on their part. So there's a couple different approaches you can do this. One of the more popular ones is just finding the most efficient sequence of questions to ask the user to give accurate recommendations. And this can be in the form of, I don't know, what do you think about the godfather? What do you think about the god? I don't know. What do you think about The Godfather? What do you think about Star Wars? And the user says yes or no, or I've never heard of that movie before. And you keep going on. I think I read a paper saying that between 10 and 20 of those sorts of questions is most optimal. In addition, I've also seen some approaches which are in a more content-based or collaborative and hybrid setting, where instead of just purely asking about movie ratings, you might also ask, what do you think about this genre? What do you think about this actor? And you can take that into account. There's other ways around. Account. There's other ways around this. As I mentioned, there's a talk in the near future about cross-domain recommender systems. So if you know a lot about what people think about books, that can inform what you think about movies. So you can always just buy a bunch of data from someone else. Or you could buy demographic information from Netflix, for instance, or from Netflix, from Facebook. I mean, Netflix can buy information. Or you could just ask for it and that can be informative. So this, before I ramble on for too long, this is still an area of open research. In some sense, the problem is unsolvable. In some sense, the problem is unsolvable, and we're just coming up with the most efficient way of dealing with it. So, this is always an area of open and active research. Okay. Yes. I think you're on mute. Oh. Can I ask a question? Yes, yeah, by all means. I'm basically done. It's just this last summary slide. I remember the ATT people, they also considered the time. They also consider the time series, like when what time the user watched something. So, a more abstract way is what order of the movies the user watched. So, order oftentimes can be quite informative. After some literature talking about how to handle, I mean, metrics factorization doesn't take care of the order. Yes, yeah, no, that is a good point. Yeah, because I'm. I know that that is a good point. Yeah. Because people's tastes, for instance, can change over time. Like, you know, when you're a kid, you might really like action movies, and as you grow up, you might go more towards dramas, for instance. I guess that's one of the more extreme examples. But I know it's like time data like that is also very important for news recommender systems because a lot of that tends to be like very time sensitive. So that's something which is definitely important and I have not addressed at all. Yeah. Okay. Yeah. Okay. Yeah. So summary and summation. So recommender systems are important because they're attempting to solve the information overload problem, which arises a lot. You have a lot of things and you want to find one of them, which you enjoy. So recommendation systems provide recommendations to users about items to maximize that user's utility or that user's enjoyment. There's a lot of recent work which I haven't covered here, which is going over a lot of new exciting areas across the main recommendation. As I mentioned earlier, we'll get more in that. We'll get a more in that in a little bit. You can incorporate new algorithms like machine learning methods, time data, spatial data. Conversational recommender systems are a thing as well. So there's a lot of really fascinating research going on. But if we are interested, then we need to take care to produce good, high-quality research because that is not necessarily always done. And as a matter of fact, I suspect that a lot of the research in this field takes place in industry. Takes place in industry when a lot of it is going to be proprietary and not necessarily known to the public because, like, you know, Netflix really has no benefit in sharing, hey guys, this is our recommendation system algorithm to people. And I hope now that I've said that, someone's not going to show up with Netflix's recommendation system algorithm. But yeah, so this is a very important field, but it's underexplored in the academic literature, and it's really rife and open for new possible research. For new possible research. So, thank you all for putting up with me for so long. And we can move to questions or discussion. I believe there was a question in the chat earlier. And Ayu Chin also has a question or comment. I think he asked that. He just hasn't lowered his hand. Okay. So, Guy, why don't we discuss your comment? Yeah, my comment was just towards your point, David. So, you know, moving. So, you know, I'm currently running an experiment on the MovieLens platform where we basically try to augment the standard ratings data set with the sort of data about what people think about movies they haven't watched. So we're basically running an experiment where, you know, every time someone shows up on the platform, we have some carefully chosen set of movies where we basically elicit sort of what they think about these different movies. And so the first order thing of the experiment is more of an economics project, really just thinking about what are the mechanisms that drive user behavior on these platforms. That drive user behavior on these platforms. So, you know, how informative are these beliefs about what things people actually consume? How do recommendations impact these beliefs? And how does consumption of goods, for instance, impact the beliefs of other goods? But I think there is a question that we want to tackle at some point, which is we think that this sort of data about what people think about movies they haven't rated or haven't watched is super valuable for actually the final ratings prediction. And similar to Patrick, we really don't know much work that really tries to do this. Work that really tries to do this. But, sort of, you know, the from a more economics point of view, it sort of seems strange the way that most people in this literature approach the problem, which is only looking at this data set of what people have watched, not taking into account the sort of decision process that leads to that. So, I just wanted to point out that, you know, there's some lurking on there. So, yeah, that's good to hear. Yeah, no, because I guess I at least could imagine that for the movies you haven't watched, you either haven't watched it just because you haven't watched it, or you haven't watched it because you. You haven't watched it, or you haven't watched it because you expect not to like it. And if you can figure out which ones fall into the latter category, then like, don't recommend those. Yeah, so it's kind of interesting what we see. I mean, so far, the preliminary results from the experiment are like people really don't seem to, like, they almost never watch movies they predict they would rate under a three. And then you see this really clean, like monotonic relationship in, you know, people are more likely to watch movies they think of 3.5, 4, 4.5, and 5. We also elicit, you know, because We also elicit, you know, because you know, uh, sort of there's an economic notion here of these are sort of experience goods, and so people have uncertainty about how much they would rate them. That, you know, there's an important role that sort of uncertainty plays, right? So there's some movies that people, you know, they don't really know what they would, what they would, how they would, how they would rate it. And so they really, you know, they give it a rating of a three and they say we're really uncertain about it. And so we see a very clear pattern of like when people are really certain about something and they're really About something, and they're really, they rate things really highly, they're very likely to watch it. But there are a lot of movies that get recommended to them that they're like, oh, I rate this with one and I'm 100% sure of that. Right. And so these are recommendations that are basically being wasted by the platform because they sort of don't have information about these beliefs. So. Okay, that is very interesting. Yeah. All right. Nancy? I wondered if the statistical problems change a lot depending on. Problems change a lot depending on the particular context because with the journals and the submissions and so on, there's it's kind of moves slowly in time, as we're familiar with as academics. Whereas if you're buying something on Amazon, they need to get the recommendation to you, as David said yesterday, just as you're checking out. And I guess Netflix is kind of somewhere in between. So I mean, I can see that the similarities. I mean, I can see that the similar structures would apply in those different settings, but I would guess that the statistical models would need to be somewhat different, but I don't know. Yeah, so are you talking about in terms of like how fast you need to get recommendations to people or you need to model different things in different contexts? Because I think both of those are good points. It sounds as though one might want to augment the probabilistic. Might want to augment the probabilistic matrix factorization model with a model that also includes informative missingness. And this guy was saying, if you have never rated a zombie movie ever, then that probably means you really don't like zombie movies. And so one could begin to learn genre tastes by simply sort of being aware of whether or not you have ever looked at a science fiction movie, whether or not you have ever. Movie, whether or not you've ever looked at romantic comedy. Yeah, that seems like it would be a good approach. I mean, you could even incorporate, like Guy mentioned the movie lens data set. No, they have like a tag genome stuff going on, which you could use to, so you don't have to discover the latent features, you could just have them. Might be an interesting area to check out. I just thought of something else, just based, I never rate movies on Netflix. I never rate movies on Netflix, but so I get terrible recommendations. But has anyone looked at whether that's a thing? Like, do users who rate a lot get really good recommendations from Netflix or is it pretty random? Has anyone looked at that? I don't know, Guy? So, my understanding, at least from talking with industry people, is that nowadays they're actually relying a lot more on sort of implicit signals, which is like, you know, the kinds of movies you watch. Which is like the kinds of movies you watch precisely because there's some selection issue in terms of who rates movies and which movies do you rate. So that's, I think, another sort of at least my understanding, active area of research. I haven't actively worked in that, but I know that in industry, you know, people think that's a big problem. Patrick, in your review, do you have the sense that people are trying to build models that make strong use of sort of human knowledge? Strong use of sort of human knowledge because we do we could learn that there's a class of movies that are romantic comedies starting from nothing, or we could try and turbocharge this by saying, hell, it's a genre, everybody knows it, and some people like it, and some people don't, and trying to build the model using that type of strong prior. I have seen some approaches which make use of Um, which make use of more content descriptions like that. Um, there's a whole sub-genre of papers which deal with like analyzing movies based on like user reviews and tags and whatnot, which is kind of similar to what you're saying. Jumping back to an earlier thing that came up, there's an issue with judging. An issue with judging contests in which you have expert judges, say you're looking at judging floral displays, and there are too many of them for one judge to look at them all. And so you will do some sort of balanced and complete block design such that every judge looks at some of them and every pair of judges looks at some of the same flowers. Flowers. And then you're still left with the problem that some judges may be really strict and others may be really loose. So they will have different spans and potentially different centers for their judgments. And I'm not sure, is anybody aware of statistical literature that tries to put judges with different ranges and different centers on common footing? Okay. It seems that that might be a component of this problem as well, because people will are not going to be using the same implicit scale. Okay, I think Jason has a question. Yeah, I have a question about matrix factorization. So my understanding is that they, when estimating When estimating, they fix the rank, the number of the latent factors, and they can do that by like cross-validation. And then in matrix completion, they just assume a low rank. And so in the Netflix problem, we would think that the matrix is low rank because there are a few factors like the genre and the decade of the movie that would contribute. Decade of the movie that would contribute to the user ratings. But in my reading of the literature, no one really defines what is low rank. So I was just wondering if you came across any definitions of low rank. No, I think it's one of those things where whatever number you come up with is going to be very obviously a much lower rank than the dimension of the ratings. Lower rank than the dimension of the ratings matrix. I think I saw somewhere once, I forget which paper it is, saying that 20 or 30 is usually sufficient. There was one really cool paper I found. It didn't seem to be widely cited or really used that much, but they did a probabilistic matrix factorization model. But they put a coupled Indian buffet process prior on the number of latent features. So they just learned that from the data. So that is one. So, that is one approach which exists. I'm sure that it would depend upon what you were trying to study, Jason. If you're working for OKCupid and you're trying to recommend dates, it's going to be a different low rank than if you're recommending movies or books. Juan? Yeah, I just attempted to answer David's early question with respect to the information. With respect to the informative missing. So, we actually, Annie and I actually have a paper in 2017 on JASA. Basically, use some of the informative missingness thing in terms of in-recommender system and hope to use it to solve the co-star problem a little bit. And that was not in the flavor of informative missing as defined in Rubin's paper in early 1976. Early 1976, in a sense, that how inverted weighting or using multiple inputation can actually address that, but more in a very heuristic, intuitive way. Try to like one step towards solving this problem. And another thing that when we do the research, we figure out that it's also Patrick's point that in terms of how accurate the recommender system is actually in the industry, and this is also related to Nancy's question. And this is also related to Nancy's question early: is that we find that overall, the accuracy in terms of top five or top 10 recommendation is just about 20% accurate. So on average, five movies pop up on your website. Maybe one of the five or one of the 10 movies is actually of your interest and the rest of them are not. I'm not sure whether this is relevant, Patrick. Correct me if I'm wrong. But this is some observations we made. So that's one of the motivation people still do research in recommender systems. Still do research in recommender system, I guess. Joan, you said that you did this paper with Annie. Is this Annie Chu? Yeah, Annie Chu and Xiao Tongshen. Right, thank you. Perfect. I can paste the link in the chat if anyone like it. We would appreciate that. And folks, I will send email to everybody, but I got an email in from Phyllis Jew, who is scheduled to speak at Drew, who is scheduled to speak at, I think, four o'clock this afternoon, but it turns out that she is going to be on an airplane because she forgot that she asked to speak on Tuesday. And so we'll have to cancel that. So we'll have a longer break than originally planned.