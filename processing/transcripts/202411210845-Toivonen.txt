So, a little bit of background. Right, so I'm primarily interested in multi-messenger observations of neutron star mergers. So, looking for GRBs and kilonovae that result from these neutron star mergers. So, how can we improve our chances of detecting these events? Lower alert latencies, new and improved multi-messenger data products, and then coincidencies with external events such as neutrinos. With external events such as neutrinos and GRVs. So, how do we do that? Well, there's a range of multi-messenger searches, and since GW170817, there's been an explosion of interest in all these. So, obviously, we have our gravitational wave searches with LBK, and then EM and neutrino quids frequencies from Kilnova and GRB, such as GRV searches like Fermi and SLIC. So, what can we learn from these MMA observations? Why do we need From these MMA observations, why do we need both the gravitational wave and the EM observations? Well, first off, heavy element nucleosynthesis. So, how are heavy elements created? And then can we observe the R-process elements in these neutron star mergers? And then the neutron star equation of state, which many of you are familiar with here, what's the mass-radius relation or the pressure-density relation of a neutron star? And then, what's the neutron star radius, and can we observe that? And then, finally, some cosmology, so estimates of the Hubble. Some cosmology, so estimates of the Hubble constant via a standard Siren method. So, how fast is the expansion of the universe, and can we ease this Hubble tension? So, a little bit of an intro or a summary of O4A and talking about O4B, our current observing run. So, O4A had 81 significant events and 11 retractions, and it introduced our early warning alerts, which are capable of sending alerts for DNS seconds before merger. So, it actually detects these BNS in the in-spiral phase, which is a big deal. Which is a big deal. And so these are actually retracted if not followed by your normal CBC full bandwidth detection, right? And this is something that all of the, so far, all the early warning alerts have actually been protracted since they haven't been followed by full bandwidth. This is very much a forward-thinking search, right? The early warning searches that we hope to have these be very useful in the future. I think for GW17817, they say you could have detected that about ten seconds before merger. That about 10 seconds before merger. Which, you know, some of this is, you know, again, we always want to cut down the latency, but for something like, you know, short GRVs and things like that, you could hope to maybe actually observe the GRV or observe the merger and things like that if you were to have 10 or 100 second negative latencies. Okay, so event latencies during the MDC and compared to 04A, right? So during the MDC, we had a median GCN latency. And so when I say GCN latency, that's So, when I say GCN latency, that's the time that the GCN label is applied on Grace TV. So, it's the time in which we're sending out the preliminary alert. And that is approximately 30 seconds. And we saw that matched by 04A right there at approximately 30 seconds again. I know you maybe can't see it up here, but they're just both fairly below 30 seconds. And the good thing to know here is that the MDC actually excludes data transfer time here. And so that means that we're actually doing a little bit better during 04A than we did during the MDC. It's about five to ten seconds. MVC. It's about five to ten seconds of data transfer and calibration between the sites. And then we also have early warning latencies, which if you look during the MVC, right, we had a median of negative three seconds. And then I show four retractions here that happened during 04A, which are reasonably consistent with those values. But again, we haven't had any early morning follow-up like this. And so then again, onto 04A quantities, so EM Bright quantities. So, EM Bright quantities, so these are the quantities has an S, you know, is there a neutron star, has remnant, is there a remnant or non-zero ejecta, and then has mass gap, which is looking for a component mass between three and five solar gases. So the point of this plot is kind of show this is the landscape of what we found during 04A, and that you see that there is essentially one standout event there up there, S23, 0529AY, which, as you know, was that mass gap event. So this just shows that EO Bright was, you know, working well. Was working well and correctly identifying the, I would say, objects of particular interest, at least for those interested in neutron stuff. Okay, so on to O4B. So a big part of O4B was the Virgo inclusion. This allows for three interferometer, three detector sky maps, which is really big for sky localization. And so this was the pre-bo4 MDC, right? This is a plot from our MDC paper, the low-length MDC paper. MDC paper for the low-length MDC paper, but it shows how big of a drastic of a difference between two interferometer and three interferometer you have for sky localization. And so this shows the surged area, and the median goes from approximately 500 square degrees to 60 square degrees for this 2 to 3 IFO comparison. So if you look at, you know, again, if you can't read this, in orange here we have the 2IFO, and in green we have the 3 IFO. So that really is a big deal. And this is really important for searching, you know, follow-up searches such as for neutron studies, right? Such as for neutrons, I'm afraid. And here's a nice drastic example of that: the 3IFO example, which is 04 the event where we had the 2IFO preliminary alert, which had a median, or sorry, a 50% searched area or 50% credible area of 700 degrees. And that went all the way down to 7 for the 3IFO update alert. And obviously, that's a drastic case, but as you saw from that last plot, you can get on average around a factor. Get, you know, on average around a factor of 10 or just under improvement by having the third inner parameter. And here we see, you know, obviously that other end, the very drastic, you know, near the 100. Okay, so now on to the main part of the talk here. So these are proposed GW and multi-messenger data products under development. And so no promises with these, but these are things that we're working on and we'll advocate for the release of. And so looking ahead at these, we have the multi-messenger focused ones, which would be kilonova, ejecta, and light curve estimates, joint distance, including. Light curve estimates, joint distance, inclination, posteriors, chirp mass information, additional external coincidences, and a machine learning classifier for assessing confidence and GW triggers, and then additional alerts looking forward, SSM, subsolar mass alerts, which Sushant and others are working on, machine learning, gravitational wave searches, such as A-Frame and Emily, and then HNAPL series from GWDM events. That's the standard center. Okay, so motivation about this. Again, astronomers want to make timely decisions on what to follow up and how to use. Timely decisions on what to follow up and how to use their resources. And so, the first thing we're going to talk about would be source properties out of information that would be useful to astronomers. This would be the coarse-grained chert mass estimates that I'm working on. And so, I'm leading the technical implementation of the CGMI package, and it's designed to release coarse-grained chert mass information. So, it would be some sort of a bin or a coarse estimate of chert mass that we hopefully release to the public. And so, what can this tell us? If you're talking about. What can this tell us? You know, if you're talking about the HazNS regime, we have, you know, we could make some discriminations about the type of the remnant. And so, this was a paper from, this is Ben Margalitz's paper who's just started at the University of Minnesota. But you kind of see here for a range of chirp masses over here, you can make some discrimination if you look at the very top column between stable neutron star, the supermassive neutron star remnant, which is stabilized by rotation, more temperature for a reasonable amount of time. For a reasonable amount of time, whereas the hypermassive, which is only very temporarily stabilized by this rapid rotation before collapse to a black hole. And then you also have the regime of prompt collapse. So you're hoping that you could, you know, use these different remnant classifications to make some discriminations of whether or not you'd expect to see a kilonova and how bright, et cetera, and how much mass is injected. And then a final interesting thing was that there's some folks, particularly at Caltech and Matthew Graham and whatnot, and Kira are working on this, but they're looking for. They're looking for EM bright BVH that would be in AG and accretion disk. So there would be mergers of BVH and accretion disk that would cause a disruption, and you would hopefully be able to see a JET or an EM counterpart to that. And so they're looking at hopefully following up high mass BVH events. So this is something that would allow them to have some idea of the mass of these BBH events. And I think right now they have, you know, some kind of classify that they're trying to do this themselves, but this would allow us to be released with some of that information. And so, you know, And so, you know, the idea of this, right, again, this is something that's, you know, I would say preliminary, but you'd be releasing some sort of a band estimate like this, and you would have a low latency estimate and a PE estimate. And obviously, all this is preliminary, but that's the ID. Okay, so on to motivation for EM follow-up. Again, source properties. We have joint distance and inclination estimates. And so distance and inclination are degenerated based on our sky maps. Our degenerate and base star sky maps. And so we want to provide distance inclination as well as inclination, so distance inclination, distance, and sky map information for a range of inclination values. And so this would be a joint distance distribution for a given line of sight. So if you looked at a sky map pixel, right, you'd really have this 2D distribution of the distance and inclination information. Okay, and then on to the mass ejecta estimates, as well as the kilo-note. Estimates as well as the Kilanova light curve estimates. So, this is designed to be a natural extension of the EM Bright data projects, it just has NS and has remnant. But we want to tell how much mass ejected there is, not only whether there is mass ejector, as well as some predictions of flight curves. And so these are the proposed Kilonova data products that we're working on. There would be this has ejected quantity, which would be the probability of having a significant amount of mass ejecta. We define that as greater than 1,1000th of a solar mass. Of a solar mass. And then you'd have three subcategories, which would be BNS, NSPH, and NOAA ejecta, which logically follow from that. And this assumes the event is astrophysical in nature. And then we'd have ejecta and light curve estimates, which would be 90% credible intervals of the peak absolute magnitudes and the mass of density. And so getting into how this is actually done, we would start with our CBC searches in a GW template match by a CBC pipeline here. Then we can use either a point estimate, which would have your low latency, you know, 30 seconds. Your low latency, you know, 30 seconds, or you can use parameter estimation, which you know, on you know, our hours, time scale. From there, we use Isaac Legregs et al. 10 to the 4 equation of state posterior draws. So those are informed by radio pulsar and GW observations. And then combining, and this is, you know, if you're familiar with this paper, but you have an equation of state constraint, that 10,000 equation of states are drawn from this posterior group. And then we combine this with the. With the oh, so first, you know, we can use up to all 10,000 of those equation of state draws per sample, right? So, you know, if we had 10 posterior samples and 10 equation of states, then obviously we've got, you know, 10 draws here that we could feed into our ejective fits. And so from there, we have separate DNS and NSBH ejective fits for the dynamical and wind ejecta. And from there, we can get this as ejecta quantity because now we found we have, you know, we start with our initial mass estimates, we've marginalized over equation of state. We've marginalized over the equation of state, and now we have some fraction of events that would have significant ejecta. And then from there, we use a Kilanova light curve model. So, this is a surrogate model that's trained on a grid of PLASIS simulation. So, this is a 3D radiative transfer Monte Carlo code. And the inputs for that would be the wind ejecta, the dynamical ejecta, the inclination angle, and then the opening angle of the ejecta. And then finally, that's how we're able to produce our 90% credible interval of the peak magnitudes. Of the peak magnitudes. So, what does this actually look like? And is this consistent with what we already know? Think GW170817 and AT2017 GFO. So, we see here that if you look at this plot here, this would be using GW170817 posterior samples and then running through this framework that I just showed you where you marginalize over equation of state and inclination angle, et cetera. So, we get this range of predictions for what the kilonova should be. And then, in red, here, hopefully, you can see this up here, we compare it to the actual kilonova of 182017. You know, Kilonova of 182017 GFO. And this is in the R band. So you see, in general, we're pretty consistent. This is, it's good to see this agreement as well in early times. And then as you get to late times, we have problems with our POSIS models, which don't necessarily rely, as was pointed out in a previous talk, the later times in the POSIS simulations, they assume local thermodynamic equilibrium. And so in later times, we don't expect this to be. And so, in later times, we don't expect this to be as accurate. So, it's good to see that reasonably consistent for about five days. And even after that, we're still consistent, even though we're outside of our 90% coverage performance. Okay, and then so taking this one step further, we ran this on a mass grid here. So this is not actual events, right? I just input the actual masses here, and we ran predictions for all these mass pairs. And so, what you see here, and I have this dotted line for this BNS and NSBH regime, which is rather Which is rather, it's not a hard cutoff, right? Because we have all these different equation of states that we're marginalizing over. In this case, we used all 10,000. And so each of those equation of states has a different upper mass bound for the neutron star. And so that means that depending on the equation of state, it may classify, one equation of state may classify, you know, the same point here as a BNS, or one may classify as an NSVH. Like the other part of this is we have different DNS and NSBH fits and different BNS and NSBH-like curve files. And so you can see here. Curve problems. And so you can see here that in general, you see the most mass ejecta and the brightest light curves at these lower mass regimes. And in addition to that, you see that in general, the mass ejected here plots are going to be a little bit more, I would say, continuous, whereas you see that the NSBH light curve model here is a little bit more pessimistic. And one thing to point out with the NSBH models is that they do depend on the spin of the black hole. So the So this assumes zero spin here on the left. Well, I guess it's your right. But with a higher spin, you would expect to see probably some more mass ejecta and eventually bright or light curves, which I can produce those plots. But for this one, we show the zero spin case. And then finally, what happens if we run these on a set of MDC events, the Mach Data Challenge events? And so here on the bottom, we kind of see an increasing set of M1, M2 pairs for, you know, kind of increasing total. For you know, kind of increasing total maps here, and again, I put this dotted line, kind of thinking of the BNS-dominated and the NSBH-dominated regime. And again, that's a loose cutoff. You want to think of it as where are, you know, most equation of states are classifying samples above here in the BNS regime, and most equation of states are classifying samples down here as the NSBH regime. And so, starting from this side of the plot here, right, you see that in general, you know, some of these, and also, I should say, we do compare back to the actual injected parameters. Back to the actual injected parameters. So, these run with the SLY equation of statement with 10 and red, kind of as a sanity check, just a comparison to make sure that this is consistent. And so, a couple things I want to point out, right? Obviously, these lower mass events in general tend to be brighter. And then as you move this way, you'll see this really large jump here in SLY between there and there. And that's because, as you know, the SLY equation of state is right just between here and here. It's like 2.1 approximately. 2.1 approximately. And so that means that you've jumped here in red, right, from the BNS regime to the NSBH regime after that point. Whereas if you're familiar with how PE looks, posterior samples, parameter estimation PEs, you'll kind of have these nice arcs where you'll have some in the BNS regime and some in the NSBH regime as you cover the region of parameter space. And that's why for these events that are kind of in between BNS and NSBH, you see these bimodal distributions where half of it is dominated by BNS samples on top. By BNS samples on top, and then on bottom here, this is dominated by the NSVH levels. And so you can see that nice shift in how this behaves and how we'd expect these are predictions for a range of events. Okay, so next, statement of confidence in a gravitational wave trigger. So this would be a machine learning classifier to determine, is this triggering noise or astrophysical? And so, you know, for some motivation for this, right, we do retract a significant amount of events. As of November 11, 18 out of 174 were retracted. 174 were attracted. The astro does not take into account all searches and can often disagree between pipelines. Those triggers are inherently unavoidable, and so we look for a reliable statement of confidence in each trigger. And the idea of this, too, the interesting part about this is we do take into account the preferred pipeline as well as the number of pipelines that's attacked at the event. So this is something I'm working on with a set of undergrads, but we have some preliminary results on the MPC that look pretty good at one thing. That look pretty good at one. So that's something that works. And then finally, external coincidences. So we're looking to ingest IceCube alerts into Raven, which is a package used to find external coincidences with GW events. And last slide here, finally, the standard Siren method. So I'm also working on this with some undergrads. This would be a framework to combine multiple GWEM events to estimate the Hubble constant, right? So you combine a distance posterior as well as a redshift from the host galaxy. And we're doing this with simulated events. And we're doing this with simulated events. And the idea here is that in the future, if we had a larger number of GWEM detections, we'd be able to put a reasonably constrained estimate on the recoup constant. In general, we need a lot more detections to be able to do this, but that's where we're at with that. And then finally, any questions, feel free to ask. Thanks to Michael, my advisor, and Marco and the rest of the organizers for inviting, and then the rest of my collaborators. And then here's these two papers that I talked about. Here's these two papers that I talked about. The MPC1 and I'm actually telling over here. Thank you. Any questions? Yes? Nice talk, Andrew. I've got a couple of, so if you could just sort of flick back to the flowchart that you had. Flowchart. Flowchart. Yeah, this one. So the end product is lovely to have, but the ingredients that go in are very uncertain and they're not actually consistent with each other. So for example, Actually, consistent with each other. So, for example, the Kruger and Foucault simulations are built on one equation of state, BB2. And so, then you're marginalizing over the equation of state by different draws. But then you also have like check mass estimates that go all the way low to a regime where you could have a supermassive neutron star, in which case the simulations, fossil simulations, don't have the actual physics to have the rotational energy of the neutron star. Yeah, and so was that the end of that? So I'm just kind of worried about why go through all this sort of process of handling putting in all these different ingredients when ultimately what you want is a big range of predictions for the light curve. And you could get at that without needing to put in all these ingredients. Yeah, so I think the idea is, obviously, you're right that these are based on numerical relativity effects in a certain regime, right? And obviously you could say based on an equation of state. In general, Equation state. In general, like you said, we are trying to make this wide prediction. And as we become more informed about the individual ingredients of this, these individual parts, we can replace those and we'll have the framework in place to make more constrained detections. But yeah, the point of this is to be able to actually go from the GW detection, whether it's a point estimate, then you do, from the point estimate, you have a reasonably measured chart mass. You could assume marginalized over the mass ratio. The mass ratio. Otherwise, you start with parameter estimation and you go through these fits. I get your, I know, your worry about the accuracy of the fits, but yeah, I think that's just the state of where we're at right now. And we don't necessarily have a better way of estimating this mass eject. And I think marginalizing over the equation of state does give a nice range. And one part of this, too, is it's good to look at kind of the optimistic end of this, whereas, you know, if we are at least producing some. If we are at least producing some samples that are possible, producing a kilonova, people are going to be following this up. And again, even right now, people are going to be following up pretty much anything that looks like it has a new trend step. I guess my point is more that I don't see the point of marginalizing over the equation of state when if you just marginalize over the kilonova heating rate, for example, the uncertainty is already three orders of magnitude. Okay. But then so you would so would you have to pick you'd have to pick a single equation of state, right? Because like we're so far away from We're so far away from yeah, I think that's a you know a reasonable a reasonable critique, but I mean we're we're trying to include I would say that extra range of uncertainty as well so we are trying to make a wide you know a wide range of predictions and it covers you know a lot of these uncertainty for example if you talk about you know NSBH events where maybe you get less than 1% of our samples end up producing you know mass eject or mass ejecta right and then let's say we do end up observing something we can kind of take a look at you know going backwards and looking at things as well but it's okay Looking at things as well. But it's okay with you. Go back to the mass ejective blocks, the color. Yeah. No, it's the mass ejective blocks. The color mass versus mass from one and two? Yeah. Oh, yeah, thanks. It just struck me that if we give away coarse grain mass information, then we have a chirp mass curve on this. There's a fairly large sensitivity in the ejecta masses. So are we releasing the ejecta masses or just the final light curve? Leasing the objective masses, or just the final light curve? Because with the objective mass and the corresponding mass information, I think I could figure out M1 and 2. Right, okay, so yeah, there's a few things there. So the CGMI, the corresponding masses submit, that would be separate and before this, right? And then this would have to, you know, undergo a whole review and whatnot. And what we actually released, yes, I think there would be some comments about what we could release and if that was releasing additional mass information. So this is something that I think if we, for example, If we, for example, released a 90% credible interval of the mass ejecta and the light curves, are you worried that that would be releasing? I'm asking the question. It looks like that would be effectively releasing M12. I'm also curious about just the inclination release. I think I know of colleagues in cosmology that the inclination, the steers, is one of the proprietary things we have that could potentially enable more. Have that could potentially enable more science things to be done outside of the LDK. So, at what point do we give away all the information needed to do various kinds of science to the external community? Here I'm just curious, it looked like you could backpit on what I'm doing. To some degree, right? But obviously, this is a grid of these individual masses. So, what you would be releasing for some event would be some 90% credit. I'd be releasing something more like this, not. Would be releasing something more like this, not like the image energy. Right, this was like a simulated grid, right? This would be like, this was a simulated grid for all events, right? Like across the grid. This would be like, if you had one event, this would be more similar to what you'd be releasing, which, you know, with the whole uncertainty on PE and the whole uncertainty on the effects and the marginalized number of equation of state, I think, you know, people would, I think there would be this comment and that would come up in review, but I think things are somewhat blurry for that. Somewhat blurry for that in terms of being able to actually get the original masses out. But that's something we could talk about later, too. Right, so we're done with the speaker again.