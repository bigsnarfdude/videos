Yeah, sometimes they just need to reset. Okay, that seems to have fixed it. Okay. All right. So yeah, so as I was saying, I'm interested in understanding genetic architecture of complex traits. And all that means is basically figuring out this mapping F from genetics and environment to trait and understanding the nature of F. So my goal in today's talk is to give you a little bit of an outline and a background on what we have. And a background on what we have learned about the genetic architecture of complex traits as we think about applying machine learning. So, what that means is, I'd like to present some evidence for non-linearities in complex trait genetic architecture. A lot of this is coming from classical statistical models, so no deep learning there. And then in the second part of the talk, I'll talk about deep learning applied to a specific problem, which is phenotype imputation, and how that's been an effective application of deep learning. And that leads to some additional. And that leads to some additional questions of interpreting the results of these kinds of applications. So, one of the things that we've learned about genetic effects with respect to complex traits is that there are hundreds, often thousands of these genetic factors, often with relatively weak effects. So, which means if you want to make robust inferences, we need large sample sizes. And in part motivated by this, there's been the growth. In part motivated by this, there's been the growth of these kinds of biobank scale data sets. So, data sets that collect information on hundreds, thousands of phenotypes paired with genetic information. So, even with these large-scale data sets, one paradigm is to be able to first identify these genetic effects. So, this is a paradigm that's called genome-wide association studies, and then say something about the collective properties of these genetic effects. Turns out, These genetic effects. Turns out, even with these large data sets, this GWAS paradigm is still underpowered. Instead, there's an alternative approach that's proven to be quite powerful and effective. And the idea is to integrate out the identities of these specific effects and still say something about this property. So one class of this type of approach is what's called variance components analyses or mixed models. And here I have an example of an application of these mixed models to learn something about a quantity called heritability. Heritability is Heritability. Heritability is basically a measure of genetic signal, so, how predictive genetics is of a trait. And often, when we think about heritability, we think about it in the context of a linear model. So, we have this linear model of the phenotype as a function of genetics. We have these effect sizes, and then we have some nodes. We are still in a high-dimensional regime, even with these large sample sizes. The number of SNPs, genetic variants that we are interrogating, are much larger than our sample sizes. So, we can't expect to be able to. So, we can't explicitly estimate these effect sizes. So, we have to put some assumptions on these. So, typically, we have some distributional assumptions. The specific nature is not so important, but we'll have something like each effect size comes from a distribution with mean zero and some variance. That's what we call the genetic variance. And then we have the noise coming from another distribution with mean zero and some other variance. That's the environmental variance. So, having estimated these two quantities, the genetic and the environmental variance, we can put those together. Environmental variance, we can put those together to get our estimate of heritability. But that's a measure of genetic C. So, this is a classic model. We know how to do estimation in this model. The challenge is in applying this to these really large data sets. So, if you try to do maximum likelihood type estimation, it just doesn't scale. So, what we've been doing in my group over the last few years is trying to come up with scalable estimators for these types of quantities. And the key idea. And the key idea in making scalable estimators is the following. We're going to take our genotype matrix. It's of the order of 500,000 individuals, million individuals, million SNPs, 10 million SNPs, and we compute a sketch of this genotype matrix. So what that involves is taking this genotype matrix and multiplying it with a small number of random vectors, B of M. And then you do your computation on this sketched genotype matrix. So clearly, the smaller. So, clearly, the smaller the number of random vectors, the smaller b, the smaller your random projection, and you're going to have an efficient method. What is less obvious is you can actually get fairly accurate estimators for small values of b. So, what we find, and this is an empirical observation, is you can get accurate estimators for b as small as 10. So, this class of estimators, we call it randomized Hasteman-Elston regression because it's related to some classical methods from quantity. To some classical methods from quantitative genetics. I won't spend too much time on the results, but the key idea is that you can think of these randomized Haysman-Elston regression as interpolating between maximum likelihood methods, which are optimal in terms of statistical efficiency, but are not very scalable. And then there's a whole class of methods that have been developed in statistical genetics, which summarize the data in different ways. So these methods tend to be low in terms of accuracy, but are much more efficient. And this randomized here. And this randomized CSML instant regression, which is part of a bigger family of randomized method moments, is interpolating between these two extremes. So we can apply these kinds of methods to UK biobank type data. So this is a setting where we are just using it to say something about heritability. So everything is linear, no non-linearities. And just to give you a flavor of the kinds of results, here are some examples where we look at how much fate heritability increases as you go from relatively common. As you go from relatively common variants to rare variants. And you see that for many traits, we can quantify how heritability increases as we are assaying newer types of variants. We can also go beyond just heritability as a global quantity. We can ask how it's distributed across the genome. For example, how it's distributed in genes, how it's distributed in enhancers, specific functional genomic annotations. And you'll find patterns like specific regions of the genome, which are enriched for trait heritability across different traits. Trait heritability across different traits, other regions of the genome which are enriched for specific traits. So we have these interesting patterns that tell us overall where genetic signal seems to be distributed. So everything here is linear, but the question is, what can we say about nonlinear effects? And it turns out this approach, this sketch and apply approach is pretty general, and it can be leveraged to interrogate nonlinear effects. So we decided to do this. So we started looking at the first type of non-linear effect. So, we started looking at the first type of non-delete effects. So, this is dominance. So, instead of assuming that each genetic mutation changes your phenotype by the same amount, we allow for differences according to whether you're a heterozygote or homozygot. So, now you can fit a dominance model in this framework. So you have an additive component plus a dominance component, and now you can fit all of them together. It's important that we do a joint fit, otherwise you can have bleed over where you might think that something is dominant because you haven't accounted. Something is dominant because you haven't accounted for the additive. Now, when we do this, so we apply this to about 50 quantitative traits in the UK Biobank, anthropometric traits, blood labs, different kinds of liver measurements, and so on. We can estimate both the additive component and the dominance component. We find substantial additive heritability across these 50 traits of the order of 20%. When you look at dominance heritability, essentially there is no signal. So we have no evidence. Signal. So we have no evidence that there is any kind of dominance heritability. We do some power analysis, and it's pretty clear that if there is, it's less than a percent. So now we moved on to another kind of nonlinearity. And this was also something that was mentioned in the talk on Monday, which is the fact that these biobanks, the unique aspect is they have not just one trait, they're measuring hundreds of traits. So, a question you might ask is: what happens? Is what happens to the heritability as a function of these other traits being present? So, this is often called gene-environment interactions, or another way of thinking about it is gene-context interactions. So, a classic example is somebody is smoking and you're looking at their BMI, and you might have a variant which has different effects depending on their smoking status. So, again, this general framework can be extended. You can now fit an additive component, and then you have another component. Component, and then you have another component which accounts for these gene-environment interactions. So it's basically a Kronecker product between your genotype and the environments. And now you can ask what is the G by E heritability. Turns out there are some issues of scaling here. So imagine if you have 100 environments and a million SNPs, even forming this matrix and operating on it explicitly is challenging. So we need to be clever about how do you actually scale these kinds of computations. But in principle, it can. Kinds of computations, but in principle, it can be done. So here's an example. This is unpublished work where we actually look at this type of gene-environment interactions across these 50 phenotypes. And here, the environments that we are looking at, so not environment, but context, is G by sex. And here you see that there is evidence for fairly strong G by sex interactions for traits which make sense. For example, in this case, the strongest one is testosterone as measured in an individual. As measured in an individual, we can also look at other types of environments which are potentially interesting because they are modifiable. So, this is an environment, a context which is statin usage. So, whether somebody is on statin medication. And what we see here is fairly substantial gene-by-statin heritability for the kinds of traits that make sense. So, in this case, blood pressure features fairly high, cholesterol features fairly high. And we also see some interesting novel results. Some interesting novel results, for example, glucose and HPA1C levels in the blood. So, there's ongoing debate about how statin usage might affect a person's type 2 diabetes risk. So, this is relevant to those kinds of analyses. So, finally, we can turn to G by G interactions. Turns out we can apply G by G like what we did with G by E. Turns out now when you form this matrix of all This matrix of all SNPs coupled with all of the SNPs, while you can actually perform the estimation, the estimator is too noisy to actually tell us something meaningful. So what we do is we don't actually compute a genome-wide G by G matrix. Instead, we take one SNP at a time and form its interaction with all other SNPs across the genome. And now we can test: is there evidence for G by G at a SNP of interest to all other SNPs across the genome? Them. So here is an analysis where we can again actually run this across SNPs of interest. So what we've done is we've done a GWAS on these 50 traits. So that gives us a set of SNPs which have a marginal effect on your phenotype. And then we can go back and test if these SNPs with a marginal effect have additional epistatic effects. And what we see here is there is fairly strong evidence for these marginal G by G effects across. G by G effects across these set of traits. And so this is ongoing work to understand exactly where this kind of epistatic interaction comes from. So what I want to give you a little bit of a flavor for is based on these kinds of biobank scale analysis, the picture that we're getting is one where there is substantial additive contributions to complex traits, very little by way of dominance. As far as G by E is concerned, As far as G by E is concerned, it can be quite substantial depending on the environment and the trait of interest. And there also seems to be this hidden G by G effects, which we learn more about as the sample sizes grow. So what's missing here is going beyond everything that we have learned. So we saw one application on Monday's talk, which was looking at higher order interactions. And so building on this to actually go beyond just pairwise interactions. Go beyond just pairwise interactions is probably going to be the next major analysis of interest here. Oh, and I should say, feel free to stop, interrupt at any point. I'm happy to kick questions. Okay, so this was all about the link between genetics and complex traits. But all of this was motivated by this idea that we actually have phenotypes that are measured across. That are measured across all of the individuals that we've collected. But the problem with these biobank-type data is the reality is a little bit more messy, right? So you have a phenotype that you might be interested in, the alcohol addiction. So if you look in the UK Biobank, in principle, you're starting with 500K individuals. In practice, you have measurements on about a percent of them. So the problem is for reasons of logistics, cost, phenotypes of interest, clinically relevant phenotypes. Clinically relevant phenotypes often are measured only on a subset of the individuals that you care about. So, if you only had one phenotype measured, there is not much that you can do. However, the fact that you're measuring many phenotypes, related phenotypes, offers you the opportunity to fill in this missing data. So, this leads to the phenotype imputation problem. So, you have data which looks like this. You have a matrix of individuals times phenotypes, and you have these missing entries, and the goal. You have these missing entries, and the goal is to be able to fill in these missing entries using the other entries that are observed. And the assumption here is there are correlations amongst these phenotypes that allow you to perform this imputation. So, the statistical learning problem is to be able to learn this function that allows you to fill in missing data. So, what we'd like about an imputation method for these kinds of problems is one, it's expressive. So, it's able to not just It's able to not just handle linear relationships, but also nonlinear relationships. It can handle mixed data types. So, continuous binary data types are present in large quantities. The missingness is structured. So this is not missingness at random. For example, a lot of these are obtained by surveys where a person doesn't answer question one, they don't answer question two. So there is structure inherent to the missingness. And finally, scale. So you need to be able to handle all of these individuals across. To handle all of these individuals across all of the feed types that are measured. So, our starting point for this was a deep learning method, but the inspiration is when you think of this matrix and it's missing, a lot of the most effective methods have treated it as a matrix completion problem. So the idea is you look at this matrix and you try to decompose it as a low-rank matrix product. And often, this is a linear low-rank decomposition. Linear low-rank decomposition. But if you want to make it expressive, we can try to expand beyond this low-rank assumption. So that leads naturally to thinking of this as an autoencoder, where we have our phenotype and the autoencoder maps it to this low-rank hidden layer, which then gets mapped back by the decoder. So since we care about imputation, what we would want to do is if you have a missing phenotype, we would plug in a default value. For example, if it's mean centered at zero. Example, if it's mean center at zero, and that would give us an output, which would be the imputed value for the speed type. So, to learn this, autoencoder across mixed data types, we have a loss function, which is a sum of mean squared error and a cross-entropy loss. So, that handles the mixed data types. So, that's fairly standard. The final aspect is dealing with structured missingness. So, to do this, we have to simulate data where. Simulate data where you know the true value and the data has been withheld. The auto encoder reconstructs this missing value and then that drives your loss function and learning. But we want to simulate data that matches our observation. So to do this, we use the scheme which we call copy masking. So the idea is you don't just randomly mask out entries in this matrix. Instead, you look at the existing missingness patterns, and that gives us a set. And that gives us a set of missingness patterns that are present in the data. And each individual randomly picks one of these missingness patterns, and that tells us which of the patterns entries are going to be masked out. So the autoencoder combined with this copy masking technique, we call this method autocomplete, and we can evaluate it using simulations where, again, we have these mask entries and we can figure out how accurate. Masked entries, and we can figure out how accurate the reconstruction of these masked entries are. So, here are two phenotype cohorts, both from the UK Biobank. One is primarily phenotypes that are related to blood lab measurements. A second one is related to mental health phenotypes, roughly 300,000 individuals. And the experiment basically varies the amount of missingness in the data, and we are computing the R squared between the imputed and the observed phenotypes. This is all in held-out data, which is not used for training. Training. We looked at a few methods. So we looked at nearest neighbors. So that's a classic method that's been used for imputation. We looked at a couple of deep learning methods that have been proposed. One is this GAN-based method. Another is a variational autoencoder method. And finally, we have a linear method, soft impute, which is basically doing this low-rank matrix decomposition as a way of doing imputation. So the first thing to take away from this is the linear method, soft impute, is actually remote. Soft impute is actually remarkably effective, which is why we use this as a baseline. So it's actually doing quite well across the range of missingness. However, having the nonlinearity as part of autocomplete gives you some additional benefit in terms of R-squared accuracies. The mental health data seems to be harder than the blood lab data because there's more missingness in the mental health data. And that also seems to be the place where this nonlinearity seems to be improving. Here is another view of the Here is another view of the same result, looking at one of the missingness percentages, 1%, and we are comparing the R-squareds for autocomplete versus soft-impute. And each dot here is a phenotype. Mostly they line along the diagonal, so they are pretty comparable. But you see that there are some phenotypes where there is a gain to be had from modeling non-delectices. Okay, so we were happy. Now we can actually impute phenotypes. So question is, what can we do in terms of So, the question is: What can we do in terms of discovery now that we have these imputed phenotypes? So, a natural thing is you have these clinical phenotypes that are only present on a subset. Now we've imputed them in the UK Biobank. So, your sample size has gone up. So, can we use this to discover novel genetic factors? Can we run a GBOS of these phenotypes? Before we do this, the key thing we want to convince ourselves is the imputation doesn't lead to any biases. It's accurate, but perhaps one method is more biased than the other one. Biased than the other one. So, to do this, we did this kind of simulation experiment where we start with a phenotype that is almost completely observed. We downsample or we mask it down to about a third. We impute using our method, and then we run GWAS on the imputed phenotype. So in this case, the phenotype that we chose was alcohol consumption. So now we take the GWAS hits for this imputed phenotype, and we go back to the original phenotype and ask, do they? The original phenotype and ask do they replicate? So it's not a replication really because it's not independent, so more like a pseudo-replication. And the main takeaway is more by and large, these hits seem to be replicating. Here, all of them replicate. There are other phenotypes, it's closer to 90-odd percent. So, having done this, now we can actually look at phenotypes that are truly missing and clinically relevant. Many of these are mental health phenotypes, those are hard to measure. Phenotypes. Those are hard to measure at scale with kind of gold standard measurements. And so here is an example of one such phenotype. So this is whether somebody is depressed and on medication. It's initially present in about 10% of the full UK biobank. So you have one genome-wide significant hit. And after phenotype imputation, the number of hits goes up considerably. And this is a story that's pretty consistent across these different phenotypes. Not entirely surprising because the sample sizes have gone up. Surprising because the sample sizes have gone up, but it's still exciting to have these novel genetic risk factors for all of these phenotypes. So, ongoing work is basically trying to figure out what these genetic risk factors for these imputed phenotypes look like. Are they different from the original set of risk factors? Are there kind of qualitative, quantitative ways by which the imputed phenotype is better or worse than the original phenotype? Yeah. So yeah, so yeah, let me repeat the question. So in your post-imputation inference, when you run the regression, some whites will impute it, and there's a good number of Will impute it, and there's gonna be a lot of variance, some while directly observed. So, in your regression model, do you have a variance specification that depends on the fact it's imputed or not? Yeah, we don't differentiate between what was imputed and what was originally observed. We treat them as identical. And we've also done different things where you can actually replace the originally observed phenotypes with an imputed measurement. Measurement and largely the results are quite similar in terms of at least the GWAS states. Yeah, okay. I think, yeah, Jesse Kardy, yes, yesterday kind of rooted this potential increase of false positives that if the variance is not properly accounted, the variance of imputed Y should be less or equal as if you originally observed. Yeah, absolutely. So that's why we have tried both ways of doing this. And the Ways of doing this TWAS and they're quite consistent, but that is a potential problem. I'm sorry, but yes, you can get the pictures. Okay, so the last part. So we have this imputation framework. It works quite well depending on the types of phenotypes you're interested in. And the question that comes to us from our collaborators who are interested in this phenotype is, it's great to have these new hits, but can you tell us why this method is performing impurely? This method is performing imputation as well as it is. So, they want to understand or interpret this underlying deep learning model. And the reason why they care about this is learning about this kind of interpretation can tell you something about heterogeneity in the underlying phenotype, can point to disease subtypes, for example. So, that led us to think about interpreting these deep learning models. And so, that's the final part. I'll go over it extremely quickly. We don't have We don't have it working yet in the context of our phenotype imputation, but we started thinking about it in the context of single-cell RNA-seq, where these kinds of models have been used. So the idea is you have your single-cell data and you run some two-dimensional method, you get some clusters, and you're trying to interpret what led to this kind of cluster. So you're trying to figure out what aspects of the data led to these clusters. And I'll just tell you the problem formulation won't be. And I'll just tell you the problem formulation, won't spend time on the details. So, the way we think of interpretation in this setting is one of a counterfactual explanation. So, what we want is to identify the key differences between two groups that have been identified in this low-dimensional representation. And for us, an interpretation would be a transformation in the original feature space that causes the model to assign a point that was initially assigned to group A to a point that gets assigned to group B. So we call this. That gets assigned to group B. So we call these counterfactual explanations. We can make it more precise. There's a question of what's a transformation. It could be a translation, rotation. That's problem specific. How do you quantify this? And so we have different metrics. And I won't say too much about it, but it turns out that there are some promising results in terms of ability to capture causal structure over more naive interpretational methods. But I can save that for later discussion. So just to summarize, we So, just to summarize, we're beginning to learn about non-linearities in complex genetic architecture. However, the methods still need to cope with high dimensions, and I think there's much that needs to be done in terms of methodology, both deep learning and otherwise. The deep learning-based phenotype imputation, I think those kinds of ideas could be more broadly applicable beyond just the phenotype imputation setting that we've talked about. And I just mentioned briefly in passing this notion of counterfactual exploitations. With this, I'd like to. With this, I'd like to acknowledge the wonderful students who have done this work and our collaborators and funding. Happy to take any questions at this time. So, one quick question, and then hopefully, we'll leave some time in the chat because I have lots of them. Thank you. Yeah, I have a quick question. So, I think your talk is primarily about common variants with complex traits. So, I just wonder if you can also. So, I just wonder if you can also comment a little bit on relevance in those biobank data. Yeah. Yeah, that's something that we're actively working on. We now have the whole exome and the whole genome sequencing data from UK Biobank. So, there are some interesting issues that are problematic with common variance with imputation. So, when you try to say something about non-linearity, there is always this possibility that the true underlying model is linear, but because you're True underlying model is linear, but because you're missing the causal variance, it appears non-linear. So, all of the results that we present here have to be caveated with those kinds of issues. And those are largely addressed by going to whole genome data and having access to the rare variants. The other thing is, of course, your effect sizes are much stronger. So, that increases confidence in a lot of these types of analyses.