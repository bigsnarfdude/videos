Okay, so now for something completely different from the morning. I'll talk about testing thresholds for Erdogania graphs versus geometric random graphs in the sparse. Versus geometric random graphs in the sparse regime. And this is based on joint work with three fantastic students at Berkeley. So Lucy Ki Liu, Siddhanta Mohanty, and Elizabeth Yang. Yeah, we just put it up on the archive a couple of weeks ago. A couple of days ago. A couple of days ago. Time applies. Time applies. Okay, so we're concerned with the following questions. So I will define for you a distribution over graphs, the G O, D, and P distribution. Okay, so what I do is I start with the sphere in D dimensions. All of the drawings. All of the drawings in this top are not to scale. And, okay, what I do is I sample n random points on the surface of the sphere, v1, v2, et cetera, to the n. And every vertex is going to be identified with one of these points. Okay, and then I'll add the edge ij to e if the inner product of vi The inner product of Vi and Vj exceeds some threshold tau P, where we choose tau p so that the marginal probability of each edge is P. So tau P chosen so that probability PI PJ freedom toffee is key. So, this is the distribution that we are interested in. And this model is very well studied in the case where the dimension d is fixed as the number of points goes to infinity. Okay, and it's studied across many different communities, you know, math, physics, et cetera. And it makes sense to study this model if you're interested in the physical world, because, for example, the Earth is a three-dimensional sphere, and we are nodes. Our uh notes, what, as far as we know. Right. According to popular uh scientific uh theories. And uh okay, and and and so and so in this case it's well studied and uh you know it's the tools that are used to study it in the finite dimensional case are you know you're sort of thinking of a random, like a pretty good random discretization of a finite object. But in But in a paper from 2011, Luke and Georgie and Gabor and Udina, I don't actually know these guys personally, so I'm not using their first names. So they suggest to study the high-dimensional model. So d going to infinity with n. With n. Okay, and the motivation for studying this model is the following. I guess here's at least one motivation for studying this model. So, you know, many times when we see a network in modern data science setting, we think of the nodes as implicitly having some vectored features that they're associated with, and close-by nodes have more similar vectors of features. Okay, so if you take a kind of an ideal Take a kind of idealized model of this, it's this, right? But we think of the feature vectors as being really large, right? So it's natural to take d going to infinity with n. That's one reason to study it. Another reason to study it is this is a really natural model of random graphs. You know, maybe it has all kinds of amazing properties that we don't know about yet in the high-dimensional regime. You know, but I mean we won't know until we try to find out. So that's kind of the the mot motivation for studying this model. The motivation for studying this model. But there haven't been a lot of works yet in this space. So, in this paper, the central question that's being investigated is, what is the clique number of these geometric random graphs? But sort of apropos of everything else, the following question is raised. So, for which D can one fall apart? The geometric random graph in d dimensions from an Irish Randy graph in which we just have n vertices and every edge is presents independently with probability p. Okay, so already in this paper, it's observed that if n is fixed Fixed, and d is taken to infinity, then the two distributions become close in total variation distance. Let me just do a little baby calculation for you so that you can appreciate why this should be the case. Okay, so how should we choose tau p? We want that the probability that u and v exceeds tau to be p. To be P. But taking a Gaussian approximation to this inner product, this quantity is approximately exponential in negative, okay, tau squared times d. There's some constant here, which we could be careful about, but we're not going to be. And so you can see that what you should do is you should set tau to be equal to square. To be equal to square root log 1 over p over the dimension times some other constant. Now, okay, when d is going to infinity independently of n, this is a tiny, tiny number, right? It's going to infinity independently of n. And so if you condition on one vector having an inner product at least tau with a bunch of other vectors, it has almost no effect on that vector's correlation with the other vectors. That vector's correlation with the other vectors. That's the argument. So, okay, so this is the hand-waving argument. And in this paper, I think, you know, taking d maybe double exponential in n or exponential in n squared or something like that is given as a bound. Okay, but this question is kind of left open. And one could even try to pin down the phase transition in D for when this becomes possible to do. Possible to do. Okay, so then a couple years later, in a paper by Seb, Jan Ding, Renan Lun, and Mickey Rotz. They ask the question more explicitly: for which D does the total variation distance The total variation distance between G O and D and P and G and P go to zero with N. Okay, and they come up with a satisfying solution in the case when P is dense. So here are their results. Okay, so Let me actually draw a plot. So here I will take the dimension, okay? And when the dimension is at least order n cubed, I guess theta n cubed, regardless of what p is, the total The total variation goes to zero. So, this is one thing that they showed. And in the dense case, so P, a fixed constant independent of N and D, here there's a test which succeeds with high probability. Okay, and the test is just counting sine triangles. Okay, and the test is just counting sine triangles, so it's a simple hypothesis test. But then, in the sparse case, they had the following result. So they showed that when the dimension is order log cubed of n and p is alpha over n for some for any fixed alpha. For any fixed alpha, the same test succeeds. Okay? But they weren't able to show a better upper bound. I mean, okay, I don't know if they really tried actually maybe because of the. Oh, we tried. You tried? Okay. So there's no matching upper bound, but they conjectured that this is the truth. So here I'll draw this as a dashed line. So this is the conjecture. This is a conjecture. Okay. Uh right. So in in our main result is uh the following. So if you if you let me take log to the 36 n, then uh here we show that if p is theta one, or sorry, p is alpha over n. P is alpha over n, the T v goes to zero. So up to Paul logarithmic factors, we resolve the conjecture of 7 et al. We also have some results in the denser regime. So, okay, for any P, okay, so I guess maybe I'll just write the theorem for a dramatic effect. Okay, so here's our main theorem. If uh P is over N for the first time Okay, so so this is only for the sparse case, but also in the dense case we have another result which is not as tight. So we show that if d Uh d is at least p squared m cubed times some poly logarithmic factors. Then also the total variation distance goes to zero. Okay, so in particular you can see that if you plug in p as a constant over n, you're not going to recover a bound, you only recover linear. So this is a Uh so this is a this is not a special case of this result. Okay, and uh you know you can be tempted to say maybe the right answer should be p cubed n cubed for the whole range because that would be consistent with the dense case and the sparse case. And I think now that we have this, okay, admittedly not log cubed, but maybe it's more reasonable to conjecture this for the full range of p. I should mention also that the previous best bound is due to Brennan, Bressler, and Nagaraj. I think the year of the paper is 20. And for them in the P is alpha over n regime, they get the total variation distance going to zero as long as D is larger than n to the three halves oil over n. Okay, so we improve on this. Okay, so we improve on this bound exponentially. And maybe you want to say you have this conjecture with the entropy. Right. Okay, so actually if you look at this test and you analyze the sine triangle to count test and you see up to where it works, so the sine triangle test Works when D is less than N times the entropy of P cubed. Okay, so notice that this will recover both the dense and the sparse results. And the natural conjecture would be to say that whenever d is larger than Is larger than n times entropy of p cube, then you can't test. Yeah, I mean, this is really great, this formulation, because I mean, the n cube, the cube in the n cube, we understand where it comes from. Maybe, you know, when you put it there, crocodiles, I don't know. Right. Yeah, I think I like this passage. That's cool. Yeah, I guess this. Cool. Yeah, I guess this formulation, it's like, you know, if there's a part in the proof where you look at it and you're like, if only things could go a little bit better here, this is what we would get. So this is why we thought of it this way. Okay, so let me, yeah, I was gonna, that's good, I only spent, no, did I spend, how many minutes did I spend? Like 10 minutes? So I'm gonna say I only spent 10 minutes, which means that I can spend the rest of the time telling you about That I can spend the rest of the time telling you about the way that the proof goes at a sort of high level. Okay, so so first of all, maybe I'll say something about this uh previous best upper bound, this one in the sparse case. This one in the sparse case. So, what they need, the property that they exploit is that if you sample v1, you know, v2, et cetera, all the way to Vn minus 1, and now you look at Vn, then even conditioned on Vn having inner product at least tau p or less than tau p with all of these vectors, there's still enough randomness left over for Vn in order to make. For Vn, in order to make its adjacency to Vn minus 1 not depend on everything that we saw by Doni Match. Okay, so basically what this requires is that the projection of Vn into spin V1 up to Vn minus 1 is pretty small. Okay, and like, you know, there's a, I mean, this is a very sophisticated argument, right? Like, you know, they're carrying out with a lot of sophistication. The magnitude of this O1 matters a lot. I'm not going to go into any of that, but it just suffices it to say that, you know, at minimum, in order for such an argument to work out, you would need that the dimension exceeds n, right? Because otherwise, this span is going to be the whole space, and Vn is going to be totally determined by everything that you see so far. Determined by everything that you see so far. So, you know, that's just to illustrate that we're doing something very different, and hopefully, in the course of this brief overview, that will become clear. It will become clear what difference what we're doing differently. Okay, so the proof outline. Outline is like this. So the first step, which I'll call step zero because it's not very interesting, is as follows. So we use a bunch of information theoretic inequalities like Prince Gruz inequality, chain rule of entropy, et cetera, to relate the T V to the To the last uh ve vertice vertex's neighbor distribution. Okay, so uh what do I mean? I mean we show that uh it's enough to to bound the following quantity. So if uh for g G n minus 1. Here I mean I'm only going to sample n minus 1 of the vertices from the geometric random graph. And a random vertex, or a random vector sampled on the unit sphere, I have that the probability that the neighborhood The neighborhood of the nth guy is equal to s is close to what it would be in GNP, which is, you know, p to the size of s, 1 minus p to the n minus 1 minus the size of s for all s, okay, of reasonable size. Meaning, like, we don't really have to consider s, which are, you know, like way larger than the expected degree. Uh, then this will imply that the total variation distance is at most epsilon squared times n. Sorry, is ge minus 1 geometric with n minus 1 vertices or n vertices? I guess that I mean the marginal distribution on the n minus 1 vertices. One vertices. And the reason that it matters is because otherwise I would have to choose the parameter p a little bit differently. Yeah. Yeah. Yeah, good catch. I hope to not get into it. Okay, so this is what we want to show. We want to show that these probabilities concentrate well. Okay, so what is this probability? Well, if I sample V1 up to Vn minus 1, okay, the probability that the neighborhood of Vn is equal to some specific set S is equal to the measure under the uniform distribution on the sphere, which I will denote. On the sphere, which I will denote by rho, of the intersection of the p caps of the vectors corresponding to vertices in S intersected with the anticaps of the rest. Okay, so really, like, every time I place, you know, let's say that, you know, V1, V2, and V3 are in the set S, right, then I'm looking at the capron V1, the capron V2, the caparound V3, okay, this area, and then intersected with the anti-caps of all of the other vectors. So really what I want to know is I want to know how well do these areas I want to know how well do these areas concentrate around their expectations. Okay, well it's a shame to erase this picture, but I guess I don't have enough space, so maybe I'll just. You remember the theorem. Yeah. You remember the theorem? Okay, so then the second step, maybe I'll just do this, which is actually really the foundation of everything, is the following lemma. It says that if mu is a measure, probably a measure, over the sphere. Over the sphere. Then with high probability over a random vector sampled from the sphere, from the uniform measure on the sphere, the measure of the cap around this vector z will concentrate really well around p. P. So this is at most some constant times square root, okay, polylog. Actually, let me be a little bit more precise here. So log 1 over p plus log of the maximum value. Of the maximum value of the relative density of this measure over the dimension. Okay, and then here there's a polylog n. All right, so let's unpack this for a second. Right, so, okay, if I have the uniform measure on the sphere, and I take a random PCAP, the measure of this pcap is going to be exactly p. Okay, on the other hand, and it concentrates extremely well in the sense that the variance. Concentrates extremely well in the sense that the variance is just literally zero. On the other hand, suppose that I have a single point, a point mass on the sphere, right? So whether or not this point mass falls inside the cap is a, this is just a Bernoulli p variable. And okay, the variance there is of order p, right? And so this is not the kind of concentration you'd expect there. But that's why there's this log of the maximum. Of the maximum value of the relative density term, right? So, this is kind of saying, like, as long as you have a measure whose relative density doesn't take two large values compared to the uniform distribution, then the measure of this is going to be of the order p times epsilon. This is, I'm calling this epsilon, which is of order square root of poly log n. Okay, I guess here. N, okay, I guess here there's the log of mu infinity over V. Cool. So we have this concentration inequality. Actually, you know, I say this is like a high probability result, but we also get sub-Gaussian tailbounds, which we will need. Okay, so. Okay, so, oh, and the proof is actually pretty cool. Like, it uses optimal transport. As far as I know, no one ever needed to prove anything like this, but I think it's a new application of optimal transport. All right, so now already, this is true for like, okay, so now I'll apply this iteratively a little bit, and I'll already get the p squared n result. N result. Okay, so how does it work? So I start with a uniform measure of the sphere, right? I take an intersection with the P cap, I get measure P. Now I take another intersection with the P cap. So you take intersection of the caps? Right, remember that this is what I'm interested in, right? So I have some set S. Yes. The size of S is like a constant, because I only have to care about reasonable size. Of reasonable size. Now I'm going to look at the measure of the first cap that I put down under the uniform measure. This is just P. Okay, now I will repeat the same argument, but my new measure will be the uniform distribution over V1. Okay, and now again I'm going to take an intersection with the cap around V2. Okay, now I'll get P squared, et cetera. Then I'll get P cubed. Every time I'll have a Every time I'll have up to 1 plus or minus epsilon. Right? So, okay, ignoring what happens with the anti-caps, I don't want to discuss that right now. So, what we already have is that this is going to be like p times 1 plus minus epsilon, okay, to the size of s, times a term that I told you to ignore. How we deal with it. And if s is a constant and epsilon is like this, then And epsilon is like this, then you can see that, and our total variation bound is epsilon squared n. Okay, so you can see that this is, this is, we're bounding this by polylog n over d times n. And so if d is at least n poly log n, and you're done. So this recovers the p squared n cubed result. And it also gets you to linear dimension, right? So this was like this barrier that I had discussed previously. That I had discussed previously. Okay, now what's the problem? So, okay, we're not at the added polylog n dimensions. And you could say, well, maybe your concentration inequality isn't tight. Maybe you can get even better concentration and have epsilon be order 1 over square root of n. But actually, I mean, we didn't write this down anywhere, but you can argue that there's matching anti-concentration. Anti-concentration. So this is an upper bound on the magnitude of this deviation with high probability. But also, you'll get a deviation that's at least this over, say, log n, also with high probability. Or I guess poly log n. So you can prove a T-mount transport, the T-munctional transport? No, I think it's just, I think you just, like, maybe, maybe we could prove it through optimal transport. But basically the idea is you just wiggle the cap a little bit. The cap a little bit. Yeah. Okay. Right. So that means that we can't hope to improve this. And therefore... What is not tight? Sorry here. I'm not confused. What's not tight? I'll tell you. So here, we just asked for g sampled from this distribution without fixing a vector visualization of it. So now what we have to do. Of it. So now what we have to do is we have to sample this graph and then average over vector realizations of the graph. So here this whole time there was like a fixed set of n minus 1 vectors that we were working with. Okay, these give you g n minus 1. But now what we have to do is we have to go and resample new vectors v1 up to vn minus 1 prime that give the same. That give the same graph on n-minus 1 vertices, and okay, and then look at this probability on average over the realizations. And then we get much better concentration. Okay, I got run off, but I was going to say maybe like one more thing about how we do this averaging thing. I don't know. What do you guys think? Okay. Wow, that's uh that's great. Okay, so okay, so how do we do it? Okay, so how do we do it? Okay, so what we do is we do the following. So we use like a version of the cavity method from statistical physics, which is something that some of you know much more about than I do. But okay, so here's g n minus 1. One. And let's look at the vertices that we said were an S, right? So we said that, you know, V1 was an S, and V2 was an S, and V3 was an S. By the way, this is not a drawing of the sphere. This is a drawing of the graph as a blob. Okay. Now what we're going to do is we're going to take a ball of radius L around each of these vertices. And if these balls are disjoint and also locally tree-like, okay, then what we can do is we can condition on an arbitrary fixing of the vertex boundary. So we'll take the vertices on the boundary and we'll give them some vectors, you know, like u, i1, etc. Which can be which Which can be, which will be essentially arbitrary. I mean, you know, they'll have the properties that vectors in d-dimension should have, like, founded inner product, etc. But, but they'll be essentially arbitrary. And then we use a belief propagation to compute the marginals on a On the Vi from S, given the fixing of the boundaries. Okay, so belief propagation, I guess it's an algorithm which computes, you know, probabilities of assignments for constraint satisfaction problems. So here, we have a constraint satisfaction problem. The vectors themselves are the labels that every variable gets. The labels that every variable gets. And the constraints are just that inner products have to be at least Ï„p. Or in other words, the constraint is that if i and j are neighbors, then they have to fall within each other's spherical caps. And at the boundary, we have the same constraint, but these vectors are truly fixed. Okay, and once we use belief propagation to compute the marginals on these On these variables, then we know how correlated they are and everything, right? So we get much tighter control over this quantity in that way. Now, it sounds crazy that I'm saying this, but it actually ends up being miraculously not so bad because if you think about what a message is, right, so if I'm at vertex i and I'm sending a message. And I'm sending a message to vertex J. So what this is saying is it will pass to J the guess for where it is, which is convolve I's measure with a spherical cap. But maybe you can sort of see that this lemma that we have here is all about how well this should concentrate. It's all about the convolution of spherical caps with measures. So, you know, using these exact same concentration statements, we're able to analyze. Statement, we're able to analyze these belief propagation algorithms. And as long as this radius is at least logarithmic in n, or logarithm over log log n, then our error will decay like epsilon to the L, which will be like, you know, polylog. And And over okay, DN square root, something like this. Just because we're able to take balls that are disjoint trees of sufficiently large depth. Okay, so in order to say this, we have to prove some weaker coupling of these graphs with Erdograini graphs to show. It's just some kind of stochastic dominance argument, which isn't enough to conclude the total variation distance is small, but at least it's enough to give us local tree-likeness of these graphs. Local true likeness of these graphs. Okay, and then like you know, many dottings of I's and crossings of T's later, we get our theorem. So, yeah, that's all. Beautiful. Yeah, it's incredible. I mean, also the uh D of order N is easy. Of older N is easy, kind of. Yeah, it's pretty easy. Right. You deserve dinner.