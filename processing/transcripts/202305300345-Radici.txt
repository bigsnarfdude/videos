They're in the folder that says, yeah, it's the third to last from the bottom. It has your name on it. Okay. Oh, is this correct? Looks like so. You said this one is the pointer and this one is for the linear division to non-linear stars. Yeah, it's on. So this button does the next one. The one in the top. And then the other one is the bottom side. Okay, so proceed, please. Okay. So first of all, let me thank the organizers for the kind invitation. I'm very happy that I have the chance to speak at this nice conference. And I'm also very happy. And I'm also very happy to speak about this result, which is based on a recent collaboration with Elio Marconi from the University of Latvia and Federico Stra from Politecnico di Torino. We started working... Pardo, did I see some... Sorry. Marcoli is in Padua and Federico Stra is in Turin. We started working on this when we were all together in Lausanne, but then we. In Lausanne, but then we soon moved in these three different places, so this is an itinerant collaboration, let's say. So, I would like to start by telling you what was our motivation to study this stability problem. Then we will see what was the literature available around this and how our result fits in the picture. Then we will go through the rigorous statement of the stability theorem and then we will see some applications of it. Like, let's say some are classical, some others. Let's say some are classical, some others less. And then I would like to conclude by telling you some open possibilities for improvement of our results. Okay, so we mainly had two motivations. The first one is more applied, and it has already been mentioned yesterday, and we talked before mine, how important it is to know the rate of convergence of approximating schemes. So, in general, when you want to compute explicitly the exact solution of a PDE, this is very difficult. Solution of a PDE, this is very difficult. So, what you do is you use some numerical methods to approximate the value and you measure the efficiency of your numerical method through the rate of convergence. So, it's useful to know this, how good is the rate in general when you approximate something. And so, we wanted a stability theorem that was so flexible to be used also not only with exact solutions of your conservation law, but also with approximate solutions. With approximate solutions, so that it would be easier to derive explicitly the rate of convergence of the approximating scheme. The second motivation is that we wanted to study the uniqueness of an entropy solution, so then I'm going to say what they are, for non-local scatter conservation laws. So conservation laws where the flux depends non-locally on the solution itself. And let's say one classical way to get the uniqueness is very roughly speaking. Is very roughly speaking the following. You argue by contradiction, you assume that you have two solutions from the same initial datum and you want to study some norm of their difference and see how this difference evolves in time. So you kind of want to see what is the derivative in time of this difference. And since those two are solutions of your PD or your conservation law, you hope to bound this quantity by some norm of the difference of the Some norm of the difference of the flux evaluated at the two different solutions. And if the flux well behaves in the sense that you have this sort of Lipschitz estimate, some stability on this flux, then you can close a Grommal argument and say that, okay, the difference at mt is always controlled by that at time zero times the sum exponential in time. But if you start from the initial determin, that you get the two solutions coincide for every time. For us now, the flux depends non-local. For us now, the flux depends non-locally on the solution itself. So, let's say this argument becomes the same as studying two different problems. So, two different equations with two fluxes, where the first flux is the one where the non-locality is fixed in the first solution. The second one is the equation where the non-locality is fixed in the second solution. So, what you want is to study something of this form. So, where you have the distance between the two different fluxes in the two solutions. So, we want to deal with two. So, we want to deal with two distinct fluxes, which has to depend explicitly also from the time and the space variables on top of the density one, which was the famous result of Khrushchev that I'm going to mention anyway. So, okay, in this first slide, I already wrote these words. So, entropy solutions and quasi-entropy solutions, what they are, they are the main ingredients of this talk. Entropy solutions, let's say, are Let's say, are a special subclass of distributional solutions of your conservation law. And they are very important, especially when you don't have uniqueness of the distributional solutions. And they are characterized by the fact that they are somehow contractive in L1 with respect to constants. And this concept is expressed rigorously through this inequality, which is the famous Khrushchev entropy inequality, which has to be understood in a distributional sense. A distributional sense, and so it must be true in tests with smooth functions with compact support which are non-negative, and it must hold for every non-negative constant. And a quasi-entropy solution is a perturbation of this condition. So you allow for some positivity, but as small as you want. Now, the history of the stability theory for entropy solutions was initiated by the pioneering work of Khrushchev in the 70s, where he proved the stability of Prove the stability of exact entropy solutions, so the first inequality here, with respect to perturbations of the initial data. Okay, so he has one flux, the same equation, two different initial data, entropy solutions which are L infinity in space. Then in the seven, in 76, I think Kutsnetsov, let's say, proved another stability result, which is more flexible in the sense that he allows for perturbations. He allows for perturbations not only on the initial datum, but also on the entropy condition. So he allows for quasi-entropy solutions still in an infinity, but now the flux has to be depend only from the density variable. Then in 1998, Bouchuel Pertam, they improved a lot the result of Kutzensov, allowing for a notion of quasi-entropy solution with a way more general error term here, derivatives of measures. Derivatives of measures, so really super weird. But we have to wait until 2003, the result of Carson Rezidro, to have the first stability result dealing with perturbations of the flux. So they allow for two distinct fluxes. They work with exact entropy solutions, this time in Bv. So the price to pay to do the stability for two fluxes is to restrict to solutions which are not just L infinity, but also B V. And in their case, BV. And in their case, the flux could depend explicitly also on the space variable, but with this precise product form. Then Renaldo Colombo, Magari Mercier and Mazigliano Rossini proved the stability still for exact entropy in BV for the general case, but under some regularity assumptions on the flux, which were in some sense too strong for the applications we had in mind with Marconi and STRA. Connectra. So we ended up reproving the result, and we were able to extend it also with respect to perturbations of the entropy condition. We allow for quasi-entropy, okay, and to fluxes. Okay, so more precisely, what is in our case a quasi-entropy solution is a function and a negative function which is in this space, which is nothing strange. This space is the first intersection, let's say, is the space of. Section, let's say, is the space of Khrushchev. On top of it, we need to ask for BB regularity. So, this space is exactly the one considered by Carson-Risebro and Colonel Bono. We have the same. And a non-negative function in S for us is a quasi-entropy solution of the conservation law if it satisfies the entropy inequality with an error term that has this precise form. So, it's characterized by two measures, which can be disintegrated, and the first And the first marginal, the one with respect to time, has to be absolutely continuous with respect to the bag. And we have this sum of two terms where the first one is the mu zero mass of the test function, and the second one is the mu1 mass of the space derivative of the test function. Okay, so a precise structure, if you want. And for us, the flux is the flux and the divergence in the. The flux and the divergence in the second component is one, and it has to be Lipschitz with locally Lipschitz with respect to the third component, uniformly locally in space and with Lipschitz constant in time. So whenever you have two values in this compact interval and the space variable in a compact set, then you have this Lipschitz inequality where the constant is some L1 function. This holds also for the divergence. Function, this also for the divergence. And we have to require this third condition, which is here in red, because we need this third assumption only because we want to deal with quasi-entropy solutions. If we restrict the stability analysis to exact entropy solutions, we don't need this. And this has to deal with the gradients of the divergence of the flux and the gradient of the derivative in the self-component. Derivative in the third component, we have to ask that on compact sets for the space and the density variable, these L infinity norms of the gradient of the divergence and the gradient of the third derivative, they are L1 log functions in time. And just to make the comparison, in the Colombo, Mercier, Rossini, they have to assume that these guys are C1 in time. And the second work of Mercier, she was able to relax the C1 to continuous. For us, it's enough L1, and also for us. L1 and also for us, this is needed only for quasi-entropy. They had exact entropy. In our case, for exact entropy, we don't need this assumption at all. So, this is an improvement, if you want, in the assumptions. And what's our result? Let me already say this slide is false, but I explained first this one. We have two fluxes under the assumptions that I just mentioned, and two quasi-entropy solutions for the concept. Solutions for the conservation laws, respectively, with the fluxes, for some choice of the measures in the error term. What we can prove is that the L1 distance between the two functions at time t minus the L1 distance of the two solutions at an earlier time that here for convenience is zero, is bounded by the integral of the same quantity times some function which is bounded by the assumptions plus The assumptions plus this term, which is measuring somehow a distance between the two fluxes, times another function which is bounded by the bibliomans of the solutions, plus an error. And this error is measured by the masses of the measures of the error terms in the perturbation of the entropy condition. Okay, and here is okay, this normal star norm here measures some norms of the derivatives of the differences of the flux. Derivatives of the differences of the flux. Okay, I said that this result is false because it's actually true for solutions with compact support. Okay, for quasi-entropy solutions with compact support. Otherwise, the true version of the result is this one. We have to localize everything. So we have to choose an a priori test function that here I call theta, which is, if you want, a sort of regularized version of the indicator function of a cone, which is shrinking sufficiently fast with respect to the. Fast with respect to the fluxes. We have this slope condition here. And we prove the same inequality, but localized on the support of this theta, where also all these bounded functions and the masses of the measures are meant on the support of. And this is not yet the very rigorous statement, but I'm not going to write that one because I'm cheating here. I'm localizing also on the bounds, on the density of the solutions. Of the solutions, okay, because they're only an infinity lock. So here it's hidden also this localization, but it's technical and not important for the purpose. Okay, so now that you have this, where is that? This monster, what you do is try to understand if you can close a Gorombal estimate when P and Q are close enough. Also, this is what you can do. Okay. Sorry. Okay, if you can say if you know that your fluxes are close in this sense, so the divergence of the difference and the third derivative, sorry, the derivative in the third component of the difference can be bounded by the difference of the two solutions on the support of your test function. Then you get this. Function, then you get this inequality from the stability one that I mentioned before. And then Gromba tells you: okay, their difference at time t on the support of theta is controlled by the difference at time zero plus these error terms that comes from the error measures, if you want, and this exponential determination. Okay, and now this inequality here has many applications, like the easy one is that you have conditional existence of entropy solutions. Existence of entropy solutions in the sense that if you are given a sequence of quasi-entropy solutions, from like you know it by chance, God gives you this sequence of quasi-entropy solutions for the conservation law, local or non-local. And the error measures have masses which are vanishing when k goes to plus infinity. And you are also well approximated, like the initial data for the quasi-solution. Initial data for the quasi-solutions are a Cauchy sequence in L1, so they converge to something in L1. Then, from the Gromval argument before, then you can get this. So you get basically a Cauchy sequence in L1 for every time, because their distance at time t is controlled by the distance at time zero, which is going to zero, plus the errors that come from the masses of the error measures, but the masses are going to zero by assumption. To zero by assumption. This is bounded in k, so what you get is a limit in L1 for every time. And this limit is satisfying actually a clean entropy condition because the right-hand side is going to zero thanks to this. Okay, so you had an exact entropy solution for the conservation row where the initial data is the limit of the initial data of the approximation. Another easy consequence of Consequence of the Grambal estimate is that you get uniqueness of exact entropy solutions because if you start from two different solutions of the same equation with the same initial datum, you're assuming that you have two different, and you have good control on the flux in the sense that your Gromval argument is satisfied in the case of P that is local. Local, then the Gromba tells you directly this because you don't have the error terms, you don't have the error masses, they are all zero. Okay, so if you start from the initial, the same initial data, this means that you coincide on the support of theta and you can make the support of theta as big as you want. Okay, and then the other application is that we can derive the convergence rates for the approximating schemes which are producing solutions which are quasi-entropic. Producing solutions which are quasi-entropic in our sensor. Okay, so they are continuous in time and satisfying the quasi-entropy condition with these measures in the end. And we can compute some classical already known rates of convergences for the vanishing viscosity and the front tracking. Here we're actually improving a little bit the result because what was known before was the rate for the flux that depends only on the density variable. On the density variable, while we also get it for more general fluxes that also depend on space and time, from tracking, we get exactly what was known before. And what is new is that we derive for the first time the rate of convergence for a deterministic particle scheme that I used to work with, but so I'm pretty happy of this, and that is used in general to study non-local scalar consequences congestion and With congestion, and here is an example of one-dimensional local standard conservation law with congestion that can be rephrased more or less in the spirit of the previous talk, because since we are in one dimension, this is always a derivative of something. So, you can write this expressing the derivative here in front, and you have that this product here is the new linear mobility, if you want. And I said that the word congestion. And I said the word congestion before because this term V here is a decreasing function of the density. So it slows down the evolution for high values of the density. And for us, like the prototype example you can have in mind is the one that is used in chemotaxis is the positive part of one minus rho or something that anyway decreases faster. And we studied this equation in a recent work with Fédéric Boustin and actually parts of this equation. Actually, parts of these equations under different regularity assumptions were already studied in this series of papers, which involve also people in this room. But I chose this one, which is the last and more general. Okay, and what we wanted to, we did in this series of paper was to study the many particle limit of it. But you see that the presence of this velocity term that regulates the congestion, since it depends locally on the density. Depends locally on the density, it tells you that you cannot associate Dirac masses to the particles. Once you have your set of moving particles, you cannot use the standard empirical measures because you cannot evaluate V at the distribution of particles. And also, the least compactness you need to pass to the limit in this equation is L1. Okay, so you need a way to approximate the macroscopic density in L1. And the idea is that once you are given Once you are given somehow your set of particles, you construct piecewise constant approximations of the density in this way. So the approximations are exactly constant in the interval between two consecutive particles for each pair of consecutive particles. And the constant is chosen in such a way to have fixed mass, one over n, there. Okay, so in the end, you have a probability measure. And the good choice for the set of A good choice for the set of moving particles is this one. So the trajectories given by this system of ODEs, where the velocity field is given by the product of what, okay, this term comes from a suitable discretization of the potentials. Okay, and this one is instead the one related to the velocity term. So it is the one that controls the congestion. So if the particle xi, so sorry, if the potentials are pushing. If the potentials are pushing the particle XI to look forward, since we are in 1D, you have only two options. So, either going forward or backward. So, if the potentials are pushing forward, then this term in front should look at the density that the particle xi sees forward. So, it has to look at the particle xi plus one. So, you evaluate the velocity term with respect to the density that looks x x i plus one. That looks xi plus one. Otherwise, you do the other thing. So, if the potentials are pushing backward, you look xi minus one. And basically, with this choice of deterministic particles and with this piecewise constant reconstruction, if the initial datum is has mass one, is with bounded variation, is in L infinity with compact support, then you can prove that the P Swiss constant approximations are. Pisces constant approximations are in the good space S that I mentioned at the beginning, and that they are also well approximating the initial detail. And you can prove that they are compactly supported and that they are quasi-entropy solutions for this choice of measures. So for mu zero is zero, and mu1 is always bounded by one over the number of particles times some bounded function of time. Okay, and so the stability theorem tells you that okay, up to choose the number. That, okay, up to choose the number of particles big enough. It tells you that the L1 difference, the L1 distance between two piecewise constant approximations, is controlled by these two guys, up to bounded things in time. So the way how you approximated the initial data, data, and square root of one over n. And so the rate of convergence is at most the worst between these two guys. Between these two guys, and what we can say is that actually square root of one over n is sharp. We don't know whether this guy here is sharp or not. This comes from our stability theorem, but maybe there is better, you can do it in a better way and get something nicer here. But what we can say is that we cannot do better than square root of one over n in the approximation of the initial distance. With Federico, sorry, okay, with Federico. Okay, with Federico, we proved that any initial datum with compact support bounded and with bounded variation mass one, we can approximate it with piecewise constants with this rate of convergence. But with alio, we said that we cannot do any better. So as soon as you have a positive sequence that is faster than square root of one over n, then you can cook up an initial datum. Can cook up an initial datum satisfying these assumptions so that for any sequence of ordered particles, the corresponding piece was constant is approximating the initial detones lower than your sequence. Okay? Only as soon as this sequence is slightly better than square root of one over m. So, square root of one over m is sharp. And I would like to conclude by telling you this. Telling you these open problems, let's say. Okay, the first thing we would like to do is to allow for more general error terms in the perturbation of the entropy inequality. So especially we don't want to ask for absolute continuity with respect to the back for the time variable. So that maybe we can allow also for schemes which are discrete in time and treat the rate of convergences for finite difference. Is for finite differences, finite volumes. Go on, you know, these peaks, maybe even JK Yoke. I don't know. Then we would like to deal with more general evolution equations. So we want to add some source and some diffusion term. And let me, we are working on this. I have a spoiler. Like putting the source term is for free. We could have done it already, but we were lazy. We discovered it at the end. Putting the diffusion is not easy at all in the sense that this. Not easy at all in the sense that this would be easy if you consider a quasi-solution in H1. We don't want to impose such a strong condition on the quasi-solutions. On the solution, exact solution, okay, but on the quasi-solution, no, because then we cannot, for example, apply this to the particle schemes because they are piecewise constants, so there's no way to have an H1 control. And clearly, then, but this has to be proven with completely. Then, but this has to be proven with completely different techniques, and one would like to prove the non-conditional existence of entropy solutions. So, find a way how to define how to find a sequence of quasi-entropy solutions for the general flux. Okay, with this, I conclude and I thank you. Any further questions? Okay, sure. Okay, um okay. Thanks for the nice talk. Uh so the first question I hope the answer can be made. Um so if I count the derivatives correctly, there is like some restriction on the type of potential you can use. Yes, because this will be in your entropy solution. You need to control some derivative. Yes, if you have the W, what you need to do. The non-local interaction, I can take the non-local interaction leaf sheet in the origin. So I can have a jump in the derivative, but no more than. A jump in the derivative, but no more than that. Yeah, like Coulomba cannot, for example. So, in this scheme where you're moving the integrals, have you considered whether they are compatible in some sense with the gradient flow structures? Yes, it's a work in practice and the super hard. Yeah, we hope we hope. Hope, yes. Let's say the fact that you are going towards the entropy, so you have this inequality given by the entropy, you would like to say somehow that this inequality is similar to the minimization that you get in the JPN somehow. But we don't know if it's true. We hope, yes. Yeah, thanks for the interesting question. Interesting question, but I don't know. I cannot even answer you. And then I'm going to see what after this. So if you have some growth condition on your D, then you would expect that the solution has some, you have some control on the moment, maybe. Yeah. So what is missing from removing your localizing function beta? Can this be relaxed by saying you have something on the GP? By saying you have something on the DK, yeah, let's see. If you can prove that you have compact support, you don't need the GPS. Yeah, I think you can say something. I don't know in which fashion you can say, but yeah, I think so. I will think about it. Thanks. Okay, so the second thing is saying that, okay, so you have some when you want to do the fuser things, you have to control a little bit through the fuser setting, right? You need to control some H norms, semi-norms. But if you do instead of P size constant, P4 is linear, I think when you do your calculation, you show propagation of DD. And DD means you control shifts. And if you have shifts, then you know that P2 is linear and P switch constant are. That piecewise linear and piecewise constant are the same up to the size of the. Totally agree, but I don't know how to do the piecewise linear. First of all, you lose immediately the fact that you keep mass 1 over m between the particles, because if you impose this condition, you can have the piecewise linear approximation that becomes negative somewhere. And so there should be a fancy way how to define the piecewise linear. We're trying this, but computations are crazy. So it may be the good. So it may be the good path to get the result for the diffusion. If you want, we can discuss about it. I will not ask. You said you wanted to put in diffusions and you don't like to go to H minus one. No, to H1. H1 alone. There. The idea is that diffusions usually work in L1 because you don't need any other. I mean, normally L1 works. Normally, L1 or majors are the normal space provisions. In something, Khrushchev also did things in L1. Yes, yes, yes. Okay, okay, sorry. I had in mind the comparison with Carson and Risebro because they have also the diffusion term, but they need to ask for the exact solutions, in that case to pH one, because then there is a way how to recover what is positive, the positive quantity in the entropy condition. Positive quantity in the entropy condition, which is actually due to Jose Antonio. When you write down the entropy condition, you say that, okay, this guy is positive, but how much is positive is a term that you don't generally that does not appear in the quantity that you have positive. You have to recover it through a nice result that has Antonio. And for that, you need to move derivatives from the test function to the Derivatives from the test function to the Laplacian, and so you need at least one derivative you need to move, and so you need to see how it is done in non-linear post-medium. So you can get estimates of positivity and everything else, beginning with a one, and then the regularity will allow you to do the things for t positive. Okay, okay, I agree, yeah, but also for. Yeah, but also for quasi-solutions, because of the exact solution, I definitely agree. But for quasi-solution, you're going to get to our