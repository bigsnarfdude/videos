Next, I'm actually not going to talk about radar exactly. It's connected, but this is the problem of passive source localization. So the idea here is we have some receivers, so platforms carrying some antennas that are just listening. They're not transmitting anything. That's what's happening is the energy. Is the energy being transmitted from other transmitters. And we're trying to, these receivers are trying to figure out where are these transmitters. And we'd also actually like to figure out what are these transmitters doing with the waves that they're transmitting? Are they radars? Are they radio stations? Are they cell phones? What's going on? And so this is a challenging problem, as we'll see. So why would we want to do this? Why would we want to do this? Well, there are lots of reasons, you know, locating emergency beacons. There's a shot spotter system that has acoustic sensors around in cities that listens. And when gunshots are fired, it sends information to the police about where those gunshots were fired, air and ship traffic management. I'm working with the Navy, so we're thinking about the problem of trying to find radars. And then nowadays we have drones. Nowadays, we have drones, so that means that drones could fly around and listen to all sorts of stuff. Okay, so here's a first, I'm going to talk about sort of a simple model and talk about sort of the intuition behind some of what we're doing here. So let's imagine we have a source here that's transmitting some waveform. And typically, especially for radars, the waveform has the form, it's a slowly varying envelope. Slowly varying envelope on top of a carrier, a wave that's oscillating at a high carrier frequency, which I'm going to call capital omega on this slide. I probably change notation on other slides. Anyway, so then what do these triangles are supposed to be receivers and they might be moving? And so what do those receivers measure? So here's on the jth receiver, you end up measuring something like this. You have the envelope that's tied. The envelope that's time delayed by some, uh, well, how much it's time delayed depends on where the receiver is relative to the source. And then that because the source, because the receiver is moving, the the wave is also Doppler shifted by some amount that depends also on where the on the on the geometry. And then, of course, there's noise, and then there's an attenuation factor that depends that that's like a geometrical spreading factor. And if there are Spreading factor, and if there are complicated propagation channels, it could be affected by that. So, the challenge is from those measurements, how do we figure out where something is? So, one of the key notions here is the time difference of arrival. So, the problem, let me just go back here. So, the problem is we don't know the waveform that's being transmitted, and we don't know this frequency. So, basically, we may measure this waveform. We may measure this waveform, this signal, but we don't know any of this stuff. We don't know how it split up into the waveform and the time delay. In particular, we don't know when this waveform was when it was transmitted. And so we don't know, we can't figure out this time delay. And we can't figure out, we don't know anything. But what you can do is you can look at differences between the signals measured on different receivers. In particular, if you can measure the difference in the time delay. The difference in the time delays, and I'll say a little bit about how you actually do that in a couple of slides. You can look at the time delay between when the signals were measured. And it turns out that since the time delay is the distance divided by the speed, so here gamma is supposed to be the position of the receiver, and the X is supposed to be the location of the transmitter. So this is the distance. So, this is the distance divided by speed. Speed of light here is the time delay. And if you look at the differences between these, well, we all recognize that if you're looking at the differences between distances, distances relative to two foci, at least on a plane, what you get is a hyperbola, right? We all teach that in our elementary math classes. And so, so this set of points x. The set of points X for which you get a given time difference of arrival, or TDOA, is hyperbola in this simple geometry. So for this, we would have to have the clocks on the receivers synchronized. And this works fine if you have a single source. Actually, there are some, you know, so you can imagine if you had multiple pairs of sensors, you would get multiple different hyperbolas. Would get multiple different hyperbolas, and you could locate where the transmitter is just from having lots of receivers. And there are actually some commercial systems that work like this. So how would you actually get the time difference of arrival if you don't know the waveform? Well, we'll assume, let's just look at the case of fixed receivers and fixed transmitter at the moment. So on one receiver, you would receive a Receiver, you would receive a wave with one time delay, and on the other receiver, you'd receive a wave with a different time delay. And then what you can do is take the cross-correlation of those two signals. And if you just plug in what these signals are, you get this expression. And if you just make a change of variables from t to let t go to t minus t1, you will see that from the Cauchy-Schwarz inequality, this quantity. Equality, this quantity, this cross-correlation will have a maximum magnitude exactly at this time difference arrival TDOA. And so I've plotted a little, in fact, this is actually a MATLAB simulation of these signals. You get a peak happening at the TDOA. So you can actually, if you just measure the signals, you can get the TDOA. And this is just a plot of, you know, if you have different values of the TDOA, you get different hyperbolas. Get different hyperbolas. And if you're in three space, you have hyperboloids instead of hyperbolas. Okay, so, but then you can also get some information from the fact that your receivers are moving. And so this is a very nice diagram from Kimberly Hale's dissertation: that, you know, suppose you have one receiver that's moving towards the source, then it will, whatever frequency this source is. Frequency, this source is transmitting, the receiver that's moving towards it will see a higher frequency. This Doppler shifted to make it a higher frequency, and a receiver that's moving away will see a lower frequency. And so if you look at the difference of those, if you write out what the Doppler shifts are, they're the velocity of the receiver. We'll assume the source is stationary. The velocity of the receiver in the direction. In the direction, the hat is supposed to be a unit vector, denote unit vector. So, this is the velocity component along the line of sight. Radar people would call this the downrange velocity. And so, if you look at the difference of those, well, let's consider the simple case where one of the receivers is fixed. Then this term vanishes. Then, what you have here is the set of points x. Here is the set of points x that corresponds to this quantity being constant is a cone, and that cone will intersect a flat plane in a hyperbola, another hyperbola. And so in this case, then we get different information from the TDA way. We get information about that the transmitter has to lie on a different hyperbolic. And so is it possible to actually measure these frequency differences? These frequency differences? Well, yes, if you have a moving receiver, you will measure a time delay corresponding to where the receiver is. And then that signal will also be Doppler shifted. And then it depends on the geometry what those Doppler shifts are. So they'll be different on the different receivers. And so if you then what you do is you compute what's called the cross-ambiguity function. So you take the multiply. Take the multiply one waveform by a shifted version of the complex conjugate of the other waveform, and then you effectively take the Fourier transform of that product. And if you plug in these expressions, you'll see, again, if you make a little change of variables in this integral and apply the Cauchy-Schwarz inequality, you see that this quantity has a maximum when the time shift that when the delay is equal to the TD away and when the Away and when the frequency, you know, this quantity has this cross-ambiguity function has two parameters. So it's when the second parameter, the frequency parameter, is equal to the FDOA, the frequency difference of arrival. And here's a little plot from somebody's paper of a cross-ambiguity function that shows this peak at that. So you would get the TDOA by reading off this value here and the FDOA by reading off the value here along the other axis. Here along the other axis. And so the idea is: if you have one moving receiver, and let's just say the other receiver is fixed, let's make things simple. Then from the TDA, you get one hyperbola, and from the FDA, you get a different hyperbola. And so if there's only one transmitter, it has to be at the intersection. That's sort of the key idea here. But of course, there are some complications. What if there are two emitters in your scene? Well, then you have two TDOAs and two FDOAs. Then you have two TDOAs and two FDOAs, and you have four intersections. Well, so what do we do about that? Well, we'll see in a little bit that the idea is: well, if one of your receivers is moving, you might as well use lots of these measurements and use sort of a tomographic or synthetic aperture radar kind of idea. But there are more difficulties in particular. Suppose the second receiver is also moving, then the FDOA current. Then the FDOA curves look like this. So, this is my colleague at the Naval Research Lab who made some plots here of different plots correspond to different velocity choices, and the different colors correspond to different values of the FDOA. And I find these curves pretty scary. So, you can imagine, you know, suppose you're trying to intersect these with hyperbolas for the TDOA. For the TDOA, you're going to have lots of intersections. And it's hard to see how, you know, under what conditions are we going to get reasonable values. Okay, so let me step back a minute and say, okay, so what's the big picture here? There are basically two sort of approaches for finding, for localizing sources. One is, some people have called them two-step methods, where effectively first Effectively, first you do some pre-processing of your data to try to isolate parts of the data where there's only one source. Typically, what people do is they make a time frequency plot and then they cut out a region of the time frequency plot. And then they, for that part of the data, then they try to estimate the TDOAs and FDOAs. And if you have an array, you can also potentially. You have an array, you can also potentially get the angle of arrival. That gives you the directional information. And then, once you have this information, these FDOAs, TDOAs that are kind of intermediate quantities, then you have to effectively solve systems of polynomial equations, or you'd use some kind of iterative solvers to try to find the source position. So, there's another approach, and that is if you're, especially if you have moving sensors, you might as well take. Answers, you might as well take lots of data and then add together that data all together. And these methods sometimes go by one-step methods. So the idea is you use all the data together and it's basically you sum together all the data as you travel along your flight path. And this is a tomographic form of imaging. It turns out when you analyze these, I'll say a little bit about this later in the talk, that the This later in the talk, that where these filter-backed projection type images focus is determined by both TDOA and FDOA. Okay, so let's talk first about how you formulate this as an inverse problem. So if you have, suppose you have multiple sources in your scene, then you could describe the source function. Source function as for each location x, you would put the waveform transmitted from that location, and that would give you a source term. And then you just have the wave equation. Now, here we're just using a scalar wave equation. Really, we should be using Maxwell's equations for electromagnetics, and that the polarization may actually have some important information. But anyway, so you would get the produce the field corresponding to those sources, and then the data measured. And then the data measured on moving receivers, you assume you have known flight paths, which I'm calling gamma one and gamma two. And then those locations would be used for the position in your field, the predicted field. So you would, in the case where you have two sensors, two moving sensors, you would have what you would measure would be functions like this. And then, of course, there's noise. So how would we formulate the inverse problem? Well, the inverse problem. Inverse problem. Well, the inverse problem is given this data, we want to figure out what that source is. Okay, so given the data, let's assume that we know the flight paths, then we're trying to find the source in this wave equation. So the problem is that this is a horribly underdetermined problem. So even if we assume that our sources are on a known plane, say, on a known surface, then that's two very Then that's two variables of position. And then, if we're trying to also find the waveforms, that's three variables. But how many variables do we have in our data? Well, the way I've written it here, it looks as if there's only one variable time. But in fact, there's really two variables because there are two different time scales. The electromagnetic waves propagate on a time scale that's much, much faster than the time scale on which the sensors are moving. And so typically, Are moving. And so typically you separate the time scales into fast time and slow time. And so maybe you have two functions of two variables, but how are you going to use that to get a function of three variables? And so this doesn't really work. So, but maybe what we should do is try to just find, let's not worry about the waveform. Let's instead look for the energy that's being transmitted from a certain location. And so then. From a certain location, and so then we would get back to a function of two variables if we were looking for sources on a known plane. And then what we would want would be instead of this source term on the right-hand side, we'd want this, well, now it's non-linear, non-linear expression in terms of the source. So we'll see. Okay, so today what I'm going to talk about is basically two pieces of the problem. One is Pieces of the problem. One is a little bit of information about these crazy FDOA surfaces. We do now understand a little bit about those. And then I'll say a little bit about synthetic aperture approaches that may be better. Okay. Okay, so what are the, here's the FDOA. We're going to ignore the fact that we may or may not know what frequency is being transmitted. You know, if you take the ratio of the two received frequencies, then the transmitted frequency can. The transmitted frequency cancels out. And so let's just focus on this geometrical question. What are the set of points that correspond to a constant difference of these Doppler shifts? Okay, so here's the notation from one sensor. If we look at that sensor, at the transmitter from that one sensor and look at the unit vector, that's the hat. We dot that. That. We dot that with the velocity vector of the sensor and then do the same thing for the other sensor. Okay, and then take the difference. Okay, so one problem is that, okay, so here's the FDOA again up in the corner. If we look at the sensitivity, if we just differentiate this with respect to the source location y, we get these, when you differentiate unit vectors, you always get these sort of projection operators. It's a scale projection with the range. Scale projection with the range and the denominator. And this is already kind of a bad sign because it shows that if your transmitter Y is far away from your sources, then the change in the FDOA is going to be small because of this denominator here. So as this denominator, as the range gets big, then the change in FDOA gets smaller. But let's see what happens if we do look. What happens if we do look far away from the sensors? So, a couple of students here at CSU looked at this question and noticed that if you go far away, the FDOA is approximately just the difference in velocity. So, effectively, if we're far away, then from two different sensors, it looks as if the directions are pretty much the same. So, these R hats end up being just approximately. Up being just approximately the vector from the origin out to your transmitter. And this is a cone. This is a lot simpler than this expression. Okay, so in the far field, if you have different velocities, you get the FDOA surface is a cone. And then if the velocities are the same, then of course this cancels out. But then if the velocities are the same, V1 and V2 are the same, then what you have is this difference of units. Then, what you have is this difference of unit vectors, and you can approximate that by the derivative of the unit vectors. And these derivatives of the unit vectors are these complicated projections. And so you end up with an expression like this, that the derivative of the unit vector gives you this essentially projection operator here. And then if you interchange the formula for the FDOA, if you interchange FDOA and this range here, then And this range here, then effectively, what you get is a polar format. This should be an approximation, by the way, not an equal sign. What you get is a polar format way of plotting these surfaces or curves. You know, all the angular information is given in these unit vectors, and then the range is over here. So, if you know the value of the FDOA, you can plot a constant FDOA surface by this polar coordinate. Okay, so. Okay, so here are a bunch of plots in the plane. So, this is the case with unequal velocities, and you notice that we do have all this complicated stuff happening in the near field, but far away, these curves all just become straight lines. And so that's at least it's simpler. If the velocities are the same, then you get these kind of figure-eight plots. So these are the actual plots going through various point chosen. Going through various points, chosen points here. And this is the approximation involving that derivative. So that you get these kind of closed curves. And if we're going to try to get the source location from intersecting time difference of arrival and frequency difference of arrival, this is actually kind of bad news in the unequal velocity case because the TDOA is a hyperbola. Is a hyperbola which becomes, you know, asymptotically becomes a line from the origin out to your source, right? And if the FDOA is also becoming a line from the source out to your point, then those lines coincide. So that nice sort of notional picture that I drew with those intersecting hyperbolas with transverse intersections, that doesn't really hold in this case of unequal velocities, at least in the Of unequal velocities, at least in the two-dimensional case. Things are better if the velocities of your sensors are exactly the same. Then, in some cases, you do get transverse intersections, but there's still some degenerate directions along the if your sensors are flying directly at the transmitter, then you get these again, it just degenerates the lines. Okay, so then here are some 3D plots. So the red. So the red corresponds to near field plots. So this, they all go through a point that's nearby, near the sensors are at 0, 0, 1, 0, 0, minus 1. And so this is a nearby point. And then the cyan color, those correspond to, those are surfaces that are passed through a point that's farther away. So basically, red is near field and the cyan is far field. And you notice the far field plots, they do. Notice the far field plots do become kind of like cones, and these are the unequal velocity case in the far field. But in the near field, you have this weird behavior. You have like little bubbles appearing, and you have these horns and pinched off things. So it's still very scary in the near field. And here's some more plots of unequal velocity. This horn kind of configuration seems to show up pretty often. Seems to show up pretty often. So, so this is because there are always singularities at the sensor location. So, I think the tips of these horns are at the sensors. But then in the far field, it's much better behaved. And then if you have equal velocities, then you have these, either near field or far field, you have these, they're not actually a torus because there's something weird going on at the center. It's not, you know, I think that's where those singularities are. That's where those singularities are, but at least far away, it's kind of well-behaved, closed surfaces. And then this is a sequence of plots for a particular velocity, but just looking at the FDOA value increasing gradually. And so there's a sequence for the same velocity that sort of goes, here's the horn case, and then it just kind of goes through this closed surface and then just winks out. Okay, oh, and then so there is an important point here. You know, I kept saying, well, you know, in the far field, you get the same directions, and so you can't tell what you're doing. But there is a nice sort of quasi-near field region, even if your TDOA and your FDOA are both, if your source is far enough away so that they're cones, both the TDOA and FDOA are cones, then if you know that you're Then, if you know that your source lies on a flat plane and you have this kind of geometry where you're flying above the plane, which is a common case that you might have if you're looking for something on the ground, then it turns out those cones intersect the flat plane in hyperbolas. And those hyperbolas can be intersecting transversally, even if the cones themselves have just a line. So, there's kind of a special case here. So, there's kind of a special case here where things might be reasonable, might behave reasonably. Okay, so this is just a summary. The far field, we understand pretty well what's happening with far field. We get cones or these, I don't know, I don't know what to call this shape, donut or something. It's not quite a torus. And then there's quasi-near field lesion where we get something good with equal velocities. The TDOA cone, or it's a hyperboloid, intersect, you know, asymptotically, it's a cone, intersects the closed, whatever this shape is, in a reasonable way, but if you have different velocities, it's bad. And basically, the near field is still really scary. Okay. Okay, so let me say a little bit about this. So, let me say a little bit about this other approach. I'll think of this called this a one-step approach. This is a PhD dissertation of my student Chad Waddington. So the idea was we have one moving receiver, and to avoid those issues with having the complicated FDOA surfaces with two receivers, we're going to assume that one of the receivers is fixed, because otherwise we just have that mess of all those weird surfaces. Of all those weird surfaces. Okay, so we're going to try to do a synthetic aperture kind of approach here. So in this case, we assume that the sources were discrete, were located at discrete locations, and each look at location E sub n, the waveform being transmitted was P sub n. And then we can write down the field. You know, this is just. This is just the standard radiation from a source collected together. And then the idea is we're going to start with cross-correlation. So we're going to think of the data that we measure as we're going to shift one of the waveforms. And I should also mention here we're also doing this separation of the time scale. So we have the fast time, the time scale in which the Time, the time scale on which the waves are propagating, and then the sensors are moving on this slow time, which I'm going to call s. So the data now depends on two variables. And so we're going to, at the same slow time, we're going to take a correlation in the fast time, and that will be our data. And so when you do this, each one of these, the field of each sensor is Uh, field of each sensor is this sum, and so you'll have a product of two sums, and there'll be a whole bunch of terms, and there'll be diagonal terms and cross terms. The diagonal terms are the case where we have the same element for we're looking at the same source for in both both factors here. And then the cross terms are where we have different sources. So, in the case where we have the diagonal, let's look at the diagonal. Okay, then we have a product of waveforms at the same source. And so we have the source kind of has a form like this. So we have it's located at a particular point, but then because of the product, we end up with a square there. And so here's where we make the And so here's where we make the assumption that we really don't like, but if we don't do this, we can't seem to make any progress. So maybe one of you smart people will figure out how to get around this. We're going to think of spreading the energy of all the waveforms that are being transmitted around so that they're all kind of, we're going to make the model the sources as if they're all transmitting the same energy. And that allows us to write this source. us to write this source term as a product of the functions of frequency or time and position. So we're going to separate out the position and then that allows us to write the data after some manipulations here. You can write the data for corresponding to the diagonal terms in terms of this, we call this the source density function, that gives you the spatial distribution of where your sources are. Distribution of where your sources are. And then we end up with the stuff from the cross-correlation. And it turns out then we get exactly a phase term that involves the TDOA and then the energy from all the sources. And it turns out this is a Fourier integral operator. And so if we form the image based on just the diagonal terms, in other words, if we try to invert just this operator, we know how to do that. We know how to do that. Okay, we so this the procedure is you use the same phase except with a minus sign, and you plug in into your TDOA, you plug in the point where you're trying to form the image, and you integrate. So this is effectively a tomographic imaging method where we're taking the data from all the from all the all the times, slow times and fast times, and adding them. Flow times and fast times, and adding them all together. So, of course, in the data, you don't know how to separate out just the diagonal terms. So, we're stuck using all the data here. And so, what happens when we do this, what happens to the cross term? So, the cross term is going to cause problems. So, this image formation is effectively back projecting the data along these hyperboloids. Okay, so it turns out that if you first Okay, so it turns out that if you form the images based on that, just the diagonal terms and your waveforms are not correlated, then it turns out that the cross-term artifacts don't really cause any problems. So the waveforms kind of cancel each other out. So as you fly a longer flight path and keep adding your data together, in other words, a longer synthetic aperture, these locations where the source These locations where the sources are tend to focus up better and better. Of course, then the question is: what happens if the waveforms are correlated? So Chad looked at a case where all the sources were transmitting exactly the same waveform. And in that case, well, we get a bunch of artifacts. But the interesting thing is that as we fly a longer synthetic aperture, those cross terms. Those cross terms seem to kind of blur out. So, what happens is we get focusing at the correct locations, but the cross terms cause blurs, cause artifacts. And the effect here is basically because if you have two sources and you add up the hyperbolas, you end up with, you end up not really getting a focus at a point. Whereas if you have just a single source and you add up those hyperbolas, they do focus at a point. So it's this kind of They do focus at a point. So it's this kind of effect that's causing this. Okay, and then we are able to analyze the image also from, so these techniques are very similar to what you do for the synthetic aperture radar. If you take your image formation operation and you plug in the formula for the data, so here's the form for the data, and you do a stationary phase calculation. And you do a stationary phase calculation, look at what happens, what the critical points are in s and t. It turns out that the rest actually s and omega, it turns out, do a few of the integrals. It turns out that the conditions for focusing are that the TDOA should be correct. So remember, the TDOA is this difference of distances. So the TDOA at your image point Z should match the TDOA at the true location. At the true location y. And when you look at the critical point with respect to this slow time, with respect to the movement along the synthetic aperture, that condition tells you that the FDOAs should match. So effectively, even though we were only starting with cross-correlations, so you might think, well, this is just going to be imaging based only on the TDOAs. In fact, the focusing condition involves Focusing condition involves this FDOA condition. Okay, and then the similar, you can analyze the cross terms in a similar way. And then there's also, we're also able to use the techniques from synthetic aperture radar to get the resolution. So basically, the idea is what you do is you plug in this expression for the data and you carry out the time integration, the time very. out the time integration the time variable is only here and here and so that gives you a delta function the frequency variable and you do the frequency variable and then you end up with a phase that has the difference of these these TDOAs and you make a change of variables and anyway after after a number of steps you end up with a point spread function that's that characterizes your imaging system that's that looks like this so it it so the idea This. So the idea here is we get a point spread function that is determined in Fourier space, spatial Fourier space, and we get a certain set of Fourier components of our source distribution. And those components are given by this set as the frequency ranges over. Well, this is all the frequencies transmitted by your source. Of course, we don't have control over that. Course, we don't have control over that. And then the range of unit vectors as your sensors fly along, as you look at the difference of unit vectors. And so this is telling us that if the difference of unit vectors from these are the unit vectors from your receivers to the source location, you want those to be as different as possible to get this as big as possible. So it's not a great thing. So, it's not a great thing to have your sensors too close together. It's really better to have them far apart. This prediction says that the resolution is better if the sensors are far apart. And then these are some plots of this set of points in Fourier space. And if you make this set of points as big as possible, then your point spread function will be as good as possible because you're getting as many Fourier components as you can. Okay, so summary here. This is the method where we look at a cross-correlation. So we just cross-correlate the data, not paying any attention to what's happening with the frequency. So we're not taking a cross-ambiguity function. And then we use effectively synthetic aperture imaging approaches. And because the sources, we're looking for sources like radar systems, which are typically transmitting things like. Typically transmitting things like chirps, so we're not assuming that the sources are random, so they're not delta correlated or anything. So, in this case, we end up with these cross terms that I mentioned. And Chad was able to analyze these cross terms and show that you get focusing only in the correct source locations. That's except in this the bad case where your receiver is flying directly at one of the sources. At one of the sources, and we're able to get resolution, okay? So, but there are lots of open questions, and so I'm hoping maybe some people in this audience will be able to help here. So, one issue is we really don't like that assumption that we had to make where we blurred all the energy around among all the different transmitters. But if we don't do that, then we're when we work out when we in the Work out when we in the in the formulation, then we have then we effectively are looking for both the waveform and the position, and we know that's under underdetermined. So we don't really know how to get at the energy of the waveform being transmitted at different locations, at least in this formulation. So we didn't do a very careful study of what happens if you have different waveforms. I said, Waveforms, you'll, I said, well, if they're not correlated, well, we didn't really analyze exactly what that means. So we've looked at the case where, oh, they seem pretty different and the case where they're exactly the same. And anyway, that needs more study. And then my colleague Jim Given at NRL and I were, we've been arguing for years over: should we be back projecting? Should we be back projecting this cross-correlation? In other words, should we consider our data to be the cross-correlation function? I think that's what we ought to do, the analysis that I outlined here. But he thinks we really ought to be looking at the cross-ambiguity function. In other words, incorporating both the time delay and the Doppler shift explicitly in the data and back-projecting that. And this is much more complicated. And we don't know how to determine which. How to determine which is better. So maybe somebody here has some ideas. We also haven't looked at these artifacts that we get when we are transmitting, when the transmitters are transmitting the save waveform. We know they blur out for longer synthetic apertures. We don't know how long the flight paths have to be in order to get them to blur how much. And then there's this case when both receivers are moving and we have these horrible FDA. Moving, and we have these horrible FDOA wild curves and surfaces. That's scary. We don't know anything about whether what flight paths are good or bad. And one challenge for the practical case is that in practice, you never really know exactly where your sensors are. So you can get some information from GPS systems and inertial systems, but for radar, what you do is you. For radar, what you do is you augment that information from the sensors with techniques called autofocus, using the data themselves to focus up the images. And we don't know how to do that here. We don't really know how to deal properly with noise. We also have, I'm talking to a colleague who is an expert in algebraic geometry, and she's immediately converting. She's immediately converting these FDOA surfaces into complex projective space. And anyway, we certainly need some help here. One interesting case is, suppose you know a little bit about the wavelength. Like suppose you know that it's a chirp of some kind, but you don't know what chirp it is. Is there some way to exploit that? Or maybe the other waveforms that are being transmitted, if you have a bunch of transmitters, maybe some of these. Of transmitters. Maybe some of the other waveforms can be treated as noise, but under what conditions? We have no idea. So, anyway, there's lots of stuff to do here. And then, of course, we would also like to do the same thing for sonar. So, I showed this picture before. You know, all the stuff I talked about today was in free space, where we understand the wave propagation is all in straight lines. What do we do in the complicated wave propagation environment, for example, underwater? For example, underwater, so there are lots of interesting problems here, and maybe some of you smart people can help solve them anyway. So, thanks, that's that's all, and I'm happy to take any questions.