Avilius talking about some nice new results on MCMC for Planted Clique. Oh, so I guess I'm starting. That's good. Go for it. Yeah. Yeah. Okay. Let me check if this works. Okay. Okay. Hi, everyone. So first of all, thank you very much for the invitation. It's it's really the same. I cannot be there. Sounds fantastic. I joined a few talks and they were really, really excellent. So yeah, I'm sure it's great what's going on there. So, yeah, I'm sure it's great. That's my order. So, I decided to talk to you about some recent work we had with Zong Zen Zhen and Elgar and Mosel. We're also at MIT. And the talk will be about some work I presume many of you may know. Like, it's about some work that happened 30 years ago that introduced actually the plantic leak model. This is the Zerus metropolis process, Metropolis process. And we will be some new results actually, like we revisited. Actually, like we revisited this work and we got some new results. Okay, so let's see what this is about. So, first of all, I mean to introduce, or I want to introduce the context where Jerome introduced the planning click model back in the 92. Okay, so the context at this time, and it's still like some very relevant open questions, is to try to understand how easy it is to find large clicks in random graphs. Okay, by random graphs, I mean Erdos Veni ZF graphs. So it's a known thing, I think Tim mentioned in the one talk I just. Thing I think Tim mentioned in the one talk I joined on Tuesday, it's a very classic thing from the 70s that the largest click in Z and half is like of the order two log n, right? It's like two log n plus smaller order terms. And you know, whenever I say something I may omit sometimes, I always mean with high probability, and that's with probability tending to one as n goes to infinity. And okay, so that's what it is. And one can ask whether we can find in polynomial time such a case. Polynomial time, such a click. And the best known polynomial time algorithm, like the best polynomial time algorithm we know, I can achieve half the size. So I can achieve like log n click with base two. And this is just a simple greedy procedure where you know you can, you know, you construct like going up like a click. It's just like every time you choose a vertex and you delete its non-neighbors, et cetera, et cetera. You choose a vertex from a neighbors, etc., etc. from the composed neighborhood, and you continue. And you know, this is a very simple greedy algorithm. So it's the same. Simply agreed the algorithm, so it's the same. You know, we don't have something better because actually, Carpenter 76 actually asked if we can do something better. And it's still actually wide open when we can beat this log barrier. And Jerome came in this, you know, like maybe 15 or so years later and proposed some other algorithm for this model, analyzed some other algorithm for this model, which is this metropolis process. So I assume everybody here knows what the metropolis process is. And, okay, what is what And, okay, what specifically about Dropbox process did he consider? He considered, like, the one that has stationarity, the specific Gibbs measure, also known as Harcourt model. So, what is this? So, you get an instance of G and half, and you define your state space are the clicks in G and half. Okay, it's a random state space, but it is what it is. And then, what you happen is you define this Gibbs measure that assigns mass to every click proportional to e to the beta time. Proportional to e to the beta times the cardinal alpha kick. So, you know, you can see beta is called the inverse temperature, it may depend on n, or et cetera, et cetera. The idea is that if beta is very large, you know, this, of course, puts more mass in the larger kicks. And, you know, okay, so to define the metropolis process, you know, of course, it's immediately defined in this stationary measure, but like I need some initialization, right? So I need this to understand where I'm going to initialize this metropolis process. So the, you know, the dynamics and The you know, the dynamics initialized, then you know, the most natural initialization is what it is allowed in our state space is to consider the empty click. Okay, so the empty click is allowed, and this is always a click in every, of course, graph, but then you know, one can consider also other instances. So, after you have these two things, you can define the metropolis process. So, let me spell it out for you: what is the metropolis process in three steps? You know, at every time you keep track of specific XT, which is like a click, and you I take you. And you update it on every time step. And what do you do? So, what you do is you choose first of all a random vertex v. Okay, so like you know, this random vertex v could happen to fall outside x t or inside xt. And then you take some cases, right? So like, you know, this is after you spell out the density updates. So like what happens, the following, like you check if the union is a click. Okay, that's your first thing you check. If it's not a click, You check. If it's not a click, you just stay where you are. Okay, that's the first step. If it is a click, there are two cases: right? Either the V falls inside and it's obviously a click, or the V falls outside and it so happens that this thing is a click, the union is a click. And in case it falls inside the click, you actually go backwards, so you delete V with some probability proportional, or not proportional, actually, e to the minus V better exactly. And otherwise, you just stay there. And okay, that's the one this case. And okay, that's the one this case, and the other case that, like, the union which is from outside makes a click, you just go upwards. Okay, like you know, you don't need to fully digest what the metropolis process is doing if you haven't seen it, but like the idea is that you are in a certain click and at every time step, you either add the vertex or delete the vertex with appropriate probabilities. And the stationary measure is this. Okay, and again, because I, you know, I can see only, I guess, part of the audience, right? You know, if you have any, any thought, any. you know if you have any any question whatsoever or comment please stop me like you know this will be you know much more fun for me for sure if it if you participate in any way okay so the the idea now is that the that the Jerome analyzes metropolis process and actually um you know you know okay maybe just a comment if beta is super duper large the metropolis process is all going go upwards at least in the polynomial time so like or imagine beta infinity which all goes upwards Or imagine beta infinity, which only goes upwards. So, in this case, it's really greedy. So, you know, a part of the metropolis process for beta super large can achieve the log n click. So, like Jerome essentially proved that for any beta that may depend on n, actually, it will not find something better. It will not find one plus epsilon log n click for any epsilon positive. There's a small caveat in the failure. I will come back to it. But, you know, and this is the result. And then he made an observation at the time, which An observation at the time, which is that even if you have the G and half, the instance of the G and half, and you add like a very large click, much larger than the logarithmic clicks that like the G and half created, like of size polynomial n, as long as it's less than root n, then basically it's a proof observation. His proof still works. So like his proof still works, and the metropolis process still fails to find n1 plus epsilon log n p. In particular, it fails to recover if you want this planet leak, this. Cover if you want this planted click, this attractor, you just plant it there. I have a question. Yeah, we have a question. Of course, yeah. Eventually, it would find the click, right? But what period of time are you asking? Right. Yes, eventually it will find the click. So, like, I'm asking, actually, that's a very good question. And yeah, sorry for omitting this. So, fails, I always mean in polynomial time. That's what I mean. So, like, fails, and actually, the zero result is a quasi-polynomial time lower bound on this thing. But of course, eventually we'll. On this thing, but of course, eventually you'll find the key. Yes, okay. I don't see, but I assume the silence is yeah, thanks. Yeah, yeah, of course not. Okay, so and and yes, and that's that's a very good question. Sorry for omitting this. And okay, so now, given this, this thing about Zero, there is another interesting thing about the root 10, the root 10 observation here. That, like, if you go above root 10, if you plant a click which is above root 10 size, then actually you beat the degree fluctuation. Then actually, you beat the degree fluctuations of the G and half. So, somehow, like this planet-like vertices now have degree, which is like you know, half times n, plus k, essentially, in the second order. So, because of this, you are, you, you, you are beating the degree fluctuation, and a very small, a very, a very naive K top-degree algorithm can recover this planet click. And Zero, of course, noticed this thing. And, you know, it's kind of interesting computationally what's happening, right? So, like, if you are above root 10, a very simple algorithm can find this. 10, a very simple algorithm can find this planet leak, and if you are below root 10, like a much more sophisticated metropolis process seems to be failing to find this planet click. Okay, at the time it was an observation, and this gave rise to the planet click model that many of you know, I'm sure, which is like as an inference task, as a recovery task, which is what it is, right? So the planet clip model is the following. So like you make some observation, let's take it in a statistical way. So you make some observation, some graph as an observation. Okay, and how is this graph created? And how is this graph created? First of all, you make a z and half. Okay, so imagine for n equals seven that this is your assembly of the z zero of a z and half. And then what you do is you choose k of the vertices. Let's say if three is like you happen to choose these three vertices, and you form a click between them. That is, if some edge was not appearing, you just form it there. You just place it there so that this forms a click. And you see eventually this graph without the colors, obviously. And the goal is to, you know, from an axis of an instance to this final graph. Axis of an instance to this final graph, and I will denote the distribution g and half, k, you want to recover this spec, you want to recover this planet K. And essentially, Jerome's result says that at least if you run metropolis process for at most, let's say, polynomial time or at most a constant of quasi-polynomial time, this will not work. This will not find the planet, will not arrive at this attractor. And you can ask, you know, from a statistics now point of view, what is the smallest K that can recover, right? So if K is super large, then Is super large, then you know, not about the metropolitan process generally for an algorithm. If k is bigger than root 10, then we know that like this top k degree works, at least the root 10 log n actually. But if k is below root 10, we know this metropolis process fails. But what about other algorithms? And there has been really, really a tons amount of work on this, some of course from the audience. And you know, the idea is here, you can make an axis with k and you can ask, okay, what about any algorithm, right? When can you trace this click? When can you trace this click? And it's not hard to see that, like, you know, if you are less than two log n, where there is some other, you know, because of the Erdos Reign structure, if you have a click which is less than two log n, there is some other just because of the Erdos Reign structure of similar or larger size. So of course, you cannot exactly recover if k is less than two log n. And above it, you know, actually there is this quasi-polynomial time algorithm that works. So if you are above, let's say, two plus epsilon log n, but you may have been thinking, okay, but I want like faster, I want polynomial time algorithms. Faster, I want polynomial time algorithms, and actually, if you go to polyomatime algorithms, like uh, you know, this is this is you know, that you cannot do much much better as far as we know at the moment from this top K degree. There's been plenty of algorithmic work. We can arrive at constant root n, where spectral works, IMP works, et cetera, et cetera. And you know, this poses the question of hardness, right? So, this poses the question of whether like this there is like actually some hardness for polynomial time methods in between, and what evidence do we have besides algorithmic failure for this? Have besides the algorithmic failure for this, right? So, the first evidence that people usually cite is this failure of the metropolis chain below root n, right? So, this is kind of the first evidence that, like, you know, maybe this problem is hard. That's how it's usually cited. And also at the moment, we have other classes of failure of class of algorithms, like the sum of squares, hierarchy, or rescue methods failed. And, you know, this has been a very useful conjecture in the literature. You know, it has cryptographic implications. Of course, Guy talked about average case reduction implications, et cetera, et cetera. Actions, implications, et cetera, et cetera. But this talk actually wants to revisit this very first evidence. Okay, so this very first evidence by Zerum that introduced this model, that like it's usually attributed to this failure of the metropolis process. Okay, and you may be thinking, why do I want to do this? Why do I want to revisit this very first thing? And I think it will become apparent if you see the result. Okay, so like, what is this result that actually Okay, so like what is this result that actually Jerome proved? So Jerome proved the following result. Okay, remember this is the Gibbs measure, right? The hardware model. And Jerome proved the following thing. Prove that as long as the plant kick size is, you know, like some polynomial n, where alpha is the exponent is less than half, and you have arbitrary inverse temperatures, and n epsilon positive, and that's the cuts, there exists some worst case initialization. Worst case initialization of the metropolis process. Okay, so the example worst case initialization of this metropolis process for which it takes super polynomial, quasi-polynomial time to reach one plus epsilon log and peak size. Okay, so if you work on Markov sense in inference and in hidden structures, you'll be like, okay, I mean, move on. This is like obvious. This is kind of the state of the, you know, what usually the lower bounds look like. I mean, The lower bounds look like. I mean, that okay, but like, in case you have not, like, maybe it's good to absorb this a little bit, right? So it says that, like, your dynamics will work as long as you start from some click X0, which is worst case placed, you know, to make you slow. Right. This is not kind of the lower bound that you would expect, like computationally, right? I mean, you know, because I will initialize from what I, you know, whatever I want, like from the empty click. So like, you know, this brings some, okay, some question marks for sure. Okay, some some question marks for sure and the you know and you know this is this is of course a very known thing and it's a tech it's a technique obviously a technical reason that this is happening and the reason is that like the most classic ways to obtain this hitting time lower bounds right because this is what these are like you're trying you're trying to to understand lower bounds on hitting a click of size one plus epsilon again like the most classic way of dealing with this is to do some bottleneck argument Some bottleneck argument. Okay, so what's the bottleneck argument? The bottleneck argument is that you want essentially to partition your space of clicks into two parts, A and A complement, so that in the A complement, all the clicks of size one plus epsilon again are contained. And the boundary of A, it so happens that it has much less Gibbs mass than. Less Gibbs mass than A itself. So, why is this good, right? So, if you do this, then as long as you initialize in A, somewhere in A, to reach a click of size one plus epsilon again, you need to cross the boundary. And since you need to cross the boundary, like this condition essentially tells you that like it will, you know, like actually, it's like a bottleneck, like the reality looks like this. So, this is your A, this is your A complement, and this is the boundary. So, like, it will take you a long time to reach this boundary. A long time to reach this boundary. Formally, like there is this implication. So, as long as you have this ratio upper bound, immediately you get that for any t, let me show you in the picture. Immediately you get for any t, as long as you initialize, and this is very important, I will explain, from a in a very particular distribution from a, then like the probability of hitting the boundary at time t is like at most n to the minus omega log. n to the minus omega log n. And you know, just by union bound, you get the quasi-polynomial lower bound out of this. And the idea is that you need to initialize from the Gibbs measure condition on A. And if you do this, essentially, you're almost at stationarity at every point. And that's how I kind of disturb. Okay, so this is the technique. This is a very classy technique. And this is what Jerome used. And actually, it's very interesting the way he used it. It's a very nice combinatorial construction. Combinatorial construction, and what he used is he created such an A. And what was his idea? His idea was that, you know, okay, let's see how the clicks look at G in half when there is not planned click, right? There is the empty click, there is this two-log n click, and you know, they, okay, let me make some picture first. There is this two-log n click, which is the maximal, and most of the clicks lie in this log n, right, entropically speaking, in this, in this log n cardinality. And you have like this one plus epsilon. Like this one plus epsilon, like log n clicks that you are trying to hit. And essentially, what Jerome noticed is that, like, he's just trying to create a bottleneck in creating, getting to this one plus epsilon log n click. And actually, you know, consider one plus epsilon over two, let's say, log n clicks. And notice, you know, make the following simple observation that, like, if you have any dynamic, right, that starts from the empty click and reaches in one plus epsilon log n click, like, Log n click, like however it goes at some point it hits some one plus epsilon log n click that it's will be a subset of the final one plus epsilon log n click, right, if you go along these dynamics. So you will hit one expandable click, right, that can be expanded somehow by the dynamic into one plus epsilon log n click. And actually his idea was that there are very, this is very rare. It's very rare to get on such clicks. And basically at the bottleneck And basically, the bottleneck, you know, based on this observation, on this observation, that like, you know, the ratio of one plus epsilon over two login clicks that are expandable divided by the ratio of the total one plus epsilon over two log n clicks is very small, it's quasi-polynomial small. It's very small, it's quasi-polynomial small. And essentially, you know, because of this thing, you know, you can create a bottleneck out of it by taking something that has boundary on this expandable px. And the idea is that then, you know, his idea was that if you plant the click of size at most root 10, the statistics remain the same. And I will come back to this later. And, you know, there are two major issues now if you see this result. Okay, so the first is this worst-case failure. Okay, so this only. Yeah, question again from Bruce. Yeah, does expandable mean you can just go up or you don't go down very much while you're going up to expand? So, expandable is not about the dynamics, it's just a property that like you are you well, we formally define about the dynamics, but maybe it's just for intuition. It's good to think of it as just being a subset of N1 plus epsilon log and click. So, like it's a yeah. So that means it's possible to go just up without going down anymore, yeah. Just up without going down exactly right for the dynamics, yeah, right, right, right. For the dynamics, it's supposed to be, yeah. So, so they have some more questions. Uh, yeah, just want to clarify here. The computation is like you already have log n size, so the probability for adding another node connecting to this current click is roughly one of n. And you need to do this for epsilon log to n. So that's n to the minus log n come from right, yeah, yeah, yeah. Right, yeah, yeah, yeah, yeah. Right, for sure. Yeah, this is what this one is. It's it's a right, yeah, yeah, yeah. I mean, uh, it's not hard to see that the ratio is small, yes, yes. I mean, it's not hard to see that the ratio is small, it's maybe slightly more complicated to see why the plantic leak of size less than root 10 will not affect it, but also maybe not that much, but yeah, right. So, there are okay, maybe one more question from Will. Yeah, yeah, sorry, yeah, just a quick question. Um, what should we be thinking of based? What should we be thinking of beta as? How big does beta need to be so that the stationary measure is actually putting good mass on the big cliques? Right. Okay, that's a calculation I haven't done personally. I mean, but I would say the lower bound applies for all beta. So like, you know, for these purposes, there is some beta, you are absolutely right, where like, you know, they are the relevant betas. And honestly, from my calculations in general about the model, beta ordered log n is where the action happens. So if Is where the action happens. So, if right, so you know, unfortunately, this is not the posterior, so I cannot answer to you what's the Bayesian because it's not really a posterior of some channel, but like I would say, but order log n is the intuition where the action happens, you know. Yeah, okay. Uh, so is there anything more? Yeah, good, yeah, good to go. Sounds good. Okay, so the first thing is like a worst case failure, right? That's a problem, and that's an ubiquitous problem, Markov Saints. And actually, Jerome realized this in the conclusion of the paper, of course. The conclusion of the paper, of course, and actually said it would be really nice to prove the failure under empty click initialization. But he actually, you know, claimed confidence that this will be true. But like, he said it will require some new proof techniques, right? Beyond bottlenecks. And there is some other thing that I kind of didn't mention, like I haven't mentioned so far. I may be thinking I'm just omitting this. Right, so this result has been cited a lot as the first evidence of hardness for the PlanetKeek model. That's kind of assuming that this metropolis will work. Assuming that this metropolis will work above root 10, right? I mean, it's kind of assuming that, like, this metropolis process would work above root 10, that's why, you know, a natural algorithm is failing. But actually, this result has never been proven that the metropolis process works above 10. And, you know, this is another big, like general problem in Markov chain theory. Like, how do you prove that the Markov chain works? Like, okay, of course, we have techniques for mixing time, but for this specific heating time problem, yeah, it's. Time problem, yeah, it's it's we there are some cases, but very few where we have such results, and it's a it's a big problem. And of course, you know, Jeremy realized this also, and that's the second question he asked in the paper. It's like, you know, actually he was more confident than the rest of the literature who actually cite this as evidence for hardness, he was more cautious, not confident. And he said, you know, the failure could persist above the 10, but you know, as they noticed, of course, the top degree, this algorithm said it will be a severe problem with metropolis process if the failure. Problem with metropolis process if the failure could persist up over time. Okay, and why do I point to this? Is you know, on this, I do not know of some prior result, but maybe, you know, I'm happy to be updated. On the second, though, there are, you know, I actually got interested to this because there has been some recent results on the positive result of question mark above root M. And actually there's been some two results that kind of show that maybe something strange is happening above root M. Strength is happening above route 10. Like, and okay, so first of all, is a result I had with David in 2019, where we were analyzing some Markov chains, other Markov chains for the planet click. And at the time, we were just trying to prove some kind of clustering or overlap gap property. If you're familiar with this behavior on the state space, and we were really expecting that, like, there will be a phase transition at route 10, et cetera, et cetera. And we're trying to understand, you know, where does the geometrical phase transition will happen? And actually, we came to the realization that the That the geometrical clustering, if you want, was happening at a very different threshold. It was happening at the, like, these are more posterior Markov chains. It was happening at the end to the two-thirds. So much above root 10. So we kind of went back and asked in the conclusion, is this really something we should be aware of? And like, you know, like, is it something going on, something wrong with this market change? And then we came back to the Jeremy thing and we saw that he never proved above the root 10 the failure, even if we kind of, you know. Even if we kind of, you know, like understood, you know, it's kind of implicit in the literature that maybe it will work. So, right, we asked actually, is this, maybe we should revisit the Markov chain of Zero. And actually, luckily enough, some physicists like found this interesting and right, there has some very nice work a year and a half now ago, that they actually ran some simulations, some very nice simulations of this metropolis process of Zerum. And actually, they came up with some definitely not a root 10 threshold. Some definitely not a root n threshold, like they came up with some threshold which is like you know n to the 0.91 as the failure of the theorem's process. That's strange, right? Okay, and the you know, it could be finite size effect. And, you know, I don't know, I got motivated after this myself a lot, at least. And I was like, okay, so should Jeremy's result actually be considered evidence of harness, right? I mean, this is, right, or is it just, you know, it just coincidentally started this very nice planted. Coincidentally, started this very nice planned click kind of harness conjecture. Okay, and right, so I want to claim like this work actually has results that claims that actually no, like it's not probably evidence of hardness. So like the metropolis process, we prove that it fails even with almost linear plant kicks. Okay, so it's not, it has nothing to do with root time, actually. Like at least that's what I will explain. You will understand what I mean. Okay, so let's. Explain, you will understand what I mean. Okay, so let me tell you the results. So, the result I want to describe says the following: this is the Gibbs measure. Okay, what do we prove? We prove that the metropolis process will take super polynomial time to recover the planned click. Okay, and there are two sets of results. If you allow me to worst-case initialize the metropolitan process like it's usually done, then we can prove it for any size k, which says, you know, it's n to the alpha for some alpha less than. You know, it's n to the alpha for some alpha less than one, like not half, right? There's not a typo, like any, any, any almost linear click, and any beta, which could depend on n. And, you know, this is failure above beyond return. Okay. And the second thing, and also not 0.91, right? It's one, the threshold. And the second thing is actually we're able also to do some ethical thing. So like, you know, for the second question of zero. For the second question of zero, like you know, like there's a question of like whether you can go beyond the worst-case installization, and actually we can prove the same result almost for empty-click initialization for the planet-click model for the metropolis process, all the way to one. And this time, though, let's go, that's why I don't have only the second result. We need to exclude some constant log n. We don't need to exclude all the constant log n, but you know, there's this technical thing. We believe it's technical, but we couldn't figure it out completely, like up to some. Figured out completely, like up to some inverse temperatures. Okay, so this is the result. Um, sorry, can I ask one thing? Like, didn't you said at some point that there's some value of beta that makes it just the greedy algorithm, right? Is that beta equals infinity? Beta, yes, and beta actually, beta little omega log n will mean it will take you super polynomial time to go backwards. So, essentially, polynomial time is greedy. For normal time, it's greedy. The beta little omega log n. But how do I reconcile that with point B here? Like, wouldn't isn't point B basically doing the greedy algorithm? No, because beta is allowed to be little log n also, if you want, or some little constant log n. This is also fine. Oh, sorry, maybe I'm confused. The greedy algorithm works for the non-planted version of clique, not for the planted. Perfect. Perfect. Yeah, it's never analyzed, actually, for the exact. Exactly. It's not hard to analyze, though, but maybe it's done. Okay, maybe it's done. I don't know this. I don't know, but maybe it's done. But we also do it the little omega, but maybe it's most certainly done. But yeah. And actually, yeah, given our result, actually, it fails. The grid fails all the way to almost linear planet kicks, by the way. Great. Yeah. It's not like top K degree. It's a very bad algorithm for the planet kick model. Nice. Okay. I see. I cannot hear, but yeah, I mean, all good, right? Oh, yeah. Will's just clarifying: it's the greedy where you take an arbitrary thing that makes it better rather than like a specific one that maximally increases. Okay. Yep. I think we're all good. Yep. So go on. Thanks. Right. Yeah. Yeah. Okay. So, you know, maybe my first comment is generic, right? Okay. Big failure, I think. It's of the metropolis process conclusion in the planet. The metropolis process, the conclusion in the planet key model. And I think it's contrary to common such predictions, at least we had. I mean, you know, and at least, and you know, I think this is not evidence of hardness. Like the first papers observe, of course, it's seminal, of course, it's super nice, but I think at least the way it's used in the Plato Kill model, I don't think it's citing as evidence is not the right thing. Okay, of course, there is one caveat. Maybe something magical is happening in the temperatures we're missing here. But like, you know, I wouldn't believe it. But, like, you know, I wouldn't believe so. And the second thing is, like, what exactly did we do, right? Which was different than Derum-Zerom's approach. And the idea is that, like, we prove a quasipolynomial time lower bound to, okay, maybe let me rephrase what I'm saying. So what do we mean failure to recover the planet click? So failure to recover the planet click would mean a lot of things, right? At the very least, it means it doesn't reach the planted click, as in the doesn't hit the planet click, as a markup saying. But, like, you know. As a mark of saying, but like, you know, Jerome used one plus epsilon log n clicks, even like as a failure. And we can prove actually quasi polymer time lower bounds for both tasks of like either finding one plus epsilon log n click like Jerome did, or actually even recovering a fraction constant log n overlap with the planet click. So like, you know, you can imagine that's a weak recovery if you want of the planet click, like that it doesn't even find log n vertices of the planned click at this at the quasi-polyomal time scale. And, you know, technique-wise, you know, the first part is, of course, bottlenecks, right? That's why we have worst-case installations. We prove new bottlenecks. And what's the difference again with Jerome is that Jerome consider like projected the dynamics along the cardinality of the click and you know eventually get like this one plus epsilon log n barrier. Like essentially our novelty is to consider the dynamics along the axis that overlaps it with the planned click. And actually, you know, this. Click. And actually, you know, this is more close to the overlap property if you're familiar with it. And actually, that's where we see the bottlenecks that go all the way to almost linear, not just by using the cardinality, if that means anything to you. I will explain. And secondly, what do we do to go beyond botLinux, right? What is this new technique? It's a very specific to the planet click approach. So I have to say, in the empty clicking sales, at least from a distance. It looks like a very specific planet click. Specific planet approach that we really understand the state transitions across time scales. But if you want, the one ingredient is that we really try to leverage one-dimensional Markov change generated by the metropolis process. We can, where in the one-dimensional inverted dense chains, you can understand everything. You don't need bottlenecks. We can really understand what happens from specific initializations. So, if you want some hint of what's happening. Okay, so now this is the end of the high. Now, this is the end of the high-level part of the talk, and I would like to tell you more details on the proof from now on. Okay, so hopefully that's fine with people. And right, so what I will do, what I plan on doing at least, it depends on the time, is that, you know, first of all, I want to describe the bottlenecks because it is much easier to explain. So, like, I will explain actually one bottleneck in detail. I won't explain. In detail, I won't explain why you get all the way to almost linear clicks a bottleneck to find log n vertices of the planet click. Then it's a new bottleneck, right? Okay. And okay. The second is I want then to tell you some new bottleneck that looks like the germ bottleneck, but holds all the way to N, to almost unit size for the one perception of log n please. And at the, you know, this will be high level, though. While I hope I will get you more the details of this. And finally, The details of this, and finally, I want to tell you some very little things, you know, about what you know, what is the beyond bottleneck. Okay, so let me start then. Uh, you know, of course, stop me at any point. Yeah, so okay, so let's start. So, what is the actual theorem for achieving, you know, that's the bottleneck for achieving order-log and overlap. It is hopefully what you're expecting to see. So, like, uh, you know, like for any k, which is like a power of n, which is, you know, the difference again, the one here, right? The difference again, the one here, right? It's not half, like it's all the way to one, and arbitrary inverse temperatures. So, like, uh, you know, and this is the nice thing about this bottleneck approach, and any epsilon positive, there exists some way to initialize the mark of the all the time, the metropolis process, that MP stands for metropolis process. I say this also to myself, okay, and for which it takes quasi-polynomial time to reach a click with overlap constant log n, epsilon log n. Up constant log n epsilon log n. So this is the theorem. Okay, so like you know, that's you know, the difference is you know, at the moment, I'm not claiming one class epsilon log n size, I'm claiming it will take quasipolynomal time to reach constant log n novel. In particular, it takes quasipolynomal time to recover the whole peak, right? I mean, obviously, but also we can do this at this finer scale. Okay, we will go after a bottleneck here, right? So we will do. Go after a bottleneck here, right? So we will do a bottleneck. So like we need to partition the clicks, the state space with the clicks on two parts, A and A complement, so that like all the A complement contains all the clicks that contain some epsilon log n vertices. And then hopefully the boundary is small in the Gibbs mass sense compared to A. Okay, what's the most natural A we could consider, right? We can just take A complement to B equal to this, right? So a complement to B. equal to this right so a complement to be exactly all the clears that have overlap at least epsilon log n and that's kind of what we do so like let me jump into right so that's a statement if you had any question maybe i will move on move on to the proof so it's a it's an it's a relatively easy proof actually so i want to explain this so the okay easy i mean it's it's it's it's something hopefully easy to communicate so so okay so we consider like what we're after here is is a bottleneck uh right so we want to we want to find a name that's Right, so we want to we want to find an A that partitions the space, a complement contains of the high overlap clicks, and somehow the boundary is small. And okay, so what we will do, right? So we will contain, take A, the simplest possible choice. Of course, much, you know, you can design much more complicated days that like, you know, has, you know, just all the clicks of overlap at most R, we will tune R, but R, you know, eventually we need to take it epsilon again, right? Okay, for epsilon small. And then we just need to upper bound this. And then we just need to upper bound this Gibbs measure. Okay, okay. So, the first thing I want to convince you is that this thing is not that complicated. Okay, so like, you know, it's not, it's not, it's a Gibbs measure on the discrete space. So, like, you know, this thing is nothing. Okay, first of all, what is the boundary of A? Maybe let's clarify this. The boundary of A, because this is just a one-dimensional constraint, it's just all the clicks that have exactly R vertices for the plan click. And the ratio is just. And the ratio is just a ratio of two things. The top is just, you remember what the Gibbs measure is: it's all the clicks that have exactly R vertices from the planned click. And the other thing is, you know, all the clicks that have at most R vertices from the planet. Okay, that's equality. So that's the, you know, and remember, right, this is a random variable. We will prove with high probability upper bounds, it's randomness over G, right? The whole state space is random. But like, you know. State space is random, but like you know, that's what we're after. So, the first observation is that, right? So, the numerator, okay, we may be able to have it, maybe not, but like there is something we can do about the denominator. So, the denominator contains, right, that's the planet click here, and this is the g and half, k. Right, so what are the clicks in the denominator? Right, in the numerator, it's all these clicks that have a very specific overlap with the planet click, like size-wise. But the numerator contains all these possible overlaps. So, what are the easiest clicks? So, what are the easiest clicks if you want to handle in the denominator? Are of course the clicks that are completely outside the planet click, right? Because these are just Erdos-Reni clicks, and we know kind of many things about them. So, like, our first observation is we want to upper bound the whole fraction by lower bounding the denominator by the clicks of zero overlap. So, like, you know, we want to upper bound this by the clicks. Okay, first of all, the numerator is the same. The denominator is just. The denominator is just the series where it has zero overlap with the planet. And why do I want to do this? Is first of all, I hope that this will work where the mass will be, right, to have bottlenecks. But also, these are, I know things. I can understand the statistics of the clicks in Nerdless Reign graph. With the planet, click, it's maybe harder. So, what I'm saying is, use this inequality here. Okay, so this is the first thing. Okay, this is a, you know, this almost really holds, right? Nothing is hidden here. And now, the question is: how do you understand? Hidden here, and now the question is: how do you understand this thing? So, there is some important statistic here to be introduced to understand this thing, which you know, actually, we will carry this throughout the talk. So, okay, this WQR. So, what's the WQR? It's something very intuitive. Okay, again, this is the plan to click. Okay, you give me a Q, which will be the size of the click, and an R, which will be the overlap of the click. And I just want you to count all the clicks that you see in front of you, let's say random variable. Of you, let's say random variable in G that have, right? So let's see what they have, they have R vertices in common with the planned leak, and the whole thing is of size Q. Okay, so I will define this random variable, right? Hopefully, this random variable is understood by everyone. It's not something super weird, but it will be very important. So, if you have this WQR random variable, you can very easily express the thing here, right? So, this thing here is equal, right? What's the sum overall clicks of overlap R into the beta cardinalities? That's also the sum overall. Cardinality is also the sum over all possible sizes, you know, e to the beta q, and how many of them have e to the beta q? WQR, right? That's what it is. And the numerator, the denominator is this wq zero. Okay, so so far, sounds so good, hopefully. So that's what we need to bound, right? And now we need a bound for all beta, right? So for all beta, we need a bound. So what's an easiest bound to, you know, if you want for all beta of like a sum over some coefficients divided by the sum over some other coefficients, just the L infinity ratio. Efficience is just the L infinity ratio of these two, right? So, like, you know, instead of having the sum over all q of w q r e to the beta q divided by sum, you just take the ratio of this thing, and of course, that's an upper bound. And luckily, you know, like of course, on purpose, it kills this beta out of the picture. So you can get an upper bound on this bottle, like for any beta, right? For any beta, which is just, you know, it could be a very bad upper bound, of course. But like, you know, it's just the maximum ratio. Okay, so this technical thing, what it says is that the bottleneck is. thing what it says is that the bottleneck is upper bounded by the following thing give me the planned click fix an r okay and then count how many clicks have overlap r with the planned click versus how many have zero overlap with the planned click and hope that like you know the maximize maximization over q will give you something small okay how sensical is this right so this is the the other question how sensical is that this thing will be small thing will be small okay so if if r was not ordered log n this is doomed to fail right if it's much bigger than order order to log n right and the the reason is that that that okay okay what is the reason so if r is k right if i'm allowed to take the whole pandiclick then you know of course i can take q equal k and there is one click of size k and full overlap and zero of course clicks of size k and zero overlap so like as long as Zero overlap. So, like, as long as if I divide by zero, I mean infinity, right? The bounded zero rates. So, of course, like R should be tuned to be small. But there is, okay, so hopefully this made some sense. But what I want to really make sense here is that like for the denominator to be relevant and not zero and not exploding this, Q should be less than two log n. So the outside of the planet click isn't that does anything. So, like, because it does anything, the maximum click of size is two log n. click of size to log n so like there is no way that like you like some q could be considered where like q is bigger than to log n because this will explode the upper bound so okay so in other words what i'm saying is that like you really need to choose an r very wisely so that like all the clicks of of overlap r have cardinality at most log m. Otherwise, this thing explodes. Like essentially, that's the context of the first lemma. So the first lemma guarantees that this is happening. So what is this lemma saying? That this is happening. So, like, what is this lemma saying? It's saying that there is some tuning of r, as long as it's a small constant times log n, and you can see the alpha being less than one matter here, but not less than half or something, where like essentially, if q is bigger than q log n, there is no such click. So, essentially, the first lemma guarantees to us that like all the clicks of size of overlap, small constant log n will not be much bigger than two log n. No, will not be bigger than two log n. Okay, how to communicate. Okay, how to communicate intuition on this, maybe even more. So, like, this is kind of a discontinuity thing. Why is this a discontinuity thing? Because it tells you that, like, you know, there is a two-log n click here, right, in the Edos Reni part. It tells you that you cannot expand it in a click in the planned click, per se, right? You cannot find a click that is, you know, an epsilon log n part which creates a click with this two log n. No, like you need to, you need to have, right? So you need to, if you have some overlap which is small, Have some overlap which is small, like you cannot have size bigger than to log n. So, like, there is, okay, I mean, hopefully, this makes some sense. Okay, so this is the first part, guarantees that at least if you have overlap at least, at most like epsilon log n, then you cannot have size bigger than two log n. Okay, which at the technical point, it makes the sensical. And the second most important property is telling you that as you have size which is less than constant to log n, then actually this ratio is small. This ratio is a. This ratio is the clicks that have size Q of any Q and overlap a little bit with the planned click are much less in cardinality than the clicks that have zero overlap. So let me explain this a bit more. So imagine that this is Q, right? This is the, sorry, this is the overlap R. Okay, for R equals zero, we know how many clicks are of any size Q. They're quasi-polynomial many, as long as Q is less than two log n. So we know everything about this. And essentially, what this thing is telling you is that the Essentially, what this thing is telling you is that the number of them false goes down for any alpha less than one, as of r is less than one minus alpha log n in a quasi-polynomial sense goes down. And then, of course, it will go up. So, like, you can prove that like for normal, for normal cues, or that's this expected q's, it will go up. So, generally, there is always this non-monotonicity happening in the landscape in the sense of like cardinality of number of peaks. Okay, and actually, if you put these dilemmas together, Okay, and actually, if you put these lemmas together, you immediately get, you know, for any q that there is this quasi polynomial bound, and that's the proof. Okay, so now you may be thinking, okay, but you know, okay, I may have confused you, and you may be thinking, okay, why are these lemmas even true, right? But okay, hopefully the lemmas, okay, they're technical, but hopefully the lemmas make some sense. And, you know, why are the lemmas true? So the lemmas are actually easy to prove the moment you guess they may be true. So like Yes, they may be true. So, like, you know, like you want to prove, you know, what it says here. And the proof goes by just the first moment, essentially, analysis, right? So, so you need to prove, you need to, you know, you know, you need to understand how many clicks are of size Q and overlap R. Okay. Just parameterize at the constant log n scale, everything, t and gamma. And, you know, what's the expected value of the number of clicks of size q and overlap r? You know, you just choose the planet click vertices. The plane-click vertices and the rest, and you have like some probability for the edges to appear. And you know, Q and R are logarithmic scales, so the binomial coefficients are very easy to binomial coefficients are very easy to approximate, and you get just a formula. And after you get this formula, then things are easy because, like, for the lemma one, what you need is like t to be bigger than two, at least two, and gamma to be less than two, one minus alpha in the t and gamma terminology. So, in particular. Terminology. So, in particular, you know, this is non-negative, non-positive, and this is negative. So, like, immediately you get an expectation that this is vanishing, the number of such clicks. Okay, you can straightforward check it. And for the second part, it's a little bit more complicated. How do you compute the ratio, right, of the click sizes, of the, yeah, of the, not click sizes, of the statistics over the clicks? Well, I mean, you can compute the expectations and it's easy to take the ratio, okay, WQR and WQ0. Okay, WQR and WQ0 they have the same t, so this vanishes, and you just have this left for gamma equals zero or not. And you know, for gamma less than small, this is negative. So this is quasi-polynomially small in expect expectations. And also, that's a nice thing, we know everything about WQ0, right? Because it's a erdozoanic league. We know that they behave like the expectation. So WGQR and WQ, zero, it's nothing else than WGQR. Like WGQR divided by the expected value of WGQ, zero, and you know, by Markov's inequality, this at most. Okay, so this is admittedly super technical, but I just want to give you an intuition. That's a whole proof, though. Okay, so that's kind of the intuition of what's happening, right? You just take the bottleneck, you upper bound it gridly, and then you just find a couple of geometric properties that hold on the way to one. So it's not something super therapy. Okay. And recall, nothing had to do with alpha equal half in this thing. How to do with alpha equal half in this in this analysis, right? So everything requires some room with that. Okay. Now I may pass super quickly and tell you about the bottleneck for achieving one class episode on game kicks, right? Because you may be thinking, okay, that's nice, or maybe not nice, I don't know, but like, this is not the Jerome thing, right? This is not the result of Jerome. So the result of Jerome was about one class section and log n click. Logan click and right, so we can prove also the login, the value of the login thing, it's more complicated. So, the idea is that, like, you know, we can get the same result with zero, but essentially with n1 here instead of half. And the idea is, you know, same thing, right? There is a worst-case nationalization that you can get, you know, to read the time, quasi-polynomial time, to reach one perception again. And what's the idea, right? So let's see. So the idea is that. The idea is that, right, again, a bottleneck, right? So, Jerome created a bottleneck. And as I said, right, it's about like these expandable clicks. And right, so what was this idea again is it essentially everything boiled down that like the number of one plus epsilon over two log n clicks that can lead essentially, that it can be expanded in some way to an one plus epsilon log n click directly, as Bruce said. Directly, as Bruce said, divide by the number of total one-perception look and clicks over two look and clicks is small. Okay, so this is very rare and they need to be hit at some point in the trajectory. And the idea of Jeremy is that like somehow if you introduce, and this is about G and half, right? There's jamming reproduce the calculation, sort of, right? So it's about G and half, why this is moly. But the idea is that if you plant a click of size less than root 10, this remains true. And the reason is. And the reason is actually because it's a fact or an easy observation also, that like if you plant a click of size at most through 10 and you randomly choose a click one perception log n, it will fall outside the planet click. So like essentially like most clicks that are of size one perception log n live outside the planet click of size less than root n. So like what I'm saying is that you know if this is below root n then if you Then, if you randomly choose a non-plus epsilon log and click, they don't care essentially about the planet click. Like, there's the mass is outside the planet click. And essentially, that makes statistics like this do not change. The statistics like this, you know, are essentially identical. But above root 10, this changes. Above root 10, this suddenly changes. And above root 10, actually, if you're above root 10, then this moves here. Like the typical click leaves in. Moves here, like the typical click lives inside the planned click if it's of this size. And actually, I think if I have heard this like a year ago, that would make me think that Markov change will work like in this regime. Because like, you know, most of the clicks of size one perception are inside the planet click. Okay, let me just say, because I've probably written this somewhere here, okay, also. So Jerome first proved it for K equals zero, then he proved it for K at most of 10 that the ratio remains small. And the idea is this: that like if alpha is less than half, the typical one perception. Less than half, the typical one-perception of gen click satisfies zero overlap with the planned P with hydrob. And the you know, the idea is you know, this fails about half. And that's that's the transition on half that, you know, made the paper relevant to root n. But actually, we can resolve this idea. And there is something missing here in this idea. It's the overlap idea. Okay, so what is the overlap idea? Lemma two, essentially I described before, was a ratio between the number of clicks of A ratio between the number of clicks of size Q and overlap R as compared to size Q. And I told you that, like, all the way to one, if R is small enough, W, zero is much larger than W Q, R. Right. So that really means that, like, if you have a small overlap R, most clicks are zero overlap. And this carries well beyond half. So like, you know, you can prove that as long as you restrict the overlap of these clicks to not be that large. Of these clicks to not be that large, and you can ask me why would you do it, but like if you restrict the clicks to be that like less than this r star, one minus alpha, maybe you can add the two even here, like to one minus alpha again, then actually indeed most of the mass is still outside. And that's relevant computationally, because computationally, from our first bottleneck, you will never see a click of larger overlap than this in polynomial time. So, like, essentially, you are looking at the wrong measure here. Measure here, if you take a uniform click of size one plus epsilon again, you will see right. So, okay, so what I'm saying is basically what we use this concept, this new idea to create a new bottleneck which combines the two bottlenecks. The first I described to you, which it will take you like quasi-polynomial time to reach a large overlap click. And the second idea, which is basically generous bottleneck coupled with this overlap thing, that like generous bottleneck to reach a multi perception click plus the overlap thing. Plus, the overlap thing, right? So, like, okay, let me let me rephrase in, or at least put a draw a picture here. So, like, Jerome was saying, okay, how hard it is to find a one plus epsilon log n click. And, okay, so like, and this is, let's say, the overlap axis. And what I'm saying is that, like, if the overlap is small, then this part of the state space can. The state space cannot be reached fast. There is a bottleneck for this part just because there is this, you know, there is this expandable idea coupled with our overlap lemma. So this coupling is fast, it's hard. But then we have also another lower bound that says from our previous time that you cannot also reach this, th you cannot also enter the high overlap region for other reasons. So essentially, you know, there is a bottleneck like this, which definitely prevents you from hitting in one plus epsilon log NP. Shift of hidden in one plus epsilon log NP. Okay, that's kind of a high-level, hopeful concept if that communicates of how you can create a better bot link. And now in the last five minutes or so, I want to tell you about the empty click initialization. Yeah, a question first from Bruce. I just want to understand this overlap idea. Are you saying if you condition the overlap to be less than R star, then it's most likely to be zero? Yeah. Yeah. Yeah, yeah. So, so the overwhelming majority of the clicks of size one plus epsilon log n condition on have low overlap, but low doesn't mean zero, right? If zero overlap is obvious, but like low, it gives you some space epsilon log n, then it will be outside the planet click. So it is a non-monotonistic, right? It's the same thing I was trying to explain before. Like it goes like this, the cardinality estimate. So like indeed, if you just take the global maximum, it's here, it's inside the planet. Maximum, it's here, it's inside the planet click. But, like, if you just see this part, it's decreasing. Most of the mass is zero. And actually, this is the part you will see algorithmically. Wait, so just to be clear, the definition of the new like bottleneck is cliques that both have size one plus epsilon log n and also have small overlap with the planted one. Is that right? The bottleneck will be the union. The bottleneck the bottleneck will be the union. So it will be it will be either you reach a click of size of overlap r star or you will hit a one plus epsilon login click of smaller overlap. So it will be the union of these two and somehow you will not right so you will not reach either of the two in polynomial time for different reasons. So you will not hit R star overlap for the reasons explained in the previous slides. And you will not hit one plus epsilon again with small overlap because of Again, with small overlap because of Jeremy's bottleneck, coupled with this small overlap conditioning that you may do just computationally. Make sense? Thanks. Yeah, yeah. Right. So, okay, so maybe do I have like five minutes to tell you about the empty click and then conclude? Is this fair? Yeah, yeah, I think we started a bit late. So go for it. Yeah. Okay. So, I mean, admittedly, you know, I go higher and higher level in the amount of details I will give. But yeah, I mean, I'm happy to talk about them also, you know, and you can. To talk about them, also, you know, and you can, of course, see the paper, etc., right? Also, yeah, but yeah. Uh, so the empty-click initialization failure, so that's a slightly different business. So it relates again with the non-monotonicity of this WQR. So it's again an outcome of this lemma too. So, okay, okay, the results, okay, I have it as a slightly more mathematical language just because it's important. Okay, so okay, we have different results for the different temperature regimes, but like one of it looks like this for any. Like this for any k which is like almost linear, and inverse temperature which is literally all again. So let's focus on this regime. The other looks like greedy, essentially, as it was pointed out. And if XT is like the metropolis process and R is like some constant log n, then it will take a super polynomial time with high probability to reach this overlap size condition starting on the empty key. Okay, that's kind of the result we're after. And you know, how do you prove such a result? Of course, like what really matters. Course, like what really matters is to understand the state transitions, right? What's the probability for any click of hip which is relevant to you of hitting this click given that you start from the end to click? Okay, so here we don't use bottlenecks, right? So like we need to kind of start with bare hands a bit to understand this probability. Okay, so what do we do? Right, so okay, so the first idea is that the okay, so the state space that we're operating, let's remember what's the state space we're operating, right? Remember what the state space we're operating, right? So we're on clicks, and these clicks add and remove vertices all the time. The state space is complicated, admittedly, it's a complicated state space, but not at small cardinalities. That's the first idea. So like if you focus, let's actually start. Okay, how am I going to structure this quickly? So first of all, I will tell you when there is no plan to click, some structure. Then I will add the plan to click and tell you the same structure holds. And then I will hopefully convince you it's useful for proving these results. So the result is as follows. So, the result is as follows. Imagine there is no plan to click. Then, let's do the picture I draw before us. There's empty click, there's two log n click, and all the mass is at log n. So, if you are below log n, actually, you know what happens, right? How many ways do you have a click of size one minus epsilon log n to make it larger? Right? Okay, any vertex will have probability to make the union bring a click one over two to the cardinality, right? To be connected with every with every element here. element here and and basically the the by you know the binomial random variable will give you that like n to the two over two to the cardinality are around the number of of upward neighbors this click has here in this state space and you know this is this is nice because it's like polynomials right and then and and this is nice because it kind of tells you that there is some regularity in this in this graph even if it's complicated in the state space right it goes first of all it expands right it has log n Expands, right? It has login downward degree, but like polynomial upward degree. So it will go up very quickly, most probably. But also, it tells you that everything depends on the cardinality of C. So like because of this, if you start with empty click, it's natural to expect that if you hit a click of size cardinality C, it will be the same as the probability of hitting some other. Okay, what I'm saying is that the probability of hitting some click of cardinality C, of let's say C. Of let's say C, it's the same as hitting some other click of the same cardinality or approximately the same, as long as C and C prime have the same cardinality, right? So like there is no reason if the graph is so regular at every level to deviate in the probability you're looking at. Okay, hopefully you see what I'm hitting here. Like, so you will go upward and you will go upward in a uniform manner. So that's the first observation. And actually, that, because it's a uniform manner, And actually, that because it's a uniform manner, that like you know, what this says is that the probability of hitting a click of a specific overlap is really for small clicks like one over the cardinality of clicks of this overlap. And you know, in our notation for before, just because we used Erdos-Ray notation with a zero overlap, it's like one over WC0. Okay, this to prove is very hard, actually. But, like, you know, hopefully, I mean, I was trying to sell. Hopefully, I mean, I was trying to sell some intuition of why this could be true, but this is where the birth and death chains, et cetera, et cetera, are coming from. And the one-dimensional structure comes because of this, because everything depends on the cardinal accuracy for low overlap. And now, if you prove this, then actually you can do something similar, slightly more complicated, but less accurate for the larger clicks. And okay, and actually you can get state space approximation, state transitions approximation for these probabilities, right? Because of, you know, for these approximations. You know, for the approximations, but you know, there's no plan to click, and planet click will destroy this argument. And then, this is the last thing that we do is we use reversibility to claim a certain monotonicity if you add the planet click in this estimate. So, what do we say? So, we say, okay, these estimates are false. The proof is false if there is a planet click. But actually, there is monotonicity that this thing is actually decreasing the moment you add the planet click. And why, why do? And why that's the case? By reversibility, the probability of hitting from empty to C is e to the beta, right? So this is like e to the beta cardinality C, the probability of hitting C, the empty from, starting from C, right? That's the reversibility. But beta is small for us, right? And C is logarithmic, so that's like quasi-polynomial. So everything will be at the quasi-polynomial order. So, you know, this doesn't matter. But then essentially, you have to study for monotonisticity. Then essentially, you have to study for monotonisticity this object. But, like, think of it for a moment, right? What's the probability if you start from C that you go downwards to empty click? Well, if I add now a planet click on the state space, this will only help this thing go upwards. So you can do a coupling that like this probability here is decreasing with k. So in particular, this thing up to a quasi-polynomial correction on smaller order correction, this will decrease with k. So we get the same estimates essentially for larger k. That's kind of the the idea here. That's kind of the idea here. And now I need to convince you. This is actually, you know, kind of interesting. And let me convince you. So if you have like these estimates for any k, the probability of hitting overlap R, you know, just total probability, is the sum over all clicks that have the specific overlap, okay? And then, okay, so this thing is at most WQ, zero, right? Because, okay, I hand wave it told you that it's upper bounded by this because of monotonicity and regularity. And how many clicks are of this overlap here? How many clicks are of this overlap here? This is a WQR, right? So, how many clicks have overlap R with the cardinality Q? WQR. So, this is just upper bounded by this, which is lemma 2, right? Lemma 2 tells you that this is quasipolynomial small if R is a small constant, okay? That's kind of the main idea. Like, you prove that this probability for any T is quasipolynomial small, and then you can do a unit bound to get this result. Okay, I mean, this is hard way, but like, hopefully. Okay, I mean, this is hard way, but like, hopefully, it gives you some idea of what tool is we use, right? So, like, we first prove that, like, you know, some bound if there is no click, and then we use some reversibility of the metropolis process to claim that this actually stays as an upper bound. And actually, it's kind of finally derandomizes the bottom, if you want. Okay, so this is the okay, let me close this. Uh, so, like, this is the work that revisits the metropolis process by zero and produces some worst case and empty. By Zero and produce some worst-case and empty-click initialization new results. And I think the most important thing is this: like it goes all the way to one, at least in my opinion. But of course, it depends on how you see this. And you know, let me close with some comments and future questions. So we want to close with the TCS comment. So like in TCS, we learn that, right, like about approximation algorithms, right? So we know that like for any graph, even worst case, there is some very, very nice algorithm by Figure that, and actually also in approximation. Here, that and actually also in approximability results, that as long as a graph has a click of size almost n, like n over poly log n, then there is some polyomal time algorithm that worst case outside of the click can find a poly log n click. Okay, so I was surprised right when we were running this work because knowing this, like essentially what the metropolis process is doing is like on the average case, where everything else is at randomly chosen besides the click, it's essentially performing kind of. It's essentially performing kind of the same, right? It's performing by finding, like, essentially, the metropolitan process almost, admittedly, we cannot get n over polynomial n because that will be rigorous then, almost fails to beat some other polynomial time algorithms on worst case environment. So it's kind of indeed, as Zero was saying, a severe problem with the metropolis process, what's happening here. Okay, like it cannot use, it doesn't use the randomness effectively. And right, so let me make some comments. Right, so let me make some comments. So, I think we should be careful with the Markov change failure for inference. And the reason is, I think, fundamentally, we don't have tools to prove positive results, right? Because if we had tools, we would be expecting a positive result for the metropolis process. And the absence of positive results makes it sort of strange to claim evidence of hardness based on failure of metropolis or Mercosan methods. Of course, and that requires, I think it's a very good future direction. A very good future direction, and you know, we offer some new tools though for the other problem, right? To prove naturalization and hard bounce, it would be very nice to get these ideas out. And you know, if you guys care about to prove beyond worst-case organization results, and you know, finally, just a comment, right, I think this shows that like, you know, for planted click, like natural Markov chains, like the metropolis process of Zerum and some other things we have analyzed with David seem to be underperforming, okay, underperforming. Be underperforming, okay, underperforming in inference. Of course, we have found tweaks to make them work in some cases by lift things, overparameterize things. But still, there seems to be some interesting other performance maybe. And that's the usual intensive PCA, another model in inference. We've seen this again, that Markov chains or gradient descent seem to be requiring much larger SNR to work than spectral methods. And I think it would be nice. And I think it would be nice to understand how generic this. I would be very curious if anybody has ideas on this. Anyways, yeah, thank you very much. Yeah, great. Thanks a lot, Delius. Any questions? Yeah, question from Tim. Can I ask? So if I ask what is the largest, or sorry, what is the smallest alpha so that? Alpha so that local algorithm, some local algorithm can recover a planted clique of size n to the alpha. So I think you mentioned at the very beginning that your overlap gap property things with David suggest that maybe that local alpha is like two-thirds or something. So, do we know? Are there positive results like that? Or are there like what would such a local algorithm look like? Is there any right? Yeah, yeah. Okay, so. Right, yeah, yeah. Okay, so you say to define the alpha local somehow or something like that, right? Right, right, right. Yeah, I mean, I yeah, I don't know. I don't know how to, okay. So, first of all, yeah, I would be expecting that this Markov chain, again, just expecting, I don't know how to prove it, at least for non-like zero temperatures, where we can do some greedy analysis. I do not know, right, maybe two-thirds could be reached by these methods, but even if it's reached, I'm not sure this is the fundamental alpha, right? I don't know. Alpha. Right, I don't know, like, that's where I'm personally stuck. Like, that's what, right? In statistics, we learned that you should sample from the posterior. So, like, and then it's natural to run Markov change on the posterior and ask this as the fundamental alpha. But I don't think we understand this. Right. In short, I don't know. I do not know how to define fundamentally the alpha local. It's definitely not one, I would say. Right. And, but, right, it's also actually hard to find what is local in the do we know? Is local in the do we know a local algorithm that can find a clique of any like size one minus a planted clique of size n to the one minus epsilon? Oh, okay, okay, okay, okay. You're right. Well, actually, right, I guess like localing kind of or something, you know, some sort of iterative thing, but so there is there is a some work by five five you actually wrong. So one of the results one of Axiom wrong. So, one of the results, one of the algorithms that can see with constant probability success at root n that it's kind of local. So, it depends how you define local, but like it goes. So, like, you know, it's interesting. So, the JEMS process goes upwards, right, in the click size. And Feiger's algorithm goes downwards. So, like, if you if you start kind of, I don't remember how it goes exactly about it, but somehow there is an algorithm that achieves root n, which An algorithm that achieves routine, which in some way can be deemed as local, and um, but right, so that there is there is some some algorithm, but I wouldn't be super satisfied. I was not super satisfied with this. Not in the sense that, you know, like it somehow gets around whatever you think the overlap gap obstruction is then if it finds this, right? So, right, but but the yeah, I mean, I think, I think the first part to make this like actually a theory will be to agree. Make this like actually a theory would be to agree on what the correct notion of local. I'm not sure. Thanks. Barrier is when your clique has to be of size K as you move around, I think. Is that right? Whereas these algorithms are like changing the size of the clique, right? Exactly, exactly, exactly. Also, Jerome's algorithm changed the size of the clique, right? Great. Other questions? Great other questions? All right, let's thank Ilyas again.