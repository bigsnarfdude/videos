Great introduction. And thank you for the invitation to participate in this wonderful workshop. It's really a privilege for me. I saw the list of speakers and I'm really amazed. So I hope this might be interesting for some of you as well. So yeah, today I'll talk about the visual learning on curved surfaces via diffusion. And I'll mention a few different works. Different works, both with my own students and with some collaborators. But just, you know, at the outset, a lot of basically everything I'll be presenting is joint work with many wonderful people. And I'll be mentioning individual papers as I go along. So the general context for this work is basically, you know, how to apply deep learning techniques to geometric data. And I think, you know, many of you know, and also this has been mentioned in this workshop already. This has been mentioned in this workshop already. When you try to apply deep learning or especially convolutional neural networks to 3D data, there are different approaches. So there isn't a canonical way to do that. So, you know, some there are methods that basically synthesize some 2D views. There are methods, you know, that work with voxel grids, volumetric data. There are methods also that work with points. So like PointNet is a famous architecture for that. And there is another category of techniques, which are what I, at least for the purpose of today's talk, I call least for the purpose of today's talk, I call intrinsic or surface-based learning. And that's basically so the intrinsic learning is the focus of my talk today. And the question there is how to apply something like convolution on the surface of a 3D shape. So why do we care about intrinsic deep learning? So there are many applications and typically whenever you have data that you especially care about the structure of the surface. So for example we Structure of the surface. So, for example, we've been working with people in biomedical imaging that try to analyze shapes of surfaces of RNA molecules. Or, you know, if you want to do, for example, human body segmentation or shape classification, like you have different classes, like this dinosaur or hand. And the advantage of, for example, when you're dealing with intrinsic deep learning is that the techniques that you develop are often invariant. Are often invariant to changes in poles. So they're not so sensitive to changes of the embedding of the surface. And there's a particular application that I like, and I've been working a lot, is shape correspondence, where basically the goal is you're given a pair of shapes, you want to do, for example, texture transfer. So for that, you need to establish point-to-point correspondences. And I'll actually show a few, you know, so this will be kind of a running target for my entire talk, but the technique. My entire talk, but the techniques we develop are generic, so they could be applied to anything. Of course, there are many other applications like reconstruction, physical property estimation, denoising. And in many of these cases, if you don't care so much about the pose, then it's useful to do intrinsic deep learning. So, as many of you know, it's kind of relatively straightforward to apply convolutional neural networks on Euclidean domains, like to the Domains, like 2D images that you can consider just as a grid of pixels. But as soon as you go to 3D shapes, then you have to do something. And it's kind of, basically, there are many different options. And one set of techniques basically work by trying to, instead of doing global parameterization, trying to do local parametrization on the surface. So basically, around every point, we parametrized a patch on the surface. Patch on the surface. And these techniques have been kind of popularized and introduced by Michael Bronstein and his colleagues. So there's a nice survey called Geometric Deep Learning from 2017. And there's another kind of more up-to-date survey that also compares to other more recent methods. But a lot of the techniques in this domain have this general flavor. So how do they work? So basically, the general idea is: you know, a 3D surface. You know, a 3D surface or a surface in 3D doesn't have a canonical global parameterization, right? So there isn't a set of canonical coordinates, but you can parametrize the shape locally. And so how does this work? So for every point X, you look at some geodesic neighborhood of this point X, and then you parameterize it through some polar coordinates. So you basically have some distance to X and some angle with respect to an arbitrary reference direction. And so this gives you a polar. gives you a polar coordinate parametrization. And then if you have a function f and some you know learned filter, you can basically just take the you know the integral of the inner the inner product between your filter and this function. And that basically is the equivalent of a convolution on the surface. And the nice thing is that if everything is done on the surface, then this basically becomes Paul's invariant. This basically becomes Paul's invariant. And so, like I said, there are many different techniques that are introduced using this idea. And probably the earliest is this geodesic convolutional neural networks from 2015, GCNN. And yeah, actually the shape correspondence was one of the initial motivations for introducing this architecture. And so there's one challenge when you do this, and I just want to spend a couple of minutes. Do this, and I just want to spend a couple of minutes talking about that, which is that even locally, there is some ambiguity when you define this parameterization. So, if you have, for example, a local patch, you know, with UV coordinates, and you try to basically apply, you know, parametrize some pointer on the surface, you need to map this local batch into the tangent plane at this point. And you have all sorts of possible direction, you know, possibilities of how you Possibilities of how you apply this parameterization in the tangent plane. So basically, there's this rotational ambiguity. And so there's a nice animation by Ruben Birsma that shows this rotational ambiguity. So when I want to apply it onto the surface, I have all these different possibilities. So that's the first challenge related is that, you know, at that at different points on the surface, the directions of different points can be, the local orientations can be different. So there is no guarantee that they'll be consistent in any way. Now, the solution, so there are different ways to solve this problem. So for example, the earliest ways to solve this ambiguity in the local parameterization is what's called angle. Is what's called angular max pooling. So, what does it do? Angular max pooling basically is the following operation. So, you know, I have this function and I have a learned kernel, and I can find basically the inner product between my input signal and the learned kernel for every possible orientation of the reference direction and the tangent plane. So, this tau of u is basically the mapping between some reference. The mapping between some reference direction and 2D and a direction in the tangent plane. And what you can do is just say, okay, well, you know, I don't know where this reference direction should go. Let me just try all possible orientations and keep only the orientation gives the maximum response of the function with respect to this current kernel. So that's one option. There's another option, which is, I would say, less popular, which is you basically try to find some canonical directions, like, for example, Directions, like for example, principal curvature directions, and try to align the reference direction to that. The problem is that this is often quite unstable. So, for example, if you're on a sphere, there really isn't any way to find a canonical direction. So, basically, this angular max pooling is probably the more common solution, but it also has some challenges, right? So, okay, so maybe just as an example, so this geodesic convolutional neural network. So, this geodesic convolutional neural networks is one such algorithm or one such technique that applies exactly this idea. So, there's some, you know, learned filters, then there's some this angular max pooling, and you know, there's some standard techniques like CRLU and so on. And so this was, you know, the architecture that was proposed in 2015. And shape correspondence was actually one of the key applications in this paper. So, what they did is they basically Paper. So, what they did is they basically trained this network to predict vertex IDs. And the idea is the following: so, suppose I give you a whole bunch of different shapes, and I know for each of these training shapes, for every point X, I know where it should map on some reference shape Y. So, this is some template, for example. All right, so I have basically a way to name every point. I have a way to assign a name on some reference template. Name on some reference template. And if I can, so basically, if I do that, that allows me to phrase correspondence as a labeling problem. Because for every point on every shape, I can map it onto the template. And if I want to, for example, map between two shapes, I can always map to the template and back. So if I have a way to inverse this mapping, which you can do relatively easily. And then basically, this is just training such a network is similar. Such a network is similar or basically the same is just training a semantic segmentation network. So I just want to predict a bunch of labels on the surface. And the number of labels equals to the number of points on the reference on the template. So and you basically use standard logistic regression to train this. Okay. And so yeah, so you know, this is something that you can do. And basically, if you look at different kinds of shapes, so there's different Different kinds of shapes. So there's different benchmarks for shape correspondence, and often they involve humans in different poses. And the goal is to basically predict correspondences between these humans. So here we have some source shape and we have different target shapes. And for each of these shapes, we have a mapping to the source shape. And basically, this allows us to transfer some color onto each of these shapes. And also, you can measure the correspondence that you predict with respect to some ground truth. With respect to some ground truth. And so I'll be showing a bunch of plots like this. So here on the x-axis, we have some error threshold, and on the y-axis, we have basically the percentage of correspondences below this threshold. So here, for example, you know, 0.1 is some geodesic radius. You can think of it, for example, as 10% of the shape diameter. And so here we know that, for example, 90% of correspondences are within 10% of the ground truth. You know, of the ground truth. So that's quite good. And basically, in 2015, this was the first learning-based approach, and it performed better than all of the sort of previously proposed axiomatic methods. Now, the problem with this method, I mean, there are a few different problems, but one problem with this method, and I just, you know, say two words about this, although that's not the main focus of my talk, is that Of my talk is that you know, when you do this angular max pooling, right? So, so this local parameterization is only defined after rotation, right? So, you have all of these different ways to take your kernel and apply it onto the surface. And when you take the angular max pooling, so you just take the max across all possible orientation, in some sense you're losing information about the direction of the kernel, right? And so, in principle, that could lead to So, in principle, that could lead to some issues. So, you know, if you so that's the first problem, and the second problem is that local orientations across different points are not necessarily consistent. And so, to solve the second problem, so I'll talk about the first problem in a second, but to solve the second problem of making the different orientations at different points consistent, there is a fairly standard tool in differential geometry, which is called parallel transport, so that if you have Transport so that if you have a way to define an orientation in one point, you can basically connect that point to some other point on the surface and compute a parallel transport between those two points. And that gives you a way to synchronize different directions. Ideally, you only want to do that locally in some local neighborhoods because if you have to do this across large distances, then it can be unstable or even undefined. Now, Now, to solve this local directional ambiguity problem, in 2018 we had this paper called Multi-Directional Geodesic Neural Networks via Equivariant Convolution. And in that work, we basically, so this was a work done with my student Adrian Poulinar. And in that work, we basically propose to use, instead of using real-valued functions, so for every point on the surface, you have a single real value, you can work with what. Value, you can work with what we call directional functions. A directional function is basically a function that takes two parameters, a point on the surface x and some vector v in the tangent. So it's basically a unit vector v in the tangent plane, and it produces a real number. And what once you work with such directional functions, then you can basically keep, you don't have to take the max across all kernel rotations, but you can basically take a directional function. Take a directional function and then convolve it with a real-valued kernel and then produce a new directional function. And so that allows you to basically keep this rotate, like you know, the responses of a kernel across all possible rotations. And what that allows us to do is basically, you know, keep the directional information in the network, and then we only apply angular max boolean once at the very end of the network. So, you know, if anyone is interested in this topic, I'll be happy to discuss this in. This topic: I'll be happy to discuss this in more detail. But, you know, this is just one way to basically solve this parameterization ambiguity problem. And we compared this approach, again, this is in 2018. We compared this approach to this geodesic CNN. And there was something interesting that we saw. And actually, this will come up again and again in the rest of my talk. And so here we are plotting the accuracy of different shape correspondence. Of different shape correspondence methods. And in these dashed lines, you see the accuracy of basically correspondence methods on clean benchmark data sets. And by clean, I mean, for example, there are many human shape data sets. And the majority of those data sets, the ground truth is given by one-to-one correspondence. So basically, all shapes have the same exact triangulation. And so what that basically allows you to do is, in some sense, You know, allows you to do is in some sense overfit to that mesh connectivity to that triangulation. And what we saw is that if you train a network on like clean data, but then test it on shapes with slightly different meshes, then the performance can drop. And sometimes the drop can be very significant. So for example, if you look at this geodesic CNN, you see that the difference between performance on this clean original benchmark and on this remeshed benchmark is very This remeshed benchmark is very, very drastic, right? And so, one, if you look at the clean data, actually, the difference between our approach and the original GCNN is not that huge. But on the remesh data, you see that basically we overfit much less to the mesh structure. And I think one of the reasons is because basically we, you know, the filters that we develop are actually kind of a bit more meaningful geometrically. Of a bit more meaningful geometrically, they really do allow you to propagate information, and so it tends to overfit a bit less. And so, this actually got us thinking like, you know, this problem with remeshing is maybe quite big and actually maybe a bit underappreciated in the community. So, basically, you know, first I talked about the fact that local parametrization is only defined up to rotation. Okay, that's a problem, but we know a little bit how to do. But we know a little bit how to deal with that problem. But there's another problem, which I think is actually much more serious, which is that the local patch operations, like this parallel transport or even this local parametrization using polar coordinates, not only is it quite slow, but it's also not very robust. So if you change the triangulation, you can often get quite different results. And it's very difficult to make it truly robust. So the general question, and this will be the main question for the rest of my talk. And this will be the main question for the rest of my talk: is can we avoid local patch parametrization altogether and yet still enable some kind of communication on the surface? Okay, and so this just to give you an example or just to give you an illustration of the severity of this problem in practice. So, there's a paper that came out in CVNE that's called Shape Correspondence Using Anisotropic Technical Spectral CNNs. And this is, you know, Spectral CNNs. And this is, you know, a kind of version of benchmark data. Essentially, they get perfect. I mean, it's close to you throughout, right? So that's one kind of, I mean, you see more or less that this is solved. And indeed, you know, if you test this method on the same, you get On the same, you get closer, but if data set, the error shoots up, you know, in between zero, so this is like basically throughout the entire shape, the error is close to, you know, is about 10% of the shape diameter. So this is very severe, and this is, I mean, often not really reported in literature. And this is not a problem just with this method. It's actually a problem with most methods. So here are some. So, here are some different remeshings of some original human-shaped data set and the accuracy between the original and the remeshed versions. And you see that more or less all existing methods, I mean, it doesn't actually matter what these methods are, but they all depend on some local parametrization in some form. And the performance often drops very, very rapidly. So, there is some various, I mean, in my I mean, in my opinion, there's some overfitting happening, and that overfitting often has to do with mesh structure. So, what that means is that these operations that are used in these approaches, in a way, they're actually not geometric, but they're more combinatorial. And so they are more sensitive to the mesh structure rather than to the geometry of the shape. But ideally, if we want to develop a successful geometric. Successful geometric deep learning architecture, it shouldn't care whether you give it this kind of mesh or this kind of mesh, because in practice, you want your method to work on arbitrary meshes. And so, yeah, so that's the question of how can we do this. So this brings me to a recent work that we did. This is a collaboration with Nick Sharp and Kinan Crane at CMU and my PhD student, Soheib Ateki. PhD student Sohib Ateki. And this is a paper that we call DiffusionNet: Discretization Agnostic Learning on Surfaces. So it's currently on archive. Hopefully, it should appear soon. And the general idea is very simple. So basically, if you look at a surface, you can apply something like geodesic convolution. You can also apply pooling hierarchies like mesh simplification and so on. So that's something that's common. And so on. So that's something that's commonly done. But both of these operations are kind of difficult and also, I mean, in our opinion, sources of non-robustness. So our general goal is to basically replace all of these operations with just diffusion. Okay, so how does this work? So essentially, we have a network or we have an architecture and it has three main components. There is point-wise MLPs, which Pointwise MLPs, which is quite standard. I'll maybe say a couple of words about that. But, you know, for those of you familiar, for example, with pointnet, I mean, that's kind of what is happening there is just per point exactly the same multilayer perceptrons. Then we have some gradient features, which are important. And then we also have a learned diffusion layer. Okay. And this is actually what allows, this learned diffusion layer is what allows communication on. What allows communication on the surface and basically what allows learning. So, how does this work? So, the diffusion is very standard, right? So, diffusion is basically the standard diffusion equation. So, we have some function x on the surface. So, let's say we have some distribution of heat on the surface. And then, if you evolve this distribution of heat according to this diffusion equation, is basically a partial differential equation where this delta is still applied. Where this delta is the Laplace Peltrami operator, then as time increases, the temperature, the heat diffuses or dissipates, and then eventually it becomes uniform on the surface. And so this is probably the most studied PDE on surfaces, I would say. I mean, it's the simplest one. And there are many, many different stable implementations and how to discretize this. I mean, it's extremely, extremely well studied. Extremely well studied, right? And I'll actually say there's like almost a trivial way to implement it, which is almost what we use. So this is just a standard diffusion equation. Now, what do we use? So basically, our key idea is to let the diffusion time to be a learnable parameter, right? So instead of predefining some fixed diffusion time, we make this diffusion time a learnable parameter, and it can be different not only for every Not only for every different task, like shape correspondence or shape classification, but it can be different for different channels. So we have different layers in the network and different channels in the layer, and then these could be variable, basically, these can be diffused using variable or learnable diffusion time. And so we have a diffusion layer that basically takes some signal, which is, you know, basically, think of it, for example, a signal from the previous layer. Here, um, from some, you know, or we have k different uh functions, and then it basically produces another k functions which are just diffused, okay? So, with some parameter t. And so the key thing here is that this parameter t, we have, let's say, k different values, so one value per channel. And what we'd like to do is basically learn this optimal parameter. Okay. And one lemma that actually Nick showed and Actually, Nick showed in the paper is that if you just allow diffusion plus pointwise MLPs, then you can represent all radially symmetric convolutions. So we kind of can represent almost arbitrary convolutions in Euclidean domains, but there is some this caveat that the filters that we learn have to be radially symmetric. So I'll show how we get rid of that assumption in just a second. So, I mean, this is actually kind of, I mean, this is probably not surprising to some of you, but to me, it was interesting to see that diffusion can actually replace convolution. In the sense, they're equivalent, at least under this additional assumption. And so, that's the general idea. Now, the question is, how do you implement it in practice? So, there's almost a trivial way to implement diffusion, which is the following. Implement diffusion, which is the following: suppose that you have a discretization of the Laplace-Beltramian operator. So, on a triangle mesh, you have some matrix L and M. These are basically the cotangent weights or the cotangent Laplace matrix and some mass matrix that basically discretizes the inner products of functions. And so, once you have this Laplace-Butrami operator matrices, then you can compute the eigencomposition of this Laplace-Botrami operator. apply potrammy operator and then if you have a function that's represented in this eigenbasis then applying diffusion is just multiplying each coefficient by the exponential of minus the eigenvalue times t. So again, how does this work? So suppose I have some function x. First, I can project it onto this Laplace Plotrami eigenbasis. Then I can apply, basically pointwise multiply the coefficients by this exponential. coefficients by this exponential times the eigenvalue times t. And here it should be the same t, so t zero. And so t is the same across all of the eigenvalues. And then we project back. So this is just a standard way of simulating diffusion in the Laplacian basis. And the nice thing is that there's a closed form expression. And if you pre-compute the eigenbasis, then this is basically differentiable with respect to t. Differentiable with respect to t. So, one thing that I want to emphasize is that this is not spectral learning, in a sense, that there are papers that apply some learned filters in the Laplace-Beltrami basis, and they're basically the filters are dependent on the basis. And as a result, what happens is that if you learn a filter on one shape and then you try to apply it on another shape, it won't be applicable because the basis is different. In our case, we only use the Laplace. Case we only use the Laplaciable Tramium basis to simulate diffusion. So, this is just like a quick trick to simulate diffusion. There's another way to simulate diffusion using backward Euler step. And so basically you can, for small times t, you can solve this implicit Euler equation to simulate diffusion. And this is also differentiable. All you have to do is differentiate this matrix inverse. The problem is, you know, differentiating a matrix inverse is a little bit more expensive. A matrix inverse is a little bit more expensive in practice, or sometimes quite expensive, depending on the resolution. So, for efficiency reasons, we use this approach. So this is basically how you simulate diffusion and how you can differentiate diffusion. But the problem is that, like I said, if you just have diffusion and MLPs, then you can only simulate symmetric filters. So filters that are isotropic. So they don't depend. Uh, isotropic, so they don't depend on the direction in which you know. So, basically, they're the same in all directions. So, if we want to create filters that do care about directions, we need some additional information or additional, you know, signal. And in this case, we use in this paper, we use basically gradients of functions. And so, here, the idea is very simple. So, you start with some signal, u, then you can compute its gradient, and that gives you a Compute this gradient, and it gives you a vector field. So basically, you have a vector per tangent plane. And now we have different channels. So we have all of these different vectors in different channels. And we can take inner products between the gradients of all of these different channels. And we can also learn. So, in this case, we learn basically a scalar coefficient to apply to these pairs, pairs of gradients, right? Gradients, right? And so, what that gives us is basically some information about how different filters, how the gradients of different filters are related to each other. That's how we basically get anisotropic information using these gradient features. But what's important, and I really want to emphasize, there's no parallel transport and there's no local parametrization. And so computing gradients is very, very simple. Is very very simple, right? And this overall operation is invariant to the choice of orientation in the tangent plane because we only take inner products. So that's it, basically. So this is our diffusion net architecture. So we take some input features, right? So maybe they're X, Y, Z coordinates of the surface, or maybe there are some input pre-computed features. And then we take these features, we project them onto some. We project them onto some spectral basis. We simulate diffusion per channel. And then we have two branches, right? So, first, we project basically the result onto the surface. And then in parallel, we compute these spatial gradient features and we concatenate the two of them. And then we apply basically per-point MLPs symmetric. So it's exactly the same procedure applied to every point. So there is no communication between. So, there is no communication between points happening here. The only communication between points essentially is primarily happening here through learn diffusion and a little bit locally happening here through gradient features. And so, yeah, so this is what we get. This is what we call a diffusion and block. And then you can stack these different blocks together and you can use them to learn different properties of the surface. So, this is what we call diffusion net architecture, and it's general purpose. So, you can use it for any kind of learning that you might need to do on the surface or on any domain actually in which you can discretize diffusion. So, we applied it to many, many different tasks. So, for example, like I said, we work on this RNA molecule segmentation. So, basically, we achieved state-of-the-art results using this architecture, and actually by fairly significant margin compared to other techniques. And so the task here is that we have some ground rules. So we have some training data for which we know the segmentation, and then we basically have a new shape. And the goal is to predict the segmentation on this new shape. And it's quite difficult because there's a lot of variability, and these shapes are like highly non-rigid. Like highly non-rigid. We also applied it to human part segmentation. So, you know, there are different people in different poses. And this, you know, data has lots of different, like quite significant variability in terms of the mesh connectivity. But also, you know, we can apply the same exact architecture, whether you have 750 vertices or even, you know, 184,000 vertices. It basically this architecture scales very, very well. Detector scales very, very well because the only things that you need to do are compute this pre-computer Laplacian basis and simulate diffusion, which you can do efficiently even for very dense meshes. And we can even apply it to point clouds. And yeah, so there are different papers that publish different results on this data. A little bit, it depends how you count. It turns out that the way that you evaluate the performance is not always the same. So there are different. Always the same. So there are different ways of evaluating performance. Like, for example, if you predict on an edge, which you know, do you take it correct if it's boundarying to some correct label or not? I mean, they're kind of subtle details, but more or less, you know, we achieve at least comparable to state-of-the-art performance. And we also apply the two-shape correspondence. So here we have non-rigid, different non-rigid shapes and our Non-rigid shapes, and our goal is to basically give an source shape to find the mapping to this target shape and use this mapping to do, for example, color transfer. And yeah, we basically evaluated all different, so here there are different kinds of benchmarks. There's FAOS is a famous benchmark and Scape are two different famous benchmarks. And we also tried something that I think now is becoming more popular, where you train on one benchmark like FAST. Benchmark like fast, and that but you test on another, like scape, right? So you train on one and test on another to see that you didn't overfit, and vice versa. And we basically achieve, especially in those difficult cases of training on one data set and testing on another, we achieve a very significant performance gain. And yeah, like I said, this is very efficient. So, you know, training takes 42 milliseconds per input on a 15,000 vertex surface. Surface and you know the pre-computation is only approximately three seconds per shape and yeah you can you don't even need to subsample so there are some geometric deep learning methods that you know require subsampling of the shape so if we don't subsample you can uh you can run it directly on the dense meshes as long as you can pre-compute the Laplacian basis yeah so that's that's the main kind of goal benefit of this Benefit of this diffusion architecture. It also has some nice properties, like because we only have to simulate diffusion, it's very robust to resampling. So we tested different kinds of resampling of the surface. And if you basically measure the accuracy of shape correspondence methods, then compared to all of those other methods that I mentioned, where the accuracy drops very, very significantly as soon as you introduce summary meshing are. To introduce summary meshing, our technique, I mean, it still is somewhat sensitive to the mesh, but much, much less so than many other methods. Yeah, and so we evaluated this on different remeshed and resampled variants of the Faust benchmark. And one thing that you notice is that, so this is basically the accuracy on the original data set. And indeed, some methods are close to zero error. But as soon as To zero error, but as soon as you rematch the shapes, you see that you know the error jumps like very significantly. And basically, you know, I mean, the errors of our technique also jump a little bit, but nowhere near as badly as all the other methods. So they're much more graceful with respect to the changes in discretization. Okay, so that's basically the main message I wanted to convey about this diffusion net. And in the And in the last like five minutes, which I hope I still have, I was going to present one very recent extension, which is especially for partial non-rigid shape correspondence. So this is a paper that will appear at 3DB soon, I think in November. And the task, so basically in the the task so basically in in the previous all of the previous applications we tried to find correspondences between complete shapes and here the goal is to basically find correspondences between shapes where there's some partiality and again this is joint work with sohe pateki and gautum pai where gautum is a postdoc in our group and so yeah so so so the goal of this of this work is is a little bit more difficult than of the previous one More difficult than of the previous one, where we have pairs of shapes, but on those shapes, we have only partial correspondence. So, here, for example, only part of one shape exists on the other shape. So, there is this gray part that just doesn't exist. Or you can even have situations where you have part to part. So, there is basically parts on one shape that don't exist on the other, and also parts on the target that don't exist on the source. And so, this is the problem that we would like to solve while also being, you know, for a You know, for allowing for non-rigid deformations. So, this is basically kind of a difficult problem that has been addressed, but mostly using axiomatic methods. And so, what we use is actually the same diffusion net architecture. And so this is the overall kind of architecture. So, we have two partial shapes. We pass them through this diffusion net architecture. We compute some features on each of the shapes. On each of the shapes. But here, there is something that is special to the partial setting because if you have only these two shapes, you cannot just compute the features and compare them because there are some points on one shape that just don't exist on the other. So to discover which points exist on one and possibly not on the other, we use this cross-attention refinement that basically takes into account both features and both shapes. And that's the difference of this work compared to the previous works. Previous works. And so, and after that, we have two outputs: we have the prediction of the overlap, and then we have basically the prediction of the correspondence. And here, we use this regularized functional map module for predicting correspondence, but I mean, that's a bit less important. And yeah, so just to illustrate what this cross-attention refinement does, it basically takes features that are computed on each shape. Like you have this source and target shape, there are some features, like you can visualize them as colors. And then after this cross-attention refinement, it basically This cross-detention refinement basically just tries to intuitively kind of discover which parts in one shape exist on the other, and if they don't exist, it kind of weigh down the features to zero. Okay, so this is after refinement, you see these parts that become basically very close to zero. And so intuitively, this cross-attention refinement allows us to discover which points or regions exist on one shape and maybe don't exist on the other. And so, yeah, so we basically predict both the Yeah, so we basically predict both the overlap region and the correspondence. And so this was actually a massive amount of work to try to compare to many existing techniques because this partial shape correspondence is fairly rich field. And so yeah, we basically compare to many existing techniques. And again, it depends a little bit how you evaluate, but if you evaluate, you know, you have the training set and you evaluate just purely on the test set, then we achieve. Test set, then we achieve basically best performance on this on these data sets. And I think, even maybe a little bit more interestingly than this, we also have this extra, we introduced two new data sets, one which is called partial to partial matching, where you only have part-to-part correspondence. So you have two shapes, and basically the overlap, you know, they're missing parts on both shapes. You have to discover that. And basically, our approach is the only one that produces. Basically, our approach is the only one that produces reasonable results on this challenging data set. And also, we have another data set which basically has very significant remeshing. And again, many existing techniques just fail as soon as you remesh the input shapes. Okay. Yeah, so if you're interested, this paper is now on archive. So there's some, of course, obviously I didn't describe many details. All right. All right, so yeah, so that's basically all I wanted to say for today. So, you know, this diffusion net, essentially, I think the main idea is that we have this learned diffusion as an alternative to convolution, which allows communication on the surface, but without using local patch parametrization or parallel transport or anything like that. And we have this diffusion overall architecture that has MLPs and gradient features. And essentially, you know, kind of at least from my perspective. Of at least from my perspective, it's quite robust and efficient general-purpose surface learning architecture. And with this architecture, we can both predict non-rigid correspondences on both complete and also even partial shapes, like this recent deep partial functional maps paper that I just mentioned at the end. So, yeah, thank you very much. I really want to thank all my wonderful collaborators and also my funding agencies. And thank you for your attention. Thank you for your attention. I'll be happy to take any questions. Thank you very much, Marx, for the wonderful talk. We have time for questions. A lot of questions, actually. I'll get up, please. Ron, please. Okay, so wonderful lecture, Max. Thank you. Thank you, Ron. So in essence, what you are saying is that when you combine different diffusion kernels, Different diffusion kernels that you apply in a spectral space, it would be more stable than computing something like a geodesic circle that you expand about a point. Why is it more stable? Is it throwing away the high frequencies? Wouldn't it damage your representation power of the functions? This is one question. I have another one. Okay. Maybe the first question, I think that the reason it's more stable is well. It's more stable is well, in my mind, there are actually two reasons. So, the first reason is primarily because of the discretization issues. I mean, discretizing a local polar coordinate parameterization, in my experience, is quite tricky. I mean, even computing exact geodesics on the surface is tricky. I mean, I've taught a class in which students had to do this and more or less no one can actually do it. I mean, it is quite unstable in. Is quite unstable in my experience. And especially if you have to do things like parallel transport, it's not easy to do it in a way that's robust and accurate and not sensitive to changes in the mesh structure. Like, for example, just to give you an example, in our MDG CNN paper, we had to do binning. So basically, we discretize the different directions by just having different bins, right? And if you have these different bins that already look That already leads to some loss of accuracy. And I think, in terms of losing high frequency, so here I'm actually not sure because what we have is, okay, we have different sources of information. We have this input features and we just diffuse them using a low. So I think we use like 128 basis functions. Functions. So the diffusion is simulated using 128 basis functions. But then the gradient features and the MLPs are in the full resolution, right? There is no spectral, there is no spectral approximation there. And that I think that really helps. So, you know, for example, for some segmentation problems or even for correspondence, you need high frequencies. Like here, so in the last, in these examples, So, in the last examples, we actually tried to predict vertex IDs. And in order to predict vertex IDs, you need really high frequency information. So, we didn't lose too much there. So, it's not, I mean, I agree with you. There's something about low frequency information that is possibly definitely happening in diffusion, but because we have these other sources of information, it seems to be okay. And I have another small question about 30 years ago there were steerable. Years ago, there were steerable filters, but by Pietro Perona. I don't know if you're familiar with that. Yes, yes, I am. Yes, absolutely. Are they related to what you have been doing with your orientable kernels, the combinations? Yes, they are related. And yeah, maybe we can take this offline. It is definitely a link, and it's kind of subtle. And actually, I mean, this is a very rich. And actually, I mean, this is a very rich literature, and maybe I did a little bit of injustice with my introduction, but my goal was mainly to motivate this diffusion. But yes, absolutely, there's a lot of interesting connections. And so there's a slight difference here in the sense that we don't actually use, like, we don't impose any special structure on the filters. It's more we deal with an expanded space of functions, which is slightly different. But yeah. Different, but yeah, there's definitely a connection. Thank you very much. Yeah, thanks, Ron. So, Hannah has a question. Would you possibly talk a bit about the choice of the geodesic radius? Yeah, so I guess this is probably related to the early part of my talk. So, first of all, I want to emphasize, and probably it's clear, but just to make sure, in the diffusion net, there is no radius, local generative. Local geodesic radius at all. We don't need to do any kind of local parametrization. So, but if you're dealing with something like geodesic CNN, you have to choose a radius indeed. And yeah, so this is, I guess, a little bit one of this black art of doing deep learning, that there are lots of hyperparameters, and this is one of them. So I didn't implement this. So my knowledge comes from basically talking to students who did. Basically, talking to students who did. Essentially, there seems to be some kind of a reasonable geodesic radius, which is, I think, yeah, something like five to ten percent of the shape diameter. It cannot be too small, obviously, because then you don't have any information propagation. It can also not be too large because then convolution becomes unstable. So, yeah, I mean, that's, I guess, I think if you look at the images, I think if you look at the images, it's probably actually quite indicative of the truth. So, like these images here. Yes, this gives you a sense of the size of the geodesic radius. Yeah, I honestly don't have more scientific answers than this, unfortunately. We have another question from Lucas. Lucas, please. Yeah, thank you. Yeah, thank you. Thank you very much for the talk. It was very, very nice. I have a few questions. Actually, first, it reminded me of two other papers that I will also write into the chat. Maybe you already know them, but just in case, I wanted to sort of forward them. So, one is also about using this heat method for the heat equation, diffusion equation to compute distances. And they somehow. And they somehow normalize the vector field that comes out of this diffusion in order to get more robustness or stability. I don't know if this is applicable here in your case because you're learning the diffusion parameter, but maybe it could be useful, I thought. And the other thing is about this PDE-based group, equivariant convolutional networks. I think you also briefly mentioned them. So briefly mentioned them. And there they have a sort of a similar technique. They also use the diffusion PDE, but they also add convection and dilation PDEs and they combine all the three of them to get very good results in image recognition. And what they point out is similar to how you said that the diffusion equation is actually enough to learn certain standard operations that you usually have in convolutional. That you usually have in convolutional networks. They also point out if you add those other two PDEs, you get, I think, all the operations that you usually would like to have. And they are all invariant with respect to those groups. So it seems that you used somehow a different set of PDEs that generated all or that gave you all the information that you needed. So I'm a little bit interested in that point that you said you used those gradient functions. You said you used those gradient functions and the diffusion equation, and they used different equations apart from the diffusion equation. So, I was wondering how arbitrary is this choice, and when can you say that the set of PDEs that you chose is complete in a certain sense, so that you get all the information that you could possibly get? That's a great question. Maybe with respect to the first question. Maybe with respect to the first question, you may notice that you know one of our co-authors is Keenan Crane, who's the author of this method. So it's not a surprise that there's a link, you know. Okay, okay, okay. Yeah. Yeah, I think that the reason why they do this normalization in the heat method is primarily because they estimate geodesic distances. And without the geodesic distances, you know, and this is this iconal equation, right? Iconal equation, right? That you need the gradient to be equal to one, the norm of the gradient at every point. And that's why this normalization takes place there. I think that's the main reason. Whereas in our case, we don't necessarily need to compute geodesics. So it might help still, maybe. I'm not sure. Because if you do it for, or if I remember correctly, I mean, he's your co-author, so but. So, but if I remember correctly, they were just doing the diffusion for a very short time and then because of the normalization, they didn't need to diffuse much longer. Yes, well, yes, but there exactly is a link between diffusion and geodesic distances. There is this Pratan's lemma, which is in the limit of t goes to zero, right? So, there's a very specific reason why they do it for a short time. And they use this implicit. And they use this implicit oiler, I think. Yeah, but I think from the because we actually want to propagate information, you know, that's that's our main goal. Yeah. For your second question, I'm honestly, I think that's an amazing question and there's definitely work to be done there. So the only kind of result that we have is this, the one dilemma that I mentioned, but it's very restricted. So we only can reproduce radially symmetric convolution. Can reproduce radially symmetric convolutions. We don't have results saying that we can represent arbitrary convolutions when we have these extra gradient features. That's actually the result that we'd like. We don't have such a result. But I think that's the kind of result that we would probably want is that with this additional information, we can represent basically anisotropic features and anisotropic convolution. So, yeah, that's, I guess, we don't know. I guess we don't know. I think so. I'm like, honestly, I've seen this paper very briefly. I haven't read it in detail, and I definitely should. So, thanks for pointing it out. I think, if I remember correctly, they mostly work in canonical domains, like 2D or 3D, like grids. So, I think there might be a slight difference there that we really care about, for example, being invariant to the choice of orientation. So, like in our case, diffusion happens on the surface. And so, there's an important thing here, which is that these gradient features are invariant to the choice of the tangent space, right? Because we take inner products between gradients. And it's possible that you can probably do something similar by doing by using another non-diffusion operator, some other PDE. I don't know. I mean, I guess I should look. I don't know. I mean, I guess I should look in detail in exactly what they do. But I think maybe the setting is slightly different. You're completely right. It's definitely related. The setting and like what they care about is tiny bit different, but it's certainly related. Yeah. Okay. Yeah. I'm also not completely sure if you can restrict whatever they were doing to arbitrary manifolds, but yeah, maybe you can. So or maybe you can change it such that you can. Okay, thank you very much. Yeah, thank you. Other further questions and comments? Pablo. Yeah, hi. Thank you, Max, for such a great talk. Nice to see you. And thanks for being here. And so, my question is basically: you need to pre-compute the eigenvalues of the Laplacian operator, but then you were saying. But then you were saying, for example, you had like 128. Now, so my question is a little bit left field, I know, but how do you think that this scales with the topology of the surface? Because say if you have a surface of very, very high genus, of like genus 300, then you're going to need a Morse function of like, you know, 600 Minimard maximum to like be able to capture that. So you're going to need as many. So, you're going to need as many eigenvalues or eigenfunctions to capture that as well. So, I know that it's kind of left to you because there aren't that many databases with like high. Yeah, I was wondering. I'm super curious. What is the task? Well, the task is, I mean, really just being able to distinguish surfaces with more complex topology, which kind of appear more naturally in geometry and topology, but maybe in graphics. Topology, but maybe in graphics and in other domains haven't been really studied in that context, maybe. But I guess that's, I mean, the basic question is, in a nutshell, how do you think that your methods scale with changes in topology, specifically with genus getting higher and higher? And kind of a comment related to that is that basically, I think that you would just need more and more agrovalues, but I think it would probably still work quite well. It's just, I was wondering if you'd. Quite well. It's just, I was wondering if you'd have considered it. I mean, honestly, thank you for the question. I haven't thought about this at all. I think that's a very interesting question. So, you know, in the paper, and also I think in the code, I'm not sure if it's released, but I should be, we compare it to this backward Euler diffusion simulation where there's no pre-computation at all, right? You just do everything directly on the surface, and you just simulate diffusion with a flash. You just simulate diffusion with the Laplacian matrix, right? And basically, in all cases that we tried, the spectral approximation worked better because it was more efficient with basically no loss of accuracy. But maybe you have just come up with a case where that is not the case, right? Maybe this, you know, using this backward Euler approximation in that kind of setting could potentially be better. And I think. And I think it's probably in the release code as well. If I mean, I can check. But I mean, that'll be very, very interesting to me. I haven't seen it, so that would be the first. But yeah, that's actually a very interesting point. Thanks. Thanks for agreeing. So thank you very much, Max, for the wonderful talk. It's actually very fascinating. But I love to have people to ask you questions, but actually, we have to wrap up because of course. 