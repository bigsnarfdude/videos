Lots of new tools, learning lots of new things, and relearning some old things for better. So it's really great. And uh, Matthier Defaque. Okay. All right. You can mispronounce it almost anywhere you can. Yes. And the title is kind of like a short version, but it's already, you know, several long words. But so these are divided different spots. But so these are divided difference operators for cohomology, like equivariant cohomology rings of regular semi-simple Hessenberg varieties. But that's a paragraph, it's not a title. So yeah, first caveat, all of this is regular, semi-simple type A stuff, because that's what I know. And all of this is going to be sort of algebraic, because I don't know much geometry, but people who do know geometry tells me that divided differences mean something geometric. Divided differences mean something geometrically, so that's great. And another note is: you can see the shared folder. I put up a PDF, it's not even archive-worthy yet, but it has the definitions and the computations and the proof for what I'm going to be talking about. So, some context to start. I'm going to try to give you a big picture of the context with. Of the context with no definitions, and then afterwards I'll go back and try to fill in the stuff. But feel free to ask questions. Alright, so we have this beautiful story that starts in the 70s and the 80s, where we have the flag variety, just of all the flags, which is a beautiful variety, and it has cosmology rings. Cohomology rings, well, a cohomology ring. So it's natural to ask while it is a cohomology ring. And it turns out that you can write some generators and relations, and it's just a polynomial array in n variables, modulo, symmetric polynomials in those variables. That's very nice. And you might say, well, that's a quotient. I'd like to face this. And then you find that you've got these Schubert. We've got these Schubert polynomials, which form a basis. And we already heard about these k-vings by Schubert classes, and so the classes give you the polynomials. This is a quotient, so there's more than one choice, but there's a very nice choice that works for uh for all of them. And so we've got these n factorial polynomials that give you a basis for the cohomology ring. For the cohomology rate. And then you've got n factorial things, so you want to do some combinatorics. And it turns out that you can define them very nicely combinatorially by looking at divided difference operators, which act on the polynomial. So you start with sort of the top-degree Schubert polynomial, which is Schubert polynomial, which is just in fact a monomial, and then you apply divided differences and you get all of the other Schubert polynomials that way. There's n factorial of these, so you might think, well, these should be indexed by the permutations. Well, sequences of these should be indexed by the permutations. There's more than one way to get down from the top to a given Schubert polynomial, but all the ways are the same because these operators satisfy the. To satisfy the three it needs. So we've got this very, very nice story going on here. If you want, you can get fancy. You can say that there is an n-dimensional torus acting on this. So this is where the diagonal matrices that Wilbur told us about. And then what you have is the equivariant. Is the equivariant cohomology ring, which is still a polynomial ring, but now you have two sets of variables, and then you mod out by a different ideal, but still related. Then you wonder, do you have a nice basis? This is still a free module over the polynomial ring. And then you have the double Schubert polynomials, which behave very similarly. Which behave very similarly. I mean, if you set your second set of variables equal to zero, you recover ordinary cohomology, and the double sort of just different. The very same divided difference operators will give you that recurrence where you start from the top degree one and you get all the other ones. And it's just you start with a different one. So you have a second set of variables mixed in. And so it's still. And so it's still the same operator, so it satisfies the break relation. So this is kind of like the fancier story. And then in the 90s and the 2000s, a bunch of people looked at this and wanted to generalize. And so we've got, we can look at a bunch of sub-varieties, the Hessenberg sub-varieties here. And in fact, we can generalize a whole bunch of this picture to the setting. This picture to the setting. So here we had, you know, a single flag variety for a given n. Here we have Catalan-many Hessenberg varieties because we have one for each of those Hessenberg vectors that we talked about. So even you expect even more beautiful cometroids here. And we can still talk about the equivariant cohomology. Cohomology rings. And here we have, so these are sub-varieties, so there's an inclusion map here to here, and it means in cohomology it gets reversed to a map from equivariant cohomology of the flag to equivariate cohomology of these other ones. It turns out this is very special, that they are inclusions. That they are inclusions. So they're injective. And next thing we have these necropohomology rings, here we have sub-varieties, and here we have super rings. Not in the sense of some fancy non-commutative anything, just in the sense that these are bigger rings. They can take. Oh, I don't know. Or whatever. Then, what else can we generalize? Well, these Schubert-Polyhammels, it turns out they do generalize. Schubert bottom also turns out they do generalize. So here the concept is the flow-up basis. So it turns out each one of these is super because they're not sub-rings. The containment goes in the other direction. They're bigger. They're bigger. Included? I don't know if there are like sub-rings or some other word? They're just bigger. Just bigger. Yeah, so we have this concept of flow of basis. Here in this generalized setting, we have many flow-up bases. They're not unique anymore. But this is the generalization of Schubert-Paul-Miltz. And in the case of the full flag variety, there's a unique one, and it is the double Schubert-Paul Milt. So that's to generalize. The generalization there, and using these, you can show that, in fact, these are also free modules of dimension n factorial. Right, so I'm running out of room at the bottom, but I want to say that these, so we also have a group action, we have supposed dot action of the symmetric group on these cohomology rings. Of the symmetric group on these cohomology rings. And so they are free modules of dimension n factorial. We can take graded Frobenius characteristic of them. There's a choice here of which base ring you take, and one choice will give you the unicellular LLT polynomials. And the other choice. And the other choice will give you the chromatic quasi-symmetric functions. And as we heard already, this uses a relation by pleptism. The plethism is just implementing this change of base ring that you're using in your applications. What else? What else is my eyes that I like? So there are these modular relations, and I like to think of them as, so Foster told us that there are these deletion-contraction-like things you can do in a more general setting, but even in this restricted settings, you don't have a direct deletion-contraction formula, but I think the modular relations are in the spirit of deletion. Relations are in the spirit of deletion contraction. They're what we can get from there. And so they are a bunch of linear relations between the chromatic symmetric functions of these Heisenberg varieties. And a couple of years ago, Abr√© and Niger showed that actually these relations generate all the linear relations. So they, you know, with a few base cases, they uniquely determine the chromatic symmetric functions. Right, so this is all. Right, so this is all very nice and known. Do you have modular relations on the UB cell? Do you have an LLT polynomial or something? Yeah, yeah, the kind of like the change of basis thing is just a linear thing, so you have exactly the same collection of linear relations. So, what is new, what I want to tell you about that I got in the past couple of years, is that in fact. Is that in fact we can fill in? There is a definition for divided difference operators here for this row. And in fact, if you line up your notation carefully, they are, it's the exact same formula. So it's the same formula, but it is a bigger ring, so it is an extension of it. But it's the exact same formula, which is very nice. Satisfies the same grade rate. Very nice. Satisfies the same grade relations. Right, but there is kind of a catch, so you have to be a little bit careful. They're not always defined. So you can take divided difference of any element of your cohomology ring. Here there are some of the cohomology rings which have some of the divided difference operators, but not all of them. It's very easy to characterize which ones they are. Which ones they are. So the domain, you know. So there is a question about the domain, and we can answer it. I should say domains. So which rings support the divided difference operators? There is also another question of the codomain, because it turns out that here you divide a difference over here and you stay here. Here we have a collection of Here we have a collection of cohomology rings. And if you do divide a difference in one ring, you might land in a different, in a larger ring. So it's important to look at where do you land. And that, we can also say where we land. So that's a little bit trickier. That's that. So this is not just regular semi-simple, it's any kind of asymmetric variety or? The regular semi-simple type A. The regular semi-simple type A. I mean, essentially, I just do the GKM rings over here. I don't actually do any geometry. And the other, the third thing I want to say is that with these divided difference operators, we can actually decompose the symmetric group representations here and implement the modular relations at the level of so these are on the level of characters, but we can actually get the same relation on the level of the representations. The same relation on the level of the representations. With a nice, you know, explicit formula, this is the explicit isomorphism. Any questions about the big picture before I start defining some of these things? So landing in a massively bigger range, so the total name is the changing Yes. It's still in a Hessenberg, yeah, it's still in one of these Comalgrings, but a different one. Yeah, different age. A different one. Yeah, different H. Sorry? Yes, so I mean I'll get into that, but that's kind of the next thing I want to say is that these, you know, there's some inclusions here, and you have reverse inclusions here in the Here in the so and I'll say a little bit about like the Borel model here versus the GKM model here and how they're related. So I'll get to that in a little bit, but I don't know if that is what you're asking. And if we differences are very nice to have explicit correspondence. Yes, yes. I'll write down the formula. It's you know this minus this divided by that minus that. No, I mean geometrically there is some correspondence alongside. Correspondence variety. Yeah, I've heard that it's related to blow-ups of some kind. But I don't know any geometry, I'm sorry. So, like, someone in this room can probably answer better. Other questions? So, let me just see. So, Laura already told us about flag varieties and has some rear varieties, so I won't. Flag varieties and has some real varieties, so I won't say much about them. I won't say much about unicellular LLT polynomials. These I will talk about super polynomials and flow bases. Well, I guess I should say light max. I'm not going to talk about more. And these bubbles I will talk about. Which bases do these? Which bases do these give you in the Hessenberg variety? Sorry? Like if you do the divided difference, do you get a basis of the Hessenberg? Is it. Ah, good question. So here, yeah, you're right. So we start with one and we generate all the other basis elements from the divided difference operators. In some cases, some flow-up basis elements, you can hit them with a divided difference and get another flow-up basis element. So that's very nice. So that's very nice. But it doesn't generate all of them. So you might have to start with more than one sort of top thing and see what you can get. But I think this is kind of interesting because it's an open question, like what is a good choice or the best choice on a flow of basis. And I think we need more conditions, and one of them might be, well, sometimes you can start and apply divided differences to get some of them. Those might be better. Those might be better, like it might be a good choice or help you make some good choices. Oh, okay, that's my next point. So, the flow of basis, there's not a particular one, just any family of ones that's. Yeah, and we know it always exists, but we don't have there's arbitrary choices to make. And there's not even like a good, even like, you know, take the lexicographically least or something like that. I think it's just like there's arbitrary choices to make. So let's get started on that. So kind of I think to go along with the question a little bit of like these sub varieties and super rings, I just want to point out that the reverse inclusions do get reversed. So for the For the three n equals three example, you have sort of this is a Hessenberg function, you know, three three three, or a Hessenberg vector. And there's not just a collection of them, but there's like, there's inclusions between them. So this one is smaller than that one. So we have actually a closet structure. So we have actually a poset structure on these, you know, Hessenberg vectors. The top one corresponds to all flags. So that's the entire flag variant. And the bottom one, as Lara computed with us, actually consists of n factorial permutation flags. So it's n factorial disjoint points that are just, I guess, floating around. That are just, I guess, floating around. And they are, you know, you look at the eigenspaces of your operator, your regular semi-simple operator, and you just take them in all possible orders to build up the fly. So these are on the variety side, and then on the cohomology side, the image gets flipped. So that the So that the this smallest variety turns into sort of the biggest ring that I'm going to call H, script H, and then this all flags thing turns into the smallest ring, which I'm going to call B, script B, or Bere O. So just a quick note there, and this is the, I'm going to be using the GKM model. For this. And so, in fact, all of these are going to be subrings of this big room. Yes? I'm going to ask a sort of basic question here. When I see, you know, talking about one of these for each Hessenberg function, are we always by default assuming that the S is like regular semi-simple? Yes, I've picked one fixed regular semi-simple. Okay, through this whole talk, it's all regular semi-simple. Yes. Okay. Yes. So, just a quick note there. Now, let me give you a description of the equivariant cohomology rings. Here I'm going to be very explicit with Polynomials and relation so that we can all draw the line. And I'm going to be pretty cagey about which subring we were going to use for a while, but we're going to need to define some SM actions, actually two of them, before we can just define everything properly. So we want equivalential homology to have, we give you some. Going to have, we give you some varieties. There's going to be the left dot action. What is the actual ring? The right star action. Okay, so for a point, you know, a single point that has a very boring ordinary cohomology, just a complex. Ordinary cohomology, just the complex numbers. Everything's going to be over complex numbers. But already, even a single point has interesting equilibrium cohomology. Because you have a torus action, you get polynomials in n variables corresponding to the torus sum. And then we have, for this B, we have all flags. So what ring do I have here? So, what ring do I have here? Well, I have all of the Torres variables, and I have a second set of variables. I'm going to call them R1 up to Rn. They're on the right somehow. And so it's, you know, these are your generators. And then you have some relations, which is that, in fact, I want for every Symmetric function, symmetric call of million n variables. I want that, when evaluated at the t's, to be equal to the same functions evaluated at the r variables. So I'm pushing it out by that ideal. That's my Borel model. And then for Hessenberg, I'm going to have some subring of. Now, it's going to be a subring of the big thing. The big thing is a disjoint set of n factorial points. So I should expect to have n factorial copies of homology of the point. And there's n factorials, so I'm just going to index them by the permutations. That's what we do. Okay. And then before I can tell you which suffering, I have to tell you about these actions. So we have the left action on the dot, the dot action on the left. And so here, what we're going to do is we're going to permute the variables. The variables when we act on the point. Questions? I mean some subring of. A subring of is what you mean by the bottom, right? Yes, some subring of. I didn't look at the top. Okay, sorry. My eyes are fixated on the bottom right. For this Borel model, what we're going to do is we're also going to permute. The Pi's, and we're going to fix the other variables, just like leave them alone. And well, before I calculate the left action, I forgot I was supposed to write down an example of this. Okay, so again, for n equals three. What, you know, how am I going to draw one of these, you know, Sn index tuples of polynomials? I'm going to just draw a graph with all permutations on my vertices and then put a polynomial at each vertex. So some example of here could be, all right. So I've got my six permutations of three, of one, two. 3 of 1, 2, 3. And everyone agrees on these labels, and the other two are controversial. So I'm going to say that's 231 and 312. And for the people who like simple reflections, it's the identity S1, S2, S1, S2, S3, S1. Here I have, I think, S1, S2S, SS, SS, SS So here I have I think S1S2 and S2S1. So that's my labeling of the vertices. If your conventions are different, translate. You have an example to translate. Okay, so I have, you know, I could put any polymer. So I could put, you know, a constant here, a zero there. I could put a variable, maybe another variable over here. maybe another variable over here. Let's see, up here I'll put t1 minus t2. And here let's make a real arbitrary polynomial t2 squared plus 5t3. So that's what an element of this ring h looks like. Now to act on it on the left, well I'm going to permute vertices and my variables, ti. And all of this is going to do on the left. It's a left action. The one. I don't like you pick because you put the edges. I think you should move the edges if you're putting any problem. Or. But like, how do you know that this is one coherent picture? No, but then the edges, because you could put all the And the edges, because you could put all the you make the complete graph if you were. Yeah. Well, I'm gonna keep drawing the convex hull of what's going on. Yeah. Okay. It's just a convex hull. It's not actually edges. This is the broom I work with. Yeah. Oh, you should write that. Oh, okay. Sure. I mean, I just need some device so that this looks like a single object. Okay. Object. Yeah, I know it's cost me to draw some edges. I'm going to draw some things soon. I can add the edges, and then I think everyone would be happy. I think, right? And I'll be wow. Happening and not happy. That's not related to the talk, though. So let me tell you the left action. So here I'm going to say this F, I'm going to hit it on the left with the simple reflection S1. So this is swapping 1 and 2. And what do I do? Well, I have to put And what do I do? Well, I have to permute the vertices. I'm going to do it this way because I'm acting on the left. And then the polynomials kind of swap. So 1 goes over here, T1 goes down here, but it also gets relabeled into T2. 0 goes up here, T3 goes down there, gets relabeled into T3. T1 minus T2 comes down here and gets relabeled into T2 minus T1, and that other problem. And that other polynomial gets t1 squared plus 5t3. So that's an example of the left action. Remute the vertices and also relabel the variables. Now on the right, you have the star action, so it's not going to actually fix everything in the part. There's nothing else in you. Keep it that way. Keep it that way. Here, we're going to fix the Pi's and permute the Ri's. Actually, I've been telling you what to do on the variables, but it should be pretty clear that the ideal is fixed by each of these actions, so this is well defined. If you permute the variables in a symmetric thing, it still doesn't change. That's you find here. That's you. Here. Could you explain quickly how that presentation you're giving relates to that ring as a subring of this? That is my next slide. Well, two slides from, I'm going to give you an example of that. But yeah, that's a great question. I'm going to tell you. And over here, we're going to commute vertices only, nothing with the variables. Nothing with the variables, and we do it on the right. So whatever conventions you're taking, make sure that these are not, you know, you're acting on the other side. So the star action is the one that corresponds to the near cellular LLT somehow? Or somehow? It's both the dot action in both cases. In the chromatic case, it's actually a twisted representation because we are acting on our ground ring. So it's a twisted representation, which is an important point. It messes up all your calculations that you forget. And on the right, it is also the dot action, but now your base rate is with the R variables instead of the C variables. So, continuing the example. So, on the right, I'm going to swap things. Swap things differently on the right. So 1 goes over here, T1 also comes over here and does not get relabeled. T3 comes up here. T1 minus T2 comes down here. 0 moves up here. And this polynomial over here moves down one spot. So that's an example of the left action and the right action. The left action and the right action on an element of this big rhythm H. All right? It's just to click, can I? So if I'm going back to the full flag variety and you had to tell me in words, the dot action versus the star action, if I understand correctly with the dot action on the full flag variety. Well, okay, so you tell me, it permutes the TI's and fixes the R's, and then on the other side, we fix the T's and permutes the R I's. Side, we fix the T's improving CRIs. So it's like, depends on what you potionize. If you potion out the T's, you get the tributes. If you portion out the R, you get a regular representation. Yeah? So? I mean, to get to ordinary coherence, you're always going to portion out by the T's. Those are your tors right now. Okay. But on this one coherence, you're going to have an action on the left and an action on the right. One of these is going to give me the Springer representation? The Star Guard is the Springer. Starship, right? Yeah, Starship. These are known actions, so yeah, yeah. Good. Now I want to tell you how these are related. I'm going to have to erase the example. Sorry. So memorize, I don't know. Just remember that you're acting on the left and on the right. And you have to be careful about which side you're acting. But now, to answer John's question, how would we view this as a subring? Well, I just have to tell you where the generator is going. You where the generator is going, right? And here I'm going to use as a hint the fact that I know how the action works, so I can go a lot of constraints on what it should map to. And so T1 is going to map to just put the same polynomial. Every you already have a polynomial in T's. Just put it at every vertex. And you can check, it's going to be correct. Check is going to be permuted here because you're going to permute the vertices, doesn't do anything, and then relabel that does what you're supposed to have. And the write action also works nicely. So RI is going to go here. I'm going to draw my labels again of the permutations because I'm going to use that information. I'm going to use that information to be a TI at every vertex, but possibly a different TI. But isn't 1, 2, 3 TI? Yes. Thank you. That's... Wouldn't have mattered in the case of R1, but would have mattered in the other case. So because it's R1, I'm going to take the first number, and that's the index of And that's the index of Ti. So this is going to be T1, T2, T2, T3, T3, T1. So that's where R1 goes. R2, I would look at the second entry on my permutation, and R3 I would look at the third one. So those are my elements. You can check that all the permutations. That all the permutations act correctly on that, so that it's at least a feasible embedding. You can also check that the quotient works out, because what if you take a symmetric polynomial in these, you get that symmetric polynomial at every vertex. If you take a symmetric polynomial in the R's, you get that symmetric polynomial with variables permuted, so the same thing, at every vertex. So those are equal elements. So it at least is plausible. But that's it's true. But that's it's true. So that's the embedding. And now let me tell you, can I tell you? Yes, now I can tell you why it's severing. So So I'm going to call it H of H. So, what is this? This is the equivariant cohomology of the Hessenberg using the notation we have already for a regular semi-symbol S at h. At h. So it's going to be those two poles of polynomials such that I'm going to have a condition for a bunch of pairs of members. So for all ik such that i less than k less than or equal to h of i, this is where my data from the Hasenberg vector Vector comes in. I'm using ik because i and j, they just look the same. So for all of them, f minus f star with the transposition i and k acting on the right. So I want that to be a multiple of ri minus rk. So that's the ik definition. So that's the IK divisibility condition. Some notes. So you'll usually see this presented differently in the literature. It's usually for every vertex, you know, the polynomial at that vertex minus the polynomial at some other vertex is divisible by some ti minus tj, which depends on which vertex you're at. While that combination of ti minus dj exactly corresponds to ri minus rk. Ri minus Rk. And you know, that difference is exactly taking, it's swapping the vertices in exactly the way that you would compare those pairs of vertices. So it looks different, but it's just a more compact way. It's also kind of foreshadowing what we're going to do. Another question you might have is, is this easy or hard to satisfy as a condition? It's easy to give examples. It's easy to give examples because it turns out if you take the multiples of Ri minus Rj, Rk, they do all satisfy the condition Rik. So Ri minus Rk is an example, and here I'm going to refuse notation a little bit. If I just want the IK condition, those are all examples. So, this entire ideal is contained in this entire subarray. And it's an ideal, so that means if you want to satisfy a bunch of conditions, you just multiply by a bunch of ri's minus rk's. So it's easy to give examples. H of i sub k means what? Just the subring where I just put one condition. I'll put one condition. Not even like, it doesn't even have to be like in a dict path shape. Like, you know, these are. Like, you know, these are an intersection of these sublanes. The lobby is a subway. No, you have to check. But it's not HIK is a subway. I mean, I know it's obvious that H. I mean H of H is a subway. But is it clear that H by H by Th is subway? No, you have to check. So you sit down and you expand it as. Not obvious, but trivial or something. Yeah. Sorry, I think when you write R there, so this thing is everything is a subarring of this H which involves only the T. So when you write an R there, which I told you is in H. So by an R, you actually mean one of those things. I mean one of these. Yes. And so then you're actually going to multiply point by point when you're multiplying by point. When you're multiplying by R I minus R. Yeah, it is. You're multiplying point by point by a difference of two pictures like that. Yes, I didn't tell you the restructure, but yes, it is point-wise addition and multiplication at every vertex. Or vertex-wise. Thank you. Great. Sorry? Is R the turn class? So I'm told that the T I's are trivial, but they've got a torus action, and the R's are the turn classes of the topological bundle. Is that okay? Is that okay? That's right. Yes. And then B is just the subring generated by T and R. Yeah. Does that mean something geometrically? Like, is it a connected component? No idea. Ask a geometer. Ask a geometry. Yeah, sorry. So other questions? Where was I? Where was I? Oh, yeah, one other very important point is that I'm using the star action to define these. I'm putting a condition. The left and the right action, you know, the dot and the star, they commute. So obviously, these conditions are preserved by the dot action. They're not preserved by the star action. So star does not act nicely on H of H. But it does kind of permute the divisibility conditions. It doesn't even permute the big paths, though, because you could permute the path. The big paths, though, because you could permute your set of divisibility conditions and end up with a set of boxes that is not even a big path. But you know, it's not horrible, but it's not super nice. Just you can use the dot action here for sure. The star action is a little bit trickier. Keep that in mind. So, finally, we have the rings, and then I said there was some foreshadowing. Let's do the dividend differently. Let's even divide the differences. So I've got this partial i, I've got n minus one of them. And then for the Borel presentation, this is very well known. Very well known that what you should do is, you know, we said it's divisible, well, divide. Do it. And here I'm taking the simple reflection ii plus 1. And I'm dividing by ri minus r i plus 1. And everything works out. This is all, this always works. always works. What I'm going to define is how do we do it on this bigger H ring? We do the same thing if it exists. So it's the exact same formula, you just reinterpret your symbols a little bit. I should note that that element ri minus rj is non-zero at every vertex, so it's not a zero divisor. So the quotient, if it exists, is unique. So we're good there. And next I should tell you, well, when does the quotient exist, right? What are the domains? So the domain lemma is that if I have, if I'm in one of these subrings, cohomology subrings, and h of i is strictly bigger than i. Bigger than I, then the i-th divided difference exists. But it could be anywhere in my big ring H, right? Because I had the visibility conditions, I divided, who knows what still holds, right? That's not clear, but then we. But then we have the codomain lambda, which is that furthermore, if I actually can hit this with the star action and be preserved. then partial I of f lands in the same subring. So if my subring happens to be SI stable under the star action, then I land back in the same thing. This is less trivial than that other question. It's not, I mean it's a computation, it's in the paper I posted, but it's this one's trickier. Yes, and it's also easy because these SIs kind of like permute the divisibility conditions, you can write down explicitly, you know, what does it mean for this, like when does this happen. If you're looking at the graph, the unit interval graph for this, it means that swapping the vertices i and j, i and i plus 1, doesn't change your graph. You get the same graph value. It's a very nice condition. Extra note, okay. So the moment graph, though. Yeah, you see the graph you're talking about. Well, the moment graph is the hexagon. But this graph we're talking about is just on three vertices. Oh, okay, the unit. With the unit on the vertices. Incomparable. Incomparably. So this doesn't always happen, but it happens often. And yeah. And this one, when does it not happen? Right? The domain limit, I think, is one. Happen, right? The domain limit, I think, is just like a technicality because when does this not happen? The only exceptions are when your dig path actually hits the diagonal at point I, when like h i is equal to i. And in that case, I mean, we can decompose stuff anyway. We don't care about the difference. I equal at the beginning and the end. Well, it always hits at the beginning and the end. But if you hit in the middle, But if you hit in the middle, then that's when you actually won't have the i-th divided difference. Oh, because you don't have one at the end anyway, because i equals n is not in there. Yeah, no. Yeah, so the divisibility condition that we're using to do this division is this box here. Oh, so basically all as long as it's connected. Yeah, if it's connected, you have all in particular that has to get there. Yes. I mean nobody cares about I mean I know it's about Well, it just breaks up into pieces. And each piece is good. Okay, so I have time to tell you this. And you can check, you know, whenever it's defined, it satisfies the braid relations, it squares. Grade relations, it squares to zero, it's a skew derivation, it's super nice. Very nice to work with. But now we can actually use it. I'm not sure if you can say just a few words about side and grade relations. Sure. So SI on grade relations. So for the symmetric group, you know, it's Si times SK. times SK is SK times Sixtus I is not right next to K. And Si, Si plus 1, Si, Si plus 1, Si, S plus 1, and I guess Si squared is the identity. So these are the brain relations, and if you throw that in These are the brain relations, and if you throw that in, you have the symmetric group, the presentation of the symmetric group. So for these operators, same thing happens. But you compose them. If they're both defined and you compose them, then you get the same thing. And this one as well. And that one is trivial, it's just you will expand it. Trivial, it's just you expand it, then you do it, and you collect some terms. And then when you do it twice, you get zero. And it's also just you compute, you get it. Other questions? In particular, I guess this means that you could index it by You can index it by a reduced word for a permutation and talk about, you know, delta sub a permutation if you want. And that's going to be well defined. Yeah. So if I understand correctly, you've got like a parabolic subs worth of things you can do and stay inside. So can you take a seat like the longest coset representatives? You mean to get four seats for the longest coset? You mean to get close? Uh yeah, so if you if you have if your if your dig path is parabolic, so it's a bunch of like, you know, complete graphs, then yeah, that'll give you I mean it'll give you a product or a Schubert, you have a lot of transpositions that you can apply, right? So they generate some parabolic subsets. Yeah, it could be trivial. Oh, yeah. Frequently it's trivial, right? Yeah, yes, yeah, yeah. Yeah, yes, yeah, yeah. The more edges in in the Hessenberg way, the more you can do. Yeah. So now it's right in in the entire plate where you have one C. Yeah, one C available, and from this one C, you get everything. And sometimes for the other cases, if you have, there are potentially many seeds where they each give you a little bit of a flow-up basis. I don't know if that always gives you. I don't know if that always gives you a question. You take some set of coset representatives for the, you know, the group generated by the SI's that have this nice property. You apply those delta i's to these seeds that are coset representatives, can you get everything? Uh I think you'd have to define those seeds. 'Cause the seeds have to be in there, so like seats have to be in there. So if I had all of this as my part, then I know I can take sort of the Schubert problem really like that product of Ti minus Ri or minus Rj. That's going to be in my ring. But if I do that, I don't know if that, but I mean, think of it this way, though. These divided differences will lower your degree. So So already that puts some constraints on how much you can get. You stay in the same culture. The same culture. Everybody's looking at me like I'm crazy. Maybe I'll excuse me. I'm confused, but. Oh, you're saying what if we have, okay, what if we have several deltas? You have a certain SIs that work, and that tells you which delta I's work. Those SIs generate a parabolic. That's always going to work because I think what you're going to do is you're going to be essentially consistent. What you're going to do is you're going to be essentially considering the cohomology of the partial flag variety as a bundle over the variable. To clarify what John was saying, if you have, you look at all the SIs that preserve this, that's like a DICPATH or Kruvenia Hessener vector invariant under these SIs. It's going to generate a young subgroup of the symmetric group. Yeah, it's generating a young subgroup of the submetric group, and maybe in this case, the C. And maybe in this case the seeds, to be precise, would be the classes you'd get for the Gmod P thing. Or just like some follow-up classes associated with maximal coset representatives. Yeah, I think that'll work. I understand why you're saying that. But in general, there would be very few of these SRs, right? Yeah, you can think of plenty of examples. You can pick up plenty of examples where there are none, right? That you can't do this yet. The seeds are everything. Yeah, in which case we still have the same problem of one. But I want to write a theorem. So coffee break. Coffee break. That's a different theorem. Okay, so stable decomposition. I want to write two theorems, actually. Let's do it really quick. Okay, so I'm going to say that if it, if an A. I'm going to say that if an H satisfies this condition, that its H SI is stable. So let's see. All right. Theorem. If H is SI stable, then this cohomology ring, as a dot-action representation, decomposes. Decomposes as, okay, so this one, so H. You can look at the fixed elements under the star action for SI. You can look at the anti-fixed elements. And this is, I'm not saying anything important here. I mean, this is just, you get this decomposition for free. The actual theorem is that we can identify the pieces. Identify the pieces, well, I mean, identify this piece as Ri minus Ri plus 1 times the fixed piece again. So the theorem is here that we have an isomorphism between these two. Well, not an isomorphism, like an equality. I want to stress that this is an internal decomposition, right? These are sub-representations and they're orthodox and they're genericable. And they're orthodox, and they generate the whole thing. So, these elements can be described as the anti-symmetric ones or as multiples of, like symmetric multiples of RI minus RI plus 1. And here, just to write down, the proof is divided differences, because you take the fixed ones. The fixed ones and the anti-fixed ones divided, like the divided difference brings you that way. The way you prove it's an isomorphism is you write down the inverse, which is just multiplication by that Ri minus Ri plus 1, you know, half. So those are, that's the isomorphism. So it's, I like it because it's just like it's there. One consequence of this is that if you are a si stable, then for the ith divided difference, kernel equals image. That's exact. But it's not just that the square is zero, it's you know it matches up exactly. Yes, to repeat the arrow from the right way, so from this side. So, from this side to this side, you apply the I divided difference. From this side to this side, you multiply by Ri minus Ri plus 1 over 2. If you want to actually align the composition being the identity. And then what we have next is very quickly, almost stable. Decomposition, which is, it happens when you have a triple of Hessenberg functions, right? So I told you like the domain, co-domain, it's kind of annoying. It's a blessing in disguise, because it means that when you have different Hessenberg vectors, you can now relate their cohomology rings. So I'm going to ask that we have three Hessenberg functions that cover each other. So I add exactly one box to go from here to here and from here to here. Go from here to here and from here to here. I want this to be a sort of an SI stable sandwich with SH1 being unstable in between. So in that situation, so I know that these cohomology rings are Cohomology rings are contained in one another in the reverse order. I know this top and bottom line are stable, so I can apply that theorem over there. Same over here. Q and then stable, okay. And now the middle part, that's the theorem, actually has the stable part of the bottom and an isomorphic copy of the stable part of the top. Part of the top. Now, this is Ri minus Rk. That K is like telling you which box is unstable. And this might be an I or an I minus one, depending on which, you know, but it's something like this. So this colomal D ring as a dot action representation breaks up into two pieces. One comes from here, the other one comes from there, with, you know, this. I'm going to go over with one. I'm going to go over with one minute and say that that is the modular relation. So you take the greatest, the greatest characteristic of the middle thing, and you get the q chromatic symmetric function. And you say, well, it's this piece, which is, you know, one over one plus q of this, because there's a degree shift. Plus q of this, because there's a degree shift. And it's the upper part, so it's q over 1 plus q of the other one. That's it. I guess the question is just wait until the coffee break, yeah? We don't have today, but we have tomorrow. The coffee break today. Are we starting here in the next half or not? Okay, five minutes. 