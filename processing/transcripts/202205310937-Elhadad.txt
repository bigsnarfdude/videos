But I'll start with good news. So about a billion visit notes per year in the US alone, and then we're one-fifth of the world population, but we have a lot more visits than others. But you know, we're talking about 20 billion visit notes that we could be getting information from to build up our databases. But there are problems. When I teach the medical students, and some of you have seen this slide, I ask them what, because I try to get this point across. I ask them, what does PERLA mean? And they tell. mean and they tell me it means pupils are equal round reactive to light in a combination and I say I didn't ask you what you stood for I ask you what it means when you read it in the chart then they start laughing the ones who get it because doctors write pearl in their physical exam they don't even look at the person's eyes they just write it down as part of the exam so you need to interpret not what's written in the chart but what the person who wrote it in the chart was thinking at the time and so usually and this is a picture of a woman with a prosthetic eye Prosthetic eye, and so I asked how many people with one eye are Perla. You can't have equal pupils if you only have one of them. And we did pretty well. Only 2% of people with one eye are listed in their exam as PERLA. But 8% are listed as Perla on the left or the right, which is a misuse of the term because you can't be equal on one side. So the point of all that is that the data that you read is not necessarily the way you're reading it, say as a non-doctor, is not indicative of what was really being conveyed by the doctor. Being conveyed by the doctor. The data are mostly missing. When someone says they have a database of 2 million patients, half of them look like that. They have a single data point because they came to the ED with a broken finger. And you can't do a lot of research on that. And so then you look, and it's very episodic because they're an inpatient here, then they weren't seen for a long time, then they came back again. So this is the kind of data that's in the health record. And then there are studies that show that as low as 50% accuracy, that's for something like do they smoke? See, that's for something like do they smoke? Do they have diabetes is 95% accurate, but do they smoke is only 50% accurate? And this is taken from I changed the year for HIP or whatever, but that's from a real paragraph in the record, which says 27-year-old woman and 36-year-old woman. So one of those is wrong since they were supposed to be current. So there's the truth of about the patient, a concept in the clinician or the patient's head, that gets recorded in the record, which we try to use. In the record, which we try to use for doing our research, but is meant to be given to another person and interpreted. Another thing is that the narrative text with Noemi talked about is complex, slight increase of pulmonary vascular congestion is a slight increase, not a slight congestion, and so on. And then the processes of obtaining it are biased. It's not only biased, it's a feed-forward network where we do something to the patient, then we measure it, then we do something else, we measure it. Then we measure it, then we do something else, we measure it. And that influences what we're measuring. We could be missing a whole line of stuff that's important because we didn't go in that direction. Here's an example where this is severity of illness. This is whether you die from pneumonia. And so the real study that was done years ago is the green, which says the sicker you are, the more likely you are to die. This is the EHR study, shows that in general, you're more likely to die to get sicker, but if you're really healthier, you're But if you're really healthy, you're very likely to die more so than the second most healthy group. And that's different reasons, but the most important was that if you come into the emergency room and die of pneumonia immediately, no one sits there and records all the symptoms you had, or you can't even, don't even know what your symptoms were. So you look like someone who came in with no symptoms and died. Therefore, you were a healthy person who died, because death wasn't part of the symptoms there. That's pre-death. There, that's pre-death. So you have to account for that. And we can account for it by doing certain filtering. But when I did that filtering, I went from a data set of 18,000 to 1,900. So I got rid of 90% of the data, which makes you think maybe there's some bias that the 10% are left. I'm going to show you some of the examples. Then what I would normally do is show you the phenotyping, and that's where I say that, well, but the good news is that doctors. Say that, well, but the good news is that doctors can do this, so we just need to mimic the thinking of doctors. What we have to not do is ignore this and pretend it's all okay and just use it naively. So, but the information is locked in there. Am I going too long? I'm okay. We did some studies. We just did lag linear correlation on some things to see how things are in the record are correlated with each other. other and so as I go as I go along something that happens to the right means the lab test follows the concept and obviously positive negative correlation so this is saying what we see this was good to see this curve means that people who are diagnosed with hypokalemia it's not sure oh here it is hypohypernia Is hypernatremia, which means high sodium, had a sodium that was high before that diagnosis. And people who were diagnosed as hyponatremia had a low sodium before that. So that means like the record was kind of working. But then as I come over here, we see black, a person had high glucose before they developed pancreatitis. Of course, pancreatitis causes high glucose, right? The pancreas dysfunctions. The pancreas dysfunctions. So, what's supposed to be the temporal is switched. I'm supposed to get the pancreatitis, then have the high glucose, not have the high glucose, which causes pancreatitis, which might also be possible, but that's not what I'm seeing here. And the problem is that pancreatitis is not a disease, it's a diagnosis. It means someone wrote it down in the record. So, you have to remember when you're doing these things, the timing of it is that when it was written in the record is what you're studying, not when it happened to the patient. When it happened to the patient. So the pancreatitis happened to the patient in the past, then they got the high glucose, and they came in and got the diagnosis of pancreatitis. So that's why we say that there's this healthcare process goes on, which is just in this black box over here. And we need to learn that and then use that both for when, remember yesterday we talked about phenotyping to inform how we phenotype and then also to inform how we analyze the data. So it's possible, but we have to do it. You know, we did a study. You know, we did a study, Noami and I. Well, skip that diagram, but here, this is time. This is now, and this is sometime in the past. So the doctor was saying something happened two months ago, two weeks ago, three days ago. I normalized it all so that whenever they were saying a go is right here. And then this is the spread of when the, and then we picked a couple things where we knew the right answer. So we saw what they said, and then we actually knew the answer for some reason. Then we actually knew the answer for some reason because it was measured somehow. And so, this is their spread. When they say two weeks ago, you know, there's a spread of when it really happened, or three days ago, it's actually fairly, and you see the different things are day, week, month, and year. You can actually come up with an equation, and it's interesting, and it kind of makes sense. If they say 14 days ago, that's the same as saying two weeks. If they say 13 days ago, that's actually much more accurate. Days ago, that's actually much more accurate. So if they say 14, don't trust them. But if they say 13, trust them because they probably said 13 for a reason. But if they said 14, they were just multiplying 7 by 2, and it really was a 2, not a 14. And a bunch of other relationships like that in there when you're interpreting it. Very much. It's 55, it's not 50. That's okay. I'm actually almost done anyway. I can't add, is my problem. Just to point out a little. Just to point out a little bit. Just okay about uncertainty. So he kind of said 14 instead of 13, is that take two measurements of glucose. Oh, so this one's interesting. Take two measurements of glucose. And then how well does the first one predict the second one? Okay? And one thing that makes sense, and this is time here, this is time here. Sorry. This is a different talent. Time, this is a different tau than we were talking about before. This is time between them, an hour or a year. As they're closer together, they're more similar to each other. That makes sense. But the other thing, tau here is the number of measurements between them. It's actually not between, but between them plus one. Never mind. So more or less the number of measurements between them. And so even if they're close in time, if you did 100 measurements between the two glucoses, there's not a lot to do with each other. And why would that be? Well, because, in fact, I'll show you here. Here. If you're measuring someone's glucose to 100 times in an hour, there's probably something going on with the glucose, you wouldn't be measuring it. So, actually, what we're finding is that doctors are in effect trying to correct for the variability based on the illness of the patient. So, look, this is just, this is not a real measurement, this is just conceptually speaking. So, we have the person being kind of healthy, they get sick, and then they therefore more. And then they therefore more variable, and then they become less sick again. And so, what the doctor does is they start, a doctor, nurse, other healthcare provider does is start measuring more often when they know they're variable and they start slowing down. So that, in effect, what ends up happening is if you just take the time, instead of looking at it as, well, I'm going to say that in the next slide, this just, this is an actual measurement now. If we say what's the change in creatinine, so that was glucose, this is creatinine, between two measurements from like an hour to a year, further on they're more different from each other. But if I say it has to be consecutive measurements, there's nothing in between, then suddenly between an hour and a year, there's no difference in the variability between those two measurements. So we end up saying, well, maybe we should reparameterize time and just say the unit of time is measured. Unit of time is measurement to measurement. So it's an index time instead of clock time. And then, and so that's just kind of shown there. I think it's easy to say it in words and see it on the diagram. And when we do that with a Gaussian process model predicting glucose, this is the one where we treat it as time, and this is obviously a hospitalization, hospitalization. You know, building this model is not very useful to predict the glucose. But if we use index time here, you can see we're actually predicting something useful on the glucose because it spreads it out in a way that, you know, we're using. It out in a way that you know we're using the doctor as a normalizing machine, and I think this is my last example. Oh, this is a phone. So this is using Granger causality just as a way of taking effects. I'll go right here. This one says: if I do a naive study of statins, they, so statin, you know, cholesterol lowering agents, they're great for muscle. They're great for muscles. They make your muscles healthy. And that's what we know to be true: is that actually statins can cause rhabdomyolysis, can cause muscle damage. So it's the opposite. People take statins, get like muscle aches and pains. So this is like a cross-sectional study of the thing. And if I go back naively, and what's happening is they come into the emergency room, and I'm learning when they come to the emergency room, they're on a statin. I started on the statin three months ago, but I didn't notice that. I noticed it when they Three months ago, but I didn't notice that. I noticed it when they came into the emergency room. And they're coming to the emergency room because they, you know, broke their finger. Well, if they broke their finger, they have muscle damage near their finger. If they get a CPK, it's high. Over the next couple days, the CPK goes down. So what just happened? They came in, they started on a statin, as far as I know, which really started three months ago, and their CBK came down. So starting that statin seems to have helped their muscles. So I'm getting this false result from the EHR. But if I spread it out with granger causality, what I see is, sure, there's some effect like in the immune system. Sure, there's some effect like in the immediate period, but as I look back, the statins are actually causing high CPK instead of lowering the CPK. So by dividing it out with greater causality. That's me. Five minutes over. I've seen him give like a 10-minute talk with like 60 slides. It's really something. Okay, so I'm going to be telling you more barriers and the way I want you to think about how I'm selecting them is that I'm going to talk about not so much the modeling and machine learning barrier because others are going to talk about them, not so much the implementation because Tell and George are going to talk about it, but what's in the And George are going to talk about it, but what's in the middle? So, what we see when we start validating things and what we should worry about. So, the first one is transferability. And, you know, you could think of it as generalizability, really, but if you want to think of it in the ecosystem of healthcare, transferability is, I think, a nice way to think about it. Because the issue is that if you learn a model on either one population or a particular institution, is it going to be? Is it going to be as good on a different population or a different institution of people? And there could be some reasons to why it wouldn't generalize. One is that there might be unobserved factors, so differences in populations might not be readily available, but we know that genomics are going to make a difference in different health processes, and then social determinants of health are really a big, big portion of Are really a big, big portion of why care practices, sorry, not care practices, health status of different people is going to look very different from one institution to the next. And then the other reason is kind of thinking about the healthcare process that George just mentioned, different care practices happen in different institutions. The other type of transferability issue is that there might be some actual problems of transferring the model even within the same population and central institution. Within the same population and same institution, just because what you learned at one time is not going to translate to what you learn a second time. So, this is an example from looking at just an ROC curve, and this is something that was trained prior to 2013, and then it's doing pretty well. And then you use the same model, but on data that's collected post-2013, and things start to integrate. To integrate. This is another example of what could go wrong: is that there's this is how many months from 2055. 2005 to 2014. And the type of labs that are measured change. So at Colombia, for example, we know that 2010 is a big year for us because we had a new Year for us because we had a new EHR coming and we had a lot of new types of labs measured. And then 2020 we had EPIC installed, and so things are going to change again in our models. So these things have a strong impact. The other barrier is, and it's kind of going towards implementation, but I'm not even talking about the trust of clinicians into a model at the point of care, but just how much do we trust a model when we develop an interaction? When we develop an iterative origin model, you know, do we know, do we actually understand well when the model is certain or uncertain about their prediction? Is it making, you know, when we do error analysis, what type of errors is it making? Do they make sense for humans or not? And are we comfortable? What is the right metric and what's the right threshold for us to say like, okay, this is something where we feel like it's doing pretty well. We feel like it's doing pretty well on whatever validation is set. The last thing that I'm, no, it's not the last thing, the prior to last thing, is privacy. You know, it has some consequences. One obvious consequence is that we cannot share data right now from one institution to another, so centralization of data is difficult. There are some changes to this. Are some changes to this? COVID has changed things quite a lot, where the NIH and NCATS in particular kind of started a large initiative where different institutions were allowed to put in COVID data and there was now a centralized repository of COVID health records from the entire country and it might expand to other use cases now that we've seen that it's actually possible to do it. Possible to do it. But even within an institution as a modeler, it's not obvious that you might have access to identified data versus de-identified data. And so, you know, we have a set of rules. Privacy researchers are doing a lot of work and showing that you can de-identify some data, even though we de-identify it according to HIPAA. And so there's all sorts of very interesting. There's all sorts of very interesting work going on in ensuring privacy of models, but I wouldn't say that it's completely one-of-the-mill and it's used right now. The last thing I want to talk about is ethics. Esteban talked a little bit about some of these questions. So this is one version of what people are worrying a lot about. So the idea that there are biases in the data, in electronic health record data. Electronic house record data. And if we learn a model on this data, the model is going to learn these biases. And we're going to be using the model. And so now we have a vicious circle where we're going to reuse some of these biases and keep reinforcing them. And then, you know, if there is an actual learning health system in place, now we're in bad shape. That's one that a lot of machine learning people in healthcare have been using. People in healthcare have been looking at, and you know, there's definitely a lot of work going on in here, and it's something we should worry about. There are other problems, so this is kind of like a diagram or a pipeline, whichever way you want to think about what kind of modeling, you know, what are the steps to modeling. One is, you know, design the question. What question are we asking? Who are the stakeholders who are included in the question? Included in the question? Are there some, you know, what are, or how do we operationalize the question into an experiment? The data is what data is captured and what phenomena and how are they measured, are we going to include. The algorithm is, you know, are we going to optimize, which algorithm are we going to use, etc. And then critique would be, you know, we want to evaluate each of these steps in our pipeline. And it turns out that at each one of And it turns out that at each one of those, there are opportunities for some ethics issues to pop up. And so it's been interesting to see now bioethics researchers and ethics and justice researchers looking at very specific data science problem in healthcare and thinking about how their own framework are applying and how that that would combine with this kind of pipeline approach. With this kind of pipeline approach. And so here is just a, you know, I'm the least qualified person to talk about ethics, but this is what my friends in ethics are saying: that there, you know, those are the traditional bioethics principles, beneficience, non-beneficience, autonomy, and justice. But there are kind of new principles that we need to come up with because we're dealing with AI in healthcare. And one of them is explicability. There's some controversy about it, but Controversy about it, but it is there. I would argue that there, and I'm not the only one, that there are other things that we need to think about. So this one in particular is a big deal, which is that now when you use, when you think of AI in healthcare, there's a modeling part and then there's the application part, right? Like hopefully you deploy your model and it's used. And so let's take an example. Now we're trying to Now we are trying to predict suicide in teenagers for transgender. And so it starts from a good idea. We know that transgender kids are at risk of suicide. We want to develop something that in the emergency department is going to identify these kids so that we can intervene better. So what's the issue of consent here? The first issue of consent is that when we're going to build this model, we're going to have a waiver of consent. Have a waiver of consent, first of all, because we have too many people to go look at. And we're going to be looking at who is labeled as transgender in the data. So there's some group of people whose data is being used, they haven't really consented to it, and the model is built about it. But now there's a new kid that comes in, and the model has learned some proxies for being transgender, maybe. And so now there's an individual question of: does a kid even want to know and be Does a kid even want to know and be labeled as transgender? There's some individual question. So there's this balance and tension between the group of people who didn't consent in the first place. If stakeholders have not been talked to, maybe some communities would say, we really don't want this model to be created for our community. We understand the value of it, but the risks are too high. And then the individual issue is. And the individual issue is could you for example train the model, set it up, and then effectively remove the transgender piece and say these are the people who were worried about having suicide. Right, and so I think that's that's exactly right. Like we need to think about the question. Like there's it's a different question. No, we need to change the question. Exactly. But some, it depends. This is an example. Yeah, yeah. So the question is how. So the question is how do you in this case, if you go back to this to here, I think the issue is one of design, right? And it's who do you talk to to figure out if it's a good idea and how do you decide what the scope of your research question is. Yeah, I actually, so I do do some transgender research and I think it's very complicated because you actually do care the person's transgender. Because if you're intervening for somebody who's suicidal, and you so from a clinical standpoint, right, you want to have specific things in place if you're going to intervene for transactions. You want to have, for example, affirming, you don't, you want to have an affirming environment, or you might end up doing more harm with it. So it actually does, I think, from an intervention standpoint. I think those from an intervention standpoint it does matter. But it could, Kenrique, I totally agree with you, which is why we care about this particular question. It's very complicated. But you could imagine that we try to build a model that just recognizes if someone is suicidal. And just one of the reasons might be that they're transgender and having issues going through their process. And then the intervention being completely different and left up to the clinician and deciding with the patient what is the right intervention. What is the right intervention? I mean, I think so, but I agree with you. I think sometimes you can just ignore that part that is difficult sometimes. I guess the point I'm making is I think it actually is, so if you take transgender out of the situation, say race, I think it is important, I actually think it's important to understand factors of the patient in the intervention to make the intervention more effective. Because one of the things we've done, clinical intervention. Because one of the things we've done in clinical interventions for years is not think about that, and it's made interventions less effective. So I'm just saying, like, to me, it's part of the ethics and design to think about this. I don't have simple answers, but to think about it more deeply. Because, for example, there's emerging literature that shows that when there's concordance of clinicians and race, the outcomes of those patients are much better. But that's not something that people have historically thought about in the Thought around in the research design. It's very complicated, but I think to Naomi's point, going back to the design is something that people should some of the other issues, so the data are biased and so there's some mitigation strategies here, but they're very difficult. Here, but they're very difficult. It's kind of like asking for health disparities to disappear, and it's not going to happen probably anytime soon. But maybe there are things we can do into making sure that diverse types of patients are represented. So one interesting case right now is in dermatology, not with EHR data that much, but with pictures of skin cancers, and in all the textbooks. And in all the textbooks everywhere, everyone has a very, very white skin. It's very easy to recognize skin cancer. And so, no, all the models are trained on white skin, which is a big issue. And here, all we need is to literally collect examples of black skins, for example, or colored skins. So, there is work on this. This is a big deal. We know that there are going to be differences in the type of data for different groups of people. Type of data for different groups of people, but we also know that there's going to be differences in care. So, Kenrick, you just mentioned like gender components, race components are things that we're starting to know as being factors in changes in care. In gender, for example, where I do a lot of work, we know that, for example, based on some data from Florida, 100,000 admissions for heart attacks in the ED. If a woman was treated If a woman was treated by a woman, she had less chances of dying than if she was treated by a man. And in between these chances of dying, if there was a man treating her, but there was a woman in the team of caregivers that were there. So there's clearly an influence of the care team and how it relates to the patient that is interesting. There's a ton of work on how to measure fairness and/or bias. Fairness and/or bias. Some of this work comes from justice outside of healthcare. There's some interesting work in trying to see if we can actually transfer some of these ideas from justice to general social justice to healthcare, like equal opportunity. What does that exactly mean? In the case of healthcare, Lin Yin, a Georgia student, works on causality. Works on causality-inspired fairness metrics. There's a lot of work here. The point that I would like you to stick to is that each metric measures something completely different from the other. And so what you find is that there's no such thing as a fair algorithm, but there's such thing as an algorithm fair according to one metric versus another. And in fact, these things are very much in opposition to one another. With that, I will stop. Thank you. For example, you were talking about identifying mitigating differences between documentation. There are also issues like what Ark was showing where there are physiologic differences and those also need to be addressed. And those are, in the EH are often mixed, because it's not always obvious to know whether this is because of your race or your genomics. Or your genomics or your cares that you see. Exactly. So they're all mixed and it's very complicated. But in any event, when there's a difference, we can try to mitigate it and make it better. And I mean, what Art showed is like with the differences in diabetes is well documented, kind of all over the place. But I don't know how much people take that into account when they. There's another bias, which is that a lot of the data that is trained for models comes from academic centers. And it's yet another, it's a beautiful place compared to non-academic centers for healthcare. And so, you know, it's something that is not really talked about in the literature, but I think it's definitely helpful. Um, I was just wondering, do they um also give uh unconscious biases? To give unconscious biases, like classes to some of the physicians and if you kind of recognize it all, is there an effect that you can see in the data? Or I mean, for example, the high tech that's like well known that both female patients represent differently. I don't think there's like unconscious bias training, Henrik. I'm looking at you tell me if that's correct. So there's so there is training, right? But it's training that so there is training, but I can tell you from a like a nursing standpoint, it's a module online that nurses typically do while they're working. So as they're seeing patients, they'll click through the module and run off to take care of the patient back. So I don't think there's a lot of actual uptake. You know, they're not getting the real They're not getting the real training. And I don't know on the position side, I think if you're a faculty member, there's, you know, on the university side that's required training. If you're a president, you're not technically a faculty employer, they probably have training. But I think that everybody is just kind of breathing. Yeah. There's definitely I think there's a push for looking at biases in within, you know, clinician to clinician. Know clinician-to-clinician relations, and there are actually a whole lot of issues there as well, so it's not a bad thing. But with respect to care and patient care, I don't think, I think right now when I talk to, I've been doing a few interviews, so it's a small number of people, but when I talk to clinicians, it's more than hearing from, you know, like the heart attack example is now famous. And so it's like, oh yeah, I heard about this. Like it's like external to the system how they get to this right now. How they get to this right now. But for example, the beta and Florida, that is old beta? No, it wasn't that old, it was in the 2010s. So it shouldn't have anything to do with the heart attack itself? No, I mean, so in the example of heart attack, the difference in symptoms have been documented forever. So it doesn't make sense that it happens. It is explained by the fact that it's still taught in medicine as being the male model. Being the male model of heart attack versus the female presentation. But even if it's out in the general population knowledge, it's still not completely cooled down into practice for sure. Excellent. And you guys are, I think, amazingly early. He's nearly early. I think we're start back at 10:30. We have 40 minutes for coffee. That's pretty good. Alright, very good. We'll be back. We'll be back in a little bit.