So thanks for inviting me. So this is a joint work between Aviad, who are both at Stampling. Alright, so the general goal of sampling as a computer scientist is you're given some sort of a distribution, either discrete or continuous, and you want to efficiently sample from this. The efficiency part is the main subject of this talk. What does efficiency mean? So before I get to that, how is the distribution actually given to you? Actually, given to you. So, this is a computational problem. There is an input, there is an output. The output is always like a sample from the distribution, but how is the input described? So, there are generally like two camps of thought here. One is you work with explicit distributions. Maybe somebody gives you a formula for the Hamiltonian of your distribution, or maybe you are given some combinatorial description of something, maybe random matchings, random spanning trees, and so on. But there is another camp, which is you have an arbitrary distribution, but you have some sort of oracle. Distribution, but you have some sort of oracle access to it. So that's the mode of operation during this talk. So this oracle can tell you some information about this distribution. You can ask it questions and it will tell you those answers. One very simple example of an oracle is maybe on a space, you can query a point X, and the Oracle will tell you. Right, so in this setup, the In this setup, classically, the task of sampling is: you can model it this way: there's an algorithm and there's an oracle, and you go in rounds asking one question at a time from the oracle, and this continues for a number of rounds, and then at the end, the algorithm is supposed to output either an exact or maybe sometimes fine-mid approximate sample from it. So, the primary goal in this setup is you want to minimize the total number of queries you ask the Oracle. Secondary goal, Secondary goal, which usually comes with not much effort, is also to minimize the internal computational time. But if you don't want to care about computation, this is like a very neat mathematical setup still to minimize number of things. So I want to give you another example of a different oracle. So this is this is the main uh this is the oracle that our main results use. So uh this is what I call the hardest margin of oracle. This is what I call the hardest margin of workflow. So this is defined on, let's say, a distribution defined on a product space. The distribution itself is complicated, but it's on a product space. So the queries are, you give some subset of the coordinates, and then ask what is the conditional marginal of another coordinate conditional on the subset that we provide. The answer would be just like a vector of like Q probabilities. So, this oracle is somewhat also motivated by practical applications. So, for example, other regressive models in machine learning, for example, large language models and so on, they train a neural network to mimic this oracle. So, for example, I don't know, at LLM, you can ask, you know, it's kind of maybe the distribution of English phrases and sentences. So, you can ask it, like, if X1 is into the front. Like, if x1 is into the frontiers, x2 is into the of, what is the distribution of the third mode? And I asked an LLM this, and statistical wasn't part of the answer. But yeah, so maybe there is some clinical statistical answer. So given this oracle, you can sample from your distribution with no assumption. There's a very trivial algorithm. You just order the variables in some way and then go over them one by one. And then each One by one, and then each time you ask what is the marginal of the current variable condition in the previous ones, and then you just handle the distribution. Okay, so besides this oracle, there are a number of other oracles you can construct. I like to divide them into like local ones and global ones. So, local ones tell you information about maybe one point or a neighborhood of a point or something like that, whereas global ones do some sort of like high-level. Global ones do some sort of like high-level integration or summation over the entire distribution. So, the one that I just showed you was this one. So, on a product space, in the distributed case, you give some subset of the coordinates, and the Oracle will tell you what is the marginal of another coordinate. The local version of this is you specify almost all the coordinates, maybe all except for one, and then ask what is the marginal of their unit function. You can also do the same thing in continuous spaces. So, for continuous distributions, the one that's used at least by machine learning people is this one. So, this is the Oracle that they train to run these diffusion models that generate continuous data looking at routes. So, here the Oracle is given a noisy sample from your distribution. Your distribution. So you promise that you tell the Oracle, oh, I sampled X from my distribution, and then I added Gaussian noise. Now, having observed this noisy sample, tell me some information about the posterior. So maybe like the mean of the poster. The local version of this, in some sense, becomes this score oracle. So when you let t go to zero, essentially you get this oracle, where basically This oracle, where basically you query a point, and the oracle will tell you some low-order information about like the density at that point. Maybe the standard point is like the gradient of the log density or what's called the score. But you could imagine similar things, like maybe like you can even go up to some low-order data. So the left ones, the local oracles, are what you typically need to run something. Are what you typically need to run something like a Markov chain. Like Global Dynamics and Langevin Dynamics exactly need these two local codes. And the advantage that they have is they are very easy to construct given explicit distributions, usually. The con is you need some sort of assumption on your distribution in order to be able to sample using these elements, right? Of course, these Markov chains don't mix rapidly on all distributions, so you need some sort of isoperometric inequality or Isocherometric inequality, or what have the ones on the right, on the other hand, they are often hard to construct from explicit representations. You can do it in certain cases, but much more rare than the words on the left. On the other hand, if you can construct them, then you can sample using these oracles. Maybe at least the discrete one, it should be obvious, but same almost the same story holds in the continuous case. The same story holds in the continuous case. We can sample using those oracles with almost no assumption on the distribution. Also, as a practical motivation, the ones on the right are things that machine learning practitioners love because they can work with arbitrary distributions. So, besides these practical machine learning applications of the global articles, there are some theoretical instances. There are some theoretical instances in which we can construct these oracles. So these are some rare gems of deterministic counting. So there are a bunch of distributions coming from cognitorics and so on for which you can do this exact conditional marginal oracle. So maybe the most famous one is spanning trees. So we all know that given a graph, you can count its spanning tree using a determination. You can count its spanning tree using a determinantal formula. So that gives you an efficient algorithm to compute the number of spanning trees. And you can easily translate that into computing marginals and even conditional marginals and so on. So I've listed a bunch, but maybe for this talk, just focus on the first and the third. So another instance is also the case of planar perfect matchings. So this is due to the famous FKT algorithm that tells you how to give an That tells you how to, given a planar graph, you can count the number of perfect matchings in it in terms of some Fafian and some again determinant formula of the same sort. All right, so in all of these cases, because we can count the associated objects, we have this financial marginal oracle, and then you can sample these that you have. But what is wrong with this trivial algorithm is that this is high. This is highly sequential. So, as an example, if you want to sample a spanning tree, what you do is you go over the edges in some R order, and then for each edge, you compute the probability that that edge is included in a spanning tree condition on the previous choices you've made. And then flip a point with that probability and decide whether to include this or not. But notice that at each stage, you need to know all of the previous decisions that you've made. So, this is very sequential. This is very sequential. There is no clear way to balance it. But all of the problems I showed you reduce to some determinant computation, and it's been known for a long time that determinant computation itself is highly parallelized. So if I give you an n by n determinant matrix, you can compute its determinant in just polylogarithic time, given enough complication. So that's the model of parallelization that people call this. Call bits. So, yeah, so if you are given, let's say, n to the hundredth processors, you can always compute a determinant and just fly by time. And so, yeah, so the question is, can we actually use these parallelizable counting algorithms to sample from these distributions? One thing I want to mention is the reason I mentioned continuous distributions, all the distributions I showed you on the previous slide are. Distributions I showed you on the previous slide are discrete. But somehow, still, I need to tell you about the continuous case. Because for these distributions, you can also identify them as distributions on Rn by just identifying these discrete sets representing spanning trees as a discrete subset of Rn. So, for example, if your graph has n edges, you can represent it as a distribution of plus 1, minus 1 to the n. Of plus one minus one to the n, where one of them indicates membership in this management, one of them indicates non-membership. And if you do that, not only do we have the conditional marginal oracle, but we have this like stronger oracle too, the denoising diffusion, where you give it like a Gaussian noised sample and ask for the positive power. I don't want to go too much into details of this now. I'll come back to it later. But sort of like the reason we have this other oracle. This other oracle, this spontaneous oracle, somehow reduces to the fact that in these cases that I show you, not only can we count spanning trees, but we can also count data spanning trees, or we can count data planet level images. Okay. One more slight motivation is besides these determinantal counting algorithms, most other deterministic counting algorithms fall into like maybe like these two paradigms of like Weitz's algorithm and Barbara's. Of Weitel's algorithm and Barbinox method. And these can also be easily parallelized. So this is for the experts who know what these are. Weitels method reduces computations to a tree, and we cut that tree at depth log n. So then we do some recursive computation on the tree, and because its depth is log n, it's just log time. You just assign processors to do one level at a time, and similar. Similarly, in Barwinux method, you write some sort of a Taylor expansion and you typically cut that at the log nth there. And that somehow corresponds to counting log n-sized objects in maybe a graph or something like that, which again, because the size of those objects is log n, it's easily parallelized. So, similar to how I motivated this Oracle setting in the classical sense, you can also You can also formulate the parallel sampling question in this oracle model. So now there is again an algorithm and an oracle, one of those two oracles, maybe the orbal ones. And the algorithm can now ask polymitting many questions at a time from the oracle, so some reasonably bounded number. You don't want to allow it to ask all of the questions there is at the same time. And then the Oracle will answer all of them. And then the Oracle will answer all of them at the same time. And then again, this goes on for a number of rounds. And the goal here is to minimize the number of rounds. That's like the first order goal. And the second order goal, again, comes usually without much effort, is to parallelize the internal information. So alright, so this is the punchline. This is the main result I'm going to show you later. Is the trivial algorithm I showed you is actually not optimal. You is actually not optimal. Actually, when I started collaborating with my co-authors, the first question I asked Aviat was: How do you show this trivial algorithm is optimal? Turns out it's not. So we can, for an arbitrary distribution, we can sample from it using the conditional marginal oracle in n to the two-thirds rules. Also, as a bonus, you can still keep the total number of queries all of n. The total number of queries all of n, so it's not much more like effort than the trivial out. Alright, so then we asked the question of: you know, can you go all the way down? So, in all of these, like, parallel sampling, in the parallel algorithms, usually like the gold standard is polyglog barn prime. So, we ask the question of can you go all the way down to polylog? Turns out it's not possible. So, there are more space distributions for which you need at least n to the one-thirds. Need at least n to the one-thirds. There is still a gap, but we at least know there is a polynomial speed up and not one. All right, so this is the roadmap for the rest of the talk. What I want to tell you is I want to first tell you some history of parallel sampling in these different Oracle models. In particular, I'll tell you about a result that I have with June and others that covers some of the That covers some of the examples that I showed you. But one of those examples remains still a mystery. And in the second part, I'll tell you how to prove the result I just showed you and what implications this has. Alright, so the same table as before, what's known for parallel sampling using these oracles. So for the local oracles, we know that if you want to, let's say, I don't know, If you want to, let's say, I don't know, run something like GlobberDynamics and so on, you can actually parallelize the Globber Dynamics Markov chain in certain settings. So there are these two works of Feng-Hayes Yin and Li Yuan-Yin, which tell you that if you have a fast mixing, let's say, global dynamics, and you also have some condition that looks like the Brushing, but it's actually a little bit weaker. But it's actually a little bit weaker. Then you can actually simulate that Markov chain in Polygon. All right. So in the continuous case, people usually run something like Launch Rand Dynamics or something like that here. Even there, under conditions that are traditionally basically necessary for efficiency of large band dynamics, we still know under those. Still know, like, under those same conditions, basically, we know that you can still get polylog inside. So, this is like things like log concavity of the distribution or knowing some sort of like isoparimetric inequality like log soul limb for your distribution. Under those settings, we still know phi lines. So, unfortunately, you need some sort of an assumption in both of these settings because any parallel algorithm. Because any parallel algorithm, you can also run it sequentially. So, in order to be able to efficiently sample in the sequential setting, you need these conditions. So, naturally, we also need them in the parallel count. But there are some open questions here. So, in the case of discrete distributions, there are many distributions for which we have efficient Markov chains, but we don't satisfy this sort of condition. For example, basis of matroids is uniform. Uniformly random basis of the matroid is from Cheyenne's stock, is an example where we don't still know how to sample it. Okay, so what's known for global oracles? So I showed you the results for the discrete setting, but let me tell you also about the continuous setting. So in the continuous setting, there is a parallelization using some technique called Picard iterations, which I'll go to. And so in practice, it seems to actually So, in practice, it seems to actually speed things up. So, this is like a picture of a diffusion model generating an image. On the top, you can see a parallel run, and on the bottom, you see a sequential run, and the parallel run is running much faster. There is a conjecture that for arbitrary and dimensional distributions, the parallel version should run in roughly the order of root n time, whereas the best sequential answer we know is like order. Sequential answer we know is like whatever A. Okay, so I'll get to those. But so we don't know much. What I want to emphasize is we don't know much for arbitrary distributions in a continuous setting. But I'm also omitting some really interesting work for explicit distributions. These are all the settings of work hold models, but for explicit distributions, there is, for example, work of Holden. There is, for example, workoff hold and that treats. Oh, I was just wondering, could you maybe give some intuition about what is happening in parallelized workout chains? Like, what are they doing? I'll get to that. There's a slide at the end where I tell you the technology techniques for instance. Okay, yeah, so I'm doing some work on explicit distributions. But also, for the setting of global oracles, for continuous distributions, the noise and confusion, basically we. Noise and confusion. Basically, we don't know much for general distributions yet. There is a conjecture, but we can't be on that. If you put extra assumptions on your distribution, though, we do know something. So, this is a result that I had with June and some partners. So, if you have a distribution whose covariance is bounded, and not only that, but its covariance under basically these posteriors, like the posteriors. Posteriors, like the posteriors you get from telling the Oracle a Gaussian noise sample. So if the distribution itself and all of those posteriors have a covariance that's bounded in the set, it's axiomy male is bounded, then we know how to get poly logarithms. We do have this for matrix. The trouble is for The trouble is for arbitrary matrix, you don't have the oral. So, as Cheyenne said, so Cheyenne told you about this high-dimensional expander framework and so on. So, all of the distributions that fall under Cheyenne's stock basically satisfy having the four-year instance. In fact, that's why we know out of the four examples I showed you, three satisfy this condition. This condition. So spanning trees, for example, satisfy this condition. Alright, so how is having a covariance that's bounded helping us? So it turns out here you actually have to go to the continuous board, even though these distributions are discrete. So there are few ways to transform a discrete distribution into a continuous one. This is maybe one of the first ones that you think of. The first ones that you think of. So you start with like a discrete distribution on, let's say, plus one, minus one to the n. Maybe this is like the distribution of like span keys represented here. And then what you do is you just take convolutions with a question of some variance C. So on the right, I've shown you this convolution for various values of C. So it turns out, if you have a covariance bound, Covariance bound, if C goes above a constant threshold, the distribution that you get at the end, like this third one here, becomes super nice. It becomes log concave suddenly, all of a sudden. So yeah, so this is basically the main observation in this work, which is if you have these uniform covariance bounds, as long as you take convolution with a Gaussian that's like whose variance is a Whose variance is slightly above a certain threshold, you get log on k distributions. And this is not hard to show, you can just write down like an explicit formula, but I'm probably going to assume this work. So you can explicitly write down the representation of this convolution at a point. And it turns out it's some formula in terms of the Laplace transform of your distribution. And remember, I told you Distribution and remember, I told you like this has something to do with like weighted counting, and Laplace transform is some sort of weighted counting. The manner that you can your oracle can compute on plus minus one to be either opposed to zero. No, zero one to numbers. Right, and then you can explicitly write down like for for this for this distribution, like what For this distribution, like what is the Hessian of the log density at the point? There are two terms coming. One term is coming from the Gaussian, basically. So this is the same, like minus I over C is what you get for a Gaussian itself. But then there is another term which is positive, and that's coming from the covariance of your distribution. As long as the covariance is bounded, basically it's enough to make C be above a certain constant for the negative term here to bound. Like the negative term here to bring over the positive. So, yeah, so this is beginning to look promising. So, when you have a distribution that's not concave, one of the results I showed you was that we can sample from it in parallel, in polyglot in time. So, we can sample from this distribution u in parallel. But then it's still like there is a question of how do you translate samples from this distribution. How do you translate samples from this distribution to samples from your original discrete distribution? Fortunately, there is by now a well-known technique for it. So, this is the question, how do you denoise these samples? And so the process is somewhat well understood by now. So you do something called denoising diffusion. That's like the name machine learning people like to use. Or you do stochastic lock localization, which is the same thing but in the theory quantum. Same thing, but in the theory commonly. And this was actually used algorithmically by Ladai, Monteneri, and Mark. We are using a slightly different discretization of this process, but I just want to show you pictures, maybe how it works. So you start from your original distribution, then you make it noisy. So you come all with So, you convolve with a Gaussian distribution to get like this continuous Laplander distribution. Then you take a sample from this. So, you use your new sample to weigh the original distribution. So, you bias your distribution, you put an external field, or if you're working with spanitrees, you put weights that are coming from the sample you get. Maybe now, according to the weight of the distribution, your distribution is now biased this way. This way. And then you again convolve this with a Gaussian distribution. So you get a nice distribution. You again take a sample and then you add whatever sample you got to basically the base. You multiply it, by the way. So if you do this process many, many times, you always converge to like a point mass. And that point mass is basically the distribution that you want to operate. Cells aren't that important. Aren't that important because I won't focus on the other results. Alright, so it turns out that this process that I told you, it's enough to do it for just log n iterations. So we need to just produce log n samples from like very nice distributions, very nice log concrete distributions. After that, this distribution is like well enough concentrated that you can actually just round it to like the single point and that would be like a good enough approximate time. Good enough about it. Alright, so that's all I had for my distributions. Any questions? So let me go back to the other question, right? So let me go back to the setting of the main result of the start. Okay, so out of the four motivating examples I showed you. Four motivating examples I showed you, one of them wasn't covered by this result, and that was the planar perfect matching case. So, why isn't it covered? So, turns out covariances are not bounded in this case. So, here is an easy example that shows you why. So, if you start with like a very long cycle, it has exactly two perfect matchings. And if you translate this into plus one, minus one to the end, you get two opposite endpoints of the key. So, the uniform distribution over these. The uniform distribution over these has a very large eigenvalue, eigenvector, like basically in the direction of the vector between these two vectors. Somehow the intuition is in all of those other cases, you get a subset of the hypercube where things are nearby each other, whereas in the case of linear perfect matchings, things can be very far from each other. Okay, so the question is, what can you do actually about like linear perfect matchings? Like planned perfect matches. So, even without any sophisticated tools, you can still do something a priority. So, this is a baseline for leading to beat. So, turns out you can use just the planar separator theorem to get some non-trivial parallel sampler. So, if you invoke the planar separator theorem, it will give you a subset of the vertices of size dropping. Subset of the vertices of size roughly 10, which when removed divide the graph into two chunks that are smaller by constant function. So one thing that I can do is I can go over the vertices in this separator one at a time and then decide for each vertex what is the edge that's going to be participating in the perfect matching that's adjacent to this vertex. And that's an easy computation. You just query all of the edges adjacent to this vertex. Edges adjacent to this vertex, you query like the marginals of those edges, and then you just sample from those marginals. This part is highly signature. So I just do 10 iterations. But the point is, after you do this, now you have two independent subgraphs, and you can sample from each one of them in parallel. That computation ends up not costing much more, so the bottleneck is basically just a sample of quite some. That'll give you a That'll give you an order of root n time. Cool. So, alright, so I told you the main result I want to show you is that for an arbitrary distribution, we can get n to the 2 thirds. And you might ask, oh, n to the 2 thirds is more than 10, right? But turns out that's not a problem. You can actually apply this theorem that I told you to just the separator itself. So to get some matching builds. Boundary builds. So basically, given the planar perfect matching distribution, you can define. So let's say S is the separator. You can define a distribution on E to the S, where E is the set of edges. So this is like a shadow of the planar perfect matching distribution, where I just report for each vertex in S, like what edge participated. This is still like a product space. I do have the oracle, the conditional marginal oracle for. The conditional marginal workflow for this shadow as well. It's just like a restriction of the original workflow. And so if I apply it to this, I get that I can sample from the separator. I can sample the edges adjacent to the separator into the one-third time and the rest of the samples as before. That's the non-trivial implication it has for this distribution. Okay, so next. Okay, so next I want to show you the algorithm, give you some elements of why it's like beating the trivial sort of n-time algorithm. And then I want to show you the lower bound. So what is the algorithm? So recall we want to sample from a distribution on q to the n. So first of all, I'm going to randomly shuffle the coordinates. But for simplicity of notation, I'm going to pretend the random shuffling resulted in one. Going to pretend the random shuffling resulted in one. All right, so I'm going to fix some source of randomness, u1 through u1. So think of this as like any arbitrary, like, I don't know, random, any arbitrary set of random variables that have high enough entropy. So the rest is going to be deterministic. No more randomness is going to produce. This is all, the rest of our task is to just convert this like unstructured randomness into like. Unstructured randomness into like a sample function distribution. Okay, so first I'm going to pretend that my distribution was independent across all the organs. If that was the case, I could easily sample from it in parallel by just computing the marginals and then independently sampling from those marginals. That's what I'm going to do. I'm going to compute all the unconditional marginals for each xi, and then I'm going to sample using the source application. To sample using the source of randomness I put aside for MI from this margin. Of course, not all distributions are independent, so what do I do? I'm going to pretend that my predictions are correct up to some point. I'm going to next compute basically these conditional marginals. The conditional marginal I compute for xi pretends that the previous predictions for the previous coordinates have been For the previous coordinates have been correct. So you compute the conditional margin of xi condition on x1 through xi minus 1 being the values that you sample in the first step. All right. And then I'm going to, so now I have a new set of distributions, PI. And then I'm going to use the same source of randomness to sample from this. So I get maybe some new values here. And the point is, I can basically tell that a sequential sampling algorithm, the trivial sequential sampling algorithm, would have proceeded the same way up to the point where zi's and yi's agree. So what I do is I basically find the first index i where yi's and zi's don't agree with each other, and all of the previous values I treat them as correct and I I treat them as correct and I condition on them, and then now I have like a smaller distribution and I repeat the whole process on that. Questions about this? Good. Okay, so yeah, so there are a couple of remarks that I want to make before I dive into the movements on. So in practice, So in practice, LLM, people who run LLMs for you, they actually like most places where you actually can use LLMs as a service actually do use this technique called speculative decoding. So speculative decoding is an algorithm practitioners use to parallelize the sampling quantum models. But of course, it's not the same algorithm that I showed you. The difference is in speculative The difference is in speculative decoding. Essentially, you run a cheaper oracle, again, sequentially. So LLMs are expensive to run, so you have maybe not as much accurate LLM, but you run that one sequentially to produce these predictions by I, and the rest of it is the same. So you verify those predictions using the more expensive model. The lesson to be learned is to get inspiration from practitioners. So there's one more bit of mystery from the algorithm that I haven't specified yet. So I casually glanced over, you know, we sample using this source of randomness, right? But what algorithm do we actually use to sample from the source of randomness? So ideally, we want this property that if Pi and Pi prime are closish to each other, Are closish to each other, there is a good chance that the samples are exactly equal to each other. So it turns out this has been actually studied before. So it's important what algorithm you use. So you want an algorithm that basically receives the description of a distribution and some sorts of randomness and outputs a sample from that distribution. But some standard ones that you might think of are not actually very good. So maybe this is like a standard sampler. This is like a standard sampler. You pretend that your source of randomness is a uniform number between 0 and 1, and you divide the interval 0 to 1 into chunks based on the probabilities. And whichever interval this lands in is your sample. So this is not so good. You can easily construct distributions, like p and pi here, that are very close to each other. They only differ on like the 1 and q values. But the chance of getting equal samples in these two distributions using the same source of randomness is zero. So what might be surprising if you haven't seen this before is they're much better samplers. So what's the baseline? So if you have two distributions and you're producing like faithful samples from these two distributions, they of course need to be unequal to each other with probability at least the total variation distance between these, right? Variations and stuff in this, right? Turns out we can match this up to a constant factor. There are samplers that guarantee that if you use the same source of randomness on both sides, the samples you get are unequal with probably at most twice their total value instance. So, this is a result that apparently has been rediscovered many, many times. So, as far as I know, the first discovery. The first discovery of this was by Kleinberg and Tardos. But we actually learned this from a paper of Yuan Hing, who probably didn't know about Kleinberg and Tardos. And there are many, many other people who have rediscovered this. So all of these previous universal samples that I know of somehow worked based on some rejection sampling. Turns out, again, like practitioners here are wise. Practitioners here are wise. So, people who use LLMs actually use a specific sampler called the Gumbo Trick. So, if you haven't seen it before, it's basically like functionally equivalent to this one. So, you pretend your source of randomness is like Q exponential random variables. And then you just divide these by Pi's, and you just output the index of the minimum. Index of the minimum. The fact that this is a correct sampler is trivial. This is like basic properties of the exponential random variable. But turns out this also has the same property, that on two distributions, the probability that you output different things is at most twice equal to the nature. So use this. And the lesson is get inspiration from practitioners. Okay, so the formal theorem that we have So, the formal theorem that we have is that the algorithm that I showed you, if you run it with any of these universal samplers, basically you get the number of rounds bounded by n to the two-thirds polygon. All right, so you might be asking, where is the two-thirds coming from? So, I want to show you some elements of the proof, and hopefully, by the end, two-thirds is clear. So, our analysis basically relies on some new variant of the so-called pinning lemons. And when you start working on this problem, it's like maybe after a while, it becomes obvious that pinning lemmas should be somewhat useful for those. Because the intuition behind pinning lemmas, if you haven't seen them before, is if you have a distribution, an arbitrary distribution, if you start conditioning a random subset of variables, A random subset of variables, then in some sense, in some very average sense, the correlations between remaining variables go down. And morally, when correlations are down, things are looking more independent, so maybe you can sample from them in parallel better. Okay, so this is a formal statement that we prove. Yeah, the proof technique is very similar to the traditional technique, but I don't know if this is actually. I don't know if this is actually like a consequence of call it the new vacuum. Okay, so what does it say? It says, suppose that I try to sample a vector x, and I take the expectation over both this sample and the random shuffle error, right? Of this sum over i of what is basically like the total variation distance between the actual law of the Between the actual law of the xi condition of the previous coordinates. So, this is the distribution that sequential sampling would use, compared to the one where I sample maybe like theta less than theta less than. So think of this theta as like the ones that haven't been sampled in this round yet. So, morally, if this total variation is small, then my universal sampler is likely to output the same value, so then I advance over the sum. So then I advance over the side. Okay, so what we show is that if you sum up this, and again, here I should have written like pi of i, where pi is the random shuffling, but for synthesis of notation, I'm not intending random shuffling. So if you sum up this over all coordinates, the total radiation distance squared, what you get is at most theta times lock. This is valid for any theta features. You choose. You can ignore the log queue, most likely. Alright, okay, so maybe I'll sketch, because I probably have time, I'll sketch over the board how this is cool then. So for now, I ignore this line. But so once you have this formula, how do you analyze the algorithm? Basically, the algorithm is each round is going to be either short. round is going to be either short, I advanced list and theta variables, or it's long. So the number of short rounds by this formula in expectation is at most square root of n theta locking. The square root comes in because you have to do coefficient to translate the squares to just like normal total variation distances. So you get that the sum of the total variations is at most this. And the number of long rounds is of course at most n over theta. Blong grounds is, of course, at most n over theta, because in each one of them you have n sub theta. So balancing, choosing a value with theta that balances these two, you get antibiotic. Alright, so let me maybe say something about how improved this actually. So the point is So the point is, here's what you can track. So let's say there is a variable xi, and what I can track is the entropy of xi conditioned, and let's say I don't, let's say pi is like our random fragmentation, so on x pi 1, x pi 2. So, this is a number that's defined for t, and I'm tracking this over time. So, on average, entropy goes down. The more and more you specify things, like this is a, like the distribution itself is a martingal, so it's entropy on average goes down. So, what you can show is that basically this total, using just like simple Pinsker's inequality, you can show that this total variation distance squared is bounded by the expected. Is bounded by the expectation of the entropy at the end. So xi condition that, again, like let me drop the random notation. So yes, maybe this is the entropy of xi condition and then x1 through xi plus 1. Okay. So how much entropy draws? So, how much the entropy drops on average, bounds, upper bounds, like the total variation distance squared. That's just an instance of quantum Q. And now, if you track this entropy, the value it starts at is locking, right, at most. And it can go down to maybe zero at the end. And this happens, so think of this process. This happens over n steps, right, going from log 3 to log 20. Right, going from log 3 to log 20. So, if you look at an interval of length theta, how much drop do you expect there to be in that interval of length theta? You expect like theta over n fraction of the entropy to drop in that. So you get theta over theta over n times log q for each variable, but I'm summing over n variables. So then that cancels this n, and that's how the formula comes. Now, this expectation that this is the drop over. Expectation that this is the drop over a specific interval of data. This, of course, doesn't hold in the worst case, but somehow averaging over permutations, let's average things both. Okay, good. Yeah, so that's all there is to the upper bound and the algorithm. Now I want to show you the lower bound. So is there a better algorithm? I told you that there isn't. So there are OR space distributions that need at least n to the Or space distributions that need at least n to the one-third many rounds. So here is a distribution that satisfies this. So we have n coordinates, and this is over like a binary line, so q is 2. So think of splitting the coordinates 1 through n into blocks of size n to the roughly n to the 2/3. For simplicity, I'm dropping many, many log factors to not mess things up. To not mess things up. To take it with a grain of salt. So there are n to the 1/3 blocks total. So I'm going to define my distribution to be basically independent across these blocks. And then for block I, I choose a uniform distribution over an affine subspace of F2 to the above the dimension. The point is the dimension of these affine subspaces, these affine subspaces are chosen randomly, but their dimension grows with the Their dimension grows with the index of the block. So they start at like very small numbers, like n to the one-third. And each time I go forward, I add like n to the one-third to the dimension. So the last one is almost full dimension. Okay, so why is this distribution hard for any algorithm? Oh, and even though I partitioned the coordinates into these blocks, like in this picture, like sequence. Like in this picture, like sequentially, I also randomly shuffle this before I give it to the algorithm. So the algorithm has no idea what the partition is. So what's the intuition of why this is hard for any algorithm? So basically the point is, think of an algorithm making a query, right? It's going to pin some number of coordinates and then ask for the marginal sum. Now, the number of coordinates, now, because they're The number of coordinates now, because the algorithm at least at first doesn't know this partitioning, like the number of coordinates that it pins in each of these blocks is roughly the same. It looks like a random subset to each one of these points. The density of pinnings in each of these blocks is the same. And this is a fact you can easily show for random affine subspaces that if you randomly pin a subset that's like much larger than the dimension, larger than the dimension, that's much larger than the dimension, then with high probability you hit nothing. And if it's like much smaller than the dimension of that mine subspace, the number of coordinates that you pay, then with high probability the marginals on the remaining variables is still like one half, one half. Basically, in the first round, the queries that are very high density, their pinnings, High density in their pinnings basically don't hit block one, and so like they're useless. And the queries that have a low enough density to just hit something from block one learn nothing about the future blocks. So blocks two and so on, they all return like the same conditional marginals, like half half. So basically the point is in the first round you can only learn about block one and nothing from the future blocks. And nothing from the future blocks. And then you can repeat this. After the first round, you can even tell the algorithm: oh, this was my first block. This is even like the affine subspace on that first block. Now go do the rest of the thing. And again, by a repeated argument, the second round, you can only learn about the second block, and so on. Yeah, so that's all there is to the lower amount. Alright, so this is the slide where I promised I'll tell you how these Markov chains and things like that work. So there are generally two techniques for building parallel sampling algorithms that are in the map in the literature. I mean these are for the Oracle model, of course, for explicit things like you can make things. So one technique is what I call fixed-point iterations, the other one is called Fixed point iterations, the other one is called predict and verify. Predict and verify is basically like the algorithm that I showed you. You predict some values and then you verify. So fixed point situations, on the other hand, this is like the basis of all of these other results that I showed before, the other workflows. They work basically by taking some sort of a time evolution, maybe it's a trajectory of a Markov chain, or it's like, I don't know, some trajectory described by an OD. Some trajectory described by an OD or SDE or some such thing. I think I've written like SDE for Langevine dynamics. So simulating these things is, it seems on the surface, it's a very sequential task. So you go from one state of the Markov chain to the next, and from that to the next, and so on, or you go from a point in SAL SD to the next point by just adding some drift and so on. This, on the surface, it seems like. This on the surface it seems like very sequential. But there is a technique called Picard iterations which transforms this into like finding a fixed point in some operation that transforms trajectories to trajectories. So what do I mean by that? So let's take this L C E. Now I'm going to define an operator that takes in a full trajectory over time and spits out a new trajectory. So in the ith iteration, I'm going to take the i minus Iteration, I'm going to take the i minus first trajectory. I'm going to evaluate basically my complicated drift function simultaneously in parallel at all of the points of the previous trajectory. And then I'm going to easily sum these up. I'm easily going to integrate these up over time. Again, I can do this in parallel. And this fits out a new trajectory. So you can easily see that basically if the That basically, if the trajectory you get at the end is the same as the trajectory you started with, then that's a solution to this. Similarly, for Markov chains, if the trajectory is the same, then you found the evolution of your Markov chain over this entire times. So the point is we want to, if you, yeah, so if you define this as the Picard operation, you want to find the fixed point of this Picardy operation. And so now what do you do to find fixed points? You just insert. So basically, yeah, so all of these algorithms are based on defining some sort of a Picard operation and iterating it until it converges to the fitness. And in all of those settings, you can show it takes maybe logarithmic iterations or something. Okay, so predict and verify is, so the algorithm I showed you is not based on this at all. This is for all of the other results. So predict and verify seems to be inherently something else. Actually, like when we started working. Actually, when we started working on this, we also tried an algorithm based on fixed-point iterations. We quickly realized it doesn't work. It just gets order of end time. So, one open question is, can you combine the two techniques and somehow get better results? So, let me conclude. So, I showed you how to reduce the task of parallel sampling to the Of parallel sampling to the task of parallel counting with some polynomial speedups over the tree-below with them. So, one open question is: this particular theoretical application is the application of linear perfect matchings. There is no reason to believe that it's not possible to do better on that. So, it's probably like n fine process. Another obvious open question is to close the gap between n to the two-thirds and n to the one-third. There. So, one thing I didn't mention is the analysis I showed you is actually tight. So, there are distributions for which this algorithm does stay 10 to the 2/3 time. So, you have to change the algorithm. And one idea here is to mix in those Picard target iterations somehow. Another open question is: in the continuous setting, with this denoising diffusion oracle, what Usual oracle. What can you do there? I mentioned that there is a conjecture that, in general, you should be able to do order of 10, but that's okay. And it seems like if your distribution is slightly non-adversarial, so I showed you for very super nice distributions like spanning trees and so on, you can get polylog at time. But even for distributions that are worse than this, maybe like a polynomial mixture of these nice distributions, the conjecture is. Distributions. The conjecture is that we should still be able to do polylog n time using the same white patterns. So that's the end of the time box. For the planar perfect matching case, if you like some growth constraints, if the growth is smaller. Like the Girth is small, or yeah. I mean, are there other problems than the one you showed? I mean, Girth is not gonna resolve that, right? Because you can have like a hidden long cycle, but I mean small cycles. Ah, okay. But then mentions are close. If you think about them, wasn't the problem that the mentions were far away in the middle? Well, I mean, so imagine like n over 2 verdices that participate in a very long cycle. That participate in a very long cycle, and the rest are just in cycles of size 4. So then the girth is small. But still, there are perfect measures that are fine. You got other parallel algorithms that you assume some smoothness of the distribution. So yeah, so I think that's a good question. So maybe the nicest distribution that I know of is like matroids. And you can argue that matroids do have some sort of smoothness. But it's still open to even using this conditional RNGR oracle to sample from matroids in Polygon. With a slightly more advanced Oracle, with the matroid. More advanced oracle, with the weighted counting oracle, we can, but we, but like the conditional module oracle, we still. The exponent? The exponent, yes, you can, you can, yeah, you can like enter the one-half and so on. And that's for retros? That's for, let's say, matrixes. But it's slightly more general for anything that was covered by Cheyenne stuff. I don't know if that's the mathematical. Is that a little bit? I mean, it's my future.  Just like some of the random piercings and it's a good tactics and whatever it is. I think professionally at this point, I guess. I don't care. Right? And this is Second. But I got a four months, so I can't call it. They're all in face of this shape.  Yeah, you know that's like a lot of money.    I'm just trying to find the totally studious code both way in combo. The way they treat lights was a disaster. Still have a link there as well. So, yeah, I'm going to go because it kind of happened during the pandemic, it swept a little on my memories I'm successfully suppressing anyway. It turns out you are a teacher. It's just that the mother tries to assassinate you, and it's quite difficult. You keep having these conversations with our teachers, and I keep having like Vietnam flashbacks, and it's just like, oh yeah, last time I taught, well, I can put the technology. But you have half an hour. You have half an hour until we start. Ah, okay. Yeah, I gotta ask you with the final strategy. Oh, it's uh, Yeah, I think it's a regular thing. Yeah, everyone should not pay shared with you. Yeah, it's well it's fine. I'm going to win to it. There was a person who was in there like they have the same teacher and they have different colours and pretty much yeah, it's it's nice and that occasionally I'll bring them to see how it sees but it's just important. So, like, every side context has also collected every time you go and you get a different color once you've got a color object. Yeah, I've got any more tracks there yet. I was supposed to go to Planet then I'll governments. Online workshops. Yeah, we still need to run this all kind of stuff.  Yeah, it was really nice. I was not sure how we could find that actually. I used both directions, but I did do the bus to the trailer. So some people would like walk through the girlhood. But I part of the tools are not. Yeah. Just those things. No, we made it. Yeah. Yeah, and then there's also like certain parts of the trail for very good reality with the matter. Yeah, yeah.  They have to apply. 