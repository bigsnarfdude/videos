So I'm a statistic. This is the work I find kind of the most close to this workshop. I'm sorry, I didn't know about 3D graphics in my talk. And so basically, I'm very grateful to come to this workshop. And thanks for the organizers for inviting me here. I think the 3D graphics is a very core field, and I really learned a lot. And I really learned a lot in this workshop so far and really enjoyed it. Okay, so now I'm talking about my research work. So basically at first I want to give you a general guy about my research. So my research field in physics we call functional data analysis. So this is actually a popular area in physics. So the idea is that we will treat any data collected over time or special locations. Over time, or spatial locations, or other continuum either functions, curves, or images. So you can imagine that if we have the 3D images, basically it's also a function of three-dimensional space. Okay, so there's all the past 20 years, there's a lot of literature developed how to analyze this type of data. So, one thing we did is we want to specify the neural network specifically for the function data. So, the ideas is that suppose we have an input as a function like XLT. So, typically, what we will do is that we will evaluate this function at discrete time points, and we trade. Screen time points, and we treat each value of the function at each time points as a single variables as an input, and we will do the neural image structure. The problem for this is that when we train these models, all this weight in the edges, actually, they can be very different, right? So, therefore, we lose the interpretations when we do the neural network. The reason is because, you know. The reason is because, you know, suppose that the two functions are the functions at two close by time points, the corresponding weights theoretically should be close as well. So how can we control the smoothness of these weights over time is our interest. So basically what we define is we define the code functional neural network. Basically now we will treat the function x of t as the X of t as a single input, and then we will define this weak functions omega t as a function of t generally in the continuum function. So now instead of the summations, we can look at as the integral of this width function times the input functions. So in functional data, we have a very cool technique to where we asset the functions. So when we ask the functions, we will write down the function as a linear combination of basic functions. So here FL are some basic functions, CL are the coefficients of the basic functions. So basically what we do is that we want to ask in the coefficients C, then we will know the contained function WT here. So we already will using, we call it a cubic B splines in very powerful width function, which selective can approximate any function, any kind. Syria can process any function, any kitting functions. So, if you're not familiar with B spine bit functions, so this is kind of giving the ideas for the b spine bit functions. So, after we get this representation for the function wt, then we can do the smooth controls. So, the idea we do the smooth controls is that in mathematics, we know that the roughness of functions. We know that the roughness of function can be categorized by the second derivative of the functions. So, therefore, we define and we call it the roughness penalty here. So, we want to penalize the second derivative of the weighted function, and in the end, we will get a very smooth estimate for the weight functions. So, this is give you the ideas. So, you know, generally, if we do a random neural network, the Random neural network, the weight at each time points can be very different, right? So, but like now, if we want to have a smooth weight functions, it will become much more smooth over time. So, then we will gain interoperability when we train the models. So, now let me talk about this different topics here. We do the image. Here, we do the image analysis. So, the ideas is that basically we know that breast cancer is the leading cancer among women. And so, now the main breast cancer screening strategy is to do the mammograph. So, our collaborators, they collected the mammograph limit for around 785 winnings, and among them, Women, and among them, 246 diagnosed them with breast cancer. I'm not sure, like, maybe the mammogram graph image can also be 3D, but now we treat it as 2Ds at this moment. So, basically, what we want to do is that we want to extract interpretable features from the mammogram image, and we want to predict the onset of the breast cancer. So, this is the sample grams from two individuals. From two individuals. And so basically, we will first do the returnings. So they have roughly the same shape. And then we will do things called a biberator slide smoother. Okay, so basically, because this mammogram image is irregular, have irregular boundaries, so what we do is that we try the triangulations. So this is the kind of two-dimensional triangulations. And so we can approximate any shape in its 2D surface very well. And after that, what we do is that, so here, for each of these triangles, actually we can define a basic function on the triangles. So here, this is recorded Bernstein polynomial basic functions. So on each triangle, we can define these six BIT functions, and theoretically, And theoretically, it can process any two-dimensional functions on these triangles very well. Okay, so what we do is that we will write down the target smooth image as a linear combination of these basic functions. So now the basic functions is this two-dimensional bivertis spline functions. And also, we will add the penalties. At the penalties, so in the two-dimensional functions, we will look at the second partial derivatives to penalize the roughness on this smooth image. And then we can fit, do the penalize the like function, and we will get a smooth image. So then we will do the record a sudden functional TCA. Okay, so this is our main methodology. So, before we come go to the functional PCA, I'll let me give just this is the regular PCA. So, I know PCA is widely used in this 3D image analysis. So, this gives you the ideas for the PCA. Basically, suppose we have the two-dimensional data and we want to do the dimension reduction. So, we want to find the directions with the greatest variations, and so this solid lines will be. And so, this solid lines will be our first principal components. So, this is the ideas for the functional principal component analysis. So, one sense, basically what we want to do is that we want to suppose we have a vector x and a covariance sigma, and we look at the principal component u here, and we look at the projection of x on the direction u, and we only maximize this. U, and we only maximize this variance. So the variance is this quadratic form. So here we call the U as a principal component, and the projection of X on the U is called the PC score. So you see here, when we estimate this principal component U, again, this is a kind of discretized vector, so we don't have any control. So, we don't have any controls on the units. Okay, so now suppose here we have this smooth function Z, two-dimensional functions, and so we want to do the dimension reduction on this image. And so, basically, we want to get the principal components, and at the same time, we want these principal components to have some smoothness, and what the image shows that. So, to control the smoothness on the principle. Control the smoothness on the principal components. Our trick to do is that we will estimate the principal components as a bivariate function, and we will write estimate this bivariate function as again as a linear combination of the basic functions. So here, this V will be our biverbarita spline basic functions. So after we get this principal component, Ï†j, and then we can do the projection of this bivariate image on this. By variant image on this FPCs, and we will get FPC scores. So, this FPC score is like our features here. So, basically, we do this dimension reduction on these bivariate images, and we project high-dimensional biological image to a very low-dimensional FPC score. So, this is a very powerful dimension reduction features. And I will show you later on: like, we are able to project to Project to a very biogram unit just to two-dimensional features, and we are able to capture the most information to do the prediction. Okay, so this is the functional PCA. So, we also developed the Kodasua functional PCA. So, the idea is that basically here we want to find the feature to predict the onset of the breast cancer. Of the breast cancer. So the conventional function PCA approach is try to maximize the variations among these features, these FPC scores. So what we do is that we will add a supervised term. So basically, when we get these features, these FPC scores, you can see here, we also want to maximize the correlations of these features with this saliva time, this technical key, is the saliva time. This technical T is a silicon after diagnosis with breast cancer. Okay, so we added this correct term here. So this is what the supervised term comes from. So then we want to maximize this both terms together. So we add these two terms together and we do a trade-off between maximize the variance among the IPC scores and also the covariance of the IPC score with the survival time capital T here. And you have a Here. And you have a parameter theta control trade-off between the fitting to these two criterions. So if you're familiar with the, so for survival time, so basically for the survival studies, typically we will have a cutoff for the study. So basically, you know, they will follow up with these individuals for 10 years, for example. And after 10 years, they have to close the study. 10 years we have a closed study, and so maybe some individual will never get the breast cancer in the end, right? So it takes required sensor. Okay, so we don't have to know exactly the onset time of these breast cancer events. So because we have this missing data and we know that the soil time for these individuals will have sour time greater than this cutoff. Than this cutoff type. So, in order to deal with this type of information in statics, we come with a very cool current structure, current definitions to measure the correlation of FTC score with this specific solar time. Okay, so I will not talk about the details. So, then we developed this criterion to estimate this. Criterion to exit these supervised FPCs. So basically, in the end, we can write down this criterion into these core vector forms and then we can do the IVT conditions. I'm not going to go to the details. Let me show you what the LPC score looks like. So basically, this one, this graph, is the first LPC. So here the CC is So here the QC is FTC score, basically the projection of the smooth image on this two-wise FTCs. Okay, so here, basically, you can think about this first FTC, 5.1, as the weight on this image, right? So this FTC score basically is the weighted average of the smooth image Z here. Okay, the FTC file here serve as the weight. Serve as the weights. So then for this super LPCs, you can see it's positive on the red regions and negative on the blue regions. So basically, the first LPC scores can be interpreted as a weighted average. Sorry, yes. So it would be connected as a weighted average of this mammogram image. And so And so, because they have this negative weight on these blue regions and positive weight on the red regions, so this LPC score can be integrated as a difference of this image on the red region compared to the blue regions. So, that's why when the doctor sees the mammogram image, they want to look at the difference. So, from here, we actually can tell the doctor which region we should look at to look at the difference between the Between the between the image areas, so we will find any signs for the breast cancer. So these FPC scores are interpretable. And so basically, okay, so this is like the top one is the from reconventional function principle components. You can see it's also very smooth by red functions. And so this one is our own SUV FTC S. FTC estimated by considering correlation with the breast cancer some of the time. So then we can look at the geometry reduction. So basically we can project all the mammogram image to this two-dimensional space expanded by the two LPCs. And you can see here that using our supervised LTCs, the projection will be The projection will be more separable. So, the red one is the factor with breast cancer, and the blue one is without breast cancer. We can do the classifications. And then we can put this estimated FTCs to predict the breast cancer. I will notify the details here. So, this is the prediction results. So, with our new feature, With our new features, we just added two features based on the mammogram image. We are able to increase the prediction for the breast cancer. So I will not talk about the details. So basically, this is the summary for our supervised LPCs. So we are able to try to extract features from these supernovas. Sub-dimensions, and so these features are integral. And so we can accommodate this irregular boundary unit by using flexible vibrator splines over triangulations. Okay, thank you for attention. All right, we have about seven minutes for questions, so plenty of time. So, why did you use Princeton polynomials? Brainstorm polynomials, you could use any blended function. Yes, yeah. So basically, we want to use any basic functions. So, this is kind of what the mathematicians find these basic functions are kind of the best options if you want to approximate the two-dimensional functions. So, we'll be using these tools here. I have two questions. So, the first one is like in the kind of bivariate spine smoothing. Does this cause the Smoothie? Does this cause the discontinuity between two triangles? You mean can we do the discontinuity or? I mean, like, because like here, because basically you think of like a smooth curve to approximate the densities within each triangles. Yes. But in this case, like for the two neighboring triangles, and they distinctly between the edges. Yeah, so actually, like, this is a very good question. So when we do the smoothing, Moving. So here, basically, you're right. Like, so we kind of will, when we do these basic functions, it's only defined on triangles. But we also actually have a constraints on these coefficients such that it will have a continuity and also smooth over the edges. So, in the end, we can get a smooth boundary function over the whole domain. Yes. Yeah, there's a constraint matrix there defined. And the second question is like does the kind of the tense the densities of the triangles affect the final components? Yes, this is another good question. So how do we choose the number of triangle matches for the limit? So actually it doesn't affect our method a lot. The reason is that so for our method, actually we encourage you to put a lot of matches. To put a lot of matches. So, in the end, we built it with a rough penalty, and we can, you know, even you have like, you know, suppose there's only 50 meshes in, if you put 200 meshes in the end, because you have with the penalty, it can still control the synonyms pretty easily. Yes. Yeah. Thank you. Yes, so I don't work on medical images, but it's something which I try to understand. Demograph images are like X-ray scans, right? Demogram images are like X-ray scans, like 2D images, right? I think it's I'm not sure. I think probably 2D image, but I'm not sure if they have a 3D image available. That's 3 which I was trying to engage. That's which kind of constraint has five different strings. And then you try to uh like each document, you try to find out what are like each like part of it. And that gives some prediction about is there some is a rest of that particular region, and there is a way to classify that as it also possible to take. If it also has to take the output of the instance combination and put it back on the terminal lab image to show the location of this kind. Yes, yeah. So basically, like, so here the end, like when we see like interoperability, so it's basically identified as areas, you know, where we should look at the contract between the teams, look at the very general regions. 