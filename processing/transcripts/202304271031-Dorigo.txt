He's going to be telling us about machine learning tools for the reduction of the effect of systematic unsettling. Mark, go ahead. Thank you. So first of all, thanks to the organizers for inviting me to this fantastic place. And this has been a very interesting week. And so it's a pleasure for me to be here. And I will ask to talk about To talk about the tools that we have in machine learning to reduce the effect, the impact of systematic uncertainties in physics measurements and inference. And I have collected a fascinating set of different techniques that can be used for this and I put together a talk of that. But this is not gonna be it. Because it would be two hours long and it would be And it would be boring. And instead, I think I will have something of that. But I will use that as a way to look forward to what we learned by doing these exercises of the coupling from nuisances and looking at the bigger issue of creating systems for inference that actually are optimized from the beginning to the end. So, this is co-design, if you want. But we get there. One. But we get there. Let's first follow the plot. The plot you can find, if you are curious, in chapter 7.2 of this book, which contains all the stuff that I'm going to script. I will introduce the topic of what is the machine learning way of looking at the reduction of nuisance parameters and optimizing inference, and how we can formulate the. How we can formulate the problem. But then I will skip on all of the ways that this has been tried. I will give one example where we have a good benchmark where we can compare the different models, but we will go very quickly. I will only talk about inference-aware approaches, which have been developed to incorporate the notion of the nuisances in the inference and optimizing for the final variance of the measurement that we want to perform. That we want to perform. And the new part of this is that with a student, we have actually applied this to open data from the CMS experiment to reproduce a previous analysis by showing the possible improvement of this thing. And then I'll talk you, well, this ponous truck, where this has led us thinking forward, okay? So this was by means of introduction. So, supervised classifying. Classifiers can be used for dimensionality reduction. So we have high-dimensional feature data like Pico was mentioned in his previous talk, and we want to construct good summaries from which we can extract easily some inferences of parameters of interest. And this compression can be formed by forward simulation of a motor. Of a Monte Carlo typically. And we have problems because a Monte Carlo can be misaligned, can have known limitations in accuracy, can be imprecise, can be unsure about the latent parameters of the physics and also finiteness of the available data samples. So these are the typical nuisance parameters that we have been hearing about this week. Well, there are also unknown unknowns, and there Also, unknown unknowns, and they we cannot do much about that, and it will be the subject. And also, the error, the third kind, it's not been mentioned here, but it's very relevant. In some cases, somebody talked about the opera measurement of the speed of neutrinos. We should keep in mind that these are on the table, although we don't see them, right? But it's a topic of another workshop, I think. So, to account the imperfections that this nuisance describes, so alpha is. So, alpha is a Newsen parameter or a set of them, we can usually enlarge the model to include their effect. And if we can do that, and if we can have a knowledge of the PDF, we are done because you can construct a likely. But typically, in our business, we don't have that luxury. So, if we can parametrize them, we still can get away with it. The effect of these alphas will typically be to widen. Will typically be to widen the confidence intervals on the parameters that we try to estimate or to reduce the power of whatever hypothesis testing we are doing. So, in general, we know that these things we don't like, and we try to, if possible, we would like to decouple from them, rotate away from them, find representations of the data that are insensitive to their them, right? Because their presence, in general, and otherwise, affects and limits the nature. Otherwise, it affects and limits the measurement precision and the discovery reach. So, what we want are sufficient statistical summaries. We want, in machine learning applications, the way we have been using them since machine learning, particle physics, and fundamental science became a thing, was to minimize the statistical uncertainty on the estimates of the parameter of interest. And then, if you can do that, and if you don't have If you can do that, and if you don't have nuisance parameters, you can, but when you have these nuisance parameters, you have a problem because this extraction of information is conditional to the validity of the model and the assumptions that you make. And when you add these Newsom's parameters, things get murky, okay? The Newsom's parameters may have ill known or unknown PDFs, and it's hard to parametrize their effect. Their effect. So, what this boils down to is that our summary statistics that we can extract is not usually sufficient. So, it doesn't contain, retain all the information relevant to the parameter estimation task. I have a toy example just to visualize this, and you can make a simple fully analytic model of what the signal and a background distribution may look like on a classifier variable. So, and here I have chosen a So, and here I have chosen a parameter alpha that modulates the background. So, we don't know very well what the background shape is. So, when we get the true positive rate and the false positive rate and construct a raw curve, we will have variations of that. In this kind of example, we assert that we know the dependence perfectly. But still, let's look at what it means when we try to estimate the signal fraction in a mixture model or try to extract some. Model or try to extract some measure of the significance of the signal contamination in a mixture. So we can use the approximate median significance, which is something that works well in these kind of scenarios. And if you analytically compute this with some data that you can try, you can see that in this example, of course, when alpha is larger, you have a more discriminator or less discriminator, and you get. Or less disseminator, you get to a situation where you get a different significance. But so the point we should reason on this, if we train a classifier with a given value of alpha, the performance is going to be underestimated or overestimated if alpha is actually different. And the choice of a critical region corresponding to the predefined false positive rate, for instance, will similarly be affected. So if we don't know, So, if we don't know what alpha is, we cannot optimize. And users parameters affect both the optimal working point, the performance of the classifier, and the relative merits of different classifiers overall. So, standard supervised classification techniques may not reach optimality in the presence of these supermeters. So, this is the whole point, if you want. And then this is an analytic example, but we are not in that situation. That situation. So, I have here a slide in place of a talk because I put references of various ways that this problem has been addressed with machine learning techniques. And I have a list of references, the backup, that you can look at when I will finally upload this talk. So, you can try to parametrize the nuisances in your model. When you can do that, that's good. It's been done in a paper by Baldi, Wideson, and others. And others, when the Newsence parameter was the unknown mass of a particle that we were looking at, a new particle signal on a large wide spectrum. So that is the way you can think at the Newsence parameter as the actual thing that characterizes your signal, and you want to build a classifier that can be active, whatever the mass of this particle is, so that it has good reach for different masses. You can do this by adding the nuisance parameter to the training features, and you can do that, and they did this in this paper. And then there's a whole field of the correlation techniques, because really what you want is to decorrelate your nuisance parameter from the features that you train your network with, such that the output of the network is insensitive to the effect of the nuisance. And so, there have been many, many different techniques for doing that. And I will have a plot. That. And I will have a plot just as a summary, just as an example of that, because you can compare many of these techniques on this given example. I'll show you this in a minute. Then there are also adversarial setups that penalize if you're training data and the data that have a different value of the nuisance. This is a domain adaptation problem, right? You can use the adversarial technique. And you can use the adversarial techniques to constrain this. All of this is in a lot of literature right now, it's expanding very quickly. So it's difficult to make sense of this wealth of results, but we can concentrate on one single example. So for the statisticians here, we discovered 20 years ago that we could tag the presence of heavy objects like a W boson inside. W boson inside the fat hadronic jets, so strings of particles, and you have the decay of an object inside a single jet. It was a surprise that it can be discriminated well from large QCT patterns inside a single jet. So a whole technology has opened up and ways of classifying and separating the signal of this heavy object from continuum. And in this context, the mass of the heavy particle that you're trying to reconstruct can be treated as a nuisance parameter, which you want to decorrelate from classifiers that enhance the signal purity. What I'm trying to say is that if you have a classifier that leverages the features of signal and background and tries to enhance the purity of the signal, they will typically scope the mass distribution that you have. So you have a background distribution and your signal. And your signal sits here. And if you train a classifier to distinguish the two, after the classification, the signal, the background that remains will be sculpted and it will prevent any further inference and make it worse. So this is not exactly a nuisance parameter, but the methods to address it are basically the same. So this is what you can see from a jet that has two components, the KOWS. The KOW position. So here is a summary plot that because many of these techniques that address this correlation, you see the QCD background is different from the signal, but after you select with a classifier, you will score the background and get a peak which you don't want. But if you can decorrelate it, it's good. So you can measure the correlation by the inverse of the shadow. The correlation by the inverse of the Johnson-Shannon divergence between the background before and after the cut of the classifier. And here you can put the background rejection factor. So you want to be as far out as possible in this plane. And you have various techniques that have been addressing this by doing planing, which is a way to pre-process your data such that you weight by the inverse of the density. You weight by the inverse of the density of the signal and the background of this nuisance variable, such that the effect of the classifier will be will not affect this shape, if possible. And in fact, planning works quite well if you see these empty points here. But other techniques have been compared on this graph. So this is the way to go to compare different techniques. Unfortunately, most of the methods that I've listed here are Are addressing specific problems, different kinds of problems, setups, and different nuisances. And so they are all good, but they are hard to compare. So that is why I liked this plot. I will rather talk about inference-aware neural optimization, which is one other way of addressing this problem of nuisances with machine learning. The idea we had a few years ago is to make the loss of the neural network aware of Aware of what we really want to make of the output of the neural network itself. Because typically we optimize a classifier to distinguish as well as possible sigma flow background, and then we say, oh, but we have systematic uncertainties. So after you have trained your neural network, you go back and change the nuisance parameters and you get different outputs and then you have to account for the effect of the systematics. But the poor neural network didn't know anything about these nuisance parameters. About these nuisance parameters, so you are misaligned. Your result is suboptimal. So, we wanted to get away from this misalignment. So, if the neural network constructs summaries that are differentiable with respect to the nuisances, and this property can be propagated to the inference, you can do a global minimization, which will guarantee that you have the perfect, the smallest possible variance of the particular instance. The problem is that you need to produce this differentiable map. Differentiable map and it calls from custom solutions. And then, how do you actually encode in these laws the final variance of the parameter of interest? You can do this by using the inverse of the information matrix of a saturated model, a saturated likelihood constructed with the summary statistic that the network provides. So, this was done with this kind of setup. I'm not going to bore you with it, but basically, you have a simulator that produces. You have a simulator that produces some multivariate data according to some parameters, latent parameters, if you want. And the neural network will try to, if you have a mixer model, signal and background, it will reduce the dimensionality of this data and produce an output that is your summary statistic, if you will. It can be multi-dimensional, and you need to be able to differentiate it. So you produce a soft maximum. You produce a soft max map of this. And then you can do the inference mock. You can create a model of your inference because you inform a saturated model of your signal versus background mixture model extraction of the signal fraction. You get the information matrix and then you propagate this back to the network and you let it know about it so that the network can train in order to minimize as the loss. To minimize as the loss the variance on the parameter on the signal branch. This has been done and shown on a synthetic example with three nuisances and a few features. And we proved that this technique is called inferno can significantly shrink the variance on the parameter of interest by basically decorrelating rotating away the effect of systematics. It's been proven on several benchmarks. It's been proven on several benchmarks, and it can basically track very well the variance, the uncertainty of the parameter of interest in these benchmarks is not very far away from what you would get with an analytical likelihood that knew exactly the value of the Newson's permits. So this works, but it's a synthetic example, right? So what we did recently was to test it on a real physics analysis. So we proved that we could do it with the complications of physics analysis. Analysis. So we took a run one LHC analysis by CMS, which was done with a neural network, not many were done with neural networks back then, and at large dominating systematic conservatives. So we landed on a TTPAR measurement of the cross-section in tau plus jet. This is the original analysis. And we reproduced it. Okay, this is all a different classifier, but basically we could get the simulation. Basically, we could get the similar results. To get to that problem, took a year also because open data can be used by anybody, but it's by no means easy. So, all this work is useful because now we have a benchmark that anybody can use because all the data are there, pre-processed, and all the tools are there. So, it's a good benchmark for future studies. What we showed is that what we found out actually What we found out actually, we sort of knew that Inferno can decorrelate well systematic uncertainties that affect the shape of our features, but it cannot do anything with systematic uncertainty that affect the normalization of the background, for instance. If you have normalization uncertainties, they cannot be rotated away by this time. So, here you have the jet energy scale invariance, and when you train the classifier, When you train the classifier, Inferno is an orange, it will decrease its effect. And binary cross-entropy will not do as well. And you can see this. And this is the variance on the Newsence parameter. And the correlation between the Newsence parameter and the signal strength is made flat by the classifier, that you are effectively decorrelating them. The final result didn't have such an impact on the profile action that you can plot because many of the systematics were actually normalization uncertainties. So we learned that we can get an improvement only if your systematics have a shape or uncertainty that's strong. Sorry, it's it's master. You don't mean for example things that only affect, say, the minimalization of the background only, but not the signifier, because that would change that shape. I mean, everything if everything is shifted up and down, it can't be. Yeah, I mean, yes, it, of course, you have a mixture, so this will an effect that, for instance, the luminosity and certainty is an effect, right? Others where they be tagging scale factors that typically underline. So you can look it up in this preprint article. Because that operates separately on the paper. then it operates at the other layer. It does, but in this particular case, there was most of the effect was from normalization kind. And you can see the distributions in the paper. So yeah, what I think I mentioned this is because now we have a benchmark. You can look it up. The code is public, the data are public, and you can play with a real CMS analysis, which I think is the purpose. Which I think is the purpose of this open data kind of things, but the bordering is very strong, very, very high when it's right. Okay, so let's go to my bonus track, so to speak, because the idea of Inferno, which is a deep learning-powered realignment of inference tools and experimental goals, variance or parameter of interest, has brought us to look into end-to-end models for the codesign of hardware and software. Hardware and software. And we will see that the issue of systematics is present there as well. Because when we are designing an experiment, a detector, for we want to measure something or a quarter of various different things, we are facing a space of thousands of possible parameters, thousands of possible choices, geometries, technologies to spend our money into designing an instrument that will measure something. And these kind of choices have been so. And these kind of choices have been so far informed by expert knowledge, past experience, but if you look at the design of collider experiments in the last 50 years, they have stayed the same. We basically have some paradigms. We track first and destroy later, because we dread the nuclear interactions in tracking. We want to get very well-measured tracks out of our collisions. And so, if there is a lot of material in your tracker, this gets spoiled. This gets spoiled. But now we have artificial intelligence tools to do the tracking, so we shouldn't be worried by them. So this paradigm is hard to get rid of, but we should look into alternative ways. We are on a rapid road of methods for inference pattern recognition, and we are designing stuff that will be operative 20 years from now. In 20 years, the way that we will extract the information from these instruments will be totally. From these instruments will be totally different. We will not be doing a filtering of drugs, I'm not sure about that. So, we should look into this and try to not do the mistake of designing experiments that our children will operate and they will blame us and say, oh, why did they do this thing when I have now these codes that will do much better? So, yeah, this also comes from the cognitive distonance that I had when I was hearing a very esteemed. Was hearing a very esteemed colleague that has been doing silicon detectors for his whole life, designing one of these future collider trackers in the way that he has been doing the same business for 40 years. I don't think this is what we want. So, we want to develop methods to analyze both the hardware and the software, the inference, all together. I have two examples that show why this is important. There are two simple examples of experiments. Simple examples of experiments that have been designed where end-to-end optimization models have proven factors of two gains in the final figure of merit. And I will show you results for one of them, which I was involved in. But factors of two in high-energy physics are huge. Saves millions and millions of dollars, okay, if you can do that. So one example of that is Muon. A muon is an experiment that has been designed to measure the scattering of To measure the scattering of muons of electrons. A beam of muons interact with the target and produces scattering with an electron. And this can shed light on the hadronic loop contribution of the muon electron, muon photon vertex, which in turn will shrink, if we can measure this well, the uncertainty on the standard model estimate of the neuron magnetic, geomagnetic anomaly, which right now is Now is in strong disagreement with the standard model. So at Fermilab, we are trying to measure this well, but this experiment planned at CERN wants to shrink this uncertainty by doing this Q-square differential measurement of this scattering, okay? If we can pull it off. So a set of experts have set up to do this and they have designed a system made of 40 of these stations where you have a target and a few silicon detector layers. And a few silicon detector layers. So, and when, and I happened to be a referee of this experiment, and I asked them to try and improve their design because I thought this would be good, but could not be best. And they ignored me, so I wrote a simulation of the full layout of this thing with variable geometry and a model also of the inference extraction, and then optimize the geometry. And then optimized the geometry for the best inference, and I published it and I proved that a factor of two could be Galton. And Mion, after some digestive trouble, is accepting my proposal. But you see, this is the relevant quantity, the differential cross section, the relative resolution and the momentum transfer because that's that's the the the thing that we've got you to the thing that we get you to reduce the uncertainty. And you can see that you can really reduce this factor, particularly at IQ squared, which is the most relevant factor. So this scattering involves an incoming muon and two particles. It's very, very simple for us, three particles, right? For a particle physicist, it's a no-brainer to fit these three particles to a common vertex. Vertex. But if you are not thinking in terms of co-design, you might say, okay, let's leave this for later. I don't care about it, because you're thinking in terms of a thick layer, a thick detector, sorry, target, where the constraint in Z will not do much. But it is crucial to have this constraint in Z. So a better way to think of the matter is interactions take place in the target. The target is thick, so you need it thick to get a lot. So, you need it zig to get a lot of interactions. It does not offer a z-position constraint. So, the fit that involves the z-coordinate is useless. No, we should restrict the layer to exploit the constraint. So, you can use field layers based in air, and this will give you the correct constraint. But wait a minute, you have a systematic on the positions because you're talking about micrometers or in positioning in Z, and you have a beam that interacts with it. So, how you will have large systematics from the actual beam. We learn large systematics from the actual position of these layers. So it's hard to position these elements. So feed layer layouts are impractical, no, because by exploiting the vertex fit to many events, we can manage to reduce this systematical surface. So the correct way is to think at the systematics that you have in your problem and try to reduce it beforehand, because it's all good if we can rotate away systematics, but it's much better. Data waste systematics, but it's much better if we don't have systematics in the first place. And you can do it only if we do co-design. So, this is the concoction that they have put together, and it costs 10,000 errors per station, so 400,000 errors of an holographic system with lasers that determine the position of these layers within 10 microns. And instead, if you think about the problem in another way, you can actually, with thin layers, you can constrain all of the not only can constrain all of the not only the positioning but also the bow and the the tilt of this uh of this uh uh detector elements to micrometer precision with five minutes of pin if you have a feed to a common vertex and three layers. So this is how you can reduce systematics at the initial, okay? Okay, so we put together a collaboration that has now 24 institutes and looks at optimization. And looks at optimization problems end to end to try to incorporate in the design of the apparatus the notion of what you're going to do with it, the way that you're going to extract inference, all of the systematics that are intrinsic in the problem, and do a full parameter scan, not a discrete scan as we do with the complex and costly Monte Carlo simulation campaigns that typically go like this. I have an initial setup. Like this. I have an initial setup informed by a 65-year-old guy. I test it with some simulation. It's good. Then I say, okay, what if we change something? Let's look at this other parameter space point. And it's a little bit better. Oh, okay, let's put a lot of simulation there. And then the CDR has to be written and you're done. And you cannot move any longer. So instead, you have a fantastically complex space of interconnected design choices. And if you can scan that in a continuous And if you can scan that in a continuous manner, you gain access to better knowledge on how to constrain all your quantities of interest. So we have started to do that with very simple problems, because this is incredibly difficult. But we want to construct a library of solutions that will allow us to tackle more complex problems later on. So I don't think we have time for going through the way this can be done, but I will show you a graph. Your graph. The Monte Carlo is here. It gives you some particle-level truth that then you can use with a detector simulation to give you the response of the detector, these 100 million channels that Macker was talking about. And if you can construct a differentiable surrogate of this whole chain, then validated with, of course, some digestive trouble also here, you can Trouble also here, you can take some initial geometry for your detector, pass it through a chain that infers through a pattern recognition model and an inference extraction the value of the objective functions that you want, the significance of the exposure, but also looking at supersymmetric signal. You can construct a multi-target objective function if you want. We do it when we sit in trigger meetings and we decide the bandwidth of every different possible analysis. Width of every different possible analysis. We are calling each other names, but at the end of the day, we decide what the experiment-wide loss function is. So we should be able to do this in a model, and then you can include nuisance parameters, cost, and other constraints, and you have a pipeline that you can navigate with stochastic gradient descent. And if you do this, you have access to the full space of parameter choices. Okay, so this I will show you one thing. I will show you one thing that we are doing right now, just another simple test of this kind of technology. This has been shown before, and Lee and Alex were discussing this use case. You have a ground-based array of spherical tanks looking at atmospheric showers, and you want to find the gamma rays of very, very high energy by laying out these tanks on high altitude. Okay? And how do you? And how do you put the tanks on the ground? Because, okay, you know that if you put many tanks packed in, you measure well the low-energy showers, but you want to measure also the high-energy showers that have a very large footprint and they are very rare. So these are the footprints of the detectors, the 6,000 tanks that have been considered by Swigo. And you will see, oh, by the way, I joined Swigo because it doesn't work well. Because it doesn't work well if you are advising on detector layouts from the outside, as I was telling you before. So this time I'm inside Sleeve. I want to do this job from inside. But okay, if you notice something, there is a dense center and a sparse periphery, right? And all of these designs don't look very different to me. But you will notice one thing, that these these cores will measure well photons that hit them in the center, which is a set of measures zero. Which is a set of measures of zero. So there must be something better because showers have a characteristic distance radius size, which is the Moliere radius on atmospheric development in the atmosphere. So which one is the best layout in terms of science output per invested dollar of these eight proposed layouts? To me, this is not the precise answer I want to answer. Okay, you can answer this by simulating showers and cooking up a utility function. Here, for instance, is a weighted average. Here, for instance, is a weighted average of flux precisions. But I think the more principle question is: what is the best possible layout given X detectors, which is not lying an answer among eight, but it's a point in R to the 2x minus 3, the possible layouts on the ground of X detectors. And this cannot be answered by discrete sampling, of course. So, So. Yeah, yeah, yeah. I'm going to finish. So we can parameterize these showers because they are complex. So as complex as they are, for the kind of order of magnitude kind of estimates that we want in order to navigate this space of designs, we can cook up a parametric model. And then we can compute in closed form the utility function for any detector configuration. Of course, this will be a Configuration. Of course, this will be a gross approximation, but it will retain all the correlations of the problem inside if we don't prove it all. So here is the pipeline. You start with an initial layout. You simulate a set of gamma and protons. You want the gammas, the protons are the background. You reconstruct the shower parameters for each hypothesis. Is it the gamma? Well, then these are the parameters. Is it the proton? And these are the parameters. By likelihood, you find. By likelihood, you find the log-likelih ratio of the two hypotheses, and this will be your test statistic. And you get the PDF by someone. And then