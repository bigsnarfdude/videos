Prevent algorithmic filter bubbles in social networks. This is joint work with Christian Brooks, Jennifer Chase, and Ellen. Yeah, so as we all know, social networks are a very prime, it's a very big role in how we function in society, get information, communication, idea exposure. Even for democracies, this is how we often come together to decide what the best decisions are. However, as you all know, like a social media platform, its profit is derived from engagement. Its profit is derived from engagement, which leads the platform to show users the most addictive content, which is not necessarily the healthiest content, either for democratic reasons or just even for the individual person. And however, ML slash AI-based recommender algorithms are also very powerful and very effective tools for curation, discovery, community building, and so many important aspects that we also need. And so there's this kind of trade-off here between these algorithms, which might be These algorithms, which might be used for creating sort of like filter bubbles or like conflict in the society, versus like the positive aspects of a curation and community building that we really value. And so our main question of this work is that can we design recommendation algorithms that preserve some of these like positive aspects of community building, content discovery, while avoiding the pitfalls of filter bubbles and polarization? So to understand this problem, we model Understand this problem, we model the platform and the users as a multi-armed bandit, where there are K categories to select for like fashion, sports, left-leaning content, right-leaning content, et cetera, and end users of the platform. And so as the platform interacts with the users, they show them some content, like I say, YouTube, you show them a video, and you receive some random reward from some institution that you don't know. And this reward could be maybe like ad dollars, time spent on the platform. Like add dollars, time spent on the platform, whatever you think is important at all. And so we assume that these distributions are known to the platform, but they have like finite mean and some just nice properties. And so as the platform interacts with the user across the time steps, at each time, every time they log in, they decide on a distribution of content to show the users, which is known as PU, where it's basically a bandit problem for each user. Each user. So for each user, PTUJ is like the probability of showing user u content from category J. So this vector for each user. So the problem is that the user, you can see the user, I think like a, almost like a context, but it's not a context here, it's like a numerical thing. And then there's a singular multi-armed banded problem for each user to see. Right, right. So if you were thinking about it in like a contextual bandness, maybe you have different parameters for each user. Different, like you know, parameters for each user, but here we're just base these things into separate bandwidth problems for now. There's no correlation between the users, right? Yeah, for now, yeah. And we're assuming independence and yeah, identically, yeah, well, not the same distribution, but everything's independent and no correlations like that. Yeah, and so the platform's goal is to pick a distribution that maximizes the expected reward at each transfer across all users. So, basically, just and like in a typical bandit problem, you have to basically balance learning. Problem, you have to basically balance learning the right distribution with you know getting good reward along the way. And so, yeah, so given this, you can imagine like with your typical you're running UCB or something, it's gonna run, and then for each user, you get like the optimal arm, and then you just show them that arm for the rest of the time. And you can imagine how there's some empirical evidence, it's still a debated topic, but there is a lot of evidence showing that that sort of thing does lead to like vote bubbles and like polarization. It's still up for debate, but there is some like. It's still up for debate, but there is some like some works showing that. And so, what we want to do is not have that sort of, we don't want the platform to just converge to one category. You want it to show like a diverse sort of categories. And so one way you might think about doing that is say like you have this platform, you have people's interests, and you don't also, like, by the way, you don't wanna just force people to show tag content. Just force people to show like CAD content on LinkedIn or something. You still want what you're showing to be relevant to the users. And so, one way you might do this is take: okay, what's the average interest across all users? And you get some vector, just P average. And then you just say, okay, we have this average, let's say, consensus, whatever. And you want each individual's distribution to not be too far from the average. And that's one way you can imagine, okay, then people are going to be closer together. We're going to have less polarization, less filter bubbles, and all sorts of things. And all sorts of things. And so, the way you're going to just model this is you can just take the norm between the two probability distributions and just have some parameter here and see your distance is at most this delta parameter. And so delta serves sort of as a personalization budget or cap-on polarization, however, you think about it. Basically, you can only personalize from the average by a certain amount of delta. However, However, as some people have alluded to in earlier talks, like Jamie's talk about, when you have small groups, this is not necessarily the most fair or way of doing this. And actually, if you just do it this naive way, you can actually prove that this will lead to tyranny of the majority, where what actually ends up happening empirically is that rather than actually showing diverse content, you just kind of ignore showing groups that are too small and still show polarized content, but to like the big groups. So basically, the small groups are only seeing things that the Groups are only seeing things that the majority wants, and the majority is still very polarized and not seeing anything that the minority groups want, which is like you know, not really so good. Like, this is called the tyranny of the majority. And we can prove that this happens to be the optimal solution under this naive formulation. And I think it doesn't really, it's kind of coarse in a way and doesn't really distinguish between different sort of like distributions that might satisfy this. So, for example, if you imagine where we have like end users and k categories. N users and K categories where each user only likes one category. There's two distributions that could satisfy this constraint. For example, you could have like a uniform where you just show everyone, you just uniformly pick from a self-categories. Or you could just pick one user, show them everything they want all the time, and then ignore everyone else. Those both satisfy the constraint, but as you can imagine, one feels like morally sort of like a better one than the other, even though both do satisfy this sort of constraint. And so, like, based on this, Based on this problem, we then came up with a different way of formulating this problem. And so, our approach relies on two key assumptions, like ideas. So, the first one is that users should be primarily seeing the stuff that they're interested in. So, you don't want that sort of situation where the minority is only seeing what the majority wants. So, you still want people to be seeing, like, always seeing things they're most interested in. And this avoids reading the majority. And then, the second intuition is that content should. And then the second intuition is that content should still contain some flavor of the overall sort of content on the platform, which is kind of like the anti-polarization or photovoltaic sort of assumption. So you can imagine sort of like a maybe like a town hall sort of thing where like during the week, everyone is kind of like doing their own thing, but then they all have to come together to a room and talk for a little bit once a week or something. You can kind of imagine that sort of situation. And so with this, we came up with two approaches of solving this. So the first one is what we call like the problem. So, the first one is what we call the personalization cap sort of framework. So, rather than that kind of constraint where the distance between the two, the average and the individual distribution is bounded, instead we use the average as a lower bound on each person's sort of individual. In favor of the trusts. This gamma parameter serves as how strict you want to be about this. So it serves as a cap on the level of politicalization. So when it's zero, that's basically no constraint. You can just show everyone. When gamma is one, basically you're forcing everyone to see the same distribution. And that's no approximation whatsoever. And so we proved that for gamma less than half, That for gamma less than half, we don't have this theory of the majority problem. Like every platform, every user does see majority what they want, and you still get to see some level of what, the interest of the overall platform. Then we also have another sort of like formulation. So with the other framework, it was sort of like a hard constraint on the like the people's distribution. But what if you wanted something a little bit softer? Maybe you were a regulator instead of like. Software, maybe you were a regulator instead of like you know forcing the platform to do this you might want to maybe tax them on declare personalization. And so we come up with this other framework, which is basically just measuring how much you violate those constraints from the earlier one with some extra parameter just to say how like aggressive you want to be about taxing. And so it's basically just how much you validate the constraints. I'm just penalize you by how much you validate the constraints. And so yeah, when n is greater than zero, it can be like a tax on personalization. Can be like a tax on personalization, while what it's less than zero is more of a subsidy on diversity. And yeah, and so regulators can pick either Edda or Gamma to steer the platform in the way that they want. And yeah, and so I'll get to more of the results later now. Actually, algorithms that work in this framework, but one more problem that you want to think about is also: regulators don't actually have access to this PU sort of like probability distributions. They can only see what you actually showed users and not the. What you actually showed users and not the actual distribution, which could differ from what was actually the true distribution. And also, they might not also be able to check every day for every person what they saw. Also, there's some privacy problems there by someone being able to look into that. And so, we also think about how the regulator actually audit the platform to see that they're satisfying these constraints. And also, you can imagine that maybe they know that the regulator is coming on like Monday, they make their things very diverse. They make their things very diverse on Monday and stuff like that. So, you can just imagine some sort of shenanigans that could happen. And so, we want to be able to understand it. Volkswagen mode. Yeah, exactly, right? Like the test, like only, yeah, putting that sort of thing where, like, oh, emissions are only as good when it's testing. Yeah, you can imagine sort of like they definitely would be like kind of thinking if this was actually enforced. Yeah. Yeah, so our main technical results are. So, we have a variant of the UCB algorithm, basically one per users, but then you have constraints that are basically constraining them globally. Constraining M globally. And so we show that when gamma equals to one, you get regret of that's like square root of n, which just matches. And one way you can think about this, when gamma equals to one, there's no personalization. So it's basically collapsed down to actually just one bandit problem because you're picking the same distribution for everyone. But now maybe your preferences of rewards are a mixture, equal, mixture across all users. So it's still basically now just boiled down to just the single band case. Does that make sense? Case. Does that make sense? Yeah, and so that's where you can get this square root n sort of regret. But then when gamma is less than one, now things get a lot more complicated because like you have this sort of like global sort of constraint, but you're still playing, trying to like, you know, maximize each person's individual rewards. And we show that we can get regret of like, that's now linear in N, because you could be losing some penalty for each user here. Oh, yeah. Oh, yeah, I'm sure everyone knows what regret is here. And so we also have lower bounds. And so for the gamma less than half regime, we show that it is, the lower bound is when you're in n. And then in the whole regime, we have that the lower bound is square root n. So basically, what this is telling you is that for gamma equals to, there's like a gap here between the upper bound and the lower bound. Bound when gamma is small. And this basically shows that as you're like, when gamma is one, it's basically one problem. But as you start having more freedom in your choices you can make, the actual choices of potential distribution actually grows much faster than the algorithms we can do right now. And so there is a gap between a load bound upper bound and the gamma-listed half-vision. And because it is actually a different problem as you start having more freedom for each user. That's like a threshold, or like is it a fundamentally different problem, or is that a function of analysis? I think it's a function of analysis. I'm not quite sure. I would imagine there's probably some threshold, but like, I'm not, it's not clear. But it's not clear that there's like a capsule movement. Right, it might, it might, yeah, it might, yeah, exactly. Yeah, so I'm not exactly sure. And we also analyzed the case where the regulator charges basically empirical distribution and like we kind of analyze all protocols. We kind of analyze all possible strategic things that people could use and make sure that it's not too bad, that you can still get your error growth supplementary and t when you have the regulator working on the empirical versus the actual distribution. Yeah, so those are our main technical results. I also quickly get into some potential extensions that we're working on right now. So, this is a joint work with Christine Lauder and Max Nico at Meta. Essentially, here we actually Essentially, here we actually want to like. In the original problem, there's no graph structure in the analysis, but here we want to incorporate the graph structure between users and show maybe some trade-offs between certain graph structures and polarization and efficiency or things. So, you can imagine maybe like if you have a star graph, this might be really good for dissemination and diversity where users are sharing information, but it might be really bad for like, let's say, you have some hate groups that want to make everyone see their stuff. Like, wanted to make everyone see their stuff. Like, in our old formulation, if you have a small group, it's a really bad group, it's still their stuff's going to be promoted because of the way our things work, but you don't actually want to have that. And so, we want to relate sort of like the graph topology to like, you know, diversity, trade-offs, also things like hate groups and other sort of things. And also, like, for example, you might want to do some like network-level intervention. So, there's some empirical evidence showing that just directly showing people content they don't like might actually have the opposite effect. But you can imagine sort of a bridging sort of framework where rather than. Sort of a bridging sort of framework where rather than directly showing them diverse content, you might upgrade their relationship with diverse people in the graph. And that way you still get the sort of same effect without having these negative feedback effects. And then also things like fast mixing, because our learning algorithm in this context really depend on how fast the random logs mix on a graph to converge. And so maybe you want to actually improve the efficiency of the graph by operating widths in such a way Of the graph by upgrading weights in such a way that things kind of like learning a sect better. Other extensions is where you should have dynamic or time-evolving networks where maybe like users are leaving and entering the platform, or maybe if users don't like what you're showing, they might go to a competitor platform. And now with competition, how does that actually affect the diversity and efficiency trade-offs there? And also, like here, we didn't actually define what we mean by polarization as sort of like implicit in our math, but maybe there's some more. Maybe there's some more principled ways of defining polarization and using that a priority and seeing how well these operators satisfy, those sort of thing. And also, like privacy concerns, you could also imagine where adding a little bit of privacy to users would actually help improve polarization and focus bubbles without necessarily making rewards worse. So yeah, so yeah, in conclusion, our work was a flexible solution to decentralize filter bubbles while adapting to the interests of the platform. Adapting to the interest of the platform. Yeah. Thank you. Any questions? I have a question about like the meta. Yeah. I guess related to bridging, but like a different type of bridging, like and related to the fast mixing. Could you think about like being thoughtful about the specific links or connections you suggest? Connections you suggest in the graph so that, like, say you have faster mixing or that are somehow more amenable to these types of matching? Yeah, so that's, yeah, so yeah, the reason why I changed the talk is I'm actually actively working on that and quite close, but I haven't fully figured it out. So basically, what I'm trying to do is, if you have some budget of ways you could add to upward connections, we're trying to think how can you allocate that in a way that incentivizes fast mixing and also. Incentivize is like, you know, fast mixing and also bridging in a way. But I'm not quite, I haven't fully figured out the full analysis, but that is like something that, yeah, that we're trying to think about. Yeah, I can do it. Do you differentiate between, so of course your first model is abstract, but when you look at future work, do you differentiate between just liking something because, I don't know, I like music, but I don't like politics. But I don't like politics, and I like this type of politics and not the other type of politics, which is the filter bubble problem, right? Because if you just give everyone all of the content, it's just going to be topics they don't care about, not because of polarization, it's just interesting. Right, yeah, so right now we're not necessarily distinguishing to that level, but the fact that we're basically trying to match the natural interest of the platform. Natural interest of the platform that should naturally address that. Some previous work that just puts lower bounds, for example, without actually adapting to what the platform does. And so here, by actually having this average, we're sort of implicitly doing that. But we're not explicitly analyzing that sort of thing. I don't know if that answers your question. Yes, thanks a lot. I have a very naive understanding question. So I'm wondering about if I'm thinking like because you also talked about regulation. Because you also talked about regulation, and I sort of like want to regulate these platforms. What is actually the quantity that I would like to measure? That I would regulate? To me, this seems to be a huge problem to find the right measurement. And sort of like in your talk, if I understand correctly, you have sort of this motivation that I have the different topics and I want diversity across the topics. But how clear is it actually that this is actually the right quantity that I would like to address and measure and regulate? Yeah, so it's actually not clear. Yeah, so it's actually not clear. So that's why I was like, this point is that there's different people have different definitions of what they consider polarization, good, bad, and like understanding these different formulations and actually how they relate to each other is a very interesting problem. And then now extending that to actually learning and are these objectives even like, can you actually do low regret and like those sorts of things? So yeah, so it's not clear here. We're implicitly just like assuming that, oh, we just want diversity. Just want diversity, and but then, yeah, so actually boiling into like exactly what do you mean by that, and what are the definitions, and actually understanding that is like an open thing is actually very interesting. So, yeah. Questions? I guess one question I have, which is something I've only thought a little bit about, but the way you describe this made me think that it sort of seems like there might be a difference between the short-term and long-term objective. Like the short-term and long-term objectives of either the platforms or their users, right? So, like, maybe what I want right now is like, I don't know, all content S all the time, right? But, like, eventually I get tired of it. Right? And that's like not good for me, and it's not good for the platform, right? Like, some heterogeneity or like variety in the content is maybe necessary both for me to enjoy myself and for me to stay on the platform. Have you thought any? Have you thought any about some of those distinctions? So, I haven't explicitly thought about that. I do agree with you because I've also thought about this in my own personal experience. For example, I watch a lot of YouTube and eventually what ends up happening is I get into these ruts where it's like either videos I've already seen very close and stuff like that. And obviously, my utility is kind of like I have decreasing marginal utility for certain things. And so, you would want, you would imagine that. So you would want, you would imagine that at some point diversity actually aligns with the user's interest here. We don't actually show that, but like, yeah, explicitly analyzing the user's reward function might maybe address that. I also think maybe Alan and Luti have some work that talks a little bit about this. Yeah, so yeah, so I think people are thinking about it as an interesting problem. And even also, another thing that you might also want to imagine is that maybe within the categories, there might be like. Within the categories, there might be like differences where maybe, like, oh, I only like certain kinds of like, let's say, like, romance content, if I don't like romance content overall. And I can imagine where having diversity could also help you kind of, you know, cut off the lower ends of the distribution by picking from the highest ends and getting something better than you get individually. But I haven't actually thought about how that works. But I know some people have. That's an interesting extension. Any other questions? No, I think lunch technically starts at 11:30, so I don't know if you can see it. I can get a little bit of a long story. So I'm sure there's some screenshots. And then a week later, I'm removing my last piece of medicine. I talked about my last piece.  400,000. Yeah, you go back and warm up. I'll put smaller plots to that. Yeah, yeah, no, right, but I also think it's like it should be so I wouldn't know how to measure that but You could not be honest. Because you always need to make some complications to write down a model. I guess the channel doesn't always make the right decisions and say, actually, describe what we are interested in reality. That's why I discuss my interest in the best view. You normally minus one if someone asks minus one. Basically, things like that. The model learns to do like I say, but it's somewhere else. I submit them in how may I have to write. But you can do better. I think that's what they're doing. I think this becomes like a mustard. So the idea of processing is a very important thing. Yeah, I mean it it doesn't have to be any possible.   Yeah, she's like, why are we using off? I don't think we have to. Oh yeah, who's CGN? Uh co-teaching. They're co-teaching? Yeah, it's a terrible idea. Basically, yeah. No, it's a terrible idea for them to co-teach because they are both on maturity. But also interesting, I never thought that he would have been for someone. Well, it's really nice to use a number of things. Yeah, no, even he didn't look at his techniques. Okay, so he uses a good experience to learn. I think it's just. I think it's just like an imaginary one. I think our working groups get a lot of stuff. I think all of us ask them. But yeah, we'll hear some topics and the mission to discover you like. We were thinking of something. Yeah, I mean, that's why we have because they're obviously only focusing on the money. There are very few people who are not on leaf. And if they're not on leave, they are so modern. The models, particular modes are changing so fast that you can. Yeah, or like they're having to like teach like the vision of people teach like a vision thing. That's kind of like. So it's more like if they want to get out of teaching, they can't very easily say what to contribute in the area. Oh, I talk more to more, right? And what's your idea of the same thing? Oh, um well I guess like a property that I noticed more about is like yeah. Honestly, but I have this course that's like I mean actually I agree with the funnels of the principles I had. But like one thing that I thought is very interesting, but I think it just part of the problem. So I saw a paper which says that like facing a prior error is some kind of opinion right now. But where the snapshots are supposed to be said, but then they triggered something. Yeah, I think that's why I said the ones that it was in that are using PSR, they have a more sophisticated update function. I think that is good because we're not very up pages. I think students are so confused because they're not. But I thought it was interesting to understand a page right now. Right. But it's not something I have. Yeah. Yeah, that's one of those. I think I actually love it. I think I actually love teaching a private learning as well. No, no, it's different. It's not as older stuff, but I have less incentives to start by French and they're very pleased. But it's maybe we understand tomorrow. No, no, I I am all for it. I just kind of want to say to people okay. To say to people, 20 minutes. I think sometimes people add slides or additional topics just to feel safe. No, I want to have something. And you think it's like she's like playing music to like ramp up the music and like yes, you just energize and like I'm just like oh that's and we think you're putting energy doing a proof. Like doing a proof plan joke and having a discussion already proof plan, like, could make me do a good job. As soon as we check professors, like, I mean, well there's they do that. Like, you know, they're teaching two courses for semesters. I'm like, you know, their life work is to like the teaching method of next course. But I guess students know that 1130 to 20. Yeah, I mean, like. Makes sense. Yeah, I I get it. Um yeah. I I think that the thing that you know we have planned now is that we have to go to different plan. I'm not sure if they realize the difference in terms of the position what they saw. But the they sort of like you know, in the comments they give them that they talk about the other So I haven't taught this course enough to be talked about some of it. Oh yeah, don't you think of the accident? I'm not yet. But like, for example, that would get talked. And the thing is that they also have an account themselves that they're talking about. Like Jonathan Rose is like the. Like nothing in the world is like for those people or like yeah, everybody's a little god. They're celebrities as we have I mean every semester is teaching more than a thousand students Rich and like yeah so they do change so um yeah I did have this yeah so I am I did an internship with Rich last year and since then So, I'm currently really interested in teaching professors. And so, we can talk more about. Well, for the first time, I actually understood this. I think we give this talk to sort of like have a useful notion of generalization. I've sort of been really interested in sort of the ones. Does it even matter if tests are direct training? Like good interest. But these aren't so many things. And the people are just allowed to test that. Yeah. Yeah. So they can set it up. Yeah, there also. Macros are sure they're sort of advanced. So that's why I'm not sure. Like, I don't know how to bother, that's not going to be rough. No, I was saying, like, it's just, we were talking about if the student can make airlocks. Yeah. I always want to have a paper. I mean, we have to do it. Development like really solid foundations. I think this is what we want to do now. Yeah, I'm still interested in this, of course, a lot of it's a bit of like those topics that we discussed. But it's like the longer my most likely. I was telling you about, I never imagined you guys. I've also said that we exactly have to be a little bit more difficult. I'm not sure if I'm doing this right. Okay, not hard, but it's Jason. Yeah, I think we would be close. Well, I'm not enjoying this. We're not sure. I really also want to like, I really want to be. Yeah, uh, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, no, no, no, let's do it, you're young. No, I had a lot of fun. But there were like a lot of other spaces where you had like