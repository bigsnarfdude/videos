Good afternoon, everyone. It was my great pleasure to speak at the Banff International Research Station to talk about my recent research. First, I want to thank the organizers, Professor David Banks, Nancy Heckman, and Nancy Reed for putting up such a wonderful workshop. I have to admit that I don't have extensive experience with computational advertising, and my research has been focused on high-dimensional statistics and High-dimensional statistics and tensor data analysis. However, I found that the computational advertising data are often high-dimensional and can be tensorized. To illustrate this point, let's look at this user online click-through data set. And this data set is from Taobao.com, which is one of the largest online shopping platforms in China. And this data can be formatted into a tensor, which To a tensor, which includes about 10 to 6 number of users and 10 to 4 items. And these measurements are taken over a certain period of time. In addition to this data set, I also saw two other talks from our workshop on high-dimensional tensors. They are by Professor George Michalidis and Professor Xuanbi, respectively. And this all illustrates that the high-dimensional tensors. The high-dimensional tensors will play a very important role in computational advertising. I also want to mention that one key task in computational advertising is to perform collaborative filtering from users' behavioral data. Here, I want to quote one sentence from Professor David Bank's opening remark on Monday. Clustering analysis and high-dimensional scaling should drive market success. Scaling should drive market segmentation and collaborative filtering. Putting all these ingredients together, I think we are motivated, are very motivated to study the clustering analysis for the high-dimensional data. And that is the focus of my talk today. The clustering data, the clustering analysis itself is not new. It's the basic problem in unsupervised learning. Learning. And clustering analysis aims to discover homogeneous patterns in a data set. Usually, the observations are assumed to be the multivariate vectors, and each vector comes from an unknown sort of population. For example, we have a data cloud like this. By performing a clustering analysis, we identify three clusters: green, blue, and red, respectively. And the classic algorithm for clustering analysis includes Clustering analysis includes like k-means, hierarchical clustering, spectral clustering, and so on and so forth. And more recently, we actually found another series of problems where the cluster appears as the product of several, say, marginal clusters. For example, in the single-cell RNA sequencing data, like this data table here, the rows are different genes and the columns are different. And the columns are different cells. We hope to simultaneously aggregate a subset of rows and a subset of columns so that this subset of cells will show elevated expression level on this subset of genes as shown in this heat map plot. And similar co-expression pattern also appears in other settings like a microbiome study, where the three axis represents different individuals, tissues. Different individuals, tissues, and microbiomes. And it also appears in, for example, the online click-through data set. And that's the example we have mentioned at the beginning of this talk. So our tasks today is to do high-dimensional clustering on tensors. That is to say, suppose we observe a tensor data set like this. We hope to say aggregate a subset of indices together. subset of indices together on each way so that the tensor will exhibit approximate blockwise structure like this. In order to systematically investigate this problem, we introduce the following tensor block model and we observe this y is a y tensor is it has d ways so it's order d tensor and s here is the order d is a block cluster mean tensor and z1 up to And Z1 up to Zd here and marginal clustering membership along each way. And to be specific, this ZK, for example, ZKJ equals to 5 means that the J's component along the caseway belongs to the fifth subcommunity. And the epsilon here is the mean zero random noise, which is the uncertainty in this observation. In one sentence, our aim is to estimate the marginal cluster labels Zk up to permutations from this y tensor. We also want to mention that this label zks are only identifiable up to permutation. For example, if we say the subject 1 and 2 belongs to cluster 1, and the subject 3 and 4 belongs to cluster 2, this is equivalent to say subject. is equivalent to say subject one and two belongs to community two and subject three and four belongs to cluster one. So we estimate, we aim to estimate this ZK up to computation. Actually, if we just utilize some tensor algebra and we can play some trick and simplify the notation of this model. And by introducing the tensor matrix product, we can rewrite the observation y. rewrite the observation y as the x plus epsilon, where the epsilon is the noisy tensor and x here admits the target decomposition. That is x equals to s times m1 all the way to md along all modes as illustrated by this picture. Here, this mk here is the membership matrix. It's a binary matrix only taking 0, 1 value. And this m i j equals And this m i j equals to 1 if and only if z k i equals to j, that is the ice component along the case mode and belongs to the j's cluster. Let's have a quick summary of the vector and matrix and tensor clustering models in this slide. When the order d is equal to one, our proposal model reduces to this univariate mixture model. When the order d is two, the proposal Order D is to the proposed model and reduced to this bipartite stochastic block model. And these two settings can be written basically from the basic matrix algebra. But the general D greater than or equal to three cases, that's a tensor block model, can be seen as the high-order extensions from these two previous problems. Okay, now the question is: how can we solve this problem? question is how can we solve this problem? How can we estimate the membership of the labels Zk from y? And before we go into details of our proposal, let's have a quick review of the classic algorithm, k-means. And k-means aim to solve the following optimization problem. Here, the optimization problem includes two arguments. The mu is the clustering centroid and the Z hat is the clustering label. Hat is the cluster label. The one classic solution to this problem is the Lloyd algorithm, which updates the clustering centroid and the clusting labels iteratively and alternatively using these two formulas. And specifically, for a given estimate of classing label, we basically average all samples belong to the same cluster and take the average as a new centroid. As a new centroid. And when the new centroids are given, then we allocate each of the data points to the centroid that is the closest to this data point. And then by doing that, we form the new clusters. This Lloyd algorithm has been quite popular in the machine learning community. Recently, the statistical optimality for Lloyd algorithm with special clustering has been And has been especially under the Gaussian mixture model, was established by this paper by Yulu and Harry Zhou in 2016. Now let's move back to the high-order clustering problem under the tensor block model we have mentioned at the beginning. A straightforward idea to this problem is to solve this constraint at least squares problem. And unfortunately, it's a mixed integer program. It's really hard to solve. It's really hard to solve. In the worst case, it is actually an MP-hard problem. So, alternatively, we want to introduce the high-order Lloyd algorithm with the initialization of the high-order spectral clustering. And we will use the next few slides to introduce these algorithms. Okay, so we want to first discuss the high-order load algorithm. Let's take a look at the optimization, say, objective function again. say objective function again and this objective function includes d plus one arguments s z1 up to zd we shall notice that when d out of d plus one arguments are fixed then the other one can be straightforwardly optimized and to be specific and for each of the iteration t equal to zero up to capital t we propose to update the the block mean and Update the block mean using this average formula and then we propose to update the clustering membership along each say mode using this nearest say neighbor search. So that's the high-order Lloyd algorithm. And then we want to introduce say high-order spectral classroom. hyoder spectral clustering and it's a very important initialization algorithm as the higher the loyal algorithm requires initialization and this initialization will play a very important role and to the final performance of the overall procedure. And to make the notation simplified, and we only focus on the order three tensor from now on, and recall that the noisy tensor has the zero mean, so the expectation Zero mean so the expectation of the observation y will satisfy this identity. Now, if we play some algebraic trick and by matrix this y tensor into a matrix, actually this identity holds. And here this m3 or mk is the Mo K matrixization operator illustrated by this picture. If we have this tensor A, it's a three-way tensor, so there are three ways we can cut this tensor into fibers. Into the fibers, and then we can align these fibers into a very C matrix, and we name them as M1A, N2A, and M3A. And because we have this formula and this M3 is the point of interest in our analysis, so we propose to the following high-order spectral classroom procedure. So, we first met this Y, then evaluate. This y, then evaluate the S V D and extract the top say singular vectors and form them as the utility k. But that's not sufficient. We further propose to project this white tensor onto this space spanned by this U1 tilde and U2 tilde, and do the matrixization again and calculate the S V D again and obtain this U hat 3. this u hat 3 and u hat 1 2 can be obtained similarly and finally we propose to run k min plus plus on this matrix and obtain the initialization dc0 and here we draw a picture to illustrate this procedure and we start with the y tensor and after maturization we obtain a very thin matrix and we calculate the left say the leading left singular vector leading left singular vectors of this Y3 and obtain this U3 tilde. Now, so imagine so there are two communities, blue and red, and along this mode. And then we can just draw a scatter plot for this U3 tilde where each row of the U3 tilde corresponds to one point in this scatter plot. And if this U3 If this U32 includes three two communities, blue and red, and we can also make them make them the color of the points as well. We can see that after the first SVD, these two data clouds, these two communities are not well separated. That's why we propose to use this U1, U3, U2, U3 tilde and do another projection. Another projection and make it say a small bin, like then matrix like this, and calculate another SVD. And after we have, then we can do this plot again. Now we can see that the within cluster variance has been significantly reduced, but that's still not sufficient. So that's why we propose to perform a third assay dimensional reduction and multiply. A dimensional reduction and then multiply this y tensor by u1 hat and u2 hat as well as u3 hat. Then we'll obtain this bin and do the materialization. And now these two clusters will separate and it's easy to make a clear cut between them. Okay, so that's the procedure for hyaloid and hyalospectral clustering. And so now let's talk about the So, now let's talk about the theoretical guarantees for the procedure. We want to first introduce some protocols for our theoretical analysis. And suppose the z here is a true membership of interest, and we estimate them by the z hat. And we define the misclassification rate in this formula. And here, this pi is the collection of all permutations of the clustering labels because our estimation is only Is only identifiable after permutations. And we will say this Z hat is consistent if this misclassification rate will converges to zero in probability. We will say that this Z had achieved exact recovery if with high probability that misclassification rate will be exact zero. Before we talk about the theory and let's talk about any condition, we would want to discuss the sufficient necessary conditions for successful reclustering. Intuitively speaking, this high-order problem becomes easier if we have significant cluster separations or smaller noise level. And to be more specific, we define this delta k as the molecule slice separation. That is to say, Separation. That is to say, for this tensor block mean S, we take any of the two slices and take the pairwise difference. And we take a minimum of this all delta S K squares. And we also want to point that this delta mean square is greater than zero is a necessary condition to guarantee model identifiability. Otherwise, if deltman equals to zero, it means that there are two clusters. means that there are two clusters are identical or have the same say the cluster mean structure. And if that's the case, there's no way we can tell these two clusters apart. And we also want to say introduce the noise level sigma square by assuming that each noise entry epsilon are independent on sub-Gaussian and mean zero on the variance sigma square distributed. By introducing the delta and the variance, we can Introducing the delta and the variance, we can define the signal-to-noise ratio as this rate. Now, here we come to the main theory of our results. And for simplicity, we assume that all the tensor dimensions P1 up to Pd are of the same order, and the rank is a constant. And this ZK0 be the initialization from the Hayoda spectral classroom algorithm. And we assume the signal-to-noise ratio satisfying. The signal-to-noise ratio set by this lower limit, then with high probability, we can prove that this initialization is consistent. And then in theorem two, we can prove that if the initialization cluster label zk0 are consistent and the SNR is greater than or equal to this threshold, then with high probability, and this zt, after the most logarithmic number of iterations, Number of iterations, we achieve the exact recovery. So now we can combine the theorem one and theorem two together and get the final set conclusion. Sufficient conditions to achieve exact clustering by how the Lloyd algorithm with how the spectral clustering initialization is this. The question next is, is this is an arc condition necessary? And to answer that question, And to answer that question, let's take a look at the statistical lower bound or statistical limit of this problem. We focus on the following parameter space so that the least separation is at least delta k. We can prove that if the signal to noise ratio is below p to the minus d minus one, then no matter regardless of what kind of algorithm we take, the exact recovery. The exact recovery cannot be achieved. So, by doing by because of the statistical lower bound, we actually show that it is a necessary condition to have the instant R greater than or equal to this threshold. Now, let's summarize the theoretical result from the previous two slides for both the matrix case and the tensor D greater than or equal to three cases. Cases. Now, the x-axis corresponds to the dimension and the y-axis corresponds to the signal-to-noise ratio. We found that there are, say, when the signal-to-noise ratio is in this region, and our proposed algorithm can achieve the exact recovery with high probability. When the SMR is in this B region, and there's no way to recover it, and this problem is basically statistically. Problem is basically statistically impossible to solve. So we have a complete characterization because the A and B share the same boundary. However, if we move on to D greater than equal to three cases, there is a significant gap between these two thresholds, so that we have another region C here. We still need to further characterize what's going on in this gap region C. So then we further consider. So then we further consider the MLE estimator, which is the exact optimizer of this optimization problem. So actually, it was shown that as long as the signal-to-noise ratio is greater than or equal to p minus d minus one log p, then the MLE will achieve exact clustering, as the misclassification rate will be zero with hyperbiting. On the other hand, the On the other hand, the exact computation of MLE is infeasible. It's a bit hard to calculate, although it's statistical optimal, but it cannot be implemented in practice. So this will raise a very natural question. If we only restrict to polynomial time algorithms, what will happen? To answer this question, we want to introduce a computational lower bound result. Lower bound result. It turns out that the tensor-cross-wing problem can be reduced to a hypergraphic planning click HPC detection problem. And this problem was introduced and investigated by a series researchers, and it was widely conjectured to be computationally difficult. So assume the HPC detection conjecture holds, and the signal to noise ratio is much less than P to minus. ratio is much less than p to minus d over 2 we can say formally show that for all say polynomial time estimators will not be able to achieve exact recovery so summarizing all the previous theoretical results when we have a comprehensive characterizations of the computational statistical trade-off for this high-order tensor clustering problem A clustering problem. So we identify three regions A, B, and C when the signal-to-noise ratio is strong and it's A in A region, then the exact class ring can be achieved by efficient algorithm, which is the proposed hyodeloid plus the hyoden spectral class ring. When the signal to noise ratio region is, when the signal to noise ratio is weak, and we are in this region B here, then this problem is a Here, then this problem is statistically impossible. When the SNR region is moderate, so whether we are in the region C here, and this problem is statistically possible, but a computational impossible. Okay, so here is the theoretical result. And we further implement some numerical experiments to verify the theoretical finding. And we consider both the And we consider both the matrix clustering and the tensor, or the three tensor clustering settings. And this y-axis are clustering error rate, and the x-axis corresponds to the signal-to-noise ratio. And we implement both the Howard-Lloyd algorithm, that's a computational efficient algorithm, as well as the oracle estimator, and that's the computational inefficient algorithm in practice. We can see that in the matrix setting, this computational. Setting, this computational efficient and inefficient algorithm have a similar, very similar performance. However, these two sets of algorithms have a clear gap when the order of this tensor is greater than equal to three. And this just matches the theoretical finding we have in this slide. And finally, I want to have a quick illustration on this online click-through prediction. This online click-through prediction data set. Again, this data is for the user's online click-through behavior from taoba.com. We hope to perform higher clusterings on this data tensor. For simplicity and the computational issue, we focus on the most active 100 users and the most popular 50 item categories. And by that means, we can construct eight. means we can construct eight tensor ym and this tensor is of dimension of 100 by 50 by 24 and it's binary and this ym ijk is equal to one if and only if the ice user makes a click on the js item in the case hour and in the m state by taking now we take average of yi say over these eight days and apply Over these eight days, and apply higher.lloid and with spectral clustering on Y. Here is the outcome. We have identified four time clusters from 12 a.m. to 6 a.m. That's the early morning. From 6 a.m. to 6 p.m., that's the working hours. From 6 p.m. to 9 p.m., and the early evening, and from 9 p.m. to Evening and from 9 p.m. to 12 a.m. That's late night. Recall that we only impute this Y, we only say, say, put this Y into this model, and we didn't use any information about time. This algorithm outputs the three four time clusters, which are making a lot of sense. And also, we further consider the user. And also, we further consider the user-item interactions on these different time points. And here is the outcome heat maps. And we can clearly see that the user-item interactions are very different among these four different time periods. Now, let's have a quick summary of what we have got so far. And today, we discussed the high-dimensional high-order clustering and we propose algorithms. And we propose algorithm high-elder load plus the high-elder spectral clustering for initialization. And then we introduce the theory for the proposed algorithm and we have identified the statistical and computational limits and find out this phase transition in this high-dimensional high-end cross-meaning problem. And finally, we briefly discuss the implication of the proposal. Discuss the implication of the proposed procedure to online click-through data. In the future, we also want to discuss some other problem settings that the high-dimensional high-order clustering can be useful. This example includes the multi-layer social network and applications in sequencing data. And here is the main ad reference of my talk today. And this is a joint work with my wonderful PhD student. With my wonderful PhD students, Wengang Han and Yu Ting Wo, as well as my colleague Myo Ying Wang. That's all I want to talk for today, and thank you very much for your attention.