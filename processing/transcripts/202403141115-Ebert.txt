Like Jean-François said, my name is James Nieberg and today I'll be presenting some work we did at Texas A ⁇ M with Dr. Nadian and Dr. Chamber. So before jumping into the details of my talk, I would like to provide a brief outline of what I'll be talking about today. So I'm going to start by introducing the concept of a sparse regression LDPC or sR-LDPC code. After that, I'll talk about how we can extend sR-LDPC codes from the single user setting to the A single user setting to the multi-user setting by leveraging techniques from coded demixing. And the resultant scheme, which we call multi-user sRL DPC codes, is a novel non-orthogonal multiple access scheme, which we find very interesting. After presenting the details of this multi-user sRLDPC scheme, we'll talk about how these codes can be applied to cell-free settings. And if we have time, we'll talk about how in these cell-free settings, you'll often In these self-resettings, you often have timing asynchrony between users on the uplink, and we can talk about how the decoding of multi-user SR LDPC codes can be modified slightly to accommodate that timing asynchrony. So with that, let's go ahead and get started. Sparse regression LDPC codes are concatenated codes. They consist of a sparse regression code and a non-binary LDPC code. So before I discuss the concatenated structure, I wanted to present to you. Structure, I wanted to present a one-slide review on both of the component codes of sparse regression codes and non-binary LDB basic codes. So, sparse regression codes are a class of error correcting codes that leverage techniques from compressed sensing. So, the idea is quite simple. You can take an information message, you can chop it up into chunks, you can take each chunk, and you can pass it through a one-hot encoder to obtain a one-sparse vector. Then you take the one-sparse vectors corresponding to each chunk. The one sparse vectors corresponding to each chunk, you stack them on top of each other to obtain a block sparse vector, which you can pre-multiply by a sensing matrix to obtain your code word. And after you send this code word through an AWGN channel, you can decode by performing noisy support recovery, which has been well studied within the field of compressed sensing. And specifically, it's known that approximate message passing techniques work really well for decoding sparse regression codes. And the final thing I'll say about these codes is that. thing I'll say about these codes is that under certain conditions they can be asymptotically capacity achieving. However their performance does suffer at high end block levels. That's one of the codes that we'll use today. The second code is a non-binary LDPC or a FQ LDPC code. So like their binary counterparts, FQ LDPC codes are defined over bipartite factor graphs. We have variable nodes on the left, you have to check nodes on the right. Each variable node represents a coded symbol. node represents a coded symbol from the field FQ, and each check node enforces a parity constraint. For example, the finite field sum of the connected variable nodes may be constrained to be equal to zero. And like binary LDPC codes, we can decode these codes using belief propagation. However, instead of passing scalar LLRs as messages along the edges of our factor graph, we will pass Q-dimensional probability vectors because this is over Q. So we know a little bit about sparse regression codes, we know a little bit about FQ LDPC codes. How do we put them together? And it's quite simple. So a sparse regression LDPC code consists of an inner sparse regression code and an outer non-binary LDPC code where the field size of the LDPC code is exactly equal to the section size of the sparse regression code. Sparse regression curve. So, what does that mean? That means that each section in this block sparse vector represents exactly one coded symbol. And due to the structure of the underlying LDPC code, these sections are connected together by that LDPC factor. So this vector has a lot of structure that we'll seek to exploit during decoding. So specifically, we know that it's block sparse, and we know that these sections are connected together by the LDBC. By the algorithms. But before we talk about decoding, let's review the encoding operation really quick. So you can start with a binary information message. You pass it through an off-the-shelf FQ LDPC encoder to obtain your non-binary LDPC code word. Then you take each coded symbol. You pass that through a one-hot encoder to obtain a one-sparse vector. You stack the one-sparse vectors corresponding to each coded symbol together to obtain one tall block sparse vector. Block sparse vector, and then you pre-multiply that by the sensing matrix A to obtain your code word. One thing that I do want to point out right now is that the sparse regression LDBC code word is a sparse linear combination of the columns of A. And in theory, the entries of A are IID Gaussian. So each entry of your sparse regression LDPC code word is a Gaussian random variable. And as we'll show, this enables us. And as we'll show, this enables us to very naturally exploit shaping gains over the EWGM channel. We'll talk about that a little bit more. So that's the encoding operation. Perhaps unsurprisingly, our decoding algorithm will be rooted in approximate message passing, or AMP, which has been discussed at length this week. So the vector that we wish to recover is the vector S. So let us do. is the vector s. So let us denote s of t as our estimate of s at iteration t. So our AMP algorithm proceeds by first computing a residual error, z of t, under our current estimate, s of t, enhanced with an on-singer correction chip. After we compute our residual error, we will then compute our effective observation r of t, which we pass through a denoising function eta of t to obtain our new state estimate s of t plus 1. S of t plus 1. And AMP has some very interesting mathematical properties. So under certain conditions, this effective observation R of t is asymptotically distributed as the true state S embedded in high ID Gaussian noise with mean zero and deterministic variance. So if you think about it, the effective channel between this vector S and our effective observation is just a vector Gaussian channel. So an equivalent way to think about it would be that you send To think about it would be that you sent your non-binary LDPC code word over an AWGN channel using pulse position modulation. And we will exploit that structure in the design of our denoising. So the purpose of this denoising function, eta of t is to incorporate site information that we have about that vector s into the recovery algorithm. And as I've emphasized quite a bit, we have Emphasized quite a bit. We have two main structures. We know the S is block sparse, and we know that the sections are connected together via the LDP signal. So, how do we design a denoiser that will exploit both of those structures? So, we may be tempted to first turn to the MMSC denoiser, which would provide really good performance. However, in part due to the presence of that LDPC code, this MMSC denoiser is computationally intractable. It's just not going to work. We could also consider using We could also consider using a section-wise MMSE denoiser. However, the section-wise MMSE denoiser will not account for the structure of that outer LTD sequence. So instead, what we propose to do is use something that we call the BP denoiser. And what the BP denoiser does is it takes that effective observation and it uses that to initialize the local observations on the LDPC factor graph. Then we'll run a few rounds of wave propagation on that graph, and we will take the soft output. And we will take the soft outputs of the BP algorithm as our new state estimate. So, in some sense, this BP denoiser will trade off performance and complexity by controlling the number of iterations we perform. It'll estimate that vector S based on the observations contained within the computation tree of the algorithm. So, this algorithm, so AMP with a BPT noiser, we often refer to as the AMP. We often refer to it as the AMP BP outcurve. And just to highlight the structure, I will present it one more time, but this time graphically. So we start with our received signal Y. With Y, we'll compute our residual error Z of T, which we then use to compute the effect of observation. The observation is used to initialize the factor graph. Then we perform an arbitrary number of leaf propagation rounds there. Then we take the soft outputs, which become the new state estimate, and the processor. New state estimate, and the process repeats itself. And this algorithm has some interesting properties that we've been able to show, but in the interest of time, I'm not going to cover those today. If you're interested, we can talk about those after. But let's talk a little bit about the performance of these sRL DPC fields. So here, we've plotted bit error rate on the y-axis and EB over N0 on the X axis, and we've plotted the And we've plotted the performance of RE, about 0.8 SR LDPC code in RET. And we're comparing this to a couple of different benchmarks. The first benchmark is this green curve. This is another SPART plus LDPC construction that was introduced by Greg and Ben Kataramanan. And you can see that our sparse regression LDPC code provides significant performance gains over that alternate, highly optimized construction. The second benchmark we'll use is this blue curve. Second benchmark we'll use is this blue curve. This is BPSK with a 5G NR LDPC code. So, this comparison we have to be careful with. So, at rate 0.8, maybe I'll back up. Recall that the channel inputs of a sparse regression LDPC code are Gaussian. And at rate 0.8, there is a gap between the capacity of an unconstrained Gaussian channel. Unconstrained Gaussian channel and the capacity of the Gaussian channel constrained at binary endpoints. So, the comparison between the red curve and the blue curve isn't quite fair, but it's still interesting. We believe that the performance improvement is in part due to this exploitation of shaping aids. The final comparison that we'll do is this black curve. This is a bit interleaf-coded modulation scheme using 4-PAM and 5-GNR LDPC codes. GNR LDPC codes. At rate 0.8, the gap between the unconstrained and constrained capacities is very small. So this comparison is what we forward. And you can see that we still outperform that black curve. So this is exciting. sRLDPC codes are interesting. They seem to perform well in the single user case. But the next question we want to ask is, can we take single user sRL DPC codes and extend them to the coordinated multi-user case? The coordinated multi-user communication scenario. So that's where we're going to go next. And to understand what we're going to do, I'm going to briefly review the theory of convex demixing. So in convex demixing, your model is y equals A1S1 plus A2S2 plus Z, where Z is Gaussian noise. And your goal is, given A1 and A2, and these noisy measurements, Y, recover. Y, recover S1 and S2. And the theory of convex demixing says that this is possible as long as vectors S1 and S2 are sufficiently sparse and matrices A1 and A2 exhibit low cross coherence, which just means that the maximum cross-correlation of any two columns from A1 and E2 is low. So if these conditions are met, then recovery of S1 and S2 from Y is possible. However, However, the techniques proposed in convex demixing rely on convex optimization routines, which are very computationally costly, especially in high dimensions, which is the setting that we're interested in. That's a fantastic question. Let's talk about that. So that's exactly what we're going to get. So conceptually, we could think about stacking A1 and A2 horizontally as one. As one fat wide matrix. And we could stack S1 and S2 as one tall vector. This model is equivalent, there's no difference here. But now we've reformulated the problem into a canonical compressed sensing formula. And in coded demixing, we propose to use approximate message passing to recover S1 and S2 from 1. And one key observation is that this sparse vector now has two sections, one corresponding to S1. Sections, one corresponding to S1, the other corresponding to S2. And so we can use approximate message passing with parallel denoisers to recover this source vector. So what do I mean by that? I mean we can start with our receive signal y, and we can compute this joint residual error. Once we have the residual error, in parallel, we can compute an effective observation for S1, an effective observation for S2, then we can in parallel do the BPG noising, come up with new state estimates. Come up with new state estimates, stack them together, and then compute the residual error and repeat this process. So, this is what we call coded demixing. This was initially introduced within the context of unsourced random access to support heterogeneous classes of network users. But we're going to use it in this sRLDPC setting as well. And I would like to highlight that this is highly parallelizable. You do have to compute the residual jointly, but most of the work. Jointly, but most of the work on computing the effect of observation and doing VP demoising can be done in parallel, which is an attractive feature for practical implementations. So now that we understand a little bit about coded demixing, how do we do multi-user SRL DPC codes? Well, we envision these codes being used in a coordinated setting. So when a user wishes to utilize the network, they can contact the base station over a control channel. It can be assigned a unique sensing matrix. Then it will do SRL DPC encoding. Then it will do sR-LDC encoding using its assigned sensing matrix in a manner identical to what we've seen. Then the users will simultaneously transmit their sR-LDPDC code words over the Gaussian multiple access channel. The receiver will perform coded demixing to estimate the sent code words, and we'll move on our way. And I want to highlight that this is a non-orthogonal multiple axis scheme, whose complexity scales linearly with the number of Complexity scales linearly with the number of each of the page is how well do these multi-user SR LDPC codes perform? So in this plot, what we seek to do is characterize the bit array performance of our scheme as a function. Of our scheme as a function of the sum rate. So the rate of the code is determined by the number of rows in the Sensi pages. And implicitly, we've been assuming that each user uses the exact same weight. You know, A1 and A2 have the same number of rows. And we're going to continue with that assumption. So the first curve I'll draw your attention to is this orange curve. This orange curve is obtained by having multiple users use normal SR LDPC encoding. Scene coding and like a time division multiple access. So everything's orthogonal. Then in green, we have multi-user sRL DPC codes with two users. In blue, we have with four users, red, we have with eight users. And what we notice is that using multi-user sRLDPC codes can obtain a target bit error rate at a higher sum rate than an orthogonal scale. And furthermore, we note that as the number of users increases, the required sum rate also decreases. And this isn't entirely surprising. And this isn't entirely surprising. We know that orthogonal schemes are not optimal at finite block rates, but this does show a lot of promise for our proposed scheme. So now that we understand a little bit about multi-user sRL DPC codes, we'd like to talk about how these codes can be applied to cell-free settings. So, in a traditional cellular network, what you do is you take it. Network, what you do is you take a geographic area and you will divide it into chunks or cells, and within each cell you'll place a base station. And that base station is responsible for serving all users within the cell. And among other things, there's two phenomenons that we notice in cellular networks. First, due to the physics of electromagnetic propagation, the users who are physically close to the base station often enjoy much stronger channels than the users who are located far away. And the second thing that we notice is that. And the second thing that we notice is that the users who are on the cell edge often already have weaker channels because they're further away from the base station, but they also experience the highest levels of inter-cell interference. And what these two phenomenons lead to is a highly non-uniform quality of service over the geographic area. Users close to the base stations in the center of the cell have much higher quality of service than those on the edges of the cell. Edges in the cell. And to ameliorate this issue, the user-centric cell-free paradigm has been introduced. So, in a cell-free system, what you do is you take your geographic area and you'll distribute access points throughout that area. And adjacent access points can be connected together via a synchrony. Then, when a user wishes to utilize the network, it can connect to the subset of access points for which it has the strongest channels. For which it has the strongest channels. And those access points can work together via this common CPU to decode uplink transmissions and also to be informed downlink transmissions. So how can multi-user SR LDPC codes be applied to these self-resendings? To understand this, let's consider a very simple example. On the left here, I've depicted three users, user 0, user 1, and user 2, and then I have two access points. And then I have two access points denoted by these rectangles. And in this toy example, user 0 and user 1 are connected to access point 0, and user 1 and user 2 are connected to access point 1. Each access point will run a multi-user sRL DPC decoder, and it will compute a residual error. After it computes that residual error, it will compute an effective observation for each connected user. Now, at this point, I wish to remind you that the effective observation is asymptotically distributed as the true state embedded in Gaussian monster. And we have within this network two effective observations corresponding to user one. So if the access points share those effective observations with the CPU, the CPU can do optimal linear combining of those two effective observations to reduce the effective noise variance, which will improve. Noise variance, which will improve the outcome of our BP2. This is the CPU will receive effective observations from the connected access points. It will do maximal ratio combining. It will do BP denoising. Then it will compute the state estimate and will feed that state estimate back to the access points, which will compute their own individual residual errors, and the process will continue. So let's consider. Let's consider another example. This time, we'll present some simulation results where we have three access points and we have seven UEs distributed across the geographical area. And note that the set of users connected to a single access point must employ unique sensing matrices. However, these sensing matrices can be reused across space. We can see here that this purple sensing matrices are. This purple sensing matrix can be used by user 2 and user 1 and user 0 because there's no overlap there. And in this setting, we know that the neighboring access points are connected via the CPU and they're going to work together to decode the outcome transmissions. So in this plot, we plot the bit error rate as a function of EB over N0. Here, the black curve corresponds to the performance of a single user. This is our benchmark. Single user, this is our benchmark. Then the red curve corresponds to the performance of those users connected to a single access point. The green curve corresponds to the performance of users connected to two access points. And the blue curve corresponds to the performance of users connected to three access points. So you can see, as you would expect, that as the number of access points you're connected to increases, your performance is also going to. We find this very interesting. We find this very interesting that multi-user SRL DPC codes can be applied so naturally to self-respect. Now, the setting that I've discussed thus far is somewhat idealized. We've assumed that there's perfect synchronicity on the uplink between users and between access points. But in reality, the users are going to be talking to multiple access points that are at different distances. There's going to be some timing async there. And it turns out that with a very simple modification of A very simple modification of the decoder, we can handle this asynchronicity quite easily. And the modifications that we propose are to take our received signal Y. First, I should say, let's assume, for the sake of argument, that we have a black box that will tell us the timing offsets. We have some sort of timing offset estimator. We're going to assume that it tells us the timing offsets perfectly. us the timing offsets perfectly. What we can do is we can take our received signal and we can upsample it such that each timing offset is equal to an integer number of samples. Get our residual error and then what we can do is offset match filtering, one for each of the users, so that we can get the proper timing per user. Then we can compute our effective observation, we can pass it through the VPD noiser, we can compute the state estimate, then we can Compute the state estimate, then we can reconstruct that original signal with the proper offset, combine them back together, and compute the residual error. And then this process can work. And when we do that, we get the following performance. So here in black, again, we have the single user benchmark. The dashed lines are the lines that I showed two or three slides ago. We've already seen those. And then the solid lines correspond to the exact same scenario. Correspond to the exact same scenario, this time with random timing offsets between like users. You can see that especially for those users that are connected to multiple access points, having a certain level of asynchronicity actually improves performance a little bit. And this is a fairly minor modification to the delivery as well. This is quite interesting. So, with that, I will go ahead and conclude. I think my main takeaway point is. I think my main takeaway point is we've studied this Spark and non-binary LGPC concatenated structure for quite some time now under AMP DPD coding. We've looked at it within the context of massive on-source random access. This is the CCS AMP algorithm. We've looked at it within the context of single user error correction. This is SRL DPC codes. We've looked at it within the context of coordinated non-orthogonal multiple access. We've also looked at it in the context of self-free systems with timing asynchrony. Systems with timing asynchronicity. And in each scenario, it seems to perform well. And it's very interesting to us how adaptable this specific structure is, how it can be applied to so many different scenarios with good performance. So with that, I would like to thank you for your attention, and I'm happy to answer any questions you might have.