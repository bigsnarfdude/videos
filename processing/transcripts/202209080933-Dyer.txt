Well, I'm just finishing a PhD in maths at Oxford. I'm going to talk about power signatures and simulation-based inference. And I'm going to try to convince you that power signatures slot in quite naturally to some of these approaches to simulation-based inference. And this is another application area that signatures could have some impact, I think. So I'll start with some context on the problem that we're considering. There we go. Problem that we're considering. And the setup is that we've got a stochastic model. We've got a stochastic model of some dynamical process, and you've got this implemented as a simulation model that tries to mimic the data generating process for the system that you're modeling. So you've got a computer program that has random number generators internally creating synthetic data for a system that you're interested in modeling. And this sort of thing happens all over the place with epidemic spreads in epidemiology or modeling financial contagion in economic networks, for example. Economic networks, for example, and just for the purpose of setting up some notation, you can represent a forward simulation of the model as a draw from the model's likelihood function, p of x given theta, where x is a time series and theta are the three parameters of the model. And to use these models in the real world, we need to be able to fit or calibrate or estimate parameters for the model, having observed some real data. Having observed some real data y from the real world, and this is you know a parameter inference problem. So, a natural approach to doing so would be to make use of likelihood-based methods, for example, Bayesian inference, where you write down a prior identity, pi, over parameters theta, that captures your initial beliefs regarding appropriate parameter values. And then you use Bayes' theorem to obtain a posterior density pi of theta, given y, which incorporates your updated. given y which incorporates your updated beliefs um so that that it captures your updated beliefs having incorporated data y or information from data why um the problem here though is that the likelihood function for simulation models in general will be intractable because of their complexity and so the question that this gives rise to is how you might perform inference without access to the likelihood function and a possible and often convenient solution And often, a convenient solution to this is to make use of simulation-based inference procedures in which the pertinence of different parameter values are assessed by comparing synthetic simulated data to the observed data in some way or other. And by doing that, lots of different parameter values. So, this problem has generated a lot of interest and research effort in the computational statistics and probabilistic machine learning communities, and two possibilities that have emerged from. And two possibilities that have emerged from all this effort are approximate Bayesian computation, or ABC for short, and that's ratio estimation, or DRE for short. I mentioned these two methods because we've done work on these two methods with signatures. I won't have time to talk about the second one, but I'll talk a bit about ABC in a moment. But just to say that the kind of sub-problem that we've considered when doing this work has been that. Doing this work has been that a lot of these approaches or approaches within these two kinds of broad classes of methods often assume IID data or don't account for the sequential nature of time series data, for example. And so the sub-question is then that we've been considering is how you might extend these methods to sequential data and dynamic stochastic simulation models. And to do so, we've been looking into these apparent signatures. Looking into these path signatures. So, as I said, I'll speak about some work we've done on ABC with path signatures. This is that we've got a preprint out on this, which I'm hoping we'll update very soon and also submit very soon. But so I'll talk about this now. For people that aren't familiar with APC, the idea here is that it introduces this approximation to the likelihood function. To the likelihood function, as the average probability with which synthetic data x simulated at parameter values theta fall within an epsilon ball around the observed data y. And you can get an unnormalized, unbiased estimate of this with this average of indicator functions using capital R drawers from the likelihood function at that parameter value. So, to get a posterior density in this way, So, to get a posterior density in this way, or you know, posterior approximation to your posterior distribution, you might perform this sort of procedure where you propose parameters from some distribution, proposal distribution, for example, the prior density. You then simulate at those parameter values and compute your predefined notion of distance, d, between your synthetic data and your observed data. And on the basis of the value of this distance function, you decide whether. Value of this distance function, you decide whether to accept or reject this parameter value. And then by repeating this a lot of times at lots of different parameter values, you'll get a collection of parameters that you accepted at one point or another. And this will provide you with an approximation to your posterior density shown as the blue distribution down here. As you can imagine, the success of this procedure depends quite critically on the choice of distance. Depends quite critically on the choice of distance function that you introduce on the data space. And the prototypical approach that people take to doing so is to propose or write down some summary statistics, S, of the data and compute possibly weighted Euclidean distances between these two summary reactors. And this can be good if you know, if you happen to know some good summary statistics that are informative of the parameters you're. That are informative of the primacies you're trying to infer. But there's a result called the Pitman-Kuppm-Dubois theorem, which says that finding sort of low-dimensional summary statistics or sufficient statistics of fixed size is not generally possible for stochastic models. So chances are you're going to lose some information when you take this sort of approach and Take this sort of approach, and you'll introduce errors that are difficult to quantify. And in any case, finding summary statistics that describe high-dimensional dynamic sequential data, such as multivariate, irregularly spaced, and even non-Euclidean data, can be difficult. So the punchline is that applying ABC to dynamic stochastic simulation models can be really challenging because it's difficult to answer this question. Because it's difficult to answer this question of what semi-statistics to use in or what distance function to use in ABC. So, for this reason, in our work, we've been with path with our work on ABC with past signatures, we've investigated two different approaches. One is to interpret the signature as a summary statistic and then to use squared distance between the The square distance between the signatures of the sequences that we see, x and y, which can be easily computed with the signature kernel and corresponding kernel trick, K. And then also, since the signature of time and baseman augmented sequences or paths is an injective map, the resulting ABC posterior is asymptotically correct as you reduce this tolerance hyperparameter. Um, tolerance hyperparameter in the ABC posterior to zero. So that's a that's that's one way that we've been using signatures in ABC. Another way is to embed it into this what's called a semi-automatic approach to ABC, which is based on this idea put forward by Finhad and Frangel in 2012 to use, which in which they propose to use the posterior mean as a summary statistic. The reason being that The reason being that the ABC posterior that you get by using that semblance statistic produces an optimal point estimates, for example, in the sense that the ABC posterior mean minimizes the square distance between a point estimate of the actual posterior mean and the actual posterior mean. If that makes sense. So, but the problem here is that the posterior, once again, isn't available. Once again, it isn't available. And so the posterior mean itself isn't available. But this can be estimated by performing this regression task where you simulate a bunch of data parameter pairs from the joint density of the model and then regress the parameters onto, in general, summary statistics of the data that that parameter generates. But again, the problem here is that we don't know what summary statistics to use or how to. Don't know what semi-statistics to use, or how to, you know, be difficult for the performance regression task. And so, another way that signatures fit in quite nicely here is that because of the good function approximation properties that they have, we can use them to performance regression tasks, for example, through using the signature kernel and kernel ridge regression to estimate the posterior mean and perform a semi-automatic ABC. So, that's the second approach that we considered. That we considered. I'll tell you quickly about some of the results that we've seen using these methods. We considered a bunch of experiments where we use some simulation models that people use in benchmarks of ABC procedures. And we compare our signature-based approaches to other baselines that are common and competitive in the. And competitive in the literature, and their ability to recover the ground truth posterior associated with these different simulation models and data sets that they generate. So in this slide, I'm showing the results for what's called the RICA model, which is a population dynamics model and exhibits chaotic dynamics. On the left, I'm plotting box spots for the past seen distance between the The fastest thing distance between the ABC posteriors and the ground truth posterior. In the middle, I'm doing plotting bot spots for the MMD between, once again, the ABC posteriors and the ground truth posterior. And then on the right, I'm plotting bot spots for the squared distance between the ABC posterior means and the ground truth posterior mean. And so you need to call these lower values are better, values to the left are better. Left are better, and the bottom three rows in each case are signature-based methods colored in light blue. And these baseline methods are in the top four rows colored in purple. And as you can see, the signature base methods tend to generate more accurate posteriors than a lot of the competing baselines. Perhaps the only baseline that can really compete with the signature-based methods is this. With the signature-based methods, is this middle row here, the semi-optimatic approach? And the interesting thing to point out here is that, well, the reason that this one does so well is that it makes use of summary statistics proposed by a guy called Simon Woods in 2010, who wrote a paper on this model and did a lot of experiments with different summary statistics and devised a set that led to good inferences with. That led to good inferences for this model. So, this model, this approach here has an unfair advantage in that way. And so, the fact that the signature-based approaches that we propose can meet and in a lot of cases exceed the performance of that approach is really demonstrates that potential usefulness of these signature-based methods. In another example, we consider. In another example, we considered a stochastic epidemic model, which generates irregularly spaced and multivariate sequences of variable length. And in this case, too, we see an advantage over other ABC approaches. I'm only showing here one of these, the Vastasine ABC approach. And we got an advantage over this, but the reason that I omitted the other approaches is because they're all the way over to the right. To the right, so including them in these plots would mean that we wouldn't be able to distinguish the performance of the signature ABC and the Rusting ABC approaches. So this is just to say too that these approaches give good performance in these sort of messier settings with multivariate irregular space and irregularly spaced sequences of variable length. As a final example in this paper, we In this paper, we considered a dynamic graph model. And the reason that we consider this is because, as people like Harold and so on, point out in these papers on signature kernels, the signature kernel can be applied to data evolving in more kind of general topological spaces by choosing an appropriate static kernel in the signature kernel computations. So by choosing So, by choosing, say, a graph kernel, an appropriate graph kernel, you can apply these ABC approach that we're proposing to simulation models that generate, for example, graphs. So maybe in social network modeling, for example. And so we tried out in this case on the left here, these are samples from the prior density, which is a uniform prior over the A uniform prior of the unit square. And on the right, these are samples from the signature ABC posterior. And in both cases, the true parameter values generating the data that we find the posterior for are represented by this red point. As you can see, the signature EBC posterior concentrates quite well around the true values of the parameters, which demonstrates that this That these signature-based approaches can be successfully deployed in these sort of USORP settings, too. So that's really the main things I wanted to say about APC. I won't have time to go into details about this other work we've done, but I'll just summarize it briefly just in case people are interested. I mentioned this other approach called density ratio estimation, which is an alternative approach to simulation. The simulation of trainers. And the idea here was that we were interested in seeing whether signatures could, you know, with their sort of ready-made expressive features for sequential data, were able to generate to produce more efficient density ratio estimators. And the idea here is that you estimate the likelihood to evidence ratio in Bayes' theorem, just to make that clear. So, we were interested in whether signatured ratio estimation could provide advantages over competing density ratio estimation methods when there are significant constraints on the simulation budget. So if you have a very expensive simulator. And long story short, we were able to see advantages using a signature-based approach to density ratio estimation. We had this paper at AI Stats this year. So, I mean, please check it out if you're interested. I mean, please check it out if you're interested in seeing another use case of signatures in simulation-based inference. So I might make up some time for you guys, but just to summarize and conclude, to perform parameter inference for general simulation models, it's often necessary or at least useful to resort to simulation-based inference procedures. This can be challenging for dynamic. This can be challenging for dynamic stochastic simulators because it's difficult to construct appropriate summary statistics or distance measures on sequences of data or time series of different kinds. And so for this reason, we've looked into the use of path signatures for this purpose as a natural way to construct summary statistics data. And so I spoke today about how we've been using them in ABC and how we've been able to get. Able to get improvements on competitive baselines in this setting in this setting. So, thanks for listening and your attention. I'm happy to take questions now or later on by Twitter or email or something. So, yeah, thanks for your attention.