Considering this workshop is on interpretability in AI, I think one obvious question that pops up when anybody talks about explainability, interpretability, transparency, accountability, all of these seemingly synonymous words is what is it really? The biggest elephant in the room seems to be what is explainability really and what is interpretability really. And I think for people who use it, that's the functional side of interpretability or explainability. I think it means different things to different. I think it means different things to different people depending on who is the end user. If it's a business owner, it's about trusting the decisions. It's about an internal auditor or regulator, it's about the fairness of decisions. So explainability or transparency could mean different things to different people depending on who's using it. And on the other side, depending on who's building it, you have various kinds of ways of approaching explainability and interpretability, be it post-hoc explainable, intrinsically interpretable. Intrinsically interpretable, transparency versus reasoning, causal versus correlation, which is something I'm going to talk about a bit more. Global versus local explanations, model agnostic versus model specific, and so on and so forth. The list goes endless at this point in time. So what is explainability really, and how do we really move forward with that? I don't think we still have a clear answer. Fundamentally, for the reason that explainability and interpretability lie at the interface of machines and humans. Machines and humans. When something lies at that human-machine interface, I think the final output depends on a user specification and requirement rather than have a clear black and white specification of what it should be. So I personally do think that we need different ways of explaining models. We need different methods to explain models. And each of them could be valuable to a different end user based on an application context. So, in our group here at IIT Hyderabad India, we've been looking at explainable AI from multiple perspectives to mirror this need itself. We have had some efforts in post-hoc explainability. We've had some efforts in intrinsic interpretability too. Just a moment. Let me probably pull up my pen. It's easy to laser pointer. Okay. So from a post hoc explainability perspective, we had developed a method called GracChem ‚Åá  a few years ago, which is widely used. Ago, which is widely used. It's an extension of GratCam, but this is also widely used by a lot of people around the world for getting visual explanations for CNN models. We extended it to canonical saliency maps for face processing models. And more recently, we proposed a method to do submodular ensembles of attribution methods that could give us sparse saliency maps or attribution maps. On the intrinsic interpretability perspective, we more recently have been looking at this is perhaps merely. This is perhaps mirroring a little bit of what Professor Rudin talked about yesterday. I suppose. How do you develop models that are intrinsically interpretable rather than have a postdoc explainability layer after training deep learning or a machine learning model? But all of these are fundamentally driven by the correlations that a machine learning model or a neural network model learns. We've also been looking at a causal perspective to explainability as to how. To explainability, as to how do you ensure that the input/output attributions or explainability that we come up with are causal in nature? And I'll probably talk about it a bit more as we go forward. We have once again been looking at this both from the post-hoc explainability perspective, as well as how do you intrinsically bake interpretability into your model from a causal perspective. So, I'll talk about both of these over the next, say, 20-25 minutes. We have also been looking. Minutes. We have also been looking at the complementarity of explainability and robustness. I think I'm quite sure all of you must have heard of adversarial robustness, where a small adversarial perturbation makes a huge change in the class label or the confidence of a model. And clearly, a cat, which is small perturbation, which becomes a giraffe, let's say for a model, would have no possibility of sanely explaining both these outcomes. So clearly, there's complementarity between explainable. Really, there's a complementarity between explainability and robustness from this perspective, and that's something that we've also been exploring in the last few years. But I'm not going to talk about all of that here. I'm going to stick to causal perspectives and just talk about two of our efforts. I'll try to stay within the time limit in this context. And our focus is on causation versus correlation in explainability. Typically, when we talk about explainability, we generally try to understand the attribution of an input on output. Of an input or output, right? If it's an image, we're looking at the attribution of an input pixel. If it's a document, we're looking at the attribution of a word, or if it's any tabular data, we're looking at the attribution of an input feature on the prediction outcome. And we know that machine learning models rely on feature correlations. So the question is, can we also find causal relationships that a model has learned as part of learning these input-output relationships? So we all know that correlation. So, we all know that correlation is not causation. There are plenty of examples. There's a typical example that's given where this is a statistics of ice cream sales and a shark attacks in a certain bay area in the world. And if you use one of these as an input variable and the other as the output variable, one would assume that whenever ice cream sales increases, shark attacks would increase. Clearly, this is correlation and not causation. And Julia Pearl, who won this Turing Award for putting all of this. Turing award for putting all of this framework together for causality has this very nice causal hierarchy in some of his recent work where he talks about the first level being association, which tries to estimate the condition probability p of y given x, which is what typical machine learning does. If you go further, you can do what is interventions, where you intervene on a certain attribute x and set it to a specific value, and then see how the outcome would have been. So these are the what-if kind of questions. The what if kind of questions, and so what if I take aspirin? Will I headache be cured? I've not yet taken it, and I don't know what has happened to somebody with my characteristics who's taken it. So, how do I judge what this intervention would result in is what this specific layer of the hierarchy is. And finally, you have counterfactuals, which David previously talked about in the previous talk too, which is about imagining. So, was it X that caused Y? Was it the aspirin that stopped my head? Why was it the aspirin that stopped my headache? Would Kennedy be alive had Oswald not shot him? So, how do we go about all of these counterfactuals is the third level of the causal hierarchy. So, but I'm going to stick to the causal relationships in explainable AI for the rest of this talk. In particular, I'm going to talk about two of our efforts here, and they are complementary. In one of them, we take a trained neural network model and try to find out what causal in What causal input-output attributions did the model learn? For example, if I knew that in a certain medical data set, I know that perhaps a person smoking would have a high risk on, say, for a lung cancer or something like that. This is perhaps established through critical studies over the decades. Over decades. If I trained a neural network model on patient data, would it also capture the same causal relationship in the model or would it rely on the model? In the model, or would it rely on other correlations to make this prediction? Is perhaps the question here. So, this was one of our efforts that was published at ICML a couple of years ago. And the complementary part of this question is this is obviously a postdoc approach where we have a train model and we want to find out what causal attributions the train model has actually learned. On the other hand, we also recently explored how if you had some domain priors, if you knew that smoking If you knew that smoking can have a higher risk on lung cancer or any other such known causal domain priors, how do you ensure that a neural network model that learns based on data respects and maintains these causal relationships? So we have domain knowledge. Neural network models and machine learning models are data-driven. How do you ensure that these kinds of prior knowledge that we get from the domain can be imbibed into a neural network model? Imbibed into a neural network model during training. So, this is an intrinsically interpretable or an anti-hoc explainable kind of an approach. So, this work is currently available on archive, but is under review at a conference. So, let me talk about both of these over the rest of, say, 15 to 20 minutes. So, the first work that I'll talk about is on our work on estimating causal attributions in trained neural network models. So, this work was published in ICML of 2019, a joint work with. 2019, a joint work with students Aditya, Piushi, and Anirban. And to the best of our knowledge, this was the first causal effort for estimating input-output attribution in neural network models. So let's perhaps go into this a bit more. So a lot of work on explainability and interpretability at this time focus on attribution. Attribution essentially is the effect of an input feature on a prediction. An input feature on a prediction function's output. This is inherently a causal question, right? We are intrinsically asking for what is the causal effect of an input feature on the output. That's what we're essentially asking for. Existing attribution methods are largely, say, gradient-based or perturbation-based. And all of them try to see how would perturbing a particular input affect the output, which is not a causal analysis per se. I'll also talk about. Causal analysis, per se. I'll also talk about this as we go in the next couple of slides. And the other popular approach, such as say Lyme and related models, is where you use surrogate models to explain as a postdoc method. And these are once again correlation-based. The main model is correlation-based. The surrogate model is also correlation-based. So, then, how do you capture causality in these kinds of models is the main question that we are trying to tackle. So, our objective in this work was So, our objective in this work was: if you have a trained neural network model, how do you estimate what causal input-output attributions or relationships did it learn? We make an assumption of a reasonably valid setting. We assume that input dimensions are causally independent of each other. In one of our recent works, we've actually removed this assumption, also studied this. But in the work that I'm presenting, we made this assumption that these input dimensions are causally independent of each other. Dimensions are causally independent of each other. You could assume that you applied PCA on your data and you removed your correlations between input dimensions. That could be a fairly valid data setup for this kind of an assumption. And we show how this approach can be used with feed-forward networks as well as non-IID setup such as RNNs, recurrent neural networks. So, fundamentally, our entire framework begins with viewing a neural network. Viewing a neural network as a structural causal model, which is the fundamental framework underlying causality, where we assume that we have a bunch of endogenous variables, a bunch of exogenous variables. F are our causal functions that relate input to output or relate different variables. And finally, PU is the distribution of your exogenous variables. So, first thing that we show is that you can view a feedforward neural network as a Forward neural network as a structural causal model. So, if you view a neural network such as this, let's assume this is the input layer, ABCDF, GHI are your hidden layers, and JKL are your output neurons. You could now write all of this out as a structural causal model, where your endogenous variables are each of your layers L1 to Ln. You have some unknown exogenous variables U1, U2, U3, U4, which in causality literature would be called an. Causality literature would be called unobserved confounders. Then you have some causal functions f1 to fn that connect each layer to the output. That's the attribution function that we typically try to understand. We want to understand what's the effect of a particular variable on one of the output neurons, which is the attribution function that we typically try to estimate in explainability methods. And we show that because the hidden layers are not of interest to us, we are typically trying to understand the Typically, trying to understand the attribution of an input variable on an output variable, we marginalize out all the hidden layer neurons and bring it down to a reduced form where you have your structural causal model, which depends only on the input layer and the output layer. Everything else gets marginalized out. And we call the causal functions in this reduced SCM to be f prime. So f prime is not the gradient, it's actually the causal functions. It's actually the causal functions in this reduced structural causal model. So, these are the causal functions we typically want to estimate from a trained neural network to get the attribution of an input on the output. We show that you could do the same thing with the recurrent neural network. You can also treat a recurrent neural network as a structural causal model where you would just end up having connections between the output at a particular time step to the input at the next time step. But otherwise, you can also write a recurrent neural network. Otherwise, you can also write a recurrent neural network as a structural causal model. Once you do this, typical ways of getting attribution where you use gradients and perturbation methods to find the attribution of input and output, you can view them as special cases of individual causal effect. So individual causal effect can be written as you have a u, you have the output y, where you set xi, which is one of the variables of x, to alpha, minus y. To alpha minus y at u. And you typically set your alpha to be some ui plus epsilon. You had a ui, you set the xi to be ui plus epsilon, which means that's the first principles definition of a gradient, where you change one of your input variables by a small amount, and you set that alpha to a ui plus epsilon. And any of your gradient-based attribution methods becomes a form of individual causal effect. But people have shown that gradients have. People have shown that gradients have are sensitive, cannot give global attributions because gradients could be different in different ranges of an input variable, so on and so forth. So, what we rely on in this work is to estimate the average causal effect of an input variable on the output. The average causal effect, if you had a binary variable, would look somewhat like this. So it's the expectation of y, given that you intervene on a specific input variable. Specific input variable and set it to 1 minus the expectation of the output y, given that you intervene and set the x to 0. That's your binary case. In a continuous variable case, this is a bit more tricky. In this case, this would be the expectation of y, given that you intervene on a specific input variable and set it to alpha, minus some baseline value that you get by setting the xi value to that reference value. So, in our particular case, what we say is that this baseline value Case, what we say is that this baseline value, you could choose to use some domain knowledge to set that baseline value. For example, if you were trying to find, say, the causal effect of blood pressure on the risk of heart attack, we know what a reference blood pressure value is, 120 by 80. You could always use that as your baseline reference value. On the other hand, if you did not know what your baseline reference value for a specific variable is, we simply assume that this can be an expectation over all possible values of xi of Possible values of xi of this interventional expectation. So, we actually late, I'll later talk about how this can be computed efficiently to estimate this average causal effect. So, now the challenge in this kind of a method is, remember, we are talking about a post hoc explainability method, which means we already have a trained neural network model, and we only have to estimate this average causal effect of an input on the output. The challenge is that this interventional expectation is non-trivial to compute. Expectation is non-trivial to compute, can take a lot of time to compute. So, I'm just going to very quickly go over the math, not go into details considering the short time here. But what we end up doing is to write out the causal functions in terms of a Taylor series expansion around the mean value of the input. And we show that if you do this and marginalize over all your other input neurons, this turns out to a form which can be pre-computed using a Be pre-computed using observational means and covariances. So, maybe I'll talk about it if there's more interest at the end of the session. But the just the simple takeaway is to use a Taylor series expansion of the interventional expectation around the mean, and you would get something like this. The second order Taylor series expansion would look somewhat like this. In fact, the first order term here would disappear because the mean of your input layer would be mu. So that first order term. So that first order term here would disappear, and you'll be left only the second order term, which can be computed efficiently because of our assumption that our input neurons don't cause each other. That induces deseparation, which means this entire quantity here, which is an interventional expectation, would become observational means and covariances, which can just be pre-computed from your data and stored even before you train the model. So this simple approach ensures that you can actually compute. Approach ensures that you can actually compute this average causal effect of an input variable on the output variable of a trained neural network using this kind of an approach. So one thing that I suggested is that to compute this baseline value here, remember I talked about this baseline value while computing the average causal effect. To compute that more efficiently, we also introduced something called causal regressors, which is simply a form of Bayesian regression. Simply a form of Bayesian regression that can help compute a value of this interventional expectation using a Bayesian regression kind of an approach based on different intervention values. This is more a hack to make this simpler and efficient rather than be a part of the core methodology itself. We also show that this is something that can be done with a recurrent neural network architecture too. The one last challenge with this approach is that in the second order expansion of the Taylor series, there does turn out to become the taylor series there does turn out to become a hess there does turn out to be a hessian here and the hessian we know can be difficult to compute for a neural network model but what we show is that using some simple linear algebra jugglery you can estimate this quantity here using three forward passes of the neural network right so it's not too difficult to do that through a simple derivation we show that this hessian vector product that we end up with in this particular term can be computed as Can be computed as this particular quantity here, which is nothing but three forward passes through your neural network, which can directly help us estimate this quantity. That makes this entire approach efficient and effective. And using this approach, we run a bunch of experiments. This is one such experiment we run on IRIS data set and we measure the average causal effect of different variables, sepal length, sepal width, petal length, petal width on different classes. And we see here. Classes and we see here that the petal length and petal width for low values have high causal effect in the iris setosa class, for moderate values have high causal effect on iris versicolor, and for high values end up having high causal effect on iris virginica. And this seems to, if you build a decision tree independently for a data set such as iris, we actually observe that the causal effects that we obtain mirror. Effects that we obtain mirror what a decision tree learns in this particular scenario to divide the data set and come up with the final outcome. So, this was just a way of evaluating that the causal effects that we were learning was actually practically useful. We also studied this on a practical aircraft data set. This was in collaboration with Honeywell, and where we tried to study the causal effect of learning an LSTM model for anomalous landing prediction. Anomalous landing prediction of an aircraft. And we found in this particular case certain variables such as the pitch and the roll to be causally responsible for the anomalous outcome of a bad landing. This was an LSTM model where the x-axis is the last 20 seconds of landing, if I'm right. And these are the different variables. And the blue and red indicate the value of the causal, of the average causal effect that they come up with. And what we found was that the value that we get actually matched the flight. Actually, matched the flight data recorder report to in this particular scenario. So, we also show that this kind of an approach holds all the axioms, typical axioms of attribution that is proposed in literature. For more details, this paper is available on archive. The code is also publicly available if you'd like to take a look. In the brief time that's left, I'll also talk about the converse of this problem, which I think is perhaps even more interesting, which is Even more interesting, which is if you had some domain knowledge which is causal, how do you ensure that the model that we develop respects this causal domain knowledge? Right? So this is a work that we call causal irregularization with domain priors. This is a work under review at this point. It is publicly available on archive too if you're interested. Once again, to the best of our knowledge, this is perhaps the first effort to integrate causal domain knowledge for attributions in neural networks. We are trying to Neural networks. We'll try to very briefly explain our overall goal here. So, overall goal is something like this. So, if you had a causal domain prior that you know, for example, you could say that you know that a certain drug has a monotonic effect on the output or a U-shaped effect on the output, which means you take the drug to a certain extent, it's going to be beneficial. And beyond a certain point, if you overdose, it's actually going to be harmful. Or you could probably have some domain knowledge. Or you could probably have some domain knowledge that increasing SAT score is going to increase the possibility of admission for college grad students, right? So, undergrad students. So, these are some known domain knowledge priors that we typically have. So, if you're learning machine learning models for such tasks, how do you ensure that the machine learning models respect these kinds of domain priors? Causal domain priors is what we try to study in this particular work. So, we call this causal regularization, as I just mentioned. As I just mentioned. So we assume that if you have a causal graph such as this, right, where you have x1, x2, x3 are your input variables, y is your original output variable, y hat is the predicted output variable of the neural network. So typically, this blue arrows here may represent the true causal information that you have in your data set. A neural network typically learns these red arrows, right? So because by the inherent architecture of a neural network, Inherent architecture of a neural network, you typically learn this relationship between X1 to Y, X2 to Y, and X3 to Y. And that's what most explainability methods study also as attribution. So in this particular work, we try to regularize for three kinds of causal effects in neural network models, control direct effect, natural direct effect, and total causal effect. I'll probably explain one of them given the limited time, and I'll stop with that for questions. I'll talk about controlled direct effect. I'll talk about controlled direct effect. This definition comes from Judia Pearl's work. Controlled direct effect measures the causal effect of a treatment T at an intervention T when all parents except that particular variable are intervened to predefined control values alpha. It's a very simple definition. I mean, although it may look complicated here, the simple definition is if you set the parents of the output variable to a specific value. Output variable to a specific value in both this case and this case, and you varied only the attribute that you're looking for from t to t star, how does the output change? So that's controlled direct effect. Similarly, you could define natural direct effect and total effect, but for that, you need to know the causal graph to be able to estimate this. What we show in our work is that control direct effect is identifiable for a neural network, and you could regularize a neural network. Could regularize a neural network to maintain such control direct effect by simply matching the gradient of the neural network model to the gradient of the domain prior that you already have with you. So if I know that a variable has a U-shaped curve, if I ensured that the Jacobian of the neural network matched the gradient of the domain prior, I can prove that I actually maintain the causal effect inside the neural network. Inside the neural network. Be sure you can do similar things for other causal effects too in our work. Our algorithm is very simple. Whatever loss function you're using to train the neural network, you simply add one loss term that ensures that the gradient of your Jacobian, which is given by this, matches the gradient of your domain prior. That's all you need to do while you train your neural network. And you can show that that actually matches the causal effects. So these are some examples of results that we show with different popular data sets like composite. With different popular data sets like Compass and Mera. Just to explain one of these to you. So, in the Compass data set, what we show is if you intervene on African American, let's say you want to find the causal effect on recidivism, we know that we don't want race to have an impact on the outcome. If you simply trained using ERM, you can clearly see that as you intervene on the African-American variable, the output actually changes between one value. Actually, it changes between one value to the causal effect actually changes. But we show that using our kind of an approach, after you regularize using our kind of an approach, there's a much lesser causal effect of varying such a variable on the output. In fact, in other kinds of raised variables, we show that we get almost zero causal effect. We show that this kind of a thing can hold with any shape of a domain prior, be it a monotonic domain prior, an inverted B-shaped domain prior, a J-shaped domain prior, so on and so on. J-shaped domain prior, so on and so forth. I'll just quickly summarize our other works here again and stop for questions. So that's just a summary of some of our efforts in developing causal perspectives for explainable neural network models. Beyond that, we have been looking at several non-causal efforts too. Even within the causal space, we have had some efforts on how do you learn representations using neural networks, which lead Using neural networks, which lead to causally disentangled latent representations. We've also developed a data set called CANDLE in the space. We've also been looking at how do you mitigate bias using such causal perspectives in neural network models. I think I'll stop here just to stay in time. I know there's another speaker going after me, but I'll be happy to take a few questions before I have to stop. Thank you again for the invitation, and I hope. Uh, the invitation, and I hope this was of some relevance to the workshop. Thanks, Vinit, for the talk. Any questions here? Yes, so I was wondering, uh, Hi, thanks for the talk, really, really interesting.