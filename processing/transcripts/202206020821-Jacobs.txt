I greatly appreciate the organizers for putting this together and having the flexibility for this talk. So I'm an assistant professor at Northwestern University and a joint appointment between computer science and preventive medicine. My background and my lab is focused on human-computer interaction, which fortunately Lena and Mary have already given kind of great introductions for. So hopefully this talk will help continue to kind of get everyone thinking. To kind of get everyone thinking about where there's opportunities for this type of interdisciplinary collaboration. My lab is the New Path lab at Northwestern, and we sort of look at how to design health tools at three different levels. We do a lot of thinking and design around patient-facing technologies that can adapt to individuals' changing healthcare needs. We've been doing an increasing amount of community-based research, looking at how to implement care into and health support services into community organizations. Support services into community organizations. And then finally, the focus of this talk is looking in the clinic and thinking about how we can design for decision support tools that embody different types of models. One thing that's really come through this week is how models can mean different things to different people. So a lot of what I'm talking about today is a particular project that was looking at machine learning within the mental health space. So, this work started a few years ago, and one of the main motivations was that we're seeing some struggles in implementing technologies that do embody machine learning models. So there's been some recent reviews like this one that started looking into why these technologies are failing or aren't being adopted into regular routine clinical care. And I've highlighted just a few here, but they've found Just a few here, but they've found things like issues with user satisfaction. Clinicians found the tools difficult to use or unhelpful clinically, or alerts are completely ignored because there's just not enough time to dedicate to forming an appropriate response. And so one of the things that really stood out to me was that it's not that the models themselves aren't performing well, but it's the way that these tools are designed and then implemented into the real-world systems that does often get overlooked. Systems that does often get overlooked. And so much of my work uses the user-centered design method, which is a core process of human-computer interaction. Typically, what we'll do is first look at really deeply understanding the socio-technical system. So who are all the different stakeholders and people who may be interacting with the technology, either directly or indirectly? What are their needs? What are their goals? What are their motivations? Using an iterative user trend design process for prototyping, which Alina's already talked about, but a lot of participatory design studies, one of which I'll talk about later today, then actually building the platforms and running field trials to see how they're used in more in-lo-wild settings. And so, today I'll talk about two different studies. I'm actually going to start with an in-lab more user study, or in this case, user experiment, and then I'll show. Experiment, and then I'll show kind of how we transition that into a broader qualitative and participatory design study. And so today I'm going to share two of our recent-ish projects, both of that were looking at the real world barriers to implementing machine learning models into clinical care. So the first one that we ran was an experiment with clinicians that looked at how they would respond. Looked at how they would respond to accurate or inaccurate machine learning recommendations when they were paired with different types of explanations. And then the second is a participatory design study where we worked directly with healthcare providers to design decision support tools. This was all in partnership with the Harvard Data Science Institute, where there was a team of data scientists and clinical psychiatrists who were working together on algorithms. Working together on algorithms for antidepressant selection for major depressive disorder. And so I started working with them over the past few years to think about what are the user-centered design questions that really need to be addressed before we go into a clinical trial. So I didn't know much about the treatment of depression prior to this study. It turns out it's a really hard problem. Right now, two-thirds of patients don't. Now, two-thirds of patients don't see a reduction in their symptoms after their first antidepressant trial. And a third of patients, which is really a huge number when you think of how many people are seeking care for depression. So a third of patients do not reach remission or see a reduction in symptoms, even after trying four different antidepressants. So it's selecting an antidepressant medication is incredibly difficult. There's a ton of trial and error. So clinicians were constantly telling us throughout. So clinicians were constantly telling us throughout these studies that often I'll have a choice of four or five different medications and really no great reason to choose among them. And so it was this kind of little guidance for doctors that really motivated this work. So this is just a little bit of background. The collaborators I was working with were looking at patients' full medical histories through the medical record codes and were particularly And we're particularly looking at three outcome variables. One was a stability score, so the probability that a patient would continue to use the same medication for at least three months. The second was a dropout score, so what was the probability of a patient's early treatment discontinuation following the prescription of a particular antidepressant? All with the goal of then making personalized treatment recommendations. So, what was So what was interesting about this was that in this domain and with many other domains, we know that these predictions are never going to be perfect. And a big part of that is we know that there are variables that influence these decisions and the effectiveness of treatments that we just can't get from a medical record. And so I'll talk more about this a little bit later. But for example, we know that there are environmental influences. There are environmental influences, social influences, and most importantly, patient preferences all play a role. And so, given that we know that these predictions are imperfect, our first question was, how would doctors actually respond to this type of model output? So, in this first study, we put together an experimental design just to look at if and how treatment recommendations would change doctors' decisions. You know, would doctors with years of experience. Decisions? You know, would doctors with years of experience be influenced at all by the recommendations or just ignore them altogether? And so in this study, clinicians saw a series of patient scenarios. So they'd see patient details, they would see treatment recommendations paired with an explanation some of the times, and they would be asked to pick a treatment that they would select for these scenarios. This was what we called a Wizard of Oz study. This was what we called a Wizard of Oz study. So it was meant to look like what we would produce from the model, but does not have a real model behind it. This allowed us to more systematically manipulate the recommendations and explanations. And so we looked at two things. One was how did people respond to treatment recommendation errors? And then the second was how did they respond to different explanation designs? So what makes a recommendation error? What makes a recommendation incorrect? Recommendation error or what makes a recommendation incorrect? This was actually very challenging to figure out. So, what we did to figure out what would make a recommendation incorrect was to work with a panel of psychopharmacologists who would rate for each of these scenarios what would be considered a correct or an incorrect treatment to give to this patient based on existing tolerability and safety standards. So, a couple quick examples. If a patient is experiencing fatigue, you want to avoid drugs. Fatigue, you want to avoid drugs that have a sedating side effect. If there's a recent history of suicidality, you want to favor drugs that are safer and overdose. So we had 220 clinicians who currently prescribe antidepressants participate in the study. The only thing I want to point out here is that they had a median of 10 years of experience prescribing antidepressants. And so this kind of aligns nicely with a question of how would doctors with years. Question of how would doctors with years of training and experience respond in particular to an incorrect recommendation? What we found was that clinicians without any treatment recommendations at all had a performance or an accuracy score of 0.357. We did try to make these scenarios somewhat complex because that is where we would expect this to have the most value. We set up the experiment to have an accuracy. We set up the experiment to have an accuracy score of 0.667. And when they were presented with a treatment recommendation, we basically see no change in their overall performance. And so this helped to kind of show that just because an algorithm or model performs better than a person does not necessarily mean that it will lead to improved decision making in the real world. What we'd hope to see in an ideal setting is that a clinician when supported by Clinician, when supported by this type of machine learning predictor, would make fewer errors than either the clinician or the predictor alone because they each bring a different kind of knowledge or expertise. But this study does align with some other recent studies in non-medical domains that suggests that humans interacting with machine learning tools may not only perform the same, but sometimes even worse than either after acting independently. I will point out, it's not that I will point out it's not that they didn't use the recommendations at all. Starting again with the baseline score with no treatment recommendations. Oops. We see a small but not significant increase in performance when they're given a correct treatment recommendation, but we do see a drop in performance when shown an incorrect prediction, which suggests that Which suggests that even with years of training, people are at risk of over-trusting imperfect tools. That's not to say that machine learning is bad in this context. It just helps to demonstrate the risk. What does it mean to over trust? Well, it means in this context, what we're trying to say is that they are changing their decision. So the way that this factorial experiment worked is they The way that this factorial experiment worked is they would see a similar, a very, very similar scenario without any treatment recommendation, with a correct treatment recommendation, and with an incorrect treatment recommendation. And so there's more alignment or there's kind of a switch in the decision for what treatment you give that patient when given an incorrect recommendation. Whereas we would hope that there would be some ability to recognize when an incorrect recommendation. An incorrect recommendation is being made. Our hypothesis is that clinicians were using the incorrect predictions more often because they were surprising. So if you see a recommendation that you didn't expect, it could be like, okay, that's surprising when you go along with that. Whereas you see when you expect, you may be more likely to say, okay, I can understand why you're saying that. I tend to use this other option. I'm going to go with that. So, the other thing, as a reminder, they saw these treatment recommendations, but we did also use explanations throughout the study. So, there was a baseline with no treatment recommendation, no explanation. There were treatment recommendations with no explanation. We did use what we were calling a placebo, which is providing some text, but no actual substance to the explanation. Substance to the explanation. We were really looking at two different explanation styles. This wasn't meant to be comprehensive, but just to get a sense of if and how this mattered. So the first explanation style was a rule-based explanation that shows what were the kind of features of a patient's medical history that were associated with the recommendation, as well as a bit more detail as to why you may want to favor or avoid that particular. Avoid that particular treatment. And then we used the feature-based explanation that was a bit more visual and glanceable that could depict those important features of a patient's medical history, but didn't go into that extra level of detail as to why. And we did find that the explanation styles led to different performances. With the rule-based explanations, we'd see pretty little difference in accuracy between the In accuracy between the correct and incorrect treatment recommendations. What was really interesting was that when you look at the feature importance explanation, so this was the more visual, a bit more glanceable explanation, we do see a reduction in overall accuracy when it was paired with an incorrect recommendation from the model. And what was even more interesting was that across the board, clinicians were rating this explanation as more helpful. Explanation as more helpful than the other one. So, our takeaway from this is that our design decisions, like the type of explanation to display, can have an important effect on clinicians' behaviors. And so it was helpful for showing the importance of these types of user testing and human-computer interaction in this area of health and machine learning. This is kind of a summary of those findings. This is kind of a summary of those findings. But really, for us as a team working on this project, our big takeaway was: all right, the design matters, but we still don't have an optimal solution. We know that there's problems with existing kind of common design strategies. And so we really needed to better understand kind of why we were seeing, you know, this overtrust or this use of imperfect tools and how to better design for real world. And how to better design for real-world medical decisions. And so, with that, we began a new study. This time, we were really focused on understanding the socio-technical system. So what were different stakeholder needs? What were their goals and their motivations? We also shifted our focus at this time to specifically look at primary care because it turns out that the majority of mental health care is initiated in the primary care setting. In the primary care setting. But the amount of training that primary care providers actually get to manage and treat major depressive disorder can vary quite substantially. And as I kind of briefly mentioned earlier on, contemporary treatment guidelines just provide pretty little support for choosing among different treatment options. So we worked with a number of different stakeholders, including primary care physicians. Including primary care physicians, residents, nurse, practitioners, all of whom currently prescribe antidepressant treatments. So I won't go into too much detail on the methods, but we ran 14 co-design sessions where we really asked them to tell us about their current practices, their opportunities for improvement, and then looked together at different design ideas and worked on iterating. We had the extra challenge of this all took place right at the end. Extra challenge of this all took place right at the start of COVID. So everything moved virtually. But we were able to learn a lot from them. I won't go through all of these. I just want to share kind of maybe two quick examples of the types of things that we learned. One was that in this case, the model does miss some very important features in the decision-making process. The most kind of critical one was the need to engage with patients. One was the need to engage with patients directly because aspects like patients' concerns or preferences were really critical in making these treatment decisions. Clinicians gave all sorts of examples of this, like a patient may be particularly concerned about a particular side effect, and that needs to be accounted for in these treatment decisions. And these aren't variables that we can identify through ICD or CPT codes, which was. Or CPT codes, which was what was currently being used in the models. It does maybe point to opportunities in the modeling side of things for future work if we could think about identifying needs or concerns through patient input or through the doctor's notes. From an HCI perspective, there was a clear need and request for tools for sharing and collaborating with patients directly, really emphasizing the need for a platform that can present. Or a platform that can present these predictions, but has more of an interactive interface. Yep. I'll just share one more example. So participants also very frequently commented that decision support predictions should be paired with recommendations for appropriate next steps in the clinical workflow itself, which does often involve other healthcare. Does often involve other healthcare providers. So, for example, some clinicians said, you know, look, if we're concerned about a patient dropping out of treatment, there's other people on the team that can follow up with them sooner. We can prescribe antidepressants that have less severe side effects. But not everyone that we talked to was able to immediately make that connection between those kinds of prediction output variables and the next steps in the workflow. So it's important that these tools. So, it's important that these tools do integrate with the appropriate processes and help reveal what those reasonable steps forward would be within the clinical workflow. And I'm not going to talk about these, but we do talk about some of the issues of designing these in more time-constrained environments and what to do when a prediction directly clashes with kind of standard practice or domain knowledge. And so, And so we present a number of recommendations for how to design these kinds of interfaces for machine learning models. In this case, we redesigned the prototype. And we also talked a lot about what this means moving forward. And this work points to, in my mind, a number of questions that we're really interested in. One is working more directly with patients and can we give patients more a voice in these Patients more of a voice in these treatment decisions, especially when there's kind of models involved in those treatment decisions. Design for instances where that model output does seem unexpected to clinicians or diverge from domain knowledge. And then finally, looking at how clinicians actually use these tools during patient encounters. We're actually working with the cardiology department right now, who just finished a year after or have been piloting. Year after or have been piloting for the past year an algorithm or machine learning model to identify people at risk of advanced heart failure. So, looking at how that's actually being used in the real world. I'll just acknowledge the many collaborators who worked on this project. And thank you all. That's a question. That's your question. Thank you. So, my question is: what form did the model output in the first study? There was some model output, and I guess I'm curious what that is, whether it's like a probability score or whatever. And then what was it that you told the clinicians at the end of the day? Was it like a sentence or was it a number or a picture? Yeah, how do those two interact? So, for the first study, what did we? I believe time has no meaning anymore. So, for the first study, they saw a ranked list of treatment recommendations. We just showed them the treatment recommendation. We didn't show them curves or probability. Oh, actually, I do think we showed some sort of like uncertainty percentage. So it was pretty kind of straightforward and short, mostly to avoid any possible. Mostly to avoid any possible confounding variables or factors that we weren't accounting for. Once we went into the co-design study, we got a little more sophisticated in what we were showing them. Oh, I'm not going to bring it back up, but I can show you later, but we did end up moving more towards a histogram showing kind of the, to help show kind of the uncertainty around some of the prediction models. The output itself was a stability score. A stability score, so the chance that someone would remain on the treatment, and a dropout score. And we actually found clinicians were much more interested in the dropout score and actually did not want to see a stability score because they were worried that it would make them ignore patients that actually needed support. Like it would make them overconfident that this patient would be fine. And then how do you define accuracy? So what is an inaccurate thing that they do? An inaccurate thing that they do? So, in that first study, accuracy was basically if they had to make a treatment decision for a patient. And through that expert panel, they labeled a treatment as correct or incorrect. And so it was, if they were picking treatments that were correct or incorrect for that patient and for that scenario. I think there's any relationship between like the There's any relationship between like the say there are different probabilities associated with these predictions. Like some of them are like super likely that this 99% or like 60%. Do you think those kinds of things influence people making a mistake that maybe at 50% you mess up, but are they going to respond more to an extreme situation and things like that? Are there thresholding that you need to do to guide people better? Yeah, so that's a great question. We talked a lot about that. What we ended up doing. About that. What we ended up doing was showing the same, like we kept everything as like the same percentage, I think, and we kept it kind of, I think, I want to say like 80, we ended up doing like 86% as just like a standard. But we thought about including that and changing that, but it was too many variables. Like you'd have to have clinicians walk through way too many scenarios to be able to test that. But I think you could do a whole study just around that. Just around that, and looking at how that kind of level of uncertainty does change behaviors. We also have a really cool group called the Midwest Collective that completely focuses on how to visualize uncertainty. So they do like a lot of dynamic graphs to help patients or not to help patients, to help individuals better understand what that means. And so I think they do a lot for like election production. Probably they do a lot for like election predictions and things like that. So there's kind of, I think, room to think about that as well. Thank you very much. First, can you hear me? Yes, and second, can you see my slides? Can you see my slides?