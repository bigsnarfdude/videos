And quick to follow, so I'm more or less like a domain user of the machine learning for material discovery. So, first of all, thanks to Professor Yen for the invitation. And it's great to be here to learn so many new things over the couple of few days. Okay, so our main talk about some of our works on machine learning discovery of the high-temperature polymer. So, this work has been through collaboration with my former postdoc, Dr. Lei Tao at Dow Chemical, and We's Group at Northwestern as well. The WISE group and Northwestern, as well as because of the Air Force Research Company. In particular, the project has been funded by the Air Force as well as VM. So the vision of my group I want to briefly mention is we believe the materials of the future need to be adaptive, multipurpose, and tunable. But the current design by the traditional triangle arrow approach is very limited, just like how dimensioned for the drug discovery. So it takes a lot of blend-based triangle arrow. You know, blended is trying to arrow. So that limited our search for the untapped region of the solution space. So, what we believe is the machine learning, high performance computing, or even the quantum computing may help us in this new quest. Hopefully, I'm able to convince you with the high-fidelity computer simulations and the machine learning techniques of us can be the Errorman or the EmoMask in the future. Okay, so here is a quick outline. So, I briefly mentioned the motivation, why the machine learning, in particular, polymer informatics. In particular, polymer informatics is so important, and how we build the trustworthy machine learning models for the polymer informatics. I will talk about our recent work on benchmarking over 80 different machine learning models. You will see a lot of them are being trained so well or even testing so well, but when you predict new materials, it totally fails. And also, a quick discussion on the discovery of the high-temperature polymer with some conclusions and the future works. So, first of all, why we care about the polymer? So, first of all, why we care about these high-temperature polymers? Because if you look at this very interesting pyramid, most of the polymers we see, majority of the polymers is actually sitting at the bottom, including polyethylene. So that's the grocery shopping bag. You usually see ABS for 3D printing, PC, PE. So, PVC, these are the typical plastic pipes in the hole. But their surface temperature is way below 100 degrees C, okay? Degrees C. Okay, but if you look at the high-performance, high-temperature polymer, you want to use them over a 300-degree C, for example, in the aircraft, in the high-temperature fuel cells, and in many other, you know, these extreme condition applications. You have very limited choices, just like a few hundreds of them. And that poses the challenging or the limitation is when you want to cook your dinner, you don't have enough materials, right? Right, so that's what has been traditionally dealing with, for example, as a Navy. So they spent over 20 years to discover these two, majority of the high-temperature piclike polymer. They have been used in the submarine these days. But 20 years, two formulation you got. We certainly don't want to do that, so that's the reason why we're moving towards this direction. But if you look at the more important thing, is the scientific challenge in is similarly like building the Legos these days. Like building the Legos these days, so you have very few basic building blocks. You can build the Aromai, Sinos, Magic Blocks, whatever you can imagine. But luckily, you have the map, you have the manual, so you can do step by step. You build it up over a few days or a few weeks. But in terms of these organic molecules, like the drug discovery, if I just give you carbon, hydrogen, nitrogen, oxygen, sulfur, if you do an enumeration, just up to 17 of them, you have over 166. Have over 166 billion possibilities. So there is no way you can experimentally synthesize all them, or even by using computer simulations, it will take forever. Okay, so that's the dramatic challenging happening in this field. So that's the reason why in my groups in the past few years, we are building these machine learning techniques and trying to automate this whole material discovery process. In particular, we make two major modules here. So one is so-called a forward. So, why is so-called forward machine learning model prediction? Once you have a new chemical formula of this material, so they can tell you quickly for the structure as well as the property of this material. But this is not enough because it's only a forward prediction. So, more important thing is you want to do a generative design. So, you want to specifically bias the generation process of their formulation so they will meet the target properties you are looking for. And among them, so there's a Okay, and among them, so there is no dramatic magic need you need to deal with. So, you need typically about four steps you want to do. So, the first one is you need to have a data repository either experimentally or computationally. And the second thing you need to do is the feature engineering, convert your material formulation of the structure into a machine-readable format, and then you pick up your supervised machine learning model so you can establish this structure-property relation, and then you can choose your favorite generating model. Your favorite generating model, so you can do the inverse evaluation. Okay, so here is an example based on the GAN. So, here is the first step what we can do. So, this is one of the largest data repository. So, we have known so far. It's based in Japan. It's called the Institute of Material Science over Japan here there. So, they have a large collection of all the literature published database for the polymeric materials we are interested in. And among them, so there are about 7,000 of the populations. About 7,000 of the polymer people has been synthesized and has measured their glass transition temperature. And for your reference, the TG is used to reference the polymer below this particular temperature is in the glassy state, while above this temperature is in the rubbery state. But in the meantime, you have over 5,000 of this polymer. People made them already, but never measured their TG value. So simultaneously, you may wonder. So if I want to know any particular So if I want to know any particular TG of these polymers, so normally I have to do the laborous experimental synthesis and do the measurement or even using computer simulations, you will take days and weeks to do that. So why not? Because we know this 7,000 polymer, we have the labeled data, we know their TG value. So why don't we do a feature engineering, train the machine learning model, you can predict their TG values within a second. So that will dramatically speed up this prediction. So that will dramatically speed up this prediction process. But before that, so we look at the TSNI plot of these two large databases, and you see there are certain overlap, but also some of the non-overlap region. So the second step we look at is how we can use the machine to recognize the right chemistry of these polymers. So this is one of the largest polymer people has been synthesized so far. And it has over 300 atoms on the backbone as a basic repeating unit. And this is Basic repeating unit, and this is a smiles representation. So, it's a ask representation for this chemical structure. So, here I just show you three representations in the literature people typically use. For example, you can use a molecular descriptor approach, or you can use a fingerprinting approach, or you can use an imaging to represent that. Okay, so I will show you step-by-step. Is this one-to-one? Yeah, it's one-to-one. Okay, so the first one is based on the molecular descriptor approach. So, this is a Descriptor approach. So, this is a mostly popular use in the chemoinformatics field. So, what you can do is actually you can really have zero-dimension descriptor like a hunt. Count how many carbon atoms, oxygen, nitrogen, so on and so forth. You can also do one-dimensional count on the fragment, carbon-carbon, single-bound, double-bound, hybrid bond, you know, and even a little more complicated, two-dimensional, like based on the topology structures. So, with the dimensionality increase, you certainly get more and more information, but the More and more information, but the descriptor gets more and more complicated. So, normally, you can easily extract 5,000 to 7,000 molecular descriptors to describe their chemical structure. So, this is a descriptor approach. Second one is called fingerprinting. So, this is another popular way. It's called ECFE, extended connectivity fingerprinting approach. So, what you can do is actually just like finding your fingerprint. So, you can start with the basic heavy atoms. With the basic heavy atoms on this molecule, for example, I label 1, 2, 3, 4, 5, 6 for all the heavy atoms, and then I start choosing one of them as my basic reference and choose the oxygen as my basic reference and start searching the nearest neighbor. Certainly, it will be just the oxygen atom itself, right? And then I start looking, searching for the second nearest neighbor, so I will find there is a double bond connected with a carbon there. And then I start searching the third nearest neighbor, so I found. Neighbor, so I found okay, beyond this double-bonded carbon, I have another nitrogen and another single-bonded carbon. So, in this way, so you will do all the neighborly search, you found all the possible chemical fragment in this molecule. And then you can remove all the redundant ones and preserve the unique one and put them into a list. And this list, actually, each of them has a unique identifier, and this identifier indicates this chemical fragment. Indicate this chemical fragment, and you can use a 0-1 representation. So it's like on and off. So the 0 means this particular fragment does not exist in the chemical structure, while 1 means it exists. So you can do the one-hole encoding process. So that's a fragment, that's a ECIP. Or you can also use imaging to represent the polymer. Imaging is a very popular project as well. So here is a polyacrylate, so you normally see, and this particular Normally, see. And this particular chemical structure has a smells representation. And what we can do is we can list all the possible symbols in the particular smiles and list it as a row. And you can establish your own dictionary. And then you can put these particular smiles as a column there. And then for each row, so you can do the same 0, 1 encoding. Okay, so it's called one part encoding. So once you have that, we convert. So, once you have that, you convert this particular small string into a 0-1 matrix, and then you can convert it into imaging. And these particular imaging patterns, you can use a convolution neural network to establish the supervised machine for you. To show you that it can work, so these are very different polyacrylate. They have very different smile strains, you have very different glass transition temperature. As you can say, we do have very different patterns, imaging patterns. So, this shows it can work. So, this shows it can work to the certain extent. So, now to recap, so we have three different approaches. So, we have molecular descriptor, we have a fingerprint, we have the imaging. So, now let's pick up our machine learning model, what we can do. So, here what we have is we choose the simplest one, just like the previous speaker mentioned, the LASO model, so it's the easiest extendable as well. And you can also use a feed-forward neural network as one of the popular deep neural networks. And you can also use a convolution neural network to deal with the image. Network to deal with the images, right? And then you can train all these models with the 7,000 label data. And very interesting, as you can see, you can train them pretty well, and also the test also looks pretty good. So both are over 0.8 or 0.7. Then the question is, I gave you ABCD for models. Which one shall you use? Can you pick up one of them? No? Right? We just blankly guess, right? We just blankly guess, right, which one we should use. But luckily, so we have, you want to see, right? So luckily, so we have another unlabeled data, okay? So the 5,000 polymer we synthesis already, but never measured their TG value. So what we do is we randomly pick up 20 of them, and using the high-fidelity simulation, so we can predict their TG value, compare with our machine learning model prediction. Immediately, so we see the convolutional neural network model with the imaging. Convolution neural network model with the imaging fails. It barely gives you any correlation, although it trains very well, tested very well, no predictability. Okay? And a similar thing for the others, for example, the LASO regression model with a descriptor, the correlation is fairly low. So only two standard, the fingerprinting with the LASSO model or with the feed-forward neural network model. And then we may argue, so this is your computer simulation. I don't trust your computer simulation because a lot of assumptions. Because a lot of assumptions can be evolved. Something about convolution filter of that. It could be wrong. Yeah, it could be. So then, at the same time, so there is a NatureCon paper published by the Penn State Group. They just made about 32 conjugate polymers. So these are the polymer used in the solar cell. And our machine learning model never sees them before. So this is truly a blended guessing process. And then we use our machine learning model to do the prediction, as you can see. Again, so the fingerprinting model with Again, so the fingerprinting model with a feed-forward neural network gave a quite reasonable prediction with a correlation 0.68. Well, your LASO model is too simple. It cannot capture the complicated relation, so the correlation is only 0.2. And then you may wonder, so why these models could work or why it fails? So that's the following study we did. So we basically enumerate all the possible combinations of their polymer structure, representation of the polymer. Polymer structure representation of the polymer, as well as the feature engineering approach, and in combined with a variety of different machine learning techniques. For polymers, typically you can use three different approach or representations to represent their chemical structures. You can use a precursor, it's usually the polymer, before you synthesize this real polymer structure, or you can use their basic repeating unit, or you can use the polymerized format, it's like oligomeric. Okay, so three different ways. Three different ways. Or, and for representing the polymer, I already show you a few of them. The smell string is one of them. Actually, we did a language model using the recurrent neural network, or even including the transformer. So you can recognize or you can tokenize these smile strings to predict their property. And you can also use the movement fingerprint. So the previous one I introduced is and off, so it's only 0, 1. But we actually introduced additional information. So we introduced the frequency. So, we introduced the frequency of this particular chemical fragment. So, that will represent how many times they show up in the polymer structure. And we also introduce the descriptors as well as the graph. We're using the wave representation as well as the imaging. And regarding the machine learning model, we basically did whatever we can find during that time. Feed forward neural network, recurrent neural network, convolutional neural network, a graph convolutional neural network, linear regression, support vector machine, GPR. Regression, supportive vector machine, GPR, plus and processing regression, random force, decision tree, okay, so on, so forth. So this is a very large combination of all three of them. So towards the end, so they will give us about 80, over 80 different machine learning models. And then we also look at how the different polymer categories exist in these two large data sets we have. So this one is a label data. So you have all the graphs. Label data. So you have all the gas transition temperatures we measure. So majority of the polymer. As you can see, the polyoxides, polyamide, polyamide, and the vinyls, and all the others. So it's a fairly large category, over 20 different categories. And on the other hand, so you see the data size 2, the unlabeled data, it's kind of like a similar composition, okay, of these polymer categories. But interestingly, if you really calculate their chemistry similarity, so we have a similarity index, it's called Similarity index is called Palimodal Similarity Index to compare how similar these two chemical structures are close to each other. So normally it's a zero to one. So zero means completely different. One means they are identical. So majority of the distribution, actually, if you zoom in, so this is a matrix we plot, but majority of the distribution is actually below 0.1, which means all these polymers are not similar to each one. So they are quite distinct. To each one. So they are quite distinct. And then we start building all the machine learning models by using this label data. So here's what we did. So we select some representative cases because we want to do a truly blend guessing. Remember, we have a large unlabeled data set, so we use a k-means clustering, select about nearly 600 of them as representative cases, instead of random selection. Instead of random selection, so we select about 600 of them. So we use the molecular dynamic simulation to predict all their TG values. This is very expensive, I can tell you. I use a DOE supercomputer, takes me more than three months, using over 10,000 of the processor running day and time, 24 hours per day, seven days a week. Dramatic computer power we have burned for all these cases. And then, so we treat this one as the most important criteria. Okay, but in the meantime. Criteria. But in the meantime, so we have the labeled data. So, what we can do is the standard way. You split that into 80% training data, 10% validation, 10% testing. So, you do all the fittings, five-fold validation, so on and so forth. And then you apply this train model trying to predict the MD-predicted polymer TG values. Okay, so as a blend guessing process. So, now let's see how it works. So, the first one we look at: okay, so these are the So, these are the benchmarking of our MD simulation protocol to show you our high-fidelity MD simulation can truly predict the TG value of this polymer. So, these are the key means selected representative case. So, we first select about 100 of samples from the labeled data sets and we plot our MD predicted TG versus the experimental known value of these 100 cases. As you can see, so they exactly correlate. So they exactly correlate to each other very well with R squared 0.9. So showcase our simulation protocol can capture the TG values of these real polymers. And then we start doing all the feature engineering on these polymers. I already mentioned for the Morgan fingerprint, the Morgan fingerprint of its frequency as well. So as you know, in the following, we also consider the molecular structures. So that's why we use molecular graphs to represent these part of our structures. These polymer structures. So here's the other case we consider. So you may wonder, so why when you consider the oligomer of this polymer, how many oligomers we should consider. Okay, so we did also convergence study. We start from the zero case, which is the basic monomer, with the repeating unit. So that's only one monomer, and up to polymerized about 16. As you see, with the number of the polymerization degree going up, so their descriptors or their fingerprint is. Descriptors or their fingerprint is gradually increased and eventually converged to a plateau value. So, here what we found is overall about 16 of this repeat unit give you a pretty good converged representation of the common structure. And then here are all the basic settings of the machine learning model we deal with from RNN, LASO, support vector machine, feed-forward neural network, 1D, 2D convolution neural network model, random forces, and all the others. Right, random 40 and all the others. And then we start training them. So we monitor all the performance. So these are just a training process. And then we start looking at how their predictions looks like. So here we have about three criteria. So the first one is the training set R square. So this is the most widely used. You also have the testing set R squared, so that's another one. But we care mostly is on the unseen data, so this blank guessing problem. So, this blank guessing problem. So, that's the unlabeled data R-square. So, if we use the unlabeled data R-square as a reference, so this red little arrow is pointing out the best performed model. So, as you can see, with the different combinations of this machine learning model and their feature engineering approach, so you have the model performance varies actually could be quite a lot. Okay, so as high as like 0.65 or 0.7, or as much. Or 0.7 are as low as 0.4 in this case. And so here's some particular cases we found out. So, for example, this FID4 neural network model I have already mentioned earlier. So, that could give us pretty good predictions. So, if you zoom in, so you can look at the training, testing, R-squared, both over 0.8. And the prediction on the unseen or on the unlabeled data is like 0.65, which is quite acceptable for our. Which is quite acceptable for our material discovery process. And well, if you choose the others, like a Dawson processing regression model, which is more popularly used because it will give you the uncertainty associated with the training process, but actually the test, the unleveled data R-squared is only 0.55. Okay, so quite low compared to that. And when you choose others, so that's another case. I will skip this one. Oh, sorry. I want to talk about it. So, here's another special treatment of the polymer structure as a 2D imaging already mentioned earlier. So, we can use an imaging pattern to represent their chemical structures. As you can see, we still can do pretty good training and the testing, but the R-square fails quite a lot because you lose a lot of physical interactions as well as the chemical features when you do this pattern process. Process. Okay? Similarly for the models. Scanning words. And the other one is actually, it's also quite interesting. It's based on the recurrent neural network. It's like language model. So you can tokenize the particular symbols of your polymer smell screen. And then you can train the recurrent neural network model or even the transformer model nowadays. And they can directly read in this particular screen and do the predictions. Okay? You train that one. It's actually the performance is pretty good. Is actually the performance is pretty good. So, as you can see, testing R square 0.89, pretty good. And testing R square is over 0.8. And the prediction on the unlabeled data is 0.59. It's actually quite impressive. And we also did the others, like the molecular graph. So this one is actually a little bit lower. Although the training R square is over 0.94 and the testing R square is 0.88. It's better than the Is 0.88. It's better than the recurrent neural network model, but the predictions on the unseen label data is a little bit worse. But it's still okay overall. So we also compare with a more widely used platform has been used in this field. So this is actually a project funded by Navy. It's a Muri project. So they spent $8 million to build this one. So they use a quite complicated approach to trying to featurize the polymer structure. To featurize the polymer structure, so they call it a hierarchical descriptor approach. At the atomic level, you consider the basic building blocks of your polymer. So, this is the basic level one. At level two, you can use the descriptor approach, consider the molecular surface, their fraction, their van der Walls, and surface aromatic rings and all these kind of features. And on the polymer chain level, you can consider the distance between the rings and the side chains and the main chains, so on and so forth. And the main chain, so on and so forth. They also trade their model over versus a variety of the properties like TG, you know, dielectric properties, and some others. But when we do the prediction, okay, so when we do the prediction, as you can see, this well-established platform on the unlabeled data actually gave you worse prediction. R-square is only 0.5. Well, our model can achieve 0.7. So, what we learned so far is actually you have to be careful. You have to be carefully choose your feature engineering, your structural representation, as well as the machine learning model, because the previous one, so this part of the genome approach, they are basically using all the GPR models in their machine learning model trainings. So as we can say, we can train many, many more, much better models to go beyond this widely used platform. We should spend almost $8 million on that. Dollar on that. And here's another comparison for sake of time. I don't have too much time to expand that, but this is on the concrete polymer I mentioned earlier, which has been used in the solar system. So what we learned so far, is if you consider the different feature importance, so the rankings could be very different, and in particular, considering the needs, what you are looking for. But overall, there are some general recommendations for the community to use when you want to pick up which is. You want to pick up which is the most reliable machine learning model, you should usually consider, okay? Because we have been seeing that a lot of publications just randomly pick up one of the machine learning model with combined with this particular feature engineering. So they train their model with a good testing and validation R-square and they start playing. So this model is useful. But we know from this blended guessing problem, it could completely fail. But if you don't pay attention on that, you could run. Don't pay attention on that, you could run into the disaster. Okay, so that's the thing. So there is a very strong interdependence of the feature representation, machine learning algorithm to affect the model's accuracy and the general abilities. So what we are looking for is actually, we particularly need to consider trustworthy. It's a quite important thing of all these machine learning models, how reliable they are. So that calls for actually the uncertainty quantification of this machine learning model prediction. Of this machine learning model prediction. So that's an ongoing paper we are currently writing. So we are really looking for new not just to predict the particular property, but you also look at how much uncertainty it associates with your prediction. So that's a very important thing. And the future work of POP, of course, we want to automate a lot of the synthesis process. You can get more and more experimental data for the validation, will give you better fidelities as well as the inverse design with the multi-objective design optimization process. Design optimization process with different enforcement learning of the others. Towards the end, so I want to show again. So, all the models are wrong, but some are useful. Okay? I will stop here and thank you very much for your attention. Thank you very much for the nice conference. So, can you have any questions? So, you know, different machine learning models have their own inductive device. And like, I just want to want to learn because I guess most of the people in this room have not learned like. People in this room have not learned like polymer physics after their high school. So I just want to learn, like, as human experts, like, if you look at the polymer structure, how would you predict the, for example, the TG hot polymer?