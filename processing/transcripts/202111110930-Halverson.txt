If you can share your screen. Well, maybe just we've discussed this a little bit in advance. So we thought rather than every one of us giving their own view of the entire thing, we've split it up into three parts, which are sort of in one sequence of slides and will will cover these in turn. And we'll cover it in turn. Okay, so before starting, let me just introduce our speakers just briefly, because all of them have a lot of things they have done. So James Halbertson did his PhD in Pennsylvania University. And after that, he has a postdoc position in Santa Barbara in the Theoretical Institute, Institute for Theoretical Physics, the Kavli Institute for Theoretical Physics. The Kavli Institute for Theoretical Physics in USA, and now he is working in Northeastern University. Bishnu did his PhD in the University of Illinois, and after some postdocs in the United States and Europe, he's now working in With Waters Strand University. Maybe I pronounce it wrongly, but it's complicated. Then Andre Lucas, he did his PhD in the Munich Tech University in Germany. And after some postdocs in the United States and the UK, now he's working in Oxford University. So it's a pleasure to have all of you here. Please go ahead. Okay, so I'll start. So after Fabian's beautiful talk, we've seen that machine learning. That machine learning is of vital importance in getting things like a claviometric. We've also seen from various angles that string theory is now well in the age of big data. There are, for example, half a billion reflexive polytopes for which you construct toric threefolds. The vacuum selection problem in string theory suggests there may be many, many, many vacua and understanding aspects of landscape and string. Understanding aspects of landscape and swampland could also profitably use big data methods. So, in this, to begin this discussion, we'll present three different views on machine learning using different tools. Maybe Jim, can you advance the slide a bit? Hold on, I'm trying to. Right, so maybe go to the outline slide. Yeah, so. Maybe go to the outline slide, yeah. So, um, I'll discuss supervised learning and then I'll pass it on to Jim, who will discuss self-supervised learning, and then Andre will uh discuss reinforcement learning, and then hopefully we'll have a excited discussion about prospects and the ability of these methods to probe interesting physics and interesting mathematics. So, maybe next slide. So, supervised machine learning in your own Neural networks, for example, we start with some input, and based on this input, we have some unique output. And by feeding a neural network many instances of associations between input and output, you tune the parameters so that it learns. And when you feed it unseen inputs, it generates plausible outputs based on those unseen inputs. Based on those unseen inputs. So the neural network has a long history. 1958, Rosenblatt at the Cornell Aeronautical Laboratory and the Office of Nevada Research developed the perceptron model. And the modern neural network is essentially a souped up version of this old piece of technology. Since the advent of, I think, Mathematica 10, it's sort of built into It's sort of built into Mathematica, so it's very easy to access, and you can just use it out of the box and address many interesting problems just by using an inbuilt implementation of these tools. The idea basically is that there are weights and biases. Biases are vectors typically, and weights are matrices. And you tune the matrix elements and the elements, the bias vectors, in order to generate an output given an input. An output given an input. So, what makes this different from just ordinary regression is that we have some nonlinearities we introduce into this architecture. So, the pictures on the slide sort of illustrate what's happening. We have some input vector. We modify these inputs by acting with a weight matrix, and then we have this nominal. And then we have this nonlinearity as the output of an individual neuron. And all these neurons are, in this case, fully connected neural network. And thus, each neuron in a given layer talks to each neuron in the next layer. And this iterates until you get a reasonable output based on this training. The universal approximation theorem is one of the fundamental results in machine learning. Saibenko and Hornick developed the And Hornick developed the initial statements. And the idea is that given a one-layer neural network with finite width, meaning a finite number of neurons in that layer, you can approximate any reasonably well-behaved function on a compact subset of Rn. And so this means that given some function that describes whatever data you're looking at, neural network should, in principle, be able to find it. The thing is that the statement of the theorem. Know the statement of the theorem says finite width, doesn't say simple architecture. The width could be sufficiently large that even if such a thing exists in principle, it may not be practical. However, we found from various examples that particularly simple architectures do extraordinarily well at addressing many interesting problems. And so this framework is sort of the setup for supervised machine learning. Setup for supervised machine learning, and I'll discuss a few of the problems that this set of tools have been applied to. So, next slide, please. So, one of the initial things people looked at are the Clavial manifolds. So, the initial thing that was examined were the complete intersection Clavials. There are 7,890 configuration matrices you associate to the three folds. Candelus and company list. Kendallis and company listed these possibilities in the late 80s. So, based on the configuration matrix, which gives a polynomial degrees in products and projective spaces that define this computer intersection variety, we can predict Euler characteristic and H11 and other properties, for example, favorability, with very good accuracy. So, since the initial work in this direction, Urban Infinite Telephone. This direction, Urban and Finatello, for example, have obtained results, they're essentially 99% accurate in making these predictions. So these kinds of topological features are eminently machine learnable. And we see similar things for the fourfolds. So James Gray and Andre Lucas and their collaborators listed nearly a million configuration matrices for the SISI fourfold. Matrices for the CISE fourfolds. And a similar machine learning of topological invariance there is as well possible and indeed has taken place in the papers that are listed. Another area where machine learning is useful is studying bondocohomologies. Here, what's particularly striking is that machine learning results translate to new analytic formulae for For various of these line metal cohomologies. So, Andre had a series of beautiful papers on this theme in which complementary to machine learning techniques, they were able to devise exact formulae. So, this is one area in which machine learning has taught us something like mathematics and physics in translating numerical results to equations. Equations. Another area is knock theory, which is on the next slide, I think. So this set of investigations began with the work of Mark Hughes, who studied positive positivity, which is a property of knots where you consider products of positive bands for these knots. And essentially, these machine learning investigations were able to determine that knots have these. That knots have these properties and these were unknown before. So, machine learning has taught us something about this structure of knots. Another thing that Mark looked at was the slice genus. So, a knot is an S1 embedded in S3. You can think of the S3 as the boundary of a B4. And similarly, you can think of the knot as a boundary of some surface sigma. The slice genus is the minimum genus of that sigma. So, from properties of the knots, the slice genus can be learned using machine learning techniques. With the Arjun Car and Ankar Parikar, we looked at studying the association between two different topological invariants, the Jones polynomial and the volume of the knot complement. So, for hyperbolnoms, the knot complement has a metric constant negative curvature, and its volume turns out as predicted by the Jones polynomial. By the Jones polynomial. Later work with my student Jessica Craven and Arjun Carr showed that, in fact, predictions of the Jones polynomial for the volume of the non-complement can be accomplished by not looking at the polynomial coefficients as we were previously, but looking at particular evaluations of the Jones polynomial at certain complex numbers. And this ties in to analytic treatments, namely Witten's perspective on the analytic continued Chern-Simons theory. Analytic continued Chern-Simon's theory. And again, we were able to write down equations that give an analytic prediction of the volume based on these evaluations of the Jones polynomial. Now, different knots can have the same Jones polynomial, but different volumes of the knot complements. So the Jones polynomial doesn't uniquely identify a knot. In fact, we don't even know if the Jones polynomial uniquely identifies the unknown. So the two knots that I've drawn at the very bottom of the slide are the thisalt weight and one of the OTI unknots. One of the OTI unknots. They're in fact no knot at all, the circle on the left in the blue. So Jim Halverson and collaborators, Fabien as well, have looked at identifying the unnaught, and they've introduced wonderful natural language processing techniques to do precisely this. So these, because there are large mathematical data sets in off theory, this is another place where machine Off theory. This is another place where machine learning can teach us something about topological invariance with these objects. Next slide. This is my last slide. So machine learning identifies associations. So there are open questions here. How does a machine learn? Can we bridge the successes of machine learning and various problems to finding new analytic results and methods? One area where this might be interesting is in terms of these calculations of Hausdorff. Of these calculations of Hodge numbers of Clabials. Of course, we already know all the Hodge numbers for the clear section Clavials or the elements in the Cortes Cocker database. What's interesting is that whatever the machine is doing is not what we are doing or what we would do. We would do sequence tracing and these kind of calculations we would do by hand, or Ruben-based calculations with a machine, these scale doubly exponentially with the size of the configuration matrix. Whereas whatever the machine is doing, if you Whereas, whatever the machine is doing, if you just do the timings of it, it's doing something polynomial in time. It's getting things that are almost always correct. So, it will be interesting if we can find new ways to calculate algebraic geometry. Another area that machine learning is very promising in is conjecture generation. So, Jim and collaborators did some nice, beautiful work in this direction. Trained models give some very sharp predictions. Give some very sharp predictions, and then these can be attacked using analytic techniques. So, again, you're bridging numerical results of machine learning to new analytic statements about theories we're interested in. The success of machine learning in addressing even simple problems, well, complicated problems, perhaps like chess and go, inventing new Jaziki, new opening sequences in Go, for example, indicates machines can do new things and can teach us new ways to attack. Us new ways to attack old problems. Ineshulingam and Gowers, for example, trained machines to essentially do mathematics, do real analysis at the level that a machine writes proofs that are indistinguishable to professional mathematician from graduate student written proofs. So we may in the near future be in a position to collaborate with machines on doing novel research. On doing novel research, interesting science. So, we should maybe think about how carbon and silicon can collaborate on problems in physics. Okay, so that's all I have to say. I'll have to see who guys are doing. Jim? Gotta unmute. I'll even, if it lets me, turn on a laser pointer. Cool. Vishnu, thanks for the good introduction to supervised learning. I'm going to talk about I'm going to talk about self-supervised learning, or what there was a nice talk at Physics Meets ML a few months ago by David Fau. He called it self-generative learning. The crux of this idea is to let the neural network be a variational onsatz for some function that you care about, period, in physics or math or really any domain. That's a very, very, very general statement, which is part of the reason I think these techniques are powerful. These techniques are powerful. One of the crucial things is that, unlike in what Vishnu was talking about, where you have labels, for instance, given some specification of some collabiyao, you have labels for the Hodge numbers, and then you regress on those labels. In this case, what you train on is the function itself, namely the loss function is some function of the neural network itself, and that doesn't depend on labeled data. So, as an example of that, that's the sense in which it's self-supervised, there's no labels. Self-supervised, there's no labels. As an example of that, we just heard in Fabian's talk that if you let the neural network be a Calabi-Yau metric, or rather, if you let the initial neural network be a variational onsatz for a Calabi-Yau metric, and then you try to minimize a loss so that you end up with a Calabi-Yau metric, what the loss is in that case, well, he gave a couple of different examples, but the simplest to understand one is to think of it as depending on the Ricci tensor. And in that case, what you have is the loss being a What you have is the loss being a function of the neural network itself with no labels. And then you just regress on it and hope that you're able to minimize that loss and end up with the type of function you care about, in this case, a collabiometric. The reason why Vishnu and Andre and I wanted to cast this in a slightly broader light is to point out that this is also exactly what people in condensed matter physics are doing. There, they say, let the neural network be a quantum state. There are various different iterations of how that is done. Of how that is done. But the simplest version is to say: let's let the neural network be some wave function, and let's assume that there's some Hamiltonian that governs the dynamics of the system. And then the loss function is going to just be the energy, the expectation value of h associated with that wave function. And then one wants to minimize on that. And the hope is that you find the ground state of some complicated quantum manybody system. So it's the same basic case as in the Calabiel case. As in the Calabiao case, in the sense that there are no labels, the thing that you care about is the neural network itself, not the predictions that the neural network makes necessarily, because the neural network is the function and it's some function that you care about optimized according to some loss that you care about. So more generally, you could just say that the neural network is you're hoping to find a PDE solution to some diff EQ DF equals zero, where D is some differential operator. Let the loss be DF squared. There's been recent work on There's been recent work on studying Hermitian-Yang-Will's equations with this in the context of line bundles. And this is a very, very general framework that, to my knowledge, the first time this has made it into our community is indeed with this collabiometric work that I'll comment on in a minute. Vishnu already brought this up a little bit. We actually didn't coordinate this, but we made the same theory comments. But the reason that this is a good idea, so generally, condensed matter physicists want to know ground states of complicated quantum many body Hamilton. Of complicated quantum many-body Hamiltonians. That's one of the things that they do. The reason that this is a good idea in the context of neural networks to let the neural network be the wave function and then to regress is that the is indeed because the neural network is a universal function approximator. The crux of that, of course, is that it does not guarantee that the learning dynamics can find a good solution. So the existence of a good solution does not mean that it is easy to find starting from some initial configuration, some initial neural network, and then doing gradient descent. And then doing gradient descent. Okay, so this brings me to talking about self-generative learning specifically for metrics. And I sort of have a slide and a half on this. So there were a number of papers in the last year or so in this direction. And I'm actually really happy that I'm the one that gets to present some of this in the context of this discussion, because I'm the one that is working least actively on this. And in fact, I've only started to work on this. And the reason I'm excited to present. On this. And the reason I'm excited to present it is because just to be a cheerleader and say that this is incredibly important work, it is opening new sorts of directions that, in principle, we have wanted for a long time, but honestly, haven't thought that hard about necessarily because we just didn't have the tools to be able to tackle the physics. So reviewing what we learned in Fabian's talk, the idea is let the neural network be a metric. We start out with something that is not a collabiometric, and we minimize some loss so that at the end of the day, we end up with something that is a collabiometric or. Something that is a collabiometric or you know up-to-learning error is close to a collabiometric. So you might use the Ricci tensor or a Mongolian pair loss. Maybe there's other losses that you could consider. You can build in moduli dependence into these stories. And this is a very central thing to lots of the things that we do in shrink compactification because collabias are so central to what we do. So for instance, in the context of the swampland, as Fabian already talked about, given a metric, you can study, well, I shouldn't say. You can study, well, I shouldn't say the eigenvalues of the KK modes. You can study the masses of the KK modes or the eigenvalues of the Laflacian. And you can see this sort of level crossing and test it relative to the Swanton distance conjecture. And this is a very exciting thing. First of all, because we can do it for the first time ever. But second of all, I think it's safe to say that many of us want increasingly precise statements about the swampland that do not require. Swampland that do not require BPS-ness in any way, shape, or form. In particular, if we want to make statements about theories that are potentially correct, we have 4D n equals one at max, right? And we have to worry that swampland statements about theories with eight supercharges or more in four dimensions are just not going to be applicable. And so, what we're starting to see with this work of Fabian and Anthony is that we can move away from contexts where we use BPS-ness as a crutch and use these numerical techniques. And use these numerical techniques. Yes, there's error bars, there's a little bit of error, but they're relatively precise statements that really demonstrate the trends that are expected in contexts where there just is no DPSness to protect you. And that's wonderful. There's this recent work on line bundle connections also that people in the audience can hopefully comment on if there's interest in it. And one of the outlooks that I wanted to mention is that moving away from BPS-ness is really a big deal. When I think about When I think about why I like these results that you guys have done so much, one of the things that I remember is it's really incredible and amazing that we've been able to do as much as we can without knowing the metric. The first amazing result, or if you'd like, the first miracle is that Yao's theorem gives us string backgrounds in the first place. It's really incredible that we can do a topological check and get a statement about differential geometry, right? So to be able to check a topological property and then have an existence proof. Property and then have an existence proof for a Richie flat metric. I think that that's something that, when you explain it to that way to other communities, it sounds very strange and obtuse. And it's so baked into the way that we think about things that I think sometimes we forget how amazing that is. That the fact that topology guarantees geometry is an incredible thing. So now we have these metrics. And I would point out that a second miracle that we always use, an amazing result, is the existence of calibrated submanifolds. So another thing that's amazing. So, another thing that's amazing that is embedded in the way that we do that would be surprising to other communities is that even without knowing the metric, we can compute the volumes of certain submanifolds by calibration. And that is also a great thing that is the trick that was used to basically control swampland distance statements in 60F theory, amongst other tricks. Without calibration, those results weren't possible. So, part of my point in mentioning these cases is that these two amazing things. These cases is that these two amazing things we didn't have to be so fortunate, and it was great that we were so fortunate for so many years that we could use these things. But now, because we have the direct techniques, we just don't have to rely on them anymore in the sense that we could imagine we start computing sub-manifold volumes for sub-manifolds that are not calibrated. So, for example, one place that that arises is for non-BPS states, yes, in Calabi Au manifolds. But in particular, in M-theory on G2, where we hope to be able to get interesting 4DNE. Where we hope to be able to get interesting 4DN equals one theories. There simply are no calibrated two cycles or five cycles. Wrapping M2 brains or M5 brains on those cycles are what give rise to particle states. This is the statement that there are no BPS particles in 4DN equals one. And these numerical techniques applied in that context would give you a way to be able to compute masses in a, up to up to numerical error in a reliable way. So we can get, Fabian already talked about certain types of non-BPS states that we can now get a hand. types of non-BPS states that we can now get a handle on. My point is that there are other states that non-BPS states that we can get a handle on now too because of the metric. There's also this result relating weak gravity conjecture and minimal surfaces. So in cases where homology classes have minimal volume representatives that are not calibrated submanifolds, there's a connection to the weak gravity conjecture that Mehmet and Cody and Liam and Mike worked on. Mehmet's here and can discuss this a little bit if people are interested. But this is one good But this is one good example of a place where you might be able to use these sorts of metrics to be able to test some of the swampland conjectures that are out there. More outlook. Generally speaking, being able to do G2 metrics is an exciting thing. There, there is no simple topological check that's in any degree general that ensures that you have a good starting place on which to regress and end up with something Michi flat. Nevertheless, this is a really important direction. So, for example, there are constructions. For example, there are constructions obviously due to Joyce, but there's also constructions called the twisted connected sum. There's a great paper by Corty, Haskins, Picini, and Nordstrom. There's some more recent constructions by Joyce and Corigianis. There are a number of places where we know that there exist G2 metrics that are really holonomy G2 and not some subgroup. And those are great places to apply these techniques to. And finally, in the context of talking about metrics, we can start talking about real vibrations. Start talking about real vibrations. We're very used to talking about vibrations where the vibration is a complex manifold, but sometimes we're not so lucky. So, well, the collabia is still a complex manifold, but SYZ is a real vibration in the sense that we have a T3 fibered over some three manifold. And so, you could imagine that if you take an SYZ limited moduli space, you might see the metric developed structure associated with the T3 vibration. That would be a nice test. That would be a nice test of ideas in mirror symmetry and might push some mathematical work in that direction. And finally, there's a case in G2 that is based on less exotic physics than mirror symmetry, very, very simple, semi-classical non-abelian gauge symmetry breaking, where the basic physics of how you break gauge symmetry, even in non-supersymmetric theories, guarantees a connection between the gauge coupling and W boson masses and a Tuf-Polyakov monopole masses. Polyakov monopole masses that are well controlled in the semi-classical regime of weak coupling and large Higgs VEV, which for us corresponds to large volume. And in that context, in G2, what you expect is a five-manifold that's fibered by two spheres over some three-manifold. And the details of that fibration need to exactly respect what the semi-classical physics tells you about the topological defects. So this is something that is true in Calabiao manifolds, and I'm happy to give examples. But in the G2 case, the same physics. But in the G2 case, the same physics arguments hold, but you don't have BPSness to protect you. So there should be these five manifolds that are fibered by two spheres, and having a G2 metric and taking an appropriate limit, you should be able to see this structure develop. And finally, the last thing that I wanted to mention is trying to get more theoretical machine learning results into our field. One place to do that is potentially with the neural tangent kernel. So the neural tangent kernel is basically the statement that gradient flow, which That gradient flow, which is infinitesimal learning rate, gradient descent, gradient flow of neural networks in the infinite width limit is governed by a time-independent parameter-independent kernel called the neural tangent kernel. And this is a major result from 2018 of Jacob, Gabrielle, and Hangler. And there were a couple other papers around the same time. Basically, it tells you that infinite width gradient descent is under far more control than you would have expected. And indeed, there are packages out there. And indeed, there are packages out there that allow you to train infinite-width neural networks, even though it's obviously not the case that what they're doing is putting an infinite neural network on the computer literally and initializing an infinite number of parameters. So these theoretical results open up ways to train infinite with neural networks that are tractable and there are packages to do it. So in particular, in the case of L2 or mean square error loss, one of the results of these papers, and in particular, a paper out of Google. And in particular, a paper out of Google that I like is that there are analytic mean predictions for an infinite ensemble of infinitely wide trained neural networks, which sounds too good to be true, but it really is true. And in that limit, these training techniques become something called kernel regression, where the kernel that's used is the neural tangent kernel. So this can be applied in different ways. Fabian and I are starting to think about how to apply this to collabiatrics. D. Luo and I are pretty far along and should have a paper within a few weeks doing this in the context of neural network quality. Weeks doing this in the context of neural network quantum states. And basically, one of the general things you want to do is to ask the question: does training an infinitely wide neural network do better or not than training a finite-width neural network? One thing to mention is that there's more recent results from last year about how you can have principled architecture design in this context. So the learning rates are governed by the eigenvalue spectrum of the neural tangent kernel. And so you might hope that you can increase the speed of learning by using clever tricks to tune the eigen spectrum of the neural tangent kernel. Spectrum of the neural tangent kernel. And so this can be done. This is just one place where real ML theory might be able to be brought into our field. And it can get you various sorts of guarantees that just straightaway training of neural networks can't get you. All right, so that's all I had to say. Okay, so well, thanks. So now we get to the third style of machine learning. So the previous two styles, the supervised and the self-supervised learning. Self-supervised learning, they were preparing input with labels or without labels before you start the actual training process. So you have to sort of have your data prepared in advance. And reinforcement learning is different from that point of view in that the data is being prepared as you go along, as you train your neural network. So maybe people are not too familiar with the setup. So let me try to explain that in this. Let me try to explain that in this diagram. So, if you look at the top of the diagram, this is what depicts the environment. So, the environment consists of a number of states, and you're trying to generate your data from exploring this environment. So, that's what the agent is doing. The agent is running around in this environment. And at each action, at each step that the agent performs, they pick up. They pick up a reward or a penalty, depending on whether the next state they go to will be better or worse, according to whatever specification you might have imposed. So in this case, you could think of getting better as having an object with more edges, for example, right? So then the first step would give you a penalty, the second step would give you a penalty, and the next two they would give you a review. They would give you a reward. And so, in this way, as the agent goes along, you produce a data set which consists of states, action, and rewards. And this is then what you use to train your neural network with. So that's being sort of produced in batches and fed into the neural network for training. And the actions that the agent performs, they're not actually random, they are indeed them they are indeed in the simplest case dictated by the neural network so the neural network in the simplest case would take a state as an input and an action outputs an action which is the action that the agent will perform next so so in this case once you've trained the neural network once you have an updated neural network and the next time the agent goes around in the environment it is hopefully more successful and then the More successful, and then this process is being iterated and hopefully converges to some kind of minimum, which amounts to the agent going most efficiently to a really good state within this environment, according to however you've defined your goals. And so, this has been applied, for example, very successful in the context of Go, the Alpha Go zero system. This is the AlphaGo zero system, which was able to learn the game of Go without human input. So, without training data, it was just playing Go against itself and thereby learning to play it very efficiently. So, this is very useful to explore large environments in particular. So, that's the basic setup. Next slide, please. And so, how could we go about and apply such a setup in the context of Such a setup in the context of physics or perhaps string theory more in particular. And so here's a very rough sketch on how you might do that. So the idea is to apply this in the context of string model search and model building. So you would think of an environment that might be a certain family of string models or perhaps a set of string data or even mathematical data. And the states, of course, would then amount to specific models, specific strings. To specific models, specific string models, or specific pieces of string data within that environment. And you would have to dream up some actions. So that would depend a little bit on what you're looking at, but they would typically amount to some sort of small modification of the model, whatever that might be in the particular case. And you would design a reward that is based on some measure of how desirable your various states are. So if you go to a more desirable state by an action, you would give it a reward. An action, you would give it a reward, otherwise, a penalty. So, that would be the basic idea of how you might apply this in a string theory context. And what might the goals be of doing this? Of course, the Google experience tells you that this might actually be a way of exploring the very large environments that we face in string theory and that we can't possibly scan in any sort of systematic way if it's 10 to the If it's 10 to the 100, for example, there's no way of systematic scanning, but these kinds of methods might still be very useful to identify the desirable models within such large classes. Once you have actually trained these networks, you can literally hope to use them for model building. So in some sense, the trained policy networks in this context, they might do what a traditional string. Traditional string model builder might do. So, in the sense that you could feed some sort of random or some sort of deficient model into this network, and it would then guide you relatively efficiently, supposing that the whole process has converged, to a successful model. And finally, you might hope that when you look at the train network, you might be able to identify model building strategies that you might apply. Strategies that you might apply as a model builder yourself. So, next slide. And so, you might hope for all these things, but by now we have a number of indications that this might actually pan out in this way. And so, I've just listed a few papers here which have looked at this in the context of various models. I have to say that the literature is not enormous yet on this. Literature is not enormous yet on this, so this is this is sort of beginning and in principle still wide open. So, let me just go through those various papers. So, there's this nice paper by Jim and collaborators on two A intersecting brain models, where they were looking at a set which was looked at sort of in a model building way beforehand, but showed that this is something that could be tackled in the context of reinforcement learning. Context of reinforcement learning and the reinforcement system, in fact, then suggested a different strategy from the ones that the by-hand model builders had taken before then. Then heterotic line-bundle models, they were studied by Mark Delina-LaFors and Robin Schneider. And so they showed that you can indeed use this to explore large classes and that there might be very favorable scaling when you increase the size of the environment, which is essentially controlled by one of these hot. Controlled by one of these Hodge numbers. There was a paper which applied this whole thing to not theory. So, this is not quite model building, but this is sort of mathematical data, if you want, trying to train an RL system to unknot. And we've been looking at RL in the context of more complicated bundles, so not abelian bundles, but monot bundles. These were. These were relatively large environments, and perhaps the difference to the previous models was that these were environments that hadn't been studied before by hand. And we found completely new models that weren't known before in this way, within environments that couldn't have been possibly scanned in any sort of systematic way. And we were able to check by another method, genetic algorithms, that in fact these scans are reasonably comprehensive. They are reasonably comprehensive. So, this all seems to indicate that there is very significant potential here. It's all still very much in the beginning. But what I would think is that RL really has the potential of changing the way that we think about model building in string theory or model building even in quantum field theory. And that this is that sort of the cold phase aspect of doing that is something that machines might eventually. Is something that machines might eventually become a lot better at than we are. There's a question of how these methods scale with the size of the environment that you're considering. And so this, again, this is something that you might want to measure in terms of H11. So most examples that have been studied so far are for relatively low H11, but we know H11 for the known cases goes up to almost 500. Up to almost 500. So, how does it scale? Does it scale polynomially? I think that's not completely clear yet, although there are promising indications that it might do. Even if that is the case, so even if we can explore basically, if we can hope to explore the entire string landscape that way, there is a question of how the good states, the states that we are after, say the states with a good Say the states with a good spectrum might scale. And there are some indications that there are very many of these two. They are a small fraction, but there's still very many. And they might scale 10 to this Hodge number. So if we go up in H11, even though we might be able to scan this entire environment, it's basically not feasible to look at all these states. So the bottom line here is that we need to refine our idea of what a good state is in this context. Of what a good state is in this context, if we want a chance of actually truly exploring the full string landscape that way. So, all these things are open and I think hugely interesting, but there is a real potential here that we might be able to do with these kinds of methods, which was basically inconceivable a few years ago, which is to explore the entirety of string theory. Okay, I stop here. Okay, thanks a lot for this very, very Thanks a lot for this very, very nice introduction. So let's start with the discussion session and we receive questions now. One comment as the questions come in is that we have some discussion time where we hope that in addition to some of the issues that we raised in supervision and self-generative models and RL, you might add other things to this little whiteboard or if you have completely different This little whiteboard, or if you have completely different topics, you know, feel free to bring them up and we'll sort of use this little space just as a way to throw things up as we chat. Yes, that's that's very important. Thank you, Jim. So, Paul. Yeah, hi. Hi, Jim. Hi, Andre and Vishnu. Thanks for the nice talk. And yeah, I just have a quick question, I guess, to Jim then concerning to this machine learning. To this machine learning TFT correspondence, which to me sounds like super cool. But I was just, yeah, I was interested in to which degree has this been explored and to which degree sort of like non-trivial examples have been explored. Like, you know, also in QFT, I guess you would first start with a free field theory and do some things there. And then you sort of slowly start adding. Sort of slowly start adding interactions. And yeah, I was just curious how much non-trivial examples have been. Yeah, great question. So at this intersection, one can talk about quantum field theories, how for a fixed neural network architecture to apply quantum field theory techniques to it in a large n limit. The machine learning result is that in the large end limit, many of these architectures are drawn from Gaussian processes, which are Gaussian. From Gaussian processes, which are Gaussian distributions on the space of functions. And those are the things that are the analogs of free field theories. So, in those contexts, you might use quantum field theory techniques to study neural networks that machine learning people care about. But because the architecture sort of defines the theory, or specifically the architecture defines the ensemble of functions, you could ask the converse question, which is say, can I to say, can I engineer architectures that give rise to an ensemble of functions? That gives rise to an ensemble of functions that is equivalent to some field theory that I care about. So, for example, if you could do that for QCD, you would have a totally new approach to lattice QCD. So, all of the work, except some of the stuff that I've been working on recently, has been trying to come up with QFT techniques to describe fixed machine learning architectures. And something that I'm trying to do now is engineer the other direction. I don't know, I don't know if that answers. I don't know if that answers your question, but the basic statement is that the free theory stuff is all over the ML literature. They don't call it that, but that's what it is. I see, I see. And it's getting slowly to interacting theory, so to speak. Yes, that's right. Okay. There's one more interesting thing to say about interactions. The reason that the theories are free is due to the central limit theorem, which has assumptions. First of all, you have to add an infinite number of variables, which To add an infinite number of variables, which requires the large n limit, but the other is that the variables have to be drawn independently. So if you break either independence or infinite n, you get interactions, and that's something that can be shown directly. Okay. And is it, okay, just to follow up really quick on this? So is it, because also you mentioned it, when you want to model some QCD-like interactions like this, where you sort of have also string-like excitations. Also, string-like excitations. So you could try to get even some sort of string theory extension of a neural network architecture. Do you think that's possible? I have, in my wildest dreams, I do have fantasies like this, but of course, we're very far from that. I mean, the basic statement is that neural network architecture. Network neural network architectures induce ensembles of functions, and quantum field theory is about ensembles of functions, but the way that they're specified looks very different. And so, yes, if you got very good at engineering neural network architectures to give rise to ensembles of functions that you care about, you might be able to engineer various different theories like this. And in particular, you should be able to see interesting physics. I mean, you might hope that you could engineer something that's sort of equivalent to the polyakov action or something like that. But I am wildly speculating now. But I am wildly speculating now. We're far away from that. There are many other hard things in the game now before we get to that. But we have made progress. For instance, we know how to do symmetries from this perspective, and that's very well behaved. Okay, cool. Thanks. Yeah, sure. Sounds very interesting. Jim, sorry, since we're just on the subject, so the related question. So as I understand it, in the infinite width limit, you say this corresponds to a free field. You say this corresponds to a free field theory, and then as you make the width finite, you sort of introduce interactions, right? That's the picture, right? But isn't it also the case that when you make the network finite, you sort of discretize your quantum field theory? That's also a good question. So, a finite network is still, in principle, a function of Rd for some RD. So, you can still put continuous inputs into the function. So, if you mean discretized in the sense, So, if you mean discretized in the sense of a lattice, the answer is that's what I meant. Yeah. Yeah, yeah, it's not discretized in that sense. It's discretized in the sense of how you build the function out of post-activations. But indeed, the finite neural networks can still be functions of all of Rn. Okay. Any other questions, comments? New ideas, suggestions? I have a quick question to you. Hello, thank you. Very nice. Talk you okay. So, Jim, how this SYC measure symmetry still duality scene, this is a project that you already started. I mean, you have an idea what would be the architecture to study these mods? Yeah, I should clarify. First of all, I didn't intentionally leave this slide up, but it happens to be up. Happens to be up. Maybe I should go back to this part. But no, I'm just speculating about things that one could do. I am not working on that myself. It's an interesting question, given collabiometrics, to talk about structures that we might be able to see now, where conventional algebraic geometry, calibration, complex manifold stories are not guaranteeing us certain behavior. But SYZ is something that's widely believed, and yet it would be. Widely believed, and yet it would be nice to see it as explicitly as possible in certain limits and complex structure. So, no, I'm not working hard. Okay, but do you still don't have like what architecture could be good for these kind of things? I should say the experts are here, but as far as I know, there's not generally any principled architecture design yet for Calabiometrics. It's just using feed forward. Is that right? Yeah, yeah, yeah. I mean, I think the first step would be to look at a relevant example where you perhaps. A relevant example where you perhaps you actually know what the vibration is and use the existing methods to compute the collabiometric on there and then check if the numerical result is compatible in some way. There is another question by Saul, but I don't know if Fabian wants to comment about this question. Yes, Michael would be on this question. So if Saul comment is on something else, maybe I can jump the line. It's something else, please. Go on. It's something else, please. Go on. Okay. Yeah. So I started looking into this with Anthony, actually. And so the question there is: as Andre precisely said, sort of first you try to identify this in a known case. And if you take the Fermat quintig, you actually know the SYZ vibrations. And Philip worked this out. You can write down the three torus. And then what you could do is, for example, if you then take the limit, the SYZ limit where this torus becomes. limit where you where this torus becomes very big or very small then essentially you just have a three sphere a topological three sphere so the spectrum should that you should be able to see then in this limit should be just the spectrum on the three sphere so this was the approach we were looking at and so in general i it's not i mean it's a little bit of a roundabout way of seeing seeing a vibration but looking just at the metric at the numerical metric it's unclear to me what the It's unclear to me what the best way is to actually identify a vibration. So, sort of doing it via the spectrum is a way of seeing a vibration, especially in the limit where the three-sphere becomes large, for example. That's a very interesting question. There have to be better ways of doing it. I just couldn't think of one yet. So, don't. Okay. Great. Thank you very much. Okay, great. Thank you, Ruby. But yeah, so the idea. So, in this case, this is not a question about a specific neural network. You would just have to know the metric, you would construct it, you would go to a limit where the SYZ torus becomes small, for example, and then study, for example, the spectrum and see that it becomes an SYZ vibration. Okay, great. Thanks. Thanks. So I have a I have a kind of teasing question for you guys. So, we have seen that one of the most ambitious goals in string phenomenology in general, and we know it, is basically to try to build a model, at least one, where everything is stabilized, where we can realize particle physics and we can realize our cosmology, of course, all at the same time. And it's very challenging with the tools that we have. With the tools that we have used so far. So, when you guys talk about machine learning, I kind of have the dream that you might actually employ these techniques in order to go towards that goal. Do you also see that as a possible future of the theory? Yeah, maybe I can answer that. I mean, yes, maybe I wasn't that clear about it, but in the context of But in the context of reinforcement learning, I very clearly see that as a possible goal and a real possibility. And it is now at a stage where, you know, the level at which we can relatively easily prescribe the properties that we want from a good string model is currently still at the level of the spectrum. And often, in many cases, only at the level of the chiral spectrum. And so at that level, And so, at that level, we can design, say, a reinforcement system that will search and find within a very large environment models with standard models, models with the right spectrum. So that's the extent to which we can go. If we wanted to go further, of course, we wanted a model that, as you say, it has the right recover couplings for the masses, it stabilizes the moduli, it does all the other things that we would want to do. Other things that we would want to do. And so we would have to find ways of computing these things efficiently first. So as these things become available, they can be fed into the machine, basically, efficiently. And you could envisage then having a system that would really search a very large environment and find standard models within string theory. I mean, as an example of how that might work in practice, Vishnu mentioned some of these analytic formulae for cohology. So if you want to know the entire spectrum of a string model, not just the charlot spectrum, you will need to compute cohomology. And computing cohomology is incredibly time intense if you do it the standard algebraic geometry way. So it's not something that you can do while you're searching at the same time. Can do while you're searching at the same time. But if you have these formula, that's very quick. So there's a potential for theoretical progress and computational progress to sort of synergize, if you want. So we are not yet in position of trying our techniques in order to arrive at that goal. We have to develop first, if I understand correctly, additional techniques, numerical techniques. Techniques, numerical techniques. We are at the goal where we can get the engineer to engineer a model with the right spectrum. But beyond that, I think we need more tools. Okay. Thanks. More questions, comments? Fabian, yes. Maybe just a short comment on Stobu's question. So the things like modulized stabilization also are questions where the community at the moment is not agreeing upon. So some people say KKLT is stable, KKLT is unstable, whatever. Is unstable, whatever. So, before we actually know the rules of the game, it's unclear. I mean, once sort of everybody agreed that, let's say, KKLT would be working, and there was no inherent instability, you could sort of compute the entity's rebrain, you could compute the uplift, you could insert a, I mean, if you would believe this is stable, you can put it in and the neural network could try to sort of find the combination of these three brains and so that does the uplift and whatever. that does the uplift and whatever but i think that answering these fundamental questions about sort of what are you allowed to do in string theory which effects can you get from string theory is nothing that machine learning at the moment will actually answer or help you with so you cannot just we don't have a within neural network where you just give it a quality collection it tells you everything that can happen with it we are not there so this relies on on on progress that's being made on the community i think That's being made on the community, identifying sort of which things you can have. And once this is agreed upon, you can build into the neural network and search for it and find then stuff maybe that has stabilized model I using KKLT if you believe the KKLT is stable. You mean something like some correction that was ignored, right? Yes. Yeah. I had the impression when you talked about self-generative or self- Self-generative or self-learning algorithms that this kind of technique could help us to understand better what actually we have, not only what we know and apply what we know to, I don't know, some combinations, but also to learn something new that we haven't identified before. Is it true or it was my misunderstanding? So these, sorry, was the question for me or was the question to somebody else? To anybody? Anybody. Yeah. Okay. Anybody. Yeah. Okay. So my take was in this one example that was reviewed, we knew the rules of the game. So when you do reinforcement learning, you're playing a game. It's you against mathematics and you're trying to find something that is consistent. So what you're putting in is you're putting in the rules of the game. You're telling the thing, here are cycles, and you're allowed to wrap manifold in it. You're allowed to do this, but the tadpole tells you the cutoff is whatever. Cutoff is whatever it is for this colour BL. So these are the rules that are predefined for us. And we're putting it in. And then the machine is finding very efficient techniques of winning this game, of finding a standard model that satisfies the rules of the game. But we are not at the point where the rules of the game are defined by the machine learning algorithm. We are putting it in. And based on these rules, the machine learning is finding newer techniques, more efficient techniques of arriving at the results. Arriving at the results that you want to have given the boundary conditions of this game. So, what we indeed found is that the machine learning algorithm finds new ways of solving the TEDpOL cancel, I mean the TEDpO condition and the PPS condition and so on in type 2A, for example, or finding an on-node or whatever. But this was not identifying the rules for the type 4, we put it in. And I think this is where you have to draw the boundary at the moment. Thanks. Mermit. Okay, so I have a question about calculating volumes of non-holomorphic cycles. So say that I have a calabia and I know the metric to arbitrary precision. And I have a real sub-manifold. And I want the volume of this real sub-manifold. Conceptually, all I need to do is I take the metric on the Calabiao, I take the pullback, and then I compute the integral with square g, and that's the volume. Does anyone have a notion of how difficult this would be in practice? In the case where you have the metric numerically or analytically, or in which case? Numerically, yeah. Yeah, it's not totally straightforward in my view. I mean, the numerical metric is in practice given at a discrete set of points on your clavier. So you have some, you dissolve your clavier into some sort of point sample, and at each of these points, And at each of these points, you have the metric given. Of course, none of these points will typically be given on any given submanifold. So I think if you really wanted to do this, you'd probably need some dedicated point sampling on the manifold you're interested in as well. Yeah, that makes sense. Yeah, I mean, just to follow up on that, I mean, I. Just to follow up on that, I mean, I guess you're saying, Andre, that there's a chance that all of the points just miss the submanifold. And then you have to be really lucky and hopeful that you learned your metrics so well that, you know, the points in the test set, which you still get metrics for in these examples, you just didn't train on them, that it's still good enough to get the right result. I mean, I guess in some sense, that's a non-trivial check because in the algebraic case, calibration does tell us what the answer is. And the metrics, if they're good metrics, I better agree. And we have the equations for those cycles because they're all algebraic. Equations for those cycles because they're all algebraic. Yeah. I mean, another way might be that you have the metric at so many points that you could sort of interpolate, right, and thereby get the metric at your sub-manifold. So I don't know. I certainly haven't tried this yet. I'm not sure anyone has actually tried this. I mean, I think people are still at getting the metric itself rather than getting the induced metric on sub-manifolds. But of course, that's an obvious next step. That's an obvious next step. Can I actually ask how, if you wanted to start by deforming the algebraic guys to non-algebraic guys, would you? I mean, how would you do that, Andre? Would you just express the algebraic coordinates in real coordinates and then do deformations that aren't complex or something? Yeah, non-complex deformations, yes. I mean, some let's see. Let's see. Yeah. Well, I don't know. I mean, I don't even know which cycles you'd be interested in. I mean, I guess that's the first thing to clarify. Line in the quintic. In the quintic. I don't know. I've never found a non-algebraic cycle in the quintic. Finding the algebraic ones is difficult enough. More questions? Comments? More questions, comments? I'd like to ask something for non-experts that there are many in the audience. So, can you, some of you say a little bit like an option of the hierarchy between these different approaches? For example, if I say supervision, this would be better to tackle this kind of problems. And yeah, self-generative and reinforcement. Yeah, I mean, a little bit of a I mean, one way to answer it is to just look at the data that you want to deal with. So these neural networks, they take an input and they produce an output. So if you have data which has both, so say if you have data that describes club Yaus, which would be the input, and as an output, you have information. And as an output, you have information about what a particular Hodge number is for each of those. Then you have input and what was earlier called labels. And then if you wanted to learn the Hodge numbers, then you would approach that with supervised learning. Because you can train in a supervised way a neural network with that labeled data and then hope that it will. Data and then hope that it will produce good predictions on new cardios, on unseen data, and predict the right Hodge numbers for that. So, if you have labels, in other words, then your first course of action might be supervised learning. This would be a first. Yeah. And if you don't have the, so if you want to learn a Clabiau metric, then your input data is really the points that you dissolve the Clabiau into. Into right, so the input, so the in this case, the neural network is the metric, and the input to the neural network is a point on the Club Yau. And the would-be output or the hoped-for output is the metric, ideally, the Richie-Flat metric. But of course, you don't know what the Rigiflat metric is. That's our trouble, right? So, in this case, you don't have the labels. Okay. Okay. Right, so that and that is the thing that points to self-supervised learning. And more generally, as Jim said, if you wanted to solve any kind of partial differential equation, then you don't know the solution, which in effect means you don't know the labels and you would be using that. And then if you have this kind of environment scenario where you don't really care about creating any data beforehand, but you Any data beforehand, but you do know the rules of the game, then you could go for reinforcement learning. I don't know if that maybe some others have better ways of answering that. I agree to really Andre said, but beyond that, if you're asking about architectures for addressing these problems, there are some works where certain architectures were successful. Where certain architectures were successful, but often it's just trial and error. But you know, oftentimes, many of these calculations are sufficiently fast on modern machines. The trial and error is a suitable way of attacking many problems. Problems that I've worked on, we've used trial and error in finding questions to ask and also designing how to answer those questions. Okay, thanks a lot. Okay, thanks a lot. Maybe since Andre mentioned solving PDEs, I'll just say real quick that different solutions to PDEs can have different scales of fluctuations associated with different frequencies. And one of the recent theoretical breakthroughs in the ML literature is exactly depending on sort of the frequencies and the problem that you have, tuning the architectures to be able to learn high frequency data or low frequency data accordingly. So that's indeed these self- Indeed, these self-generative techniques can solve arbitrary PDEs, sort of in principle, but in practice, they may not be that good and might require some tuning of the type that I mentioned. Okay. Thanks. Anyone else? Well, I'll take yes, Ignacio, go ahead. Yes, I have a Yes, I have a pretty naive question regarding the reinforcement learning environment. Like when you construct this environment and rules, I believe that most of the times in these cases, you don't have all the rules of the game. So, how can I trust the result of these reinforced learning machines? I mean, maybe. I mean, maybe the machine is taking a way of solving the puzzle that is violating some hidden rule that I'm not sure if it is there or not. Yeah, I mean, I think that's exactly what Fabian said. You do need to know the rules in order to do this properly. And where you don't, we really need some sort of theoretical work to clarify those rules. So I think. Rules. So I think the body-like stabilization example is a good case at hand because there is doubt over the various results, and we're not quite sure if you fed something like the KKLT scenario with all its rules into machine learning system, which I think might in principle be feasible. You might then end up with results. The machine will just follow the rules, right? But if the rules... Just follow the rules, right? But if the rules aren't quite correct in a physical sense, then of course you might end up with incorrect results in the same way that the model builder would end up with incorrect results when they follow the same rules. So I think in the works that I talked about, I think the rules that were fed into those RL systems, they were all pretty established rules. I mean, the rules of how anomaly can... Of how anomaly cancellation works, tadpole cancellation works, the rules for how you compute the spectrum of a string model, and so on. So I think in those papers, it was made sure that no dodgy rules were being used. Okay. Well, and regarding the scores, I believe when using reinforcement learning for playing games, there's some score there that is showing how many points you get for doing. How many points do you get for doing this or that? So, in this sense, how can I tell what score I should assign for this property or I don't know? Yeah, I mean, the rough idea is, of course, you design the score according to your goal. So, if your goal was to, say, to find something like a model with the standard model spectrum within a large environment, then the reward that you the rewards that you design would be would be the larger, the closer you are to achieving that goal. But how you do it in detail is in fact an art. And whether what you've designed will actually work in practice can very much depend on how you design that function. And I mean, I don't know if the others have a better intuition, but in what we've been But in what we've been doing, we've certainly just been looking very much poking in the details and playing with different possibilities until we got it to work. Okay, thank you. I will take the opportunity to ask a very, very naive question. So please don't take it so seriously. So we have ready. seriously so we have read in news a lot of about machine learning and for instance in in in medicine this is is very well used nowadays but usually they said for instance in an x-ray screen or whatever they said well machine learning is able to identify some patterns of cancer for instance but we don't know what is this pattern what is what is the machine or the algorithm what is finding What is finding what it's looking for? Is that happening in your case for models in string theory? That you have some result that you don't know? Why is the algorithm getting or identifying some pattern in this sense? I'm happy to comment on that. I don't know if Andre or Vishnu want to tackle that, but I mean, your question is about what's called interpretation. Your question is about what's called interpretability in machine learning. And people will sometimes incorrectly say, oh, machine learning is totally black box. We can't understand anything. And that's false. That's not sharp enough. It's not that it's totally figured out. It's figured out to different degrees in different sorts of circumstances. And indeed, purely in machine learning circles, this is one of the big important questions. How do we understand what the machine is doing? Machine is doing, and also under what circumstances do we care? So, for instance, with self-driving cars, you might imagine that if an accident happened and there was a lawsuit, there's a question of who is at fault from the perspective of litigation. That's a very important question. In the context of things that are happening here, I'd like to give two examples of places where the machine is suggesting interpretations. Interpretations. One is that if you sometimes use simpler algorithms, not just fully connected neural networks, you can see what the important variables are for helping the algorithm make its decision. So for instance, in a decision tree, there might be 10 input variables, and the trained decision tree might tell you, no, no, but variables one and two in this particular combination are the most important thing for making the decision. And so, because of that, that can place importance on certain inputs with respect to. On certain inputs with respect to the problem that it's trained on. And that can actually lead you to ask: you know, huh, why is it that that input is so important? And can potentially lead to a conjecture that you then prove as a theorem. And that's one thing we did in our 2017 paper. But something that happens in reinforcement learning that is also interpretable is that the trained agent can experience what are called rollouts through state space. That is, you can watch the agent play the game, and particularly when you yourself are a Particularly when you yourself are a domain expert, sometimes it can do things that are surprising that you can actually try to understand. And sometimes it does things that you already know. So, for instance, in chess, the best algorithms learn all the most famous openings, but they also play in ways that were surprising to grandmasters. And both of those were understood, both in terms of things that were already known and also suggestive of things that could be done in the future by grandmasters that would help them play better. And it actually has started affecting how people play. And it actually has started affecting how people play. In the context of string theory, in our first paper, there was one case where we were able to set up the rules of the game in a way that recovered a known strategy. And based on seeing how the agent played the game, we could see how the trained agent was becoming biased to do certain types of things over others that were better for achieving its goals. So that was also interpretable. And, you know, this is the sort of thing that you can do when it comes to interpretability. So it's Can do when it comes to interpretability. So it's not true that you can't say anything, but it's subtle about how to do it in any given case. Yeah, maybe to add one example, which is the one that Vishnu mentioned earlier, which I think is really important in algebraic geometry, which is one of the tools we use in string theory, it is actually typically very complicated to compute certain quantities, such as homological quantities. And you do it sort of one. Of one instance at a time, and every single instance of these computations can be absolutely horrendous. And then you use some of this data and train a neural network in a supervised way. And you're surprised that relatively simple networks can learn this data. And this is very suggestive. It says that some of the usual computations that we do, they are probably too complicated, and there is something much simpler there. So there's sort of a generic hint. So there's sort of a generic hint there that you're being given by neural networks. And in fact, this hint has now sort of materialized in some sense in that we know that at least in some cases, there are very simple formulae for these things, you know, despite the horrendous computations that you would normally do. So to add one more example, in supervised machine learning, for example, with NOTS, you can assign relevant scores to each neuron. To each neuron that's in your network. And by looking at relevant scores for the inputs, you can sort of sometimes determine which inputs are crucial to determining what the output is. So in the case of Knox, for example, we looked at evaluations of the Jones polynomial at different phases. And it turned out that certain phases work better than other phases of predicting the volume based on those evaluations. It turns out there's a reason for that in Chern-Simon's theory that That the machine sort of discovered. In effect, instead of working at integer level, you're working at a fractional level. And it turns out the analytically continued theory is good enough to do that. And you can predict volumes that way. So that's an example where we can reverse engineer what the network is doing based on analyzing which features of the input are the ones that it focuses on in making a prediction for the output. I see. Thank you. I see. Thank you. Fabian? Yeah, I just wanted to add to that. I think you can maybe make a little bit even bolder converse claim. So if machine learning works well, this tells you that there is some symmetry that the machine learning algorithm is picking up. So if you were just trying to train a neural network, let's say I was just devising a Was just devising a completely random function where I assigned to the number one, I assigned the number 17, the number two, I assigned a square root of minus two, and so on. And then I want to predict what pi is with this network. If all of this was completely random, there was no pattern, your network wouldn't have anything to pick up or to go by. And then to predict what the random label that you dreamed up for pi would actually be. So I think just the fact of things working or not working tells you that there is some structure in your data. There is some structure in your data. What the structure is, and finding out what the structure is might be complicated. But if you have a machine learning algorithm, a neural network that works on string theory data, this tells you there is structure that's inherited from mathematical description of string theory that makes actually these algorithms work. There is no randomness. There's something that underlies it, some pattern, something that actually makes the network work in very specific cases. And then it's a very interesting question to. And then it's a very interesting question to ask: what actually can the neural network not learn? So, are there things that are completely random? For example, you could try to find the integer factors of, I mean, the prime factors of an integer with the neural network. I just give it a fuck, I just give it a number and ask it to predict the prime factors. We don't know a pattern for this. How this happens, I mean, we know sort of probability density, how many prime numbers we have in a certain interval, so it will hit the correct. So, it will hit the correct result with a certain probability density, and you could ask whether it follows this probability density. Then, if other than that, these numbers are random, you would have identified already that sort of the neural network would be doing the best thing it can. It would be guessing according to the Euler-Fire function or whatever. But if it works much, much better than you would actually expect from it doing it randomly, I think this might be taken as a very strong hint there is actually a pattern that it picks up. And then the question would be, what is this pattern? And then the question would be: what is this pattern? I see. I see. Thank you. Thank you, Fajan. Hey, I've also got a suggestion or proposal. In the machine learning community, it's very standard to open source your packages. And I feel like this is maybe lacking sometimes of papers which have been recently published in our community. So I would put that forward as a proposal which we as a community can maybe improve upon. Can maybe improve upon and copy of a functioning system in another well subfield of science. Seconded. I mean, people, of course, feel free to discuss. Yeah, don't know. Just wanted to put it out there. Okay, thank you, Lauren. So if yes, Mehmet. I want to take 30 seconds to shamelessly advertise. We have this open source software package called CYTools that can make Clavia hypersurfaces in their ITs and compute intersection numbers and so on much faster than SAGE. I just wanted to mention this since we were talking about open source software, and I agree that. Software, and I agree that this is a very, very important point. You can maybe share it on the chat. Yes, I am doing that right now. That's nice. Great. Okay, that's it. Okay, well any other questions, comments, suggestions? In addition to, you know, since we don't have a theory of which problems are machine learnable in an easy way, and oftentimes people try various things if we're hit upon something that works, it may be useful as well to publicize experiments that fail. As well as publicize experiments that fail. This may give us some sense of where the demarcation is between problems for which certain tools are successful and certain tools are not. So another thing we might want to consider doing is just having a database of problems people have thought about and whether they've succeeded or not. And one doesn't usually publish papers on things that fail, but I think knowing what fails may be as useful as knowing what fails. Fails may be as useful as knowing what succeeds in determining when certain tools are applicable to certain problems. Mehmed, you have another question or is just your hand that is still I don't I don't know. Oh, I need to lower my hand apparently. Okay, thank you. Sorry about that. No, no, no, no problem. Just so let's see. So let's see if there is another comment, question. I have a since MetMAT has a shameless plug, I also have a shameless plug. So I, okay, so we're theorists, and I think the better that we can understand ML theory, the better it's going to help our techniques, and it's also going to help us bring our techniques to ML better. It is a wonderful thing that there's an NSF institute that supports this direction. And in advocating that we be better theorists in terms of Be better theorists in terms of understanding ML theory. There's a colloquium tomorrow linked on the webpage that I just put in the chat by Yasmin Bari from Google, and she's going to be discussing various aspects of infinite width networks and gradient descent training and how they correspond to kernel methods. If you're interested in theoretical aspects of ML, this is tomorrow. And if you sign up for our mailing list, I think you'll get the Zoom link tomorrow morning. It should be a great talk. So that's the shameless plug. So that's the shameless plug, and it's going to be a great theory ML talk. So thank you. I will take this opportunity to ask you another question. Maybe we can close this session with this topic. So of course, as community, you have your own problems, your interests. But also, people attending this workshop, also we have another interest. Workshop. Also, we have another interest more in the phenomenology of string theory and all that. How different are those interests? Do you feel that still your problems inside machine learning and the application to machine learning and string theory is still, let's say, in some sense, apart from the interest of the community in phenomenology? I mean, we are, and maybe it's the same question that we start with. We start with. We are close to solving phenomenological problems with machine learning. How far are we from that? I think we're there. I mean, in the sense that one of the papers I mentioned, we find new standard models, ones that were not known before and that had not been found with any sort of systematic scanning method before. Before. So, I mean, I would not see machine learning as something where a fraction of our community develops in a different direction. I would see it as a way in which string theory incorporates a new tool. And this is, in fact, this is what we're doing all the time. With mathematics, we've been doing this all the time, right? All the time, we need a new piece of mathematics to understand this and that. This and that. And now we have discovered that knowing a new piece of computer science is a very useful thing. And I think that's what's happening. This is not to say that some of us might not also develop some interests to do with machine learning that go away from string theory and maybe go into other directions of physics or even in computer science directions. But yeah, I don't see any conflict here. See any conflict here. Yes, thank you, Mandio. Please, we have still some minutes if there is another question. Well, maybe that was a good point to close with a general thing, but I will come back to a more technical thing. So, Jim, you were talking. So, Jim, you were talking about infinite wis networks. Can you say a little bit more about this? So just to understand what this is? Sure. Before we get there, let's consider a case where I have a random variable A drawn from any distribution I like. And then I'm going to take a sum of those draws into. Sum of those draws independently drawn from that distribution, and I'm going to take n to infinity, the number of terms in the sum goes to infinity. That sum is drawn from a Gaussian. And the main difference in the infinite with neural networks is that I'm going to, instead of saying sum of AI and studying that, I'm going to call it sum of AI HI of X, where HI is the post-activation of some deep neural network, and then I'm just adding up a bunch of these activations. So it's still an infinite. So, it's still an infinite sum. And indeed, in these cases, the central limit theorem still applies. So, if you, the infinite width limit is the limit that you take that sum to infinity. And so it is a neural network where the width becomes wide, but the central limit theorem still applies. And so the reason that that's significant, this is a result of Niels from the 90s, is that it means that the neural network as a function is drawn from a Gaussian distribution on function space. So that's called a Gaussian process. So that's called a Gaussian process. The reason that Neil was interested in that was because at the time and still, Bayesians are using Gaussian processes as models for Bayesian inference to do supervised learning. And Vishnu actually has a paper that uses this. And at the time, it looked like neural networks and Gaussian processes and Bayesian inference were totally different beasts. And they are the point of this work at the time in the 90s. Of this work at the time in the 90s was to say no, they are not different beasts. In particular, from a modern perspective, with some additional backstory, if you freeze all of the weights in your network except those at the last layer and you take the infinite width limit, gradient descent with that network is equivalent to Bayesian inference. Okay. Is equivalent to Gaussian process Bayesian inference. And so these are very different parts of the learning world that are coming together. The learning world that are coming together in the infinite with limit. The reason that it's interesting to myself and to some physicists is that Gaussian processes, we know one. Free field theory is a Gaussian process. And if you look at axiomatic field theory textbooks, you'll find that they have theories called generalized free fields, which don't necessarily have phi box plus m squared phi as the action, but might have some generalized quadratic action. And indeed, they're all Gaussian processes. They're Gaussian measures. And yeah, so that's. And yeah, so that's, maybe I'll let, if there's follow-up questions, I can go from there, but that's the basic idea. In the infinite width limit, the functions that the neural networks are are drawn from a Gaussian distribution on function space. Okay. Thank you. So then this is directly connected to what you were doing with this duality, maybe with QFT. Yeah, that's exactly right. So our paper from last August basically said: if at infinite n you're Gaussian, If at infinite n you're Gaussian, then at finite but large n, you have small non-Gaussian entities. And we know what that means in quantum field theory. That means that you have weak interactions. Small non-Gaussian entities in your function space density, in your action, correspond to weak interactions. And we develop techniques for addressing that. Okay, great. Thank you. Thank you. Well, I think this is a good time to stop here. This session. Thanks a lot to our speakers here in this session. Speakers here in this session: Andrea, Jim, and Bish Nunso. Thanks a lot. And if we finish this part of inviting us exactly. If you can advance two slides, Jim, if we can have one more shameless plug before we close. Yes, hold on. I actually just stopped share. Let me correct my error. All right. Yes. So there's a string data meeting. It's an annual meeting now. Data meeting. It's an annual meeting now. It's had several iterations. This year it will be online because of the current world situation. But there'll be lots of, I hope, machine learning and meeting. So the link is there. I'll put it in the chat as well. Nice. Thank you. Thank you again. And now I just I just Saul is there. Yes, yes, go ahead, please. Thank you. Thank you. Thank you, everybody. And now it's the time for Damien Major Gapea to let us know what he knows on machine learning. So, are you there, Damien? Here, here. Great. I heard you, but now I don't see you. Heard you, but now I don't see you. Maybe it's my connection. I don't know. Are you there, Damian? Yes, we can see him. Yes, I'm here. Yes, I'm here. As you can see him, I don't see him. So, well, I don't know what's going on. Anyway, so can you please share your slides? Yes. I don't know what's going on. I don't see him. Ah, now I see you. Okay, did we it's not working? I see only Andre Lucas on my screen. I don't know why. It's kind of the configuration of your Zoom. No, no, it's no, now I see you, Nana. Now I see you. Onion is the one I cannot see. For a reason. Who knows? Anyway, so are you already sharing, Damien, or not yet? Or not yet? Ah, yes, now it comes. Wonderful. Now I can see.