From Wisconsin, who's going to give us a view on the shape of the SARIA training. Great, thank you. Thank you so much for the invitation. And I changed the title of the talk because, in fact, my goal today is to convey kind of a high-level picture of a series of problems that may be of interest to you all. Okay, so let's get it started. So, we already heard two talks about machine learning two or three before. And what I would say is: okay, neural networks are great. However, Okay, neural networks are great. However, and this is one of their big flaws, is that they're extremely sensitive to adversarial perturbations of data. Okay, so then this is the main problem that I'm interested in at a high level. So what does this mean? It means that although they are very good at predicting clean data, so you provide like clean data, you don't modify them in any way, they are very accurate and they can generalize well. But if you construct like very structured but small modifications of But small modifications of these data points, which look exactly the way, I mean, the same way as they were originally. So, for us, there is no difference between the first image and the second image. But the point is that for the neural network, it does make a huge difference and the accuracy drops immediately. Okay, so then essentially, these systems are very good with clean data, but extremely brutal against perturbations by adversary. So, of course, you can imagine: well, maybe this is. So, of course, you can imagine. Well, maybe this is something that you can do in the computer, but is this something that can be transferred to real life? And the truth is that, yes, indeed. So, you can actually physically print patches and put them in stop signs. And then you have your image recognition device, and the thing is going to still perform poorly. So, this is something that we should all care about, especially if artificial intelligence is going to kind of be spread around, especially if you are driving. Um, especially if you are driving or something is driving the car for you, you don't want to hit against a car or something like that. So, I mean, in fact, this is what people do. I mean, they design patterns on cars and then they say, are they are the kind of, you know, the systems, the AI systems, are they fooled by this perturbations of the data? In this case, would be you put a pattern on top of a car. And the truth is that. Of a card, and the truth is that yes, they misclassify, so maybe you cannot read it from there. But instead of predicting that the green thing is a card, in this case, it's predicting that it's a cake, I think, which is pretty absurd. And so then the question is, I mean, again, this is a big question. It's not about loss minimization, it's about making AI safe. And I think, again, this is something that we care about, we should care about. But of course, you know, this is something that computer scientists care about. This is something that. Anti-scape care about this is something that maybe statisticians care about, but there is also something that as mathematicians really should care about. And so, maybe just to make it concrete, I'm just going to take the working definition of what do we want to do when we want to train models to be robust to adversarial perturbations of data. And so, in fact, this definition that I have there is coming from a group of computer scientists at MIT, maybe about six years ago. And then they say, instead of minimizing And then they say, instead of minimizing the standard risk problem, which is what you would do if you don't care about adversarial perturbations, you're going to put in front the loss function, you're going to put a supremum. And what is the supremum modeling? It's basically modeling the possibility that a data point X is perturbed within a small radius around the original data point. So you want to make sure that the models, in this case, represented by these parameters data, are By these parameters, theta are somehow insensitive or less sensitive to these perturbations. Okay, so let's imagine that this is the thing that we want to optimize as opposed to the standard and classical risk. Now, in terms of pictures, what does this imply? Like adding this kind of supreme one over here, what it means is that you not only care about mismatch between what you want to predict and what you observe. Let's say that what you want to predict is the light colors in the background and what you observe. Light colors in the background, and what you observe are the data points with colors. You don't care just about mismatches. You now are going to care about potential kind of boundary effects. So you want to care about what is happening at the interface between one class and the other. So not only you penalize mismatches, you should also penalize something that has to do with perimeter. Okay, so that's just a small picture. Now I'm going to introduce another model for adversarial training, which Training, which is instead of looking at a pointwise perturbation as in the first definition, we're going to imagine a situation where we now explicitly mention an adversary who is playing a game against the person who is training this system. And in this game, the adversary can choose a strategy by selecting a new data distribution that we're going to call mu tilde. And so basically, there is a clean data distribution mu, and then the adversary cancels. And then the adversary can select a mutilde. And the goal for the adversary is to make the risk worse for the learner. But in order to do that, it may have to pay something for it. We have to keep the constraint that this perturbation should be small. So then in this case, we're modeling that by saying, okay, the adversary will have to pay something to be able to make the change. And so now suppose that you work with this definition and suppose that this data here represents a neural That this data here represents a neural network, for example, but it could be anything at this stage. And you, of course, would like to say, Well, I don't want to kill myself if I'm in a self-driven car, so I want to solve that problem so that the model is robust to perturbations. And so you ask the most basic question, how do you find solutions to that problem? And if at least if you cannot find solutions to this problem, you cannot prove that you have reached a global solution, et cetera, can you at least find upper and lower? Can you at least find upper and lower bounds that are meaningful and still can guide kind of the choice of a robust parameter? So, those are questions. And now, going back to the title of the talk, at the end of the day, what I want to illustrate today is a little view on how to characterize the geometry of this problem. Okay, so that can be done in two ways for the two players. One is how do you understand what's happening at the level of the learner? What is happening when you add an adversary? What is happening when you add an adversary to the problem with the decision boundaries of your classification rule? And also, what is the geometry of the adversary attack? What is the attack you're trying to do? Okay, so this is what I want. And now I will give you two results in that direction. One kind of like saying something about the adversary, another one saying something about the learner or the classifier. But in the end, I want to solve the problem for the Uh, you know, solve the problem for the neural network, but I'll do something first, which is uh, maybe consider uh models, learning models that are pretty general. And I'll make sure I'll explain what I mean by that, that will simplify, certainly simplify the problem. And eventually, I will return to the parametric problem. And so, for those of you who have seen maybe some of this before, maybe the last part of the talk will be the new thing. If you haven't seen this, well, then no worries about what I just said. Worries about what I just said. Okay, so let me describe one setting. So I'm going to call it the agnostic learner setting. So again, I've also spent time with machine learning people, so they use this terminology. So agnostic learner, what does it mean? What it means is that you are, as a classifier, you are allowed to pick any function that you want to classify without any restriction other than you have measurability. Okay, so it sounds a little bit absurd that you can't distinguish any sort of data. Can distinguish any sort of data, but it's a meaningful object, even without the adversary. This is what in statistics is called the Bayes classifier. If you're not doing anything robust, in this case, it would be the Bayes classifier version for the adversarial problem. So for theoretical reasons, it is important. Later, I will tell you, well, this is not only important, this gives you lower bounds. And then later, towards the end of the talk, I will tell you this can also help you get an algorithm for the parametric problem. So I understand. For the parametric problem, so I understand I'm kind of like going to an abstract setting, but it will be uh hopefully rewarding. So, now, okay, so the type of data that I'm going to be considering is you have features, so those are representations of your data points, and labels, just imagine these are colors or just you know classes. And then loss function here is very simple. If you predict something and it coincides, it coincides with what you should predict, then you don't pay anything, otherwise, you pay one. Otherwise, you pay one. This is the idea. And this is the way that you should imagine your data. So there are points in space with some colors. And now I go back to my formulation of this min-max game, where again, I have the risk and then this cost. I have to tell you how am I going to choose this cost. So this cost is going to be an optimal transport problem. But you have, in this case, the coordinates are X and Y. So there's a feature and then there is a label. I'm going to. Label. I'm going to put this restriction here on the cost. This basically saying the adversary cannot change labels. It can change features, like in the images of the panda, but the panda continues to be a panda. It's not going to change that. So somehow here, formally, we can just write it in this way, and that should be capturing that. So now, when you put that constraint, then everything kind of becomes just everything is happening at the level of each of the conditional. The level of each of the conditional distributions of your data. So you just have to look at what happens with each color. So you're going to be allowing to perturb each color separately. That's what really is happening. And one thing that I should mention, this comment about the agnostic setting versus something else. Notice that if I allow any measurable object there, well, I'm taking an info or a bigger set than if I restrict my class of learn. So this is. My class of learn. So, this is this problem here, the agnostic problem, is always going to give you lower bounds for any model that you can consider. That's why it is agnostic to the learning model. Okay, so right now, the most important example of this cost function is the one that people in computer science would care about the most, which is the one where you say you don't pay anything, or the adversary doesn't pay anything. Or the adversary doesn't pay anything if it moves data within a ball of radius epsilon, otherwise, it pays infinity. In other words, it can only move stuff within a ball of radius epsilon. And so, you know, the reason why this is an important choice is because when you make that choice, that recovers the problem that was introduced by the computer scientists. So, this is the problem that people would call actually adversarial training. So, in a sense, like this kind of rewriting of Kind of the rewriting of the problem in terms of this explicit min-max game between a learner and an adversary is more general and it captures the thing that people care about. And now what is interesting about that formulation is that, in one picture, I can describe what the adversary is trying to do. And this will lead us to the first characterization of the geometry of the attacker. So, what the adversary is trying to do is you can imagine these distributions, mu1, mu2, mu2, These distributions, mu1, mu2, mu3, they correspond to the conditionals of class one, class two, class three. These are the clean data points. Now, what the adversary wants to do is make these classes overlap as much as possible, because when you put different classes on the same spot, this is the place that is more difficult for the learner to distinguish. If you have to say which color is it, and there are three different colors that could be predicted, then you're going to have more trouble than if there was no mixing. No, you know, mixing. Now, of course, the adversary has to pay. So, to move things, it will have to pay. So, of course, there's going to be this balance between not moving too much, but at the same time, reaching as much overlap as possible. So, that can be represented mathematically in that problem over there, which I'm calling a generalized Clari center problem, which essentially is saying what I describe in this picture over here. You want to move mu i to mu i tilde, but mu i to mu i tilde, but you somehow want to have big overlap in this case, small overlap, sorry, in this case represented by this measure lambda. So it's a generalized barycenter problem because instead of like targeting one single distribution, so all measures going to one single distribution, you're just going to a part of one distribution. This is kind of one way to interpret it. And now what we prove with Matt and my student, Jaguar Kim, is that in the same way that this paper by Agra and Carlier 20. Paper by Abu and Carlier 2011. This generalized vary center problem can be realized as a multi-margin of optimal transport problem. So there is a way to write that. And I'm not giving you the MOT problem because maybe that's a little bit more cumbersome to explain, but there is a version of that problem that looks like what we call the stratified vary central problem, which is, it looks more like a very central problem. If you're familiar with those, you would recognize. Familiar with those, you would recognize this type of cost function over here. And in this case, this couplings pi i, what I'm representing is what colors you're picking. Okay, so what groups are, how are you grouping the classes and how are you merging them together? This is kind of like what you're what you're trying to do. And of course, you still have like these marginal constraints where everything that you aggregate for each of the colors should correspond to where you're starting. Okay, so this is the actual theory of the way it reads. Know the actual theory of the way it reads. Essentially, what we say, and this is what the first main point that I want to describe here is that the agnostic adversarial training problem is equivalent to one of these MOT problems. And the consequence is that you can use computational optimal transport to find one of them. This is kind of the idea number one that I want to describe. And you can use that to understand the optimal risk, et cetera, et cetera. Okay, so I need to. So, I need to speak up a little bit. So, that's the part that I wanted to describe about the geometry of the attacker in the agnostic setting. So, again, the main idea is this is equivalent to a generalized problem or MOT, what it means for practical purposes, computational optimal transport. That's what it is. Okay, second part is I'm going to restrict to the binary case. And so, in the binary case, what happens is if you Binary case, what happens is here we're only going to think of two colors. And I'm going to try to describe a little bit about what's happening with the learner. And so, what is happening with the learner is that you can actually rewrite this adversarial training problem, the original one, as a standard regularization problem where you say, I'm going to try to make my, you know, mismatch with the data as small as possible, but then I will be regularizing, and I will be regularizing in such a way. And I will be regularizing in such a way that I make my decision boundaries as small as possible. And well, we, how do you measure the size of your boundary if you have some notion of perimeter? So there is a notion of adversarial perimeter, if you like, that plays the role of regularization. And this corresponds to these two pictures that I showed you at the beginning, where I was saying, well, now that you have an adversary, you also are trying to make the decision boundary small in some sense. So we, with Leon and Ryan, we prove this. Leon and Ryan, we prove this result. We identify what is the non-local perimeter that's non-local. This is the only non-local that this talk has. And you can characterize it. So it's kind of like this Minkowski type total variation, if you like. And the perimeter would look like this, where the measures that you use depend on the distributions of the different classes. We have something more, and this is. We have something more, and this is more interesting. And I will use it in the last part of the talk, which is you can also characterize solutions, or not characterize, but let's say find that there are regular solutions to the adversarial training problem. So the first statement would be something like this. You can start, first of all, you can prove that there exist solutions. This is not so trivial. There are measurability issues, but those ones can be overcome. The important thing is that you can find what we Can find what you would call epsilon in irregular, epsilon outer regular, a minus and a plus that are solutions, and that anything in between would also be a solution. So in irregular, that you have this inner bulk condition with radius epsilon, outer regular, that you have the same thing from the outside, and that anything in between is a solution. Well, at least if you have, for example, Euclidean balls, you can get something about the existence of regular. About the existence of regular solutions. In fact, the original problem may not have regular solutions at all, but automatically by introducing the adversary, you can always gain at least some regularity. Okay, and then this is a very simple result in the sense that we use relatively soft arguments, but it's one concrete setting where we can say there exist regular solutions to the problem. And you'll see in a moment why do I care about that? So, maybe this is because this last part is coming. So, so far, what I've told you is you can use a computational optimal transport to solve the agnostic problem. And in fact, you can use it for both the adversary and the classifier. And the second idea that I'm telling you about is that at least in the binary case, there is some regularity estimate that you get about solutions. There exists some solutions that are regular. Now, how is that useful? How is that useful? So let's go back to the parametric problem. So now we are going to imagine that we don't have this agnostic setting. Now we really are working with neural network, linear models, something that people use in practice. We want to solve that problem and we want to solve it. We want to know how to solve that problem. So the way we're going to do it is we're actually going to turn this min-max problem into just a standard race minimization problem of this form for some distribution. Problem of this form for some distribution that may not be the same as the original clean data distribution. So, what I'm going to tell you about is how I'm going to construct this mu A and how I can combine some of the results that I just mentioned to essentially guarantee that I would be able to produce a neural network just using the standard training mechanism to be able to solve the adversarial training problem. Okay, so let's discuss that. So, imagine that you have, this is your input data. Have this is your input data, it's binary, so just two colors. This is your input. We're trying to train a model to be robust to adversary. So the first step is I'm going to solve my agnostic problem. So this is what I do with computational optimal transfer. So what that is going to create is it's going to give different colors to the original points. It may change some points. Okay, so some colors may be, I don't know if it was perceptible, but maybe some colors became blue and some red became blue. And some red became blue. So we solved something. We can do this, let's say, OT, okay? Computational OT. Then the next idea is to say, remember the second result that I told you about. So you can start with a solution to the agnostic problem, and you can construct these sets that have certain regularity, and you can try to use that for the following purpose. So in this case, this set A minus and A plus is going to be unions of balls. And now what the theory would say is that if you have What the theory would say is that if you have a set that essentially contains, for example, all the A minuses, but does not contain all the A plus, that would be a solution. This would be the result that I showed you earlier. So then essentially, this provides the following algorithm. You can say, well, now I have my A minus, I have my A plus, I can find this, I know what these balls are. And I can say, well, I'm just going to fill the space in these balls with more data. I'm going to do some data. These balls with more data. I'm going to do some data augmentation. So, what that means is I'm just going to add more and more data. So, the blue balls are going to have more blue points, and the red points are going to have more red points. I can do that. I mean, whatever my computer allows me to do, I'm going to augment the data. And now what I'll do is I'm going to train my usual model on this augmented data set. Okay, so then I say, you know, the last step is just use the regular. Last step is just use the regular standard algorithm, the training algorithm, to just solve this problem. I mean, this is what most methods would do. And then, you know, the answer, if this thing is able to find a global solution, my claim would be that this is a solution to the problem. Now, the question is, why does it work? Or I mean, why does it have a good chance of working? Let's put it like that. There are two ways to discuss this. To discuss this, the first one is because actually, in practice, there is one algorithm that people use that is not so different from this, except that they don't have the OT step. They cannot prove anything. They cannot prove the correctness. They work with the original clean data. And so what they do is they do this data augmentation and then they do the empirical risk minimization. And then they do numerics and it works fine. What we are saying is, well, we have a way to actually justify why it works. And this leads me. why it works and this leads me to the second paper that i'm listing here which is um it has to do with approximation results of neural networks so and this is the the part of uh of regularity that becomes important if you have a regular function and you want to approximate it with a neural network of a certain size you have precise estimates about how many neurons do you need to be able to approximate that function in other words our result would say something like if Would say something like: if your neural network has this number of neurons at least, then provided that you know you can find the global solution, so then you have to cross your fingers that the optimization algorithm actually finds the global solution or something reasonable, then we would be able to prove that this is solving the adversarial training problem for the neural network, because in that case, it would coincide with the lower bound. And of course, if you have, if you coincide with the lower bound, then you're also a minimizer within the class of neural networks. Okay, so this is what we would do. So, this is what we would do. And so, I don't have too much more time, so I'm just going to wrap up. What I presented today is three ideas, essentially. One is about how to use computational optimal transport to at least solve one sub-problem of this adversarial training problem. The second idea was it would be great if one could show regularity of these solutions, because if you can do that, then that leads me to the third idea, which is you can use those regularities. Which is, you can use those regularities to connect with the approximation theory of neural networks to be able to justify that what you're doing is provably mathematically doing what it should do. And this is, I think, quite meaningful for the machine learning community. So I have been working on this topic for about three years. Most of the work that I presented was part due to Matt and Ryan. To Matt and Ryan and Leon, and a lot of friends. We have more papers actually out. One recently, I also work with my brother on this stuff. There's a lot of adversarial friends. It's great to explore the mysteries of these things. And that leads me to my last slide. Thank you so much for your attention. Thank you, Nikolai, for a nice talk. Questions? Comments? Have we tried to implement the algorithm? Not yet. It's in the process, but okay. Questions? Just a basic question here. When you say it is algostarial. I'm just saying uh when you see this white noise that you're adding to the is there a standard way of of doing it I mean there is a there is a ah yeah there is so that one so I mean and this is why there was a formula that I didn't mention uh and this is exactly so you see that there's a sign of what they're doing there is imagine that that j is like the loss function and then you're taking so imagine that your primary Taking, so imagine that you're parameterizing this with a neural network, for example. And so, what you do is you take a gradient with respect to the data, not with respect to the parameters, in the ascent direction. You're trying to make the loss worse. For one good reason, you take the sign. So you just care about the sign of where the gradient is pointing. But this is what essentially people do to build these attacks. You could also do it targeted. So, in other words, there's something called backdoor attacks. called backdoor attacks, which is maybe you have a lot of classes, but suppose that I want to, I'm an adversary that maybe I'm okay that you predict well other classes, but suppose that I want you to perform poorly in a specific class. I can also, one can also do that. And so in that case, it would be very difficult for you to detect that I have attacked you because you would only see it if you knew what class I was attacking, for example. So there is a lot of stuff like that. Like that. And yeah, I mean, it is all important. I also should say that this is the case for supervised learning. I think the most important example is actually the reinforcement learning setting because that's the one that is going to drive the cars will be on. Any more questions or comments? I have a curiosity on the last comments that you made about the oxygenation with the neural networks. In which kind of normal In which kind of norms, or in which sense, and what are the estimates of that norm? Right, so for example, when Stefan mentioned some of that, and in fact, in this paper that I mentioned at the end, this is for sets, actually. So, it's about the regularity of the boundaries of the sets. So, what they would say is like that this, you need to essentially control the baron norm of the, you know, locally. Of the, you know, locally of your decision mark. And so it is in that setting. So then essentially, anything that one can say about regularity of these solutions would, one would be able to, presumably one would be able to connect this. It would give you like a lower bound for how many neurons do you need to be able to approximate that to some accuracy. Yes. Okay, thank you. So there are no. Okay, thank you. So if there are no questions or comments, just thank people again. And we will see them in thirty minutes.