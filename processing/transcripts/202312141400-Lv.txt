So, today I'm going to share with you a recent work of mine with my collaborators, Ying Ying Feng from USC, who is in the audience, and Zhou Ming Jing and Xing Zhou from USTC. The title of the work is Safari, High Dimensional Manifold-Based Inference. So, I want to mention a few keywords here. So, essentially, we're trying to do inference for Inference for learning algorithms, so-called so far. So, actually, Way was one of the co-authors of the earlier paper. And the view we took in this project actually through the manifold view, so which I want to share with the audience here. So, first, I want to explain a little bit of background why we as a team were interested in this problem. And of course, I will mention in the details of algorithm Safari. Of algorithm so far, followed by theory, and given a time limit that probably I will stop naturally whenever time is up. So here I want to mention one of the applications we had in mind when we were starting the project years ago. So oftentimes we work on real applications, right? For example, you think about brain science, memory, and also these days people talk about auto-driving, right? Auto driving, right? So essentially, in a system, on one hand, not only you have a lot of input features, right? Which, for example, could be images, videos, and audios, and all the external features. On the other hand, we also have a number of tasks simultaneously. Just imagine if you want to sit in auto-driving car, of course, you want a system to be comprehensive, reliable, right? Meaning that you're not just focusing on one single target, but you're trying to sense the surrounding environment. To sense the surrounding environment, interaction, and so forth, right? Think about the example of memory. As we know, that when you go into a talk, of course, you hear a lot of ideas and information, right? Suppose you heard about a number. Of course, the numbers basically record information, hold information for a short while. And most likely, out of end of day, not out of the same chain, when we'll forget most of the information numbers. Forget most of the information numbers, right? So, the reason is, um, we also have the memory state, for example, with very, very short time spin, for example, like one second or two seconds, all the way to a few minutes or to hours or even days. Okay. So, so which means in this application, we have two sets of variables. One set of variable is called memory states. You can argue that memory state could be discrete or even continuous. It's up to you like neuroscientists. It's up to you like neuroscientists to argue, so which way could be the better way to model, right? But we know that we have short information, medium-term information, and long-term information. On the other hand, for every memory state, it's actually encoded in a different part of the brain. For example, if you look at two seconds, this is actually reaching in the brain, but for a long-term memory, it's a different part. If you happen to have a little bit of neuroscience background, you know that the shorter memory. You know that the shorter memory actually is encoded in the frontal part of the brain. It's, for example, like when you hear a number, it goes in from the front, and some information will be retained and held as long memory, and long-term memory actually in the back of the brain. So the question is, how do we understand the complex association network between two sets of variables? One is memory states, and the other one is the And the other one is the brain regimes. So here's, of course, scientifically, we can ask several questions, right? For example, how would every memory state interact with brain regimes? The next question is, how would we make the key memory components, do we only have, do we human beings only have like one short memory plus one long-term memory? Or there could be more complicated memory capacity, including human brain. Capacity to include the human brain. So, actually, a number of years ago, a group of authors, I was planning to work with several neuroscientists. And through the study we did in a neuroscience setting, one interesting found is through the data analysis, we found that there's actually four components total. So, two on the fast side, one in the middle, and one slow. Two fast means the two regions corresponding to shorter memory. For example, if you kill a phone number, Example, if you hear a phone number, it might be able to memorize for two seconds or one minute, but sometimes it's very easy to forget up to one hour or even one day. Okay. So this gives us basically like the motivation. Even for this example, we're trying to understand the complex association between memory states and also the brain activities. So ideally, we want to decompose the complex structure into The complete structure into different layers. For example, layer one, layer two, layer three, and every layer would contain the underlying association. So, in this case, the layer three memory component. For example, if the study recover like four components, that means we have four layers of memory encoded in the brain. The next question is: for every layer of memory, so how are the memory components mapped in the different brain regions? All right. Brain regimes. All right. So, plus, you know, if you look at across different layers, for example, from short-term memory to medium memory and also long-term memory, the structure could be different, right? Because different parts of the brain can be activated. So in this work, actually, motivated by this application, we're interested in the inference of a layered latent response association, a predictor association problem. But what we found actually is a problem. But what we found actually is a problem is non-trivial because underlying the latent manifold structure, which is going to elaborate a little next. All right, so given the application I mentioned, probably you have some quick idea in mind, right? So for example, so since we have two sets of variables, a natural tool would be something for multi-task learning, which has been a lot of interest in machine learning and statistics over the past many years. And so in this case, so every And so in this case, so every task represents a distant memory state. For example, memory with time constant one second, two seconds, one minute, and so forth. All right. And we know that the advantage of using multiple path learning is we can potentially exploit the dependency across different responses. And then presumably, across different layers, some natural structure could be shared, which means if you jointly do inference, Doing inference, potentially we can gain efficiency. And we all know that multi-response regression model takes very simple form. So, here, to simplify basically the technical presentation, I'm going to just mention the linear setting. Of course, if you're interested, you can always consider different extensions, right? So, in this case, on the left-hand side, we have a big data matrix. It's M by Q is a response matrix. N means number of subjects. A means number of subjects recording in study, and Q means number of responses. In a brief example, do we have different memory states with different time constants from one second all the way to even dates? X here will be another big data matrix, which we call design matrix, and individuals for P features. In one example, we know that in human brain, easily they do like 100 times 100 times 100, easily it's a million voxels. Is there a meaning of voxels? And C would be the coefficient matrix scientists might be interested in. So essentially, it tells us how different, for example, memory states would interact with brain regions, of course, the error matrix. Okay, so how do we model structure I just mentioned? So ideally, we want to have lineage interpretable response, which is Y versus X, which is pretty. Versus X, which is predictor association network. We want to have flexibility across different layers. And we know that mathematically is a very beautiful object. Something we use, some single value decomposition, because given the P by Q coefficient matrix, we can easily do S B D composition like this. We have three matrix here in the middle. We have the diagonal matrix of singular values. So, right, so we know that these single values are all non-negative, like D1, D2. non-negative like d1 d2 and also on the left hand side we have um so uh we have the left singular vectors on the right hand side we have the right single singular vectors matrix um here everything is color coded just imagine for example the first layer could be like shortened memory the blue layer and which is given by d1 so the singular value signifies the overall contribution of particular layer just imagine A particular layer. Just imagine most of the day when you think about memory, so lots of activities actually is happening in the front part of the brain, which means lots of information going to the short-term memory, which means a lot of variations in neuron firing. On the other hand, some information go to long-term memory, which is actually less active. And for every layer, ideally, we want to have interpreted structure. For example, for all the brain voxels, we want to have a sparse set of. We all have like a sparse set of voxels. For example, this could be shorter memory. And on the line side, we have blue backer here. Again, we have sparsity because maybe the sport memory stays for like one second through maybe five minutes. We can call it short-term memory and then we can go to different layers. All right. So this is sparse SVD structure, the main target we're trying to infer on. Of course, there's lots of structures that have explored in the literature. For example, Loire Rankin's way, which Literature, for example, low-rankedness rate, which means if you see low-ranked matrix, which means that in this case, mid-human brain, we only have a small number of memory components according to the study published in recent years, could be a small number. And as I just mentioned, the interpretable response-produced association network actually is naturally included in a sparse left-singular vector and also sparse right-singular vector. So, this explains why. So, which explains why the model is actually quite appealing. We know that the structure, as you mentioned, the multi-response regression with latency SV structure actually contains a large class of classic multivariate learning methods. For example, if you're trying to do like the damage reduction and visualization, oftentimes we do BCA and SVD. So, this can be functional. So, this can be formulated on this framework. And also, if you want to do sparse vector analysis or sparse vector autoregressive analysis, naturally it belongs to this category, which means we have two sets of features. As I just mentioned, in this case, we want to have two features simultaneously. Although the which is good for interpretation, right? Just imagine if you want to communicate to a neuroscientist, say, I have two layers, but supposedly two layers actually. But supposedly, two layers actually the single vectors, if they are correlated, which means it's hard to parse, right? So, I didn't want to do orthogonality across different layers. On the other hand, if you work with scientists, you know that interpret is so important. You cannot just give like Blex model and with prediction. Oftentimes, you don't want to get insights. So, which means sparsely here would be ideal. And we know that if you're trying to do, for example, That if you're trying to do, for example, like generate orthogonality, there's many tools you can use, right? For example, you can do QR determination, but once you do that, mathematically, the sparsity cannot be retained and vice versa. And that's why a few years ago, the group of researchers actually came up with a learning algorithm, something so-called a sparse orthogonal factor regression algorithm, SOFAR. And I believe we also partly are responsible for coming on the name. Still remember the lovely conversation years ago. All right. So basically, so far provides efficient learning algorithm for covering the latent SVD structure, which I just mentioned. Think about green example in mind. So, optimization problem for SOPARN actually is quite intuitive. So, here basically, You. So here basically, our target is trying to get a triplet. Like this is singular values, left singular vector matrix and right single matrix V. And of course, our, you know, since we're working with multivariate multi-responsive linear regression model, we have the sum of squares at this loss function. But to interpret low-rankedness, we work with summation of singular values, which naturally promotes the low-rankedness. Promotes the low-rankedness. And for encourage the sparsity in both left and right singular vectors, what we found is actually it's good to weight other layers by the singular values, which means for u, left singular vector matrix, we multiply by d, the singular value, same thing for v. We found that if we do this one, estimation is more natural, and also average development is also natural. Also natural. So, in terms of interpretation, in the end, whatever layer we cover, we have to attach some kind of importance, right? So, naturally, single value would be a good matter for importance, but ideally wants to derive the significance, which we're trying to do in this project. Certainly, in order to maintain the interpretability, we want to have the orthogonality among the left and right singular vectors. Singular vectors. All right. So, despite results in the multi-response literature, most results actually focus on the estimation part. The inference actually is more intricate for the reasons I'm going to mention. So, here I want to mention a brief numerical example. So, ideally, for the Spark SVD structure, right? So, we have different layers. For example, the top panel would be the first layer. Just imagine. Panel would be first layer. Just imagine this could be fast component memory. Second one could be the next layer. So, ideally, for example, for first layer, we have left singular vector, of course, have different compositions, right? Just imagine first entry. If you look at the estimatory results, the distribution of estimates given by the blue curve, we know that is never ideal, right? So, but looks like it's pretty close to the reference matter. The reference matter, we are also familiar with standing Gaussian distribution. Same thing for another component of the left singular vector, and also the singular values d1 and d2. So this is actually given the Sufarian algorithm, which I'm going to explain next. The natural question is: based on improved observations, we have good imperial features, right? But can we explain this theoretically? So, our team actually had a few. team actually had a few simple questions in mind. Number first question is, because our goal is trying to do inference for latency SVD structure, right? The question is, how do we go beyond the including space for the inference problem? Because we're so comfortable and familiar with taking doing inference doing optimization in the including space, including, for example, capital ingredient and high-order derivatives, right? But the question is, sometimes question is sometimes if you go with slightly different view it might be an interesting surprise and a payoff and second question is how can we you know utilize the underlying manifold structure given by this very specific problem because we're trying to recover sparse SVE structure of course we have sparsity we also have the orthogonal constraint and the question is how can we inference for life effectiveness How can we inference for left-hand vector and also simple values? And next question is: We know that since we're trying to encourage sparsity, you might imagine that we'll use localization, right? So naturally, there's bias. The question is, how can we get rid of bias? And turns out this is a part that puzzled us for a while because we're trying to de-bias for the so-for estimator. But it turns out we've got many ideas. But turns out we talk many ideas, didn't work well. I'm going to mention a little bit more detail in a second. All right. Of course, we want to study theory and also we want to provide the variance estimation in order to implement the idea in practice. Okay, so this is the model actually we have in mind, a multi-response mural model, and we're trying to infer the SVD structure. Okay. The difficult problem. The difficult problem is just imagine if you're trying to do inference for the first left singular vector, all right? Again, the exception is a long vector. And if you want to inference for this one, because number one, localization has bias. Number two, there's so many other parameters because we have many other left singular vectors, and also we have the right singular vectors. So everything else can go to the other category, something called nuisance parameters. Other categories, some concentrance parameter, but the nuisance parameter actually is highly dimensional and also highly complicated. In this case, what part of us actually is how to construct the device mechanism and turns out the manifold here actually, before we realize the issue, we got in a lot of trouble. But once we utilize the manifold view, it turns out everything comes out quite surprisingly clean. Here, I want to mention one. Clean. Here, I want to mention one reading word because the multi-discrown regression with SF destructions I mentioned actually encompasses so many ideas, right? Think about specific example PCA, but even for sparse PCA, only a few users ago, Javonka and Van Guer actually provide inference for the leading eigenvector and eigenvalue for the SPAS PC setting. But if you think about what my second one is, third one, and turns out during that, And turns out, doing that actually is something non-trivial. So, our main idea is actually trying to combine two ingredients. Our first ingredient is actually something probably you can already aware of from literature, which is actually the classical naming near negativized idea. So, essentially, this is heavily used in the recent double machine learning paper. So, we're trying to construct a score function. To construct a score function that satisfies the memory of orthogonality approximately, that's why we have the close to orthogonality. And second one is actually a managed view, which I'm going to mention next. Typically, we're trying to do devicing, essentially, we're trying to construct a score function that is locally insensitive to the losing parameter. But the punchline is usually people do it in the full including. Usually, people do it in the full inclusion space, right? If it's p-dimensional or p-square-dimensional. But what we find is for this problem, actually, is more helpful for Schevania algorithm. If we can enforce the local instantiveness, only satisfied by manifold. Just imagine manifold actually is lies in the foot including space. So the manifold we have in mind here is actually something so-called stable manifold, which some of you might be familiar with. Which some of you might be familiar with. Actually, steeple manifold is connected to gross mounting. Gross mounting actually is a space of all subspaces, but even think about every subspace, you can rotation, right? So the three-dimensional space we live in, you can arbitrary rotation. So if you take our rotation in mind, into account, essentially we have a hinge space of all the autonomous frames in this case will be steep manifold. In this case, will be steeple manifold imposed by the SAVB constraints. So, specifically, because we're trying to leave inference for left singular vector, which is L, but the little thumbnail is actually more helpful to combine L and D, which means we wait the vector singular vectors by the singular values. This is our target, which means the last singular vector, the left singular vectors, and the whole matrix V will become Lucin's parameter. The manifold structure actually. True. The manifold structure actually is about V. This actually essentially forms the staple manifold. And a short override, it looks like my time is really limited. I'm going to briefly mention the NIA idea without going into technical details. So essentially, for our Safari, we consider two settings. One is the stronger Is the stronger latency factors, which is a little bit easier at least, but again, covers a lot of classical cases like PCA and SVD and factor analysis. And weakly orthogonal factors are actually more practical, which means the respiratory factors could be correlated. And as byproduct, after you can do the inference for lattice single vectors, we can also do the inference for single values through this very, very classical identity. Classical identity because again, the u vector is our left single vector weighted by the single value. So this equation automatically holds. But again, doing inference for this one is not like a very, very straightforward allowance you put in efforts. You can figure out details. All right. So anything is the high dimension missions parameter I was referring to because we're trying to do inference for UK. Of course, all other UIs will become part of the fusions. And this is the V and this all. This is all the right single vectors. So, this is actually comes from the sleeve manifold. Okay. So, essentially, in terms of construction, of course, the star is loss function, a little square regression, right? So, we have the like R star linears. Because we're doing weighting, U actually is a scale. So, essentially, it no longer has like a length constraint because it could be arbitrary. So, but we have oscillatory, and this is the automatic. And this is the autonomous constraint which comes, which gives the stable manifold apart. And as I just mentioned, the main idea is we calculated this is the green function, which is called score, but the Euclidean viewering actually is not working because if you check this one and turns out it is sensitive to the user parent, which means if you perturb the using point little bit, actually is actually sensitive. Actually, it's sensitive because the way we calculate gradient. So, here we can try to profile all the information using the parameter, which means we can find the matrix M. In this case, again, it's a real high-dimensional because it's matched by dimension of the eta k, which is loosened the parameter. And this way, so that this is actually becomes insensitive to the choice of news in the parameter. And after that, we can construct the biased function for the high-dimensional singular vector, which is the initial estimator minus another matrix we have to construct times the modified score function. Again, I'm going to skip the technical details, but the varying is in order to construct a modified score function, so we have to carefully grading unstable manifolds. Gradient on stable manifold, which we can do based on the Riemannian geometry. After you do calculation very carefully, you can figure out what is right term because just imagine if you live in three-dimensional space, right? If you take derivative of three-dimensional, we live on the two-dimensional sphere, we know that the room actually is tied into space. If you have outward vector, you have project down to the plane. So that is the idea based on the stifle part. And the double part. A stiff part and the double part actually relates to the navis inverse. All right, so so this is actually gradient I was referring to the on the steeple manifold. So this is actually the conventional embedding gradient, but in order to pull into the tangent space of the manifold, you have to project it. Again, it has finite form. And then we can start an M matrix in order. You can start an M matrix in order to profile out the information in the looseness of parameter. And for W, again, we have the natural construction, even though a little bit tedious. All right, in the paper, we also consider the case of weekly orthogonal factors, which is actually challenging because the variant presented is strongly. Presented is strongly orthogonal case, which means all the ratings are close to orthogonal. Again, it covers a lot of cases, but in practice, things can be correlated. But the main idea is if you want to do the inference for layer 10, what you need to do is, since there's correlation across different layers, you have to do some surgery beforehand, which means you have to remove top one minus one layers and then try to do all this idea, naming optimization plus. Naming optimization plus the state of naming for idea. All right. So in the paper, we show that under the conditions for both cases, one is strongly orthogonal, the other one is weakly orthogonal. We're able to show that the singular vector weighted by the singular value indeed has asymptotonality with the variance. And also we have asymptote normality for the singular value. And also we have estimation. And also, we have estimation for the variance, estimate variance. So, the variant I'm showing you is only for strong alpha case. In the paper, we have the complete variant. So, this is the picture I was mentioning at the very beginning of my presentation. So, if you use the two ideas, one is naming near optimization ideas. Second one is if you use the steeper manifold gradient and expansion on the manifold, you're able to get rid of the bias and get very nice distribution. That very nice distribution given at the finance epoch level. And if you look at our simulation results, the coverage, based on 95% CIs, the imperial coverage is pretty close to the nominal level, 95%. And also we have the reported length on the CI. Okay, I'm going to skip the reader application. To sum up, in this project, we're just pondering about one problem. Just ponder about one problem. Suppose you have multi-task and learning problem, and how do we recover the high-dimensional lighting visivity structure? Not only do you want to have estimation, how do we attach the significance? How do we CI and how do we do testing? And what we learned from this priority is actually, you know, utilizing the online manifold structure, in this case, steeple, could be the key because without any structure, we will try many different ideas and it's never worked. Sometimes it means. Sometimes it means kind of work, but actually, the derogation is super, super tedious because, in the end, everything should be estimable, right? Just imagine if you have so many terms, if you want to estimate, it might be unpleasant. And also, we had a theory for the estimator, and you might have noticed that we did not mention the y singular vectors. So, this is something we are working on at the moment. Actually, it's more complicated than the left singular one, which we thought from the beginning. One which we thought from beginning.