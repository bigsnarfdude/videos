Thanks, Greg. So, yeah, I was going to say that my meme density is very low in this talk because it turns out that I'm the meme this time. And also, I forget who I was talking to a couple of days ago about how people wanted to have a little bit of entropy in their talks. And I ended up generating it kind of in, you know, not by choice. But so the motivation for some of this is essentially. Of this is essentially dealing with uncertainties. So I'm really interested always in uncertainty quantification and really uncertainty propagation, right? Everything that we do in astronomy, you know, if someone doesn't have an error bar, it's because it still exists. They just haven't told it to you. And this is sort of ubiquitous across the physical sciences. And this type of thing will have an impact in any downstream application, right? For statistical applications, machine learning, anything in between. The question always is how much, right? How much, right? Do we have to really worry about the uncertainties? How much do we have to worry about the uncertainties? How much do we have to worry about certain assumptions? And the question was that I had was how do existing methods kind of implicitly or explicitly deal with these? And were there sort of easy strategies to incorporate errors into existing methods if they don't? And so I just want to outline kind of some of the ways in which it turns out that a lot of methods kind of seem like they incorporate uncertainties, but actually the am I okay? You're fine. Am I okay? You're fine. Okay. But not quite. The second motivation is one that I talked actually with Derek yesterday about extrapolation. So, a lot of applications in astronomy often are dealing with training sets that are very biased. And so, and they're often biased in a very particular way, which is that the noise generating process is often much smaller. Data are higher signals to noise, the ones that you can observe. They're brighter, they're closer, they're easier to measure versus data that you actually. Versus data that you actually apply this to. And what this means is that you never have sort of this clean separation of your parameters. You're not measuring the true data, you have some observed data. There's this in-between part, and you're always training on that thing at the end. And so what this means is that if you're using sort of data with different noise properties, you're conditioning on some different noise process. So you're actually, if you train some machine learning algorithm on the end and you're trying to do inference, then you're actually training on a different noise process than the one that you're observing. On a different noise process than the one that you're observing, which is a problem. So you're really extrapolating to a different regime. The other case is widespread application to sort of out-of-distribution data. For example, you know, I was so proud that I got up this morning at six and I had a nice meeting with my department chair at seven and then I fell back to sleep at eight, which was an out-of-distribution event. And also, it led me to having some missing and censored data because I only saw half of the morning session talks. But both of these are processes. But both of these are processes that matter both for my personal life and also a lot in astronomy. So, what does this look like, right? If we write down sort of a normal ML workflow, we have something where we have some observations, we have some targets, and then we have some function that goes between them, right? Often optimized over some hyperparameters, which I've written through here as five. And the question is, where do errors enter into this process? And the answer is yes. It's everywhere, right? You have uncertainties in all different parts. You have uncertainties in the inputs, things like the You have uncertainties in the inputs, things like the observables and the targets can have uncertainties. We saw a great talk by Pingbua earlier, sort of talking about what to do if you have uncertainties in the targets, for example, for classification. You also have uncertainties in the model itself, things like hyperparameters, or if you want your models to be probabilistic and actually give you errors on the prediction, some type of scatter. I'm not going to talk as much about sort of the kind of epistemic, like sort of uncertainties on the model itself, but I will talk about all the other areas since they all kind of blend together and lead very naturally to. blend together and lead very naturally to what I call SPI. A good motivating example I like to use is dust mapping. So we're interested in sort of inferring the distance to a source and the further away it is the more dust there is in front of it and so we're measuring kind of this integrated extinction along the given line of sight. What this means is that we have a bunch of data that looks like this. You know, here are a bunch of stars that we observe. We measure some extinctions to them and we're interested in trying to fit some monotonically increasing function since unfortunately we can't have negative dust. Since unfortunately, we can't have negative dust just like we can't have negative GC counts. So, you can use this to sort of come up with very cool video. So, this is a 3D dust map from Greg Green. It was published a couple of years ago. This is essentially looking out at 3D structure constructed from hundreds of millions of stars as we sort of orbit around the sun. So, I'll just flash it up again so everyone can kind of see. This is all derived sort of using this type of approach. So, just as a motivating example. Okay. Okay. And now I can't skip ahead because I'm on the video. There we go. Okay, so where do we start, right? Step one is: let's just add in the errors. Okay, does this make sense? The idea is, well, we have some function. We normally write down something like, you know, f of y given x. And we can just say, oh, yeah, let's just do f of y given x and sigma x, right? Or maybe some sigma y's here too. We'll just throw them in there and we'll condition on them. What does this sort of imply? What does this sort of imply is that you're predicting some particular value, and depending on whether your data point has a small error bar or a very large error bar, you know, in sort of given directions, you're going to be doing a slightly different type of average. And you probably expect that the actual value at that point will be slightly different. So it's not crazy, but it doesn't necessarily seem to be like a natural thing. So, does this work in practice? Your mileage may vary. So, I'd say like, you're welcome. So, I'd say, like, you're welcome to try it, but unless you have some very, very flexible functional form, or as we'll get to later, sort of modeling some probabilistic distribution, this doesn't quite work. So the next thing that comes up, and this is something that all astronomers think about, is they say, well, we know what the noise is. Why can't we just Monte Carlo it, right? So does this make sense? Let's say that we have some unknown noise distribution around some point. It could be pretty complicated. Let's just simulate a bunch of Let's just simulate a bunch of essentially fake data. We're going to augment our data by just simulating a bunch of possible realizations of that noise process and see what it does, right? So this does make sense. The problem is it doesn't actually work. And I'm going to give you two reasons why. And if there's one thing you take away from this talk, it's that if you want to carlo and feed it into a machine learning method, be very careful. The first one is to ask what happens if you have two data points, one with very small errors and one with very large errors, and what that has on your loss. Large errors and what that has on your loss function. So, in this case, we have one data point with very large errors. Let's say that we can multiply the point all the way up and down. And then we have another one with data that's like this, right? And let's say we simulate from both of these different objects. What's going to happen? Well, if we adjust our prediction for a machine learning method up and down, this is going to have a tiny impact on the result, right? If the actual data points are more or less at exactly the same location, so if you move it up and down, it's just essentially a tiny different distance. Just essentially, you know, a tiny different distance. So, if we're doing something like a mean square error, it's going to have a small contribution. This one, though, this chunky boy is going to have a huge impact. You're going to have data points way out here at like 20 sigma, right? And originally you thought, oh, I'm just trying to simulate all these possible values, but suddenly in the loss function, you care a lot about a potentially 20 sigma event, right? Like that has a huge impact on your loss. 20 squared is 400, 1 squared is 1. So this point now contributes 400. So, this point now contributes 400 times more than originally. So, what you've done actually when you Monte Carlo heteroscodastic noise is you've actually become more sensitive to the noisiest observations and less sensitive to the best observations. So, it's the opposite of what you think you want. There's also a good math reason as well that I'll sort of go through later. But this is at least the intuitive explanation, what you need to be very careful. Okay, so let's just go through very quickly kind of what we can do to derive this. Kind of what we can do to derive this. And I love linear regression because everything is just linear regression in the end, and it's very easy to explain what to do. So we can imagine we have some model, we have a bunch of data drawn from some distribution, and then we're going to fit some linear function to it. Okay, now we have some error bars. So we're going to sort of scatter our points sort of up and down relative to this line. And this noisy data, we're going to call tilde. And we're going to have some epsilon, which is going to be some noise process. So what do we do here, right? What do we do here, right? Like, how do we sort of fit this line, assuming that this is true? The basic idea is we actually need to construct this dependency graph. We need to show where exactly our noise is coming in. So we can do something like this. This shows essentially we have X and A and B. That gives us a value on the line. We have some measurement error, and that gives us our observed value. So we sort of have this flow going from left to right of where we start and where we end, and where exactly the errors come in. And where exactly the errors come in, in this case, sort of at this final stage. So that gives us two different probabilities depending on what we condition on: the noise process, the probability of observing this noisy measurement given this unknown true value and the measurement error. And then, of course, the probability of getting your unknown true value given your X in A and B. And you can also keep going down if you want to also simulate your X values too. So we have a noise model and a latent model. And both of these are important because often what can Are important because often what we want in ML now is that we want to actually train our algorithm on this latent data that we don't observe and marginalize over this noise process. That's how we can extrapolate to new data is if we know what this latent distribution is and we have some different noise process, we can always just put that in afterwards. Okay, so what does this look like in practice for linear regression? We have essentially two cases. The first is we can write down kind of a way to marginalize over this, right? So, this is kind of the Bayesian marginalization. We say, okay, we're going to marginalize over all possible values, do this, and that will give us what we actually want, which is the probability of a noisy value given all of these implicit things. You can also sample for it, which is a lot of how hierarchical models work. So, that's essentially the So, that's essentially the main thing. And luckily, for a line with Gaussian uncertainties, you can show essentially pretty directly that you essentially can just completely analytically marginalize over the implicit process. You get your noisy value out. So what does this mean, right, in terms of what we can do here? We can keep doing this. You know, we can add more and more pieces of noise. So we can say, let's imagine we have noise on our inputs now, somewhere over here. We have some extra noise process, and that's equivalent to now scanning. Noise process, and that's equivalent to now scattering our points kind of in a different direction. Say we're moving sideways, and we can write all of this down. The point is that by going back to basics, we can and showing that you just have to write down a bunch of integrals and marginalize over this distribution, as long as you know what the noise process is, you can essentially construct a new loss function that takes the noise into account and then train on that instead of your normal standard cookie-cutter ML thing. Cookie cutter ML thing. And so, in terms of what this looks like in our data, the idea is that first we need to introduce some probability to do this marginalization. So, our machine learning model naturally must become probabilistic so we can evaluate some numbers, right, to figure out how likely it is that we observe some unknown data point. We can imagine making it more complicated. And in the end, we get something that does make sense, right? And we've moved from essentially a model where we're just looking at a point. A model where we're just looking at a point estimate and using ad hoc metrics to one where we have a full probabilistic model that actually is dealing with our noisy data measurement process. And one thing that's fun about this, by the way, is that you can approximate marginalizing over the noise using all those Monte Carlo samples. The only difference is that you have to do it inside rather than outside of your sum. Normally we write down some sort of loss. Normally, we write down some sort of loss function. We have something that looks sort of like this, you know, sort of some log likelihood. And when you augment the data, what you're doing is you're adding more stuff out here in front of the sum. But what you actually want to do is to actually be doing all of your marginalization inside this point. So, in other words, your integral belongs inside the log likelihood. It doesn't belong outside the log likelihood. That's essentially the main takeaway here. And you can. Main takeaway here. And you can implement this in a lot of ML packages using operations like LogSumX. And if you do, you get some pretty fun results. So, this is work that was led by an undergrad, Alpha OPSI, who is now in her final year at UTSC at Scarborough. And I will erase this, hopefully. But what we're showing here essentially is that if we build this model, the same one on the right, and all we do is we just marginalize over this process. We just marginalize over this process properly. We go from this one on the top, which is just using some very naive mean square error estimate, which is the green line compared to all the gray ones, to the one at the bottom, where the differences are often so small except at the edges that you can barely tell the difference. So it really does work. I'll sort of close by adding in a couple previews for kind of ongoing work that's really what we call SBI. But this is sort of what we mean: is that you can use mean is that you can use simulation-based inference, which really just means predicting probabilities from machine learning instead of predicting values per se for functions. And once you do that, you can then build this type of models and incorporate these types of uncertainties into your predictions. So this was work that was led by Bin Xiaowang, who is actually on the job market this year. He's a postdoc at Penn State. And just so I can disambiguate, I'm also working with a separate I'm also working with a separate Wong, Yihan Wang, who's an undergraduate in stats, who's been working more on sort of some of these error calibration problems. But just to show some fun pictures, when you do this to extrapolate to out of distribution errors, you have some data that can be very messy. These are the same types of spectra that have been shown sort of in other cases. But the idea is if you have some measurement error that's very different from what you've trained on in your training set, and you apply a naive method. And you apply a naive method, you get something that looks like this. The corner, this is a corner plot. The values don't matter. The only thing to pay attention to is the location of the colors on these contours. The light blue is if we apply a very slow nested sampling algorithm to try and get sort of a ground truth estimate. The orange is when we do sort of what we call normal SVI, which is completely ignore the fact that we're out of distribution. And you'll notice that they do not agree at all. They're crazy. Phrasing. And then, if you do something that's a little bit more robust using this essentially marginalization approach, which we call SBA, we get agreement that essentially is perfect. So this really does work. We can also apply this to deal with missing data, but being a little clever about how exactly we try and marginalize over negative infinity to infinity, which turns out is very difficult to sample from. But with a little bit of adaptive neighborhood searches and some nice proposals. Searches and some nice proposals, we also can ensure that you can get essentially really good matches to deal with this problem. Again, using the same exact sort of foundational framework. It also really works. So this is a project that we just actually put up on the archive, I think, yesterday, by Zichang, because he needed to defend his master's thesis tomorrow. He's a student at Tsinghua. And this is just showing that we are actually able to sort of denoise these things and uncover in the simplicity. These things and uncover in the simplicity structure. So, this is all real, like the gray is what we observed, and then the actual reconstruction versus the truth is blue versus red. So, you can see this actually kind of really does pull out as some structure. That's more or less essentially it. So, you know, Zitchong's been working on really fun stuff to extend sort of this framework to deal with mixed data types. So, not just continuous, but also discrete data to accommodate the fact that many times when we're training on different types of observations. Training on different types of observations. We've pulled them from different surveys, say, you know, an X-ray survey, an optical survey, and they have very different data properties. We might want to actually incorporate that information directly into the inference as this kind of mixed, discrete, continuous framework. And he showed that using this new code, which he calls Zephyr, which doesn't actually stand for anything, but is very cool, that we actually outperform sort of the other state-of-the-art methods in literature. So it actually does work. Actually, it does work, and essentially that's where I will end. So, the summary is pretty much that hopefully the kind of nice linear regression overview wasn't too bad, but that hopefully was a good way to kind of illustrate why we really care about coming up with these probabilistic models in order to do this data marginalization. I talked about how this actually worked in practice. So, seeing is really believing for being able to pull out some of this stuff. And then that this is also does seem to work quite well. This also does seem to work quite well for extending things to doing to sort of out-of-distribution measurements and also accounting for sort of missing and censored data. And we're sort of now looking at sort of next steps to improve sort of the underlying framework and also make it a bit more computation feasible. But if you have any questions, definitely feel free to check out the paper or come talk to me afterwards or ask me them now. So thanks, everybody. 