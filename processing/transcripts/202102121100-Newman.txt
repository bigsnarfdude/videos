Talks, and you know, just I think it's the organizers. I want to thank you for organizing such a great workshop. It's been, you know, really has been effective, even though we've been online. So, so thank you for all of that. Really excited to talk about our recent work. This is work with my wonderful advisor, Lars Ruthado, and our equally wonderful collaborators, Joey and Bart. You just heard Bart give a phenomenal talk. It's a tough act to follow, so. It's a tough act to follow, so I'll do my best. But hopefully, this we have some fun for the last talk of the workshop. So, without further ado, let's get started with just kind of a shallow, let's take a look at the shallow look at deep neural networks here. So, broadly, we can think of neural networks as solving problems in two different categories, function approximation. And unfortunately, I couldn't get the videos to loop, so let me show it here, where we're trying. So let me show it here where we're trying to approximate some true function and we're trying to make the neural network iteratively better and better to approximate this function more accurately. Neural networks can be very efficient. So throughout this workshop, we've heard about some really interesting concepts, but they become hard to handle in high dimensions. And something like a neural network may be a case where you could approximate these high-dimensional functions and solve more problems than you. More problems than you could without this. On the other side, we can do classification with neural networks. This is my family's dog, Honey, and we want to run her through a neural network, her image, and hopefully get out the label of a dog here. So this is classification can be used for image classification, object detection, any kind of decision making. So there's really just a ton that we can do with neural networks. with neural networks can be applied to all these different, and it can be applied to so many talks that we've seen at the workshop. So what's been, what's the hype with these DNNs? And they're incredibly expressive tools, as I just mentioned. They're also universal approximators. They can be efficient approximators and they're very versatile. There are tons of applications to which they can be applied, things like computer vision, speech recognition, and more recently they've really gained some. More recently, they've really gained some ground in scientific applications to avoid, say, for example, expensive PDE solves or things of that nature. But there are a lot of challenges associated with deep neural networks. They're not the answer to everything. They don't always produce a generalizable model where they're trained on some data, but we want it to work on other data. But we want it to work on other data it hasn't seen. That can be challenging. They're hard to explain. They are over-parametrized, and what these parameters are doing is difficult. Might be worth looking into which parameters are the most important using BART's method, HGSA or something like that. And something we're going to tackle in this talk is training in neural networks is very inefficient. It takes a long time, and there also aren't guarantees. And there also aren't guarantee there aren't convergence guarantees that we might want, especially in applications where the fit matters a lot, where we want a really reliable network. So before we get into this, I want to take a step back and just review what a neural network is for us, give us some notation, and hopefully put us all on the same page for the rest of the talk. For the rest of the talk. So, this is how I think about neural networks for this talk. It's a mapping from inputs to targets, and there's just something that goes on in the middle. So, Y in the talk is going to be the inputs, C will be the targets. And the neural network in the middle in the cartoon kind of version, you see this kind of all these different layers connected with nodes here connected with edges. Edges and these the terminology we use here are the green layers. These are they'll be called features. We have input features, hidden features, targets. The connections between the layers are called, we'll call weights, the network weights. And then the key piece of neural networks is we're trying to capture very complex relationships in the data. So we need some nonlinearity. And that's where we use something like a sigmoid function applied to each node or. Applied to each node, or probably the most popular one is Relu, right now, a rectified linear unit. So that's the anatomy of a neural network in a slide. There are lots of types of neural networks. The most basic type is a feed-forward network, where here's the picture you saw before, but we're going to write it out mathematically, where we now have, we map from input to the first hidden layer. Input to the first hidden layer, U1, using some affine transformation. This operator K here could be a dense matrix, could be convolution, something like that. And the sigma is some nonlinearity applied entry-wise. We then can kind of create these layers by performing a bunch of these mappings to get from one hidden layer to the other. And then we have some mapping to X, which is going to approximate our targets here. And you can use data. And you can use different activation functions that and just trying to oversimplify it a little bit right now. The most of the getting closer to state-of-the-art methods, we can talk about residual neural networks, which look similar, but we have this additional identity mapping or a skip connection or some other vocab here that maps. When we get to the hidden layers, we don't just We don't just map some, do some nonlinear transformation of the features, we add that to the current features. And this has been shown to, this allows us to create much deeper, more expressive networks. It avoids things like the vanishing or exploding gradient problem. And there are some really nice connections to ODEs and PDEs, which allow us to bring some mathematical insight into these neural networks. Neural networks. So, this was where we looked under the hood a little bit into the black box of neural networks, but for this talk, I'm actually going to not look so closely at this and really think about a neural network as some function that maps inputs to approximate targets and is parametrized by some. Here I wrote theta bar, we'll use theta a little later. And theta bar is just a collection of all the different weights of the network. Different weights of the network. It's a vector with all the weights. So, for most of the talk today, we're really going to ignore all the inner workings, even though we know they're there now. So, let's now again, we're trying to figure out how to train neural networks more efficiently. So, let's just, how do we actually train a neural network? So, if we have one input target pair, Y and Z, I think of neural network training as having four. I think of neural network training as having four steps here. Where first we forward propagate, meaning we send the input feature through the network. We then want to evaluate how well our network is doing. So we measure the difference in our approximation to the true target in some loss function L. And then really where all the magic happens in neural networks is in this backward propagation phase. Here we're just going to in an overview. Going to in an oversimplified way, I'm going to just write this. This is just going to be the gradient of the loss function with respect to these parameters. And then, in order to train our network to make it better, we update those parameters with some sort of gradient descent type of method. And here we have a learning rate, and there's a lot we can do to change this part of the training. But once we've updated the parameters, we repeat this process over and over. We repeat this process over and over again. Of course, we do have to do some resampling, so we don't want to do this for just one data point. We want to do this over many in hopes that this can generalize to lots and lots of data. There are really two schools of thought or two schools of training that we've heard about throughout the workshop, actually. It's been nice to hear. But the first one is stochastic approximation. Is stochastic approximation. It's something like an SGD method or a variant like ADAM is under a stochastic approximation scheme. So, this is a very popular way of training neural networks. And the idea is that we're trying to minimize the expected loss. So, we have an expectation over all the possible data input target pairs, Y and C, and we're trying to find the best parameters for that. These methods tend to be memory. These methods tend to be memory efficient and produce networks that generalize well, but they are highly sensitive to the choice of hyperparameters like the learning rate that we saw, and they can be slow to converge. Another, we actually heard a couple of, you know, I think the first talk today did a really nice job talking about using sample average approximation methods. And the idea here is to first approximate the expected loss with, say, some average and then minimize. With say some average, and then minimize this approximation of the expected loss. So, here t is a set of training data, and we want to measure this, minimize this average. What's really nice about this SAA regime is we have, this becomes a deterministic problem. So, we can use, say, second-order methods or any sort of deterministic optimization method to minimize. Minimize this loss. One of the kind of pluses and minuses here is that this depends on very large samples to get a good approximation of the expected loss. What's nice about this is we can potentially parallelize over the data, and that can, again, make training, can speed up training, but this can be expensive memory-wise. So there's a trade-off between these two approaches. For our talk today, For our talk today, again, we're trying to improve training. So, we're going to look to employ second-order methods to speed up convergence and give us a little more knowledge about the shape, the landscape of the loss function we're trying to minimize. And most notably, we're going to choose the optimal network weights. And I'll describe what I mean by that in the next slide. So let's go back to our picture of a DNN where we map inputs to targets. And below, you see some illustrations of some popular network architectures. We have the residual network and AlexNet, VGG. These are convolutional neural networks. And they all look a little bit different, but they have something in common. And this is that they all have a separable structure to their neck. Structure to their network. What does this mean? We have a mapping from inputs to outputs, Y to Z, and then we have this final layer where we map outputs to targets here. With this separable structure, the first part is we'll call a feature extractor. This is a nonlinear mapping and it's parameterized by just theta here, not theta bar. We're done with theta bar for the day. But I'll call theta. But I'll call theta here the nonlinear weights of the network. And then this final layer is some linear model. And if we're doing a classification problem, we can think of that as the classifier, hence the title of the slide. But we're trying, we're looking at this separable structure. We see it across tons and tons of networks of state-of-the-art networks for the problems we're trying to solve. The problems we're trying to solve. What we're going to take advantage of is we can do a lot with this linear model. We want to exploit the fact that we know it's linear and see if we can actually find a better way of choosing these weights, W, than just the kind of gradient descent type of way we saw earlier. So to motivate this, we'll look at this toy example here where we have some inputs. Here are the Y's. Some inputs here are the y's. These are just points in R2, and they're classified, so they have corresponding targets, which are labels, that are based on the distance from the origin. So you'll see we have red and blue here. So two classes here, binary classification problem. We then are going to map to the outputs here. The outputs are also going to be in R2, so we can visualize this. And you see the outputs. This. And you see the outputs in this plot over here. All the dots are the Z values here. So we have this nonlinear feature extractor. This, the part in Magenta, I think, maps these points to these points. The scale is a little off, but that's the mapping that you see happen here. And you'll see that we have the layers that we have actually are a little bit wider than. Uh, actually, they are a little bit wider than R2. It's a very simple network, but it's a little wider so that we can break this topology. And actually, our goal is to get to a point where the red and the blue are linearly separable. And if we get to that point where the outputs are linearly separable, then we can draw this black line. That's the linear model, W we're trying to find that will separate these classes perfectly so that we actually get a good, a good classification. A good classification, a good labeling scheme for the points, for the original points. Let's see what else did, yeah. Okay, so this is what we're looking to improve is how do we draw this black line here, that W. So there are two ways we can go about it. We can look at, treat the W as network weights. As I said, we can train it with a gradient descent type of method. With a gradient descent type of method, or we can try to solve for the optimal W at each training iteration. That's what optimal W means here. So what you're seeing here are the output features. We're going to train that network in these two different ways. The first one is you see the output features here without any training. So we've initialized the networks both ways. And now I'm going to show you a video where you're going to see how the class. How the classification improves as we train the network. So, each frame of the video is going to be a new training iteration after we've updated the network weights. And let's just see what happens when we do this. So again, here this is using just updating W with some grading information. And here we actually solve for the optimal W. And let me just play this one more time. And let me just play this one more time. It goes kind of fast. So you can see the black line here, the W changes every iteration to separate the red and the blue as best as it possibly can. And you see by the end, we're doing a really nice job separating those colors. We're not doing nearly as well in this other without finding that optimal W. You also can notice that we get there, if we look over here, we get to that right. If we look over here, we get to that right now. We're already in a really good spot of classifying those points. So, this is some behavior that we want to see going forward. We want to see that we can converge to a good solution quickly, and we can even get to a better solution because we've actually tried to fit the data well at the end. So, that's where variable projection comes in. Let's see. Okay. So let's formalize this a little bit more. Here we have the supervised learning problem where we have some given training data set and we're trying to find the network weights by minimizing some objective function phi. This objective function is a mix of a loss function here and some regularization. Notice in the loss function, this is the DNN, which is the This is the DNN, which is the feature extractor plus the linear model that we saw before. We're going to consider objective functions that are smooth and strictly convex with respect to w, but this is not such a bad assumption to make. This really depends on choosing convex regularizers, but also the loss functions we use. And the common ones in neural networks fit the bill here. So let's just looking at a couple of loss functions. Just looking at a couple of loss functions. Here, let x again be the output of the neural network, not just the feature extractor, but the whole neural network. The least squares function is good for function approximation, data fitting, things like that. We see that this is a nice looking function for a particular choice of C. We all have seen this function many times. I know my class, I show this function all the time, my class in non-linear optimization. Optimization. But for classification, we use something like a cross entropy loss. And here, this is the cross entropy loss for the input X. And what this, think about C as a vector of probabilities. In this case, this tells us that we should be in class one, not in class two. And what we're seeing here is when the inputs, when we get a large value in the first entry of X and a small value in the second entry, Small value in the second entry, we're minimizing that loss. It's getting closer and closer to this probability. How this, what we're doing here in this part of the function, is we're converting x into a vector of probabilities. So I just want to show you, if we just look at this part right here, what does that, the first entry of this vector of probabilities look like as we change the input x? And you'll see that as we get to this range, the probability gets closer and closer to. The probability gets closer and closer to one in this region. So that's again just looking at the input x and the first entry of this vector. And now if we look at looking at a plot here where we, I'm plotting the probabilities now. So I now have, I'm pretending this is the input into the vector probabilities is the input into the loss function. We see that the loss function decays when the probability is close to. When the probability is close to this choice of C, a probability of one for the first class and zero for the second class. So this is what, but everything, again, is nice, it's smooth, it's convex. This is what we're hoping to see so we can use variable projection. So going back to the problem at hand, we want to minimize this problem. We want to take advantage of this fact that we have a nice. This fact that we have a nice function and we know the separable structure here. The main idea is to eliminate this W and exploit the natural coupling between the features that come out of the network and the model that we have to fit at the end. And this should accelerate convergence the way we saw in those in the red and blue dot video. So, to this end, we form the reduced optimization problem, which Optimization problem, which takes becomes an outer and an inner optimization problem. In the inner optimization problem, we find that optimal w which depends on the choice of theta. And in the outer optimization problem, we try to minimize those nonlinear weights by minimizing the full objective function up here for this optimal w of theta. And that's it. And that's it. VARPRO becomes, it's actually a very simple idea, but we've seen some promising results already. One important consideration is that we should use sample average approximations to avoid overfitting, particularly with this inner optimization problem. If we have small batches, we're going to be able to fit this perfectly, but we're going to. Perfectly, but we're not going to be able to generalize well. We want to have a large batch so that we're really looking kind of globally at our data as much as we can. Now, hopefully, by the end of the talk, you're all going to be such fans of VARPro, you're going to want to implement it in your own neural networks. And I want to show you that you can do this quite easily. There's a nice connection between using VarPro and NoVAPRO. Between using VarCro and no VarPro. And that is if we solve this inter-optimization problem well, as we should be able to, because we have a nice objective function, then the gradient of the objective function with respect to w at this optimal w should be zero. If this is the case, then the gradient of the reduced objective function with respect to theta is equal to the gradient of the full objective function with respect to theta. With respect to theta, evaluated at this optimal W. So, if you're using any sort of first-order method to train your network, you can throw VARPro in and you don't have to change anything else you're doing in your code. So if this idea seems interesting to you, you can use it. But again, this depends on us solving this inner optimization problem well. So that's what we should talk about next. What we should talk about next. How do we solve this inter-optimization problem? So, again, we have a nice function. One thing I want to note is that there are some, this inter-optimization problem is modest in size and in particular for a few reasons. One, it's independent of the feature extractor that we use. It doesn't matter what the depth of your network is, it doesn't matter how many layers you have, it doesn't matter how big those layers are. Because this is Because this is, we're only concerned about the final step of the neural network, the optimization problem only depends on the number of output features of your network and the number of targets. And these are typically not too large in common problems for which we use neural networks. On the other hand, though, this for something like a cross-entropy loss, there's no closed-form solution for this W of theta. And we've seen that in order to And we've seen that in order to make VARPro to take advantage of VARPRO, we have to solve this inner optimization problem accurately. So we get that gradient to be zero, and we want to solve it efficiently as well. So the overhead is minimal. So here we use a Newton-Kreloff trust region method where we are going to update our current guess for W theta, Wj in some direction delta W. In some direction, delta w. So this method, what we do first is, let me just describe this little cartoon here. So in this cartoon, we see the contours of our function, objective function phi. And this is the point that we're currently at. We're trying to get down here. So first we build a quadratic model around the point. Now, again, just this is a cartoon, nothing numerical here. No, nothing numerical here. So that's the quadratic model we're trying to minimize, but we're using a trust region method. So we can only take a step within this ball of radius delta J. And the step we would take would be a step in this ball to make this minimize this quadratic model as the best we can. This, in comparison to something like a line search, where you would minimize the quadratic model and then change the step length. Model and then change the step length, we can potentially make better progress. And this trust-reaching methods tend to be a little more robust for what we're doing. So now we have a way of getting this delta W, but I promised you efficiency and accuracy. So we have some robustness, we can actually get to the solution. This is efficient because we're going to use a Krylov method with a low rank approximation to this Hessian. To this Hessian. And even better, we're going to be able to reuse that. All the work we do here, we're going to be able to reuse that later in the code when I talk about our, put it all together. And this is accurate. The problem is modest in size, so we have the option to use double precision. And it's something we can actually handle. So this, but now we know how to optimize W well. You all can go and use our code or write your own and you. Use our code or write your own, and you can use VarPro for all your neural network needs. But we want to go further. We can optimize W now. We want to find a way to make to opt to train these nonlinear parameters even better. So here we introduce our Gauss-Newton-Krelov-VARPRO method, GNVPRO, where again we're trying to minimize the reduced optimization problem. We're using VARPRO, so we're getting high accuracy. We're getting high accuracy and accelerating convergence, but we want to use these second-order methods to go even faster. However, this is going to require a careful Jacobian implementation. So let me just write up, this is what we saw before, we're minimizing a quadratic model with a trust region method, but here we're updating theta, not w. And also, we're going to approximate the Hessian here with a Jacobian transpose Jacobian type of thing, a little bit more going on. A little bit more going on in the middle. But this Jacobian is what is a bit more, takes a little more thinking to compute. And this is something that you can't do just with your current neural network model. You would have to use our code for the moment. So let's look at this Jacobian a little bit more carefully. Here we've A little bit more carefully. Here we've expanded it. And notice I'm vectorizing the W here so that we get a nice matrix for our Jacobian instead of some other structure. This part of the Jacobian, the Jacobian through the feature extractor, is something we know we can do with automatic differentiation. That's something that we can't avoid doing. But the challenge here is this Jacobian. Here is this Jacobian, the Jacobian of the W that we solve for. To find this Jacobian, we actually are going to solve for this implicitly by remembering that we solved the inner optimization problem well, so our gradient is equal to zero. We then can differentiate both sides and expand. Here you see that Hessian show up again. So remember, I said we could reuse information for that Hessian. This is where we get to reuse it when we need this Jacobian. When we need this Jacobian to improve the training of the nonlinear weights theta. So we would solve this problem implicitly. And I just want to say that this is, we do it a little differently depending on the loss function. For the least squares case, we can actually solve for this Jacobian explicitly. Explicitly. And just to give you a little taste of how we do that, here's our loss function. What I'm going to do first is I'm going to matricize this. So what this notation, this Y means each column is one of those training samples. And C here is one of those training targets. So the columns correspond. And this, we apply the feature, the nonlinear feature extractor. Feature, the nonlinear feature extractor to each column. Okay, but now we get this matrix of output features, and we can solve for Wθ explicitly using the S V D of these output features, and we can form the Hessian reusing this S V D. So we get an explicit solve, and we get to the work we do can be used to also train the nonlinear weights theta. nonlinear weights theta. On the cross entropy side, we don't have this, we can't solve for this, we can't solve for W theta explicitly. So instead we solve for W theta with our Newton-Krylov method, and we reuse that low rank factorization from the Krylov method to approximate the Hessian. So you see the kind of the parallels here, they're a little, they're slightly different, but They're slightly different, but they're, you know, we can. This is something that is new. We far probe for nonlinear least squares problems has been known, has been around for a while, but extending this to non-quadratic objective functions is new. But luckily, there are nice parallels here. So I think it's time for some numerical experiments. You've put up with the notation for a while now. For a while now. So let's look at our first example, which is a PDE surrogate modeling example. And let me just, let's just do a quick problem setup where we have some PDE A with the solution U, and it's parameterized by Y. Our goal, we then are going to measure, we solve this PDE, and we measure, we get some discrete measurements of our solution U. Discrete measurements of our solution U and store that in C, which we'll call the observables. Our goal is to figure out a mapping from parameters to observables with a deep neural network so that we don't have to do these expensive PDE solves anymore. This is a data fitting problem, so we're going to use a least squares loss function. And we have, so we can use GMV Pro with the explicit formulation of the Hessian and things. Of the Hessian and things like that. And the two PDs we're going to look at are a CDR, a convection diffusion reaction, and a direct current resistivity. These are the two we're going to look at here. So here are some of the convergence plots of the loss function. So remember, we want to get a good fit. These are applications where the fit really matters, right? We're modeling physical phenomena. We better not be too far off. Not be too far off. So here you'll see the blue is the training data, red is validation. So this is data that on which we didn't train. And what we really care about is how does the validation data we want to track, the loss should track with the training data. That's an indication that we're generalizing. Well, the generalization gap is small then. So I want to point out the first column and the last column here, where this is a Where this is using Gauss-Newton to train theta and W here. And in this case, this is our GMVPro method. So this is these two methods. The only difference is whether we're using VarPro or not. And you'll see that in the VARPRO case, we converge to a lower loss value and we get there more quickly for both PDEs. So, this is what we're hoping to see and followed from the derivation that we did. That is what we expect. Something that was interesting to us, we compared this to an SGD method, an ADAM method. Again, no VARPRO here, so we don't, because we don't want to overfit. And you'll see that even with GMVPro, we converge to a better loss value. A better loss value. The validation data tracks pretty well. We're generalizing decently, and we get there faster. The way we compare these two in terms of speed of convergence is we look at the work units, the most costly step of the neural network training. And the work units are the number of forward and backward passes through the network. That's really the bottleneck. For SUD, you get two work units per EPOD. For SGD, you get two work units per epoch. You get a forward pass and a backward pass to compute the gradients. For GMV Pro, you get two work units, the same two as above, but then you also have an additional forward and backward pass every time you, as you're trying to approximate this Hessian. So there's an added expense here. But even when we're comparing, we count that added expense, we're still converging to a better loss faster. To a better loss faster. So we're accelerating convergence and exactly what we were hoping to do. To give a little bit more of a visual here, we can actually look at the DCR case where this is the PDE we're trying to minimize. We have a source term here. This is a dipole source. And we have some region of conductivity parameterized by... Of conductivity parameterized by y. This region is some just ellipsoidal region that we're trying to detect. And the parameters, we only have three parameters here, which are depth, volume, and rotation. And we're trying to say, if we just know these parameters, can we approximate, can we capture these observables? And the observables we're measuring are the differences in the electric potential on the surface. So here you'll see the differences. So here you'll see the differences. Each pixel is a difference in the X1 direction, and in this image, it's in the X2 direction. And here we've plotted it on a log-scale log absolute value to really enhance the contrast. So you can kind of see the ellipsoidal shape right here that we're trying to detect. So, with this setup, let's see, let's look at a bunch of the Look at a bunch of the same four methods we saw before. But I want to point out: this is the image we saw on the previous slide: that GMVPro does a really nice job capturing the shape, and SGD does not really capture the features here. It may be able to fit some data points, but it's not really capturing the structure that we're seeing that we do capture with GMB Pro. So, this is again something we're excited to see that we're because we fit the data. We fit the data, we may actually be capturing more interesting or more important features of those observables just as a visual here. So the last thing I want to talk about is that was a least squares example, but we also can use GMB Pro for non-quadratic loss functions. So here we're going to look at an image segmentation problem. To look at an image segmentation problem where we have this is a hyperspectral image, and each pixel of the image is classified by the material or the crop that is located there. And our goal is to, if we're given a pixel, we want to be able to output that label, whatever class it should belong to. I want to note that the distribution of the classes is Of the classes is very imbalanced, right? There are some classes where we see very few pixels, and that can make this problem a little harder. So here's some results about this. Here's the ground truth you saw on the previous slide. Again, I want to first point out the full Gauss-Newton versus the Gauss-Newton with VARPRO, just to show that VARPRO is what we need to make these second-order methods, second-order training methods for. Methods, second-order training methods for neural networks work. This is something we are advocating for. So, Gaussian really doesn't do well in the accuracy. Even on the training data, it doesn't fit the training data well. You'll see in this confusion plot here, our confusion matrix, that we want to see lots of ones in the diagonal. That means our predicted class and true class match, but we're missing a few of these classes completely. Few of these classes completely. Remember, we had some of those really small classes that are hard to detect. So we don't do well with just Gauss-Newton. But if we add VARPRO in, we really improve our results per class as well. We're taking that into account when we form the fit on that last layer. One thing that was interesting to us comparing to SGD is that SGD did a little better on the training accuracy. Accuracy, but we actually generalize better in this example. We're not claiming that we can, we were, we're not trying to say we're outperforming SGD, but with, in this case, we had with the same network architecture and we actually gave SGD more more iterations, more work units to train. We were surprised and pleasantly surprised to see we actually generalized pretty well. So, this is. So, this is encouraging and needs further exploration. But it was, you know, just an exciting first, an exciting result to see. So I think I'll wrap up now. It's Friday afternoon, right? And a little, and five minutes early. But in our group, we like to call GMV Pro the backslash of neural networks for those who know the MATLAB backslash. We're not claiming, that's actually not, we're not quite there yet, right? That's actually not, we're not quite there yet, right? Backslash is much more robust and effective, but we're hoping to get there. That's more of a goal where we want to be. What we've seen in this talk is GMV Pro accelerates training of neural networks and we get to a high accuracy. We can apply this to non-quadratic objective functions, and it did not depend on the non-linear feature extractor. So this can be used for a whole range. Can be used for a whole range of neural networks as long as you have the separable structure. Going forward, we want to look at more applications, image classification, other benchmark applications. We're working on implementation in PyTorch to make this more usable for more people. And we're looking to extend VARPro as well to get the best of both worlds, right? SGD still is going to generalize better in. Going to generalize better in more cases, but VarPro, we still saw the benefits that we accelerate the convergence. And if there's a way to bring these together, that would be excellent. So with that, thanks for listening. Thanks for being here on Friday afternoon. And let me know if you have any questions. Okay, great talk, Liz. Are there any questions? Hi, I hear Pang. Thank you very much, Manitabas, for this very interesting talk. It's quite impressive. So I just want to wonder why this Gauss-Newton method, the full Gauss-Newton method performs so much worse for the last example. And also for the first 2PD example, it doesn't perform as well. Yeah, it's the one reason is, I mean, in general, second-order. I mean, in general, second-order methods are not great for training neural networks. We've had some problems with the conditioning, and we really, when you include VARPO, it can improve the conditioning of your Hessian. And we think that's also part of the reason that's converging a little better. But yeah, yeah. But like, you know, if you use a second-order method, and it helps, it's supposed to help. Method and then it helps is supposed to help for the preconditioning right because you're grading the mass band several orders magnitude magnitude. But if you bring in their second order information, it's supposed to help to bring down their condition number. Is that it? I'm blanking right now. Can I talk about that? Can we talk offline about that? Oh, yeah, sure. Yeah, just a little. Do you want me to give a quick question? Do you want me to give a quick comment list? Yeah, that'd be helpful. Sorry, I just lost trans one thing that you need to keep in mind when you use second-order methods is that the initialization really matters quite a lot. So I think what we observe typically with full Gauss-Newton schemes is that you get stuck in the nearest local minimizer, and that's not a good one typically. So that means, you know, maybe we need to rethink how we initially. To rethink how we initialize. With Wapro, you have the advantage that basically you initialize the weights of the feature extractor, as Liz showed, the thetas, but the W is always chosen optimally. So it kind of alleviates that pressure. And then also what you see typically with these variable projection methods, as this pointed out, the problem itself. So the Hessian becomes better conditioned. So an iterative method has an easy. Conditioned. So, an iterative method has an easier time in solving it. Because you have this bilinear structure in the neural network. You have the W times the feature extractor. And that structure, by exploiting it, you typically make this problem actually quite easier, as is documented in many applications of VARPRO across image processing, non-linear data fitting, and other places. So, by the way, for all this different methods, So, for all this different methods, you use the simple average approximation instead of a stochastic approximation, right? Except for the SGD, yes. Except, yeah, of course, for SGD. So, for the full Gauss-Newton, maybe as large as that might get trapped somewhere in the local minima and cannot go further down. Perhaps if you use a random sampling or like a stochastic sampling, it helps. Yeah, that's definitely something we're looking into. Definitely something we're looking into. That's a great suggestion. Um, you know, seeing if we can like we still probably want large batches for it, um, yeah, sure, but but we can certainly resample and do things like that. That's a great idea. Yeah, I will say to that point, though, Pang, I don't want to go back to Fulgaus-Newton anymore. I mean, now we have this code and it actually really works very well. You know, we've We've noted that it doesn't really take this effort of choosing parameters, initializations, all that much. I really don't see a reason not to go back. And also, as Liz said, the computational cost is pretty much the same per iteration, given that you can solve the classification problem quite easily in most cases. Even in larger scale cases, you should be able to do that. So, for me, you know, there's no way back. It's always been kind of a goal to go away from Full Gauss and go to Vapro. Now, with the work that Liz has put into this, it's working very seamlessly. Yeah, it looks like quite impressive, but intuitively speaking, so here you separate their optimization with respect to their full parameter into two different regions. So, like if you explore the optimization in the global space and the full space. A global space and a full space, then it's supposed to converge faster than if you explore them like separately. Sort of like a pica iteration in one subspace and another subspace, right? But if you glue them together and perform the optimization in a global space, then intuitively speaking, you should converge faster. But it's quite kind of intuitive here. But yeah, I will look at the paper more carefully and see. I will look at the paper more carefully and see. Yeah, maybe there's some details that I understand quite well. Thank you. All right, great. Other questions? Other than my question? All right, then I'll just ask. So I'm just curious here. I mean, as I learned a while back, at least the argument that was given to me why people in machine learning tend to not use second-order methods is that they. Tend to not use second order methods is that they kind of overfit to the data. And so they would, you know, stop at a worse tolerance and only do, I don't know, a couple hundred iterations. And that would make sense because the data is so noisy that you don't want to overfit. Is the argument here to say that, well, I do want to overfit because I'm trying to solve this massive PDE and it's not really data, rather, points that I've sort of sampled in three space or four space. And so that's why it makes more sense to use second-order methods. So that's why it makes more sense to use second-order methods? Am I understanding that correctly? Well, yes, I think so. But also, it's kind of scary when you have an optimization problem and you don't optimize it, like in the SGD regime. So we're looking to, with GMB Pro, we're saying we want to be able to optimize for the training data. And then we have to worry about the generalization after. We've kind of gotten lucky that these examples generalize well, but there's. Examples generalize well, but there's that's something we have to look into more. But we at least want some guarantees that the data that you're training on, you can actually fit, and second-order methods can do that. But I think incorporating some of the randomness resampling will also give us some better generalization for future experiments as well. Yeah, I mean, but for the PDE, right, you want to solve it, right? Yeah, so I guess in that sense. Yeah, so I guess in that sense, okay. Have you also sort of taken, I guess, if I understand again, you train a neural network, and that's treated as sort of the solution to this PDE, but that's obviously based on some training data that you maybe synthetically created, or maybe you actually have the data. I just want to make sure I understand. The neural network is actually not the solution. Oh, so you're not using the non-profit. I don't know if it's the solution. It's right, it's the mapping from, it's kind of. Right, it's the mapping from it's kind of avoiding all that, right? Okay, so it's not necessarily replicating the PDE dynamics or anything. No, no, like, so I maybe, maybe we're talking about two different things here. But yeah, okay, maybe we're talking about two different things. I've seen this work where people would say instead of doing a linear unzatz like you would use for infinite element method or wavelets, you would have a nonlinear approximation of. A nonlinear approximation of your function that's supposed to be solving the PDE, and that non-linear approximation would be sort of the output of a neural network. Is that what you're doing, or is that something different? Yeah, yeah, I think that's similar to what we're doing. All right, so then that's why I'm saying that that sort of says why you want to have a much more exact solution because you're trying to solve a PDE and the end and not solving it. Yeah, that was the motivation is that we're thinking about these applications where you really actually want to make sure that the output is. Sure, that the output is exact, it fits well. Okay, great, yeah, very nice. Thanks for the question. And maybe I can add a remark here also for Thomas's question. So what I found or what we found is that it's not so easy actually to minimize the loss function in neural network training. So in many respects, sometimes, especially when you try playing with new ideas. When you try playing with new ideas about architectures or about loss functions or regularizers, one thing you have basically a triangle. You have the data, you have the design of the model, and you have the optimization. And they all need to go hand in hand in order to really test also if model A is better than model B. So, one motivation here really has been to create a method that can really just minimize the loss function over the training data very reliably. Data very reliably. That's why we aspire or have the aspiration to make it the backslash of neural networks. I wouldn't recommend you use a backslash for any matrix, right? But it's good to have it in your toolbox so that at least you can see, okay, that's what I would be getting if I fit the data completely. Right, right, makes sense. Yeah. No, I mean, very nice work. So, are there any other questions? So, I think. So I think just to give you a heads up, I guess, yeah, technically the workshop ends in nine minutes.