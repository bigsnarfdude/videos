And I thank you for that, and as well as the organizer for this nice workshop, actually. I just wanted to say that I enjoyed very much this workshop, seeing friends and colleagues, and also for the stimulating lectures. But I have also to confess that it was quite exhausting because for us in Europe, it means really a double life with the day in the university and the evening watching the lectures. Watching the lectures. Anyway, that was nice. So I will try not to exhaust you too much more and give a lecture about some non-conventional results. So I hope it will be stimulating and not too difficult. So the motivation for these lectures comes from some non-linear equations and with mean field terms. I will not spend Minfield terms. I will not spend so much time on this. And I'm going to speak actually of two papers. They are connected with the lecture of Jos√© on Tuesday and the one of Juan today, because they deal with, they are related with diffusion equation with mean field terms like Keller-Siegel model. But again, I would like to focus today more. I would like to focus today more on the functional setting and the functional inequalities which are behind this. And well, okay, I said I wrote that it's unconventional inequalities in the sense that I want to study cases which are maybe intriguing. So one is a reverse HLS inequality and it's an interpolation inequality where the kernel has a positive exponent instead of a negative exponent. It is a bit strange because usually you take two terms and you control a You take two terms and you control a third one. Here, you take one term and you control two terms. So, it's okay at first sight, it's intriguing. So, this corresponds to the first paper I'm going to present, which is this paper with Rose Carillo, Matthias Delgadino, Rupert Frank, and Frank Hoffmann, and which has already appeared. And the other one, it's going to be more focused on the two-dimensional case and some critical phenomena in two-dimensional, and you will see. Phenomena into dimension, and you will see the logarithmic HLS inequality Juan was speaking about. And again, this is an intriguing inequality in the sense that, well, you have a Logarhead, the Logarit may change sign. I will present inequalities with two logarithms, both may change sign, and you want to write inequalities. So you have to be a bit careful because, well, you have logarithms on both sides and they are not signed defined. Okay, so this is some more. Okay, so this is some more recent work that I have done with Robert Frank again and Louis Jean-Jean, and the motivation comes from a Schrodinger Poisson system in Dimension 2. Okay, so let me start with reverse hardy little boots of LF inequalities, and let me show you directly what it's going to be about. So I will present the reverse HLS inequality, tell you what happens with minimizer, and actually there is a nice relaxation. Minimizer, and actually, there is a nice relaxed problem, and there is a similar solution which may appear there. And then I will give you just a short hint about the free energy, which is the tool that you have to use for the first diffusion equation, which is hidden behind. But again, I will not spend much time on it. Okay, so what is the reverse chalice equality I'm going to consider? Well, I just consider a convolution term with this power here, but usually This power here, but usually when you speak of RD Little Wood Sobole F inequalities, you have a negative power. And here I want to consider the case of positive powers. Okay, so the game is to take some rho with so rho to the Q will be in L1, but here again, I'm going to consider something a bit strange. I'm going to take a Q less than one, and this is related with a fast diffusion equation, an exponent of the fast diffusion equation. And the reason. Equation and the result is that well, this convolution term which you have here, actually, so which shows up here in the inequality, actually controls the product of an L1 term, a mass, and it is LQ norm, but remember Q is less than one, it's not really a norm. And this is the product, you have an exponent alpha, which is just fixed by the scaling properties. Here, everything is written on Rn, and there is no real. Or n, and there is no really any restriction on n, but of course, some properties depend on damage. There is some restriction on the exponent, and this is what happens here. I will come back to this. Actually, I will spend some time on this. And otherwise, well, it's not very difficult to see that you can symmetrize this problem and reduce it to ideal functions, at least when you look for optimal functions or try to find. Try to find the optimal function for TZ equality, the first inequality. So there is no real difficulty in the analysis, but what is interesting is the properties of the solutions. Okay, so let me give you a hint on what happens. So here is a diagram that you will see many times. And there is, okay, so on the horizontal axis here, it's just axis here it's just the the lambda so the parameter lambda which is the power in the convolution term and here you have something which ranges between zero and one which is just my exponent q that i was speaking of before now the inequality takes place in the gray area and there are various regions which are all of different interest but remember well there is a special curve here and this is the one which corresponds And this is the one which corresponds to conformal invariance. So maybe I'll give you some more details. There is a special exponent Q corresponding to this lambda for which you have conformal invariance. In that case, the exponent alpha is equal to zero. Okay, so you have the convolution term controls, well, the critical LQ norm which Which with just the correct exponent for having the homogeneity, and there is no overturn, no mass here. Okay, so this is something which has been studied by several people. So do and so in 2015 and then go and go in in 2017. And well, you have nice optimizer, which by the way, are bar and blood type, so they are this form, and then you know what are the optimal constants, and it's given below. Okay, so. Below. Okay, so not a big deal. This is rather classical in this business of HLS inequality, at least in the case, in the critical cases. Now, what I'm going to be, there are two other regimes I'm interested in. One for which, which is above, so the curve of Duenzu and Vienne is this one here. And well, you have a gray area corresponding to an alpha. Area corresponding to an alpha which is, I don't know, positive, negative, I don't remember. Alpha equals zero is here, and alpha as the other sign here. And the dotted line corresponds to the threshold for the inequalities. Actually, the inequality are not true on this threshold. Okay, so here are the exponents with the inequality written and with the conditions. So actually, I was Exaggerating a little bit because for when you are in this region, alpha is negative, so you can put it in the other side, you are on the safe side. I mean, two terms, this term and this term, control t is one with a positive exponent. So this one. More intriguing is the range, of course, where alpha is between 0 and 1 here. And then you have this product here. And what happens is that the bottom line corresponds to alpha equal to. bottom line corresponds to alpha equal to one and below there is no such inequality you can make the quotient of t's term divided by t's term as small as you wish okay so how do you establish such an inequality actually this is not so complicated because what you can do is first consider so this local quantity which is just a moment of order lambda the mass here and interpolation The mass here and interpolate the Q norm here, and then you have the exponent by scaling. And well, this can be done with a complete elementary estimate. You cut it in a ball, you study it outside of the ball, and that's it. And it's all stuff that goes back to the 30s. Okay, so that's the first ingredient. Second ingredient is to observe that by rearrangement inequalities, okay. Uh, okay, you can uh reduce the problem to a symmetric non-increasing functions, okay. And once you have done that, you can simply estimate this quantity by the moment you have a new Carlson inequality, and that's exactly what you need. So, after working a little bit, you get the inequality in the form I was announcing with an estimate of the best Council, which is, of course, not optimal. So, so far, not a big deal. So far, not a big deal. Now, the question was: can we do something? Oh, just one remark. There is a very, very simple case which corresponds to lambda equal to, because x minus y squared, well, you can expand it. You have x squared, y squared, and the scalar product of x dots y doesn't really count because if you look for symmetric function, this quantity is going to be zero in the configuration term. In the contribution term, and you can really reduce it to the case of the cursor inequality. And by the way, it gives you some optimal constant on this line here. So, lambda equals 2. Okay, so that was just another case for which we have explicit solutions. So, essentially, on these red plane curves here and here, we know what the solutions are, but it's more complicated, of course, when you are away from them. Now, what happens when When you approach the threshold, sorry, the threshold line, so the bottom line, which is here. And why do you don't have solutions? Well, it is not so complicated to see. So first consider the case where you are below this threshold line. And consider the special solution where you combine a function here with something that concentrates. Concentrates and then it's very easy to see that the LQ norm, LQ integral, which is here, okay, doesn't see the part that concentrates. So you get just get this. There is no plus here, sorry for that. While the non-local term, of course, sees it and you get the calcium term here, the moment. So it means that by using t's as a test function, Using t as a test function, you can get an estimate of the optimal function in the quantity if alpha is bigger than one using this. And then by taking the limit m going to infinity, this quantity you see that there is by no way you can control this, you can make this as small as you want. Okay, so in the range alpha bigger than one below the threshold line, the constant actually in the inequality is just zero, you have nothing. On the threshold line, we're Thing on the social line, well, you have to work a bit more, so you have to use essentially you can use this as a test function and then you have to cut it appropriately. I don't want to spend time on this, but on the threshold line, there is no inequality as well. The constant is equal to zero. Okay, so it's only in the interior region, this gray area that you have to work on it. Now, something you have learned on the way is that if you have something that concentrates That if you have something that concentrates, rho to the q doesn't see it, and then it gives you a relaxed inequality where you allow part of the function to concentrate and part to stay with a density with Lebesgue's measure. Okay, and what it gives you is that, well, the non-local term is transformed into, well, the non-local term for the regular part plus the moments, which corresponds to the mass which is concentrated. To the mass which is concentrated at x equals zero. Okay, same for the mass, and the whole to the q well doesn't see the concentration. Okay, so you have a relaxed inequality, which of course holds with exactly the same constant because you can use the previous test function. And now question comes. So the idea is very simple. You just consider a smooth idea decreasing whatever you want whole. Are decreasing whatever you want whole plus the Dirac mass at the origin. That's the relaxation that you're considering. Okay, you have the equality of the same constant. And the question, well, it's not very difficult to prove that this inequality to this relaxed inequality to admits an optimizer. Now the question is, does it have the arc mass at zero or not? Okay, so is the optimal function singular or not? Optimal function singular or not, and actually, we don't have a complete answer so far, okay? So, existence of a minimizer. So, I already anticipated a little bit. Actually, the situation is you have to go more into the detail. So, remember that in the plane curve in the middle, you add optimizers which are smooth. Actually, you also have these smooth optimizers in the upper part and The upper part, and the question appears actually in the light gray region, which corresponds to alpha positive. So, alpha negative, there is no issue. You have a minimizer and it's a regular function. This is not a problem. Conformally invariant case was already known. Okay, and you use standard tools to do that. Now, what is more interesting is what happens in the second case. In the second case, so below the conformally invariant curve. And in that region, okay, you can prove that the relaxed inequality 2 admits the same as the same optimal constant and admits minimizer. The question is whether there is a singular part or not. Okay, one thing you can say at least is that the regular part, the whole of an Of an optimal function is not zero, and actually, it's a positive function. Okay, just using test function, if it's not the case, you get a contradiction. So, that's again not a very difficult thing. Well, once you know that there is an optimal function, and with some care, you can write the Lacronic equation, which of course mixes the Dirac mass at the origin, which shows up here and here. Which shows up here and here and here with the regular part that I call here O star. And you have this original configuration. Another question is, is this M or M star here zero or not? Okay, so we know that it's zero above the conformally invariant curve, but we don't know what happens again. Well, there is something which is rather wired in some sense. If you have some ideas, If you have some regularity of the regular part, then there is no concentration and there is no diagnosis. So if the rho star is as T's regularity, then that's enough, m star is equal to zero, and then you have an optimizer for my reverse inequality, HS inequality without relaxation. Okay, of course, the difficult question is to prove that it's the case. To prove that it's the case. So there are various things you can do. And okay, so I give you just a few hints of what you know and what we don't know. Okay, so explicit optimizer, we know them on these blue curves, Q equal to or the conformally invariant curve, which is here. There is, well, the inequality is only inside the region which is here. Okay. Okay, and actually you can prove that above the curve q equals n minus 2 over n, the regular part has enough regularity, and then it's fine. There is no concentration. There is no singular part for the inequality. I can also prove that below this curve, which has an explicit but complicated form, you get something. So, well, there is a tiny area that you can cover with this. Cover with this, and it's a regularity issue. Now, in this dark gray region, we don't know much. Actually, recently, in a paper, Carrier, Levin, and Falk, they gave a perturbation result for two equal four. So, here it's plotted in dimension four, so it would be this limiting case, but in the higher dimension, it's at the bottom of the scale. They have a small Discovered, they have a small perturbation of the bottom line from which they prove that actually there is a singular part for the optimizer. So the reverse HLS inequality does not admit an optimizer, but the relaxed one admits one. Okay, so it is a bit long and technical, and I don't want to spend too much time on this. Let me just make first a remark on uniqueness. There are cases where you know the Uniqueness: There are cases where you know the uniqueness of the optimal function, and of course, it's well, okay, it's in the case where there is no singular part, so this is fine. So, here I've plotted in dark gray the results for which we know that you have uniqueness. Actually, I know two ways of proving this. One relies on a convexity property established by Lopez and And again, I don't want to spend so much time on that. Of course, you see that there are strange, you know, this little corner here, or it is one for which we don't know the answer. It would be very strange that you don't have uniqueness as well, but it's still open. And you can also prove it. So here I'm just showing you two cases, dimension four here and dimension 10. Patterns are the same, but the curve are just. But the curve are just changed a little bit, the values of the curves are changed a little bit, but the patterns are the same. And okay, and so you see here form is a case where actually close to this neighborhood, you can prove that there is a similar solution. And this can be proved using geodetic complexity with Complexity with the metric based on the free energy. And the free energy is the tool that connects with the evolutionary condition. So, let me just give you very quickly the free energy point of view from an evolutionary collision. Okay, so for the moment, I just freeze the potential V just to give you an idea of what's going on. So, let's consider this fast diffusion equation. So, Q is my exponent, which is between 0. Exponent which is between zero and one, and I take an external potential v, and here I fix it. I take it of this form, which means it has the right behavior for large value of fixed when lambda is bigger than two, and it has no special problematics equal to basically. So that's why I chose this one. And now we introduce the free energy functional, which is Free energy functional, which is the potential, the potential term plus the entropy term, and the entropy term it comes with this minus sign so that you have a convex functional. So u to the q, the integral of u to the q is concave, but with this minus sign, the whole thing is convex. And it's quite easy to see that you have minimizers and the mass constant, which are of these form. Okay, now if you look at the first diffusion equation and use this free energy, you compute its time derivative, and you have exactly the same structure as the one that was shown in stock with a fissure information, which is here, which has a site who is decaying in order. Now, what is nice in this time? Now what is nice in this toy model is that if the exponent q is above this, well you always have a finite bar and bad profile, a stationary solution or minimizer for the free energy and things is fine. But when so for lambda equal to but for lambda bigger than two you see that there is an integravity condition which means that when you parametrize When you parametrize, so you have the stationary solution with a Lagrange multiplier here, which fixes the mass. So you can reparametrize the mass in terms of mu. And now you see that there is a limitation. So if the you take mu large, you see that at some point you hit a threshold, and this threshold is given by this m sound GCR. So what happens if you force So, what happens if you force you try to have a mass which is bigger than Ms R is that there is no space. The only thing you can do is put some mass at the origin. Okay, and that's the dr mass. And what you get is this. Okay, so the regular part now is fixed and there is a similarity which appears. Okay, so now this is something you can also do for the non-linear model. And this is a model I like very much. And this is a model I like very much actually. What does it mean? The kernel corresponds to a spring force. Lambda equals 2 is just a linear spring, and if lambda is different from 2, it's a non-linear rappelling force. So in terms of particles, it just means that, well, the particles, they are attached by some spring, and then, okay, there is a rappelling force. And now you do it at the level of the continuous equation. Continuous equation. Now, if you think to what it means to have lambda large, it means that essentially as long as modulus of x is below one, as long as the particle are the distance which is less than one, basically nothing happens. They don't see a very strong force. But as soon as modulus affix becomes larger than one, then the rappelling force becomes huge. Things will end up going to infinity. The rappelling force becomes infinity. So it's a kind of Kind of inelastic limit where okay, as long as the rope is not tight, no force, and then when the rope is tight, you get a huge weaponing force. And now this competes with the so this force forces the particle not to go too much too far away. Now, the fast diffusion, it's very good to diffuse at infinity, but close to a singularity, it's weak. Singularity, it's weak, so it doesn't prevent the formation of similarity. So, it is a nice competition in some sense, in the sense of the competing competition that were introduced by Jose on Tuesday. Okay, so you can introduce the free energy function now, which is now a non-linear potential term. You have the entropy, which is here, and formally, I guess you have this, but understanding what is really going on for Understanding what is really going on before the evolution equation is not done yet. Okay, so not exactly. So there are a few cases which have been studied. So I already mentioned it is a paper by Cario Delgadino, Frank and De Win. I apologize, I forgot to mention Delgadino earlier. And they proved that you can have a partial mass concentration close to lambda equal to four. For Namitana, and so on. And but it's only a perturbation result. And also, actually Tao and my former student Sing Yu Lee, they have studied the large time asymptotics, but in the regime where you don't have concentration, so for Q large enough. Okay. And they have a nice result of an asymptotic time behavior. Okay, so my. Uh how am I doing time-wise? Sorry. Okay, well, I have to seven minutes. Yeah, so I have to rush a little bit. So let me go to these other non-conventional inequalities that I wanted to present. And actually, the motivation comes from a different diffusion equation. It's a non-linear Schrodinger-Poisson system. And I'm thinking to dimension two. I want to use the Poisson equation. And what everybody knows is that What everybody knows is that the Poisson equation has a kernel which is Lagariot-Link. And it's interesting to take everything critical in the sense of this scale. So I have this mean filter here, which is going to have a logarithmic growth. I want to consider a limiting case of a non-linearity, so again, the Logariatic growth, and to have make things more interesting or to really investigate what happens in the prediction. Really investigate what happens in the critical case. I take an external potential, which is also of your guide, and then I want to see who wins in this game. Okay, so actually to determine what are the ranges for this term alpha, beta and gamma, for which I can say something. So, of course, the main tool is going to be the energy. And here I'm going to investigate the region of the parameter alpha beta n gamma for which this. This energy is going to be balanced from below. Okay, so by minimizing, I would have standing waves and apologize. Oops, why is it so? Okay, I have to read this. Sorry. Standing ways, so you can rephrase the problem. And actually, the question you want to ask is minimize the energy by fixing the mass for this logarithmic or potential for a self-constant potential, which is like this. Self-constant potential, which is like this, and whose growth at synthetically is given by the Gaussian kernel here. Okay, so far, so good. So, a fundamental tool for that is the inequality by the legatic HLS inequality studied by Canon and Laws, where they established the optimality case and they proved with also Wechner that the optimal case is given by the bubble. Not a big surprise for the specialists. Specialists. This log HLS is the dual in terms of Legend transform of the on-free inequality, which corresponds to the Louvre equation mentioned by Juan earlier. And it has plenty of application. It can be studied with fast diffusion flows with very nice properties, and you can use different fast diffusion flows. It's general of the It's dual of the underfree type inequality. Actually, this was something which was really observed by Kelvin and Corians in pregnant paper applied to a classical model. And you have all the proofs. I don't have much time to spend time on it. Okay, so the first generalization is when you introduce the external potential. So the V which is here, so it is the 2 log of 1 plus x squared, which is here. Plus x squared, which is here, that's my potential v, and essentially you interpolate between the log at HLS and just the Jensen inequality. So, this is something I studied with Xin Yuli, and you take tau between 0 and 1, and this is perfectly fine. Okay, and then first surprise is, okay, you can also take tau bigger than one, and you still have the inequality, and you have to be careful because it changes the sign here and it changes this coefficient, this coefficient, and you have to be a bit careful. This coefficient, you have to be a bit careful. It's a bit striking. You can even take the limit tau going to infinity, and you get again a nice inequality, which tells you that you can control the potential term coming from the Poisson equation in terms of the mass and the kind of logarithmic moment. Straight. Yeah, of course, everything is fine. You have the homogeneity. Fine. Now you have the homogeneity in the sense that you have one over m in the quadratic term, so everything is one homogeneous. You can multiply by constant, you can eventually scale, and there is no extraction. Okay, so this inequality here, of course, is not very difficult. It's Carson type in some sense, and you can prove it directly. And then you can reinterpret this inequality as a kind of interpolation between log HLS. Interpolation between log HLS and these inequality. So this is fine. Free energy point of view. Okay, so essentially I consider the same energy written in terms of rho. So rho was modulus of u squared for Schrodinger. And at the moment, I don't have a gradient u in L2 term. Okay. And you can already do a few things and start playing with the parameters. So here I fix the entropy or logo. Things, the entropy, roll of rho, and I play with these two parameters. Of course, this is related with a Keller's siegel type inequality with a critical gauge potential V here. And the result is that you can bound this quantity from below for a fixed mass, which you can rephrase in an interpolation equity if you wish. Okay, and there is a result. You wish. Okay, and there is a result where you have a certain range of parameters for which you have a lower bound, and threshold cases for which you know the value of the constant, and outside of this region, no lower bound. Okay, so this is the graphic. Okay, we have these two parameters inside the white area. There is a minimizer. This dotted line is the one I had found with Axinu earlier, and you can You earlier, and you can extend it a little bit, and you know a few things. For instance, on this line, the inequality is not achieved, but you know what is the optimal constant. It's finite. Here, you don't know actually, so it's open. And in the gray area, there is no such inequality. Okay, proofs. Well, okay, you have, of course, to use the standard tricks of translating. So this deals with the A negative. This deals with the A negative. Use scaling, so T is enough to end the T is upper line here, or do something a bit more subtle, but not a big deal. Concentration appears again to prove that above T's line, you cannot get an inequality. But on the line, we don't know yet. To prove that the inequality is achieved on the first on the first threshold b equals a minus 2 a minus 1 it's a bit more tricky and again you have a relaxed problem so the relaxed problem well it reminds you can type in quality in some sense and you actually use a scaling reduce the problem sorry why is it okay i'm not that full screen sorry so you use a scaling So, you use a scaling and you get a relaxed problem that you can study. So, I should like to prove this. Oops, sorry, that okay, this right-hand side here has a minimizer. And it's achieved only in the limiting regime as lambda goes to infinity. You can prove that it's not achieved at the level of the free energy. The free energy assuming, well, it's bounced from below. Mean well, it's bounced from below, but there is no minimizer. Okay, so now you can start playing a little bit more, and now I put a coefficient C here. And again, it's tricky because it's a whole log row, you have to be careful, it changes sign. So the way you handle it, and you see, for instance, okay, turns out that there is no minimizer, but the way you prove it is by introducing a sound. Introducing a sum of, yeah, I'm sorry because you don't see the bottom line. Okay, so we can concentrate. You concentrate on various points and each of these points concentrate and have to be located at a certain distance. And so it's a kind of array of points. And then this sends you the free energy to minus infinity. Okay, so conclusions, Schrodinger energy. Conclusions here, Schrodinger energy. Well, you have one more thing to interpolate with, which is the ultralom of the gradient. There is a very nice thing you can do is to use the Euclidean logarithm of LF inequality, combine it with the HLS inequality, and you get this new inequality, which is here with an optimal constant. Actually, it is 2 pi. This 2 pi is fixed by the scaling. No way you can change it. It's not an optimal inequality in the sense that I combine two different inequalities which have not the same optimizer. Have not the same optimizer, and you can prove actually that there is a lower order correction term. You can put an L2 norm here of you with some constant. Anyway, this is another story. Coming back to Schrodinger, well now it gives you a rather complete answer. We have the parameters alpha, beta, and gamma char here. You have a statement, I don't have time to explain it, but it gives you beautiful domains for which you get a result. Get a result. Well, it's almost complete, so that you in the dark area, you have that Schrodinger energy is bounded from below and probably minimizes. Well, we don't do it, but it shouldn't be so difficult to prove it. No, you have some special division which are reminding the various limiting curves that you add for the free energy. There is a small corner here for which we still don't have the ends. Here, for which we still don't have the answer, we don't know if it's very familiar or not. Shouldn't be so complicated, but actually, we didn't have time to study this. So, okay, I apologize for being a bit long. If you want to see the slides, they are on my webpage, and you can also find the papers there. Thank you very much for your attention.