And also, thank the workshop on organizers for inviting me to this wonderful event. Okay, so today what I want to share is the statistical inference for pairwise comparison models. Actually, this topic is quite similar to the field that Professor Jules talked yesterday. And but in our study, we just focus on the directed graphs and we focus on the specifics. We focus on the specific kind of data that is pairwise comparison data. And this job actually is the joint work with my former classmates, Rejen Han and Yimishi. Actually, we are undergraduate classmates in pure math, but now we are all the distinctions in academic. Sorry. Yeah, but this is the recent work. Yeah, our recent work last year, yeah, December. And so we focus on, we want to do this research because we are all fan of tennis games and we want to find a practical model that can model the ranking of the tennis players. So we refer to these pairwise comparison models. This pairwise comparison models because this model, this kind of model can model the pairwise comparison data like the ranking data. So the pairwise comparison model actually is the special case of the ranking models. But in pairwise comparison model, it only focused on the relative preference of strength between the two subjects, the pairwise subjects. And these models are very And these models are very popular nowadays, even in the large language models. And later I will show how we can adapt it, we can use our statistical models to be adaptive to the large language model or AI trend. Okay, so but at the beginning, our motivation is from the tennis games. The tennis game, actually, the tennis game data is the data of comparisons. We just compared the We just compare the, we just have the two players and they play with each other, right? And one can defeat each other. In one game, there are two outcomes, win or lose for one player. So in total, we will have n subjects, n players. But in practice, these players, they are not fully connected. They are always new players who do not play games with. Do not play games with all of the players, right? So, such for this kind of observation, we have n by n matrix, but such matrix actually is incomplete matrix. We will always have some entries that is not observed. We will always have two players that will not play games with each other. So, our goal actually is to like to complete such incomplex matrix using. Such an incomplex matrix using some statistical models. And one of the very popular models is the Bladley Perry model. So maybe you have already heard of that. So this is the very famous and popular fundamental pairwise model. So we just assume that each player, they have the latent score. Such latent score actually just shows the strengths, the latent strengths or the The latent strengths or the some general strengths of one player. So, for each player, they have their own score, and then based on this score, we can calculate, we can model the probability of wins. So, the probability of wins always involves the pairwise subjects. So, the probability of the i flare wins the j player can be modeled by this, some kind of logic. This is some kind of logistic. So now we just assume that this u is a positive value, but usually we just let this u equals to exponential of the exponential of x. So such x can be in the real space, right? So that will be the logistic model. So this is the very famous valid tarry model. But in practice, actually, the data. The data structure will be more complex. It's not win or lose. It will be in a BO3 game, it will have scores, right? You can win two games, you can win one games, and there are always the ties, right? So for this simple model, it doesn't consider such complex structures. So we just focus, we just consider the general type of pairwise comparison models. Pairwise comparison models. So the general pairwise comparison models just generalize the traditional models to handle the more complex data structures, more data type of the outcomes, and more complex comparisons. Okay, so this model actually can be, oh, this is the type, this will be the directed graph. Be the directed graph. Actually, the models are directed graph, but such directed graph can be, you can interpret it as the multiplication of an adjacency matrix and the score matrix. For adjacency matrix, it just reveals the links of each vertices. This is a given sensor matrix. And for score matrix in this page, the score matrix. In this page, the score matrix X, it just shows the scores, the potential outcomes, the outcomes of each player, the outcomes of every entry. So in total, the observations actually consist of the multiplication of the agency graph and the score matrix. Okay. And then since we have And then, since we have a genesising matrix, actually, we can have the graph neighborhood of each subject. Sorry. Okay, so now this is the very important part of the general pairwise model because we didn't focus on specific kind of data or specific kind of model. We just want to have the general form. Want to have the general form for the pairwise data, for the pairwise comparison data. So we just use this f function to model the probability of wins, probability of the outcomes. So this each outcome, possible outcome comparison data Xij actually can be modeled by this conditional probability. Conditional probability. So, this conditional probability is conditional on the difference of their latent scores. So, this is the general form of the comparison outcomes, right? But actually, this F must satisfy some specific assumptions in order to adapt it to different scenarios, different cases of the comparison models, not only in the breadth tally, but also the Not only the breadth tally, but also the depth model, the raw copper model, and the paired cardinal models. Okay. Oh, no. It can be categorical now. It can be categorical and it can be even continuous. Yeah. So now this is a general form. It is not only zero and one now. Yeah, yeah, yeah. The full matrix is observed, but actually, your observation is an incomplex matrix. Yeah. Okay. Yeah, actually, we will have a sumption for this function f, and this actually corresponds. Actually, correspond to this u, the difference of u, we use y to denote it must be bounded. Another URI but we focus on the difference. We focus on the difference because this is something like the component. Is something like the comparison models. We only look at the whether win or lose, but not the specific, the absolute value of the use. Actually, this u is not identifiable if we do not have any constraint. So we only model on the difference of view. Okay, so this function f This function f must satisfy these assumptions in order to adapt it to different most cases of the comparison models. Actually, the examples we show later will all be the special case of this general form. This function f, first it is a probability mass, right? It is the density function. So the normalization condition actually is. Condition actually is quite regular, and then is the symmetry of why and we need this symmetry condition because we must know that if I win straight, the probability of y win stray must equal to the probability of i, right? So this symmetry actually is the it Um, it is because the nature of the comparison data, yeah, and then monotonicity. So, this is the monotonicity just based on the difference of this y is the difference of their latent scores, right? So, if their latent score difference is too large, which means that they will, their win probability will be so if x is So, if x is smaller than zero, then the win probability will be quite small, yeah, quite low. And then it's boundless. And the first four, actually, they are the regular assumptions. If we just observe the general form of the comparison data, and the assumption five actually is the assumption that is required for. Assumption that is required for the good properties of the estimators. Okay. So based on this, based on these assumptions, we can have the general model for the outcomes, for the comparison outcomes. And now, based on this one, we can get a full matrix of X, right? But actually, in practice, the observation. Observation is not fully observed. So we should define a random graph models to model the comparison graphs. So the comparison graphs actually is sampled from this random graph models. And this random graph in this random graph models, the edge is actually generated by the uniform distribution with the deterministic probability. Deterministic probability: this Pij belongs to this range. And in homogeneous setting, this the order of this lower bound and upper bound of this range, the Pn over Qn, actually they are of the same order. But in our case, we can allow a slightly heterogeneity to the observed data, which allow this to diverge, this fraction to diverge. So, this is the combined with this outcome matrix and the random graph and this comparison graph matrix, then we can obtain our observation. And then our whole analysis is based on this, based on the observations here. And then in our work, we're not only Our work, we not only focus on the estimation, but also focus on the statistical inference because we want some uncertainty quantification for our estimations. And also, we want to carry out the asymptotic normality so that we can do further testing and some confidence interval, some further inference on our data. And such is the On our data. And such asymptotic normality actually is studied previously in several works. But however, they cannot be directly deduced from the existing work of the Vladitarian model to our general comparison models. So this is a comparison of other works in our works. Our work is more Our work is more general and heterogeneous and allow more sparsity. Okay, so for the estimation, we just need to maximize this likelihood. So this is a constraint optimization because we have to have the identifiability constraints on the latent scores, true latent scores. The general uh model that can uh yeah, yeah, BT is a special case, but we can contain more. Special case, but we can contain more. Yeah. Oh, that is the yeah, this is a special case. This is the special case. But in our work, we just focus on this, this F, the F that satisfy these assumptions. Yeah. These assumptions. Yeah, we do not have the specific form of this F. We just need some assumptions for this F so this F can be representative to most of the pairwise comparison models. Oh, yeah, yeah, yeah, yeah, yeah. That I did that. Oh, yeah, yeah, yeah, yeah, yeah. Oh, yeah, yeah, yeah. You are right. So, uh, previously, I when I made this slide, I forgot to make it to be the more common form of the exponential, the logistic form. Actually, if you turn it to be the logistic form, the exponential of u and also the exponentials, and then it will be the, yeah, yeah, yeah, yeah, yeah. Sorry, sorry for making you confused. Yeah, yeah, yeah. Confused. Yeah, yeah, yeah. Just now I just mentioned that, yeah, but I forgot to, yeah. Okay, so this is, yeah. So for the estimator, actually, it's just directly derived by this maximum likelihood. Yeah, maximizing this likelihood, we can get the MLE U hat. U hat. And such U hat will have very good purpose, have good statistical property based on those assumptions. And the first assumption actually is the assumption to ensure the unique existence of MLE under this optimal sparsity condition. Yeah, and then these are some regular assumptions on the derivatives. Assumptions on the derivatives to carry out the asymptotic normality. Okay, so this is the asymptotic normality we have derived for our general comparison model. And this general comparison model, actually we just focus on this f, right? And you can see that this asymptotic variance is quite, yeah, in the concise form. Yeah, in the concise form. So, whatever comparison model you obtain, you just figure out your form of F and plug in your F here, and then you can calculate the asymptotic variance directly. So, this is the very beautiful form of the asymptotic variance. And for the proof, I want to mention that for the proof, we are We are different from others is that we just use the spectral expansion, and then we can have the, we just decompose the pseudo-inverse in the Hazen matrix pseudo-inverse into this form of orthogonal projectors so that we can have a lot of forms, a lot of products to be zero, and then our proof will be much more easier. Yeah, so what we need. Easier. Yeah. So, what we need to do actually is to verify these three statements. And in this statement, what I want to mention is that to verify this, we just use the leave one out technique similar to Gao Chao's paper. Because when we want to carry out the element-wise variance of UI hat, it will involve UI hat itself. So we have to leave the UI this. The UI, this element out when we carry out the asymptotic variance. So, sorry? Oh no, not dense. Yeah, not dense. Oh, this the sparsity actually is this is log n over m p. Over NP, yeah. So it's n minus the omega and minus one, yeah. Sorry. The the convergence rate, right? So that is this square root. Square root, square root of this, square root of this, yeah. For this is an element-wise, element-wise, yeah, yeah, yeah. Number of neighbors, yeah, yeah, number of neighbors smaller than and yeah, yeah. Summation of this, yeah, this number is the number of the uh. The number of the adjacency neighbors, yeah, of i. Yeah, so for different elements, they will have their own convergence rate. Sorry? Very slow. Actually, yeah. Yeah, it depends on how many labels you have. Actually, yeah. If if it's farce, then yeah, price slow. Yeah, so yeah, based on the previous proof, we can also have other byproducts like this individual estimation error, because previously, previous work, we always consider the overall estimation error. We just consider the infinity norm errors, but now we can have the Have the estimation error for individuals for each subject, like previously what we have shown. This is the asymptotic variance for individuals, right? So the element-wise variance. So here we also have the element-wise estimation error, and the estimation error actually depends on the degrees, this, this, um, yeah, um, number of the Number of the genesis enables. So, this is this just serves a local normalization factor for each individual. Yeah, for each individual, we can have their own estimation error. And also, since we have the normality, right, so the direct opinion is to just carry out the confidence intervals. So, the confidence intervals is carried out by plugging. intervals is carried out by plugging the variance estimator. Sorry, the typo, this will be J. Okay. So we just plugging the variance estimator and then to obtain this confidence interval. So this is the confidence interval for our general comparison models. And no matter what model you have, what matter what model you have, what comparison model you have, if you have the specific form, you just check whether it just satisfies those assumptions. If it just satisfies those five assumptions, then you can directly plug in the formulas and obtain the confidence interval easily. I had yeah yeah yeah I had different drone awesome yeah yeah yeah yeah okay so we in this work we haven't uh carried out this this part yeah we just focus on the aspect We just focus on the estimation itself, estimator itself. So you mean we should focus on the y, right? Yeah, yeah. Yeah, the difference. Yeah. But this is individually this one? Yeah, but but this is also for the individuals, yeah I think it can be directly derived if they are as empathically independent. Our work the work we refer to, they always focus on the latent scores, the estimation of latent scores themselves. The rankings, right? But this is a pairwise. Yeah. But for ranking, this is another framework. We just, this is a pairwise. We only have two subjects. For ranking, we have the black pit loose model, right? So that is not in our framework because in our framework, we only have two comparisons. So, yeah, well, then the computation of such confidence interval actually is pretty straightforward when we plug in the estimators. So, one very important thing is to verify whether our model works, right, or whether our general model is really general. So we just Is really general. So we just test on several specific models like the BT, the BT we have seen before, right? So, and then is the system most stellar models. And this model, we just replaced the previous logistic by this probi model. And this is also the two outcome model. Yeah, we focus on one and minus one. And the asymptotic variance actually. And the asymptotic variance actually, we just plug in the form of the model into the formula and can directly get the asymptotic variance. And what I want to mention is that the condition, the convergence condition actually for different model is different. So it is model-based. For different model, you will have different conditions for convergence. So, and then the raw cooper model. So, for this model, we not only focus on the win or lose, we also focus on the ties. And this model actually is very important, which I will show later. And for this model, it just involves a region that is for tie, and then it involves the parameters for this tie probability. For this high probabilities, this is the threshold parameter, which is predetermined. And then the asymptotic variance is also easily derived. And then there is a model. This is also another model considering the ties, but now the parameter is not larger than one, but larger than zero. And remember, And remember that their convergence condition actually for different models, they are different. You should look at your specific model to calculate the constants, those constants. And then this is the cumulative link model with work outcomes. So this model is Is quite popular nowadays because nowadays we not only focus on the win or lose, but we also have BO3, BO5, right? Best of three, best of five. So in best of three, we will have possibly four outcomes, right? So one will win twice, one will win once, or one will lose once, lose twice. So in this case, in B03 game, this is the cumulative. This is the cumulative link with the link model with the four outcomes. And for the VO5, we will have more outcomes. So these are categorical models. And for this model, you can also calculate the constant and to verify the convergence and then get the asymptotic variance. Okay, and the last one is the one I mentioned before. One I mentioned before, that is the continuous outcome. So, in cumulative link model, we have the categorical outcome, but for multiple categories, right? But for now, this paired cardinal model, we just focus on the continuous outcome. Like in the large language model, the experts usually give the specific score to your answer, other than it's good or not, right? It's good or bad. It's good or bad. So, in this case, we just use this paired cardinal model to model the continuous outcome. And what I want to mention is that for the asymptotic variance for this paired cardinal model, actually it has, it is, the asymptotic variance is independent of the two latents for U star. So, it only depends on your variance parameter and Variance parameter and the number of the adjacent neighbors. So now these are the so for most of these or for all of these pairwise comparison models, our general model can be the representative of them. And I believe for other models, for for other models, for other very rare, rare comparison models, we can still be the representative to rows to those comparison models. And now I want to show the numerical studies of our work. So the goal of our work actually is the inference, right? So we just compare these three models and show models and to show the to show the coverage prob coverage probability and standard deviation of each model. And so for this for CarlyMo model, actually the CP they just remain because we have known that it such a paired cardinal model, they just Paired cardinal model, they just have nothing to do. The asymptotic variance actually is independent of the true latent variables. Okay, and we can say that all the coverage probability, they just reach at the desired levels, desired coverage. Aggregate all the network. Yeah, we just have several graphs, and the maximum likelihood is just a product of those graphs. Do you mean that? So this is the, and the normality actually can be verified by this QQ plot. We have the QQ plot for those methods, and we can see that the QQ plot matches the The QQ plot matches the normal distribution very well. And then we just come to the raw data part. So for this raw data part, actually we just now I say that we three, me and my co-author, and we are all tennis fans. And there is a big discussion in the tennis forums. The tennis forums that whether Andy Murray can be included into big three or to become big four, or just there is only big three and Andy Murray can be excluded. So previously, there is no scientific analysis for that, right? But now we consider we can use the statistical tools to testing whether ND Muri is big four. Andy Murray is big for. So we just use the ATP data and which include the result for how many years? Nearly 70 years, right? Across the over 2,000 players. And then we just focus on the best of three matches because in some openings they are BO5, right? But now we just focus on the B03. Now we just focus on the BO3. So these are normal games, normal competitions. And we just use the cumulative link function with four outcomes I mentioned before. And then we plug in the data into the cumulative link model and then carry out the latent score, estimated latent score, and the estimated standard deviations. So we can see. Deviations. So we can see that these are the first four ranks of our analysis. So we can see that it just shows the scores which matches the reality, right? So the big three, actually, they are very similar in any movie, although his score is quite far from the other three. From the other three. But how about testing? Because we have their estimated standard deviation. So we just do the hypothesis testing for those player scores. So the null hypothesis is whether Andy Murray's score equals to other three. And then alternative hypothesis is not equal, right? And then we just do the hypothesis testing, use these statistics and get the p-values. And get the p-values. And since this is a multiple testing, we just adopt the Benjamin Hopberg procedure and let alpha to be alpha over 3 and alpha over 2. And then, but we find out that the p-values actually, they exceed the corresponding critical values. So, which implies that the difference latent scores between the big three and Andy Marie actually is not that statistically significant. Significant. So we think that actually anti-murray can be included to be the big four. So if you are the anti-murray span, you will be happy, right? We don't look at the fifth one because when we look at the forum, they usually discuss big three or big four, but no one care about the big one. Okay. Okay. And big 100, right? Yeah, yeah, yeah. We just use the data in 2016. Yeah, yeah, yeah. Because maybe after the COVID, they will be largely different. Right. So currently, we just focus on some causal effect of the COVID-19 on the rankings. 19 on the rankings on their latent scores on this comparison models, but these are our undergoing work. And now, these are some traditional statistical testing for the ATP players, right? And now I want to show what our statistician can do to use the traditional. The traditional comparison model to improve the performance of large language model. So, if you have ever used large language model, I don't know whether you have used ChatGPT before. So, when you regenerate, when you are unsatisfied with your previous answer and you regenerate the answer, there will be three buttons here, right? So, they will ask you whether this. So they will ask you whether this response is better or worse than the previous one. So better, worse, or the same, right? So these are some survey that ChatGTT will give you when you regenerate some answers. And based on your answer, they will fine-tuning their model, their language model to make their answers. To make their answer to satisfy you better. Yeah. So this is some fine-tuning from the using the humans feedback. Right. But directly, previously, they just used some expert to give the scores, to give the score of each answer. So give the 80 score, 100 scores, but that cost is too high. It is too expensive. is too expensive but if they use they just use their customer they just use their customer to give this to to click these three buttons it will be much cheaper for them to collect the feedbacks so in this case um the trajectory segment actually is the observation and the actions so the observation and actions in each stage can be um can be represented in pairs Represented in pairs of this sigma. And actually, human preference can be generated by a reward function that if the reward of the first model is larger than the reward using the parameters in the second model, then we just choose the first, we just have the preference over the first preference of first. Preference of first one over the second one. So, this is the deep reinforcement learning from human preference and their reward model, their reward function in financial actually is the bladder tarry model. So, they just prefer the first over the second, prefer the first answer over the second answer. So, this is the black. second answer. So this is the validary model. And what they do is to maximize this minimize this negative likelihood, right? The log likelihood. So this is exactly what we have shown in previous slides. But in their CS people, they just use this one to do the fine tunings. So in this case, case what they do actually is to plug in the data plugin on the plugin with the plugging this preference into the black parry model and use the deep reinforcement learning to train this model but we notice that they just use the accept or reject but there is a button same right they didn't use that and let's look at the That and let's look at the SHP data set. So, for this HSP data set, this is a human preference data set over some responses. They have the choice A, choice B, and the human give the comments of A and commons of B and give the scores. And this is a very commonly used training data set in large language models. But we notice that in their model, they just choose. They just drop the weak signals, which means that they just use the four ratio smaller than 0.5 and for ratio larger than 2. And what in the middle, there is a large amount of samples, they just remove the whole samples. So, in this case, we consider that, or we just think that the black terry model actually is. The black terry model actually is too limited. We should consider the general, more general forms, right? So, this is another motivation to propose such general compressor model. And if we consider Davis model, which consider the pi's, then which we just involve the weak signals. Actually, the data set is large amount, but we just choose 10,000 and use 10%. Uh, um, 10 percent of the weak signals, but in practice, the weak signals consist of 40 percent, but they just directly remove them from the observations from the training samples. But if we involve them, we will find that the reward accuracies will highly increase. So, I think from this point of view, actually, we can largely improve the performance of large language. Performance of large language model by using the more general comparison models. Yeah. And this is a preliminary study. And we just use small sample size and use A100 to run for one hour and get this result. But if we just run the large models, I think the result will be much more better. And this is our undergoing work. So for conclusion, we just establish. In conclusion, we just establish an asymptotic normality for the general compressor models. And we also provide the easily computed formulas to construct the confidence intervals and statistical testing. And we just show some practical implementation of such general comparison models and a promising avenue. And a promising avenue for future investigation actually is towards some to use some machine learning techniques to involve these general compressor models and to make the test those tasks to improve the performance of some specific tasks. Okay, so these are all for today's talk. Thank you so much. But think about games and good players, whether you know what is the winning. So what is the winning part of the probability? Usually just a scalar may not be enough to capture all the characteristics of the scalar. They have different characteristics. Yeah, yeah, yeah. Or we just consider the general explorer. They have used some not this model, but beta model, I think. Model, but beta model, I think Yanting has proposed a lot of scalar for HU. Yeah, but they have involved in some existing works, they just involve some covariance, covariance, some some um latent layer for that to to uh interpret such view. Yeah. I think the I think the not coherent yeah yeah yeah yeah yeah oh latent oh yeah yeah yeah oh okay I see oh I see so um this is largely uh related to your talk yesterday right your last work yeah yeah yeah I I think that yeah I think it's very interesting yeah yeah yeah I think that's very interesting that will be our possible That will be our possible future work. Yeah. Thank you so much. Thank you. Okay. Thank you. Then you hope that one can recover the ranking of the use of the supposition. Sorry, what uh you mean the whole vector of u? You mean the whole vector of u? Yeah, so you mean the absolute value of U actually is not that not of our interest. We should focus on the difference, right? So that is Professor Ju's, similar to Professor Ju's question. Yeah, yeah, yeah. Yeah, yeah, yeah. The original ranking, right? Yeah, ranked the use. Yeah. But this is a parallel comparison model. So we can use another model that is black loose model that is for specifically for ranking for several players. Yeah, for several players. For several colours. But for this model, for this model, we just consider two comparisons. Yeah, pairwise comparisons. Oh, neural network. Yeah, yeah. That's interesting. That's interesting. Thank you so much. I will have a try. Thank you.