Thank you so much, and thank you to the organizers for inviting me to this wonderful workshop. I'm going to continue a little bit with the theme that verbal Esreal had and talk about neural networks. And I know that this is probably going to be new to some people. So there will be an introduction about, I call it something old, it's at least 10 years old, something new, which is the non-linear diffusion aspect. There's going to be something borrowed, which is my variation on the Is my variation on the theme by Linek Giza and Franz Spach. And as you can tell, I've got more slides than minutes, so there's a good chance that we'll not actually cover that. But feel free to talk to me about it if you have that interest. And then something too. So let's do the conclusion. All right. Thank you so much. All right. So what is machine learning? Ultimately, machine learning is a toolbox to solve problems that are not super well posed. So in this case, comparing apples and oranges or distinguishing apples from oranges. So we can all see the pictures in the top are very similar to the Pictures in the top are very similar to the pictures in the bottom, but all the pictures in the bottom display apples, all the pictures in the top display oranges. So we have a problem where we want to distinguish two objects and we can't easily devise a good measure of similarity and we can't directly come up with a good model to solve this. So instead, we have this, we try to approach this by leveraging a vast amount of data that we have. A vast amount of data that we have. So, for example, our problem could be classifying images or predicting the next word in a sentence. And we've got a large amount of labeled data. So labeled images from online or newspapers, books, all the written words in the human language. Now, the third ingredient that we need is a model which has a large number of tunable parameters, for example, a deep neural network. And in the simplest case, this could be these RGB images, which I showed you, which have, say, 256 by 256 pixels. They have three color channels. That is roughly 200,000 variables to describe one of these images if we describe it by pixel values, which may not be the ideal way to describe an image, but it is the one that typically computer scientists give us. So we've got a function model which takes an image and some parameter. An image and some parameter, and so our goal is to find a parameter such that we can distinguish: do I have a stick or something? Doesn't matter. So we try to find a parameter such that the parametrized function can actually distinguish the apples from the oranges. So, for example, we try to interpret the output, a positive output as saying the model tells us it's an apple, a negative output. us it's an apple, a negative output it tells us it's an orange. And this is not super deep or anything. We essentially know the labels yi and we try to minimize the misfit or the expected average misfit of the output of our model to the true label that we know. This is what's called supervised learning because we know the labels. This is something that we've been doing for like decades, if not centuries at this point. Centuries at this point. Generally, the model classes were linear functions or polynomials, right? This is trying to match a label to a parameterized function is something that we see in the first week of a numerical analysis course. Just traditionally, those functions would have been linear functions or polynomials. Nowadays, it's deep neural networks. And so, that is deep neural networks, it's what's called deep learning. And in traditional models, we could usually solve for an optimal parameter in some sense. In deep learning, we can't because the model is more complicated. So we have some optimization that comes into it. But ultimately, the setup is very, very classical. The big problem that we encounter in machine learning applications is that these problems happen in very high dimension. So for each. In very high dimension. So, for even a very small 256 by 256 image, we've got a 200,000-dimensional space. So, we can't really do polynomials in 200,000 dimensions. It's not going to be a very productive approach. A neural network. Sorry. Let me just use the keyboard if that works. Yeah. So, a neural network is a function which has which is Which is composed by alternating some affine linear maps. So we've got an input here. We apply an affine linear map to some higher dimensional space. Here in point, we apply a coordinate-wise non-linearity, for example, by throwing away all the negative parts of the numbers. And then we repeat and repeat and repeat it. How exactly this process works is not going to be super important for this talk, but for example, This talk, but for example, if the linear maps are convolutional, then we call this a convolutional neural network or CNN. So that's what you've probably heard a lot about if you've been reading about artificial intelligence. So we've got, most importantly, this is a non-linearly parameterized function class because we alternate between something linear. So it's linear in the parameters of these linear maps, but the parameters of these linear maps. The parameters of these linear maps, so these matrix entries, are not going, it's not going to be linear in those. And that is where all the complexity comes from. That's where the optimization comes in. So why do we use neural networks out of all the useful function classes? So there are probably a lot of things that you could say about this, but one of the takes is that there is a good function class which goes by a number of names. Which goes by a number of names depending on which authors you read, where we can say that if we've got even a shallow neural network, so nothing deep, nothing super complicated, we can approximate all functions in this function class up to error of order one over root m using order of m parameters in any dimension while every linear method of approximation. So, if we've got any Approximation. So if we've got any m-dimensional space in our function space and try to approximate by elements of this space, we've got a lower bound that scales like m to the minus 1 over d. So if d is 200,000, 1 over 200,000 is almost zero. So basically you can pour in as many parameters as you want and you're not going to reduce your approximation error. While the non-linear method of approximation by neural networks actually performs really well. Works actually performs really well. This, if you're coming from approximation theory, this is called the Kolmogorov width of the function class. So we've got a lower bound on approximation by linear methods. Additionally, this space has a really useful property, namely, we can estimate integral quantities really well by sample averages uniformly over the function class. So this is the So, this is the Monte Carlo rate. This is kind of the central limit theorem scaling. And usually, you get this for a fixed function. Here, we get it over the entire function class. And that's really important because what do we do? We get a finite data set. We try to optimize something for a specific purpose using this finite data set. And then we want it really to do well on data that it hasn't seen before, because otherwise we've memorized our data set, and that's not super useful. Second, that's not super useful. So, the fact that estimating averages like this guy is essentially one of those functions that we want to estimate. Knowing that this average approach is close uniformly over the function class is one of the key points here. So, we've got something that performs really well in terms of function approximation and something that performs really well in terms of estimating the average performance using. The average performance using fairly few data points. Whereas the catch, why is this not a super great problem? Well, it's because of the training aspect. We are trying to minimize a function. We are trying to find the deepest valley in a complicated landscape, and that can be a really hard task. So minimizing a function, in this case, I call it R, is the task of finding a valley in a landscape where we've got horizontal coordinates. Whether we've got horizontal coordinates corresponding to the parameters, and the function gives us the height over some elevation. And if we were given this in real life, we would probably try to draw a map, but our models have at least a couple of thousand parameters for even a simple model. If you go to GPT-3, that's the thing that ChatGPT is based on, that has 175 billion parameters. Parameters. That's several, that's, I think, on the order of between 10 and 100 gigabytes, just all the parameters of the model. So we can't really explore the parameter space because of the high dimension. So instead, for this optimization process, we follow the trajectory of a particle where we have the hope that it takes us to a good point in our parameter space. That can be That can be a hiker who carefully at every point looks where is my direction of steepest descent, and I follow that and I always decrease my function. Or that can be a heavy ball that's just barreling down the mountainside. So that one of them is the gradient flow, the other one is essentially Newton's second law. The gradient descent selects the locally optimal. Locally optimal direction, and it is very robust, but it is incredibly slow. Whereas, using momentum, we actually leverage more global information because our particles have a bit of memory what the slope used to be. And so that is very fast, and we can prove that it works really well in convex optimization, but we can't really prove very much about it in general. This is what we use, probably 99. This is what we use probably 99 out of 100 times. My talk is going to focus on this one, in part because it's easier to analyze and in part because it better fits the theme of the workshop. But if you want to talk about momentum, I have, I don't know, it's a topic that I'm recently really excited about. So come find me after the talk. So my first bigger part of this is going to be about nonlinear diffusion and how that comes into this optimization story. Optimization story. So before we go deeper into specifics for neural networks, we need to agree on one thing, which is we always have more parameters than data points. This was found to be very beneficial from the standpoint of optimization because it eliminates strict local minima in our landscape. It gets rid of a lot of points where we could get stuck because. Points where we could get stuck because it always gives us some direction in which we can improve the model. So it makes the optimization process a lot easier. It also benefits us in terms of generalizing to previously unseen data points. The idea there is if you've got barely enough parameters to really match your data, you can do it, but you have to contort yourself into some weird shapes. Whereas if you've got a huge number of parameters, If you've got a huge number of parameters, then you can actually find a much more simple explanation because you have a much better overview. Obviously, if you've got a neural network which has a lot of parameters, then you can find an easy explanation. You can also find a really, really complicated explanation. And the set of parameters that really interpolates all your given data exactly is typically. Exactly, is typically, right, you've got m parameters, n constraints, it's going to be an m minus n dimensional manifold. You can make this precise for almost all labels and under some additional assumptions. So, however, we've got this huge number of different parameters that all fit our prescribed data exactly. In practice, we often find parameters. Find parameters that actually perform well on previously unseen data. So why do we find those? And so the conjecture that was formulated in the late 90s was that we often find minimizers of the function where the objective function is in some sense flat. Now, why would this generalize better? Well, we can write one as say 10. Write one as say 10 to the 9 plus 1 minus 10 to the 9, or we can write it as 1/2 plus 1/2. The second one kind of feels more stable. We're not relying on some miraculous weird cancellations. And so this corresponds to smaller parameters, to something that is less steep, less unstable. And so this is the idea that this actually works better. This is also one of the reasons why it's conjectured. Of the reasons why it's conjectured that second-order methods like quasi-Newton methods don't work super well here. Because quasi-Newton methods try to get rid of the next order in the approximation, so you don't see the steepness or flatness quite as much anymore. There are other reasons why they're complicated, but this is one of the reasons why we don't necessarily expect them to do super well. So this is the function that we would like to minimize. That we would like to minimize. We've got the output of our model. We've got the output that we'd like to see. And we're trying to minimize the average misstate between those. Here, we can easily compute the gradient. That's not hard. Evaluating this gradient is hard, though. Because if we've got a big data set, say 10,000 points, which is not super big in the context of deep learning, this is somewhere over 10,000 points. Some over 10,000 points. If any one of these gradients at one of the data points takes a long time to evaluate, then this takes a huge amount of time to actually compute the gradient. It's not that we can't do it, although we may hit some numerical stability issues if we really, really wanted to. But it is not super useful. And so, what we typically do instead is we sub-sample our data set. is we sub-sample our data set and have what we call a batch or a mini batch of say 50 to maybe 500 data points, maybe 2000 if you're Google and you really use big batches. And with these, say, 50 data points, we just try to estimate what the true gradient should be. Oh, there's a gradient missing here. So the interesting thing about this is that About this, is that these estimates for our true gradient are actually exact on the manifold of minimizers because you interpolate all of these labels exactly. We have exactly zero on the manifold of minimizers. So even for the stochastic gradient estimates, this term is actually zero. So we have no stochasticity on the manifold of minimizers. We also typically have a very low rank. Have a very low rank noise in a very high dimensional space. So a possible, if we typically think of stochastic gradients coming from statistical mechanics, we would think of adding a Laplacian. That's not really a super useful model here because we've got this degeneracy in our noise. We also have the low rank in our noise. So a possibly better model. A possibly better model of stochasticity would be to say that the misfit between our gradient estimates and our true gradient is bounded by some constant times the objective function itself. I can actually prove this under a suitable Blipschitz condition on the model or something. And here, over-parametrization essentially says at the global minimum. Says at the global minimizers, the objective function is zero. Now, do we have a good way of seeing does this model actually prefer flat minimizers? That was kind of the starting point for me in this investigation. And everything is complicated as long as you have finite step sizes. So let's go to a continuum model. And the evolution of stochastic gradient descent in continuous time, this SD. Descent in continuous time, this SDE gives us a PDE model. So now I'm in the world where I'm more comfortable. So we've got a partial differential equation which describes the law of trajectories of this optimization process. And we can rewrite this. Right now, I understand that this is a lot to follow. So I'm asking you to trust me a little bit. But I think we've all seen. But I think we've all seen something similar pop up at some point. The details are different, but it's the same techniques essentially. I'm making the assumption that my noise is identity, so it's isotropic, but it's not homogeneous. Namely, I'm trying to capture the fact that the noise is zero at the set of minimizers, but I'm not trying to capture the lower rank structure. So we obviously have. Obviously, have that this function is a solution because that is what right that cancels out the, that makes the term inside the gradient constant. And under some reasonable growth assumptions, say that f only grows polynomially at infinity, this is the only solution of this equation. Of this equation. Unfortunately, to use this Liouville theorem, I need to assume that the infimum of F is strictly positive. And that is going to be one of the major sources of my discontent with this result. And if you want to talk about trying to, say, generalize these things a little bit, I would be very open to having this conversation because right now, practice and theory in deep learning are not exactly matching in a lot of places. So we've got an invariant distribution which is just given directly by this quantity of the L P D E. We've got the objective function here, f, and the exponent depends on the noise intensity. By comparison, right, if the infimum of f is zero, the density actually becomes infinite, whereas if we had some Gaussian noise, standard noise, then it's concentrated. Then it's concentrated but bounded. Under some reasonable assumptions, this density, this invariant distribution is integrable. So we actually have an invariant distribution. And the interesting part to me now comes as the limiting process. If I let my noise decrease towards a critical threshold, do I end up at minimizers where my objective function is flat? Where my objective function is flat in a reasonable sense? The answer is yes. Namely, we get if we take the noise scale intensity down to a critical threshold, then we converge to a distribution on the manifold of minimizes, which has a density proportional to some quantity that depends on the Hessian. And so we can interpret this as a flatness criterion. So we don't find a unit. So, we don't find a specific minimizer in this regime, but we tend to concentrate around minimizers where the objective landscape is flat. So I think this continuum model does explain some things about the optimization process in deep learning. I'm going to skip the discussion where I compare this to classical noise. The bigger question is: well, I've proved something about invariant. Is well, I've proved something about invariant distributions. Do I actually converge to an invariant distribution if I follow my parabolic PDE? And again, I'm going to assume here that I'm bounded from below and bounded from above by something that grows like one over theta squared. And this is the not over-parameterized regime. Again, source of frustration. But then, if I follow my PDE, this. This converges exponentially fast to zero. So essentially, we converge in a suitable sense exponentially fast to the invariant distribution. The proof uses crucially a Poincare-Hardy inequality, which was proved by some people who some of whom are in attendance at this conference. So I'm very grateful for this result. This result, um, there is a proof, um, there is a slight improvement of this. Like I said, I really wanted this for the case where my infimum can be zero, and I really wanted this in the realistic setting for deep learning. So I can get that a little bit, but not really. I can make my objective function zero at a finite number of points. I need a growth condition which prevents. I need a growth condition, which prevents it from being C2 smooth. I need a growth condition at infinity, which means that my gradients can't be Lipschitz, which is useful if I want to discretize anything. But long story short, you can push the envelope a little bit here, but it's tricky. And yeah, I would love to talk to people more about how to actually get a better convergence and a more realistic assumptions to the invariant distribution for my stochastic. To the invariant distribution for my stochastic radiant flow. The second story we're definitely going to skip. So, Gesein Bach proved a really interesting result. You heard all you need to know by Javier Fernandez-Real. And so, my conclusion is that over-parametrization is both a key concept and a real challenge in the analysis of deep learning. And we have some interesting PDE models that can say something. Can say something about what we're doing in practice. There are a bunch of challenges. I've not even started talking about the low rank in my noise models. And so I do think that there is a lot of room to get some new problems and some new models for some interesting diffusion equations. And with that, thank you for listening to me. So, thanks a lot. Do you have a question? Can you just go back to slide 16? The way that you went from the stochastic differential equation to the PD was a little bit too fast to be followed. So, just where did this specific power of F come from, right? Give me one second. This type before. Yeah. So we've got the eta sigma inside here. Eta sigma is so the eta is a model for the step size. So if you take smaller and smaller steps, eventually the stochasticity disappears. The sigma is the inherent stochastic scaling from my gradient estimate. So those two are just multiplicative. So that is in So that is in here. That is the diffusion, if you will. This is the advection. And then this is just algebraic manipulations. And so this works out very nicely if you want something that you can model in divergence form. Maybe I'll ask you later. Yeah, thanks. Okay. Thanks, Jeff. Well, I basically have the same question, but so as far as I remember, this stochastic process that we gave us like more standard operator, but I don't recognize it. I think the difference to what you usually get is that there is an F inside here and the F, the fact that you The fact that you get so both the diffusion and the advection come from the same function here. And I think that is where a lot of the analysis changes a little bit and where the PDE becomes really non-linear. I'm not sure that answers the question, but I'll claim that this is essentially the same thing that you. The same thing that you, the same machinery that you usually do in stochastic and applying stochastic analysis, you've got Ito's Remmer, and then you walk out with this equation instead. Fair enough, it's it's it is you're right, you are you are linear in row, you are. You are linear in row, you are degenerate. You've got a degenerate equation, but it is a linear equation. Sorry. Yeah. Also, do you foresee a way to write this, for example, the Delaplus Trammanifold with weights? Because this looks. I've not thought about that, but I wouldn't be surprised if you could do that. Yeah, with this F, I think. Yeah. Yeah. Well, and basically, yeah, no, that's that's an interesting thought. Yeah, I know that I went very fast, but I was hoping that people would have interesting suggestions. Okay, so we'll go and I'll possibly.