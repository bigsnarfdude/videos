And uh we have uh two tools. The first one uh by sorry by Einer Obermanner on the thermal video demo functional on graph with additional permit shows penalization. Thank you for the introduction and thanks to the organizers for reinviting us all here. For rewriting as well here. So, what I'm going to talk about is some rather old stuff. This is an article that I wrote five years ago, more or less. But the reason why I'm presenting it here is first the type of the conference. I think it fits rather nicely for that. Then there is, now as you see what I'm talking about, there's an obvious link with the work done by the organizers, the work on bad structures. And then thirdly, I think nobody has heard this talk before, except. Has heard this talk before? Katrina, I'm not so sure. So maybe she bite. Alright, so let's get going. I'll start off with some motivational stuff. There's going to be no mathematics in the first five minutes of this talk. What we're interested in are mathematical biology, maybe engineering, and we look at Engineering, and we look at elastic vesicles. So think of cells, for example. We look at the elastic properties of those. And in particular, what engineers and biologists were interested in at the moment were these irregularly shaped vesicles. So here you have these sunnellobacteria. You see the cells of the sun bacteria. They have these sharp foals, but they don't have. But they don't have the shape of virus capsids either. Virus capsids are these very regular shapes, surfaces of polyedra. So this is neither spherical nor is it this regular polyhedral form. And one would like to come up with a modeling device that gives you those as minimizer or so manager, or almost as minimizer as what in the engineering literature was discussed for a The engineering literature was discussed for a while was the suggestion that one of the drivers of the development of these sharp holes should be elastic inhomogeneities in the vesicle. So that your vesicle should be composed of two materials, in a simple model, one soft and one hard. And that the arrangement of the soft and the hard phase is part of the three variables. So the amount of Variables. So the amount is fixed, but you can freely arrange the soft and the hard material in your topological sphere. Alright, and the idea is then that you get something as in these numerical simulations. So the blue phase is the soft phase, and the idea is that it focuses in some ridges, and then it's cheap to bend along those ridges, which allows the red material, the hard one. The hard one to stay roughly flat, not to have to a little purge. That's the idea behind it. I know it's not an engineering talk, but can you address a little bit the mechanism behind creation of the soft and hard materials? Well, it's two different elastic moduli. Yeah, the bacteria, I mean. Oh, no, sorry. Oh, can't help you with that. But yeah. But yeah, in principle, of course, an interesting question. All right. So, but if you approach this from a calculus and variations point of view, you're immediately a bit skeptical about this idea. Because if you have two different materials, what we've known for some while now, it's 40 years, that in general what you should expect is some. Is some homogenization-like effect. So that you have infinitely fine phase mixtures between the soft and the hard material for optimal structures in general. And yeah, so here the classical works by Mevatz Artaf, Rofi Charkaf, Constrang, of course, others. Most of you are very familiar with this literature. And in that, dimensionality does not matter? I mean, you're thinking of really just the surface. Yeah, that's right. Surface. Yeah, yeah, that's true. But this is very general. I mean, this is any dimension. Where you expect this in general to happen. Okay, so the question is somehow can we make a setup linked to this vessel vesicles where we can make these homogenization effects that we know from the calculus evaluation. Can we make them appear in the setup? Yes? So I don't understand. You have two materials. You have two materials that have already been somehow mixed at a certain scale, or they are free to move, and somehow they mix and they form some pattern just because of variational reason. So, what is the situation? Because usually when you talk about homogenization, the structure is sort of prescribed, right? I mean, it's not unique domain or? Yeah, for instance. So, here's a domain, it's not fixed. No, I understand that, but let's say. I understand that, but let's say that so you expect to see your material which is alternating strong and weak, right, in a more or less finer scale. But that's you impose this? So you are assuming this is the structure? In principle, it could be all on one side. Yes, precisely. So this is one of the three variables in the optimization problem. And you say that what you expect is that you see mixing? From in general. So this is a very general reason. So that's a very general remark you expect to see next time. I think it's an optimal design. Yeah, it's a mix of design. It's not a marginalized guy. Sorry, I'm using wrong words. Sorry if I'm confusing people, that's absolutely right. But this is relaxation. Yeah, yeah. I think it's completely. Alright, so um So okay, what w how do I want to go about this to make these things reappear? So the engineers were looking at shell models where you have two terms in the energy, a membrane and bending term. This membrane term measures the difference of the induced metric on the vesicle with respect to the reference metric and the bending term the Wilmoid. With more energy. So, what I, as a first simplification, I'm going to do, I'm not going to do this for vesicles, I'm going to do this for graphs. Be able to use the tools that are available for sober MP tasks. Okay, that's the first thing. Now, some more modeling. So, in the next three slides, I'm still not going to do any rigorous mathematics, it's going to be a sort of a physical. It's got to be a sort of physics-style argument that leads to a certain simplified model that then I'm going to analyze very easily. Okay, so the models that the physicists were looking at, what might be a, that's what the engineers that make these numerical simulations, what would be a sensible model? You look at topological spheres, you You fix the surface of the sphere, and not only you fix the surface of the sphere, so measure of the surface, you also fix the measure of the hard component and the measure of the soft component. The volume is not fixed. Volume might not be fixed. In the end, for me, it's not going to matter anyhow, because I'm going to talk about graphs. In the moment, I'm just spitballing, how might you How might you model such a thing? And now, within this class, we associate a scalar elastic modulus to the soft phase, a scalar elastic modulus to the hard phase, one larger than the other. And what I'm looking at is the Wilmot integral. So Wilmot integral in the soft phase gets multiplied with one here, and in the hard phase it gets multiplied with alpha, alpha, large and okay. Okay, might do that. And I should remark, when I say Rule Moore integral, it's the sum of the squares of the principal curvatures. I'm subtracting here twice the gauss curvature, so maybe non-standard way of calling this simple. Alright, next step, still, we're still in the realm of somehow. Realm of somehow physics-style argument. To get to something that is more treatable, I want to consider the large contrast regime. So send the modulus of the hard component to plus infinity alpha. So what you expect then is a, what you could have then is a variation problem where you still have fixed amounts of flat and soft material on the surface, but the hard material now doesn't Doesn't tolerate any curvature at all. It wants to be flat. And you're allowed to arrange your flat area around the sphere as you like. But the standard is heuristic, but when you say that you fix the amount of soft material, you are saying the surface portion that is covered by the soft and the surface portion. So the total area of the surface. So the total area of the system. But again, it's not going to be any goal. As soon as I'm passing to the occupation. Can I answer a question too? Are you committed to this energy density or are you able to, for instance, study h squared only? That's already. You will see in a second why I'm doing this. In a second, why I'm doing this, when I'm considering the large contrast here, because it's going to lead to something that I can rigorously treat by knowing results in the literature. So this is already a question that I would like to answer later. Once I can do this, I can in a better way answer to your question. So what I'm going to present to you is very limited to this particular form. To this particular form. But what you were talking about, well, maybe one can come up with some. Because VESIOs are actually modeled using U squared, not U squared minus UK. Python declared so. Yeah, well, if you fix the topology. Well, okay, if you have two phases, then of course it's critical. But let's postpone this question to talk about it later when I've shown you what I've actually. When I've shown you what I've actually proved. Alright, so, okay, as I already said, I don't want to consider that because I want to consider graphs. To be able to use all these nice tools from a theory of solar functions and functions in PV. And then another major modification that I'm going to do, I'm going to go away from the hard constraint and I'm going to work with the branch multiply. That's what I'm going to pee. Multiplying it. So I'm going to penalize the area of the part of the graph. So S graph U is the second fundamental form of the graph of U. Where this is non-zero, it gets penalized by lambda. And here in the first part, you integrate just you can just integrate with this Wilmot integral, the other so the sum of the uh squares of the principal curvatures. So it's zero in the half part anyhow. It's zero in the hard part, anyhow, right? And it's in the soft part, it is what it is. So, this is the actual variation problem that I'm interested in. There's no boundary condition? There's no boundary condition. So, I mean, what I'm going to do is I'm going to do a comment analysis. And for that, boundary conditions to remember. Uh in FIMOM Oh guys, but he's giving a German to what I think for given for given boundary conditions of fixed lambda level. So there's some metaphysical contact. So the boundary problem. That's not what I'm looking at. I'm looking at the counterpart so far. But I don't think it doesn't sure yes. Well you you might wonder what would happen if you bound it on your problem or I don't think so, but we might discuss. I don't, I think we might discuss. So, what is forcing the I mean it's related to what is forcing the problem to do anything? This again, I mean for microstructures, you need boundary conditions, right? Yes, you might put boundary conditions. In microphones of boundary conditions. But I have to confess, I have not really thought about what would be a good way of specifying these things to get good variation problems. I'm really interested here in the relation of what happens in this limit to the asymptotic limit to infinity. Just say that your boundary conditions are compatible with the recovery sequence that you show. That's exactly what people are. You should be dying for doing what everybody else wants to do. In particular, you can also put boundary conditions at the brain. Then they will work in general something that takes you zero. I'm quite sure actually that you can set up all the conditions that give you nothing about something. Okay. All right. All right, let's go on here. Find more billions. Okay, so this is really the function that I want to talk about. This one here is the one that I want to talk about, and I want to look at it in the limit lampitude. Let's rewrite this a little bit. And okay, so there's some notation for the second fundamental form of graph U. You have the formula S of gradient U, second gradient of U. U, second gradient of u uploads the formula that gives you the second metaphor. Okay, success. Okay, and now what I introduce is the so-called cone-strain integrand, which is which is a parabola, but it has a discontinuity in zero. So it's a parabola, but in zero it's so in which it's it's a parabola that that starts at at lambda. That starts at lambda, but in zero, it has a discontinuity and descends down to zero. And with this notation, we can rewrite the function that we just had. So there's a renormalization that ignored that for the moment. But if you see the constraint integral of the second fundamental form is exactly what we had in the slug. And then we have, of course, the And then we have, of course, the variant. Alright, so this is now the functional that we want to look at. And I mean, as soon as you see this constraint integral appearing, then you think, well, myself have been working on these gamma limits with this kind of integral before, for second gradients, for divergence free stresses, and of course the organizers have worked on this as well. The organizers have worked on this as well. So, this was one of their tools in the derivation of the result on the optimal shape of light structures. Alright, now the limit functional. I'm going to tell you, so here you have this large expression of what the limit functional will be. So, how can you read it? What's going to play? What's going to play a sort of important role in this talk is this function rho circle of psi, which is the sum of the absolute values of the eigenvalues of the symmetric matrix psi. Sum of the absolute values of the eigenvalues. Okay. Then let's bh is my notation for the space of function. For the space of functions in W11, such that the gradient is in Vd. So you push this up, gradient showing Vd. And now the function, the limit function that we're going to get in the gamma convergence analysis, is given on the absolutely continuous path by this row circle, so the sum of the absolute values, of the eigenvalues, of the The eigenvalues of the second manifold times the integrand. And then for the cantra part and the jump part, you have some, yeah, you have to somehow say what it means. It's really an analogous expression for this expression here, for the counterpart and the jump part. But I mean I I don't want to go I don't even want to explain limitation here. It's really the the generalization to the singular singular part. Singular task of the second fundamental. Alright. Okay, so more or less I've already told you what I'm going to do. Here's the result. So it's a weak star convergence in dh, gamma convergence of f lambda to f. And the f lambda was dysfunctional with a renormal where you renormalize with lambda to the minus one half. With lambda to the minus one half. Maybe let's skip that. Sorry, integral zero more. That's the quasi-complexification, the symmetric quasi-complexification. It's also the quasi-complexification. It's not only the symmetric, the quasification. It's only the same. Yeah, in the end, it's the same. It's a limit as well, that takes to infinity or finite. Yeah, yeah, right. But for so, in the finite, for finite lambda, you see, so before taking You see, so before taking the limit, you see that diff quasi-convexification, quasi-convexification are basically the same. I'm going to comment on this later. Alright, maybe, but let's skip back to the definition of the f lambda. It is. So the at lambda is we have some renormalization, lambda to the minus one-half, constraint integral of the second part metaphor. And this converges, I'm telling you, in the center. Be telling you in the set of common convergence to this, twice the sum of the absolute values of the eigenvalues of the same quantum activity. Okay. So, alrighty, here the full gamma convergence statement. As usual, compactness, lower bound and upper bound. Compactness is really, and there's, okay, there's a technical assumption here in red that I don't want to escape. But I don't want to escape my gradients to plus infinity. I don't want to have flat bits to go up. They wouldn't have any energy at all. So technical assumption that the gradient has to be bound. And then under this assumption, compactness is for free more or less. So this is the compactness in BH, BH, the space of functions whose gradient is in dB. And you have a compactness. And you have a compactness result in the space, which immediately gives you compactness here in this. Alright, so let's talk about a little bit, before talking about the proof of lower bound and upper bound, let's give you a summary of this talk in one slide. So you consider graphs of functions, like omega omega, second fundamental form S of the graph Q. You define value functionals as. Value functions as second fundamental form squared of the graph u plus lambda times penalization of the area where the second fundamental form is zero. You limit the functional is a generalization of twice the sum of the absolute values of the eigenvalues of the same fine metal form. It's a suitable generalization to functions whose gradient is a dv. And the result is that you have weak star pH gamma. That you had with star pH number. From one to the other. Okay. Now, it is a relaxation of this from smooth function to the actual IGB. So the limit function should be low semi-continuous with limits. So yeah, I I absolutely agree, it should be low semi-continuous, but I think What I just mean is this formula that you have to treat I don't want to didn't want to write this down at all right for the capabilities and it's just so you have to think of in this way that this generalization is again maybe just Just a question concerning the junk part. So writing the singular part of row zero applied to the singular part of the VHN on the jump part, give you exactly the R cost. Yes, yes, you can. It's not, as you, I will say something about this later on. Usually you have just a cell phone. On, usually you have just a cell formula for the jumpout, and then here you can compute it. Yes, you can see the expression of the gradient of the jump set into the formula, and you use the same formula. Alright, so let's get started very slowly with the lower bounds. Okay, so it's a thing. We want to gamma conversion. Gamma conversion is related to relaxation, and relaxation is related to quasi-convexification. So that will be very convenient to quasi-convexify in the right sense for the lower bound. We've heard a lot about quasi-convex functions this morning. Here I'm going to, for us, since we talk about second gradients, what matters for us is two quasi-convex functions. Just in the definition. Functions. Just in the definition of quasi-convexity, you replace first gradients by second gradients, which is your definition of what it means to be a two-quarter convex function. And then you have relaxation, which is finding the lower semi-continuous envelope of a given functional, that amounts, that is, it is related to the quasi-convexification, of suitable quasi-convexification. Suitable quasi-controlled xification of the integral. I'll have to comment on this later on, but the con but I think this is more or less everyone here knows that there is a relation between the two of them, right? Lower semi-continuous envelopes or functions and quasi-convexification. And this is the right quasi-convexification to talk about here. Okay. Now. Now remember our constraint integrand, so this round with a jump down to zero at zero. For this, we have an explicit formula for the one quasi-convex envelope. And so this was proved by Colin Strang. I like quite a lot the proof that you can find in the book by Deco. Direct methods in the Kepler's variations. And actually, with this proof in Dachoronia, you can easily work and then show that this is not only the one positive convexification of F, but also the two quasi-convexification. This is also going to be on the next slide. All right, but we look at this, at this explicit formula. So you have this think of Think of is what's that convex envelope of this function here, which has a linear regime up to a certain value, which is here square, we don't matter. So this, the upper, the first line is the linear regime where the quasi-complexification is given by the sum of the absolute values of the eigenvalues minus. The eigenvalues minus something that gets small with lambda. So that is linear in sigma. Or linear growth in sigma, to be more precise. And then you have, below a certain threshold, you have quadratic growth. Okay, so you have this explicit formula, and we see that if you renormalize by square root of lambda, then you have pointwise convergence of the integral to Pointwise convergence of the integrant to this row circle. Okay? So you have pointwise convergence of the integrant, and we have to translate this pointwise convergence of the integrant to gamma convergence. That's the task that we have. Okay, so here, the integral is a constraint integral of the second fundamental form times the square root of the area integral. And this is we normalize. So the little f lambda is the integral that. Little f lambda is the integral that we actually want to look at. Okay, I already said the two-co-quasi-convexification of the construct integral is identical to the quasi-convexification. This is just, you just copy the argument that you find it by polynomial and you see that everything goes through. And then, little lemma, which is also not a lot of work. not a lot of work but even if when you when you plug in now the uh the other ingredients to your little f lambda the integral that you're interested in you have the you don't have the constraint integral of the second gradient but the constraint integral of the second final metaphor different object a priority and of course you also have the area integral but nevertheless you can again modify the argument that you have in the In Lagoonia, to prove that still, if you do quasi-convexify this expression with respect to this variable, you get this. So you get what you expect. So you get the two-quasi-convex envelope of the common strand functional, what we wanted, applied to the second fundamental form. And so that you replace the second gradient by a second fundamental form doesn't play a role. The metaphor doesn't play well in the end. Okay, and now we set up to prove the log on. The main tool is going to be the blob analysis by Irene and Schepperman. So what you do, you look at these integrants. So for the lower bound, you work directly with two quasi-convexified integrants little f lambda and you And you, well, for the lower bound, you assume that this is bounded, so subsequence converges to some quickly start, some measure, and this measure you decompose into an absolutely continuous part and a singular part. And then you want to show inequalities for the Harlan-Nicodim derivatives of this measure and compare them with a limit function. So, in particular, So, in particular, at the back regular points of the Slimen measure, the inequality that you want to show is this one here. Sorry, no, no, this is not the inequality, this is the defines your limit measure, but then it defines your model as your Radon-Nicode derivative with respect to L2 and so on. And then you want to show that this expression here is larger. Is larger than the limit integral at this point. Okay? So you do this in the regular points, you want to do the analog in the singular points, and if you do this for every point, then you have a little block. Okay, so as I said, the tool is the blow-up analysis that we know. And so basically, what you do is you go along the steps. Is you go along the steps of the analysis by Rien and Stefan Unler, at certain points you have to modify Galilee, of course. Because what is different here is, in comparison to their paper, is here that the integral varies as well. We have an integral that is not fixed, but I mean we have a pointwise convergence to the integrant that we control explicitly. So it's not that bad. You're able to control that. Another difference is that here, these integrals. that here these integrants they have two growth right so you have not really so you not really you don't really have V E control but yeah so you have the energy control you have to replace this and this again requires a slight modification of the market but the main idea is this to use the blowout that we know work sorry your question is is there novel is there monos Is there monotonicity on the lambda or is just a control of the point? We're sending lambda to infinity. Yeah, but I mean the F lambdas convexify, they sort of relate each other in a monotone way or is they convexify each other in a monotone way. So we just have to. There is an explicit formula. There is an explicit formula for the little f lambda. Yeah, right. Because this probably tells you that you sort of can fix the f lambda. You that you sort of can't fix the F lambda, that you don't worry about if it's small or if it doesn't mind. No, no, no, it's not as simple as that. It's not as simple as that. If you go back to the formula. Yeah, sorry, Heinrich, it goes in the wrong direction. It's just. So there's really some not to be anything trying to understand the monotone, yes, but it goes in the wrong direction. Square root of lambda go. I'm sorry. The square root of lambda go. The square root of cancels. Square root of lambda goes. This one. This one cancels, but with the normalization. And then you you're you're left with this one. Is there normalization for instance? Oh, because of the land animal. You have to look at this one. This one is yes, you have a monitor of cleaning, but it's increasing well. It's not what you want. Uh for the pattern at the I tweet or everywhere. So far, I've just done the seat for the six to feet. Okay, let's go to a job now. Okay, so here this was four or four of the back points of the limit led of the back regular points of the limit measure. So I didn't tell you everything, but more or less I told you the story. Now the jump out. Jump out. So, in the blow-up analysis that we know, what you get is as a lower bound for your energy. Is a self-problem? That's a huge guess, yeah. You're integrant in the PB analysis, but you have to go from A to B with your function. With your function, and you have to do it in an optimal way. And this optimal way defines you your integral, your relaxation. So, okay, so two things. Here we're not doing it for, we're not doing it B V, but in BH. So one has to do a little lemma that it's the same thing for second, that it doesn't play a huge role that you're talking about second gradients. That you get the same cell problem as you get for B. Okay. So let's. Okay. So let's admit that. And then the thing is that you can explicitly solve this cell problem, right? In the current setting. And why is this? So we used Romani's results on the properties, on the rank one property of the singular part. And then we observed that for rank one, we have a uniform bound of all the integrals. Bound of all the integrals. This is not true for every integral, but for rank 1 integrals, it's true that f lambda is bound below uniformly by the limit. So this tells you, with this observation, you can just throw away the dependence on lambda of your functional. You just remain with a little f, which was the sum of the absolute values, of the eigenvalues. It's rank one. Eigen value is rank one. So it's basically it's an infinity norm of the second fundamental form that you're dealing with. And what you do then is, well, you slice your cell problem in the direction of the job and you apply the fundamental theorem of calculus to the normal. Right? One-dimensional. It's a one-dimensional problem. So, what you get there is then the difference you have initially you have some normal to the right of your jump, some other normal to the left of your jump, and it's just the difference between those two normals that you get as besides. Okay. And yeah, okay, so this does it for the lower bound. I still have five minutes to tell you something about the upper bound. So you have to do two things. For the lower bound, I was working entirely already with the two-quarters of vexification of the function. This was okay, for the lower bound, that's no problem. This is good to work with the two-quarter-convexified integral because this is continuous. This is non-continuous. The non-quasi-convexified integral is non-continuous. Integral, it's non-continuous, so this is a bit ugly. In the upper bound, you have to deal with this. Okay. So you have to actually show this result here that, again, for continuous integrance is well known. For first-order gradients, everybody knows what it is. For second-order gradients, by Piralzzi Poggilioni, Poggiolini, excuse me. Tortureini, excuse me. I don't know that. Sorry. Already, many years back, that quasi-convexification really gives you what you want, right? That for a given function u, if you integrate the quasi-convex envelope of f of u, then there exists a function w that's close to u and w1p. With w1p, that achieves almost optimality up to some, up to some delta. So, for again, for the integrals that are continuous with respect to the decisive variable, that is well known. For non-continuity, you have to do something. And so what you have to do, what you can do, is a sort of, so you have your domain, you fill it up with very small cubes, such that the complement of these cubes is very small. On the small cubes, Small. On the small cubes, you approximate your function that you may assume to be smooth by second-degree polynomials. So on these cubes, you may then use the definition of quasi-convexity, two-quasi-convexity. The complement is controlled in your construction, in your so that you should say first, I glue together the cubes by some Whitney-type construction. And Whitney type constructions come again together with estimates. Get with estimates. And these estimates are good enough to tell you that the error that you make in the rest is small. And then gluing everything together, you get this type of result. Okay. It was maybe a bit. Let's jump back to that. No, no worries. So here, it's the arcours of the scalar product. It's the angle that is between the two. So if you integrate up the second fundamental form, that's what you get. It's a difference between the angles. Alright, back to the back. Okay, so I told you how to get back from this constraint, discontinuous constraint integrant to the two-quasi-convex spike integral. Now, how do you work with two-quasi-convexified integrant? Basically, by modification. And you want to approximate a given integrant u, you modify with the modifier will converge to your limit. And then again, you have that two-quasi-convexity. The two quasi-convexified integrand that this converges weak star to some limit measure. And you can show that this limit measure mu has to be absolutely continuous with respect to the total variation of the brain of u. And then it's actually convenient, in particular for the jump part, to again use a blow-up argument, which is maybe a bit artypical to do that also for the upper bound, but it's convenient to control. But it's convenient to control that really in the jump path the right things happen. And then you use what I said before to sew everything together. That's what you can do. We finished, okay. Thank you for your attention. I had some remarks, but maybe I can sneak this in. Alright, okay. Alright, okay, I'll I'll finish with some remarks. So we we started off, or I started off by saying, well, is it may find this be a good question to explain why you observe sharp phones in such problems. And I already said, well, probably not. One way that you might force sharp forms in the limit problem is of course if you set boundary conditions that correspond to the sharp form. Sharp fold, right? It's not very surprising. Yes, you are the minimizer, but I mean, that is really a bit artificial. But if you look at now, okay, so I have worked with graphs, but let's imagine that for more general surfaces like spheres, you you would get the same result. Would it be true that this could be a model that explains the uh the apperation of these sharp folds? The apparition of these sharp holes? And the answer is not really. There are results from start the beginning of the twentieth century that tell you if you look at this functional depth on topological spheres that are smooth enough, then the only minimizer is the round scale. So there are not that many shortfalls then. So I could why is this the total mean curvature? Because don't you have the sum of the absolute values of Values of convex bodies. You're saying that with convex bodies, then at any of the jumps, one of the curvatures is zero. No, no, no, no, no. Convex sufficiently smooth. But so what do you mean by total mean curvature? It's integral of the mean curvature. But that's not row zero, right? Because it's in convex surface. I have a simple question. I would like to work out. It is like existing tests. I would say so, but maybe we I think so, but maybe we I think so. So there is no