Um thank you Robert for uh not getting too upset with us. Um it's a great pleasure to have Robert McCann with us today. He's going to speak on the one-blood problem facing consumers with linear and non-linear conferences. Okay, so thank you very much. I'm very grateful to the organizers for the chance to speak and for organizing this conference in such a beautiful location. And my condolences to those of you that weren't able to attend in person. To those of you that weren't able to attend in person, I'm very excited. This is the first in-person talk I've given in more than two years, and it's also the first lecture I've given without a mask in more than, I guess, two years. So I'm going to talk about two parts of my talk. I'm going to talk about joint work in progress with Shang Jun Zhang, which started when he was a postdoc at ENS Paris. He's now at University of Waterloo. And there is a version of my slides on this website if you click on top one. On top one. And so I'm going to, in order to set the stage for the work in progress, I'm going to review some older work that we did a couple of three or four years ago, which has to do with the monophilous problem of nonlinear preferences. So let me set the stage a little bit. So the monophilous problem that I want to describe is an example of a problem from theoretical microeconomics in which you have asymmetric information. And so actually, this kind of asymmetric And so actually, this kind of asymmetric information problem pops up in many different contexts, but I like to motivate it by the following example. So imagine that you are wanting to manufacture something like automobiles, and you're wanting to market them to a population of consumers, and you know what the so what is an automobile? Okay, I don't know exactly, but it's maybe parametrized by its It's maybe parametrized by its fuel efficiency, its comfort, its performance, its reliability, the compatibility of passengers it can carry. So maybe an automobile could be like a point in a five-dimensional space. What is a consumer? Again, I don't know exactly, but maybe consumers are parametrized by things like their age and their income and their wealth and the size of family, distance of commute, risk aversion. So maybe a consumer is a point in the six-dimensional space. And now you've done a bunch of market research. And now you've done a bunch of market research that tells you what is the value of a vehicle type X to a consumer of type Y. And you also know what it costs you to manufacture a vehicle with, sorry, let's say a vehicle with parameters X to a consumer of parameters Y to a consumer of type X. And you've done some, you know what it costs to manufacture a vehicle with parameters Y, and you also know what is the relative frequency of a consumer type X. Frequency of a consumer type X as compared to X prime in the population, and then you want to decide which kind of vehicles you should manufacture and how you should price them in order to maximize your profits. And it's a monopoly problem, so you're only in competition with yourself, but the consumers always have the option to not buy a vehicle from you if they don't find any of your vehicle types and prices of fuel. And I'm going to review some examples in the history of this problem. Some examples in the history of this problem, which goes back 50 years in the economics literature, and then present some assumptions and results, and maybe sketch some proofs, although not so much. And then the new work that I want to talk about is a new duality principle, which certifies solutions to this problem. And then I want to go back and revisit an example, a well-known example of Roche and Chenet from about 25 years ago, which was really kind of the first multi-dimensional example that we've worked out in some detail. And they have a nice example on the square, although Example on the square, although the solution that they claim doesn't quite match up with numerics that appeared about five or six years ago by Nervo, and I want to explain how to correct the solution that they propose 25 years ago. And in order to do that, I'm going to introduce the new free boundary problem from partial differential equations. So that's the program for the talk. So here on my slides, I have kind of how do I formalize this mathematically? So I'm going to have sets X and Y into Euclidean spaces of And y into Euclidean spaces of possibly different dimension, and the x is going to be the space of consumer types, and the y is going to be the space of product types. And then the given data, and so z will be some range of prices that you can, you're free to price the products however you like as the monopolist. And g is the value of product type Y to consumer type X if they buy it at price Z. And potentially, G has a non-linear dependence on Z, although which could reflect the Although, which could reflect the value of money, money has different value to different consumers, or maybe their utility is concave and not linear in the amount of money they spend. But in the most classical examples, g separates into a function of x and y minus z. So if everyone has the same sensitivity of money, then it's just adding the z variable. And the relative frequency of biotype x as compared to x prime is going to be captured in a probability. To x prime is going to be captured in a probability measure Î¼ on the space capital X of the buyer types. And finally, the monopolist's profit pi will be a function of all three variables as well, X, Y, buyer type, product type, and price sold. And what the monopolist wants to do is choose a price menu, so assign prices to each vehicle type. And if they just don't want to manufacture a kind of vehicle, they'll assign price plus infinity to it. And so probably we shouldn't close brackets on the range of Z intervals. And so what is the monopoly? And so, what is the monopolist problem? Well, they want to maximize their profits, but in order to figure out what their profits are, they first need to think: if they fix a certain price venue, V, which products are the different consumer types going to consume? So this is kind of what's sometimes called a bi-level optimization problem. So first we're going to solve this problem that's lower down on the screen. So given a price menu V, each consumer type X is going to choose that vehicle Y, which maximizes their utility choice. Which maximizes their utility G as a function of their identity, the vehicle type, and the price they pay. And that's going to give you a map, a Y depending on X and V. And then what the monopolist wants to do is choose V so that their profits for each type averaged over the different types X as a function of this mapping Y of V and X will be a maximum. And the constraint on the price, apart from the fact that it'd be lower than continuous, is that there's a null option, which I'm going to represent as zero just for some. Option, which I'm going to represent as zero just for simplicity, which is that the consumer can always choose to buy the zero product at zero cost, and the monopolist is not allowed to charge for that. So the d at zero has to vanish. So this is this bi-level optimization problem where each consumer X computes a maximization to figure out which product they're going to choose. They'll choose the zero option if they don't like any of the products offered by the monopolist, and the monopolist tries to choose the V to maximize attack all possible. Yes. Why does pi depend on x? Why does pi depend on x? Well, I have some examples on the next page. And so, for example, in the insurance industry, if I sell you a health insurance policy, I care a lot about your health. And so that's a case where pi would depend on x. And now, I may or may not be allowed to adjust my prices depending on x because there may be regulations saying that I can't discriminate against pre-existing conditions, even if I know what you're conditioning. So there's two possibilities. One is that. There's two possibilities. One is that you know stuff about your health that I don't know, and that used to be a big problem. It's not so much of a problem anymore because Amazon and Google know just about everything there is to know about us. So, in the future, there will be no asymmetric information because the monopolists will know the identities of the consumers. But in the past, if you were a travel agency or an airline, you wanted to discriminate against. Okay, this is a little bit of a depressing theory because it's the theory that is supposed to explain why economy class has to be sufficiently uncomfortable. Has to be sufficiently uncomfortable that anyone who has the means to buy a business class ticket will be strongly incentivized to pay 10 times as much to get a business class, or whatever, to buy a business class ticket. And it's also a theory which says that what you pay to buy a product doesn't really depend much on the cost to manufacture that product. It only depends on what the market will bear, what other people are willing to take for that product. And in fact, there were examples of calculators, I think, going back in the 1980s or so. Sharp electronics manufactured. Sharp electronics manufactured different calculator models. So there was like a scientific calculator that had trig functions and exponentials and logarithms. And these were all accessed through a second function key. And then there was like a grade school calculator that had just addition, subtraction, multiplication, and division. And actually the calculators were identical except they had different face plates on them. Only the scientific calculator told you which keys would give you the trig functions and the lot of functions. But they were priced differently because Sharp wanted to make sure that they charged the people that needed the scientific. That they charged the people that needed the scientific functions more, but they didn't want to shut out the low end of the market, they just wanted volumes to checkbooks or copy data. And so, right, so in the airline ticket context, there was another policy, aside from the distinction between economy and business class, there was another policy that some of you may remember where you used to get a better deal on your airline ticket if you included a Saturday night stay in your trip. And the And the rationale for this was that the airlines wanted to discriminate between the pleasure travelers and the business travelers. And so the business traveler wanted to be home with their family on the weekend, and the company was paying, so they didn't care that much about the price of the ticket, whereas the pleasure traveler was willing to address the dreads of their travel in order to take on a rice. And in the early, around 1971, 72, there was this economics literature by people like Spence and Merlise that later got Nobel Prizes. And Merlise, that later got Nobel Prizes for this work, where they were trying to model optimal taxation in the case of Merlise and educational signaling in the case of Spence. So, this is another kind of depressing, for those of us that are academics, which is probably most of us in this room, it's a depressing theory from our point of view because in Spence's theory, the value of an education, so the education has no intrinsic value to the employer. What the employer cares about is whether or not the prospective employee is smart and willing to work hard. And so, the only value of how And so the only value of having, say, a PhD is it shows that you're smart and able to learn. What you actually learned, not so relevant. And so in that context, the employer might offer a contract depending on how advanced your degree is. They might offer you a higher salary. And your identity includes things like your ability to learn, your smartness, your willingness to work hard. The employer doesn't know that. All they see is what degree you buy. And so. And oh, yes, let me also mention that although I said this is a bit of a depressing theory from the if you're a utilitarian, there is a version that this, so the nice thing about mathematics is kind of universal, something I've always loved about mathematics. And so if you don't like, if you're not sympathetic to monopolists that want to maximize your profits, maybe you're a socialist. Maybe you're sympathetic to governments that want to maximize the quality of the service. That want to maximize the quality of the services that they're able to provide to their citizens, subject to the constraint of not losing money. And so it turns out that the same mathematics that describes the monopolist's profit maximization is more or less the same, is intimately linked to the mathematics that describes what the government wants to do if they want to maximize the services they provide subject to a budget constraint through the use of a Lagrange multiplier. So you would couple the, instead of maximizing profits, you'd couple it to the problems with a Lagrange multiplier. With a Lagrange multiplier. Okay. And so here's a little bit of history. So as I, up in red at the top there, as I described in much of the literature, g separates into a function of x and y minus the amount paid. And basically 50 years ago, Sven Merlies observed that, and earlier, the mathematician Lorentz observed that if B is supermodular, so in other words, if its mixed partial derivatives have a sign, then what you can deduce from that is the math, this is the case where. That is the map. This is the case where both the agent type and the product type are one-dimensional. So maybe products are parametrized by their quality, and agents are parametrized by their wealth. And so if you have this mixed sign condition on your benefit of product Y to agent X, what it tells you is that the quality of the product consumed is going to be a monotone function of the wealth of the consumer. And then once you know that this mapping Y from agents to products is monotone. Agents to products is monotone, non-decreasing in this case, you can turn the problem into an ODE with boundary conditions. So, about 25 years after that, Rocher and Chenet studied the problem where the space of products and consumers still had the same dimension, and they looked in particular at the case where the benefit of product Y to consumer X was the inner product of Y with X. So, this is analogous to the Brenier situation in the optimal transport world. And what they observed was that for this particular And what they observed was that for this particular choice of benefit, the product consumed would, the correspondence between the identity of the consumer and the product consumed would be given by a convex gradient. And they also observed this bunching phenomena that I have on my slide, which in this case, this is the square example that I mentioned in the introduction. So you take the space of consumers to be uniformly distributed over a square in the X space, and you take the And you take the bit of data missing from my slide. Oh, yes, here it is. You take the profit to the monopolist to be basically the amount of money they take in minus what it costs them to manufacture product Y, and you assume that what it costs them is quadratic in Y. And then what Spence and Relise observed was that the convex gradient map that gives the correspondence between agent identities and product consumed has three different regions. Has three different regions depending on the rank of the Hessian of the convex function. So in the top right-hand corner of the square, these are the agents who either are the wealthiest or care the most about the product of buying. The function will be uniformly convex, and the correspondence between agents and products will be, let's say, a homeomorphism. So, in other words, those consumers will buy customized vehicles. They care enough and are willing enough to pay to get an individualized vehicles. In the opposite Vehicles. In the opposite corner of the square, there's always going to be a positive fraction of people that are priced out of the market. So people that don't have enough wealth or don't need a car that much, if you haven't raised prices high enough that they aren't consuming cars, then you aren't optimizing your profits yet. But if those two regions are separated by a region of non-zero measure, along which, so the lines, the anti-diagonal lines I've drawn here are consumers who are all going to choose the same vehicle. We're all going to choose the same vehicle. So those consumers are forced to compromise between the two vehicle attributes, and they'll choose from some standard line of vehicles. And so the function is going to be uniformly convex across those lines, but it's going to be constant up here, or at least affine linear along those lines. And if I think about what this means in the space of products consumed, so if I take the image of the uniform measure under the gradient of this optimizing convex function, what I will see is this cloud of vehicles corresponding for custom vehicles, for top-end consumers. I'll see a Dirac map. Consumers, I'll see a Dirac mass at the origin corresponding to the consumers who are priced out of the market, and they'll be connected by a one-dimensional curve, which Spenson rather said was a straight line, which is this line of one-dimensional compromise vehicles that the intermediate consumers consume. And they called this phenomenon where all of the consumers along one of these lines consume the same vehicle, they called that bunching. And there were a lot of open questions about that, like, is this bunching an accident of the particular example I chose, or is it? The particular example I chose, or is it robust? And how much does it depend on if I change the model? All right, please stop me at any point if there are questions. After Rocher and Chonet, this was a problem that Guillaume Carlier's thesis was based on. And so Carlier had a nice work with Tomasham Robert, where they showed that the optimizing price corresponds. Price correspondence. So basically, the convex potential giving the correspondence between agents and cars is the Legendre transform of the optimal price venue. And they showed that this correspondence was locally on the interior homeomorphism. So in other words, they showed that the potential was C1 smooth. And in an unpublished work of Caffarelli and Lyons, they were able to improve the derivative to show that the corresponds to actually a Lipschitz map locally in the interior. And the other thing that Guillaume. And the other thing that Guillaume Carlier did in his thesis was he showed that if you look at other preference functions with still the price decoupled from the intrinsic value of car y to agent x, then even for general b, there was enough compactness in the problem to show that an optimizer existed. And that without loss of generality, you could take the price menu to be like a second b transform of itself. So a v convex. How about itself? So it'll be convex function. And my favorite PhD student, Xi Ving Chen, about 10 years later, showed that, extended this regularity result, the C1 result of Carlien-Lachon-Roder to other benefit functions B, assuming that the benefit B function B satisfied the modern or won conditions from the regularity theory of optimal markets. And here you see the definition of the B transform. It means that the B transform of V is a maximum. transform of V is a maximum, exactly a maximum over product types Y of the benefit of car X to agent Y minus the cost of type Y. And with Figali and Kim, we showed that for these other benefit functions, including ones considered by Carlier, if they satisfy the strengthening of this modstrene or long regularity condition, then the principal's optimization problem, which in general is just continuous and compact, acquires a convex structure. Acquires a convex structure, so it becomes maximized some concave function on a convex set of potentials, and that opens the door to doing numerics and to theory to showing giving conditions under which the solution will be unique and under which you can use gradient flow to compute it if you want, and showing some stability for the optimizer. So, in some sense, addressing the robustness question of Roche and Shonet. So, now about five years ago, independently, Zhang and Moldova. Zhang and Moldeka and Samuelson were able to show that if you generalize the situation to the case where the benefit depended in a non-linear way and identity dependent way on the agent type and the product consumed, sorry, yeah, the benefit and the price, the price, coupling the price of the agent type and the product consumed. You would still have existence. And so I can mention this related work on a related problem. So I guess Roger Meyers. So, I guess Roger Meyerson in the early 1980s thought about how to optimize, how to optimally auction off a good if you had a number of potential bidders in the auction and they had different probability measures for their valuation of the good, how you would design the mechanism design problem of maximizing the revenue to the auctioner or the seller. And this has been extended to the case where you are trying to offer. Are trying to auction off a collection of different goods and bundled them by Das Balakis, Teckelbom, and Samos. And they give a duality context in that, which is a little bit inspired about a duality that I want to describe later, sort of finding the optimality of solutions to our problem. And whoops. And so I guess the first bit of work, the older bit of work that I'm going to describe today is kind of the generalization of this convexity result that I had with Viali and Kim to this case where the evaluation. Where the valuation to agent X of product Y depends nonlinear on the price of Sam. And so let me describe these various hypotheses. I have a slide devoted to this. So first of all, we're going to take this benefit to be a continuously differentiable function of all of the variables. And we're always going to need to assume that the dimension of this, for technical reasons, we need to assume that the dimension of the space of consumers is at least as large as the dimension of the space of products consumed. Of promises consumed. And then for each pair of agent identities, I'm going to make the following hypotheses. First of all, for each x, the mapping of product and price combinations to utility and gradient of utility with respect to identity x should be a homeomorphism. The range of this mapping should be convex. And let me call the inverse of the homeomorphism y bar of g, of course it depends on the damping function g. From the dynamic function g. And then here's a kind of crucial definition for the first half of the talk, which is that I want to look at curves in this, for fixed consumer x, I want to look at curves in the space of products and prices, and I'm going to call such a curve a g-segment if its image under this nonlinear mapping here is an affine linear function. And this regularity hypothesis, the strengthening of the regularity hypothesis of Strengthening of the regularity hypothesis of Maud, SchrÃ¶dinger, and Wong is that for every one of these g-segments, if I now change the, if I no longer look at consumer at type x, but I look at different consumer type prime, x0, the utility as a function of t is convex along all such g segments. So this mapping is actually a linear function of t if x equals x naught, and I'm asking that if you perturb x to a nearby x naught, that this linear function becomes convex, and even to a far away x naught, so it's unbelievable. Even to a faraway epsilon, so it's under global condition. And then I need a couple more innocuous hypotheses. So I want that the utility of any product should go down as the price you pay for the product goes up. And I need to know that if you increase prices high enough, you can force all the buyers out of the market. And I need to know that the principal's profits are a continuous function of the products they sell and the price they sell them at in the agent. Sell and the price they sell on that, and the agents kind of the agents groups. Well, if there is a G segment, then the G segment, the existence of these G segments is a corollary of the convexity of the range of this map. So I have this nonlinear map. I'm assuming that its range is convex, and that means any two points in the range can be connected by a segment. And so that's only going to exist. Oh, okay, then you just write bars right there. Oh, okay, then you just re-maritize this to get the linearity of one, and then you get the... You automatically have the linearity under this, of the image, and you're interested in the pre-image about it. Other questions? Okay, so I guess the first thing that I need to review is how to describe how to. I originally posed the problem in terms of the price menu proposed by the monopolist, but I want to reframe. By the monopolist, but I want to reparametrize it in terms of the utilities provided to the agents for a given price making. And this was sort of the technique that Carlier used to show the existence of solutions in general. This is the compactification trick for those of you that read Bilani's first book about optimal transportation, the way that going to C transforms or P transforms in this case. So I like to use the variables B and C for cost and benefit, and P is minus C. So whenever you have a price menu V, you have Whenever you have a price menu V, you have an associated indirect utility U, which you get by looking at what product will agent X choose facing that price menu U. And I'm going to think of the monopolist profits as being a function of U rather than being a function of. And what I know by the envelope theorem from this maximization is at the point y where this maximum is attained, which is to say at y v of x, a first order condition should be satisfied. A first order condition should be satisfied by g. And it basically tells me that g and its derivatives at that point should coincide with u and its derivatives. And so that means that I can identify the product consumed and the price paid in terms of the identity x of the person consuming it and the indirect utility they drive as well as the variation in that utility with respect to perturbations of their identity. And in fact, the formula that identifies it is just the inverse. Formula that identifies it is just the inverse of the nonlinear homeomorphism from the previous slide. And so then what the profit that the principal is now wanting to maximize, which I called pi tilde when it was a function of price menu, I'm going to call it pi when it's a function of indirect utility u, is simply the average with respect to the distribution on agents of g composed with y bar. And so it depends nonlinear on u and its gradient through this nonlinearity y bar. Through this nonlinearity y bar and also the nonlinearity g. And I want to maximize this profit, but I can't maximize over all functions u. I only want to maximize over those u's which arise from some price menu, which is to say over the g-convex u's. And moreover, the g-convex u's that are bounded below by the utility that the agent would drive by choosing the null product. Okay, and this is, so this is a much more standard. I mean, in the Roche and I mean, in the Rocher and Chonet example, this would reduce to some quadratic functional gradient u minus a linear perturbation. So it would be much like a Dirichlet problem. I mean, the Dirichlet principle for solving Poisson's equation, let's say. Except that there's a funny constraint is that when you want to solve Poisson's equation and you use the Dirichlet approach to doing that, the energy approach, you minimize the Dirichlet energy subject to boundary conditions, you minimize overall. subject to boundary conditions, you minimize over all fun L all L2 functions, or maybe W, W12 functions, so all Soblet functions, U. Here you max you would maximize over all convex Soblet functions, U. So there's a convexity constraint. And the bunching that you see in Rochen and Chenet's example is a result of that convexity constraint blinding. Okay, and again, because the set of views that arise from some price menu is going to have a nice Arise from some price menu is going to have nice compactness properties. It's clear that it's pretty, and this lower bound on you is enough to tell you that the maximum of this profit is going to be attained. And moreover, you can read off something about the, as long as the distribution of agents is absolutely continuous with respect to the Bay measure, then the correspondence between agent identities and products consumed is going to be of the form like RFG composed with. Of the form Y R of G composed with units gradient. That's the analog of Ruchet Engineering's observation that the correspondence was convex gradient in the bilinear case. And okay, so here's a more interesting theorem, which is that if this hypothesis about the convexity of the utility B along B segments for other types of consumers is actually a necessary and sufficient condition given all the other hypotheses. Given all the other hypotheses, for the space of G-convex functions, which is the space I'm maximizing over, to be itself a convex set. And so that's somehow the key to further analysis from my point of view. And then once you know the space of things that you're maximizing over convex, you want to know whether the objection functional that you're trying to maximize is a concave functional. And it will be concave provided that the Provided that the principle's profits is a concave function of time along any G segment. And this time it's a G segment centered at the same consumer type, X. And of course, if you replace this concavity hypothesis by a convexity hypothesis, then the principles of maximization problem would be a convex problem, which is, of course, maximizing a convex problem is much harder than maximizing a concave problem, but you still know things about convex maximizations, like the maximum is always going to be attained on the boundary. The maximum results can be attained on the boundary, and in fact, it can be expanded point of the complex set. So in principle, that could be useful information. And if you want unique types of solutions, you need either strict or uniform concavity of the maximization. And you get this if you make an appropriate strict or uniform concavity assumption about the principal's profits pi along the G segments. And with such an additional assumption, With such an additional assumption, under the hypothesis that you're dealing with a concave problem, you can show that Principal's optimal strategy is going to be unique, at least in the space of, so the price menu won't be unique, but it's G-transformable for mu almost every agent. And it's stable in an appropriate sense. So if you approximate the data G, pi, and mu, with a sequence of triples in the C2 cross C0 across weak star topology, then you can conclude that the convergence of this. The convergence of the solution in computer quantum topology on that support of the limiting measure of agent types. So a nice thing to observe is that this convexity, so I have this convexity or canavity assumption G3, convexity assumption G3 along G segments. What does it boil down to in rotation on A's case? Well, the G segments are just normal affine segments for the bilinear cost. And so it's just normal. It's just a normal affine convexity, but this is an affine function. And so, in some sense, it's on the boundary. So, you can perturb this, whoops, you can perturb this bilinear thing to take it out of the space of things that satisfy G3 or into the space that satisfy G3. So, in that sense, the robustness of the bunching may be a little delicate because if you perturb it into the space of things for which still satisfy G3, then the solution is still unique and you can expect. Tree, then the solution is still unique and you can expect stability, but if you perturb it out of that space, then somehow all bets are not going to do. And here's an example of perturbations that respect or violate G3. So if I take the Rocher and Chenet potential and I perturb it by a product of a function of x and a function of y that are small enough in C1 norm, then if both of these functions are convex, then I respect G3, but if one's convex and the other one's not. G3, but if one's convex and the other one's not, then I violate G3. And I don't know. I mean, I guess I can say a few words about proofs, but so somehow this is the proof of the convexity of the space of utilities. So what does it mean to be G-convex? It means that for each agent type, Agent type, you have some potential U, and it's touched from below by a copy of G that you adjust the product and price until your potential U is touched from below by G for the corresponding product and price. And then you can read the first order conditions in X off of that. So equality should hold at X naught. And so if I have two G convex functions, u0 and u1, then I'm going to be able to find a two product price pair. Two product price pairs for which I have this inequality for all values of x, for all dot here. And I have equality produced here in both of them when I substitute my product x naught, or my Asian identity x naught rather. And in order to show the space of such potentials as convex, when I show the average of u0, u1, is again a supremum of different copies of g for different choices of price product pairs. And so Product pairs. And so I'd like to get an inequality like this for the average of u0 u1. And so you can imagine that you'd have to choose the y and the z depending on the y naught and z naught and the y line and z line some way in order to touch this average from below by an appropriate g. And so what way are you going to have to choose it? Why, well, okay, so it's natural to wanna add these two inequalities and get something like this. inequalities you get something like this and you'd like you'd like this average to be touched from low by g for some choice let's call it yz one half and with equality holding at x0 here and really the only way to do that is if you examine the first order conditions what you can see is that both the value of this function and its derivative at y1 half and z1 half need to be an affine linear combination of what they were at y0 z0 y1 z1. Z naught, y1, z1. And so this is exactly where this formula for g segments comes from. So the only hope to have this is by choosing y1 half and z1 half to be the midpoint of the g segment joining y0, z0 to y1 and z1 relative to the base point x0. And that's the choice that turns out to work, basically. So hypothesis G3 gives me this inequality here. The convexity along G6. the convex Q G along G segments for all choices of dots. And this is the hard direction, so the the converse direction is. I wanted to draw something but I don't have my iPad to draw on, so I have to wave my hands in the air. And the proof, once you have the convexity of the space of indirect utilities, it's very straightforward to get convex. It's very straightforward to get convexity, or concavity rather, of this principal's profit. And so basically, if you look at a utility, which is a convex combination of a utility u0 and u1, then what we're willing to assume is concavity of the profit derived from agent type x as a function of t when you compose it with this nonlinearity y-bar of g, which gave the inverse map to my linearizing map. And so all I have to do is integrate that against some non-negative measure, and I get a concave function. And conversely, if the first concatenity failed at some x, then by concentrating the agent types around that x, you could make the integrated inequality fail. But it's necessary and sufficient. And as in the case of the where the price decouples from the benefit, even in the case of the benefit, The benefit. Even in the case where it's coupled, you can understand these hypotheses, and in particular the convexity hypothesis G3, they have a differential geometric interpretation when things are smooth enough. And so let me describe that. So as long as the dimension of the two spaces is the same, the space of agent types and product types is the same, in some sense, the space of product types is higher when I add the price in as an extra variable. When I add the price in as an extra variable, and I need to increase the dimension of the space of consumers to match that. So now I'm going to have an extra parameter, which I called x naught. It's not the x naught of the previous slide, it's just a single dimensional parameter. And I'm going to call the extended, additional one-dimensional higher parameters y bar and z bar, sorry, x bar and y bar instead of x and y. I'm going to extend my direct utility function from the sort of 2n plus one-dimensional space to the 2n plus 2-dimensional space. 2n plus 2-dimensional space by just multiplying my di by this extra parameter x0. And I need to do this in order to have this differential geometric interpretation. I don't know, it reminds me of contact geometry a little bit, but maybe that's not fair. And then what I assume is that this extended thing is non-degenerate in the sense that the determinant of the matrix of mixed partial derivatives is invertible throughout the product region. Although, since x naught is entering in a kind of trivial way, I just needed this one value. kind of trivial way. I just needed this one value of x0, which I normalize to be minus 1. Now my function g was supposed to be a monotone function of the price, the third variable. So let me assume that when I invert that function to the third variable, the inverse function h also satisfies the invertibility and convexity hypotheses of capital G1 and G2. And then I get this characterization of G3, which says, Of G3, which says if G is smooth enough, hypothesis G3 is equivalent to the non-negativity of certain mixed fourth derivatives of G bar along all smooth enough curves for which one of the curves, let's say the curve X not Y bar T is a G segment with respect to X if S not a point, the S naught we're gonna have value that is inside here. And you can also understand this convexity of this fourth derivative as being a curvature condition. That's why I have one, I have a sort of trictorial slide to describe this. So here is my space, my augmented space of agent identities, my augmented space of product identities augmented by the price you pay. And so what are the hypotheses? So G2, so what I'm going to do, I look at the mixed partials of G bar. And of course, that might not be a symmetric matrix, but I can. Of course, that might not be a symmetric matrix, but I can symmetrize it by, I can couple it with its adjoint and put zeros on the diagonal to get a big 2n by 2n dimensional matrix. And I can treat that thing as a metric tensor by my non-genericity hypothesis G2. It's not positive definite, but it still has non-zero determinant. And so there's some geometry associated with it, which is like a pseudo-Riemannian or semi-Riemannian geometry. And in fact, in fact, because of the form of this matrix, this is a little bit like Lorentzian or. This is a little bit like Lorentz in our geometry, where you have both positive and negative eigen directions, but because of the special form of this matrix, you have the same number of positive and negative eigenvalues. And plus one time-like directions, and plus one space-like directions. What does the convexity of the image of the linearizing map mean? It means that if you fix, say, x naught and you look at the set of all product price pairs, that should be geodesically convex with respect to its metric tensor. X with respect to the symmetric tensor. And by symmetry, since I wanted to assume my H satisfies the same hypothesis as my G, if I fix a product price pair, the space of agent types across the dummy variables should be GS equally convex for this metric tensor. And what does the convexity hypothesis G3 mean? It means if you fix an X naught bar and a Y naught bar, and you look at perturbations of X naught bar and perturbations of Y naught bar, the sectional curvature of the two-dimensional subspace. Of the two-dimensional subspace tangent to a pair of perturbations is non-radiant. Okay. And somehow, initially, it was a surprise in the quasi-linear context, this kind of geometry was discovered by Kim and myself almost 15 years ago. And initially, we were kind of surprised to see semi- or pseudo-Romani geometry coming up in this economics problem, but with hindsight, you could say that it's sort of coming up for the same reason that it could come. You could say that it's sort of coming up for the same reason that it comes up in general relativity. So if you have a necessary, so general relativity, the basic hypothesis was absorber independence, or independence, invariance of the theory under diffeomorphisms. And here the situation is, you know, I may measure my agent types using one set of variables, Jan may measure them using another set of variables, somebody else may describe the product types in terms of different variables than I use. And if the problem is convex in my set of variables, or has a unique My set of variables, or has a unique solution in my set of variables, it ought to have a unique solution in their set of variables as well. And so there should be some invariance of the problem under how you parametrize it. And that once you assume that kind of invariance, then properties of the problem like uniqueness of optimizer ought to be described in terms of geometric quantities that don't depend on the parametrizable coordinates. Okay. So now we get to the second part of the talk, which is the work in progress. So I'm now going to specialize to Rocher and Chenet's choice of preference function. Chenet's choice of preference function, the bilinear guy, the quasi-linear price coupling. And then the profit optimization problem is linear in U except for its dependence on the cost to the monopolist manufacture product type, rad U. And the convexity constraint is that for any price menu, V, the resulting indirect utility, because it's given by Legend of transform as a convex function, and its gradient image underlying space of quadratic. And U has to be non-negative because any agent can always on there and you need price zero to get the zero product. And then our duality principle says that the profits maximization is given by this very peculiar thing that we struggle to interpret. So it's the minimum of a certain set of maps S, from the agent identities to, let's say, the product takes consumer art in general. General of the Legendre transform of the cost composed with S. And the constraints on the mappings are that this inequality has to hold for every u in the space of feasible utilities. So I try to interpret this on the next slide, only with partial success. So I've restated the problem here. So what I'm saying is the monopolist's maximum profit Monopolist's maximum profit coincides with the net value of a cooperative, which is able to offer its members X, the goods Y, at the price C that it costs the co-op to produce them. And you minimize this value over all possible distribution of co-op memberships, but then it satisfies, space S satisfies a very peculiar constraint, which basically we say that if the members whose true tribe Members whose true type is S of X pretend to have parameters X instead, then the expected gross value of the resulting assignment, after they pretend to have type X, should dominate the monopolist's expected gross revenue. And so I have a little proof sketch. So this is sort of the easy direction of duality. And it starts from the fact that C is a Lagrangian transform. So it's bigger than. It's bigger than the maximum of a bilinear guy minus S evaluated, I guess, grad U in this case. And on the other hand, S evaluated by grad U is dominated by this. And so you couple the defining property of Lejeune transforms with this inequality here to get this lower bound for this mechanization. And the opposite direction, the hard direction of duality goes through the usual Roche-Rockefeller interchange to not finding a saddle point. Not finding a saddle point. So if you introduce a Lagrange multiplier, T for the maximization that defines the Legendre transform C, or maybe the fact that C is self-dual under two Legendre transforms, then you get a problem which is linear in the variable U that you're maximizing over and convex in the variable T that you're minimizing. Convex in the variable t that you're minimizing over. And if so, t composed with grade u, grad u has some special structure, but if you relax that to it being a general map from the space of agent types to the space of products and consume, then you can imagine interchanging the infinite suit, and you get this duality, the whatever it's out. So, maybe even more. So, this duality for us is going to be a technical tool, and it's a technical tool which allows us to verify. And it's a technical tool which allows us to verify this translation of the Rochester problem into a free boundary problem that I'm now going to describe, at least on the square example. So I want to revisit this example here that I talked about earlier, and let me first mention that there's these numerics by Nirabo from five or six years ago, or seven by now. Oh, shoot. So I have it further along. Here we go. Okay, so I'll get to Nirabos and Nirics in a moment. So here's what I'm going to say about this problem. To say about this problem. So the problem decouples. So I described earlier how the space of consumers is going to decouple into three regions according to the rank of the gradient of the convexified, or of the indirect utility, so the convex function. It's going to be rank 2 up here, it's going to be rank 0 down here, and it's testing the map rank 1 function region. And so if I want to describe those regions. And so, if I want to describe those regions more analytically, I could use calculus variations. So, of course, the indirect utility is going to vanish on the space of agents that get excluded, that choose to buy the null product. In the budgeting region, you might expect it to satisfy, since it's a function of two variables, but it only depends on their sum, at least if Rochen-Chenet's picture is correct. And so you can try to reformulate it in terms of this function of k of the sum, and by using variational calculus, you discover if you. And by using variational calculus, you discover if you assume that the isochoice guys indeed aren't aligned with the antidiagonal, so that the strip convexities only in the diagonal direction are orthogonal to them, then you can sort of deduce this function k-ask at its form. And I've written it down here. It has a quadratic part, a linear part, a logarithm. And you have boundary regions, boundary conditions where it matches the zero function. And you have two boundary conditions. Both the value of the function and its gradient have to coincide at the lower boundary. To coincide at the lower boundary, and this allows you to sort of read off what the constants have to be. And finally, in the uniformly convex region where you have the Hessian of rank 2, the convex constraint is non-binding. And so then the problem reduces to Poisson's equation again. And because I assumed that the distribution of agents was uniform, it's Poisson's equation with a constant right-hand side. And again, at the boundary of this region, you should have two boundary conditions, namely, Conditions, namely, so by Caffarelli and Leone's result, the optimal u is going to be C11. So both its gradient and its value are going to coincide across the boundary. And what you notice is that's sort of too many conditions. So the normal derivative of the gradient being zero is a Neumann condition on this boundary. But you also need the normal direction to coincide. And that's sort of your problem is over-determined. Like a Dirichlet problem for Poisson. Like a Dirichlet problem for Poisson's equation, but with over-determining on the boundary. And when the tangential derivatives coincide, that sort of determines the value of the function. And when the normal derivatives coincide, the function side to coincide as well. And so here are Mirabeau's numerics. What he finds are that here are my three regions, and he finds that the intermediate, the bunching region, is not just an interval, it actually has some geometry associated to it. Geometry associated to it. And this is the space of products consumed, and yes, there's a diagonal component to that space of products consumed, but there's also one-dimensional spaces on the boundary of the set of customized products. And one-dimensional distributions on the boundary, which you see in red, coincide with these curved parts of omega-1. And so the analytic description that I'd like to give of this is that the region where the optimal potential The region where the optimal potential has rank 1 splits into two parts, omega 1, 0 and omega 1 plus. And things are reflection symmetric along the diagonal, so there'd be an omega 1 minus down here. And in omega 1, 0, the isochoice lines are parallel to the antidiagonal. But in omega 1 plus, they start to rotate. And then you have to find this free boundary separating omega 1 plus from omega 2. And the free boundary is going to be determined by those over-determined boundary conditions on your partial. Those over-determined boundary conditions on your partial differential equations provide the fact that you're trying to match not only the normal gradient, but also the tangential gradient, or the values of the function. So how do we do this analytically? So on omega 1, I call the part of E1 on omega 1 E1 plus. So U1 plus is given by a new system of ordering differential equations, which basically determine the The height and length of the isochoice segments together with the profile of this convex function along them. And the boundary conditions for this are going to be that on the lower boundary, u plus has to agree with k of x1 plus x2, and the gradient of u1 plus has to agree with the gradient of k, which is something parallel to the diagonal. And on at the boundary of separating omega1 plus from the 1 plus from the rank 2 region for the Hessian, so the Poisson problem, these overdetermined boundary conditions, so the Neumann and Dirichlet conditions, hopefully will only be satisfied by one choice of the length of these isochoice segments. So we parametrized the isochoice segments by their angle to the horizontal. So that's, I'll try to do this on the next slide. So omega 1 plus on each one of these segments, RS. Each one of these segments, we have an angle theta to the horizontal. We say how long is the segment as a function of theta, how high is its y-intercept, that's h of theta. And then using the calc, and we're eventually going to need to know the value of u along it, but using the calculus of variations, what we discover, here's our parametrization length. U plus is given, we parametrize along the segment, the variable along the segment is like a radius r, and so here's the y-intercept, and we go distance r along the segment from the segment. We go distance r along the segment from that y-intercept. What we find is that u plus has to be basically, so u has to be affine linear along the segment, so it's given by a slope m and a height b. And there's this little non-linear equation which couples m, the second derivative of m and the first derivative of m, to the length of the segment r as a function of the angle of theta. Of the segment r as a function of the angle theta. And so this equation is non-linear, but once you solve, so for the idea, so I've written the r's in red and the m's in blue, because the idea is for any choice r, that tells you, if you choose r of theta, you're telling me where the free boundary falls. And for any choice r of theta, you then solve this ODE for m to get the slope of the lines, or rather the slope of the function along those lines. And then you can determine d by just integration. So the Integration. So the y-intercept and the offset of the function u, once you know m and r, you can use quadratures to determine h. So basically, the whole thing boils down to this two, it's an ODE and two variables, m and r, and so it's not quite enough, what do you say? The ODE by itself is not quite enough to determine two variables, but if you fix R, the ODE is enough to determine N. If you fix R, the only is enough to determine M. And then what fixes R is the only problem. So sorry, so we, so let's see, we can't quite close our argument without making some ad hoc kind of assumptions about the smoothness of the problem, because we don't quite know. What we'd like to know is that there exists a unique R such that when you solve the PD. Such that when you solve the PDE and the ODE, both the function and the gradients match along the boundary. And we quite, we haven't shown that, but we showed that if such an R exists, then it's unique and it solves the problem. And why would we expect such a choice of R to exist? Well, if you were, we already know by Roche and Chenet's work that a unique solution to the problem exists and it's smooth enough. And so basically what we would need to know is that. Basically, what we would need to know is that if the sets where it's Hessian has a different rank are smooth enough to do this, and omega 1 just has these three components that I described, the omega 10, omega 1 plus, and its reflection omega 1 minus, then we'd be in good shape. But what could go wrong is that maybe omega 1 actually has like five or seven components and is not simply connected or something, but we don't know how to show that yet. Alright, so I think this is maybe a good place to. Oh, yeah, I have some summary slides. So, the summary for the first part of the talk is somehow that when you have convexity in a variational problem, or concatenation in this case, it's a powerful tool for optimization both from the numerical and the theoretical point of view. And the duality between the price menu that the monopolist describes and the buyer's indirect utility allows you to simplify the problem and show not only existence, but also give criteria for convexity in the problem. And that leads in principle to. And that leads in principle to necessary uh well, okay, so we gave some necessary and sufficient conditions for convex to the problem. Um now you might want to also write down necessary and sufficient conditions would characterize the solution to the problem when it's convex. And in some sense, that's what this new duality is doing, at least for the bilinear preference function with the quasi-linear price covenant. The necessary expression conditions for convexity of the problem are related to curvature conditions, which govern regularity in the theory of optimal mappings, or more generally, Optimal mappings, or more generally, when you do a non-linear coupling of the price to the benefit, you get into this more general class of equations, which Trudinger calls for Scribe-Jacobian equations. And that allows us to adopt the previous theory to payoffs, which depend on linear or on Z. We get this new duality, certifying solutions in the quasi-linear, bilinear case. And we finally show that this famous example of Roche and Chonet, whose solution was a little too simple as Too simple as Mirabose numerics showed, actually requires unexpectedly solving a free boundary problem where you're trying to match both the normal and the tangential derivatives of the solution of Westline's equation to the normal and tangential derivatives given by the solution of coefficients. All right, thank you very much. Thank you. Alright, so we have questions. And anybody from the online audience wants to ask questions, can you just so my first question was: if you consider So the my first question was, if you consider the variation of the last problem where you sort of push it into the complexity so you have the same as Ax, Bx, B C, do you still get just bunching or well okay. So we don't know that you get whether you I can't tell you presumably yes. Presumably yes, but I can't guarantee that. But what I can guarantee is that the solution to the perfect problem will converge to the solution of the original problem that's the perturbation was to. And the solution to the particular problem is all. Is there a way to understand probably maybe from the relative size of omega 1 and omega 1 plus? Is there like a boundary case where omega 1 disappears and you just have omega 1 plus? So if they parallel line, you know, they we don't have parallel lines for the same. We don't have RLS percent currently, but they're all right. So that's a good question. Let's see. We haven't really thought about that problem, but let me go back to the pictures. Let me go back to. So in some sense, the boundary conditions down here are inviting omega 1, 0 to exist. Yeah, so omega 0 should exist, but I'm just like... So in this picture, I'm thinking where the I'm thinking about the region where you have the region where it's zero, then you have the region where it's bunching with parallel lines, and then you have the region where actually the bunching is in all the lines which are all the same way. And I'm thinking about squeezing the middle, the one with parallel lines. It sort of seems to be quite a screen. So I don't know. So let me let me I So I don't know. So let me let me I I don't know the answer to your question. Um what I do know is that the the fact that the rank zero region has non-zero measure was discovered before Richard and Chonet by England Armstrong. And he made a big deal out of the fact that this is not true in Is not true in one dimension. So, in one dimension, it can happen that omega 0 classes to a point. But in higher dimensions, using something with a little bit of an isoparametric flavor, it's an integration by parts argument, that the fact that the boundary is scaling differently from the volume allows you to show that it has, if you have it, price. So, I guess Armstrong called this the desirability of exclusion. And he observed that when you go from one dimension to higher dimensions in this problem, The higher dimensions in this problem, you necessarily get this positive fraction of agents that are priced out of the market. Now, how you could show, I mean of course it would be great to know that to have some kind of result like that saying that the rank ones region has to have non-zero measure, but nobody knows anything like that. Another open problem that nobody knows much about is the problem of replacing the square by the q. So you can expect, I mean, you might guess that there's going to be a rank 0 region or rank 1, region, rank 2, region, and rank 2 region. You'd like to know that they all have positive measure. But apart from, you know, we did, so Flavian Leger did some numerics for that case for us. But it's quite complicated regionalized. So Guillaume had a question. Guillaume, do you just turn on your mic and speak? I think you Robert when I spoke to... By the way, I was trying to see a conjecture about the 3D gaze or i dimension cases which you should yeah well I would conjecture that all three all the regions have non-zero measure but that's I would say it's more of a guess than a conjecture. I mean I don't have any real evidence for it. Any other questions? Let's back over again. So I can unplug the computer now.