Thanks, Alex. Yep, so this talk will be based on two papers and the second part of the talk will be based on joint work with these three guys from Vienna. So let's just start with the motivation directly. With the motivation directly. So, what I want to do here, I want to forget about the math finance motivation and go over to statistics and say, okay, let's just consider a measure mu here, which is on a product space. And for simplicity, just think about the unit cube, so d-dimensional. And we're somehow interested in trying to understand the dependent structure between the canonical projections. The canonical projections, which I call x1 and x2 in this talk, for reasons which I explain later. So, both of them are d-dimensional. And I want to in the end assign a number to this dependent structure. And part of this talk will, of course, try to explain what's a good number and how I arrive at this number. But for now, just intuitively write there two extreme cases which we can think of. Which we can think of, and the first one is: well, you give me x1, I tell you what x2 is deterministically, right? So there is really no randomness. And the second one is, well, knowing x1 tells me nothing about x2. Both of them are independent. And we're working on optimal transport, so I will rewrite this as couplings. The second one is just a product coupling, the first one is some kind of push forward here. Is some kind of push forward here of the first marginal of mu, which I denote by mu1. Alright, so so far so good, I hope. If there are questions, right, just interrupt me at any point. So why is this interesting? Well, I first got interested in this problem when somebody at Columbia gave a talk on this paper. So Surov Chatterjee in 2020 asked a similar question, really coming from statistics. Really coming from statistics. So, somebody gives you samples x1 up to xn and he asks, okay, can we find a statistic now which fulfills these three properties which are listed here? So somehow it should be simple in whatever sense we specify later. Then it should measure the degree of dependence between the two random variables x1 and x2. In particular, it should fulfill In particular, it should fulfill this property, right? It should be zero if and only the variables are independent and one if and only if they are measurable functions of each other. And then lastly, they should have a nice asymptotic theory, at least in the benchmark case of independence. So when I'm on this product measure space. Okay, so if we go back to the literature, so just to conclude. To the literature, so just to convince you that this is an interesting problem, many people have worked on recently. We can see in the case where d is equal to one, so if we just have random variables, there has been a lot of work. Actually, Chatterjee was not the first one to come up with the generalized correlation coefficient in this setting. But since he's worked on this, actually, many people get interested and try to understand further properties of what's by now known as. Properties of what's by now known as Shattergy's correlation coefficient. But then there was still an open question: right, how do we generalize this to dimension d greater or equal to 1 or even to general Polish spaces? Okay, and for d greater equal to 1, really my colleagues at Columbia did great work. And this is so they answered part of these questions, they worked on RKHS. On RKHS, and the measure still had to fulfill certain properties. So, I was asking: can actually optimal transport give us a more direct answer for this problem? And also, there, I'm not the first one thinking into this direction. This goes actually back to Genie in 1915, who was already there in a different language thinking about similar problems, and again, recently. And again, recently many people have picked up this work. And I really like, for example, this last work by Munk, who is generalizing some of the ideas I want to present to you here now. So, okay, optimal transport, enormous light which everybody knows, just to fix notation, right? So, in particular, I want to focus, at least in the beginning, on the one Basserstein distance. Basserstein distance, so no B will appear in the beginning. And as life is short, right, I'm just gonna give you the answer directly. So, what's the kind of correlation coefficient I wanted to look at? And this is the following one. So, four reasons I will explain you in a second. I call it T right arrow of the measure μ. And what we do, if we look at the top part here, is Part here is hopefully clear, right? We take an integral over the first marginal, so over mu1, and then for each x1, we just look at the disintegration, so the conditional distribution of x2 given x1, and compare this to the second margin. And then we normalize by something which actually just depends on the second marginal, and this is well defined. And this is well defined if this is not a Dirac measure, but that's what we're going to assume from now on. So now why is this a good correlation coefficient? Well, because it, so I'm going to go through the program of Chatterjee now and convince you that actually all of the three properties are listed there are satisfied. And the first one is actually, well, it satisfies this second point in Chatterjee's program, right? So it's bounded. Program, right? So it's bounded in between 0 and 1. It's 0 if and only if μ is the product coupling, and it's 1 if and only if I can write μ2 as a push forward of the first margin. And that's exactly why I write the arrow from the left to the right here, and that's also why I call it the random variables x1 and x2. So this correlation coefficient is not symmetric. I'm only asking here in the boundary case. I'm only asking here in the boundary case if I can write x2 as a function of x1 and not the other way around. So both of them, this is actually a final property as having both of them fulfilled at the same time, but of course I could symmetrize this in some way to actually get both results. So let me continue like this, maybe. So so is it clear that there is some model to Nicity here important. So if you have a a measure which has a Yeah, maybe let me I give you some properties and also give you some examples. Who really likes this slide? Yeah, I actually wanted to show you some photos or some simulations then I wanted to. The relations then we want to do. I mean it's it's just uh do you have PDF do you have no it's just uh well I'll try to when does the lecture start oh it's been uh I don't know, this is just not reacting to anything, so Can you tell how you're using that? Uh uh MFP A MP is a mm-hmm. This computer destroys I don't know. I'm gonna use my laptop because I have the zoomly thing on it and Because I have to zoom in on it and it might just crash again and terrify. Uh have another power supply that also running basketballs. No, no, it has power, it's not the power, it's just uh Yeah, but I could put published a block, but it's a strange publisher, but it's not possible to print. I'm already sending this to the point where we were doing it too many times. Let's see what is that choice of working. Well, it's working now, but let's be when we Maybe yeah, but I don't sorry. There's no animation in this thing. Alright, short break, let's continue. So, as it's really simple, I want to As it's really simple, I want to say just a few words on the proof. So, again, still motivating why this is the right quantity to look at, at least from optimal transport point of view. So, what we can see is if mu is the product coupling, then the disintegration is just the second marginal, right? And we are comparing here the second marginal with the second marginal. Zero, no problem, right? And this is actually an if and only. And this is actually an if and only if statement. So one property, namely, oh god, oh, the independence is immediate. What's a bit more tricky is actually the rest. So let's go through this slowly. The Basserstein distance, where we all know it's an infimum over couplings, right? So in particular, we can In particular, we can bound this from above by just plugging in any coupling we like. And what we do here is we just plug in the product coupling. So then we get this expression on the right here. And so if we then integrate with respect to the first marginal, this only comes in here, right? So we see that we actually get exactly the term on the bottom. So we get something which is bounded in between 0 and 1. No problem. One, no problem, everything decouples nicely here. And then we can think lastly about equality. Well, when is this actually an equality? And the answer is, well, Wasserstein distance product coupling, this usually doesn't work well together, right? The only way I can actually get an equality here is if there's only one coupling I can choose, which is the Dirac coupling. And again, the Dirac measure. And this is again. And this is again an if and only if statement, right? So, in particular, we get that our Wasserstein correlation is equal to 1 if and only if I can then find this function f here, which is just given by this integration space. So, measurability is also local. Right, so a few properties which come more or less from the definition of this whole thing directly. So, if we think thing directly. So if we fix the marginals then actually this Wasserstein correlation is convex. If we disturb, so if we change the space a little bit, right, so we apply some isometric isomorphisms to both x1 and x2, right, then nothing happens too, which is also just a property of the Wasserstein distance. And as I said before, right, this thing is not symmetric, so looking at x1 and x2 and switching. at X1 and X2 and switching actually gives you different results. They're easy examples which you could co-op to use. Well, okay, so those are the basic properties. Now if we want to think a bit more benchmarking this thing, we can look at some standard distributions. And the first one is actually the normal distribution. And okay, I have to cheat a little bit here, right? So for normal distributions, Bit here, right? So for normal distributions, Wasserstein 1 metric is actually not so compatible, right? So we take a Wasserstein 2 metric, and what we have to do then is, okay, we put a square root outside and a square inside to make this compatible. And then now we just take here bivariate normal distribution as our mu, which has a correlation coefficient rho. Right, and for Wasserstein, two major For Wasserstein 2 metric, we all know how to compute Wasserstein distances between normals there. So everything can be computed explicitly, and what you get out in the end is something which just depends on rho, but it's not just rho or absolute value of rho, but it's this slightly convoluted term, right? So let's just look at this in a plot. So I'm pretty sure that's not where Jan's computer collapsed. Jan's computer collapsed. So, what we see here is just rho on the x-axis, right? And on the y-axis, I just plot different correlations now. So, Pearson correlation, just the normal rho, is the blue line. And now Wasserstein correlation is given in red here. So we see it's convex in rho, and it's somehow a middle case between distance correlation and Henninger correlation, which are two other correlations which you can Correlations which you can come up with. And before I go on, maybe just to give you an overview, I don't want to go into too much details here, but these are actually standard correlations people have used in the past. So this is actually how I motivated the talk, right? This is strategy's correlation. And we see immediately why it only works for d equal to 1. Well, we compare here y greater or equal to y. It's not so clear. Equal to y, it's not so clear what we do there if we have now a multi-dimensional version of this. Hellinger correlation is really just the distance, the Hellinger distance between the coupling and the product coupling. And distance correlation here can be defined, for example, by looking at distance covariance, right, and then normalize as you usually do. Okay, so now some empirical results for normal distribution. Results for a normal distribution, we could compute everything explicitly, right? But for other distributions, that's not so easy. So, in particular, if we want to stay on the unit interval, we could just take a uniform distribution on this and now say, okay, we want to somehow correlate x1 and x2, right? And we can do this how we usually do in Marfinance, right? We just take two independent copies and take rho times the first one plus. Take rho times the first one plus square root of one minus rho squared of the second one, and then apply different functions to this combination of random variables to mimic then different dependence structures. So in the limit, if rho is then equal to 1, we see this functional relationship. Then x2 is just f of x1. And if we do this now for just the identity, then For just the identity, then actually all of our correlation coefficients we know should actually have similar behavior. And now, here in the picture, what we see is actually again rho on the x-axis, on the y-axis we have strategy correlation in pink here, distance correlation, Pearson and Spearman all behave similarly and then Wasterstein correlation is in the middle. So for these all had similar behavior. All had similar behavior, but what's maybe more interesting is actually what happens if we now have a non-linear relationship. So, if we take, for example, now the absolute value of x minus the midpoint of the interval, then we see Pearson and Spearman correlation don't pick up any functional relationship at all. Distance correlation picks up a little bit, right, but essentially also just captures linear relationships between x1 and x2. And X2, and the Chattergy and Wasseste correlation still go up to 1 in the limit. That's numerical error. So that's me being poor at simulating. This should actually be zero. Yeah. What happens if I just simply use it? Yeah, that's mutual information measure. Yeah, so also measuring because if it's total dependence, then we suppose they are density and so on. If it's total dependent, then you get infinity. And so almost a single one. Yeah, so infinity is maybe not so nice, right? So you know, some So no, somehow he was saying you can do d over 1 plus t to transport it. Exactly, pick 1 minus e to the power minus whatever images. Yeah, but I mean this so you could I mean be pretty far away from from infinity dimensions because I mean entropy there are lots of infinities you can get there. Of infinities, you can get them. This will forget some of those. No, I mean, there are estimates for entropy. There's this correlation rho, for example. You can recover it by taking the ratio of the two densities and take a law that will give you the correlation rho. Yeah, but I mean here, I mean, so you so you would take the So so you would take the the product yeah take the bibliography normal density divide by the product of the GUD variant standard But everything which is singular would then get infinitely completely independent like a maudi support of two points Points so you don't get a deterministic function. Sorry, it's just um in terms of entropy regularization, right? You just also does the same thing between mosh maps and uh product things, it's so. Yeah, but I think it's not an if and only if statement, right? So the the point is really we want it to be one if and only if we have a model. But I I'm not saying uh But I'm not saying that there aren't other concepts. So, as I've told you in the beginning, right, there is a rich lit literature on this. And I've just named a few of these concepts. Okay, so let's maybe look a little bit at estimation. So, in these plots, of course, what I've done is I've actually drawn some samples and estimated coming from these samples, and this actually connects the Samples, and this actually connects this Wasserstein correlation now to the adapted Wasserstein distance, which we already heard a lot about this week. And in particular, I want to point again to the picture also Gutmund has shown you in his talk. So if we think about conditional distributions here, the left and the right picture are really very different, even though in normal Basterstein distance they're quite close together. Together. So, what I want to emphasize here is that actually, even though this is nice in Wasserstein distance, our functional, so this Wasserstein correlation will in general not be continuous in the normal Wasserstein distance because it has this disintegration inside. So, actually, to get something consistent, to get something from the data, we have to use an estimator which is To use an estimator which is actually consistent in this adapted Wasserstein metric. And that's actually the joint work with the guys from Vienna that we came up with a general way to find empirical measures which are consistent in this adapted Basserstat. So let's quickly review this. So say we get some samples here, again, just x1, x2, so a pair. So, a pair, and we want to build an empirical measure. The idea is quite similar to the normal empirical measure. We just have to do one additional step. And this additional step is here, right? So, instead of looking at just the Dirac at x1 and x2, we now apply a binning function to it first. So, this function has finite range and in the limit converges to just the identity. Just the identity, but whenever we're in the neighborhood, we just choo choose the midpoint of this neighborhood and map all the mass to this point. So to make this a little bit clearer, I want to give you an example here. So think about this spinning function now in one dimension, just as I said, mapping to the midpoint of this interval. So in zero one we just map to one half, in one two we just map to one point five. We just mapped to 1.5. And now we take some samples, and we see here the empirical measure looks wild. In particular, if we have some absolutely continuous marginal here, we never see the same point twice almost surely. So all these disintegrations of the empirical measure will actually be Dirac. So that's not great if we know that Wasserstein, adapted Wasserstein, somehow measures. Adapted Wasserstein somehow measures the distances between disintegrations, and in the limit I have something which is not the rug, but I won't capture it by just looking at this. So what we do is super simple, right? So now we just spin these points. So all these points here are mapped to 0.5, all these points are mapped to one point five and so on. So in the end you get like a tidied up picture, and now you actually you see that you have nontrivial disintegration. Have non-trivial disintegration. So here you have, for example, probability 2 over 5 that you go straight and 3 over 5 that you go down. So that works and actually you can show using tools which are by now well understood for the classical Wasserstein distance that this new empirical measure or adapted empirical measure as we call it is actually consistent in this adapted Wasserstein distance. Actually, consistent in this adapted Wasserstein distance. And that then means if we plug it into our Wasserstein correlation, this will give you a consistent plug-in estimator for the true Wasserstein correlation which we want to estimate. And it's even a bit better, right? So you even get convergence rates as this Wasserstein correlation, or at least the top part of the fraction, is Part of the fraction is Lipschitz continuous in adapted Wasserstein distance. As soon as our kernels of μ are nice, so we have some regularity on the kernels, then we can actually also understand the convergence speed here. And we see the convergence rate, at least for d greater than equal to 3, are optimal in the sense that these are the bounds which mimic the bounds for the standard Wasserstein distance between the Distance between the true and the implicit dimension. So recall that the implicit dimension here is 2d, not d, because we have two time steps. Okay, so one last point. So the third point of Chatterjee's program was to understand the independence case. And here there's still work to be done. But what I can tell you for sure is this: right? So in the Is this, right? So in the dependent case, we actually want our Wasserstein correlation to be equal to zero. What I can show is that it's bounded by this fraction. So this is the number of bins we have divided by square root of n. So if our number of bins doesn't grow too quickly in n, if we have this relationship and it also doesn't grow too slowly, so if we compare it to screen. So, if we compare it to the square root of log n, then it actually goes to infinity. Then we get a test here coming out of this where the c just depends on the second marginal, which at least if I send n to infinity gives you the right power function. So we never make an error asymptotically, but unfortunately this doesn't tell you yet much about the power function for finite and or even for asymptotic results. Results. This I think is not well understood yet. And the main problem here is actually that I would like to get a CLT for my estimator or for some estimator which is consistent in this adapted Wasserstein sense and converges to the true Wasserstein correlation. And if we just do a naive implementation here, then we see actually we come. Implementation here, then we see actually we compare it to standard normal. This has no good coverage at all, right? So it's much more spiked, it's not even symmetric. So it's at least I conjecture at the moment. Either you have to do some transformation or this it will actually not be a standard CLT. So there is for sure some work to be done. And then the second problem with this approach is of course if we think about the definition. We think about the definition, right? Wasserstein distances in the high dimensions are actually computationally heavy. And now we actually have something more difficult, right? We have an integral over Wasserstein distances. So we have an additional layer of complexity coming in here. So it would be maybe natural to ask, as Chatterjee actually did in his paper, is there an easier-to-compute estimator which is still consistent in the limit? There is no reason why. In the limit, right? There is no reason why there should only be the plug-in estimator which does the job. All right, so thanks for the attention. And I don't have a nice QR code, I assume it, assuming, but maybe we can still check out my website to find more information. Okay, thank you, Johannes. Very interesting talk. Tarija. It's great just to talk. Do we have questions? We've had a few questions already. But any questions online? The question from Tom Wade, do you want to unmute yourself and ask it? Maybe I can. Maybe I'll read it out. So, question from Donaway, could we define an inner product? Could we define an inner product from Basel-Stein correlation since the usual correlation is reduced by the inner product in Rn? Not uh super obvious to me right now. Obvious to me right now. So, as I said, right, this will usually not be symmetric, so the first thing you would need to do is maybe symmetrize it in some way. But yeah, I can just say I haven't thought in this direction yet. I mean, I was actually going to ask you about this. I mean, is there any reason you don't symmetric? I mean, I guess in the example you had, right, there's clear directionality because. The example you had, right, is clear directionality using conditional distributions. And I guess often this might be some sort of regression type problem where you've got a predictor and a response variable. But in other cases, I guess it would be natural to think of this as being symmetric, right? I mean, there's obvious ways of doing this, right? Just taking averages or whatever. Yeah, exactly. Is there any reason you haven't presented that or? So I thought this would be a more sharp result, but I mean, once you have But I mean, uh once you have this, as you say, right, you can there's nothing there's nothing that goes weird if you try and do this or no I don't think so. So I mean if at least if the spaces are the same, right? So just have I don't know t-dimensional cubes or whatever, there's nothing which goes wrong. Okay. Okay. Any other yeah? That's so you seem to suggest that the rate of the estimation will only depend on the number of pins. The number of pins. What about the shape, or do you just assume that they're uniform? So these are upper bounds, right? Yeah. So the upper bounds are usually attained by some uniforms. Yeah, I mean, sure. I mean, that's what I meant, right? If you get an actual CAT or whatever, then I think I I hope you can actually do some uh scaling here, so some transformation before you you actually compute the buttons. So if you go back, do you go back to one of the very first slides where you list these three properties? So, this one about being one if and only if one is a measurable function of the other. Do you need, so if you have a function that is sort of two to one, uh, two to one or one to two or something, right, does the directionality cause problems in one direction? If I have if I if I mean I can have situations where I can map either the same thing. Either the same thing to different points, but coming back is one-to-one or. Yeah, so I mean, this is like causal and by causal optimism. Yeah, that's all right. But because you have this one-directional definition, does that cause a problem if that happens in one direction? I think you could pick it up. I think you could also, because he's measuring conditional from one to the other, so if you go this way, it could pick up a conditional measure. Positive. So if you're having two points going here, then that would give you that, okay, this is dependent. But if it's this free, then you'll see that it's not dependent. So that's not, so the definition here, right, is. So the program is saying it's not not entirely clear what what what's meant by true I mean you can just max up the two directions or middle. Max of the two directions or mid of the two directions and the interpretation? So, in some sense, this is better because they can at this point can actually pick up the causality, which should be the direction. Which is actually better than what the programme is asking. I think if you want to actually run it, you probably have to do both of them, as you say, symmetrical. Yeah, but really, I mean you just turn it s switch everything, right? There's nothing uh problematic here. Okay, good. Unless there's any other final questions, I suggest we thank Johannes and Iva Geh. And that's actually also in chatter D'Correlation, it's just one one way or another. It's clear from the definition of that.