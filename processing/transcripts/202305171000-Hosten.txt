Yeah, today's activities. So, welcome to the third day where we're going to be working hard. And then tomorrow we'll have the relaxed time and the tour. So, yeah, so also hi to the online participants that are joining us. And yeah, so we have Sir Kan today, and he's going to tell us. Today, and he's gonna tell us about maximizing the KL divergence to Toric models. So, looking forward to hearing about it. Yeah, I'll do I'll do here for maybe a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit of a little bit. Maybe okay, I'll do this. Sorry, I want this. Yeah. Sorry. Thank you. Thanks, Jose. Okay. Thank you very much. Thank you, Berent and Carlos, and also Elena. Hi, if you're there for organizing this workshop. Yeah, so it's a fantastic place, of course, and it's great to work hard in here to see people. Okay, so I'm going to talk about maximizing the Kolbeck-Leibber divergence to Toric models. And I should right away say that, whoops, this is joint work with Julia Alexander. Let's see, is Guido here? No, not yet. Okay. No, not yet. Okay. Okay. So is it really? Oh, shoot. Okay. Okay. So, okay. I was going to thank him because a year ago, almost a year ago, we were in Hawaii and Yuli and I, we sort of gave talks, back-to-back talks on log vulnerable cells for linear models as well as for. For linear models, as well as for Gaussian models. And then Guido sort of talked to us, and he even sent a bunch of emails, including some sort of directions and problems. And so we owe this project to Guido. Okay, so let's jump in. So I'm going to be working with algebraic statistical models in the probability simplex. So delta n minus one is the n. So, delta n minus 1 is the n minus 1 dimensional probability simplex. If I give you two points in the simplex, p and q, we can define the Kolbeck-Leibwell divergence with that formula. It's given by the sum of pi times logarithm of pi divided by qi. You can quickly check by taking derivatives that if you fix q, uh if you fix q yeah this function is a convex function strictly convex function in p okay or the or the simplex so it's a it's an you know uh this function is is an important function for for maximum likelihood estimation um so as i said we're going to talk about uh we will work with algebraic statistical models so i when i say a model what i mean by that is uh the simplex intersected with some uh variety okay that we like Variety that we like. And the fundamental problem, of course, here that most of us study here is that given a point in the simplex or think about as a data, we want to find the maximum likelihood estimator, Q star, on our model, such that the Culver-Lactor divergence from P to Q, where Q is overall M, overall model, is minimal. Overall model is minimized. Okay, so the solution, the optimal solution to this optimization problem is the MLE. Minimize. Correct. We're getting there. Yes. Exactly. Yeah. We're minimizing. So, okay, this is the cartoon that we should have in mind. We have our probability simplex, the tetrahedron here, the blue, you know. The blue, you know, squiggly thing is supposed to represent the algebraic statistical model or any model, really. Okay, and then somebody gives you data, the red point P, okay, and we want to find, in a sense, the closest point to P on our model. It's kind of misleading, of course. This is not really a distance. Cobalt is not symmetric, for instance, in P and Q. But that's the analogy. We're trying to find the closest one. Trying to find the closest one. Okay, so that's good. All right. So I do want to change the question. Again, this is the question that we did not ask, but Guda sort of pointed us to ask, is that if I give you a model, and you think about the function that eats a point and gives you the divergence from your point to the To the your model. And now we want to find those points in the probability simplex that's going to actually maximize the divergence. So these are the points that are in a sense the farthest away, okay, in the KL divergence sense from the model. Okay? We're interested in both the maximum, the actual value, the maximum, and the maximizers of this function. Of this function. Maybe quick motivation. So, as far as I know, that this problem as it stands, the way I suggest it, is formulated in 2002 by Nihat I. He is the person from Leipzig and worked in information geometry, sort of the differential geometric take on statistics. On statistics and a lot of other things. And if you read sort of his initial paper from O2, so he sort of gives this kind of two kinds of motivations here. One of them comes from neural networks, which is supposed to represent or simulate sort of actual biological neural networks. And there is sort of this sense there that in these networks, or these networks develop, Or these networks develop in such a way that the information, the mutual information in the input and the output of a layer is sort of maximized. What you should have in mind is that sort of this principle that this happens like in our brain too, that if you have neurons and they fire together a lot, then their connection should get stronger. So, again, that's one motivation. And of course, the problem that I stated doesn't sound like that, but it is motivated in Nihara's work from this perspective. The second perspective is the following one. So suppose you have a system and you're looking at the state of the system. You can think about this as a probability distribution. And you want to measure the distance of your... measure the distance of your state of your system from some nice sort of product state yeah think about the following so you have you have your probability distribution okay and you want to measure the the distance how far it is from the independence model okay so you want to measure how dependent you know your variable is yeah okay so these are the two maybe underlying uh motivations but Motivations. But again, the way I suggested it is a mathematical problem. In particular, I'll tell you about what happens in motivation two sense. But it does have this neural network connection. Okay. Is this good? So these are at least two things to think about when you start up. Okay, so the formal setup in my talk is that we have an algebraic statistical model. Have an algebraic statistical model given by a toric variety. Okay, I'll think about my toric variety as a projective variety in Pn minus one. Okay, and this goes by with a matrix. Yeah, it's going to be a D plus one by N matrix. I'll put a one one one one one on top to make it really projective. And then I can assume the rest of the columns or the second to the last row of the columns are non-negative integer vectors. And I'm going to assume that the rank of this. And I'm going to assume that the rank of this matrix is the plus one. So this defines the toric variety via the usual monomial map, given by this matrix. I explicitly wrote this down here. I want you to note one thing. Here we go. So, you know, in the storage variety, I am putting these what I'm going to call weights in front of the monomial. The monomial that defines the map as well. So, in the usual setup, these weights are one, one, one, one, one, one, one. Yeah, but we can choose different weights. And in fact, from a statistical perspective, choosing appropriate weights where the amount degree goes down, okay, is a good idea. Yeah. So, I will also think about this weight here. Okay, so again, so in this setup, the problem is to find the maximum of Find the maximum of this function. Okay, so it's the this function gives us the divergence from a point to our model, to our toric model now. And we want to also understand the maximizers. These are fixed constants. I'll set them up from the get-go. It's kind of a twisted toric right. Okay, so let's do a quick example that we are all familiar with. Let's take two binary independent random variables. This is a two-dimensional model given by the following, this matrix A. Note that this matrix does not fit into the description that I gave you, but we can turn it into that description. This is more natural A, there you go with that. Our model, or the Toric variety, consists of all two by two matrices. All two by two matrices, okay, or they're at most rank one. Therefore, the determinant vanishes on this variety. So, this is one of the first sort of things that we think about. And so let's think about the actual problem now. So, maximizing the divergence problem. Okay, so we have our varieties in the probability simplex, three-dimensional probability simplex, and I try to do my best by hand to draw this two-dimensional surface. To draw this two-dimensional surface, as are the surface of the blue with the blue lines. Okay, that's supposed to be the guy, yeah. Okay, uh, um, and so the vertices correspond to these sort of the direct distributions, yeah. Um, so I want you to focus on the uh, I'm going to ask you that, so let's look at the point I'm going to call p prime. That's this point here, that's at the midpoint. That's this point here, that's at the midpoint between the two vertices, yeah? And there is another one on the other side, yeah, okay. It's very easy to compute the maximum likelihood estimator for this model, for any independence model. Okay, you have to take the marginal distributions and then take the products. And then the MLE, in this case, is this guy, which is at the midpoint on this dotted line. It's also the unique point that intersects the toric variety. Intersects the toric variety. This line intersects the toric variety, that's the only one. And you can compute the divergence from p prime to q star. So you didn't always glory the formula. This is the divergence function. And when you compute this out, you get log two. So the divergence from actually any of the purple points, you can take either of them. You can took either of them to the MLE and hence to the model is log 2. I'll quickly claim that this is the maximum. And these two points are the unique maximizers. So how do I know that? Okay, so this actually comes from one of the first results in this direction, is from 2006 by Nihat Ai and then Knauf. And then Knauf. And this is very general. So, if you have an independence model of random variables, discrete random variables, where the first one has D1 states, the second has D2 states, and the third one is the D3 state, et cetera. But I'm going to assume that I'm going to order them increasingly or non-decreasingly like that. The maximum, okay, the maximum divergence from any point in your simplex to the independence model is at most the sum of these logs. So you take the log of D1 plus log of D2, dot, dot, dot, up to log of dn minus one. You don't look at the last guy, yeah, you remove that one. The log of dn is not there, yeah. Moreover, Moreover, this bound is actually achieved. This maximum is achieved. If and only if the last one, the dn is greater than equal to this alternating sum of GCDs, of the D1s. It's a remarkable theorem. Okay, so what is this to do with the earlier case? Well, there are two special cases where the maximum is achieved always. Maximum is achieved always. And the first case is that if you have two random variables, no matter what, d1 and d2, and d2 is always greater than or equal to that, so greater than or equal to d1. That's that formula there. So the maximum is to be achieved. Yeah. And then also the second case is that when d1, d2, dn are all the same. Okay, either 2 by 2 by 2 by 2 or 3 by 3 by 2 by 3, et cetera. So in those cases, this is condition holds. And there are other cases which condition holds too, but this is one of the cases. This is one of the cases. So, and therefore, I know in the earlier example that I have my maximum because the formula says the maximum has to be achieved as block two. And I told you two points with that maximum. The very first case, the very first case where this formula fails, i.e., the maximum is not achieved at the sum of the logs, is the two by three. Logs, okay, is the two by three by three case. Okay, so in the two by three by three case, the maximum is not log two plus log three. It's not log six, yeah, it's smaller. Okay, that's the very first case. And I might tell you a few things about that later, yeah. What's the intuition behind this kind of Merbius formula? Uh, you okay, so um, I and Kraft actually characterize what kind of um What kind of distributions do you have to have to achieve this? And that has to do with how the D1s and D2s line up with each other. It's not very intuitive, but when you start playing with three-dimensional, four-dimensional tensors, that's sort of the natural conclusion you get to. Okay. Okay, so I hope, oops, I'm going too far. Sorry about that. I'm going too far. Sorry about that. So, okay, so this is for the independence models. This was one of the first second motivation, really. In a sense, it's been at least partially answered. I have to say something one more time. So there is a similar kind of theorem, more recent one, that's proved by Nihatai, Johannes Rao, and Guido Montafor, when the model is a restricted BOSON machine. Is a restricted Boltzmann machine. It's a similar sort of upper bound, it is given by sums of certain logs and then the maximums achieved under certain conditions. So I'm being vague, but there is sort of some improvement in that direction too. Okay, so before I go further, I want to take a little detour. I want to take a little detour. So this is all for Torrek case, right? I want to look at discrete linear models. Discrete linear models, yeah. So, when we started working on this, you know, we always talk to Baron too, and this is one of the things that you need to sort of ask: okay, before Torik, you know, why don't you look at the linear case? Yeah, yeah, so let's look at this because I think it gives some sort of point of view, okay, of what's going to come within the toric case as well. So, what is this linear model? So, this is a model which is defined by linear polynomial. Which is defined by linear polynomials in our parameters. In this case, just a few years ago, Julia and Alexander Heated proved that the Logarikmid-Worner cells for discrete linear models are actually polytopes. So let me remind you what I mean by Logartmid-Worner cell. So you remember again the maximum likelihood estimator estimation problem, right? You have a data and you want to find the closest point, you know. And you want to find the closest point, you know, on your model, you can look at the fibers of this map. You think about it as a map. So, I give you a point on the model, and I ask you which points, which data points actually give you that guy as the maximum likelihood estimator. So, the set of all those points in the linear case form polytopes. So, important observation here is that remember when I fixed the Q, the divergence function was convex, right? So, this means that if you take your Q to be the MLE, since I have a polytope now, right? So, on that polytope, the divergence function is again convex, it's linear, which means that on that particular log-voronary cell, the divergence function is. The divergence function is going to be maximized at a vertex of the spolytope. Okay? Okay. So maybe a little picture I lifted from Julia's paper. So this is a, so you should think about this as a linear model. The dotted, the dots there are on a line. Yeah, that's our model. And for each point, we draw the corresponding log Voronoi polytope. Log Voronoi polytope, and all of them are, as you can see, triangles. And on each one of these triangles, the divergence function is maximized at a vertex. All right? Okay. Oh, okay. All right. So here is another result by Julia. So it's another subsequent paper. Julia has proved in 2022. proved in 2022 is that that this Logert-Navorner cells of all of these interior points on your model have the same combinatorial type. So in the earlier, in the example before, you saw that those are all triangles, and they're always triangles, they never change. So that's the result. So if you give me a non-linear model, for every point, the log Voronari polotop is going to have the same combinatorial type. Polotop is going to have the same combinatorial type. So you said the maximum is at the vertex, but do you know which vertex does this change? Okay, it's coming. Yeah. So, okay, so I'm coming to the result. Yeah, I would like to tell you what exactly which vertex is the maximizer, or which was the candidates at least. Yeah. So I'm going to think about our linear model as the image of an affine. As the image of an affine map, you know, C minus Bx, where B is an n by D matrix. To make this land in the probability simplex, you know, the rows of the B has to sum to zero, and the entries of the C has to sum to one. And we know precisely, so given the point on the model, we know precisely how to get the vertices of each log of or noise L. log of or noise L. And the vertices are in bijection with these positive core circuits Z of B. So these are minimal dependencies of the rows of B. But when you don't have zero, they are positive, no negatives allowed, such that this condition holds. Zi is Qi's are equal to one. From there, we can get the vertices. So we know exactly what the vertices look like for each. Exactly what the vertices look like for each point on the linear model. And now from here, we can actually compute. We can compute the divergence function from any of the vertices given by the same co-circuit. Okay. So for each core circuit, there is a vertex in each log-Bornoise cell. And for each one of them, Yeah, and for each one of them, we can actually compute the divergence to the MLE, and that's actually a linear function. Okay, Zi times log Zi is fixed. Yeah, this is a linear function in the Q's, on the points on the model. All right. So if you want to find the maximum divergence, what you do is that you optimize this linear function over your model, okay, for each co-circuit. Okay, for each core circuit. That gives you a vertex of the model, right? And you pick the winner out of them. So this tells you we precisely know what the maximizers are for the linear case. Do I have, oh no, this is going on. So again, so going maybe to the picture, right? These are the log bornoid cells. Let's say, you know, the vertex going a little. The vertex going along the sort of the top line here, this edge here, that corresponds to some co-circuit. And we can find the maximizer of the divergence function from those vertices to its MLE by solving a linear program. And then we get the maximum out of them. Is that good? You went to the theorem? You mean to the theorem? Yeah. Can you remind us about a positive core circuit? Oh, yes. Positive core circuit is that these are the dependencies of the rows of B, i.e. combinations, linear combinations of the rows of b that gives zero. Okay, but the minimal, so their support is minimal, and the sub and it's a positive. Yeah. So it could have zeros, but otherwise it has positive numbers. Excuse me? Excuse me? Because the i's they're positive, yes. Yeah, okay. Is it a general fact that the optim is always attained in the large wall noise cell? It's not clear a priori, right? You have a point on the distribution, you have a distribution in the model, and you want the furthest, it will be like like the furthest. A priori, not clear that that's actually the long corners. Every point is in some logo or cell. But you want the furthest. Well, I think you want the furthest from the model. That's right, from the model. From the model. Yeah. Yeah. You can also ask for a fixed point in the model. Of course. Of course. Yeah. For each politop in the toric. Of course, I can do that. Yeah. But I'm going to find the maximum among all of those, right? Yeah. So first of all, you find the, okay. So first of all, you're going to pick one vertex. It lies in some long boron, right? Yeah. Exactly. At the end of the day, it need not be a vertex of the center. Of the day, it need not be a vertex of the symbol, it's going to be on some facial, of course. Yeah, even in this earlier picture, you see that, yes, yeah, yes, yeah, yeah. But it is so the log Voronoi cell where the maximum is given comes from a vertex of your model. You take a vertex of the model, you take this long Voronoi cell, and a vertex of that log Voronoi cell is a maximizer. But that log Voronoi cell of vertex will be a phase of the original sample. Well, it's going to lie. Well, it's gonna lie in this in the face of the original simplex as this as seen here. This triangle is not the entire entire gun. Okay, the other one is, but not this one. Oh, yeah. Oh, yeah, yeah, yeah. Yeah. Okay, very good. So, again, this sort of, as we see further, you know, you'll see that this gives you some point of view for the for the torrent case as well. Toric case as well. Okay, let's talk about the toric case. So, in here, so we have to ask ourselves again what are the log-born oil cells in the toric case, and we know them. So, if you give me a point on your model, then we can look at the polytope given by all points in the probability simplex such that A times B, the matrix A, is equal to A times Q, where the Q is from your model. And let's call that B, the A times Q. Yeah, and let's call that B, the A timescu. So, this is a polytope. For each point on our model, we do get this particular polytope, and these are the logborn cells. Every point in this polytope maps to this Q to get this MLD. Sorry. Because after Baron's question, I'm wondering, does it make sense to ask the furthest point from not the model, but a given point? Yeah. And then you don't know if it's. And then you don't know if it's in the same Voronoi cell. No, no, no, no, no. So if you give me a point on the model, Q, yeah? So you ask me what is the farthest point in its log Voronoi cell, Verono cell from that. It has to be one of the vertices of that poster. Yeah? That's a convex. That's a convex problem. Absolutely. Yeah. It just happens to be linear in the linear. Yes. Yeah. Yeah. Okay. Okay. Let's say this is the Okay, let's say this is the, you know, on this full log born or cell, yeah, the divergence is maximized at a vertex of that polytope. Yeah, now, but there are many types of polytopes now. In the linear case, there was only one type. So as you change your Q on the model, i.e. the A times Q is equal to B, as you vary that, the politops change. I mean, the combinatorial types change too. Type change too. Okay? Yes? Okay. But the idea is the same, right? So I would like to be able to think about the maximizer in each polytope and somehow take the maximum out of them. Okay. Okay. So here's again a nice picture that I lifted from one of UL's papers. So in this case, the dotted curve, you know, the curve is a twisted cubic. It's actually the sort of. Actually, the one with weights, with binomial coefficients, so that the ML degree is equal to one. And then you see the Log-Vornois cells in here. And there are actually essentially three types of Log Voronois cells here. There are some triangles, these are from different points of view. Yeah, you get to get the first one. There are some triangles, there are some quadrangles, and then again, there are some triangles. In this case. In this case. All right. Okay, so here is a fundamentally important result that we need to discuss. And second, it's by Nihatai, one of his first papers. And then it was refined by Ferromotus in 2007. So this theorem characterizes the critical points of critical points of this divergence function okay to your model okay so among them among the critical points of course we had the maximizers yeah so a point p star in our in our tropical simplex is a critical point of our divergence function okay for a toric model if and only if you do the following let me try to explain so we have we have p star yeah which you can take as a as a vertex Which you can take as a vertex of its log Voronoi cell, right? Okay? You draw the line from P star to the through the MLE. Okay, there's a line goes through them. This line is going to intersect the politope at another point. Okay, let's call that Q, what did I call it? P star. I called, no, P hat. Excuse me, I meant it P hat. Yes, it's P hat. P hat, yes, it's p hat. Yeah, so L, no, what is it? L Q star is MLE. Q star is MLE. P star is the vertex you start out with, yeah? And you draw the line, and there is another sort of antiportal point, sort of, yeah, in your polytope on the other side. Let's call that P hat, yeah? Okay, exactly. So I draw the line segment through them. So this line segment, a line line. So, this line segment, a line lies in the polytope, it's convex, yeah, hits the polytope on the other side, okay? So, for P star to be a critical point, the support of P star and the second point P hat have to be disjoint. Okay, it's a very combinatorial condition to be a critical point. This follows from actually computing the gradient of this function. If you compute the gradient, If you compute the green Littletos function, that's the conclusion that comes out of it. Yeah? Okay. Like for instance, if I have a codimension one model, i.e. a hypersurface, a torrent surface, given one equation, by this theorem, you can quickly prove that there's going to be exactly two critical points. To two critical points, so the log vonora sides are one-dimensional in this case. Yeah, so there's a vertex and an opposite vertex. Okay, so if there is a p star that's a critical point, the opposite vertex also has to be another critical point, yeah. And the exact two, okay, given by I tell, I told you what the p-star is actually, yeah. Now you can you can check this, okay? And one of them is the maximized, or both of them are maximized, maybe, yeah. Q star in this situation we can compute. Yeah, it's you have to start with P star, and then you go in the direction of the difference vector of the exponents. That's the linear space of the political. So you can explicitly get it, you know. So, you can explicitly get it. Okay. So, I go back to this picture because this is the co-dimension one picture. Yeah. So, the two purple guys are the vertices of the log vulnerability cell, the line segment is the politop, and they have disjoint support. Okay. Okay. So, I just told you that there are different types of polytopes. Of polytopes as you vary your point on your toric model, yeah, as opposed to the linear case, this was only one kind. So, how do you so it would be really nice according to this theorem, we should pay attention to these politopes and the vertices and the sort of supports of the opposing faces. Yeah. So, that these polytopes are the log-vonner polytopes, they're combinatorial types, are parametrized by this object called the By this object called the chamber complex of your, like we can say the toric variety. So, what's the chamber complex? You take the convex Hull of your A1 through AN that defines your toric variety. Let's call it QN. It's a polytope. The chamber complex of this polytope is the common refinement of all of the triangulations of the polytope. So, picture is best here. Okay, here is a Here. Here is a case where I took a pentagon. The vertice of the pentagon are my A's, A1, A2, A3, A4, A5. So this is the toric surface. And I drew all possible triangles whose vertices are the vertices of the polytope. And I superimpose them. You get a polyedal complex. Polydale complex, and this polyuthal complex is called the chamber complex. Okay, the chamber complex, each cell in the chamber complex tells us different, so each cell, yeah, each cell of the chamber complex gives you log Voronoi cells with the same normal fan, hence the same combinatorial type. Okay, for instance, just a quick thing. So, let's give So let's give you three examples here. If I pick this red, sorry, the green vertex in the middle, that's a chamber. That guy corresponds to one, exactly one polytope. It's a triangle, the triangle. And I also told you, I'm not telling you what the exact triangle is, but I'm telling you the supports of the vertices. Okay? Of this triangle. Of this triangle. Then there's this blue line segment there in the middle. Yeah, I also do that one too. That blue line segment gives you a quadrangle. Every point on that blue line segment gives you a quadrangle with exactly these supports, whose vertex has exactly these supports. Okay? Yes? How do I get the Article complex here? Oh, you mean the ACE? These ones? Okay. Once okay, so remember that Vogue-Vorner cell is the politope. Yeah, in this particle, it is given by you know P1, P2, P3, P4, P5, non-negative, some up to one, and A times P is equal to this green point. Okay, so I'm so I'm calling P1, P2, P3, P4, P5 A, B, C, D, E. I'm telling you the support of its vertices, yeah. Support of its vertices. So the blue one gives you a quadrangle with those supports. The entire blue line segment gives you that. And then the pentagon in the middle, that chamber, the purple chamber, gives you pentagons with these supports. All right? Okay, so one here observe. So let's put the theorem to work. Yeah, here. So what does the theorem say? Remember, I have to. The theorem says, remember, I have to look at for vertices, okay? Um, so let's say, let's look at these combinatorial types, yeah. So I look at the vertex and there's a vertex which supports AD. Every vertex of log vulnerable cells coming from these points have that AD there, yeah. And you see, there's another vertex which supports BCE, which is complementary, okay. Okay, so potentially, potentially, there is a critical point somewhere on a polytope coming from this point, from the points on the blue line. But there is a condition, right? The condition has to be that if I draw the line segment from AD to BCE, right, that line segment has to go through the MLE. Okay? So, So, okay, so all right, so maybe I go on, yeah. Okay, so there are a few helpful observations you can make here, yeah. So I'm going to present you an algorithm to compute the maximizer, yeah. The first of all, so if you look at an open chamber of dimension K, this will contribute critical points. When I say contribute, I mean that that could be a critical point on one of the log vorno politops coming from that chamber. Yeah, it will contribute a critical point. Yeah, it will contribute the critical point. Okay, as well, only when the dimension of the chamber is sort of not too much. Okay, what am I telling you here? The reason I'm telling you here is that, maybe I go back here. So what I'm telling you here is that imagine this pentagon coming from this pentagon that has to be the chamber. There is no way, there are only five variables. There is no way, there are only five variables. There is no way you can find a vertex and another vertex with complementary support for these pentagons. Okay, there's no way. So therefore, if you're running an algorithm, you can completely get rid of, remove this pentagon, and you don't have to check it. Now, of course, if you have, if your codimension is high, then Then you have all chambers, yeah. But if your co-dimension is low, then you have to check low ones, yeah, just from for support reasons. So the other observation is that suppose you look at the chamber, okay, in your chamber complex, and suppose this chamber does not contribute any critical points because of support reasons, like the Pentagon, yeah? Then you can immediately show that any other chamber that contains That any other chamber that contains this chamber is not going to contribute critical points either because of support reasons. So you can also cut down work on your algorithm. Finally, this is something here. So if you have your polar top, and if you look at chambers that actually touch the boundary, that you can also get rid of because again, each vertex is going to have some common support. Um, some common support among them, yeah. So, what I'm saying here is that in this picture, in this picture, yeah, all the only thing that you have to consider are these five green points and the five blue lines. Nothing else you have to consider. Okay. I mean, of course, I'm picking a, I need a disclaimer, yeah, I'm picking a convenient example here. I don't mean to say that it's always going to be like this. That it's always going to be like this. Yeah. Why interior pentagon does it? Because the interior pentagon, if you look at the log polytops of the interior pentagon, the support of each vertex has to have three guys. But I have five variables. There's no way you can. Absolutely. But I mean, this won't work. Even six con is not going to work necessarily. Yeah. Yeah, it could be on a six column that it's supported on the folder. Yes, it could be. Yeah, yeah. Okay, so these are sort of things you can do. Yeah. Okay, I'm still running out of time. Oh, yeah, this was this was what I just told you. Yeah. For this Pentagon, you only have to consider the green points and the blue lines. And it turns out that, so the green points, by the way, so they correspond to us triangles, right? So there is a pair of vertices, A, B, and B So there is a pair of vertices A, B, and B C, B E, which complementary support, but that line segment lies on the boundary of the simplex, and there is no way that line segment is going to go through the MLE. So even the green ones are not there. We only have to look at the blue ones. And it turns out, if you do the computation here, each blue one contributes exactly one critical point. And you have to pick one out of them. Have to pick one out of them. Yes, yes, yes, yes. And I'll give you an example where you have infinite many. I will do that. Yes. Yes. But with a generic one, I think it's going to be the only one. Okay. So let me sort of recap. Yeah. So if I give you a chamber, how do you test whether there's a critical point coming from there, from the chamber? So you have to get the combinatorial type. Uh, get the combinatorial type with the supports of the corresponding log bornary polytopes. Yeah, polytope. You have to look at vertex facet pairs, okay, where the vertex and the facet have disjoint support. Okay, then you can parametrize the line segments, okay, from the vertex to the points on the facet as the point varies in the chamber. Yeah? Chamber, yeah, and then you intersect that with your toric variety, okay? If this intersection happens to have points which are in the probable simplex, i.e. you get MLEs, okay, those are the points that's going to contribute, those are the critical points. Okay, okay, so quick summary of this algorithm. Yeah, so we can call, I can call this a combinatorial or and numerical algebraic geometry algorithm to compute maximizers. Algorithm to compute maximizers of the divergence function. First, you have to compute the chamber complex. This is a challenging combinatorial computation. You could use things like Topcom if you can install it on your computer. G-Fan at least comes with Mokoli 2. If you use the brew, homebrew, it just comes down. It's all good. So, you can compute preferably, yeah, and that's what we have to look up to symmetries. In many of the examples, we're looking at the politopes and the chamber copies have a lot of symmetries, and it's good to know them. All right, so then you eliminate chambers if any of the helpful observations apply. Yeah, you can do that, okay? And then for each good chamber where there's potentially critical points, you have to do this challenging computation, yeah, all right. Yeah. Okay, so because you have to intersect the autoric variety with this construction where you parametrize the line segments, the lines. And that we were not able to do that in general with symbolic methods. So we looked at doing this with Bertini. And for some reason, I don't know why, maybe it's my incompetence, I was not able to get Bertini working in my Macaulay 2. In my Macaulay 2, okay. So, but eventually, I was able to download Bertini itself. Okay, it took a while also, and then you used Bertini to actually do these computations. And then among them, you identify the maximizers. What's the algebraic description in order to put the problem on the circuit? Okay, so the problem is the following. So, I have a chamber. I can parameterize the points on this chamber in terms of the vertices of the chamber. terms of the vertices of the chamber. Then for each point, so and then I can also get the vertices of the corresponding log borner cells. So I pick a vertex with a potential facet that has complementary support. I also parameterize the points on the facet and I parametrize the line segments and I plug them into the toric equations and that's all. Yeah. I have to say something. So, you don't have to do the distance anymore, the maximum. Well, okay, very good question. Because these intersections in the chamber, so in the good cases, you have finitely many points. So, I can just compute the divergence to the MLE numerically, yeah? But if it's a infinite, it's a positive dimensional component, yeah? Positive dimensional component, yeah, you have to do some more things. Yeah, you actually have to solve an optimization problem on a semi-algebraic set, and that's that's difficult, yeah. But you're solving here the critical equation, yes, yes, yes, I am, yes, I am yeah. So, this in itself, so there's do you describe a significant piece of combinatorial complexity here, of using all these cases? Yes, yeah, but then once you have one of these pieces, Once you have one of these pieces, there's still algebraic complexity of the C. Absolutely, absolutely, yes. So again, I think I had two steps here. So in many cases, there's a combinatorial complexity. It's governed by the chamber complex and the combinatorial type of topology. Again, remember, in the linear case, there was only one type. There's more here. And then there comes the algebraic side of that. In the linear case, it was just a linear problem. That's more here. That's more here. Okay, so I'm running out of time. Sorry. I want to give a few, just a few examples very quickly, maybe. So let's take the what I've called a multi-normal Veronaiser surface. So this is the two Turner random variables, you know, the IID, and you record, you know, how many times you saw one, how many times you saw two. Yeah. That's the A is given by the, defines for us the Veronica as a surface. Finds for us that Verona is a surface, but I'm going to use multinormal coefficients to make the ML degree to be equal to one. So those are the weights. Yeah. That's the picture. So the big triangle is the triangle that defines the Veronese with six points. There are some midpoints in there. And then if you do get rid of the, if you use the helpful observations, certain guys are gone. You have to consider the green points, the blue line. Green points, the blue lines, and the purple chambers. It turns out, you know, some chambers contribute multiple P and F pairs. Okay, you have to check each one of them. There are 27 pairs like that. It turns out, though, no age or region actually contributes the critical point. If you compute these equations, you get the Bertini, the solutions are, even the solutions. The solutions are, even the real positive solutions you get give you, tell you they're on the boundary, they give you the green points. And when on the green points, the middle vertex is the maximizer. Okay? With that P. This is the vertex of the log order cell. In fact, I'll state as a theorem. If you can generalize this, so you have if you have. So, you have if you have a dr d plus one array random variable, m of them you do you know get id and then you record the you know number of ones, number of twos, et cetera. That's sort of the twisted multinomial Veronese. Then you can prove that the maximum divergence to the corresponding toric model is at most m times log d plus one, okay, and at least m minus one times log d plus one, because you can exhibit a point. because you can exhibit the point like this yeah and we conjecture that the maximum divergence is actually m minus one uh times the log d plus one yeah and it is attained at the vertex of the muddle vertex okay this is consistent yeah this is consistent with the with the case when d is equal to one yeah you have a curve yeah the binomial curve but proved this conjecture for that case yeah right so i'm out of time Sorry, I'm out of time. I want to also show you the independence model. This is a two by three case. Okay. The politop is the top Leron. The chamber complex on the left is color-coded up to symmetric classes. You can use a lot of the helpful observations to get rid of many, many, many, many chambers in this case. And all you have to do is sort of look at this middle, there's a middle chamber, the middle chamber. The middle chamber, and on this middle chamber, it's a bipyramid of a triangle. Yeah. Okay. And you only have to look at the edges of the equator in the middle. Those are the only ones. And surprisingly and nicely, every point on that line segment, on each one of them, give you exactly two maximizers, actual maximizers. Okay, actual maximizers, and therefore you get this in the probability simplest, you get this hexagon many maximizers. Okay. It can be positive image. Oh, well, of course, yes, yeah. But it's in very, I think, symmetric cases, yeah. As like the regular Pentagon, as Bar was saying, yeah. Quickly, so what is next? So we are hoping to not hoping. are hoping to not hoping we are going to finish the writing this paper by by uh by end of june okay but there is a few things to be done for instance we want to revisit two by three by three independence model this is when the max the maximum was not attained uh you know john uh johnnes rau actually computed the maximizer in this in this case using his algorithm yeah we want to uh prove that our algorithm also really works or competitive by by uh settling that By settling in that case. The next case is actually a two by two by two by three case. This is the case where again the maximum is not attained by the corresponding formula. And I think still if you can do two by three by three, I think that one is also achievable. We should be able to do that. We are really hoping. So the systems we have to solve have a multi-homogeneous structure. It will really help to be able to use that. At this moment, neither Bertini nor Mokodu II have the, nor Continue to have the, nor homotopic continuing.jl have the capabilities of doing this. Yeah. But I'm thrilled to hear that Jose and others are working to implement this. And then when they have that, we're going to use it. So there are some classification of ML degree one toric models in low dimension for two and dimension three. We want to actually complete the work on those models where we find the maximum. Those models where we find the maximizers and the maximum divergence of all models. And then there are probably other cases, like yesterday, we were chatting with Carlos. One can look at things like the reflexive polytops of dimension two or three and their toric models and then compute all of the maximum divergences for them. Okay, thank you very much.  Um, so yeah, so I mean, you know, we kind of know we need a big sort of car product. There's also some kind of L3 or, you know, factors that are big nightly. So, like, I guess, like, they're half like I have that you know if I say here in my group all Windows and I know it'll work and I think it's working product and I build a new cell from the old cells that you can work by the product. If you have a construction, okay. However, the construction has the one nice part, okay, and then there is a so this construction if you don't load one ourselves for each model, but comparable yeah, I can build the Yeah, I can build the corresponding log born cell. Sorry, log born cell by so by this sort of, you know, how do you piece together like even the MLEs, right? There is a sort of form. You use that, that gives you a non-linear object, yeah. But then you have to add, you have to add these kernel directions to complete it to the polytope. So it has sort of a non-linear part and a linear part. It seems like it doesn't. It seems like it doesn't at this moment help to relate the divergences of the two polytopes to the divergence of the bigger polytope. It's not obvious how they relate to each other. Absolutely. But even in the independence model, yeah, when Independence model, yeah. When the maximum is achieved, it's sort of you can build the maximizer of the big model from the pieces. When it's achieved, though, when it's not, you can't. That's like an obstruction. That's the reason sort of why the maximum has to be lower than the upper bound. Yeah, yeah, yeah. I actually just wanted to rub to this. Yes, yeah. What about binary degrees? Because then maybe you would natural question is to find a maximum to you fix the number of binary variables and now you maximize the distance between all the three models. To all the three models, you know, that for minimizing this, this whole new algorithm, that you know that you can very nicely minimize the likelihood of all the three models at once. That's a very good question. Okay. I think at some point we looked at the path, right? But we didn't, we were not very serious about it, maybe. So thank you. I like that question. So I don't have an answer, but we could be maybe done. Yes. But I'm not sure if I can do it. I was wondering what the reflexivity assumption would afford you in this situation. Okay, I put that little last line there because Last line there because I had a conversation with Carlos yesterday. Yes. And I was like, oh, these are, of course, nice politopes for which we can. I don't, I know, I know Carlos and his student are thinking about what statistical relevance these politopes have. Yeah. But I don't know and I don't have an answer for that. This was also secretly a question for Carlos. Why, I guess, are you considering the ML degree one reflexive polytopes? Okay. Okay. Anyways, thank you both. Okay, I think we'll and that's there. Anything right? I don't know. Okay. So then thank you. So we'll have a two-minute break and then we continue the next talk. Speaking I need to migrate