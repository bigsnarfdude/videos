Thank you. Very good pronunciation of my last name. So, thank you for that. Yeah, so great to be here. Thanks a lot for having me and sticking around for a last talk. I just want to mention, so I'm in the math department since this has come up a lot. We have people from different fields. Also, I'm in the applied math group. However, this is joint work with my PhD student, Amatio, who is actually in computer science. Is actually in computer science, Harlin Lee, who is in data science, and Nia Sharon, who is also in applied math, but in Tel Aviv. So it's a UNC work with one person from Tel Aviv. Okay, so the talk is about trajectory inference, which is a problem that has been around for a while and there exists a lot of solutions for that. I will give you an overview of prior work. The reason I only give a half an hour talk is because what I'm going to present, I feel like is a little idea to improve. Like it's a little idea to improve stuff that has been around, but I think it will have some impact for computation and you know geometrical meaning of trajectories. So this is somehow the motivation. So it's not very difficult to do, but I think it has some good properties. So I will explain that. Okay, that's not how you do it? Yes. This is how you see who has already given a talk, right? Okay, so this is the outline. I will give an introduction to what I mean by trajectory inference in the Waselstein space. I have a couple of slides on optimal transport. I don't actually think we need that. But for notation purpose, I will keep it. The main thing I will need is the Walserstein geodesic. So this is something you can keep in mind while. Something you can keep in mind while I give the presentation. I will talk about prior work and then the proposed method, which we called Wasserstein-Lane-Reasonfeldt algorithm, short WLR. If you have ever heard of the Lane-Reasonfeldt algorithm for whatever reason, I think you already know what we're going to do. If not, I will give an introduction. But it's basically a numerical algorithm. Okay, so what is trajectory inference in the Wasserstein space? So the problem you should imagine is you have Imagine is you have measure data or point cloud data that changes over time. So you have an ordered sequence of measures. And so here the x-axis is time. And then you see how the Gaussians evolve. And so the point is, we don't know how individual particles relate between time steps. So if I have one particle in the time step T0, I don't know to which one it belongs in the time step T1. So what we want to do: Time step T1. So, what we want to do is infer likely trajectories that happen. So, this is basically the thing you want to do: figure out how to match those things. And on purpose, I draw these kind of arrows that look very unlikely. So, if I start at the bottom right, is it likely that I'm going to end up at the top left in the next time step? Probably not, but it depends on what kind of geometry you're interested in. And this is obviously a problem for optimal transport, if you think. Obviously, a problem for optimal transport, if you think about it, right? So the first kind of, maybe not the first kind of work, but an influential type of work basically said: let's take this point cloud, compute optimal transport for the first to the next, you get trajectories, right? Then take the second point cloud, compute optimal transport to the next, get trajectories. So this is something like a linear interpolant, right? And this is very, very natural to do. Okay, so the problem I want to talk about is this kind of I want to talk about this kind of time-evolving measures. So, if a sequence of measures ordered in time, and here I wrote, you know, space of probability measures with some finite moment, if you want. And the aim is to find a curve that either exactly interpolates the measures or approximates them to some degree, right? So, it's a classical approximation interpolation problem. I'm not assuming anything else other than knowing. Anything else other than knowing time step T0 have this point cloud, time step C1 have the next point cloud. There's no other information, just the point clouds or the measures and the support space. Okay, so a classical interpolation problem. And what I'm going to propose is something which is similar to things other people have proposed. So it will be based on beast blinds, which is, you know, I will introduce later work that already. Introduce later work that already exists to some extent. The new thing is somehow the fact that the trajectories I'm going to impose are intrinsic to the Wasserstein geometry, so we're never leaving the Wasserstein space. And therefore, we can easily deal with non-uniform mass and trajectory splitting. So when you think about optimal transport, trajectory splitting, mass splitting is very natural, right? So if you start with a lot of mass in the beginning and then you have little mass, but few of them in the middle of the... Little mass, but few of them in the next step. Naturally, you have to split, right? But most of the methods that exist can't do that. So, this is like two of the benefits we get out of the intrinsic property. And the reason we call it efficient, maybe flexible is the better word. So it's a numerical iterative scheme. You can basically choose how many steps you want to do, how accurate you want it to be. And depending on how much computational time you have and how accurate you want it. So there's like some degree. Wanted. So there's like some degree of flexibility to it. Okay, so this is the introduction of what I want to do. There's a motivation. Maybe you have seen this. Very influential paper from Schievinger from 2019. Basically what they tried to do is, so this is a paper published in cells, right? So this is a biological journal. And what they were trying to do is you have, so you model cells as point clouds in gene expression space. In gene expression space. Basically, you sample, you have cells, you measure gene expression, and this is how you think about them as objects in this space. And then you do something to the cells, and you see how they involve over time. And you want to interpolate in between steps. So, for example, they start with embryotic cells, and then they want to see, they do something to it, and they want to see which of them are going to be stem cells. Something like a reprogramming kind of problem. Of problem. And as you go with the process, you will see some of them will be stem cells, but others will be something else. And the question is, why is that? So, what did we do wrong to not get stem cells everywhere and stuff like that? So, basically, it is a point cloud evolving problem. And you're trying to understand, like, if I start here at the very bottom, am I going to move at the boundary? Am I going to move in the middle? What is the most likely path? And the thing that I just described before with the linear interpolation is exactly. Described before with the linear interpolation is exactly what they did in this paper. So they basically just said: point LAD1, point LAD2, do optimal transfer. Point LAD2, point LAD3, do another optimal transfer. And this is how you infer the trajectories. And it worked very well. So I recommend reading the paper if you haven't done so. Yes, understand the evolution of cells in gene expression space. Cells are models pointed, as I said. However, and this is why you actually need optimal transport, when you do measurements, you destroy the cells. Do measurements, you destroy the cells. So, you have a very large cell population, you do measurements on some of them, they are dead. And you wait, next step, measurements on other cells, they will be dead. So, you actually don't, you cannot follow an individual cell over time because whenever you know what its current status is, it's gone, right? So, this is why you need optimal transport. Okay, so this is one motivation. Obviously, there are many more, right? This is how I got interested in the topic. Okay, so yeah, this is very quick optimal transport introduction. So I will talk about two things, both the Monge and the Contorage problem. Whenever I talk about splitting, obviously you need the Contorage problem. I just wrote down the Monge formulation, which is basically what people usually do. So when you think about cells, you have uniform mass in every cell. So basically, you have points cloud like that, right? So they don't. Like that, right? So they don't have non-uniform mass distribution. However, there are many applications where you would like to think about problems where there's not uniform mass in every point, but more mass. So maybe one motivational thing that you may have heard of is something like a supercell. You take many cells and then you combine them and they're so you take many cells which are similar and you just combine them into a bigger cell and you just give more mass to that. It makes computation easier. It makes computation easier. And this would be a motivation for not considering this setup. Okay, so this is the Monsch problem and equivalently, but equivalently, equivalently interesting, the Cantoric problem, where we actually allow for the mass splitting. And this is, you know, since this happens naturally in optimal transport, if we do trajectory inference, it should also happen naturally in our trajectory. This is somehow the motivation for the talk I'm going to give. What a talk I'm going to give. So, just a quick note: we're going to denote the coupling matrix with a P. So, this is what I'm going to need later. And yeah, basically, we're solving the linear program. So, this is not about Synch1 at this point. The method I'm going to introduce will work best whenever you have sparse solutions because it's less computation later. If you think about doing entropy, you get a lot. Entropy, you get a lot of trajectories, and then if you split the gun, you get more trajectories. So, this would be computationally expensive. So, the spaster the solution, the better for this method. Okay, so this was very fast optimal transport. I have just one more on geodesics, because this is basically what we're going to need. So, I think everybody has seen the geodesic definition for the MANCH problem, right? The Monch problem, right? So you have two measures, you compute optimal transport, you do a linear, basically linear interpolant, right? And then you push the measure with that. Maybe the new thing is the notation I'm using. So the algorithm I will introduce later depends on computing averages. So if you're in a linear space, an average is just a midpoint, for example. And here we can do averages by just saying take the geodesic and then take the midpoint on the geodesic. So this is why I'm calling it OT average. So, this is why I'm calling it OT average. So, this is something to remember for later. And similarly, if you want to do it in a discrete setting, you do a linear interpolant on the ground space, and then the weights come from the coupling matrix. Yeah, so I guess you have seen this as well. So, the one is Wassenstein geodesic, and the other one is if you do it with Euclidean geometry. Okay, so the important thing is remember when I say. Remember, when I say OT average, I mean take the geodesic and evaluate at some t on the geodesic. And we're going to use the midpoint later. Okay, so prior work. This is a non-exhaustive list. The thing I want to mention is there is a lot of, so basically there are three kinds of ways to do the trajectory inference. One is based on splines, which is what I'm going to talk about today. Splines only interpolation, stuff like that. Interpolation, stuff like that. Then there's a very oh, this has actually moved. Okay, never mind. So, this should be here. There is a lot, very long list of methods that do neural nets. So, basically trying to solve an ODE and then infer trajectories based on that. And there's other methods, mostly for the cell problem that I described in the beginning. So, what I want to talk about today is this section, which is on splines. And the question is, why do we need more spline methods? Is why do we need more spaline methods? Well, I will introduce those two first papers quickly. And when I read those papers, I felt like there's something missing to it. And I will show you a method that is somehow in between. So this is the contribution. Maybe I should mention there has been work before that. So those papers are very recent. There has been work on doing like cubic splines in Wasserstein space. Basically, working by doing. Working by doing so, basically, what they do is minimize energy on the measure space. So, this is a way to find clients as well. Maybe not computationally efficient. So, you know, you have to balance the theory and the computation. Okay, so the method I will introduce later is a combination or combination, maybe a generalization of the first two. First one is the one I have already introduced, the Schiebinger one, with the cells. And then the second one is a And then the second one is a spline method, which is very fast, but has some drawbacks. So I will show you in a second. So, this is what I had already mentioned, the paper by Shivinger on Cells from Cell. So, basically, what you're saying is you have measures evolving with time. You draw samples. Samples are uniform. So, this is the assumption they're making. And then you just compute optimal transport from one to the next. Optimal transport from one to the next. So it's like a linear interpolant. It wouldn't matter here if the samples were non-uniform because you could still do it, right, with a coupling. So there's no restriction to that. I said computationally reasonable because you only have to solve as many optimal transport problems as you have time steps, right? So for every time step, there's one optimal transport problem. What I will show you later will potentially need more optimal transport. However, Optimal transport. However, it will infer smooth curves. So, this one cannot give you smooth trajectories because it's linear, right? So, you know, if you want this little, if you want it a little bit smoother, you need some more computation. Okay, so this is one of the influential methods. And this is the other method, which is based on Splines from 2021. So, I'm doing a very basic problem so you can understand what's happening. Happening. So we have four time steps, and every time step has exactly two points in the point cloud. Okay, so it's very basic. This is just to make sure, you know, to explain the method. So you should think about two points and they evolve over time. Okay, so what they do is the following. They have the point clouds evolving over time and then they use optimal transport to couple those. So those are the optimal transport trajectories. Transport trajectories. If we did the method from before, we would be done. So, this would be exactly what they did before, just linear interpolation. And now, since they have the coupling, they can fit splines to the coupling. So, this is what they do next. So, you have coupled for every time step individual points, and then just use regular spline. Okay, so this is a method that uses splines. It is very fast because Very fast because it only needs optimal transport in every step, like before, and then you just fit the spline, which is also you know computationally efficient. However, okay, I'm glad I didn't do every individual item separate because you would be stepping the pressing all the time. However, it cannot handle mass splitting a non-uniform R because this thing only works if you have one point. only works if you have one point in every step, right? Imagine you have a larger point here. What would the trajectory look like? It should be splitting, but you cannot do this with a spline on Rn, right? So this method does not support stuff which is natural in the waste sense. Yes, and the other point is it depends on splines on the support space. So it depends on the fact that we have a spline on Rn, therefore we can match that. So these are the two. So, these are the two somehow drawbacks I would say the method has. However, it's very fast and it produces smooth curves. And on the experiments I will show you later, whenever the point clouds are uniform, like here, the method I'm proposing and their method basically looks the same. It's just because if the support space is Rn and it's uniform, it's basically the same, as you will see. Not exactly, but up to an error that is not visible. Okay, so. Okay, so these are the two methods I want to somehow interpolate between. And this is the proposed method, Wosenstein-Lane-Riesenfeld. So if you have not seen the Lane-Riesenfeld algorithm, this is one slide to explain it. Have you ever heard of this? Okay, so it's basically a numerical method to find B-spline. So it is an algorithm that depends on a degree, which is the degree of your B-spline, and on a refinement. Spine and on a refinement level, meaning the larger the refinement level, the you know, it's like an iterative scheme, so the smoother you will look like. And the point is, as the refinement goes to infinity, you will actually get the piston. Okay, so what does it do? This is an example, not on point clouds. Okay, this is an example on Rn or R2 in this case. So let's go back to let you know, forget about the point clouds. Let's just say we are in R2 and we have. And we have four points on a square, and they are ordered. Okay, what does the method do? It has two steps. First step is doubling points. So we just double the points. So in every point, you now have two. And then the next step is compute an average. So we're computing average between every points that we have, which in this case is just a linear midpoint. So you have two points, compute the midpoint, and you insert it. Since we have doubled the points, we will keep the corner points. Points, you will keep the corner points, right? Because the midpoint between two points at the same point is exactly the point. Okay, so this is one averaging step, and then you can do as many averaging steps as you want, depending on how, which degree of B-spline you want. By doing a second averaging step to get a degree two B-spline. If I do a third step, I will get a degree three. Okay, so another step of averaging. So just take the points that you had, forget about the original points, and you will. Original points, and you will end up with something like that. And this was one refinement. So, if you imagine you're doing this many times, you're basically just cutting off the corners and creating a smooth curve, which will be the B-spine. So, this is an iterative scheme that in the limit will produce a B-spine. And it depends on two things. One is the degree, how many averaging steps you do, and the other thing is the refinement level. How close do you want to be to the limit curve? Okay. Okay, um, I have a code to show you that is very simple to implement. And by the way, this has been around since the 1980s, so this is not a new idea. Just to mention that, you know, we're not dealing with something recent. So how does it work? Doubling step, averaging step. And if you think about instead of doing individual points, we now want to do point clouds. So on the corners, we have point clouds. So on the corners, we have point clouds or we have measures instead of actual points. We can do doubling because this just means store more of the same. And we can also do averaging. Instead of doing a linear midpoint, we're just going to do the geodesic midpoint. And that's the basic idea. Okay. So as I said, it's very simple. Maybe let me show you the algorithm first. The algorithm is exactly the same: double points, and then do as. Double points and then do as many midpoint averages as you want. And it will produce a V spline in the Wassofline space, if this is how you want to define it. And here you can see multiple sets of refinement. So you should read the time from left to right. This is how we, you know, this is the easiest way to plot it. They could also be on top of each other, right? But then we cannot see it. So time goes from left to right. And then in the first refinement step, we get. In the first refinement step, we get in-between point clouds. Second refinement step, we get even more point clouds in between. And as you know, take more refinement levels, you will actually see smooth curve connecting them. Okay, that's the idea. So it's actually very simple. Just take an algorithm that has been around before, a numerical scheme that produces B-span and replace the averaging step with something we're interested in. In this case, Wasselsland geometry. Geometry. Okay, I wrote a lot of things here. Yeah, so geodesic midpoint. And I think the main point I'm trying to make is intrinsic to the Bosser-Stein geometry because you take measures and the only thing we're doing is doing midpoints. So we're just staying on geodesic space. So it's not leaving anything, like it's not leaving the Wasserstein space. And we're also not, like in the Chewy setting, the algorithm from before, we don't rely on splines on our end. So if the support space is something else, it's So if the support space is something else, it still works. It doesn't have to be Rn. Okay, so double check that this is actually interesting. Let's look at the edge cases. So interesting in the sense of it's a generalization. So what is an edge case? Edge case number one is we have one point cloud as a starting point cloud and one point cloud as the end point cloud. Nothing in between. So this is the setting here. Starting point, end point cloud. point and point cloud we will uncover exactly the optimal transport so it is you know it is optimal transport in this setting where we just have two points if you only have one point per point cloud like the regular rn setting then we just uncover the spline on rn so it is actually a generalization of those two settings right so in the one point cloud in the point cloud setting with one point it's just the baseline we know It's just a baseline we know, and if we only have two point clouds, we just get optimotrans. So, it is exactly a generalization of those things. Okay, and here comes the thing that I think is the benefit of our method. So, this is the splitting, right? So, if you have in the beginning just one point, in the end, a couple of points, if you just did optimal transport, it would be a straight line that splits, right? But now I have. Right, but now I have put in something in between, which is another point cloud, and what you can see is that it's still splitting, and the trajectory spent towards the weights, right? So it's actually very natural. This is what you would expect from this display. And if I move the in-between points, the trajectories bend towards those, right? So you can see it here as well. So it's a very natural behavior. We have the splitting, and we can bend with respect to the weights. The weights. And maybe this doesn't look very nice, but it also shows that the method is natural to some extent. Here, the points in the middle, I change the weights. So one point is stronger than the other. Therefore, the trajectory should bend towards the stronger points. And this is what you see. So more trajectories go to the point with more mass. And if you move it down, they bend down, and more of them go to this point. So, you know, intuitively, I think this is something that does the right thing. Okay, so the point of this slide is we can deal with trajectory splitting and we can deal with non-uniform. Okay. Here are some examples. So these are like data sets we found from other papers, and we wanted to show we're doing well with respect to those. Those. So, this is the pedal data. The way it works is the point clouds start in the middle and then they evolve to the outside. So, time goes from inside to outside. This is our method. This is the other spline method that I mentioned before. And they do exactly the same if you look at it. And the reason is it's uniform mass and it's supported on RN. This is why it does the same. Why it does the same. Yeah, then this is a neural network method which needed a lot of training and then did not really capture the points. This is a more interesting example. So again, here the point clouds evolve from left to right and it's non-uniform. So this method, the other spline method cannot do it. So we compare to something else, which is called Wasser-Stein-Pischer-Rau. And if you look at it, you have some trajectory. Look at it, you have some trajectory intersection, which is very unnatural for optimal transport problem. However, if you use our method, you basically get what you would expect. You go to the middle and then you split. And here's another neural network method which somehow failed the point of the splitting. Okay, so these are visuals to convince you that the algorithm does something interesting. Of course, if you want to publish in a computer science journal, you need this. I don't want to go through the table, but I want to tell you there exists a table. And the point of the table is comparison of runtime. We are obviously slower than the other spline method, but it's not crazy slow. So you can still compute all of that. And it's much faster than neural networks because you need training. And the other comparison we did was leave one out kind of thing. So you have trajectories, you have many point clouds. Or you have many point clouds, and then you remove one point cloud, fit the trajectory, and see how close you were to the left-out point cloud. This is one of the testing cases. And in most cases, we do pretty well. So whenever it's bold, it means, you know, this might not won. And we are always the first row. So we have a couple of bold and a couple of underlined, which means we have the second place. So it's not always the best, but it competes very well with most methods. This is the point of the table. This is the point of them, of the table. And also, we compared to a lot of different data sets, right? And when you try to publish this, then the question is: but why didn't you do the other data set? So we actually have two more data sets, but it's not on this table yet. Okay. One of the obvious questions is, so the B splines are not interpolating, right? B splines just approximating. Just approximating. So, can you do interpolation? The answer is yes. So, the scheme I showed you is actually based on something more broad. It doesn't have to be Lane-Reasonfeldt. There's a whole set of algorithms which are called subdivision schemes. And you can just use whatever they do and replace the averaging step by optimal transport. And here is one example, which is called a four-point scheme, where we do interpolation of point clouds. So, here we have point clouds again with different mass. Point clouds again with different masses, which is the thing I'm trying to sell. And then the time involvement is around the circle. And then in this case, we can actually fit trajectories exactly. I don't want to advertise interpolation because especially in this trajectory interns problem, it might lead to strange behavior. So I think the B-splane stuff is actually more natural in that sense. But I want to say we can also do that if you want to. Okay. Okay, here's actually a theoretical guarantee, and this is actually the last thing I'm going to talk about, which says that if you do this scheme iteratively many times and the iteration level goes to infinity, we will get a curve and let me just put the theorem. Okay, we will get a curve in the limit and we have a convergence rate of how fast we're going to get. Convergence rate of how fast we're going to get to the point. So, this is basically the theoretical guarantee. So, how are we proving the convergence? Basically, the way you're doing it is you have your point clouds or your measures, and you fit piecewise geodesics. And then you refine the data, you fit more piecewise geodesics. You refine the data, you fit piecewise geodesics. So, you just have a sequence of functions, and then you show the sequence of function converges to a limit. That's basically how it works. That's basically how it works. And the reason we have this convergence rate is based on the Lane-Riesenfeld algorithm. So the proof is actually very simple. We know everything about Lane-Riesenfeld in the linear setting. We know that this is its convergence rate. The only thing we need to do is replace Rn with a metric space. And in our case, the metric space is the Wasselstand space. But the proof goes through in exactly the same way. So it's actually very nice. And yeah, in the end, we're showing that the sequence of piecewise geodesic. Piecewise geodesic interpolants is a Cauchy sequence in the Wasserstein space. And we get this rate. The reason is just because of Lane-Reasonfeld does the same thing. So it has nothing to do with the Wosserstein space. It just has to do with the fact we're using this algorithm and the Walserstein space is a matrix space. So yeah, very simple and surprisingly simple, considering the fact that the method does well. Okay, so that's all I want to say. That's all I want to say. I will just click through all of this so I don't have to do it. The summary is: we're presenting another method for trajectory inference, which I think has a lot of properties that we want. So one is the intrinsic property, we're not leaving the Wassostein space, the splitting property, the non-uniform property, the flexibility in the sense that you can choose how refined you want your curves. Refined, you want your curves to be. And the fact that we can easily extend it. So you don't have to do lane reason fail. This is just what we use to showcase what we're trying to do. But there's other methods we can do. Okay, so this is the paper. You can find it on archive. And yeah, I'm happy to answer questions. Thank you. My question is that here you only use the position of this point. Is there any equivalence if you have, for example, for one of one time step, you have position plus velocity. Uh position loss velocity but skipping, for example, the next one. Yes, so there is so the algorithms I just presented just depend on position, but if you have position and velocity, so I haven't done it, but I think it works. It's something which is called a mid-subdivision. So it basically just point and direction, and you can do the same kind of thing and that. And you can do the same kind of thing, and the trajectories will follow your velocity field basically. It helps that, for example, you need less time steps. You mean computationally? I think the thing it buys is that you can basically tell your trajectories what the right direction is. I'm not sure if you need less refinement for that, but at least it's geometrically meaningful in that sense. Geometrically meaningful in that sense. But yeah, I haven't done it for Wattenstein space, but I'm pretty sure you can do the same thing with emit teams. Naive question. So in the case where you get splitting trajectories, so I sort of saw one time variable that sort of takes you to your start to your end. Is there a variable that tells you how you sort of split the project? You know, how do you select which project that you're on? Maybe it's I'm not sure what you mean exactly. So, I'm never choosing anything. This comes out of the algorithm. Like, if you do optimal transport and you have one point in the beginning and five in the end, you will need five trajectories, right? So, it is automatic from optimal transport. Yeah, I get it automatically because I'm using the geodesic midpoint. So, it's basically all from optimal transport. I'm not choosing anything. Anything? Yeah. For having the velocities, I understand doing that in real space means that you have the position and velocities, and it helps, for example, to know the direction of the trajectory. But having velocities might give us this option of going to the phase space, in the six D space. In that case, Wasterstein flows or Gradient's flows are not valid anymore. We have Hamiltonian. It anymore, we have Hamiltonian. Is there any equivalence of having an OT map for Hamiltonian? This I'm not sure of. That's a good question. I'm not sure. I have to think about that. We can discuss later, but I'm not sure exactly how to do that. Thanks, that was a great one. Okay, thank you. Yeah, I know how to do it with positions, but I'm not sure how you do it if you go up to face-to-face. I was wondering what once you have your algorithm, that final trajectory is that going to be something? So it's not well defined what it is. So my recommendation would be to call the limit curve a B span in Bossus plan space because it follows from a Space because it follows from a B-splane algorithm, but there's no, you know, no official meaning, right? It is not a geodesic in general. However, if you sample all of your measures from a geodesic, right, you have a start point and an end point, and everything in between lies on a geodesic, you will get the geodesic back because we're not leaving it, right? So this property is called geodesic reproduction. You start on a geodesic, you will get it back. So the limit is geometrically meaningful in that setting. But if you leave the geodesic, you won't get it. But if you leave it at your desk, you won't get it. Cool. So if the data case where it is at your desk, you could find gradient flow gradient flow. Is there some sort of a case that you supposed to think about? I mean, you can think of this as a gradient flow if you want. This is not your original motivation of the work. Yeah, I think it really depends on what you want to do. Yeah, I think it really depends on what you want to do. So, this was really just about: can we infer something in the middle? If it is really a gradient flow, I'm not sure. But I think you can think of it as the gradient flow. Maybe this is just to help me understand. So, as you increase, like you split R time, right? You would then have to solve order of two to the R number of optical transfer problems, is that right? Number of aqual transfer problems, is that right? Or am I missing? Yes. So if you, yeah, this is what I meant with, you know, time. The more refinements you do and the smoother you want it, so higher degree, the more optimal transport you have to do. So it's two to the r, where r is the number of refinements. This is the computational bottleneck. Okay, so then my question is like um did you gain anything from the fact that that these are now Apple transfer properties? Yeah, I'm not sure. So I have encountered this problem a couple of times. So whenever things are very close, it seems to be faster, but I'm not sure why. So maybe somebody here knows. But yeah, so I think there's a gain if they're very close, but I don't know what the reason is. But yeah, it can be so. But yeah, it can be. So this is slow if your point clouds are large, right? So if they are like in the whatever, couple of thousands, it will be slow. So you should be aware of that if you want to use the method. If you really have large point clouds, you probably just want to do the linear interpolation. This is only meaningful or computationally reasonable if you're not in a crazy large setting. So this is all like interpolation. So let's say you have like a certain number of points here and then you want to scrap kind of relevant to like is there any kind of naturalizations of these points where like do you think viable I think yeah so I haven't done it we have thought about it a little bit I think we should there should be a way to do a little bit I think we should there should be a way to do a little at least short extrapolation, like short time, but I have not implemented it yet, so maybe ask me in two months. Yeah, I think there should be a way to do very short time extrapolation for the beast plans. 