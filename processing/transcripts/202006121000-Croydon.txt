Okay, so David will tell us about variant measures for KDV and TOTA type discrete interval systems. Okay, so well, thanks for the introduction and I guess everyone else for turning up. So yeah, I'll be talking about KDB and total type discrete integral systems. And so yeah, maybe these are a bit of a different flavor to some of the other models that you'll have been hearing about. That you'll have been hearing about from Ivan and other people this week. But I say, well, to begin with, don't panic because I'll be introducing all the models that I talk about sort of from the basics. So it doesn't really matter if you haven't met them before. And I'll also, at least two places, I think, try to connect them with things that you have met this week. And well, just at this point, though, I guess I'd highlight that these are a bit simpler than the That these are a bit simpler than the kind of stochastic integral systems that you've been seeing this week, in the sense that the models that I look at will be completely deterministic. So once we have our initial condition, then the dynamics are completely deterministic. Hopefully, though, I'll sort of be able to convince you that there's still enough interest in their behavior when we start these systems from random initial conditions to study these from. To study these from a probabilistic point of view. And so that's kind of part of the motivation of the talk, really. So yeah, and as Ivan said, so this is joint work. So all of it is joint work with Mahiko Sasada, who's, well, normally in Tokyo, but now in New York, and also Satoshi Sujimoto, who's in Kyoto with me here. With me here. Okay, so before I get to the discrete systems, I thought I'd just kind of remind people or introduce people about the sort of underlying continuous systems that these are meant to be related to. And so the first of these is the sort of famous classical KDV equation. And so this is this PDE that's meant to be a model for describing waves in. Our waves in shallow water. And so, in particular, it was introduced to study sort of the behavior of solitary waves, so solitons, in sort of narrow canals. So, here we have a picture of a weight being dropped. It creates this kind of this waveform that's preserved, its shape's preserved, and it moves at some constant speed off to the right here. So, this is the basic interest in the KD. Interest in the KDB equation. A priori, it's not really related or doesn't appear to be related. It's this total lattice equation, and so this is a model of a one-dimensional crystal. So here we have Pn representing the momentum of the nth particle, Qn representing the displacement from its equilibrium, and we have some coupled differential equations to describe their dynamics. And just as with the Just as with the KDV equation, this is a system that emits solitonic type behavior. So we see these kind of traveling waves as solutions to these equations. And a bit more than that, actually. So solitons, I guess the other kind of property that these have is that these behave something like particles, so that these are preserved by the dynamics. So for example, here we have a large soliton moving at some speed, overtaking a small soliton that's moving at a lower speed. At a lower speed, and there's some interaction here that kind of affects the trajectories. But after the interaction has happened, then the sort of waveforms are reconstructed and they move off sort of more or less independently. And similarly, we see these kind of soliton-type decompositions in Toda systems too. Yeah, and actually, I should say that actually there's a very nice paper just from last year by three authors, Killip. Three authors, Kilip, Murphy, and Vizam, maybe I could have put them here, who use this kind of solitonic decomposition of solutions to KDB to show invariance of white noise for this equation. So this has been earlier showed on the Taurus, but they used it to show the invariance of sort of the whole line white noise as a solution for KDB. So that was kind of a nice recent result in this area. In this area. So, yeah, so these are the sort of continuous models that underlie everything I talk about. The discrete models I'll start by introducing are going to, well, the simplest one that I'm going to talk about is the so-called Box-Bohr system. And I'll talk about other systems later, but in the sense, if I go quickly and you don't kind of pick up the details of those, that doesn't matter. You can kind of keep this in mind as a sort of Matter, you can kind of keep this in mind as a sort of canonical example of most of the things I talk about later. And so, this is a simple particle system. We just have one-dimensional configurations. And so we have, as per the name here, we have boxes and balls. So the boxes are just the integers. And balls just sit in the boxes. And so we write e to x is 0 if we have an empty box, e to x is 1 if we have a ball in the box. One, if we have a ball in the box, and to begin with, let's just think about a finite number of balls in the picture just to make things easy to begin with. And so, the evolution happens, this is described here. So, we go along the configuration until we reach the sort of leftmost ball, which of course is possible in the case of a finite number of balls. And we move this to the first space available on its right. So, we just move along. On its right, so we just move along this little red line from two to three here. We then look along to the next left ball and move that to the first empty available space, and so on. And so, when we get to this ball at position five here, we can't put it in seven because that was already taken, so we have to move it all the way down to nine, and so on. And so, one time step of the dynamics is just has happened when we've moved all of the balls exactly once. So, in this case, we move according to all of these coloured lines, and we end up. According to all of these coloured lines, and we end up seeing this time one configuration down here. So, hopefully, that's kind of reasonably clear to understand. And maybe less clear to understand is this formula over here, which kind of describes that a bit more formally. So, in particular, to see a ball, so to have a configuration after time one, having a ball, so being equal to one, we need to see an empty space in the previous. Space in the previous configuration. And then this infinite sum here just represents the fact that we're carrying at least one ball into that site. So we have at least one line, one of these kind of coloured lines that passes into that ball. So we've sort of seen more balls than we've put down. So that's the role of this infinite sum in here. And of course, in the case of finite number of balls, we can sort of. We can sort of start this recursion here by assuming that we just have kind of initial condition to the left of the particles being zero, so we see no particles after one step of the dynamics to the left of where we originally had particles. So yeah, being probabilists, of course, when we see sort of zeros and ones, we want to make these all random and ask about what happens in this case. And in particular, a natural kind of question is. Particular, a natural kind of question is what kind of measures are going to be invariant for these dynamics. And of course, you can immediately see if we're going to want invariant measures here, so the process is somewhat transient in the sense that each particle is moving at least one step to its right. We're going to need to kind of keep on adding particles into the system to sort of preserve the mass of the system in any kind of finite region. And so we're going to need to think about infinite numbers of particles. About infinite numbers of particles in our system. And so that's the sort of first question: how do we extend the dynamics to that case? And well, to this end, we have a slightly different description of the dynamics. So this is equivalent in the case of a finite number of particles. But it turns out to be useful to add this kind of extra auxiliary process. And so this entire picture. And so, this entire picture here represents again just one time step of the dynamics. And what we're thinking about is we've got this little carrier, or maybe a truck or a lorry, coming along. When he sees a particle, he picks it up. And when he sees an empty space, he puts it down and so on. So as we pass these three particles here, he picks them all up and then puts them down according to the empty spaces he sees and so on. And after he's passed all the way from the left to the right here, then we All the way from the left to the right here, then we see the new configuration. And so, again, we can write this down mathematically. This is the description of the carrier evolution from left to right, where un is just the number of balls being carried from sort of n to n plus 1 here. And it's kind of easy to see if you think about it for a moment or two, then actually this carrier process is exactly the number of arches. Number of arches, arcs that cross over the relevant interval in this picture here, and it actually represents the same thing as this infinite sum that we had here in this finite ball case. And so that's why we can write down this dynamics in this way when we have this carrier variable. And so, in a sense, we haven't really done anything here apart from changed the language a little bit in the finite case. And our problem has become whether we can. Our problem has become whether we can define for a given configuration this carrier process. And so, this is a natural first question. And the answer is in general, no, we can't do that. And even if we can do that, in general, we can't do that uniquely. And so, this is a first kind of problem of this system. But, nonetheless, if we're thinking about invariant measures, you can hope that if we can do this in some sensible way, then we will be able to iterate this and continue on for all time. Have to iterate this and continue on for all time. I mean, that somehow the configuration after one time step will be suitably nice to enable us to do this again, and we can fill out some kind of lattice of variables here. So we have some configuration at some time. We construct some carrier that kind of corresponds to this configuration, and we use that to define the new configuration. And it's kind of easy to see that, again, somehow if we can define all these variables. If we can define all these variables, then we can write down the dynamics in some sense locally, in the sense that at any particular space-time point, if we know the particles that were there originally and the amount of balls brought by the carrier, then we can just compute the amount of balls taken away by the carrier and the amount of balls left at that location. And this is the formula for doing that. It's not that enlightening to look at the particular form, but that's exactly what this is describing according to. This is describing according to the dynamics I just described. And so this kind of system of solution, equations that I've written down is known as the ultra-discrete KDV equation for reasons that I'll say in just a second or two. And I'd also just like to comment that this is just one version of the ultra-discrete KDV equation. So there's other kinds of particle systems that you can imagine. You could, so the one and the infinity subscripts here refer to the fact that I only allow one. Here refer to the fact that I only allow one ball in each box and I allow the carrier to carry an unbounded number of particles across any location. And it's not very difficult to imagine ways in which you can generalize this to allowing, for example, J balls in a box or restricting the carrier to holding a maximum capacity of K, for example. And we can write down the corresponding equations in these cases as well. And we'll be able to deal with these generalizations in a similar way to. In a similar way to what we do for the simplest model. Yeah, and so here's the questions that I want to talk about today. So, first of all, can we actually solve these equations? So, what kind of conditions on the initial configuration do we need to be able to actually find uniquely solutions to this system of equations? So, that's the first question that we've got here. Question that we've got here, and so I'll answer that both for this BBS system but also for other related systems. And then I'm going to talk about the sort of the simplest kind of invariant measures, and this is in the case of when we just ask for the initial condition to have a product measure, so namely be IID, what measures of these are invariant for the dynamics. And of course, in the simplest BBS case, the only possibility is. EBS case, the only possibility is just some Bernoulli marginals here. But in the more sophisticated models where we allow variations in the box size and so on, then we'll be able to see a richer class of invariant measures that I'll talk about. Yeah, a couple of things that maybe I just wanted to note, but I'm not going to have time to talk about today. About today are put down here as well. So, there's been a couple of other nice developments in this area related to this kind of solitonic type picture. So, this traveling wave type decomposition of solutions that you can see. So, similar to the continuous KDB equation, we can see solitons in the BBS. So, in this case, these are just strings of consecutive ones or zeros. So, here you can see this string of length three, it's just progressing. String of length three, it's just progressing forwards when it's in isolation at speed three, and this one here is moving forwards at speed one, and you can see that these are interacting and then recovering after the interaction. So it's very much like the solitons we see in the continuous model. And yeah, there's been a very nice paper recently of Ferrari, Pablo Ferrari, Ngohen, Rola, and Wang, where they use a soliton decomposition like this to give a very rich class. To give a very rich class of invariant measures for the system. So, this is just to really highlight that the kind of invariant measures I talk about today are a very special sort of subclass of these invariant measures. And yeah, this soliton decomposition is a very useful thing for other kind of properties. So, another direction in which we've been looking is hydrodynamic limits. And so, this soliton decomposition here turns out to be useful for that as well. But, like I say, I'm not going to. As well, but like I say, I'm not going to talk about these things today. But if you're of course interested, I'm happy to talk to people another time. So, what I am going to talk about, though, is a whole collection of related systems. And so, here's kind of just a picture to show where they all fit together. So, in a sense, you don't really need to learn what all of these systems are today. It's just maybe helpful to know that there's a whole collection of these systems that sort of sit between the box four systems. That sort of sit between the box ball system and the KDV and the tower system up here. And so, just very briefly, so this is the Kdv equation that we just saw. We have this lattice structure that looks like this, and we have some, well, this is the more general version where we allow variations in the J and the K here. And I should say, also, more generally, so, of course, for the box-ball system, it makes sense to talk about integer variables, but more generally, that's a restriction you. Generally, that's a restriction you don't need. You could look at this system for R-valued variables, both in terms of the parameters here and in terms of the actual configuration and carrier variables here. So yeah, there's no particular problem with dealing with these kind of generalizations in the same way. And there's also kind of multi-coloured versions which correspond to higher-dimensional versions of the systems, which are kind of interesting. But again, just for time, I won't. Just for time, I won't talk about the details of those. So, this is the system we already saw. So, between that and the KDB equation, we have this system here, which again, I'm not going to say too many details about, but this is a system that arises from some natural discretization of the KDB equation. And actually, from this system, we can, by some suitable continuous limit, get back to the K dB equation. And so, this is really quite closely connected. So, this is really quite closely connected to the original continuous model, and it's also connected to the ultra-discrete model in the sense that if we take some ultra-discrete limit, which is maybe for people in statistical mechanics more naturally called a zero temperature limit, if we just make some suitable exponential change of variables and let epsilon go to zero, then we can recover this ultra-discrete KDV equation that I wrote down. Discrete KDV equation that I wrote down here, and so that's the sort of connection between the continuous model and the sort of DVS that I introduced. And so that's the sort of KDV type systems. On the other hand, I mentioned the Toda equation. So here's the connection between the systems. And probably the easiest way to see the connection is by the BPS, the Box Bore system. And so, yeah, to just And so, yeah, to describe this, what we can do is think about the configurations of the BBS, not in terms of the zeros and ones, but in terms of the lengths of the consecutive zeros and ones here. So Q would represent the sort of lengths of the consecutive balls, and E's the consecutive zeros here, or empty boxes. And so I've got kind of three blacks, two whites, and so on. So I just write these down as kind of these integer lengths here. And this is going to be my new configuration. Be my new configuration, something that looks like this. And it turns out that we can again describe the dynamics just in terms of some lattice. So now I've got sort of two inputs for the configuration plus the carrier. And I get sort of two inputs or two outputs plus the carrier out here. So I get kind of a different lattice structure. But again, I can describe the sort of evolution of the system via this kind of local set of equations. And so this is the ultra. Set of equations. And so this is the ultra-discrete Toda equation. I also have kind of a positive temperature version of this. So this is the discrete Toda equation. And again, this sits naturally between the Toda continuous version by a continuous limit and the ultra-discrete equation by taking kind of a zero temperature limit. And so this kind of ties together all of the pieces on the tota side and the KDV side here. And I should say also there's connections kind of Also, there's connections kind of horizontally in this picture as well. So, at each of the continuous, the discrete and the ultra-discrete levels, there's kind of ways of connecting the two systems together. And so, yeah, it kind of makes sense to group them together as a class of systems to try and study in one kind of common fashion. And so, that's why I'm going to talk about these four particular systems today. Yeah, and just as a kind of connection, my first small connection, maybe small. Small connection, maybe small, to the other kind of talks that you'll be hearing about hearing from today. So, just last year, there was a very nice work by Jeremy Castell Daniel Reminik who connected the KPZ fixed point equation to the Kp equation. And actually, the Kp equation is a two-dimensional version of the KDV equation, and there's all sorts of connections between the Kp equation and the systems that I've written down here. Systems that I've written down here. So, for example, both the discrete KDP and TODA equations are just or can be obtained from natural discretizations of the Kp equation. And so, yeah, maybe it's kind of a larger grounder point. I mean, it kind of seems like maybe it's kind of a general topic of interest, maybe to start thinking about how these discrete integral systems or deterministic integral systems that I talk about. Integrable systems that I talk about, whether there's any kind of particular deeper connection with the kind of stochastic integral systems that you are meeting in the other talks over the rest of this week. And I'll give one very small connection later on in the talk, but yeah, I don't pretend that this is giving anything, any particular deep insight into why everything sort of ties back up to the Kp equation as they do. So Yeah, so that's kind of the models. Actually, before I go on, I just wonder how much time do I have? I don't know what time I started. So you started about 20 minutes ago, so you have about another 20 minutes. Actually, that's fine. That's kind of roughly what I was hoping. So hopefully I managed to say everything I want to say. So yeah, so the goal of the rest of this talk, so we've now introduced the models, so we've got these lattice systems. The models, so we've got these lattice systems, and what I want to do is explain two things basically. So, first of all, a general method for solving these lattice equations, and then secondly, a general method for identifying IID type invariant measures for these systems. So, those are the two main topics I've got left, and really these are in somewhat independent directions, I should say. But of course, at the end, they do. But of course, at the end, they tie together to prove the invariance of the measures that I want to prove. And so, yeah, so the first part of this mission is to try and solve the equations. And so, this is a deterministic exercise, actually. And so, everything I'm going to say for the next 10 minutes or so is going to be completely deterministic. And it's going to describe a general method for how. Describe a general method for how to solve these equations based on a certain way of looking at configurations in terms of a certain path encoding. And again, this is simplest to describe for the basic box-ball system, and so I'll start by doing that, and then I'll try and at least hint at how we can do a similar thing for the more complicated systems that I wrote down. And so here's essentially the entire story for the Story for the box bore system. So let's recall we have these configurations. So again, just to begin with, let's think about finite configuration for simplicity. And I'm going to write this, encode this in terms of a random walk path. So I just start at zero, and I'm going to jump down one each time I see a particle and jump up one when I see an empty space. And so I just can encode this configuration in this black line here, this. Line here, this random walk path, not this walk path, I guess. And what I want to tell you is how we can see in this picture as well, quite straightforwardly, a way to describe a carrier process and also a way to describe the dynamics of the process. And so, for the first of these, I'm going to start by introducing the past maximum process. So, this is this red line here. So, this is just the past. This red line here, so this is just the past maximum of the path s. And well, because I'm assuming I've got finite configuration, maybe I've got no particles on the left here, so I just have a vertical line, so my path maximum is just coming up sort of linearly on the left there. And so it follows this red line. And I'm going to define a process to be the difference between this and S. So this is this blue line shown here, and it turns out that again. It turns out that, again, it's a reasonably simple exercise to check that this exactly corresponds to the load that's carried by the carrier at each particular location. You can roughly see that in this picture. So I come with the carrier empty. When I see particles, I go up and down and so on. And the time when we have to worry is when we get back to the maximum. If we jump up, then this is when the carrier, of course, is empty and it remains empty. And so we replace this kind of jump here with this flat part in the past. This flat part in the path. And so it's kind of a reasonably easy exercise to check that this corresponds to a carrier process. And I should say, maybe this is well, that's another good time to make another point about, so I talked about the uniqueness of solutions. And so you can see actually I could have equally have written down a process that satisfies all of the equations for the carrier if I'd started the past maximum at any fixed level. So let's suppose that I just assumed. So let's suppose that I just assumed that the past maximum at like minus infinity was given by one or something, and so I just redrew my line, red line along here, and I defined a new process u to be this new maximum minus s, then this would solve all of the equations of the carrier. And so I could use that to define the dynamics just as I did before from the equation. And whichever carrier I choose, the dynamics. Whichever carrier I choose, the dynamics of the system is then going to be described by adding, reflecting in this choice of past maximum that I choose. And so, in particular, this is just the reflection in the past maximum. So, probably the most natural choice, and the one, well, it is the most natural choice, as I'll justify in a minute, is the real pass maximum. This is going to be the choice that we make. If we reflect in this line, this gives us a new random walk path. And so, again, the down jumps correspond to. So, again, the down jumps correspond to particles, and it turns out that this new configuration exactly corresponds to the BBS dynamics here. And so these are the carriers, so why do I choose this one? So somehow if I choose a different version of the pass maximum in the way that I just described, so actually what that would mean in terms of the dynamics is that the carrier would be sort of coming down linearly from infinity. Coming down linearly from infinity. And so it would mean sort of bringing in some infinite number of particles from minus infinity. And so, I mean, for physical reasons, you can think that that's not a very sensible choice. But actually, from purely mathematical reasons as well, as I'll describe in a slide or two, it also doesn't really become a very good choice after one time step of the dynamics. Because actually, for example, if we start with paths that go off to minus infinity down here, after one step of the dynamics, Infinity down here, after one step of the dynamics, then we're outside of the domain of the system, of course, because this kind of reflects up to something that doesn't have a past maximum. And so this is a bad choice kind of mathematically as well if you want to define a continuous, a continued evolution of the process. And so that's why we rule it out mathematically. Okay, so just as a comment, so I would think many problems have seen this before, so this. Probably have seen this before. So, this reflection in the past maximum, of course, was not new to this study. So, this is well known as Pittman's transformation. So, Pittman used it to connect, well, to show how you could construct a three-dimensional Bessel process from a one-sided Brownian motion. So, this was in 1975, and there's been many variations of this study, and it's kind of appeared in many, many different places. So, yeah, it's kind of a very natural transformation. It's kind of a very natural transformation in the area of stochastic processes. In a sense, it's a little bit magical why it appears if I just write it down like this, but actually, given the way that I've defined the path and the carrier here, actually, it turns out that this Pittman transformation formula down here is exactly just equivalent to this formula here. And all this formula is doing is representing some kind of conservation. Is doing is representing some kind of conservation of mass. So, somehow, this thing on the right is the number of particles that we had at the space-time location before the carrier came. This is how many particles the carrier brought to that site. And then on the left-hand side here, I've got the particles left at the site and the particles taken away. So we just have a conservation of mass. And so that's really what's giving us this Pittman transformation in this system. So, yeah, because I don't have too much time, I'm not going to tell you any of the details for any of the other. To tell you any of the details for any of the other systems, but we can do similar kinds of things. So, if we define the path encoding suitably, we can also write down suitable versions of the past maximum process. So, these are either in the ultra-discrete cases, things that look exactly like past maximums. So, here we've just got a two-step average, but it's still a pulse maximum. In the positive temperature version, so the discrete systems, I've got sort of I've got sort of the natural or a natural sort of positive temperature version of a supremum in some sense. So the log integral exponential transformation. And of course, under the suitable ultra-discretization, sort of zero temperature limit of the path here, then we can recover again this operation from the discrete one here. And so these are sort of naturally connected just as you might expect. Yeah, I should say, I mean, there's no parameters appearing here. Say, I mean, there's no parameters appearing here. So, the parameters J and alpha in the discrete KDB and ultra-discrete KDB, so these are appearing in the path encoding. And actually, these transformations here are just for the case when we're at K is equal to infinity or beta is equal to zero in these models. So, at least in the ultra-discrete case, we worked out the story, and with a more novel version of a pass maximum, we can also Pass maximum, we can also write down a sort of Pittman type transformation in the general case as well. But yeah, it's a little bit fiddly to write down, which is why I don't do it here. And another just minor comment is we need, as well as the Pittman type transformation, we need a shift in the total ones just to represent the shift in the lattice variables that we saw earlier. And so, well, here's some pictures. The exponential ones are just a bit smoother, I guess, than the Just a bit smoother, I guess, than the ultra-discrete ones, as you again might expect. Yeah, and well, I don't have too much time, so I would go kind of very quickly on this slide, but this is essentially the general argument that we use. So essentially, what we want to do in each of the systems, we've got some configuration, some carrier. We want to change coordinates in some sense so that we have a conservation law of this form. So the a minus 2b is a convenient. It's a convenient conserved quantity to use to get the Pittman transformation in the end. And if we then use this kind of change of variables to define our path encoding, then the only remaining problem is to compute our appropriate path maximum. And so this boils down to solving this recursive equation. And so of course this is the same level of difficulty as finding the carrier process originally. And so I don't claim that we've done anything apart from change notation. Anything apart from change notation, but the couple of nice advantages are that A, we can actually solve this equation in examples. So, namely, and then we get these various operators here. But a bit more than that is you can see, so if we reflect in this past maximum of Hitman, if we start with some function that's basically asymptotically linear, so you have like SN over n converges to some limit, then this is a property that's converged, preserved under Under this reflection in the past maximum. And so, this kind of path encoding picture gives us a very convenient way to check what kind of sets are preserved by the dynamics, and in particular, allows us to give a suitably useful for our purposes class of solutions of initial conditions for which we can find unique solutions to the initial value problem. And so, this is an important part of the story for us. Part of the story for us, and so well, this is just a story for the BBS. Maybe it's too much notation, but the point is: like I say, if we start with some initial condition that's got kind of a positive gradient, so this would be the case, for example, if your particles were just IID with density less than a half, then our path encoding would satisfy this kind of condition. Actually, just an ergodic initial condition would be enough for this. Then we take our initial path encoding, we do We take our initial path encoding, we do our Pittman transformation, and that gives us our configuration and carrier variables across the lattice. And so that's the general story. And we can do essentially similar things for the other models where we replace the past maximum and the path encoding by appropriate things. Okay, and so here's just a little picture about what these kind of things look like when we iterate the Pittman. We iterate the Pittman transformation, and this I think is with ID particles here. And yeah, I don't have too much time, but maybe videos are kind of nice things to see. So here's, oh, my video's gone a bit slow, but let's refresh this page. My video is dying a bit, so maybe it's not very nice to see. Here we go. Yeah, so here you see the kind of this is just showing a slight. Here you see the kind of this is just showing a slightly larger scale simulation. So you kind of see the solitonic behavior, the larger solitons kind of moving through the system quite quickly. This is actually an invariant initial condition, some Bernoulli IOD condition with parameter less than a half. So this is the BBS, and here's just for fun, the corresponding discrete KDB version. So you can see similar kind of behavior, but the system looks a little bit smoother in terms of the Looks a little bit smoother in terms of the kind of configurations that you're seeing. Okay, so yeah, so I guess I've got about seven minutes left, so I should probably go on to the last part of my talk. So this is about the invariant measures part of the talk. And so, again, very briefly, so what I talk about is just a small part of the work that's been done in this area in recent years, and maybe in some sense, the simplest part of the work. So there's been other Part of the work. So there's been other nice work, particularly this kind of soliton type decomposition approach to studying variant measures, which is, of course, a very natural way to do things. I can also say that by just looking at the stochastic processes I defined, so this carrier and this path encoding, we can also use kind of stochastic sort of natural versions of reversibility for these processes to study invariance of. To study invariance of configurations. And these kinds of arguments extend beyond the BBS to all of the systems I studied. So, this is another convenient viewpoint. But today, I want to just say a little bit about what we did about invariance based on the lattice equation type picture. And so, here's again a sort of simplification really of our main result, but it sort of does the job for the BBS and. Job for the BBS and the variations of that we studied. And so essentially, what you might hope is that what we're asking for is if we have configurations that are IID, what we want to know is which of these are going to be invariant under the dynamics. Okay, so which IID configurations satisfy this property here. And so this is a global invariant. And so, this is a global invariance condition. And it's natural to ask how much we can tell from local invariance of the system. So, namely, if we given a measure mu down here, if we can find a measure mu such that this product is invariant under the relevant dynamics, is this enough to give us the invariance of the whole system? And it turns out that essentially the answer is yes, as long as these measures are supported on suitably nice sets. On suitably nice sets, where we can solve the initial value problem. And so, this is the additional condition here. And so, this is the non-local part of the theorem. And actually, this is, in a sense, the harder part of the theorem to check, but that we already checked now using all of these path encoding type stories, which is why I spent so long talking about that. And once we've got that, then all we do is we have reduced this problem to this kind of simple inverted set of lattice equations. So, we just now need to solve. Lattice equation, so we just now need to solve the problem locally to determine our invariant measures. And so, that was kind of our general approach. And this turns out to work for all of the systems that I introduced so far. So, just to describe the KDB results very quickly, so in this case, we can solve these equations in the ultra-discrete case. So, we just have up to trivial measures, and yeah, I should say we solve them. We're a bit lazy because we assume either a density or a suitable lattice. A suitable lattice support, we can show exponential geometric conditions, invariant measures for these systems. So that's for any parameters. In the discrete KDB case, we can only solve the problem when one of these parameters is zero. And so these, again, it's not that informative to tell you what these are, but these are generalized inverse Gaussians and inverse gamma distributions. Distributions. And so the more interesting thing to tell you is why we know that these are the right distributions in these cases. And so this links us to another very nice classical area of probability. And so this is related to certain characterization theorems of various types of random variable and based on sort of independence preservation under certain transformations. And so perhaps the most classical of these. And so, perhaps the most classical of these is this theorem due to Katz from 1939. So, what he showed, for example, was that if you have two independent random variables, you transform them to x plus y, x minus y, then this output is independent if and only if these variables are normal. And so this is a characterization of, I guess, coordinates being invariant of some kind of rotation. That preservation of invariance of the coordinates under this rotation is unique to the normal. Is unique to the normal distribution, the bivariate normal distribution. And since then, there's been various other kinds of characterizations of distributions of this form where we start with independent random variables, we have some map, the output remains independent, so this of course looks a bit like this picture down here, and linking this kind of invariance of independence under given maps to particular distributions. To particular distributions. And so, in the discrete KDB case, the kind of relevant theorem we could kind of read straight off the books is this one here by Litakin Veselovsky, which is the positive random variables that says if we make the transformation output given by these two expressions here, then exactly this tells us we must have a generalized inverse Gaussian and a gamma distribution as the original marginals. And so, again, this is quite a classic. Again, this is quite a classical result that appeared in studies of exponential versions of Pittman's transformations and other places. Yeah, and actually as part of what we did, so I said we could only solve the discrete KDV equation when the beta is equal to zero, and that's because we only have the path encoding in this case. We can actually solve the detailed balance equation. It's easy to check that this is a solution of the detailed balance equation much more generally. Much more generally, and maybe a problem if you don't care about anything else apart from nice classical problems in probability is to check that this mapping, this discrete KDV mapping, is actually a characterization of the generalized inverse Gaussian. And so, namely, the only solutions to this equation are given by these forms here. And then, in that case, well, that would be kind of a nice result to prove. And actually, I should say that in the ultra-discrete case, essentially, we prove that. Essentially, we prove that for the corresponding ultra-discrete version of this map in our work up to technicalities. So, yeah, and actually in our work from other discrete integral systems, we notice and can make other kind of conjectures about these kind of characterization theorems. The one final thing I wanted to say, I realize I'm kind of taking a bit of a liberty, is just a couple of words about. Just a couple of words about related stochastic integral systems. And so recently, there's kind of a paper that pulled together various results in this direction and characterized all of the invariant measures of at least a class of polymer models. And so the basic sort of structure of their model was the following. So we have, again, a lattice system, but now the maps are random. So the sort of input can be, the randomness of the map can be represented by an additional random parameter here. Random parameter here. And so the natural invariance locally invariance condition locally looks something like this. And well, to solve this equation in this paper, what they did was they rewrote this map. It's a map with sort of three inputs and three outputs. So we put this random input down here. And we can express this new map in terms of the map R star and R star inverse. And it turns out that this invariance here is equivalent to Is equivalent to an invariance of product measures under these individual maps here. And so this was used to characterize various invariant measures for these stochastic integral systems. And yeah, I just wanted to say in the last minute or so here that essentially we can do the same thing for these TODA systems. So we can split the maps up in the same way. And actually, it turns out that at least in a couple of examples, That, at least in a couple of examples, the maps that you see exactly correspond in both the tota cases and a couple of these stochastic integrable systems. So, namely, the directed last passage percolation corresponds to the ultra-discrete toda, and the directed polymer model with sight weights corresponds to the discrete tota map. And actually, we also notice other connections with a couple of other stochastic interval systems and can approach them via similar arguments. So, maybe just as a final comment, I just And maybe, just as a final comment, I just, yeah, maybe this kind of motivates a little bit more systematically kind of thinking about which discrete integral systems sort of match up with stochastic integral systems via these kind of transformations and exactly kind of how we can use these kind of connections to connect invariant measures and other properties of these systems, I think, is an interesting future direction of research. But yeah, like I said, I'm sorry for taking a liberty and overrunning my time. I will end there. Okay, thank you. That okay, thank you. Thank you very much, David. Uh, let's all thank the very next talk. And well, you know, even though you did go a little over, I'd like to see if there's some questions. It's a very interesting subject. If anyone has something they want to put into the chat or they can unmute themselves. 