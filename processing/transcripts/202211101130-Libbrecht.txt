Success? Okay, good. Yeah, great. Now, of course, to handle this, the main thing we need is new antibiotics. But as Leon had talked about, that's challenging to acquire for many reasons. So, in the meantime, we need to use the antibiotics we have. To use the antibiotics we have better. And one way to do that is by being able to identify resistance in bacterial infections that we see. So Leonid showed you this figure. And Lena talked about we can do AST, antimicrobial susceptibility testing. That is, isolate a bacterial sample from an infection. From an infection, tests to see which antimicrobial drugs it's resistant to. But that's slow and expensive, and it doesn't tell you anything, doesn't tell you anything about the mechanisms of resistance. Now, in principle, we should be able to tell whether a bacterium is resistant. Bacterium is resistant to antibiotics based on its genome, because that's where resistance is encoded in most cases. As Leona talked about, there are some exceptions to that. Now, one way to do that is just to look for known resistance-causing genetic variants. Unfortunately, although we know some resistance-causing genetic variants, those known variants explain only a small fraction of. Explain only a small fraction of the resistance we see. So, what we need to do is identify more broadly resistance. And a great way to do that is using machine learning. So what I'm going to talk about today is some work we've been doing trying to use machine learning to predict drug resistance in bacterial genomes. All the work I'm going to Genomes. All the work I'm going to talk about today is focused on microbacterium tuberculosis, which is the bacterium responsible for the disease tuberculosis. But we've done a little bit of work as well on other bacterial species, although I'm not going to have time to talk about that today. The main thing I want to tell you about is some work we've been doing using deep learning to Using deep learning to get very accurate predictors. And that's what I'm going to start with. But at the end, hopefully if we have time, I want to briefly talk about some other work we've done. One, building a interpretable predictor of drug resistance. And I'll tell you what I mean by that when I get to it. And two, to deal with a problem that. Problem that exists, which is bacterial samples that include multiple strains of bacteria. I'll talk a little bit more about that when I get to it. But again, I'm going to start with this model using deep learning. We're calling predictive model in the sense that the main thing we want out of it is just to know, given a genome, whether or not it's predictive. It's predictive. So, very briefly, the pipeline goes like this: we get a bacterial sample, not our group, but we, in general, sequence its genome. We get its genome sequence from that, and do AST to determine whether it's resistant or susceptible to a number of antimicrobial drugs. That goes into a drug resistance sequence database, like the ones that Leonid told you about. Like the ones that Leonid told you about, where we have many sequences and various drugs, either resistant, susceptible, or it's worth noting that we haven't tested every drug against every genetic sequence. So some of these entries in this matrix are unknown. Okay, so this is what the data looks like. Again, we have zeros and ones for each drug. Uh, for each drug, is it resistant or susceptible? And then I didn't show this in this figure, but a bunch of these are question marks. We're using here two databases of MTB sequences and associated resistance, reseq TB, and Patrick. This work predates the great work Leonid's been doing assembling additional data sets. So, this data set could be expanded now. So, in total, we have just under 8,000 bacterial isolates that we have sequenced. Now, I'm hoping to give you in this learning problem both some strategies that one can use to do this specific problem effectively and also some. Effectively, and also some things we've learned about learning problems for this type of task in general. So, perhaps even if you're watching this and you're not necessarily interested in this specific problem of drug resistance predictions, hopefully there's some takeaway lessons for you. And I think this is the big one, which is that when learning a machine learning model, there's a trade-off between expressiveness. Trade-off between expressiveness and generalizability. That is to say, you might want a very expressive model. That is, one that can pick up any association between genetic variants and disease. For example, it might know that only in the case where you have exactly these 10 genetic variants, but not these other five. Not these other five, that gives you resistance, even though if you change just one of those, you would be susceptible. For example, a very expressive model could learn some very complex dependencies like that. The problem is that more expressiveness usually means less generalizability. That is, if I train a very expressive model on Model on a small training set, those very complex rules I learned aren't going to generalize to some new data, which of course is the whole point. The whole point is to be able to predict on new sequences whether or not their resistance are susceptible. And of course, like an extremely expressive model would be one that just memorizes for every possible genome sequence whether or not that's resistant. Genome sequence, whether or not that's resistant or susceptible, that certainly can capture any pattern that relates sequence to drug resistance, but just memorizing the input data isn't a very generalizable method. So a good rule of thumb is that you want around as many parameters in the machine learning model as you have. As you have training examples in the training set. Okay, so I showed you our training set. It has around 10 to the four examples and around 10 drugs. I'm surrounding everything to powers of 10 here for simplicity. So it's around, depending on how you want to think about it, it's around 10 to the five examples we have for training. Now, Now, our genetic variation data consists of estimated single nucleotide polymorphisms. Those are genetic variants that have one base pair switching, for example, from an A to a T or a C to a G and so on. We have around 500,000 of those SNPs. Of those SNPs. On average, we have 50 per gene and we have 10 to the four 10,000 genes. Okay, so the simplest thing you might do is, and people do this, is a GWAS, a genome-wide association study. That's really a one-parameter model where you just look at every single step and ask, is it correlated with drug resistance or not? And this is a great thing to do for a kind of simple data set like this. For a kind of simple data set like this, but it's not really a predictive model. But I just wanted to have that on that as a comparison. Okay, so I'm going to focus on this line here. So if we were to take a logistic regression model, which is a simple machine learning model that just has one parameter per SNP, in a sense that's associated with how important that SNP is to determining. To determining resistance. So a higher value of the parameter means that that SNP probably causes resistance. We have around 10 to the six SNPs. So that logistic regression model is going to have 10 to the six parameters. When I said we only have 10 to the five training examples, so even this very simple machine learning model, logistic regression, that's not very expressive. Very expressive, you're already going to have a hard time learning that model. And we came into this, we don't want just a very simple associations between genetic variants and resistance, we would like to be able to learn more nuanced rules. And probably we're not going to be able to do that given that we already have more. We already have more SNPs than we have training samples. Okay, but the real insight came here that SNPs don't act in isolation. Resistance is caused by changes to genes for the most part. That is, if you get a variation in a gene, that's going to cause some change to that protein, which is then probably going to go on to cause resistance. So let's say we. So let's say we do something simple, and just instead of asking whether each individual SNP is correlated with resistance, let's represent our genome sequences according to which genes are mutated. Okay, so here's what that looks like. So here's our genome sequence and the And the gray squares are genetic variants, and these blue intervals are genes. So, we're going to say we're not going to remember where the individual genetic variants are. We're just going to remember for each gene how many genetic variants it have relative to the reference. You can call these this measurement, just how many genetic variants you have, a gene burden measurement. Have a gene burden measurement that is how burdened is that gene. Now, this is great because now we have one feature that is one input to the machine learning model per gene rather than per SNP. So, and we get about two orders of magnitude reduction. Now we have fewer features than we have sampled, and now we get to do some interesting machine learning. We can afford to. Interesting machine learning, we can afford to because now we have enough training needed to do that. Okay, so I'm going to skip through this pretty quickly. This is the machine learning model we used. The machine learning model we used is a neural network, specifically it's a recurrent neural network. So a recurrent neural network is one that looks like this. It walks across the sequence and does some computation over each position with an input and an output. This makes a lot of sense. This makes a lot of sense for genome sequences because genome sequences are arrayed linearly. And I'm going to talk a little more later about why it helps specifically. Okay, and then if you really want to see the whole neural network, here it is. We've got some convolutional layers. I'm not going to talk about what those are. Some LSTM layers. That's the recurrent structure I showed you just a minute ago. And then a dense layer. That's where each neuron is connected to each other neuron. Neuron is connected to each other neuron. But really, what's going on here is we just have an input, which is a vector of length almost 4,000, where each variable in that vector indicates the number of genes in gene I. And we have 12 outputs, one for each of the 12 drugs, saying, Are you resisting? Drugs, saying, Are you resistant or susceptible to that drug? And then this is just learning a big function that maps sequence as represented by gene burden features to resistant or susceptible to those 12 drugs. Okay, so we trained this using back propagation, which is the standard way to train neural network. And then we tried to use it to predict test setting. So I'll show you our results. Here's kind of the standard. Here's kind of the standard way one might evaluate accuracy. But I'm going to skip through this because I think this is the wrong way to do it. But you have to do this because this is the standard way. So we're using an ROC curve. It's a curve that looks like this, where you have the rate of false positives on the horizontal axis and true positives on the vertical axis. You want the curve to go up. Axis, you want the curve to go up and to the right. So, a good way to measure accuracy, or at least a very general purpose way to measure accuracy, is you measure just the area under this curve. That's what I'm showing you on this plot on the vertical axis. And you can say for this drug on the left, my red, that's our method, the LRCN, it's doing just under 0.9 area under the RFC curve. Okay, so here's a comparison of a bunch of methods that we tried. Methods that we tried. The red is our method, the LRCN. And the other colors are a bunch of existing methods that one might otherwise have used to do this task. Some of them are kind of baseline methods that are the first thing you would think to do with machine learning, like a random forest or logistic regression. And some of these are previously described predictors for drug resistance and tuberculosis. And you can see generally the Tuberculosis. And you can see generally the red is above all the others. I'm connecting with the lines just to make this plot easier to read. We have error bars here having to do with a measure of kind of the variation as a function of the training set. You can see that, especially for some of these drugs where we don't have very many measurements, we can't actually tell which method is doing the best. So you can see for this one, the error. The error bars of the teal and ours, the red, overlap. We don't know which one is better. Same thing goes here. On the horizontal axis, there's a star. If we can tell with statistical significance that the red is above the others. And you can see it's only that case for some of them, but it is the case for most of them. Here's a related measure: the area under the precision recall curve. I'm not going to tell you much about this other than the red is above the others. But I actually don't think. But I actually don't think this is a useful measure because a drug resistance predictor has is very important how many true positives you get versus true negatives versus false positives. So, really, the false positives versus false negatives are critical. And only some of your predictions are actionable. Are actionable. And in fact, what we want to be able to do ultimately is for a clinician to say, okay, I've sequenced your bacterial sequence of your infection, and I'm going to use that to make some health decision. So how do we do that? I think we asked around, I'm not a clinician, but we asked around what people thought was the level of confidence you need to make. Of confidence you need to make a clinically actionable prediction, and the consensus was the following: the standard thing to do is assume that a infection is sensitive. So, if I give you a prediction that it's resistant, it has to be a very confident prediction. And the threshold for confidence that was the consensus was 95% specificity. 5% specificity. That is, if I give you a sample that is truly susceptible, it can only be a case that 5% of the time you tell me that it's in fact resistant. And we can do this using, we can make a curve over what threshold we apply to the machine learning model. The model gives us a score, and we can put whatever threshold we want on that score. We choose that threshold such that we hit 95%. Such that we hit 95% specificity, and then ask: given that we only get to make predictions at that confidence, how many of the truly resistant isolates did we detect? So that's called the recall. So this measure is recall at 95% specificity. How many can we detect very confidently? And here is the result. You can see that some of the methods just hit zero. It's very possible that. It's very possible that if a predictor is not good enough, you just simply cannot make any predictions at sufficiently high competence. And if you use one of these other predictors, for many cases, you just wouldn't be able to make any predictions. They're clinically useless. But at least for some of the drugs, most of the predictors, including ours, the LRCN, the red one, do make. Red one do make some clinically useful predictors. You can see the recall is not super high. It's highest for rifampicin, which is one of the first line drugs. So that makes sense. Meaning we're able to detect about 80% of the truly resistant samples with extremely high competence. You can see for some of the other drugs, we can make, we detect very few. And for a lot of them, it's a Very few, and for a lot of them, it's around you know, somewhere around 50 percent. So, it's not great, it's certainly one would hope that it would be better, but it's nice that a predictor like this can make even some clinically actionable predictions. Okay, so now I just want to show also that this recurrent structure we have, why is that useful? Why is that useful? The innovation we have is that we're taking into account the order of genes in the genome. And just to make sure, like, does this, is the order helping? The answer is yes. We tried shuffling the order. That's these other colored lines. The red line, the original, is at the top. I'm showing you in gray just all the other lines I've showed you in the past plots, just for context. And yeah, if you shuffle the gene orders, you do, if you shuffle the gene order, you do really. You shuffle the gene order, you do really badly. Another question you might have is: okay, what if you use just the initial SNP data instead of these gene burden features? Yeah, that doesn't do, that doesn't work either. These other color lines, the teal through the pink, are if you apply these existing models using the SNP, the full SNP. SNP, the full SNP data rather than the gene burden features, it turns out that you don't do very well, which is not a surprise because of that problem I told you earlier, where you end up with a model that's too expressive, essentially. So you're going to overfit on your test set. Or really, what's going to happen, we have a validation set where we set various parameters. Those parameters, and those are parameters that influence the That influence the expressiveness of the model. You're going to have to pick such constrictive values for those parameters such that you're not going to be able to get a very good model. You have to choose those very constrictive values because otherwise you're going to overfit. You're not going to do well on the testing. Okay, just for the sake of time, I'm going to. Just for the sake of time, I'm going to skip over this and just briefly say that it turns out it's the arrangement of genes into operons that drives the performance of that sequential model. That should be no surprise. It turns out that in bacterial genomes, genes are assembled into operons. These operons contain genes that usually have related functions and often get transcribed at the same time. So the fact Same time. So the fact that there is this arrangement necessitates using a model that walks over the sequence of the genome. And we did some testing that I'm not going to skip over on this slide to do that. Okay, so that was all I wanted to talk about for the predictive model. I'm almost out of time here, so I'm going to skip over this interpretable model and just tell you about one. Model and just tell you about one other piece of work we've done, which is on deconvolving mixed samples. So it turns out that it's often the case that an infection can have multiple strains of the same bacterium. And that can cause a problem for all sorts of reasons for an informatics pipeline like this. One, if you try to get a single If you try to get a single genome sequence out of it, it's probably going to be wrong in various ways. And two, if you try to treat one of those bacterial samples, perhaps the other one is resistant to the drug you're using for treatment. So we developed a method that can detect and deconvolve those mixed infections. And the way it works is like this. If we take the genome and look at all Take the genome and look at all the sequencing reads we get for, for example, from position I, we'll get some alleles that match, that have one letter and some alleles that have the other letter. So we call that the allele frequency of one versus the other. And for a mixed infection, you expect to see allele frequencies that look something like this. Let's say it's an infection. This. Let's say it's an infection with 30% of one strain, 70% of the other. There's going to be some noise, of course. They're not all going to be exactly 30 and 70. Not all of the loci are going to have exactly those allele frequencies, but you're going to have two peaks where some of them have frequency around 30, for example, around 30% of A's, and some with around 70%, for example, 70% of T's. Or I guess it would be reversed in the figure I showed you. Or, I guess it would be reversed in the figure I showed you. You can model this using a probabilistic model that I've depicted here, but I'm not going to explain. And it turns out that once you learn that model, you can use that to detect mixed infections. You can put a probability on, and really a p-value on mixed infection versus not mixed infection, and you can deconvolve those two genome sequences. Genome sequences. Very importantly, unlike with, for example, a human sequence where the maternal and paternal sequences are present at equal proportions, so you can't tell which allele is from which without some very clever assembly. As long as the mixed infection isn't at exactly the same proportion, it's not exactly 50-50. You can, in fact, phase and assemble. And assemble, in most cases, the entire sequence. That is, you take all of the alleles with 70% frequency in this case and put them into one genome sequence and all the ones with 30% and put them in the other genome sequence. Okay, and here's the paper that talks about this. I didn't have time to go into it in detail, but I hope you'll take a look. Okay, with that, I'd just like to again thank the organizers for. Just like to again thank the organizers for inviting me here and thank my group. Most of the work I've told you about was from a great master student, Amira Sang Safari, and that last work I told you about on deconvolution is from Einar Gabasov. Here are all the people who worked on the research I showed here, and thanks to the funders. And thanks to everyone for your attention. Awesome. Thank you, Max. So I think there's a question in the chat. We're at time and we're kind of into the break, but I think that's all right. If there's a couple of questions for Max or for John, we can take those. So Max, do you see the question in the chat there? Yeah, I'll read it out. Thanks for the talk and specifically for the great instruction of the trade-off between expressionism and generalizability. There's a line of research in deep learning that suggests over. Line of research in deep learning that suggests over-parameterization in deep neural networks increases expressiveness while still maintaining or even improving generalizability as measured by performance on test data. This is called the double dip or double descent, I think is the name of this approach. Any thoughts about this benign overfitting in the context of your data? Yeah, so this kind of double-dip phenomenon, it happens in very It happens in very specific circumstances. And kind of the hypothesis is that my understanding is this occurs when you're relying on the learning process of the neural network to, in a sense, perform some kind of regularization, where something about doing gradient descent has a kind of regularizing effect, meaning that it Effect, meaning that it encourages the model towards specific, you know, to make specific assumptions that turn out to hold in practice. We've looked at this a little bit. You know, we've asked if we, it greatly increased the number of parameters. You know, does that help? And in this case, the answer was no, it didn't help. I think it's my guess is that it happens more in these cases where you have, you know, not. Have, you know, not 10 to the four examples, but probably more like you have 10 to the 10 examples, but 10 to the 15 parameters or something. That's maybe when you start seeing those cases. Or it could be that we need a better architecture that has that regularizing effect. We don't know, but we haven't seen it in our data. Thanks. Oh, yeah. Oh, yeah, go ahead. There's another question in the chat from John, which I see there. A cool idea with burden test plus recurrent neural networks: would this still work when gene order varies greatly across the population, e.g. for E. coli? We've asked this. We've done some work on E. coli. Funny enough, the recurrent network work does give a bit of improvement. I don't know why, because I think the order we have in our reference, which is the order we're using for the neural network, is. Order we're using for the neural network is probably not the actual order we that exists in the strain we're looking at. So I can so the answer is it shouldn't, but sometimes it does. I don't know why. But good question. I've got a quick question too. So there was an earlier talk on an earlier day, and it's escaping me who gave the talk, but about burden. The talk, but about burden, you know, burden tests that go where you can have opposing effects. And I think it's often a reasonable expectation that, you know, most mutations break the gene. So you can assume that they all go in the same direction to break the gene. But have you, is that, first to clarify that, is your assumption here? And have you tried, do you think it helps at all to kind of have these more complex models where the burden can go in opposing directions? Go in opposing directions. Yes, I thought that talk was very interesting. Apologies to the speaker. I also forget the name of the speaker. So, yes, we're making this assumption that in some sense, all variants go in the same direction. They're doing the same thing to the gene, and that they're equally, equally, that they have equal effect. Each one gives you a plus one in the feature value, which is a terrible assumption. Surely it's wrong. So, I think a better model. Better model would be something that has sort of a more nuanced idea of genetic variants. For example, it's encouraged just to use the number of variants in a gene to avoid being too expressive, but that when there's very strong evidence in the training set, can say, no, it's this specific variant that causes resistance in the gene. That causes resistance in the sheet, not any of those other ones. Or it's, you know, most variants will do it, but not this one that sometimes happens. So, you know, maybe you need more data to do that. I think probably, even with the data we have, we could probably train a better model that kind of that could kind of get the bust of both worlds, that usually uses gene burden, but sometimes can dig down all the way into the specific variants. That would be neat future work. That would be neat future work. Yeah, cool. Thank you. Any other questions on Zoom or in the room for either Max or John or a general discussion? Other thoughts and questions from anybody? I had a quick question and maybe sort of comment for. Maybe, sort of, comment for John. You know, one of the points that you touched on that I thought was pretty interesting is the often less than good concordance between studies. If you train on one data set and then test on another, and you do not so well. But it seems like what you mentioned is a lot of that is actually due to data quality. It's not necessarily. Not necessarily because there's different genetic determinants and sampling problems and things like that. It could just be to you know actual base calls and stuff. So I think that's you know really interesting. And I guess you seem to allude to that Unitigs can solve this. And I largely agree with you. But do you think, I mean, are there cases, like, does that break down if you just have, I don't know, like below a certain sequencing depth, you don't get a good, you actually do get unit tags that are dropping out. You actually do get Unitigs that are dropping out of a data set versus another. Or, I mean, yeah, so maybe you can comment on, you know, do Unites solve that problem, or do you think that it's a little bit more complicated? Yeah, I mean, I think they go a long way to sort of solving the problem in quotes with SNPs, which just are really difficult to call jointly and consistently. Just finding sequence elements is fundamentally easier to do. Maybe in Maybe in lower quality data or messagenomes, they might not end up in the assembly. So perhaps they would still be there in the reads, a bit slower to query. But some problems with Unitigs are, for example, if you had two SNPs next door to each other, that would break up the Unitigs into lots of small pieces, which would then be at lower frequency. So if you actually did the SNP as a Lower frequencies. So, if you actually did the SNP association, you would be more power to find the results. But it breaks the unitigs into many smaller k-mers and many smaller units. So, you actually lose power in that case. So, that's an example of somewhere where they don't work as well. I mean, one thing you do have to be careful of in the calling as well is that if you just build unitigs in a graph, they will be different between cohorts. You need to be a bit careful with the sort of construction and calling, but it is possible. And calling, but it is possible to sort of make them consistent. But you still end up with between study differences, even if you kind of solve that genotyping problem and make the genotyping consistent. But that was something that we found worked. Yeah, yeah, cool. Another question in the chat for John from Leonid about how replicable is Amina for different pathogen? Can it be turned into a recipe? So do you, which So which part do you mean specifically, Lena? Do you mean the training of the model? Do you mean the construction of the website or the whole thing? I guess I meant the whole thing, but so, well, not so much the construction of the website, because I think that part is, yeah, sorry, I meant the technical part. So basically, let's get a bunch of pathogens, isolates, analyze them together, and them together and get the phenotypes and then train the model and get a predictor out that basically ends up only looking at a handful of the unitegs and then we can have the same type of approach that applies to a different bug let's say you know e coli since since you were you were asking max about e coli yeah i i i'm not sure i mean it it's relative Not sure. I mean, it's relatively easy to do. It's not a complicated model, and we kind of have the bits of software that make it easy enough. And what Marie did, I think, was sort of quite well written and replicable. I mean, I was actually surprised how well it worked in the NUMO case. And I think part of that came down to the training set being good and quite big, and two different cohorts. I think getting something as high quality as that would be. Getting something as high quality as that would be one difficulty. I mean, in Pneumo, it seemed to be working quite well across sort of a range of different mechanisms, sort of SNPs in the chromosome, well, they're all in the chromosome, but SNPs and gene insertions. So, yeah, I'm not sure. In our first paper where we tried this, it did work less well in other species and other phenotypes as that paper and bio. That paper in bio, you can look at to see quite how badly it did in other phenotypes. But this was, I think, actually the best example we had, and I think it's probably down to curation of the training set. All right, great. Any other questions for either speaker? All right, well, All right. Well, if not, I'd like to thank all the speakers from this session, which was, I think, a nicely coherent session, if I may say so, on AMR. I think the talks all nested and complemented with each other nicely. So thanks, everyone. We'll take a break until 1:30 local time. 