And so, just the basic problem formulation I've written here in the first yellow box. We want to minimize, say, some measure of risk of some uncertain objective function f, which depends on what I'll call the simulation or state variable u, plus say a cost on the deterministic optimization or decision variable z. So these could be controls or decisions. Z. So these could be controls or design variables. And then the constraint in this optimization problem is I'll refer to as a simulation constraint. This is what encodes the physics of the problem, for example. And so given a controller design Z and some realization of the uncertain parameter Xi, we could potentially solve this equation for U through a simulation. Through a simulation, a large, say, large computational simulation. And so, if we can do this, if the simulation constraint has a unique solution, we could solve this equation E equals zero for each z or for as for u and define that as an implicit function of z and xi and plug that into our objective function to arrive at the lower optimization problem, which is an optimization problem. Which is an optimization problem only in the decision variables, the designs or controls. And so, a common choice for this functional R, which I glossed over here, R is some functional mapping a set of random variables into the real numbers. A common choice, as we saw in the previous talk, is to set it to the expected value. And so, how do these optimization problems compare to? Compared to, say, traditional stochastic programming problems in terms of numerical solution. Well, if we consider the first problem, which is sometimes called the full space problem, where we keep the simulation constraint explicitly as a constraint, and we use the sample average approximation to discretize the expectation, we arrive at the following nonlinear optimization problem on the left here. And you'll notice that. Here. And you'll notice that to solve this problem, we have to maintain the state variables, these simulation variables as explicit optimization variables. And so for each scenario, we have to store UN. And then for many numerical methods, we also have to store the associated Lagrange multipliers. So this results on the order of m times n storage, where n is the number of samples and m is the size. And m is the size of the vector un, and for large applications, this could be in the billions of variables. So, in this case, memory is the limiting factor for numerical solution. On the other hand, if we consider the reduced formulation where we solve the nonlinear constraint, the simulation constraint for each scenario and plug in the implicit function to our objective function, we notice that to evaluate the function. We noticed that to evaluate the objective function, we now need to solve the simulation constraint for each scenario. And furthermore, to compute the derivatives of the objective function, the gradients and for and if we're doing second order optimization, Hessian applied to a vector. To evaluate those, we have to solve additional linearized equations for each scenario. So the limiting factor here is computation. In particular, we require Computation. In particular, we require order n nonlinear solves per iteration and order n linearized solves if we're using derivative information. So both of these formulations are limited, one by memory, one by computation. And so in this talk, we'll focus on the reduced formulation and see if we can do better than naively solving these problems, especially in the case of risk-reverse optimization. In the case of risk-reverse optimization. So, how do we define risk? There's a number of ways to quantify risk in our optimization problem, but to and these the choice of risk quantification is strongly dependent on the application. One could choose, for example, what I'm calling here optimistic formulation, such as minimizing on average or minimizing the expectation. Minimizing on average, or minimizing the expected value of our objective function. Or we could think of a reliability approach where we minimize the probability that our objective function exceeds some threshold. However, in this talk, I'll focus more on what I'm calling conservative formulations, which include risk-averse optimization. So a basic concept of risk would be to minimize the expected value plus some measure of deviation. Plus, some measure of deviation, or minimizing, say, buffered probability, which was introduced by Johannes Roysett and Terry Rockefeller in 2010. And we'll choose the conservative formulation based on risk measures and buffet probability. Just to set some notation or some terminology, I'll refer to a risk measure as a functional, an extended real-valued functional acting on some set. Acting on some set of random variables here denoted by x. And I'll assume that risk, the risk of a constant is constant. So this is a basic assumption that says deterministic quantities are riskless. And so some examples of this would be the expectation, a mean plus some deviation measure. So here this is the pth order deviation, or the conditional value at risk. Or the conditional value at risk, which I've listed here in the middle of the slide. The conditional value of risk, roughly speaking, is the average of the one minus beta largest scenarios where beta is some confidence level between zero and one. So in the figures here at the bottom, the left figure is a probability density function. To compute the conditional valued risk, we'd first compute the upper beta quantile of this distribution and then integrate. And then integrate the probability density function above that quantile. And that integral, that average, is the conditional value of risk. The buffered probability is closely related to the conditional value at risk. In particular, one could think of it as the inverse of the conditional value at risk. So if we're given a threshold little x, we can solve the equation c bar beta of capital x equals lowercase x. capital X equals lowercase X for beta for the confidence level and then define the buffered probability as one minus beta okay so as Sasha Mafosalov and Stan Yaryasov showed the buffered probability can be rewritten in a convenient as a one-dimensional optimization problem which I've listed here at the bottom similar to the conditional by risk which again has a form Has a form, can be computed by solving a different one-dimensional optimization problem. And so these optimization formulations are rather nice when formulating and solving optimization problems because we can incorporate these auxiliary variables t as additional optimization variables. And then the key challenge in using the conditional value risk and buffer probability will be to deal with these positive part functions or hinge losses. Are hinge losses max zero and x that show up in the objective functions for C VAR and BPOE. One particularly popular class of risk measures are the coherent measures of risk, which satisfy four axioms that I've listed here. They're convex, they're monotonic in the sense that if I have two random variables, one being larger than the other one, Homer, surely then the risk of the larger one. Shirley, then the risk of the large one is larger than the risk of the smaller one. Translation equivariance is if I add some deterministic quantity to my random variable X and compute the risk, this has the effect of shifting the risk by the same deterministic quantity. So again, deterministic quantities are riskless. And the final one is positive homogeneity. So if I scale my random quantity by some positive. Random quantity by some positive constant t and compute the risk that has the same effect as scaling the risk by that quantity. So, in many applications, this could say refer to a change of units, right? So, if I go from cents to dollars, the units on my risk change by that same factor. Some examples of coherent measures of risk we've seen already: the expectation and the conditional value risk. Expectation and the conditional value at risk. Some popular risk measures that are not coherent. The mean plus deviation, which is very popular in engineering, violates the monotonicity condition. And the entropic risk measure, which is also a very popular risk measure, violates positive homogeneity. So the coherent measures of risk satisfy these four axioms, and these four axioms give us a lot of very nice. And these four axioms give us a lot of very nice properties for the risk measures. In particular, if we view these risk measures from a convex analysis perspective, we see that the convexity of our risk measure, along with, say, let's assume it's proper and lower semi-continuous. This allows us to write the risk measure as being equal to its second fential conjugate. Potential conjugate. Furthermore, the translation equivariance and monotonicity of the risk measure ensure that the effective domain of R star, so this is the set of theta in the definition of R star up here, the set of theta for which R star is finite. So this set, the effective domain of R star, is a subset of probability density functions. So random variables theta that integrate to one. Theta that integrate to one and are greater than or equal to zero almost surely. And then finally, positive homogeneity ensures that the fentral conjugate of the risk measure R is in fact an indicator function for its effective domain. So what this means is that the fentral conjugate is zero on its effective domain and infinite outside of the domain. So this gives us the well-known biconjugate representation of risk measures as worth. Of risk measures as worst-case expectations. So, in some sense, coherent risk measures encode some distributional robustness, as we saw in the previous talk. And so, in some literature, we refer to, and through the remainder of this talk, we'll refer to the effective domain of our star as the risk envelope, and then any maximizer in the by this biconjugate representation here as a risk identifier. Presentation here as a risk identifier. So, and I plotted a risk identifier here on the right for the conditional value risk. The red dots correspond to one over one minus beta and the blue dot. Risk identifier highlights bad regions in the parameter space, if you will, regions that result in large objective function samples. Okay, so this is a nice formulation for risk coherent risk measures as worst-case expectations, but as you could probably imagine, to evaluate a risk measure, we have to solve, we essentially solve a maximization problem. So we probably don't have differentiability of the risk measure. And in fact, we have the following result. If we have a coherent risk measure, then it's Frochet differentiable if and only if it is the X. It is the expectation of some probability density function theta times x for all x. So essentially, the only Frochet differentiable coherent risk measure is the risk neutral or the expectation risk measure. So what are the consequences of this? Well, if you recall from earlier in the talk, solving simulation constraints, stochastic optimization problems are quite Problems are quite challenging for a number of reasons. And since we're focusing on the reduced formulation, the primary reason is that evaluating the objective function and its gradients are very expensive. And so now that we're considering, say, risk-averse simulation-constrained optimization problems, non-smoothness plays a huge role. In particular, algorithms for non-smooth and non-convex optimization problems are typically sublinearly convergent. Typically sublinearly convergent. So they're rather slowly converging, meaning we'll have to evaluate the objective function a large number of times. And so this could be extremely computationally prohibitive. And so here's just a simple example. It's a small non-convex and non-smooth example. We're minimizing the conditional value at risk of an objective function subject to a constraint through the Berger's equation. Bergers equation. Berger's equation. Berger's equation is a one-dimensional non-linear differential equation. And so we applied a bundle method to solve this non-convex example. And it required on the order of 100 million nonlinear and 100 million linear solves to solve the problem. So this is, if we were considering a much larger example, a more realistic application, this would be far too many solves to be. Be far too many solves to be practical. So we tried just to smooth the positive part function and the definition of the conditional value at risk and apply a Newton type method. And this already reduced the number of nonlinear PDE solves by two orders of magnitude. So there's some benefit to using second-order optimization algorithms and smooth risk measures. So what this suggests is, as I said, Real application problems are currently intractable to solve using traditional non-smooth optimization approaches. Our example suggests that we need either better non-smooth optimization algorithms or differentiability of the risk measure. And just to make these these solving these problems practical, we need to exploit adaptive or variable fidelity. Exploit adaptive or variable fidelity approximations whenever we can and only refine those approximations as we iterate through our optimization. So the first topic that I'll cover in terms of smoothing is our epiregularized risk measures. And the reason I'm introducing these is because they play an important role in the primal dual algorithm that I'll introduce later in the talk. So epi-regularized risk measures. Regularized risk measures are, we start with our risk measure and we essentially define the regularized risk measure by adding a regularizer phi to this and computing the informal convolution between R and phi, as shown in this gray box in the center. So this has a number of nice effects. One is that it's an approximation of the original risk measure, and the error that we commit is order epsilon. That we commit is order epsilon, where epsilon is this regularization parameter here. Another nice effect is that we don't lose a lot of the nice properties of coherent risk measures. For example, we're still convex, we're still translation, equivariant, and monotonic. However, something has to give, since we're smoothing, we're going to lose positive homogeneity. We additionally can prove differentiability of these regularized risk measures. Of these regularized risk measures. Under certain assumptions on the regularizer, we get Hadamard differentiability, and even stronger assumptions, we can get continuously Frochet differentiable. So here's a little example on how to apply epiregularization. So if we're in the setting of, say, expected disutility, since we're minimizing, so V is some scalar disutility function. Some scalar disutility function, and our we define our risk measure as the expectation of v. And if we define our regularizer phi to be the expectation of some scalar function, little phi, and so here little phi could be, say, x squared, for example. Then using decomposability of the space x, x was assumed to be an LP space, so it's decomposable. We can apply results from Results from Rockefeller and Wetz's book on variational analysis to pass the infimum inside the expectation. And in doing so, we see that we get the expectation of an info, the infimal convolution of V with phi. So this is somewhat simpler than the original infimal convolution that we had because this is done in scale in the real numbers, and we can typically Numbers, and we can typically evaluate this informal convolution explicitly. So here's a simple example. If v is the positive part of 0 and x, and phi is 1 half x squared plus x, then we get some, roughly speaking, a Huber regularization of the positive part function, which I plotted here on the lower right. Okay, moving on to the prime. Okay, moving on to the primal dual algorithm, we'll start with a general problem formulation related to the one we first introduced early in the talk. The reason I've switched notation here is so that we can handle these auxiliary optimization variables that show up in the buffered probability and the conditional valued risk. And so G, lowercase g will be our deterministic objective function, capital G. Objective function. Capital G will be our random or stochastic objective function. And psi will be a functional that maps random variables into the real numbers. And we'll assume that psi is real value, convex, monotonic, and positively homogeneous. We'll further assume that this problem, that a level set of this problem is non-empty and bounded, which ensures the existence of solutions. The existence of solutions. But under these assumptions that I've listed here, we have that, in particular the assumptions on psi, we have that psi is continuous and sub-differentiable. We also have that psi is, as we saw with the coherent risk measures, can be written as a worst case expectation. Since we don't include translation equivariance in the definition of psi, we have that the set that we're maximizing over for this worst-case expectation. For this worst-case expectation, is a subset of the non-negative random variables theta. And so here, one thing I'd like to point out is that the set of random variables x, we'll assume to be just L2. And this is important for our method. So, in any case, if we use this worst case expectation formulation for psi and plug this into our optimization problem. Into our optimization problem, we see that we get a min-max problem where this L functional, the little L here that I've defined, looks a lot like the Lagrangian functional from nonlinear programming. And so this is the observation that we'll use to define our primal dual algorithm. In particular, we define a generalized augmented Lagrangian functional. Lagrangian functional in the following way, here denoted by capital L. In particular, we take our maximization problem over theta and A, and we subtract off from the ejective function a strongly concave penalty. And so this is a classic approach to defining the automatic Lagrangian, and it works equally well in this application. In this application, so the objective function in this maximization problem is strongly concave and quadratic, and we can, in fact, solve this optimization problem for theta and then plug that theta in to arrive at our generalized augmented Magron gene. So, this is closely related to epiregularization, as I mentioned earlier. In particular, this essentially is the epi-regularization. Essentially, it is the epiregularization of psi with the functional that I've denoted here by capital Phi, which is given by expectation lambda y plus one half expectation y squared. As a result, we know that the error that we commit associated with this approximation is order one over R. Moreover, we have continuous differentiability. We have continuous differentiability of our Augment Lagrangian functional as long as lowercase and capital G are differentiable, continuously differentiable. So by using the generalized Augment Lagrangian functional, we're approximating the original optimization problem. And our approximation quality is 1 over R, just to recap, and we have continuous differentiability. So, I'm calling this the generalized augmented Lagrangian functional, and it might not be so obvious from the formulation that I had on the previous slide. However, by solving for theta and plugging the solution into the objective function, we can rearrange terms to arrive at the following form, where the first three terms look a lot like the traditional Agnomagrangian. And then the last term maybe is not so familiar. Here in the last term, we have the Familiar here in the last term, we have the project P sub A denotes the projection onto the set A and I D denotes the identity operator. So, if we move away from stochastic optimization for a second and consider nonlinear optimization problems, if G of W represents some equality constraint, G W equals zero, then the functional psi would be the indicator of the singleton set with. Singleton set with the single element zero. And in this case, the set A would be the entire space X. So under these assumptions, the final term in our Augmented Lagrangian disappears and we get the traditional equality constraint augmented Lagrangian. Similarly, if G of W were to be an inequality constraint, like G of W less than or equal to zero, then psi would be the indicator of the not. Indicator of the non-positive random variables. And our set A would be the set of all non-negative random variables. And so this is the traditional Lagrange multiplier space for these constraints. And we see plugging these definitions into our Augment-Lagrangian, we get the traditional Augman-Lagrangian equality constraint optimization. So there's a very close tie between our generalized Augment-Lagrangian as well as the Augman-Lagrangian from nonlinear optimization. Granging from nonlinear optimization. So, using that as motivation, we can define our primal dual risk minimization algorithm as an augmented Lagrangian algorithm, where our first step, we approximately minimize the augmented Lagrangian for the W variables, where our dual variables lambda k and our penalty parameter rk are fixed. We then update our Lagrange multipliers or our dual variables through the Dual variables through the classic projection formulation I've written here. And then we decide whether or not to increase the penalty parameters rk plus one. And so there's a number of ways to make this algorithm concrete. If w is a Hilbert space, for example, then we could stop based on the typical projection, projected gradient criterion that I've listed here. And then we could take a page from the augment. Take a page from the Augment Lagrangian book and update penalty parameters in the usual way. So, what can we say about this algorithm? First, if the first step in our algorithm returns epsilon minimizers in the following sense, the sense that I've written here in the first equation. And if our epsilons, if we have some sequence of epsilons that converge to some epsilon star. Converge to some epsilon star, which is possibly zero, but in general is non-negative. And if our penalty parameters rk are sent to some r star, which could be infinity, but in general doesn't have to be, then the sequence of iterates we generate by our parameter dual algorithm satisfy the following condition. Any weak accumulation point of the sequence of iterates will be an epsilon minimizer of the original optimization problem. With the original optimization problem, where the epsilon here depends on epsilon star and r star, and the Lipschitz constant of psi. So you'll notice that if epsilon star is zero and r star is infinity, then our epsilon expression here is going to be zero. And so any weak accumulation point will be a minimizer of the original problem. We can also analyze the dual variables in the setting of. Variables in the setting of epsilon minimizers, if the epsilon satisfies the following conditions considering the rate at which they go to zero, then we have that the entire sequence of dual variables, lambda k, converges weakly to a maximize of the dual problem. And this result is essentially a result from the proximal point algorithm. From the proximal point algorithm applied to the dual problem. So, those first two results are quite nice. However, they require epsilon minimizers, which is typically impossible to guarantee for non-convex problems. So, in the case where we have non-convexity and we can only guarantee that we have some notion of stationary point from our sub-problem solves, we have the following results. So, if we define stationarity, Through the first equation here, this is essentially that minus the gradient of the Ottoman-Lagrangian is in the epsilon normal cone for our feasible set WA. And if we assume that epsilon k goes to zero and rk goes to infinity, then we have any weak accumulation point of our sequence of iterates is in fact a stationary point for our original optimization problem. So, this last equation here is that the negative gradients, the typical optimality criterions, the negative gradient of our objective function is in the normal cone associated with our feasible set at the point W start. So, just to recap, under these conditions, if we're in the non-convex case and we can produce approximate stationary points. Stationary points, then our algorithm will converge to a stationary point, at least along a sub-sequence. So, to see the application of this, we'll first consider the positive part function. So, psi here is just the expectation of max and zero in capital X, which covers a large number of risk measures, the conditional value risk, the buffered probability, as we've seen, as well as mean plus semi-deviations and other risk measures. And so, here, Measures. And so here the set A is the set of all theta and L2 that's that are pointwise bounded between zero and one. So in this, for this choice of psi, we can equivalently reformulate the optimization problem as a smooth problem by adding slack variables. And I've done this in the middle of the slide here. And so if we apply augmented Lagrangian to solve this optimization. Solve this optimization problem by penalizing the equality constraint, we arrive at a bound constraint optimization problem, which we can solve explicitly for eta and s. In doing that, interestingly enough, we arrive at our algorithm. So essentially, our primal dual algorithm, at least for this choice of risk measure, is equivalent to applying Augman Lagrangian to the smooth reformulation. A second application are the more general coherent risk measures, which I've included the axioms again here on this slide. For this class of problem, for this class of risk measures, one particular challenge typically arises, and that is the projection onto the feasible set or the dual feasible set D here, this risk envelope. This risk envelope is typically not easy to compute. And so, one idea to simplify the computation is to decompose the risk envelope D into two sets A and the set of all theta that integrate to one. And if the set A is easy, if the projection under the set A is easier to compute, then we can use that in the following way. In particular, Way in particular, we can write our risk measure by adding an exact penalty on this constraint expectation theta equals one. We can write our risk measure as a max min problem. We're minimizing over t essentially the Lagrange multipliers associated with this constraint expectation theta equals one. Under certain regularity conditions, we can interchange the max and the min, the infant and the superior, to arrive at the following representation. To arrive at the following representation. And then we note that if we define this worst-case expectation over theta and A as psi, we arrive at a formulation that looks a lot like the conditional value at risk. So using this, we can include t as an optimization variable in our optimization problem and then apply our primal dual algorithm with psi as this. With psi as this worst-case expectation, since we've assumed that the projection onto A is easier to compute than the projection onto D, we've made our algorithm feasible. And so the key theoretical challenge here is to interchange the min and the max, which is possible under the following regularity condition that I've listed here at the bottom of the slide. So, to wrap up, I'll discuss three numerical examples, all from PD constraint optimization. The first two examples are optimization problems constrained by linear elliptic equations. The first one is a 1D example. So, the spatial dimension for our PD is one-dimensional. The uncertainties show up in the diffusivity coefficients epsilon and the right-hand side F. Hand side F and in our second example, the diffusivity equation is defined over a two-dimensional domain. Again, we have uncertain diffusivity. We have an advection field that is also uncertain, denoted here by V, and an uncertain right-hand side, F, an uncertain source or sink in this equation. And in both examples, we're minimizing, in fact, in all examples, all three examples, we'll be minimizing. In all examples, all three examples, we'll be minimizing the risk associated with some quadratic objective function of the state plus some penalty on the control. The first example, we're minimizing, well, our control penalty is the quadratic. Our second one is the L1 norm. And we also note that the 2D example, we include constraints, additional constraints on our control variable Z. Our third and final example is the optimal control Burgers equation. Again, Berger's is a non-linear differential equation. And our uncertainties for this example show up in the viscosity coefficient nu, the right-hand side f, and then the boundary conditions d0 and d1. Again, our objective function is a convex, well, as a quadratic objective function. Quadratic objective function. However, since our PDE is non-linear, when we solve the PDE and plug it into our objective function, the resulting optimization problem becomes non-convex. So we look into solving these three optimization problems with four different risk measures, the mean plus semi-deviation of order one, the mean plus semi-deviation from target, the conditional value at risk, and the buffered probabilities. Additional value at risk and the buffered probability. So, all of these risk measures depend on this positive part function max, zero and x. So here are the results. For comparison, we've solved these problems with the non-smooth, non-convex bundle method, in particular the trust region bundle method by Schram and Zove. You can see here in the blue colour. You can see here in the blue column the number of function evaluations and number of gradient evaluations for our primal dual optimization algorithm. To solve the subproblems, we use a generalized Newton method where we take, as the Hessian, we use the so-called Newton derivative of our smooth risk measure. And so if our bundle information or our bundle method doesn't Our bundle method doesn't handle constraints. So, each problem that includes constraints, this includes all buffered probability problems and then the 2D elliptic problem, we weren't able to solve with our bundle method. But to compare the unconstrained problems, we see that the speed up that we achieve by using the primal dual algorithm is roughly between three and 19-fold. So, it's a significant speed up for some of these problems compared to the Problems compared to the non-smooth method that we implemented. So, just to conclude, hopefully I've convinced you that the numerical solution of risk-averse simulation constraint optimization problems can be very expensive. This expense is compounded when we're dealing with risk-averse optimization using coherent risk measures because we don't have differentiability. Moreover, non-smooth. Moreover, non-smooth optimization algorithms tend to converge rather slowly, especially for non-convex problems. We were able to get around this partially by smoothing the risk measures using the informal convolution, these so-called epiregularized risk measures. This epiregularization approach ensures that we have continuous differentiability of our risk measures, and this motivated the And this motivated our generalization of a method of multipliers, which we call the primal dual risk minimization algorithm, to solve these problems. In particular, our primal dual algorithm solves a sequence of smooth optimization problems at each iteration, which allows us to apply rapidly converging Newton-type algorithms for each subproblem. We showed convergence of this algorithm. Of this algorithm, both for approximate minimizer and approximate stationary points. The second is very important for non-convex problems, which is fairly common in simulation constraint optimization. And finally, we presented some numerical results that suggest significant improvement over the bundle method that we compared with. So finally, I've included some references here: our Thomas Sirows and My Seros and my paper on the algorithm is published in math programming. It's online now. So feel free to download that. We also have a paper on epiregularization and then some other references on just general PD constraint optimization with uncertain inputs. Thank you. Perfect, thanks a lot. Are there any questions? Let's see if there's one in the chat. Yes, I have a comment maybe. Hi, Drew. Thanks for such a nice presentation. Very much enjoyed it. Just one note for historical. I think this is historically correct. I think that minimization formula for the buffered probability, that's actually due to Matt Norton, according to Stanley Riasev. But I think because of the publication history. Because of the publication history and delay in publication, this is not apparent from the chronology of the various papers that they had. I think you're right, Johannes. I remember Stan telling me that. Yeah, sorry. Stan told me, I think it's from Stan, so I think he knows. Well, I apologize to Matt. Yeah, well. Yeah, well, no, no worries. The other thing is, I wonder why you're calling it generous augmented Lagrangian because to me, this is just augmented Lagrangian, period. You know, in this broader mindset that we are not only talking about equalities or inequalities, but we talk about laws and proper laws of continuous convex function that could be anything. And from there, you get various augmented sensor variations. Various augmented institutive variation analysis. Absolutely. The primary reason for calling this a generalized Augman-Lagrangian is because it's not, right, we derived it in the same way that you would derive the Augman-Lagrangian in the dual sense. However, you know, it doesn't, I guess coming from a nonlinear programming perspective, it doesn't look like the Augment Lagrangian that you're. It doesn't look like the Agman-Lagrangian that you're familiar with. Right? And so we called it generalized Egypt-Lagrangian for that particular reason, to speak more to the nonlinear programming crowd and to not ruffle any feathers since it doesn't really look like the augment Lagrange that you're familiar with that I showed on whichever slide that was in the talk, slide 14. In the talk, slide 14. That was the primary reason. I agree that it's absolutely just the augment Lagrange gene applied to it, well, at least the same technique applied to a different type of problem. Hi, Joe. Very nice talk. I wonder, so in your epidemic regulation, so you use Regularization to use this epsilon. I wonder like how this number of iterations in your optimization depends on this epsilon. So in general, for epi-regularization or for the primal dual algorithm? Yeah, for the primal dual algorithm. So for the primal dual algorithm, you have these sort of competing quantities, the dual updates and the penalty parameters, which seem to Which seem to help with the instabilities that you might get just by refining the penalty parameter R. In particular, this is well known in nonlinear programming. There's a relationship between the quadratic penalty method and the augmented Lagrangian method. And typically, the augmented Lagrangian method works much better because at some point you don't have to. Because at some point you don't have to increase the penalty parameter anymore under certain assumptions. Whereas with the quadratic penalty method, you always have to take the penalty parameter off to infinity. And the same sort of at least hand-wavy argument would apply to the panel dual algorithm. So you get some benefit by incorporating the dual information. And in fact, in Thomas, in my paper that's published in Math Programming on the primal dual algorithm. On the primal dual algorithm, we run examples or some numerical tests where we compare the primal dual algorithm with epi-regularization, where we increase the penalty parameter much like you would with a quadratic penalty. And so in this case, for these tests, we see quite better, I wouldn't say significantly better performance, but in the long In the scheme of the grand scheme of things, it's significant better performance because we reduce the number of function evaluations by, say, an order of 10, and multiplying that by the number of samples that we use to discretize the expectations and so forth, this results in many orders of magnitude reduction in the number of PDE solves. Thank you. Yep. Okay, perfect. Thanks a lot, Ru. I'll see you all again in 10 minutes.