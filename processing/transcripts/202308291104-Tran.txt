So our talk today is about sparring and feature expansion and the publication I want to focus on is on time series data. This page on several books joined with my collaborator Rachel Bo from UT Austin, Eden Schaffer from UCLA, Lembo from Dark Aussie, and my PhD student Asha Shaha and my former master student Nicolas Pichash. So what is the problem we are trying to solve? It is a very well-known regression. A very well-known regression problem. We build a set of samples where we both know the input and output. We want to build unknown function map that map the input to the output. Where when we emphasize that the dimension of the input would be very high. We don't know the distribution of the input data as the general setting or machine learning. And for the output data, normally we assume that people may have containers have noise based on the measurement or based on the real. Or based on the real latency. And for the simplicity, in this case, the output is only dimension one, but the code can be extended to hyperdimension. So to solve that problem, one-known problem, first of all, what is it used for? It's used for like this regression problem can be used for prediction, using for classification, using for blind source separation, and some of those applications I will talk about in this talk. So there are so many methods to solve this problem. So there are so many methods to solve this problem, ranging from you want to represent the function using freeboard, you can use the polynomial expansion, octagonal polynomial, you can use polynomial chaos expansion, you can put it very popular and you can do all of your work. Also, so what is our approach? Our approach is a sparse random feature expansion and the goal we want to do here is we want to construct a sparse representation with a normal lab of the With a neural network-like feature space. And we want to provide some TICO guarantee and general support. And the key idea here is based on three components. The first one is random feature expansion, which I could explain the next slide. The second one is fast promoting model like basic pursuit or basic pursuit denoising lasso. And the third one, for the theory, we use a technique from compressive synthetic. Okay, so what is the random feature expansion? So, what is the random feature expansion? What does it look like? So, we want to see in the feature, it is like in terms of the visualization, it is look like the same structure of a shallow neural network. But the input data here is this kind of dimension 4, people 4. And then we want to have only one hidden layer. But for the random feature-computing work, we want to have a hidden layer, have a much, much more number of nodes. Like the number of nodes here, and is like very large. But for the second situation, I only put five here. And the output here is one. And the output here is one. So, for the random feature, the difference with the shallow network is for the first layer, we have some initial word and by us, we randomize using certain distribution like Gaussian, sub-Gaussian, etc. And after we randomize, initialize, and then we phrase. So, in terms of the formulation, mathematically, what it really means here, it means we want to approximate or rise of function unknown function as a linear combination of some. Of some function phi, which is in the product, which is sample data x and the random weight vector omega j and counting point with some by s. So in this case, what is known? So omega j and v j or j here, the width and the bias are initialized from some random distribution, from some distribution and then fit. And then the only unknown we need to find is Cj. So that is basically the CJ. So that is basically random feature expansion, and our goal here is to quite search. Okay, so why random feature? So as summarized actually in Editor By talk at the very beginning, it and enlarge the enable the modeling and large model capacity. So there's a couple of benefits in terms of using random feature. When only the final layer is trended and now the problem becomes linear and then you can have a much lower cost in terms of trending than the non-convex look much like. Than the non-convergence of pre-management-based approach. And the motivation from the technical point of view, then based on the work by Rahman Rich, that is the principle that we follow for, it is the two-layer randomized method without the need for the training of a particular algorithm, can achieve the similar result as the result obtained for solar network. And then, for the second approach, I might emphasize about the width. Might emphasize about the width of the hidden layer need to be very large. That is observed, improved by the neurotransmitter by the work, Jacob and the other, saying that if we have a very wide network, then after we initialize the random initial weight, then the those words doesn't change so much during the training. So it doesn't change so much. Why wrong with the initialize anything? Okay, so that's the first component, what is the random feature component of the I. The random feature component of the algorithm. The second one here is why we use past it. So, even in the setting, when we work, we have focused on some real data set and even like with some epidemic data, like we have a bunch of, for example, COVID data set, even we have a data record of every day. But in terms of maybe, for example, in care debt, or one week, we only have only less than four hundred number of reports because actually one year, most in the meantime you only have three hundred sixty five days. You will have 365 days. So, the specificity here is in the context that we want to leverage the structure in the data scale segment, where basically we don't have so much data, but we also want to have a rich representation of the function. And the second upper component you can see is the low-order additive function can be seen in another field, like dimension reduction, active subspace, and dominant interaction of high-dimensional functions. Interaction of high-dimensional functions that are observed in dynamic processes. And it also has some relation with the over-primetric regime. What does it mean by that? It means that the number, as we see over here, is the number of unknowns is much, much more than the number of hidden data. So we want to get some kind of regularization to visualize, which can be seen in compressive sensing and also in over-prime chi neural network. So going from the input to the first layer, you group the variables? No, I don't group the pedal. So the whole thing input, one input, two input, three, four, that's just made like that, the whole input layer. So those IP is four in this case. So one package. So you have one very web, very input, input into a weight. So just different weights for different. Weights functions. A lot of new variables. Yeah, yes. And then different weights for the input. Different weights for the activation is five, yeah. The activation is the non-linear activation function. We input and the first linear function. Oh, yeah. So first one is the linear, and then the upper that's non-linear. Okay, so in term of Okay, so in terms of mathematical formulation, as I explained roughly from the beginning, so we approximate the unknown function f as the linear combination of some unknown coefficient C and some multiplied by a non-linear activation when combined the inner product of a data sample and the band of weight. So different a little bit with the previous three talk when we use a weight by separation some function on By separation some function on the data on x and some function on the omega. In this context, we look at the non-linear activation on the inner product of the data sample and some data. And our job now is the end here is very, very large. So activation, the non-linear activation function 5 here is also only chosen. You can do value, you can use sigmoid, you can do tension hyperbolic. You want to apply that far. Complex, we want to apply that from an ecosystem. And then L is very large, but to work in a data scar setting, then what we want to do here is to promote, then we can use either L1 or the N result and N0 regulation on C to robustly identify the coefficient. So, in terms of TO result, if we don't use the random weight omega giant in this setting, Random point omega agile in this setting, then if we use like random matrix to represent, then the predefined feature result for the core result, which is deeply incompressive, can be found based on the both of Candace, Tao, and the other in the So, to summarize what is the main structure of our Sparse random feature model. So, the first step is withdraw and random weight vector. And random backup. And then, after that, we just construct a random feature matrix by taking a non-linear activation function on the in that product between draw input data and the bandwidth. So, m here is the number of given data, and n capital N is number of nodes. So, N here could be specified or could be chosen. In our theory, we quantify how much N we need. How much end we need to obtain certain type error reconstruction. So, in terms of modeling, spast modeling, then you can choose either like the N1 optimization where you want to say that we want to minimize the N1 on the vector proficiency, so that the difference between the output Y and then the approximation AC is marked by some error which is based on also 25. Which is based on also quantified by the number measurement. On the other model, we also propose for surrogate modeling, it is a combination of rig regression and the N0 hard resolution. So the reason why we want to read regression by adding the decolon regression here is it works very well also, but it works very well with pattern feature. And then we want to make sure that we want to control the number non-zero confusion in the C is not so far. In C is not so not about S. So the N0 of C just means that the number of zero in C here is at most S, but we don't know the location. I'm sorry, your second problem is every hardware. Sorry? The problem with the L0 constraint is entry hardware. It's LDR. So you're not going to be able to do that exactly. The first one is. I'll add the numerically. Yeah. The numerically here, it is actually, if you see in the formula here, you can rewrite those two terms. You can rewrite those two terms, but the other constraint makes it. Yeah, exactly. I agree. So that I think, like for it is in general, it is NPH. That's why we have relaxed using N1. But in this context, when we can use, for example, the hard transfer in pursuit promoted by Foucault, then we can use an approximation of iterative speakers of. I mean, that, you know, although the worst case of that problem isn't hard, indicative programming solvers are actually quite good at solving that one of the problem up to even like 10 million variables. Indicator programming solvers get exact solutions, yes. Exactly. Exactly. Yes. Yeah. So if you're interested, you can look at in this paper, we also have some tearful guarantee using the hardware soul proposed by and then combine with. And then combined with the property of the matrix A that we investigated in the previous paper, then we have some measure of Shinbaum. Okay, so in terms of what is your result, I just want because quite complicated, I just want to summarize what is the key idea behind the theory of our results. So the first one, I just want to say that to recall which cells are sparsity, then the random feature expansion can achieve. Random feature expansion can achieve the bound order square one of the product of n, where n is number network. The second one is our result when we add the specification to do the derivation, and then we just like a little bit worse than the other one by order n over one over d, where d here is a dimension. But D here is a dimension. So, which does it mean by that? It means that if we work with very high-dimensional input data, then whenever we're used to this, you know, so likely our model might benefit when we work with higher-dimensional data. And then, what's the key idea of the how we can prove that? So, the key idea here by the observation that when we view the random feature projects by taking the non-linear activation on the input data, Equation on the input data and the random bit, then the mutual current of that random feature matrix is very small. So the mutual, so one of the techniques in compressing is using mutual currents. If I have small mutual current, then the matrix will satisfy, like we can recover the sparse problem. Whereas the mutual current basically just means that it looks at the largest absolute of the inner product of any. Absolute of the inner product of any two column of the images. And that is the assumption. And then the outcome we get here is of order n minus whatever plus one over v and then the requirement in terms of we can say the practice how we choose how many random features we need and what is the number of measurement we need to attain the accuracy. So of course I discussed some other parameters that I didn't mention for the theory but basically what it is the number of random features here is all Random feature here is of order d squared, where d here is a number of dimensions. S here is a sparsity that you want to control how many terms you want to possibly function. And then the number of measurements here is of order s square plus f square. Of course, there's some other term that appear to be there, but maybe what you see research, you can look at our breakdown under some assumptions of our data. So, you have 15 minutes. I will talk about some of our Will talk about some of our applications and then we want to say a couple things about that. So, the first one is we want to focus on the data scatter regime where the number of samples is much more than number of models. We also want to verify that our model work in practice with real data set and within data set. So, of course, we run all our comparisons, all the simulation data, but also on real data set, like you see how it is. So, basically. So, basically, for the first model, which is the first one that we propose, the sparse unknown feature expression, we compare our method with shadow network with the same number of unknown parameters and the number of unknowns is more than the number of parameters. Then our method was better than shadow network. We also compared with polynomial KOS dispersion. I have to say that polynomial KOS is very good for a lot of problems. But if your data, the input data here coming from an unknown distribution, then maybe colonical is not a problem. Then maybe colonical is not a well, maybe one-choice might matter too. For the half model, then we use a replication with the hard press holding constraint. We also compare it with the popular sparse detective model and on the focus of the serial quality. We also talk about the application in epidemic data. The first two have a good focus today is on the time serial data, but we work with epidemic data and work with the new series. Data and work with the music. So, the first applied question I'm going to talk about here is about epidemic forecast. So, those are four data sets that we, those are four real data sets that we work in our paper, which is a COVID, omelet, Zika, and flu virus. Where the COVID, I believe, is in Canada and from other countries. Real data set. So, what is given? So, we have given the term. So we have given the time-dependent data, which is number of infectious people, given by day or month, depending on how the data is reported. So, given the time-dependent data of a multi-dimensional system, it is multi-dimensional because actually the number of infectious people is affected by so many components, and sometimes you don't even know how many product is. And then, the goal here is to predict how many people are infected by the next seven days or next 14 days, and that is short amount of time we can take. And that is short amount of time because actually all the kind of achievement, all kind of decision making is only happened for the next seven days of what is happening. So that is our focus. Okay, so how we put it in the context of using sparse random bridge expansion. So the first idea is more than only sparse random is we project our data into a higher dimension. So there are two ways to think about that. It is working. One way to think about it is by Cambi Lan equation. We want to say that the rate of change of the inflictions is a Of change of the infections is a function of time delay equation of a variable. On the given perspective, when you think about it from dynamical system point of view, it is from the token theorem. You have one variable along a multi-variable dimensional system. Then from token theorem, which is embedding theorem from differentiation, saying that using only the data from that trajectory, then we can have a when you break it into like the embedding dimension, then it preserves the problem. In the dimension, that it preserves the property of the optimal system. Yeah, so the idea here: so the first step is we project into the higher dimension, which is a vector z higher than one, and then after that is automatically applied our method by a proximated function f by the back dampers function. And then here's the result. So on the right-hand side here is a real data set from the second wave COVID in 19 in Canada. 19 in Canada. The dashed line just means that we get that cutoff. Like, for example, for validation, that dash line means that the number. And after that, one predict the amount of infections over the next seven days. Before the peak, after the peak, and the rapid peak. And the round pick is actually very hard to predict because of the oscillation. So the first two on the top, in term of colour code, the virtual is blue. Ah, sorry, the virtue is blue. The blue, ah, sorry, the virtue is black. Our model is blue. The other popular compartment model that people use is SEIA and SBYA. And the reason for that is like in the first few, we don't have so many now, then of course we should charge SEIS. Okay, so as you can see, like before the quick and after the quick, as in the first two pictures on the first row, our model matched very well with the ground too, where the other model fell. During the picture, During the peak time, corresponding those four in the middle, then as you can see, the blue one, which is our model, kind of keeps the check, like predict the check, while the other button is like all over the place. Of course, we also have the real data experiment with the other type asset, but for the time delay, I will start about the first one for me. Any question about the first application? What? What? Yes. Did you ever compare with using just like an L2 regularizer rather than a sparsity enforcing regularizer? Yeah, yeah, we did. Yeah, but it doesn't give a very good solution. We can also compare with like you don't have some outlier and noise. Yeah, yeah. Remember that in this guy, look at the number of data sets. It's only one footage up by HY, the data set. Or the each word, the data one, I think it's I believe it's better than two hundred level data set. And then the level L is very, very large. Yeah, and it's very large. Okay, so yeah. And that's even with like large values of regulation parameter, compared with each resolution. The second type of ligation we talk about here is another kind of interesting ecologation, which is the signal recomposition and feature extraction. Signal decomposition and feature extraction. So to capture in this kind of figure. So basically, that is our input. It's a very challenging time series signal where you have discontinuity and you also have some linear oscillation, but also non-linear oscillation happening. And then so we put that data, and given that, I want to extract that time series into three modes. So number of modes here need to be given. If it's not given, then what? Be given. If it's not given, then what we do? Then we have time to mode and then we can find the best in our system. So in this case, even that hammer data I want to decompose in the intrinsic mode, so what does it mean by intuitive mode? That means that it's like example intrinsic mode function, like when you whistle, that is indigenous mode, for example. Or when we play a sound, that is indigenous mode. Okay, so our scheme, what we do is kind of like you think about it, that's you transfer somehow in a like fluidoma, but not I would say generalized away, but we have to some special achievement about that. So we transfer our data into the time frequency domain. And as you can see, what are those ones? Those ones are the non-zero coefficients that we recover from the sparse random feature expansion. And then from that, you use the next. And then from that, you use a density separation. Easily, you can side-see those three components. Where the one around here actually, because of this 3D hepatic contact. So for each of those modes, the kind of code it is those three modes that we are recovering, which is agree with the virtual hepatic contact. So what is linear, and then some like cosine function with the frequency from t and then what is the common law. As you can see, it agrees quite well with the approach. Okay, so mathematically, what is our approach about? So, our approach here is based on time-serving data. So, we want to build the method based actually from short-term Fourier transform. So, in the one sound certain what we really mean here is we write our function our function in the short time Fourier transform and then we do some kind of integration of our selection. Some kind of integration deposit actually. So, integration deposit, most of the time we use make equally distant. But in this case, we want to emphasize that when we play for the music, then a certain time the note appears, and then after that, it disappears. So, we want to capture some kind of local information, local window. So, that's why we have those local window that you see here. In this case, we use Belgian. We don't know where that happened. So, we do random. We pick those random, we do a lot of them. We pick those random, we do a lot of them. And then after that, in the next positive, we hope that we can capture those, which is already valid on the matter. So it is the left window, and then the other one is to capture the frequency. So local intern and then also some frequency people. So now it is in almost the same setting as a sparse random feature, where we have some non-linear activation function, which is based on time and frequency, and our And frequency, and our goal now is to constrain just par C on C. As I said, M dar to capture and raise the function, and C here sparse so that we can capture the important book. And after that, we can choose your favorite basic pursuit or basic pursuit in noise so problem. So okay, so what about the decomposition? What is mean by that is we just collect on the time frequency, collecting. On the time frequency connecting with the non-zero coefficient, that's what the picture you saw in the picture. And then do the SD clustering, and then we have we can check one more for cluster. So, of course, we run with synthetic data. I leave it in the paper, but now I want to see that work also with your data set. So, what does it mean by that? I explain and I apply and so basically, the ground shoe here is float and get. It is flute and guitar by the better. Right now, with the flute work, those two instruments play, but they don't play the same work at the same time. But it's seen quite common you can see about that. So that is the key point. And then our goal here is to separate back, to separate back the float and the beta. So that is virtual. And then I was in my student new class, create all those kind of from the draw like dark image. And then that is a learning signal. Oh, that. Signal over there. So, in terms of graph, you can see that it's quite agree with the graph too in terms of capture the notes and also the tap. And then, in terms of like the sound, you can see, let me see what I have here. Oh that's the real data so this is that is the groucho and that is the one that we love from our And that is the one that we love from our pigment. So you can see that when you hear that, you can see that there's actually two instruments going there. So the only flute, the grouch, and the low one there. So it's quite much with that between the ground true and the low one. Okay, so I have a few more. Okay, so I have a few more minutes. I want to also highlight another kind of interesting example that we work on. It's on denoising the very strain data. So in this case, what we want to do here is we want to denoise it in the gravitational way, in the back. So basically, we have the black hole data where we can sometimes coding, we can do it in the time serate data. So basically, what we want to do here for the purpose is we want to see when the black hole works. We want to see when the black emerges. So the brown shoe here in this picture, which is in the blue-black colour, and the green highlight, it is brown true. So when you see that highlight of those blue one, basically that is to indicate where the black hole was. And the lung coefficient that we obtained from our model is on the like brown one happened over there. So as you can see there, we can capture the time version of the The version of the black hole. And also, you can by keeping only 5% of the non-zero coefficient, then we can have the noise data, which is in blue, agree with the noise. So we have to end the talk. I will say that it's a new way to think about sparse approximation. Instead of using polynomial or free, we may want to make a chart of using sparse and then feature expansion. And then we have some constraints. And then we have some construction generator mesomethore results with their shipbuild, and we also show a variety of lycation and more glycation. You know predictions, these COVID predictions, we had this seven-day interval. Yeah. Send the interval. Yeah. I did not understand why don't the predictions start at the correct value? Oh, that is that self-whatever. Oh, it is at the correct value, right? For the SEIR, I mean for the competition model or for three graphics. There are three graphics, yeah. So one may start. This one, self and yeah. The other model they they predict in a different way. They have to re-solve the OE system. So that's maybe the reason why they don't exactly that good question. Why they don't start exactly here. But because the way they do is they have to best those other environment levels of the SEF and this so the only fit. So maybe sometimes it would not really fit. Family thanks all speakers again and enjoy your afternoon.