Some of us have. Some of us have the genetic. We're getting away fast. So you have the I mean. Is this text available to us? Yeah, you can upload it as a file to the participant website. Okay, my name is The Wolfras. I'm the bearhand. If you cannot say that in case of emergency, call me Teddy. And I'm a combinatorialist, algebraic, enumerative combinatorialist. And occasionally I do number theory too. And as a former student of Jordan Zuyberger, I can claim that I'm also an experimentalist. So today I'm going to So today I'm going to bring to your attention some questions that you may or may not be interested in, but I have quite a few problems to share, therefore choose your pick. And due to shortage of time, because we cannot have a lot of time individually, so don't expect me to give definitions. If you are interested in the problem, in the gist of the problem, In the gist of the problem, then I'm willing to provide details. And the first problem is pattern avoiding permutations. So if you have permutations Sn, then now we are not counting permutations that avoid a certain pattern. We are counting involutions that avoid a certain pattern. If you do that, then if the If you do that, then if the pattern you want to avoid is an example here, 4, 3, 2, 1, then the number of such permutations is given by the Molsky numbers. And then this is a work of Amitai Regev. On the other hand, independently, we're working on a different problem in partition theory. You take a partition and then you have hooks and then you calculate. Then you calculate the hook length of each cell in a partition in its Young diagram. Then, if the cell, if none of the cells contain the number T, then it's called a T-core. And if it avoids a number of them, like ABC, then it's called an ABC T-core partition. That means it avoids three of them. I'm interested in three of them, multiple of them, in particular three of them, and it Particular three of them. And it turns out that if you do count the number of all partitions, of all partitions in the world, there are finitely many of them, avoiding the successive numbers, n, n plus 1, n plus 2, then such number was, we did this work with Emily Levin, and it turns out that it is the same number as the pattern avoiding permutations. The pattern avoiding permutations. So, the first question for you all is combinatory: is there a bijection between the two? Not just a numerical accident. So, I'll move on to the next one. What is it in this is somehow that this is a finite number, that if you eventually, if your partition is big enough, the hooks can't avoid anti-polymetric. Yeah, they can't avoid it. They can't avoid it. So, for instance, if you are all To avoid it. So, for instance, if you are only interested in all permutations avoiding the number 4, there are infinitely many of them. There are infinitely many of them, like you suspected. But the minute you choose two numbers that are relatively prime, then it's a finite number. So, this actually, you can think of it like Frobenius, coin exchange problem. Then the number Problem. The number of numbers that cannot be exchanged with two currencies, as long as these are relatively prime, it's finite. The same thing. Or you can think of it in terms of semi-groups of numbers. The laptop might be easier. The laptop, yeah. Yeah, thank you. All right, the next one the next problem is the following. So briefly, the Q's that you see here are now the Q analogs of natural numbers and the Q analogues of factorials and the Q analogues of binomial coefficients. So there are several. Are these are there? There are several versions of Q-Catalan numbers. One of them is given here, and this is a polynomial in the variable Q. Then there was a question proposed by William Chen, and he asked, what if you look at these polynomials and check the second derivative? So that means strictly convex, strict convexity. That means for each real value of Q, these numbers R, for each n also, are For each n also are positive, and as long as n is bigger than or equal to zero. I worked on this problem and I did a lot of work on it, almost complete, but failed in one case to complete. And that failure is that I could cover all of the Q values, real numbers, except in the range between minus one and zero. And this one is missing from that, and so. And so that product, as you can see, is a big product, but I reformulated the problem and rewrite the problem, the missing part, in the following form. For x between 0 and 1 and more than 2, more than 1, then can you prove that this is a convex function of x? If you can prove that, then I will be happy to have you as my co-author. Have you as my co-author? I'm serious. I'm serious. So, this is the problem I proposed in the spirit of this talk because we talk also analytic numbers here, analytic combinatorics. So, all right, the next problem is about hook lengths. I will not go into details. There is a way to compute hook lengths of a partition in and in its young diagram, and there is a concept of also called contents of the diagram. Contents of the diagram. And then, for instance, the dimension of the representation of the general linear group, which is indexed by partitions lambda, then it is given by this formula. The hook, it involves hopes, it involves also contents. And then using work in gauge theory, these two authors, Neksar Okunukov, Next Kunukov generated this, discovered this formula, the Hook-length formula, as a generating function. And the right-hand side, you can interpret it in terms of multi-color partition of integer partitions. Then the left-hand side is explicitly, I mean, exclusively in terms of books. It doesn't involve sets, I mean, contents. But Stanley has a version of this. A version of this, the Q version here, but involving hooks as well as contents, so hook content identity, and he discovered that. Then my wish was now to do this for symplectic groups and orthogonal groups. But then in that case, in the literature, I have found that there is a definition of a symplectic content different from the previous one, while you maintain the Hook Lanes formula. Formula, but changing the symplectic context to fit this discovery, this analysis, then don't worry about the details, this is just the definition of the hook, I mean the content of the partition. Then I have the following conjecture. Actually, it's more than a question, it's a conjecture. And the conjecture is this. It is very similar and in the spirit of the other two formulas, and it's given this way, and it involves. This way, and it involves the symplectic content and the hook length, the standard hook length, the familiar hook length, and it is very similar to the earlier ones, as you can see, and especially the Stanley's hook content formula. And so what the story about this? The only story I can tell, the only development in this direction is recently with George Andrews and Christine Ballen. Christine Valentine, we were able to do two special cases, the case t equal to zero and the case t equal to minus one. For instance, if you take the case t equal to zero, the right-hand side, the second part vanishes. Therefore, that means the right-hand side becomes a partition, the number of partition of a number just using numbers of the form for k minus 2 type. And that gives us some leverage in our analysis. In our analysis, and then we were able to prove this. But in other generalities, it's still option. It's a conjecture. Well, this one is in the same spirit, so I'll just move on to another one so that I have enough time to mention this. And here is about low concavity. So, if you have a sequence and if you operate X, if the sequence is X, If the sequence is Ak, if you take Ak squared minus Ak minus 1, Ak plus 1, this is like the rule here is the convexity in analysis, the discrete counterpart. And then we call this a low-concave sequence if it is a positive number all the time for any k. And if you repeat this operator again and again, m times, then if it happens to be positive all the time, then it's called m fold law. Time, then it's called M-fold law convex, but if you do it infinitely often and it's still non-negative, then it's called infinitely low-concave. Now, it is a familiar story in combinatorics that there is this notion of for graphs, the chromatic polynomial of a graph. Then, in that context, June Ha, a filmed alestep, proved that the absolute values of the coefficients of any graphs are low-concave. Graphs are low concave. Now I'm daring more for more. That is, actually, they are infinitely low concave. I did a lot of experiments with many types of graphs, and I'm convinced that they're infinitely low-concave. I'm a little puzzled at something because the sequence is zero eventually. Yeah, that's a polynomial. Yeah, it's a polynomial. Yes, so so so so after degree iterations, would you just add the zero sequence? No. Would you lose one one termination? Okay, when you do convexity, okay, to help you out, if you have a finite sequence because these are coefficients of polynomials, then you can pad it up by using zeros. If you do that, you still maintain those things in between. You don't lose one zero each time. No, no, it doesn't shrink. In general. In general. In general. In general. Okay, I for to be fair, there are many other problems, but I will stop here. And please upload this. Yes, yes. Yes, I will. Next, probably. Can I see the kind of slide two? This one? H2. H2 repeat. Okay, the bottom. You want to call it a rapid or something? I'm not going to do a slightly different way. Great. Can you try to plot that? This is surely I can bet. It's convex. Can I prove it? Not yet. Is there a square missing on the n factorial Q up at the top there in the denominator? No, it doesn't, because there is n plus 1. It doesn't because there is n plus one joining the factor, n plus one factor joins the other n factor to make it n plus one factor. Thank you. We don't need dinner tonight. Now we have stuff to chew on. I don't have slides, but I can just maybe write here. So I want to take two graphs, x and y, these are graphs, both with invertices. And I'll imagine that the vertices of x are chairs, and the vertices of y are people. And so some of the chairs can be adjacent to each other or not. And some of the people could be adjacent. And when two people are adjacent, we'll say they're friends with each other. So the edges in Y are encoding friendships. And so I want to define a new graph FS of XY. So this will be the first. So, this will be the friends and strangers graph of x and y. And the way that this is defined is the vertices here are going to be the ways that you can put the people into the chairs. So there are in factorial of these. It's just basically bijections from the vertex set of x to the vertex set of y. So the vertices are bijections. from v of x to v of y. And the edges are given by what I'm going to call friendly swaps. So if you have two people, so imagine you have some configuration of people sitting in chairs, and you have two people who are sitting in adjacent chairs and they're friends with each other, then they can swap places. Okay, so the edges here are friendly swipes. Can we describe it again? Yeah, yeah. So imagine you have a configuration of people sitting in chairs. So that's one of the vertices of this graph. This graph has in factorial vertices. And then imagine I take two people who are sitting in adjacent chairs and they're friends with each other and then I swap them. This gives me some new configuration of people sitting in the chairs which is a new chair. People sitting in the chairs, which is another vertex of this graph, and that's when I draw an edge between those two vertices. So then, I guess one problem is to try to understand what happens when x and y are Erdős-Reini random graphs. So understand fs of xy. fs of xy when I'll say they're independent Heritage-Renny random graphs with invertices and edge probability P. P could be a function of n and one thing that we know about this so I have a paper with Noga Alon and Noah Kravitz and we basically determine the threshold probability P for when this graph is going to probability p for when this graph is going to be connected. So basically it's around 1 over root n. So if p is significantly lower than 1 over root n, then this will be disconnected with high probability. And if p is significantly bigger than 1 over root n, then this friends and strangers graph will be connected with high probability. But then you could say, okay, well, if p is much smaller than 1 over root n, then you'll have maybe lots of connected components. And you could say, well, how many are there? Component. You can say, well, how many are there? You get kind of estimates on this. Or alternatively, if p is bigger than 1 over root n, then this is going to be connected with high probability, but you could say, well, like, how connected is it? Is there some kind of measure of how connected it is? Or you could look at other things like what is the diameter of this graph? What would be like the minimum degree or the maximum degree? Other statistics that might be interesting. So this is kind of just a very open-ended kind of Open-ended kind of general problem to try to understand. And I guess I'll also discuss a different problem that's with these Franks and Strangers graphs, but it's for maybe not necessarily Erdős Strange random graphs. So here's the So here I want to just fix x and y. And they can be, so I don't really know what the right choice is. And maybe there are interesting choices that you could consider. So maybe you want one or both of these to be like a path or a cycle or maybe a random graph, G of N P, just whatever you think is interesting. Think is interesting, if you'll say, etc. And I want to consider a random process where you do the friendly swaps randomly. So do, I guess, random friendly swaps. And I guess there are two ways that I imagine you could do this. One would be to choose two people, so two vertices of y at random and swap them if you can. And otherwise, you do nothing. Okay, so the idea is: I have my graph X and my graph Y. I pick two people. These are vertices in Y. If those people are friends and they're sitting in adjacent chairs, I swap them. Otherwise, I do nothing. Then I just choose another two people, swap them if I can. I just keep doing this. This gives you some kind of Markov chain process. And again, you know, I think this is sort of a vague, open-ended question of just like what happens. Open-ended question of just like what happens, you know, can you try to understand what's going on? And then the other sort of variant of this would be: instead of always choosing two people who may or may not be friends, maybe you want to just choose two friends at random. So this would be choosing an edge in Y and saying, okay, well, we know they're friends, and if they're sitting in adjacent chairs, you swap them. Otherwise, you don't. Okay, yeah, so that's. Okay. Yeah. So that's uh any questions about the questions? At least immediately yeah, changing from people to friends seems like you're doing the same markup process just faster, right? Yeah, I think that's probably true. Yeah. I guess it might be conceptually easier to think, for example, let's say y is like a Let's say Y is like a cycle, so it's like very sparse. I'm thinking maybe you don't want to have like really long gaps between when you're actually doing something, and so you want to say, well, let's just choose a random edge in the cycle instead of just two people. So I would like to ask you, Colin, and also everybody else who presents an open problem, please type it up, a paragraph and and upload it, because I will have to write a report of this whole meeting at the end and and that helps. Meeting at the end, and that helps. It will ask what kind of useful evaluations were fostered by this meeting. But I do have something to say. Okay, thank you. Next, Laura, please. I will take you on something. Okay. There's usually a button. Okay, but then I have to probably shut off the screen project. Maybe I should just continue talking about. No. Don't worry too much, I think. You lower the brightness? Wow. Wow. Problem solvers. How about this one? For a quick answer. Okay. Thank you very much. I'm Clara Eslava. Thank you very much. I'm Clara Eslava. I work in probability and combinatorics. Anything that goes, that has an algorithm and you have to analyze what happens almost surely, asymptotically, that's where I'm at. So Stefan was talking about recursive trees, uniform recursive trees, so I will remind you what they are. You start with a vertex one, you add vertex two, and then you add sequentially all vertices with labels Vertices with labels, but vertices are labeled by the time they arrive, and then you add them uniformly at random. Okay, so I think some people are familiar with this. If you have questions, ask me. Now, I will tell you, I think, two facts about these recursive trees, and then I tell you the question. You, the question. So, if you want to know the degree, for example, of the root, okay, so this is going to be the T7, right? Like, there are seven vertices, this is one. Okay, so the degree at time n of the root is just a sum of pernully random variables, independent pernicious random variables, because for each vertex i, For each vertex i, I have to ask vertex i, did you connect to the root or not? And that will happen uniformly at random. So there is some Bernoulli random variables with success probability 1 over m minus 1. And then people know, you can see that this has an asymptotic distribution. This has an asymptotic distribution. Its mean is log n and it converges to a normal asymptotic colour. Okay? That's some sort of parameters. And in general, we don't have colours. Okay. Okay, in general, if I look at vertex J, then what happens? j, then what happens is that I need to change this 2 to j plus 1. And depending on the value of j as a function of n, then the convergence is going to be different. And what I want to say is the asymptotic degree of each of the vertices depends on when in the process they arrive. When in the process they arrived and they are not exchangeable at all. So, what about the maximum degree of this graph? So, if you have the maximum degree, you look at all vertices at time n and you ask for the maximum degree. Then this is going to be more or less um Um well this is a result by Luke and and Lou. Uh if you divide by log base two of n, then this converges asymptotically surely to one. And there is also something related to what Keldin's, what it's the same flavor, what Keldins was pointing at in the morning. If you look at the distance and now I'm Distance and now I'm didn't do my homework during lunch. So this maximum degree of is of that order, so I want to look at the tails. So let's see what this probability should be. E to the minus 2 to the minus i. It's a combo distribution. It's what would it be? A combo distribution. Well, yeah. Maybe there are typos there, but it's a it's Adobe exponential distribution. And well, I will tell you lots of things on Thursday about the recursive trees and these maximum degrees. But the main thing I want to tell you today is that all the degrees are going to be stochastically dominated by if you By if you have a vertex i, a vertex j that is smaller, it's going to have a larger degree stochastically. So the root has a smaller degree than the vertex that attains a maximum degree. And I'm fascinated by that. Can you say that again? You're fascinated by that? By the fact that the maximum degree vertex is not the root. Vertex is not the root. Oh yeah, that is fascinating. That is fascinating. So I will tell you more. Okay, so this is just notation for the five minutes I have. Vertex I is high degree if it's degree at time n. Degree at time n, it's larger than c log n. Of the order of root. If this c would be closer to logways 2 of n compared the logarithms, then it would be essentially the maximum or a close relative. So, in general, I want to know what is going on with these vertices, and the question is. And the question is in TN conditional on I being a high degree vertex. Can you know what is the size of the subtree hanging from it? So now I'm going to kind of weird drawings. You have the root, you have You have the root, you have your random recursive tree, you have I, which you know it has lots of vertices, way many, or the other or the root. Can you tell me what is the size of the subtree here? Or in general, can you tell me anything about these high-tree vertices? For example, I have a friend that works with uh Friend that works with energy of graphs that is related to some chemistry intuition. And one question is: could you know what is the degrees of the neighbors? And in general, the problem with this, I think it has been mentioned twice, that once you have extreme conditions, then usual techniques are not Usual techniques are not not useful anymore. So if you want to know what is useful, I can spend a few more minutes if I have, or we can chat later. But the fact that high degree vertices are okay, so what is the probability that the root is a high or yeah, let's fix ideas and say that a high degree vertex is going to be 1.1. 1.1. Right. 1.1 log n. So then you ask the root: are you going to be a high-degree vertex? And then you will say no, because the root degree is going to be concentrated around one log n. So being high-degree vertex is a very weird event already. So conditional on that how the rest of the tree is modified. So recursive trees are So recursive trees are kind of the easiest increasing trees because all the attachment has happened uniformly at random. So every time I add a vertex, it's uniformly chosen, where to be placed. So what is this conditioning doing to the tools we have? So thank you. Question for Active The size of the sub-tree of the bigger degree in that bigger degree that is not known yeah that is not known okay no so uh there is there is a lot of information about the the vertices that are the maximum degree vertices so there might be tiles there is not one maximum degree vertex and okay order the vertices in decreasingly by degree you you can say some things about what are those vertices What are those vertices? But you cannot say anything about the neighborhoods of those vertices. You cannot say anything about the neighborhoods, and you cannot say what is the sub-resolution. Easy to explain why one isn't the maximum for chance? Well, if you okay, so you have this degree, you know that this is expected, and then this low. And then this log raise 2 of n is more or less 1.4 log n. Everything is like it. Oh, because of the constant. Yeah, this is... I don't remember if it is. 39238. So because of... It's the same. Because of concentration, you know it's not going to be the root. But the max degree has to be one of the early ones, right? No, not really. I'm going to. Not really. I'm going to what is that? Spoiler alert. If this say if I okay, if I attains the maximum degree, then the depth, so the distance to the root, the depth of I is ah. Ah yeah, you should not improvise in terms of alpha login over squadrons and tending to a normal. Okay? So, yeah, I will tell you. So we don't know what the depth is, I mean, we know the limit, but what it tells you is that if you want at every, you ask for a maximum degree vertical. For a maximum degree vertex at every tn. And then you do this rescale. The fact that you have to rescale by an increasing function, log n, it means that you are not picking the same eye for all the trees. Because once a vertex arrives, then the depth is fixed. So that means that your vertex is changing and yeah, well maybe it is one of the mid early vertices, right? What if you rear instead of load n, load 2n, and at the bottom 2, load 2 n? What in this rescaling? Well, it doesn't work. Just what would you because I would Because I'm just wondering if it is a question of which way to normalize. No, well, for the root, this is the right normalization. Because the expectation is log n. In the expectation, this is just the harmonic number. So so the right expectation in in asymptotically is log n. And then the question is, by what do you have to scale so that you still have randomness? Case so that you still have randomness. But then, if you move the constant, then the limit fails. Yeah. If no more questions, are there more questions? There should be a lot of sort of self-similarity in this object, right? Can you explore that to make any progress? Exactly. For example, I'll try to. I'll try to do it okay. So, one of the usual techniques is you have vertex 1 and vertex 2 over here, and then this is kind of polyawns or Chinese restaurant. You will keep adding vertices, but now you will tell me first if it is going to be connected to the sub-tree of two or to the other one. Then, once I know the labels that come into here and into here, they are going to be recursive trees. And for example, if you have the root and you have log n vertices, then you know the sizes of these subtrees. And all these subtrees are going to have the distribution of a recursive tree conditionally on knowing the labels. So these are these are the kinds of uh techniques. Kinds of techniques that I'm not sure if you like this is already imposing that you will have enough edges here which are not natural for vertex eye. So versions can work. Okay, thank you. So then next is Robin. Yeah, so uh kind of last minute, so I didn't really have a chance to check again. I've thought about this question on enough for a while. Didn't have a chance to check again whether perhaps anybody knows something about it now, but I don't think so. Uh if you if you start to think about it and, you know, get interested in it, then You know, get interested in it, then we need to do a literature search. But it's a path counting problem. So, kind of like oriented percolation, all right? So take the positive quadrant in the integer lattice and have directed edges, always going up and to the right. The right. And so you want to know how many paths get to a point ij, but you're going to remove randomly some of the edges. So let's say each edge is kept with probability. Think of maybe three-quarters and deleted with probability minus p. And p has to be big enough, it has to be bigger than the critical value for oriented percolation, so which is bigger. So that value is strictly greater than a half, I don't remember what it is. And so, with positive probability, there are paths that reach there. And when there are paths, so the critical threshold for there being a path with any reasonable probability is different from the value of p needed for the expectation of the number of paths to be positive. Positive, right? Because, for example, let's take a point on the diagonal, so n, n here, right? So there's 2n choose n halves, so roughly 4 to the n times constant n to the minus 3 halves. And so if you have p equals 1/2 plus epsilon, then so there's Then, so there's two n different, so for each of these two n choose n paths, there's n horizontal steps and n vertical steps, so two n steps that have to be there if the path is going to be there. So 4 to the minus n. Well, so if p was a half, you'd have the expected number of paths that survive would be kind of small. But if p is a half plus epsilon, then the expected number is exponential. But that doesn't really help. There aren't any usual. But that doesn't really help. There aren't any usually. So there's some, you can do this asymptotically and say that let's say that p is bigger than the critical value, so the number is going to be, you can get unlucky at the beginning, and those two edges aren't there, so then there aren't any, but with some probability, at least, you know, some particular constant, as n goes to infinity, you get some paths. You get some paths. So, how many do you get? So, this is, you know, speaking of algorithms, this is an easy algorithm problem. You just start counting recursively the number of paths that get to each point. So, it's a quick computation to compute how many paths get here, ordering squared. So somehow that means, you know, it's algorithmically easy. It's something we can, a quantity we can understand pretty well. But the question is, once you're in the supercritical regime, Supercritical regime. What is this number of paths? So there's the number of paths, if I take limit of the number of in the, if I go to nn, so number of paths to nn, let's take a conditional probability, so conditioned on, that it's at least one. At least one. So that's some kind of random variable. I'm not writing this very well. But I take that, maybe the log of that divided by n. So it's the exponential growth rate, if I see some. So I think it's pretty easy to convince yourself that there should be some constant. There should be some limit, almost really constant exponential growth rate, if there are any at all. On that event, On that event of positive probability. But we don't really know anything about this number. And in fact, you know, often in the supercritical case, once you have some of these things, then you get as many as you expect to have. But I don't see why that should happen in this case. So maybe there's an even more supercritical case with P really close to 1, where you have as many as you think you should have. So the question is whether, what is this value and is it? And is it equal to what you get if you do the naive thing and you count first moments? Then you take, you know, 4p squared, a n, so you take the log divide by n, so log of rp squared. Is it that, right? Is there a pot high enough p so it is that? Or is it that for every supercritical beam? Super critical. And you can play the same game if I go in some other direction and I have n, alpha n, and then you can look at the paths that get there, and you can make sure p is supercritical. So you have to have a bigger p to hit that point. That point's harder to get. There are fewer paths that get there, but you become a torx. And once P is supercritical there, then. Then, are there the right number of paths? And if not, can you make a phase diagram for alpha and p that says what happens in any part of that space. For example, if p is supercritical to get there, eventually it will fail to be supercritical to get, you know, the same p will fail to be sucritric. The same p will being failed to be supercritical over here. So we know, and when this happens, the expected number is still growing exponentially. So if the expected number is really, once they're super critical, is really the just, if the typical number of paths, which is like this, is the expected number, then either there's some discontinuity and all of a sudden it becomes zero, or all of a sudden it stops being the expected number and starts being a smaller number. Expected number, besides being a smaller number, can't be a larger number than the expected number most of the time. It could be a much smaller number. So, anyway, that's the open question. It's just figure out any non-trivial piece of information about the number of paths down. This four would be squared. Where is it coming from? So, along the diagonal, there were roughly four to the end paths, and each one was in with probability p squared to the end. Squared at the end. So that's where that was coming from. Okay, more questions for Robin? Then I have a question for Sebastian. Yeah, I should say I posed this problem at an A of A before, and it's still open, so be warned. The problem comes from a data structure question, but the task itself doesn't have to do anything with it. So I'll define it that way. I'll define it that way because that makes it more natural, but you can translate it to a completely combinatory question if you want. I have some slides with some information I collected about it, but we don't need that for the definition. Start with the random permutation pi. And in the computer science sense, I'll just write that in an array by putting the numbers. So here's pi 1, pi 2, and so on. You write those down. Two and so on, you write those down. I'm asking questions about this permutation, and I want to treat different permutations as the same if all the questions are answered the same way. So it's a lossy way of looking at a permutation. The first type of question is called a range minimum query. And so that's given, apart from the permutation, two positions i and j. I and J. And then it's, I guess, what the name suggests. It's just the minimum in that range. I should say phi of k it might sometimes be convenient to define it as the index and not the value. We only care about And not the value. We only care about where that minimum is. It shouldn't make a difference for this case. The simpler version is just that. You have a permutation, you're only allowed to ask all these questions, all positions i and j, but then it's a contiguous range and you only tell me where is the minimum. You think about it for a while, you notice the only thing you can ever get out of the permutation. Get out of the permutation can be expressed in a different way, namely using a tree structure. You can ask arrangement from 1 to n, that gives you the position of the minimum. So I can find where is the number 1. Say that's here. Then any query that straddles that position will always just return the 1, so I have no way of comparing elements left and right of the 1. But I can compare. The one. But I can continue the same way. I can ask one up to this position. That gives me the minimum left of the one. So I could find what I call two here, but the real two could also be over there. And so you can recursively construct a binary tree like this by continuing to ask questions like that. Now that tree will be an increasing tree, it's a binary tree, so it's an increasing binary tree. So, it's an increasing binary tree. If you start with the random permutation, it will also be a random increasing binary tree, which is the same as a random binary search tree, if that's more familiar. So, you have a permutation, and you just insert the numbers one after the other in a binary search tree. The shapes are equivalent. So, that's the easy version. The question I'm asking for these are The question I'm asking for these are: How many bits of memory do you need to store the outcome of these questions? So it's an entropy question. There's a certain probability for each of these trees. So the probability for a certain tree shape to come out of this process. In a simple case, we have an explicit. In this simple case, we have an explicit formula for that, but that's not my point. The question is: what is the entropy of this distribution? So, what's the expected value of the log of 1 over the probability for this tree? Log of one over the probability for this tree. Make sense so far? That's from a data structure perspective, the minimum number of bits you need in expectation to represent such a random permutation in this lossy sense when you're only allowed to ask these range minimum queries. And you can try to match that by constructing a random data structure for that, and I'll talk about that tomorrow. Tomorrow, but maybe in a way you won't recognize this problem. I don't think I have time for that part. So that part is actually known because we know what that is. It's roughly that, and there's a series that expresses what this constant is. The one by you are writing with quite small letters. Oh, sorry, yeah, pardon me. That's 1736. 1736n, if I remember correctly. Now, to put that in context, if you just take all possible binary trees and take the uniform distribution, as opposed to what I get from starting with the random permutation and then constructing this tree, you would have all binary trees, catalan n many, take the log, we get two n bits up to lower the terms. Terms. Right? Okay, so that's so far so good. That's all clean because the combinatorial object is very clean. Now I'll add a second operation. The same, but now you're allowed to ask the maximum as well. So you get a second tree where you can find the maximum, which will not be at the same position. You can, again, You can again do the same construction. Now, the question is: how many bits do you need if you want to answer both questions? I could build separately the two and just add up the space, but that's not optimal. You can do better because those two trees, they are correlated. So the the real question is what is the probability for these two trees combined? And it's all super random. It's all super informal. So I'm asking about the probability of the shapes of two trees which result from building this min-oriented and this max-oriented tree. And the entropy of that distribution, that's the question. So that question for both trees. I guess I should say pi and then And then one over the probability for, say, the max tree of pi and the min tree of pi. That's the formal definition. I guess it's too small to read. I have some examples, some connections. Some connections, but I think in the interest of time, I'll leave it with that. One remark: you can use this construction of trees to define equivalence classes of permutations. I regard all permutations equivalent if they lead to the same tree. If I do this with just range min, a convenient representative for these equivalence classes are the These equivalence classes are the stack sortable permutations. 2, 3, 1 avoiding. Is that right? 3, yep. If I take both, I again get equivalence classes, different ones. Now convenient representative are the Baxter permutations. So there is a lot of structure in those, and you can try to exploit that. And you can try to exploit that. It hasn't helped me with this entropy so far. Just to have it clear: the first entropy question, that's solved, and that's the number. Yes. And that means that you need less information to encode those trees than binary trees. The uniform random binary tree would need two n bits. And this distribution is is very skewed. Distribution is very skewed. It has a lot of weight on the ballots trees and much less weight on high trees or almost paths. And then just to make sure, then this expected value you would expect to be smaller. Yeah, so what we can see, I guess it should be smaller than two times this, right? And the second piece of in that's that's even not the best bound because we know how many Baxter permutations there are. We know that again the uniform case if you just take these pairs of trees, the uniform distribution, you need three n bits. Again, works out very cleanly in this uniform case. In this uniform case. So it should also be less than 3n. And it's hard to do experiments on this and get proper numbers because it's inconvenient to generate something around 2.6 or so, but I'm not sure how accurate that is. But I would just expect to see. Yeah. I'll post the write-up I have. The write-up I have. It's handwritten, so you might not love that. I can type out a few more. So both of these equivalence relations that you've defined are examples of lattice congruences on the weak order. Basically, you have the weak order on permutations, and you want to define an equivalence relation that sort of respects the lattice structure. The lattice structure, the meet and join operations in some sense. Basically, all I'm saying is that these have sort of been studied a lot, and it could be interesting to ask similar types of questions for other choices of these equivalence relations. Okay. I'll ask you what the join, meter join exactly is. I'm not sure I I understand oh hi, is it a pair of uh trees or or is it? Trees or pi for me was the permutation. And so I'm imagining this is a function that takes a permutation and constructs the shape of the tree. And there's one for the max tree, one for the min tree. So the outcome of that, of these two, it's a pair of trees. Just wondering before that, what's the probability if you take trees that are identical? Take trees that are written. In this case? Yeah. So you want trees that are valid pairs of trees, so that there exists a permutation that yields those two trees. You can't combine any two pair of trees necessarily. There are two trees this time, right? So if you ask what's the probability of this tree, Of this tree is true. In most cases, which means the maximum immediately leads to the same tree. Oh, the same tree? I think that's not possible. There's no permutation that yields the same tree for min and for max. Right, because the n and unless the permutation is 1 and 3, right? Yeah, for n equals 1, they will not be at the same place. Yeah, yeah. Okay. All right. Thank you. Then enjoy your dinner and then tomorrow at nine AM we meet for June, June Fulstock.