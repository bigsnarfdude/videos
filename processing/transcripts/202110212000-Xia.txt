Okay, is it does it work now? Yes. Okay, okay. Yeah, thank you. So I just mentioned the symmetric rearrangement for a bounded domain omega, which is just the open ball with radius r. So here the radius r. The radius r just satisfy that this omega sharp has the same volume as omega. And for the function u, the Schwartz symmetrization is just a function on this omega sharp. So this is again, this is a radio symmetric function, which is defined in this way. So from the definition, it's a little bit It's not that obvious what it is, but in fact, from I just draw a picture here. So you have domain omega here, and you have this omega sharp, which is a ball which has the same volume with omega. And you have a function u on the left-hand side. And on each sub-level set, so here you have this red part which gives. Part which gives is given by the sub-level set. And then you just arrange this red part to be around a ball center at the origin, and which has the same volume as the one with the same level of this original U. Then you get this graph of U sharp. Okay, so this is. Of u sharp. Okay, so this is the function u sharp. And of course, by the definition, we have that the sublevel set for each time t, each parameter, each time t, the volume of sublevel set of u sharp is the same as this minus absolute value of u. So here I just use t. I just use t is a negative number. Okay. Yeah. Yeah. For this was symmetrization, which I think is well known. So we have the following several principles. So if you have a function u, which is a supplier function w1p, then you will have that the integral of the LP norm of u is the same as L P norm of U sharp. p norm of u sharp over omega sharp and there is also a poor utility principle which says the w1 gradient u to the lp norm on omega is bigger or equal to the gradient u sharp to the lp norm on omega sharp and there is another useful inequality called hardy eatenwood inequality which said if Equality is that if you have two functions, two integrable functions u and v, and then the product of u and v will have this comparison. So I just give a quick review of the proof for this principle. So for cavalry principle, you just use the layer cake form. use the layer cake formula and you can you can represent the LP norm of the function u to be a one-dimensional integral over this minus t to p minus one then the volume of sublevel sets and then because the volume of the sublevel set is the same with the volume of the rearrangement you squeeze up then you get the equality here Then you get the equality here, and the integral on the then of the integral on this U sharp is just the L P norm of U sharp. So this is a very simple formula. And for this Poirus principle, something more will be used. So first you use this Coer formula. You can represent the Dirichlet. represent the Dirichlet energy to be the one-dimensional integral over the integral of gradient U on the level set or inch level set. And also you can take the derivative with respect to T of the volume of sub-level set. You get this the right hand side, the one over gradient U norm integration on the Uh integration on this level set. And because you have this the property that the volume of sublevel set is the same, then first by using the isoparimetric inequality, because U sharp has the sub-level set, which is just the shape of a ball. So then you will see that the area of the level set, the boundary of the sub-level set will be. Of the sub-level set will be bigger than the one of Yusha. And then, and because the volume for each T, the volume is the same, then by the derivative of the volume function, you will get the one over gradient U norm, the integral over this level set is the same. So, then by using Hoder inequality, we just estimate this the area of Estimate is the area of level set u equal to t by the integral of gradient u and the integral of one over grading u. But the last formula for u sharp, because u sharp, the level set of u sharp is just the round four. So and u sharp is radio gradient, radio symmetry. Gradient radio symmetric. So here you just have for U sharp, you have this equality. And then by using all of the above fact, we will see that the Poya-Usical principle holds. That means the L2 norm of breeding U will be bigger or equal to the L2 norm of breeding U sharp. So here the important thing that we use this as a parametric. We use this as a parametric inequality. Okay, and we have lots of interesting and important consequences for this Schwarz symmetricization, like we have a sharp estimate for this Faber-Crown inequality. So the first derivative energy, initially eigenvalue of bounded domain omega will be Will be a tense is the minimum at the Schwartz arrangement, omega sharp. And also, you have this same Venant principle for torsional rigidity. So the torsional rigidity will attain its maximum also at its Swartz rearrangement. And in fact, this two principles or two inequality just Inequality just follows directly from what we have talked before: the Pavili principle and the Zuto principle. So you just use the variational property of the first eigenvalue and the torsional rigidity. Okay, so and there is another application by Talenti. So he can use this WAS metric to get a priori estimate for Poisson equation with dirichlet boundary condition. So you can compare it with the one of which has the Which has the radiosymetry function v. And here, F sharp is just the arrangement of the function f. Okay. Yeah, this is just some application. So now we turn to this Talenti soul symmetrization. Symmetrization. So first we have this bounded convex domain with a C2 boundary. And we know that the Steiner formula, you can, if you plus, if you add T times the ball, the unit ball to the boundary domain omega, then you explain this. You compute the volume, then you will get the Wk omega, which is called the K-Square Mass integral. So, here, because we assume this omega S has C2 boundary, so this Wk omega can be interpreted as the mean curvature integral. So, the W0 is just the volume of omega, and the Wk omega is just the integral over. Just the integral over the boundary of the mean case k minus one's mean curvature, yeah, and we can define this uh theta k as k mean radius, which is just you can view as a normalization of this WK with some power. And then the exegial facial inequality for queer mass into because we assume omega is a convex domain, so this. X domain. So this example of Fair inequality for queer mass integral, I just mentioned, I just indicate about the isoperic metric type inequality for this queer mass integral. That means you can, if you fix some theta L or W L, then the WK will attain its minimum at the ball. So here just so here K. So here k should be larger than L. Okay, so this is a generalization of classical as a parametric inequality. Okay. Yeah, so then Talenti and Tsou studied some asymmetrization motivated from this Alexandria French inequality. So we have a class of admissible functions which vanishes on the boundary and which is strictly convex and which is C2. And we define this omega sharp k minus one to be the open ball, which having the same WK as omega. That means you just define a ball so that this omega So that this omega k minus one sharp, this four has the same case query mass integral as omega. Okay. And then as a similar, similarly, we can define this k minus one symmetra like Schwartz symmetricization. So here I just call k minus one symmetrant of u. So here the difference is just that we just that we replace the volume of the volume function to be this wk function the the case query mass integral okay and by this definition similarly you have that in fact this wk minus one equals to w k minus one of u sharp k minus one so here theta k is just you have this normalization so you can view it as the same So, you can view it as the same with WK, okay. Um, yeah, the k minus one case is just was uh symmetrization because now you just having the same w0, which is having the same volume. Yeah, so this is a very natural generalization, and you have this uh case Henschin integral, so you have the integral over omega of my. The integral over omega of minus u sigma k of Henshian U. And when k is one, because you have the boundary condition that u equals to zero on the boundary. So when k equals to one, this is just the Dirichlet integral. And k equals to n is a Mauge pair integral. And this Henshi integral has been studied very well in the last century. And also in And also in recent decades. And Talenti and so should follow Poya-Yuska principle. Talenti is for two-dimension and also for modular pair in two dimension. And so generalize it to any K and any N. Okay. So that what this inequality says, it's just say that when you consider this. When you consider this k-henshian integral over omega, then it will be a tense is minimum at a very radiosymmetric one over that is this talented toll symmetricalization over this omega symmetry k minus one symmetry symmetric of omega. Okay, so and also you have equality that Have equality that when equality holds if and only if this omega is a bore and you is radio. Yeah. And so this proof, in fact, when k is just the k is one, you just reduce to the classical Poya-Yusk principle. But in that case, you do not need to assume omega s convex. And here, in fact, the proof just Here, in fact, the proof just used crucially the following two ingredients. The first one is executive French inequality for queer mass integrals, which we just mentioned before. And the other one is about the release work on this Kinhasian operator. Okay, so next we talk this convex dematerialization. So this is So this is another direction for this Schwartz generalization of Schwartz symmetricalization. So we consider a norm on this Euclidean space. That is, you have a positive, even, convex, and one-homogeneous function on this I n. And you can, the following is just some notation here. So F0 is still normal. And here we call this WF. We call this WF is the unit wolf ball. In fact, this in convex geometry, we know that F is just the support function for this WF. And also you call this boundary, the wolf shape. And we denote the volume of this WF as kappa n to distinguish with this omega n, the volume of the unit. The volume of the unit pore in Euclidean norm. Okay. So when this F is just Euclidean norm and everything goes back to the that means this wolf ball is just the unit ball. So everything will go back to the Euclidean case. And this anisotropic dirtier Dirichlet integral or energy is just the integral of some domain omega of F grading U to the and square. And this isotropic Laplacian is the following operator defined by the divergence of this gradient of one half F square. But the gradient is respected to this one half F square. And then F square and then you have this gradient, and then you acting on this gradient u. Okay, then you take the divergence over with respect to x. Then you have this anisotropical ablation. When f is just the Euclidean norm, then here the gradient of one half s square is just the cushy itself. So when you're acting on gradient u, it is just. You're acting on grading you, it is just grouping you. Okay, then you take divergence, it's just the Laplacian, yeah. Okay, and uh, this Avino, Ferhona, Niels, and Tropetti, they introduced this convex symmetrization. And this convex, I think they use this convex terminology because this f a n as a norm, it is just a convex function. Okay. It is just a convex function. Okay, so this is very natural terminology. So, and this convex symmetric, they find that it diminishes the isotropic Dirichlet integral. That means it has a Poya-Usko type in principle. Okay, so in fact, the following is just the definition of convex symmetricization. So, you just take a So you just take a wolf ball with some radius so that the volume of this wolf ball is equal to the volume of omega. And similarly, you can take this convex symmetrization of some function u, which defined on this omega star. And it has the same similar Similar representation that the sub-level set of this U star has the same volume as the sub-level set of U. Okay, so the only difference is that here the omega star is not the unit ball, but the wolf shape, which depends on this norm F. Okay. And they find that. And they find that for this convex symmetrization, the L2 norm of F grading U is larger or equal to the L2 norm of F on this gradient U star. And equality holds when omega is a wolf ball. And here, and also U is also radial. But here, the radial means radial respect to F. To f, or you can say that ux is just a radial function u depending on this f0 of x. Okay. And again, they can prove the following talented type comparison. So I just think I just skipped this. Okay, so our aim is to study this talented toll symmetrization in the anisotropial case. In the anisotropial case, or in this convex symmetricalization case. So, in fact, our motivation still comes from this convex geometry that about the executive finish inequality for mixed volume of two convex bodies. So, if you have a norm, because for a norm, we can reinterpret this norm as a support function of some convex form. Of some convex body. So here the convex body is just, we use the name wolf ball. And if you have another convex body, omega, let's see, has C2 boundary, and then the Minkowski sum of this omega and omega f has the volume, which has the following interpretation, for following expansion. Expansion. So here you have this mixed volume, WK omega and Wf. Here Wf is some fixed but arbitrary convex body, which we assume with some regularity. And so here this WK omega Wf has differential geometry representation, similarly as what we have seen that the WK on. have seen that the double k on omega and all. So here you have the, it can be interpreted as the mean curvature, but here the mean curvature interval, so here the mean curvature is depending on this F, so which is called the anisotropic mean curvature, so which we will discuss later. And then this Alexandria French for mixed volume is. For mixed volume, it's just that similar as before. So you have this WK depending on F and also W L depending on F. And then you have this sharp inequality. And equality holds when omega is a wolf ball. So this is just the very classical Alexandria Financial inequality, but still for two domains, just for two complex. Just for two convex bodies. And in fact, if we interpret it in this way, then this is an as a parametric type inequality, right? Yeah. And this is our motivation. So then we want to study some kind of symmetricism, which really has this Poya-Yuska principle. Principle depending on this factor to this mixed volume. So, because we need the curvature, the curvature, so we need some regularity for this F. So, we let this F to be a strongly convex norm and so that extension of F square is positive definite. positive definite. And we denote this AF, the matrix AF to be the following thing. So you can view it as the sum of all the product this matrix can be viewed as the product of the matrix of Henshi of one half F square and the Hanshing U. Okay. But here, because F is not differentiable. Is not differentiable at zero. So we need to be careful. So the definition is only make sense when gradient U is non-zero. And when gradient U is zero, we just regard this matrix as the zero matrix. And in the case, F is not a Euclidean norm. And when F is Euclidean norm, it's just a great Henshin U. Okay. And we call the And we call the this anotropical k-hension operator as so this s k is just the k-hensh or sigma k okay the the case elementary function on this afu because this afu is defined by the product of two symmetric metrics and the first one is uh is positive definite then it has real eigenvalues yeah this is Real eigenvalues. This is just an excise in linear algebra. And then you have this antropical k-henshin integral. And here when, because you have if you have this property that u vanishes on the boundary, then you can interpret it in different ways. Okay. And when f is clearly known, everything. When f is completely normal, everything goes back to the Henshin U and the Henshin integral. Okay. Yeah. So we just want to look at the symmetricization that omega k minus omega star, which having the same WKF as omega. So in one direction, when k is zero, it is the convex symmetric The symmetricization of the four Italian guys. And when F is in another direction, when F is occupied norm, then this is just the Talenti tall symmetricization. Okay. Yeah. And the other definition is similar. Okay. It just has the property that the sub-level set has the same set has the same WKF. You can just think in this way. Okay. Yeah. So we derived this Poya Lucal type principle for this K-isotropic K-hension. So this is the inequality here. And the notation here is when you have this When you have this star up star, then it is this is depending on this F, and this when this subscription K minus one, then it depends on this WKF. Okay, so this is the notation. And yeah, this is just the result. And this one just generalized the one for convex symmetrialization and also for. Max symmetrization and also for talented soul symmetrization. And yeah, my collaborator Dellapetra Gavitona get this result for the simpler case that n equal to k equal to 2, because this is just the two dimension. So they can use a direct computation. You have a very complicated direct computation. Complicated direct computation, but it still works. And for general case, that is, for general N and K, there is a difficulty that we need to study the K Henshing operator on non-symmetric metrics. So this AFU is not symmetric. So we need to study this thing. So yeah, the following, these slides just says that we can Just says that we can add some p here, so as L P norm, okay. So I just skip this one, yeah. And this one says that we can use the previous Poya-Lutka type principle to derive this anisotropic soap-leaf type inequality with optimal constant. Okay, yeah, in general. Okay. In general, you can derive this one because this F is just a norm. So you can derive this one from the classical Sobliev type inequality with some non-sharp constant. But here, by using this sharp Poya-Yusko principle, it is okay to derive this optimal isotropic. Optimal isotropic sopliff type inequality. Okay, yeah, this is just what this slide says. Yeah, and also a talenti-type comparison principle. And I just give this one. And I will make some comments on this study of for non-symmetric metrics. So we can interpret this SK or sigma k of S k sigma sigma k of non-symmetric matrix in the following way, just as really did. Okay. So then you can formally you have this derivative of SK of with respect to this Aij. You have this following one. And differently, so here this Aij is non-symmetric. So this makes the computation much much more Much more difficult or complicated. So, we have the following two propositions. The first one is a very general one. So, in fact, we can compute this SKIJ to be the right-hand side. So, you can think that if A is symmetric, so this identity is very simple. You can just use some. Use some normal coordinate to get for principal eigenvalue, then you have this kind of identity, and then you can interpret it as the metrics version. But for a general metrics, so this one is not that easy. So we take about three pages to compute this. To compute this identity. And the next one is just like the divergence-free property of this K Newton. This is Sij is called the Newton transformation. And this is the divergence property, a free property for this Newton transformation. And when there is no, the F is occupied, this is also simple. Is also simple, but when you have this F, still, we need to be a little bit careful. Okay, so and we need to study this anotropic integral on this as a tropical radio function. And this just be the formula here. When you take the n as a tropical radio function, then in fact you take everything to You take everything to become the one-dimensional one. And in for one-dimensional case, in fact, there is no isotropic and anotropic the same. Okay, so the formula is the same for one dimension. Yeah, and we need to study the anisotropic coverage of level sets. So here is what we have mentioned: this This anatropical mean curvature mean. Okay, so when you have a closed top surface, you have the unit normal, and the anthro tropical unit normal is just the gradient F acting on this new. And then the anisotropic principal curvature is the differential on this new F. And then you take this anotropic case mean curvature. Case mean curvature. Sorry, here it should be HKF. Okay. Yeah. And we derive the following formula for this HKF on each regular level set. When F is a classical formula due to really. So for level sets, for regular level sets, then the mean. Then the case mean curvature has this formula. And when k equals to one in n such of case, it reduces to the following formula due to me and Kufang Wong. Yeah, and here this case is just the answerpic case mean curvature formula for the level set. And from this, the level set. And from this, you can see it seems much easier than the from the representation, it seems much easier than the Euclidian norm case. Because here Fij or Fi L is easier than the data ij minus Uiuj, like something like that. But it really has this very neat formula. Need a formula. Okay, so we still need to use the first variation formula by Rele, and which was derived also by He Yijing and Li Hai Zhong in an anthro tropical case. So, here that means if you take a look at this sub-level set and of set and of some function u and you take the first derivative of first variation of this this mixed volume Kf WKF then you have this four following things okay it it gives you this SK kappa F which is just HKF okay the case and stropic mean curvature Curvature. Yeah. So I have still two minutes in the two minutes. I just sketch a proof of this Poya use principle to see where this thing, Alexander, what we have mentioned, the ingredients used in the proof. So first we have this Alexandria French, which is the most important ingredient here. Most important ingredient here. So that is, sorry, here I omit this F. Here should be there. There should be F. Okay. So theta K minus 1F and also theta KF. Because this omega T is the, because for any this theta should be the anotropic version of the queer mass integral. Of the queen's integral. And then, by using holder inequality, you will have the following one, following inequality. And the first one, by using the first variation, you get the derivative of theta k minus one. Okay. And then it follows that the theta k minus one f has the following inequality with its derivative. Derivative. And then how to relate it to this Henshin integral? And you just using this Coerr formula. And when you're using this Coerr formula, and then you combine the formula for the K-man, the case and isotropic mean curvature, you have this formula. And here, this part is just the same. Just the same part here. Then you can replace this term, the integral here, to be this one. And you replace this integral to be this one along each level set of U. And then here is just a one-dimensional integral. So you can just compute it. You can just use one-dimensional change of. One-dimensional change of variable, you will easily get it the following one-dimensional integral. And in fact, for the radiosymmetric one, this formula is very easy to get, which we have mentioned before. I think then we get this Poya-Uska principle. I think my time is over, so I will stop here. Thank you. So I will stop here. Thank you.