Thanks for the introduction and thanks for the invitation. It's a pleasure to be here and talk to you guys. So, as Lin Lin said, I'm going to talk to you about a Flamewood approximation for an incommensurate Schrodinger equation. And this is a collaboration with Hua Ji Chen, Beijing. With Huaji Chen, Beijing. So he was actually supposed to be here too. Couldn't make it because of these issues, sadly. So this was a very close collaboration between us and our group. Tim Wang especially was a graduate student at the time and did a lot of work on this. So anyway, originally it was going to be a talk between both me and Huaji and split in between. So sorry if I can't give as good insights into the density functional theory that shows up in this as he would have been able to do, because that's his area of expertise. His area of expertise. So let me just kind of show you the model I'm going to play with in this paper. So this is the Schrodinger equation, but I am going to restrict to being an R D, where D will either be 1 or 2. The one-dimensional case is just for exemplifying the analysis, not so much for the physics. The two-dimensional is trying to aim more for the physics. The obvious thing you'll probably notice right now is that this should be an arc 3, not 1 or 2. Not one or two. And the reason we're going to do that for now is just to emphasize the analysis that comes into play in the inconventory Schrodinger equation just from the in-plane effects. But the intention is to extend into the Z direction, and also we're working on developing the self-consistent nonlinear D of TMS. Of course, there's a big cost, and that's computational cost, trying to upscale into density functional theory and R3. But so this is going to be the model. So, this is going to be the model. V1 and V2 are going to be two periodic potentials in the plane, or if it's 1D on the line, and you've got the kinetic energy term here. I'm going to introduce a smooth-out Gaussian because my objective is to compute observables. I will primarily focus on the density of states for this talk. And also, this is the density of states of objects. So, some form of trace of the smear-out Gaussian of the Hamiltonian recenter at the energy of intra. Re-center at the energy of interest. And so here I is the identity operator. This bar I'll kind of define later. The idea is it's spatially average, so it's a thermodynamic limit. And this is in real space. And the objective is to create an algorithm that approximates this using a plane wave basis. And this is in the actual commensurate settings. There's going to be no supercell approximations in plane. Just for notation, Rj will be my lattice for sheet J. Will be my lattice for sheet J, Rj star will be my dual lattice, and Rj star will be the reciprocal lattice unit cell, so closely related to the bryonzone, and gamma J is the real space unit cells. And by the way, if there are questions in the process, I'm very happy to take questions in the middle since there's a lot of modeling going on. Okay, so B1 and B2 are going to be periodic with respect to R1 and R2, respectively. This is the geometry picture for sheep. This is the geometry picture for sheet 1. So, R1 is this lattice, R2 is the red lattice, and the same for the two-dimensional setting. Okay, and so just to kind of briefly talk about the models, so I'm focusing right now on a linear Schrodinger equation and how you can go from that linear Schrodinger equation in real space to observables. Just to kind of get an outside, just a view of all the modeling that's going on, though, at least from my perspective, you have this, first you have the many-body Schrodinger. This first, you have the many body Schrodinger equation, which is described by density functional theory. Which, if you make it conchammed is a significant approximation, a single particle approximation. There's different routes, and we've seen talks on this about how to go to observables, starting from some kind of density functional theory to observables. One route is to create a type-binding model through vinylization or whatever technique you choose. From there, you can create a VM model or a momentum space model. Model or a momentum space model or just a configuration space technique, and you can get observables. Alternatively, you can create some kind of linear Schrodinger, and from there, you can go to BM, as we saw from Eric Kansas, or you can just try. Right, so there's the linear Schrodinger, and what we're going to do here is introduce how to go from here to observables. But the objective for me and Huaji is to go upscale from this back to density functional theory and go straight from density functional theory to observables. Density functional theory to observables using the structural framework I'll introduce here for the linear showing here. Okay, so let's dig in. So I'm first going to go through just some very basic Fourier analysis to kind of give the idea of how the basis is constructed. So I'm going to use psi of substitute subscript C as just a Fourier variable. So this is our basis. So we have a continuous basis because C lives in A continuous basis because C lives in Rd or R2. And then Vj is going to be decomposed into Fourier waves because it's periodic. And so each Vj will have a decomposition into a different Fourier basis. And we're going to be able to describe how the Hamiltonian talks to these different wave numbers through two very basic relations on Fourier analysis. You take the product of two Fourier waves to get a Fourier wave, which is the wave number that's the sum of the other two wave numbers. Of the O2 wave numbers, and the Laplacian acting on a wave number is simply diagonalized by the plane waves. So you get this eigenvalue of the Laplacian. Our operator acting on a plane wave is simply described then as the Laplacian part acting on the wave number, and then the shifts coming from each of the Fourier modes from both of the two lattices. And so what you see is that H maps a single plane wave, indexed by C, to a set of plane waves. Here we have C. To a set of plane waves. Here we have C and then we have C plus G, where G can be in either of the two dual lattices. Okay, so this is just an example of what it looks like in 1D. So if I had just two cosine potentials with incommensurate relationship here, and then I'm going to compare it to the periodic, this is what applications of H would look like. You start with some frequency, say 2 here, and you act the integer operator on it here. You just get more a mix of plane waves. You just get more a mix of plane waves. If you act on it twice, you get a higher mixing of plane waves. If you do the periodic, it looks similar, but it's different because there's actually only one dual lattice coming into play. So that's kind of, yes. This is all in configuration space? This is actually in real space. I'm just taking a plane wave and acting a Hamiltonian on it and plotting it in real space. So I just picked a cutoff from minus 4 to 4, but you could do this over the whole real line. And this is the 1D problem. And this is the 1D problem. So, one thing that you can notice here is that there's more frequencies showing up, right? Here's like double humps, wherever there's single humps here, and that's because there's two sets of frequencies. So, your cartoon before of your system showed two layers, but this is still two layers, right? The bottom set is the one layer. That's coming from this guy. This one's two layers in the sense that there's two potentials. Now, it's not two layers in the sense of vertical displacement because I've taken out the Z variable. I've taken out the z variable. But later on. Or degrees of freedom. Or degrees of freedom if you decided to do it that way. You still get two layers if you have a z direction and you put one potential of one z and another potential at another z, like localized that way. Which we haven't done yet in these first pictures, but we will do later. Yeah, good questions. Any other questions? Okay. All right, and so I'm just going to repeat this. Alright, and so I'm just going to repeat this relationship since we use it some more. So, this is just that same picture from before in this 1D cartoon. So, we have the operator here, it's just the Laplacian and then four wave numbers. And then if you act H on psi, you just get the shifted C. And what's important is if you start doing higher order polynomials, you're going to see that actually as you include all these shifts, you're going to get all kinds of plane waves like this. So, two pi times an integer plus two pi alpha times an integer. Plus 2Ï€ alpha times an integer. And they can be different integers. And so what's really happening is you're getting a higher dimensional lattice that's in play. It doubles your dimensions. So this is for acting a polynomial on h. And the reason we want to do that, recall, is if I go back to the very early slides, our goal is to do a smoothed out delta function acting on EI minus H. Delta functions centered at E can be approximated well by chip polynomial expansions. Polynomial expansions. And so, really, what you're imagining is a bunch of polynomials acting on this Hamiltonian. And so, if you want to understand what that object is doing, it's mixing all these plane waves together. And it's really mixing it in this way. So, if you start with the wave number C, it will feel all of these plane waves. And so, I index my degrees of freedom for a starting wave number by the direct sum of the two lattices. And so you double your dimensions. W dimensions. So the upshot is we have a continuous basis, but this basis is discretely interrelated. And so the objective will be to make a matrix that describes all the relationships between the plane waves starting from some plane wave. And then we'll have to bring them all together at the end to create a density of states. All right, so this is our Hamiltonian written acting on a single plane wave, as I mentioned. On a single plane wave, as I mentioned before. So you just have all the shifts by the reciprocal lattices. I'm going to use bold G to mean it's a collection of two reciprocal lattice sites, since that's indexing my degrees of freedom. And that will correspond to the wave number starting at C then moved by G1 and G2. And what you could imagine, now this is formal, this is more just to give the intuition. Of course, this can all be done in a rigorous way, acting on test functions and that kind of business. And that kind of business. But if I start with some wave function, let's say, and it's a linear combination of the plane waves that can talk to each other, so CG will be the coefficient, G living in the dual lattice, the product of the dual lattices. Then I can turn this eigenvalue problem, so E minus H acting on psi, into E minus a matrix acting on the vector C, where C is the coefficient of this wave function. Wave function. That matrix is described here. So this is the wave function. This is the matrix that bookkeeps how these plane waves talk to each other. So if you start with a plane wave indexed by G prime, which would mean psi of C plus G1 prime plus G2 prime, it talks to psi of C plus G1 plus G2 through this Hopping function, this Hopping relationship. And so what you find is that this eigenvalue problem is equivalent. finds that this eigenvalue problem is equivalent to this eigenvalue problem. And I didn't write equals because, well, one, I want to relate it to the actual Gaussian acting on it. And secondly, there's not necessarily point spectrum. So I don't want to write down h psi equals e psi because that's too much like lying. Okay, so you have this relationship that somehow this is directly related to this. And it turns out that if you just do polynomial orders of e minus h acting on this, the same thing happens here. The same thing happens here if you try and write how the bookkeeper of the how the related numbers are talking. And what basically that means is that the functional calculus carries through. You can just act the delta function on this operator, and you will get this one as the way that it's describing how wave numbers are talking to each other. So the point then of this slide is that at the end of the day, if you want to describe, if you want to spectrally look at only energies near E for epsilon small, Near E for epsilon small, this Hamiltonian. It can be completely described through this discrete Hamiltonian. Okay. And so now I'm going to kind of at least tell you the theorem. So this is the thermodynamic millinet of the density of states. The way we define it is we just take an indicator function, so it's one on a ball. You smooth it out a little bit, apply it to both sides of your operator, take a trace, re-normal. Renormalize and take the limit as R goes to infinity. So a classic thermodynamic limit like you would expect. That can actually be rigorously shown to exist. And furthermore, it has this formula. So you take that discrete formulation, starting at some wave number C, it feels all of the Gaussian effects, and then you just project onto a single degree of freedom of your four-dimensional lattice, or your higher-dimensional lattice. And then because this And then, because this is a continuous basis, you have to integrate over all bases. Maybe a physical intuition for why this is happening is if C describes a wave number, what you're doing is you're projecting your Hamiltonian onto that basis in some renormalized way and then integrating over the whole basis. That would be kind of the intuitive picture for this decomposition, which can be done out rigorously through some careful Fourier analysis and operator theory. This assumption also for now includes. This assumption also for now includes that the Fourier modes are exponentially decaying, which is technically a strong assumption. We're assuming analyticity basically of V1 and V2. If you start weakening that assumption, you're just going to weaken your convergence when you do algorithms, which I'll describe soon. Any questions so far on the formulation of the density systems? Yes. You're assuming H is like a Schrodinger type? Sorry, what? You're assuming H is like Schrodinger type? Right, so this is the same H is. Right, so this is the same H as before. This is minus Laplacian plus V1 plus V2. V1 and V2 are analytic potentials on R2, or R1. And so this H hat of C is exactly this H hat of C. So it's a matrix acting on R1 star cross R2 star, mapping to the wave functions acting on R1 star cross R2 star. All right. Okay, so the next thing to do is how to build an algorithm. And now that we actually have a formula for the density of states, and we have it described through an integral and a matrix, the natural idea is, well, truncate your matrix, discretize your integral, which is exactly what we're going to do. So the question is, how do you discretize it well? How do you truncate the matrix well? In the most efficient way. So, the first thing I'm going to talk about is the matrix truncation. And I'm first going to tell you how not to do it. And that's the naive approach. Take a G in your cross product as dual lattices and just make them both bounded by a constant. So that'd just be just a finite truncation. Centered, we know we're projecting onto a central degree of freedom. So a circular cutout around that degree of freedom might seem to make sense. The reason it doesn't is because of the The reason it doesn't is because of the kinetic energy term. High wave number means high kinetic energy, and so you expect it to contribute less to low eigenvalues. And if you just do a finite truncation, you don't take account of that. So let me go back to the 1D setting and kind of show you what that looks like. So imagine if we just did a finite truncation R. And I'm going to show you what your degree of freedom space looks like as a function of the wave number. So the x-axis is the wave number for different Rs, and see how your basis expands. And see how your basis expands. So if you just have y equals 1, you just have one degree of freedom centered at, maybe if you start at c equals 0, it would be c equals 0. If you increase that radius, you just see you start expanding outward and you're somehow discretizing. As you expand more, you discretize finer, but you also keep expanding. If you keep doing this, you're basically going to roughly sample high wave numbers just as much as low wave numbers, even though you know high wave numbers don't contribute as much to the Contribute as much to the spectra of interest as the highway numbers. So there's a massive inefficiency happening here. We're doing a full four-dimensional lattice. Or if you're, in this case, it's a two-dimensional lattice because R1 star and R2 star are H1D. But if you're in a two-dimensional system, this would be a four-dimensional lattice, and you're just doing a circular cutout of that. It's going to grow very fast. And that's just your degree of freedom space, right? You have to double if you want your number of matrix entries. All right. All right, so let's look at that degree of freedom space. This is G1, this is the G2s, and you just plot your degree of freedom space. And what you notice is as you go to G1 plus G2 higher, that's increasing your kinetic energy. You go back to the matrix, your kinetic energy term is C plus G1 plus G2 squared. So that's growing with G1 plus G2 quadratically. So what we So, what we can do instead is truncate our G1 and G2 such that G1 plus G2 is less than some number. And so, that's in some sense an energy truncation. And then we just, we had like an infinite strip in our degree of freedom space. And so then you just add another truncation that stops that strip from growing. And that's the L truncation. So we'll just make it G1 minus G2. And we just call it W and L because, I mean, you look at the picture, you're going to need L longer than W, it's width and length of a rectangle, basically. W, it's width and length of a rectangle, basically, at least in 1D. In 4D, each of these would correspond to two dimensions. And this is how the degree of freedom space grows. So now it's going to have L and W. W does not need to be as large, so I'm going to show you some pairs of L and W truncations and what degree of freedom space looks like in planewave. So this is again 1D, C is the wave number corresponding to your degrees of freedom of G1 plus G2. So that'll be your starting wave number. So that'll be your starting wave number plus g1 plus g2. So if you truncated by w, that's going to truncate by default how far your c can go. And so I show you first a low l and w, you just have one degree of freedom. If you fix w, say seven, it's decently high, this just becomes a straight cutoff here. Your degrees of freedom don't grow in momentum, but as you grow L, you start sampling more. Grow L, you start sampling more and more finely your wave number space. Yeah. So can you remind me again what's the justification for truncating with Del rate? Yeah, that's a great question. And I'm going to get into the numerics of that. It's actually a much slower convergence rate. And the convergence rate will actually depend on your Gaussian smearing width. So it's a lot slower. And it's based on Coomps-Thomas type estimates because the idea is you have a matrix that's describes hoppings. Matrix, this describes hoppings. And so for large L, if you look at a degree of freedom at a high L and on low L, you have to hop a lot to reach there. And so that controls your decay. But it's the same principle of decay as a regular tight binding model, just with nothing else happening. If you just took a tight binding model, let's say you wanted to do local density of states somewhere, you do a circular cutout, you'll have edge effects that comes into your local density of states, it's exponentially decaying, but it depends on your spectral resolution. It's the exact same here. Your L-trocation is doing the same thing. Your L-truncation is doing the same thing. You're hopping, even if it's in momentum space, there's still, you gain nothing from the fact you're in momentum space. Your convergence rate is still very slow. So I've seen this before when the hopping is like kind of small, but you're saying that it'll converge eventually, even? So even if hopping is small, if your kinetic energy is small as well, everybody still talks to each other just as strong. It always matters what's the ratio between this and this. Yeah, so if the hopping, if the bees The hopping, if the bees are much large compared to the kinetic energy, then how do you argue for truncating? I guess maybe. Just the hopping distance. I mean, even if you did like a discrete Laplacian, you can truncate at some point. And that's because if you're looking at something kind of in the center, and that's, it's a very slow decay, and it depends on your spectral resolution you want. But at some point, if you're just projecting one site, it will still decay, even though nothing's nice. Everything's ugly. Everything's okay. Yeah, so there's no real perturbation theory happening. It's just Kumspon assessments. Yeah, great questions. Any other questions? Okay. Right, so the intuition for the L direction is everyone's talking strongly to each other in the L direction. The W direction is where you're going to have an energy decay. And then energy decay in the W direction is very parallel to what you're doing if you're doing like, say, McDonald's model. Parallel to what you're doing if you're doing, like, say, McDonnell model or something, and you truncate your cone. You're growing in energy in momenta because this linear dispersion. Here we have a quadratic dispersion. So, if you cut off there, you're not losing much by cutting away those higher modes. While here in the L direction, you don't have any gain. So it's going to be a slow decay. So, a general L is going to be much larger than W unless you have a very wide Gaussian smooth. Okay. Okay, and so that's the degree freedom picture for the 1D system. All right, so then if you want to make a matrix truncation, you just define a degree of freedom space based on this truncation. So you do W, you do L. You say I'm only going to keep G1 and G2s within this set. And then what you're going to do is restrict your matrix to only including these degrees of freedom. So you'll just throw away everything else. And then you have a finite matrix, starting at some weight number C. Tricks, starting at some width number C. And then this will just be the discretization. So I'm going to have discretization steps of width h. I'm also, because wave numbers that are higher than w, I'm pretty much throwing away, that's also going to give me a natural truncation for the integral itself. I'll throw away all the wave numbers that are higher than w. Keep all the ones within the w ball. And so then once I've done that truncation, you just discretize it. This will be the discretization set. Set. And so when we started with an integral, now we just have a discrete sum where we had an integral, and we have the Gaussian acting on now a finite matrix, project onto 0, 0 sides. And so this object is now computable through, for example, chip-tip polynomial expansions, which would be much more efficient than eigenvalue solves because these matrices get too large for eigenvalue solves, typically, especially when you're in two dimensions. When you're in two dimensions. Okay, and so then this is the error decay. So this is our approximate density of states. It's a function of energy. This was the original one. And again, this is density of states with Gaussian smearing. So all the error analysis will depend on the smearing because the smaller the smearing, the finer of spectral information you're picking up from your single particle model. The parameters are the discretization and these two cutoffs in the plane wave. And these two cutoffs in the plane wave space, the W and the L. So the W, as advertised, decays very fast. It's exponential, and it only depends logarithmically on the resolution. The L, as I was mentioning to Jacob's question earlier, it actually has an epsilon in the prefector here, or in the exponential. And so that means actually that you're going to need an L that's inversely proportional to epsilon in order to get your decay that you want. So that the L direction would be quite expensive. L-direction would be quite expensive. Satellite discretization is also expensive, which actually is not surprising because you're in wave space. And so the mentality in wave space is you're kind of like in your band structure space. So if you're going to have band structure-like features, the features are also as thin as a band, which means as thin as your Gaussian smearing. So it makes sense your discretization is going to require high accuracy. Yeah. Independent dimension? Yes. This bound is independent dimension. This bound is independent of dimension. What's not independent of the dimension is the scale of your matrix. And so when you do computation cost, the D is going to show up. So L and W are your momentum colours. Yeah, so one is like the high energy momentas and one is the one that's sort of densely sampling your momentum space. What allows you to prove exponential kind of data? This one? Yeah, or both. Either you talk. Right, so this one's just Kumbs Thomas type S and then you have a hot. This one's just Coombs-Thomas type estimates. You have a discrete hopping matrix. The hopping is fixed. And so, if you want to see the talking of two guys far away, it's a Coombs-Thomas. The other one is the fact that you have these kind of local couplings, but the regions you're coupling keep going at the higher energy on the diagonal. And so what happens is basically eigenvalue perturbation theorem. That's a sparse matrix type argument. It's yes, it's like a sparse matrix type argument. I guess the easiest parallel I would say is if you were imagine you were doing like just the matrix you know minus five minus four minus three minus two minus one zero one and then you just had an off diagonal term of like say a third all the way along both diagonals and you want diagonal values here near say energy zero this is going to perturbatively talk to this one and the same idea is happening here you have high wave number wave numbers that correspond high wave numbers that correspond to, or sorry, degrees of freedom corresponding to high wave numbers have high energy on the kinetic energy term. And so it's like having a large part here. That would be the parallel. And so this talks perturbatively down here. It can be rigorously written out. If you want to see, well, there's a paper on this already on, and then just say Margan is applied to t BM type models or if you're doing like momentum space models. It's not a regularity, it's not a matrix. This is a result that applies to incommensurate cases. Yes. So where is the incommensurability showing up here? It has to show up somewhere. Yes, the incommensurability is showing up in the degree of freedom space, which plane waves are talking to each other. If you had periodicity, you would have truncated. You would have at some point been repeating wave numbers, and then you would have had to change your degree of freedom space accordingly. Accordingly. For example, you can't say, let's go back to. The computational domain depends on the commensurability. Because otherwise, for example, say this lattice site might be the exact same wave number as this one, and they should not have been booked as different degrees of freedom. Right, but when it's not periodic, you still have, in common several, you still have coefficients that are close to almost rational, or that income type that conditions that tell you how irrational these ratios are or not. The ratios are all along so. Do they see they show up in in like the calculations? In in in the way you describe the idea of spaces. I mean if you were doing like a rational approximation to an incommensur, it says something like this or something. Yeah if you do a rational approximation then in principle what you're doing is more, let me go back to the integral, more densely populating this but discretizing this in some sense. And you would also introduce a finite truncation on this automatically by periodizing, but as you do higher order But as you do higher-order rationals, then your matrix gets bigger and bigger and bigger. So as you have rationals approaching an irrational, this matrix will go from finite towards infinite, and your degree of your sampling will go from finite to infinite and become uniformly sampled to normal. That's what you expect to see. Alright, so based on these decay estimates, you can estimate how large of a matrix you'll need. And I mean, this is just the, if you have an n by n matrix, the n is going to scale roughly like this, epsilon to the minus d, and then log of epsilon inverse to some power. So what's nice here, opposed to just the direct cutoff, is that you've saved half your dimensions. This would have been epsilon to the minus 2d if you were doing the R cutoff, because you would have had this type of Because you would have had this type of decay in all directions by that analysis, and you would have had to have a really big matrix. So we kind of have the dimensions of our matrix. The pre-factor, so to speak, is this W, which is logarithmic. And so that's still decently large, but at least we've reduced ourselves to a two-dimensional real space in terms of asymptotic scaling with epsilon. Still expensive, though. I'm not going to pretend otherwise. So it's choosing L to be different than W? Yeah, so this is if you pick L and W. Yeah, so this is if you pick L and W and H to try and match these three error terms asymptotically with epsilon. And that's where you benefit it. Yeah, that's where you benefit. So you pick L inversely proportional to epsilon with the log in there. W just has to be inversely proportional to the log, and H is inversely proportional to epsilon. So the H and the L are still ugly, but the W is having one out of three. Okay, this is a little bit of comments on the Okay, this is a little bit of comments on the proof, but we already discussed that a decent amount, so I think I'll skip that and start going to some results. So, this is some density states pictures. Now, we haven't pushed the numerics super hard yet, so you're going to see definitely a lot of wiggling right now. This is for a fixed W truncation because we actually observe W decay is just really fast. To be honest, it's probably faster than what I showed you because I was emphasizing the exponential decay, but the Laplacian also behaves quadratically. Quadratically. This is for various L's. So in 2D, we didn't push the L as far, naturally, because it's more computationally expensive to do two dimensions. The 1D even pushed a bit farther, and you can see the convergence that's happening in the density states for a toy model. This one had a periodic Gaussian potential for the V1 and the V2. The next parts of the convergence I want to show you is the L and W truncations. And it's much less convenient to do density of states because of how local density of states is. Density of states because of how local density of states is in spectra. And so instead, what I show you is just the total energy. So a fermi-Dirac distribution acting on H, taken thermodynamic limit trace. So this is H, the discretization error. I show you from different betas. So beta is inverse temperature. So as beta gets larger, that means temperature is lower, and your computation is converging slower, as you expect. Same thing over here in L. You're going to see your decay rates are. L, you're going to see your decay rates are much slower for higher betas, and then faster if your beta is more smooth. So, this gives you kind of an error control here. W truncation is just so fast, it wasn't even really worth showing. Okay, so that's just the basic method for computing density of states. The next thing I want to tell you about is some other observables you can get out of the same system. So, the first one is kind of just almost obvious because if you look Most obvious because if you look back at our Hamiltonian, we just said, well, our basis is to take the Hamiltonian and project onto a single wave number. You can think of that as a local density of states in plane wave space. So we simply plot that as a function of wave number and energy in the 1D case and get pictures like this, which are decently parallel to band structure. And so this is an object we're quite interested in and potentially other ways of bookkeeping this wave number. Of bookkeeping this wave number space, so it's easier to compare to Moray systems. But the idea is, out of objects like this, we can start trying to compare towards band structure. And when we're doing actual physical systems, like say twisted bilayer graphene, maybe at a magic angle, these pictures will be much more interesting to see. This is for quite a bit of smearing, which is why you mostly just see a parabola with a little bit of features. But we could push the accuracy. You can see the L and W are not very carefully chosen yet. The L should be probably a lot larger. L's should be probably a lot larger. We want to get a good convergence. So, this is like a projected band structure where you project the local density of states onto some. Yeah, we're just plotting the color is just what's the local density of states. The x-axis is the wave number. This is 1D. And then this is the energy axis. So we're just literally plotting this object as a function of energy and wave number. And are these like little mini gaps in it you see where it kind of goes dark every time? It kind of goes dark every year. Yeah, this is where all the coupling is happening between the two layers, is in this region. Yeah. Right now, a lot of it's smeared out, which is why you can't see a lot, because the R epsilon is 0.5. Okay, and in the interest of time, I won't bother talking about more details about mathematics, but you can describe this through Fourier transforms and Schwartz functions and all of that jazz. As what you can also do is describe the operator through its kernel. It's very related to what we were already doing, but the density of states becomes the integral of the diagonal of the kernel, spatially averaged, where g is a smoothed out delta function. The local density of states in real space can be described as just evaluating that kernel at a single entry. So that would be like taking your del epsilon E acting on H and then just projecting onto a single spatial. Projecting onto a single spatial variable. The density also comes out of this type of relationship because you do the same thing except you evaluate xx where instead of a smoothed out Gaussian centered at an energy, you just put the Fermi-Dirac distribution. And so this is the natural object for the electron density. Now there's an explicit formula for that kernel. So the local density of states, you can also describe by You can also describe by, it has this kind of formula. So it's basically, let's see, right. So this is the kernel. These are basically some kind of Fourier modes that describe how the G's interact with a plane wave. And so in particular, if you just plug in x for y here, so that you're computing a local density of states, what you realize is x times g1 is basically x modulated by the lattice g1, right? Because if you multiply by g1, Right, because if you multiply by g1 and then you're doing an exponential on that, that means you're modulating. And so, what you can do is rewrite local density of states in configuration space right away through this kernel relationship. So let bj be the modulation of x with respect to sheet j. So basically what that looks like is if you take an x in your plane, you look at the vector corresponding to where he lives in the unit cell of sheet 1. And b2 is the vector saying where he lives in the unit cell of sheet 2. Vector, say where he lives in the unit cell of sheet 2. And then you say, okay, your local density states at b1, b2 is, and then this is the formula. There's some formula here for mg, which is related to the hat integrals. And then, and this is just the Fourier decomposition of it. And then here are some density pictures. These are preliminary calculations using self-consistent density functional theory, but for not very large L's and W's. So take it with a grain of salt. This is with a grain of salt. This is with an LDA potential and a Yukala potential and a pseudo-potential for V1 and V2. This is the density of states for a fixed Z. This one is for Z near layer one, so you basically just see the layer one lattice. You would see a similar density picture if you were doing layer two. And then finally we stick a Z in the middle and so then you can see both mores showing up, or both lattices, and you see the Moray pattern. And then this is again, so for a single sheet, and then for a mixed, this is using a GTH pseudo-potential and Ukawa potential. Trying to a very early preliminary calculation for twisted bilayer graphene at five degrees, again low L and W's at the moment, and seven degrees of freedom in the Z direction. So very preliminary calculations, but they're including self-consistency. And so then just finally for future work, we want And so then just finally for future work, we want to develop the code so that we can develop the self-consistent field theory and say functional theory, study the convergence, develop the algorithm to handle all the features that come into play, such as the L-direction, and then potentially how it relates to type binding models and compare the numeric results and see how DFT compares to type binding generated by binarization. Thanks for your attention. Oh, the second system. So you use that for Lukawa? Is there any hope to extend that to Coulomb? Yeah, that's very hard. We threw in Coulomb just to see what happens and it didn't converge so far. So right now it's Yukala, but we're trying to see how small we can push that M parameter in there right now. Parameter in there right now. That's what we're thinking. And we have some other ideas of how to try and take a limit with M and things like that, but it's all very preliminary. We still have a lot of planning to do. I don't know if you have any thoughts, but happy to hear. Yeah, expensive, difficult to agree. Just a naive question for Eric and Daniel. Can you compare the models that you're both solving? Yeah, that would be great. Yeah, we shouldn't prefer that. Actually, no, just one of the assumptions. Yeah, no assumption. Okay, so you can go to code channels that this is this is more elaborate model. For now, yeah. Well, these pictures do have z dependence. Though preliminary, I mean the degree of freedom is seven. So there's a take a large vacuum and then periodize in the z direction. I'm just asking, are they the same model that you're solving? Just see if you agree with this if I understood things correctly. This, if I understood things correctly, if you say it's accurate, Eric, that right now you have like a density functional theory that builds a linear Schrodinger model and then you solve the observable model through the, or rather you derive a BM model through the linear Schrodinger that's derived from a DFT. And so my goal is to go from like a DFT self consistency over to observables. The main novelty is the go. The main novelty is the Gaussian, right? But the theorem, it's really any decaying analytic function. No, because I mean the theorem itself, yes, this one. It doesn't have to be a smooth out Gaussian. I put that there so it's more intuitive. It's more like you want a G and then you want it decaying for a strip along the real line. But yeah, it's a smoothed out function. There's no, I'm not getting any. Smoothed out function, there's no, I'm not getting any information about continuous spectrum, point spectrum, or any of these kind of features. I can't get any of that. All that's a mystery to me. Which is why I avoided writing down the eigenvalue problem. Yeah, but so basically, it's a formula about the entire x of the microphone. Yeah, in a weak sense, yeah.