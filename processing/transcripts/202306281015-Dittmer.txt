Thank you. So, maybe I start with a little bit of background on this project. Before I joined the group with which I did this work, in the very beginning of the pandemic, lots of papers came out, which all basically said, okay, we are doing machine learning and we are saving the world now from the pandemic. And they looked into those papers, and most of them really didn't stood up to what they were promising. And that basically became And that basically became the theme of our working group that we tried to look into: okay, why didn't this work out? And stuff like that. Because it should have been the perfect moment. We had lots and lots and lots of data, and we got it in a very short timeframe. And yeah, I mean, it should have worked, but it didn't. So when we got our data from the hospital in Cambridge, then we, yeah. Then we want to do it better. And basically, that was my job. So we got the data, people gave it to me and said, okay, do something cool with the data. And I heard a talk a couple of months ago where one of the authors said, if you haven't worked with hospital data or if you haven't worked with data from biological experiments, you have no idea. Biological experiments, you have no idea what messy data is. And I who utterly agree now because it was really horrible. So I focused on two projects. And yeah, one of them I want to talk about inspired by this data. And the first one was, okay, how to deal with this messy data. And that was turned more into a software engineering project, actually. So we wrote a paper about essentially software engineering for data science. Software engineering for data science, how to approach that, how to take ideas from software engineering and apply them to data science. And that was like, okay, how can we make this data that we got from the hospital operational? And the other question was, okay, now that we know how to get this data operational, what's actually useful to predict? And that wasn't that easy because, yeah, like I said, the data was messy, which meant it was also kind of hard to figure out. Was also kind of hard to figure out not only what would be a useful thing to predict, but which labels are actually accurate so that which data is that we which of the part of the data that we have is actually so accurate that it's useful to use as a label. And we looked at a couple of things. So for example, we thought about, okay, it's probably useful during a wave to figure out will this patient need ventilation? Patient needs ventilation. So we looked into that and the labels were, so we tried to do it, but we concluded it's actually really hard to get reliable labels on that. Even though we worked with lots of clinicians, it was almost impossible to do. Another label that we looked into was: will this patient go to ICU? And that was also quite tricky. Was also quite tricky because at times the hospital's ICU was overflowing. So they opened other parts of the hospital or basically converted them into ICUs as far as possible. And that was really hard to incorporate into the data because you have to look into lots of things. And yeah, even though our data was really comprehensive, so it was essentially like the dream that we always hoped for, like including. Like including even the number of the bed, or so the yeah, so the bed number and then the patient was, we had all of that. But it was really hard to make sense, okay, which parts do we want to use and which parts are reliable and stuff like that. So what we finally decided on, because that was quite reliably recorded, was will the patient die? Will the patient die? So that's kind of hard to miss as a label. And yeah, usually we then had a desperate precaution. So we focused on that. And the first thing we tried was to predict, okay, will the patient die within the next two days? And that was kind of hard. And we tried, then after a while, we decided, okay, maybe other timeframes are also useful. So two days was kind of interesting because it made. Days was kind of interesting because it made planning for the hospital easier. But also, the UK had lots of laws around 28 days. So we decided, okay, maybe we can predict it for 28 days. And that was much easier. And yeah, at first that made us quite happy, but then I really got skeptical about it because, okay, yeah, there are laws around 28 days, but it's kind of arbitrary to say 28 days. It's also kind of arbitrary to say two days. And so I Days and so I tried to predict it for different time frames, and sometimes about was harder, sometimes about easy was easier, and that basically led us to rediscover what statisticians have been doing for like decades, and that's called survival analysis. So focusing on many different timeframes at the same time. And this is basically how this project got started. Okay, maybe as a disclimber, so this is about So, this is about medical imaging. I will have medical data. I will have imaging data, but not medical imaging data. Okay. First, I quickly want to talk about what is survival analysis because, like I said, I didn't know about it until we rediscovered it. And then some of my colleagues point out to me, okay, hey, we've been doing that in statistics for like decades. Then I want to talk about the Then I want to talk about the Cox model. That's kind of the, I like to call it the linear regression of survival analysis. Why I call it that, we will see a bit later. Then we will move on to, okay, what model do we use or which model did we came up with and connect that to, okay, going back to the Cox model. Because if you read any paper on survival analysis, there's a really good chance. So if you look into any paper on survival analysis, Yeah, if you look into any paper on survival analysis, you will definitely find the Cox model in there. And there's a really good chance that the Cox model is the only model that you will find in there. So it's really, really important. So we want to connect our model back to the Cox model. And since we want to connect it all to deep learning, we also have to talk a little bit more about how do we train the thing and that connects to loss functions. And then we look into numerical results and the conclusion. Cool. Cool. So, what is survival analysis? I always like to start with the data because that's where you usually also start. And in survival analysis, the data looks a little bit different. So, if we look into normal supervised learning, we usually have labels, we usually have features X, and we have some labels Y and we don't. And we have some labels Y, and we don't have labels Y here. Maybe you can think about the label as EI or TI, but you actually need both. So what's what here? So we have, like I said, we have the features. So that's, for example, what's describing a patient. But you can also, maybe I should mention that also applies a verbal analysis to a non-medical setting. For example, you can analyze Analyze failure of a material in some part, yeah, in mechanical engineering and stuff like that. Okay, so we have these features. So we have a data set here, and these features come from some R2DN. Okay, that's not the microphone. Where's the converter? Microscope. Where's the converting? Ah, here we go. Okay. Okay. So we have these features here from R to the N. And we have these E here, which are from 0 and 1. And we have some T time that is positive. Okay, so these are the. Okay, so these are the features. And the time T records, okay, this is the time of latest recording. So well that's really a matter of taste, I would say. Or yeah, lack of taste. So, and we have this I here. And we have this i here. So this i um notes whether the time of the latest recording was the time of death. So do we have a death record at this time key? So we have some study or the patient comes into the hospital or some study starts, time passes and maybe the patient dies at some point. And if the patient dies at point T, then we record time t and we record a one year. Okay, and now Okay, and now there's a special thing about survival analysis. This is why we have a triplet there. And that is something else can happen. And that is the patient leaves the hospital, maybe because the patient got fine or some other reason or leaves the study. Maybe the patient moves. And that basically is the patient basically vanishes. We don't know what's happening to the patient. And that's called censoring. And then we And that's called censoring. And then we record a zero. And we also record the time t, but that's the time where we, yeah, as the latest record, um, that was the last time we saw the patient. So these are the possible outcomes. We have a patient that is leaving, then we're recording a zero, or the patient that's dying, then we're recording a one. And we always have this team. Okay. So, um So that was the data. Let's talk about the goal. Like I said, I was really annoyed about this. Okay, we can, it's hard to predict two days. It's easy to predict 28 days. Something in between has a difficult, different difficulty. Why focus on these number of days? And what we do in survival analysis is we actually want to predict a curve like this. So we condition on the features that we have and then for the And then for each point in time t, we want to have the probability that the patient is alive at the time t. So on the horizontal axis, you have time, time is passing. And we always assume that we start a patient with a patient that's alive. So we always have the probability of one that the patient is alive in the beginning. And then unfortunately, we don't have resurrections. So it's getting monotonically decreasing in time. Is getting monotonically decreasing in time that the patient is alive. So we have a monotonically decreasing curve. Yeah, if you have like really long time frames, you obviously can assume that the probability that the patient is dead in the end is 100% if we talk about decades, if we look at some study on the population level. But in principle, it doesn't have to go to zero. Okay, so this is what we want to predict. So this kind of curve. kind of curve. Okay, let's talk about what people usually doing. And what people usually are doing is using the Cox model. So the Cox model, yeah, like I said, I like to call it the linear regression of survival analysis for different two different reasons. Tries to predict this curve by, yeah. By using this approach. So we are guaranteed to be positive because we have exponential here. Then we have the split where we have one function that looks at the features and another function that looks at the time. So we split up our two components here. And if we look at the time-dependent component, then people usually either do some lookup table or Either do some lookup table or some use some splines to implement that. But what that essentially is, is it's called the baseline cumulative hazard function. So if you look at the average patient in your data set, then that's the average mortality of the patient or how the patient will develop. So, and that thing has to be monotonic increasing. That thing has to be monotonic increasing because it's a cumulative header function. Often people also define the non-cumulative header function, and then you can define this one by an integral. Okay, and the other thing is we have a fixed point at zero. So if we look at time zero, then we want to have the value zero here, because then the whole thing here becomes zero and we get a one. That's a property that I talked about that we assume. Property that I talked about that we assume that we start with patients that are alive. Okay, so the other component is one of the reasons why it's essentially linear regression is because there is a linear regression here. So we have this part that looks on, focuses on the features, and we want this to be positive. That's why people usually use the exponential again, and then they use a linear regression here. And then they use a linear regression here, so a scalar product. So we have the features, and we have some weights for the features, and we weigh the features. And the larger the feature is, the deadlier, essentially, the feature. So if you have a larger value in that feature, then that's bad. So you have the baseline, how people usually behave, and then you essentially adjust. And then you essentially adjust the upwards, downwards based on the features that you have for each patient. Okay. So that's the Cox model. And it's used everywhere, like linear regression in regression or logistic regression in classification. And that's quite nice and established, but it also has its limitations. But it also has its limitations. So, like I said, if you look at this, then we have this baseline and we only can adjust it upwards and downwards. But the principal shape that we have here is basically fixed. We only can bend it a little more or a little less. And we want to generalize that. So, the most general way we think you can write it is. Write it as yeah, to basically take this together, and then you look at the cumulative hazard function and not just the baseline cumulative hazard function. So, this one not only depends on t but also on x. And we want to have the same property. So we want to have a fixed point at zero, and we want this thing to be monotonically increasing. But we want all of these things to happen while depending on t and on x, not making it easy. Not making it easy and splitting it up. So, this is kind of our wish list. We want to create some neural network, Lambda, that we can give the time to and we can pass the features to. And we want that this neural network is monotonically increasing in time, is flexible enough. That's kind of important. And we'll talk about that in a minute. And we want this zero property. So that's our wish. Property. So that's our wish list. Okay. So let's go through our wish list. If we focus first on the monotonically increasing NT. So there are already networks called Monday networks and they give you networks that are Give you networks that can give you monotonically increasing results. So, what's going on here? If you've ever worked with input convex neural networks, you might find this, yeah, this looks familiar. And so if you have a convex, if you want to build an input convex neural network, so you have a function where you plug in some element from R to the n and you want to get a real number, and that thing should behave convex. Thing should behave convexly, then you can build neural networks that have that property. If you restrict some weights to be non-negative, and for the convex neural networks, you want to have special activation functions with certain properties. Quite simple. Most activation functions we have, not most, but decent amount, for example, RELU fulfill these properties. So here we use a quite So here we use a quite similar approach. We have the output of the last layer, or might be the input of the network. And then we have our affine mapping like we usually have in neural networks, where, but the special thing is we restrict the entries of the linear parts of the matrix to be non-negative point wise. And if we have the activation function, which like The activation function, which like everyone usually does, we assume is pointwise. Then we say, okay, that activation function should be monotonically increasing pointwise. Yeah, so nothing special there. That's kind of every activation function you ever used in practice. For example, the hyperbolic tangent. And this is what they used in the Monday network. Okay, so we could use that to... We could use that to use our features and our time, or to plug in our features and our time. So, our features gives us the dimension n and time the plus one. And then we can plug in this tuple and say, okay, that's the input of the first layer. And then we can have multiple layers and the result is monotonically increasing in every entry of this vector with respect to the input. To the input. So, if we in the end collapse it to be one-dimensional, then we have our monotonically increasing function in every entry of C0. Okay, let's get to the second point of our wish list. The flexible enough. Again, if you've ever worked with convex neural networks, it's Work with convex neural networks, it's kind of nice because you can prove stuff obviously about them. But if you want to train them, it's often really hard. So, usually in deep learning nowadays, we have like dozens of layers sometimes and even more. But if you use convex neural network, it's quite well known that if you use more than five layers, something like that, you basically hit a limit and it doesn't get better. And we haven't really figured out why that's the case. And we haven't really figured out why that's the case, but that's kind of a problem. And we have a similar problem, or we found that there's a similar problem in this case. So we want to make the whole thing a bit more flexible. So what we did here is not we didn't look at n plus one, but we notationally split that up. So we use, we call it one to plus. So we have a plus here. And now we have two inputs. So we split up. Inputs, so we split up time and features, and it's sufficient for us if we're just monotonically increasing in time. So we don't care about being monotonous in the features. Okay. So we played a lot around to make the model as flexible as possible. So we did a lot of we trained a lot of versions of these to figure out, okay, how well do we do? To figure out, okay, how well do they train? And this is basically what we came up with. So here's the part that we already looked at. So we have our non-negative or affine mapping that is non-negative and has non-negative entries for the matrix. And we have our activation function that should be monitoring or increasing, for example, again, hyperbolic tangent. But one of the changes that we made is, for example, here. We made is, for example, here we have a ResNet, which are easier to train, so that already makes things a little bit nicer. Another thing that we use is that we plug in the input of the first layer into each new layer again. That's also a well-known trick for convex neural networks. So we stole that from there. And that leaves this part. That leaves this part where we incorporate the time. And that's essentially a bit messy, but here we also have restrictions that we want to have non-negative things. And we had a mark product. So we basically use the same tricks that we used up there that I talked about to make the whole thing. Uh, makes the whole thing monotonically increasing in time. So, we need a couple of other restrictions. So, not just monotonic increasing, but also that's non-negative. But that's all relatively easy to implement. Okay, so this gives us, yeah, we found after lots of experiments, this gives us lots of flexibility and it also trains faster than the other version. Okay, let's get to the zero part. That's actually quite simple. So, the solution is simply to say, okay, I have now this network where I plug in the time and I plug in my features. Maybe first I don't plug in my raw features. Maybe I first go through some other network or some feature extractors and handcraft a feature extractor that gives you a vector that you didn't want to use as the real features. Use as a real features. And so that's the thing we talked about. And then you subtract simply what's the value for t equals zero. And then you have your property that you have zero as a fixed point. So now we have everything that we want. So we want to be monitoring increasingly time, flexible, and we want to have this property. And using that, we can go back to the equation that we had in the beginning. And plug that in. So we say, okay, we have everything to define this. So we just plug in our Monday Plus network and we have what we want to do. Okay. We call this Sumo ‚Åá  based on this model, which I will talk about in a second, which is called Sumo that was published by someone else. We also call this one here Sumo Plus. One here, Sumo Plus. That's we use the Monde network, but instead of the Mondele Plus network that we just developed, so the simpler one that has also all the properties that we want to have that we have here, only that it's less flexible. And this is a model network. And you see, this is designed quite a bit different. So we don't have the structure that we usually have based on the accumulated pattern function, but they use the sigmoid function. They use the sigmoid function. And that actually prevents them from even getting the value one for time equals zero. In practice, the network can usually approximate that, but you don't have the guarantee that that works. You just have to hope that the training works out and that it's close enough. In many cases, that works, but I will show also a counter example in a second where you see that this can also work. Where you see that this can also go wrong. Okay, so we have these networks. So this is published somewhere else. This is the Sumo network. Then we have the Sumo Plus network and the Sumo network. Here we have all the guarantees that we ever want to have. Here we have here, this one lacks a guarantee that we are one at time zero. And this one should be more flexible. Okay. And we can use arbitrary. Can use arbitrary feature extraction networks here. So that could be, yeah, a large language model, it could be an image processing model. So it could be a transformer, could be a CNN, could be anything you want, which is quite nice because that for the first time, they could have done that, but they haven't implemented it. But this basically for the first time allows you to use any kind of data, for example, imaging data in your survival analysis, which people haven't done before. Which people haven't done before. Okay. So let's connect it back to the Cox model. So this is what the Cox model looked like. And this is basically how we could implement it. So we can just say, okay, we have here this baseline cumulative head function. And we could just get it by using our MondePlus network, where we say, okay, we have constant features, for example. Okay, we have constant features, for example, zero, and then that's obviously independent of the features, and we can implement it that way. So we can use what we built already to implement also the Cox model in the context of deep learning framework. Or to write it a little bit more simple, we can use our Sumo Plus Plus network, or it's a Sumo Plus network, and plug in constant features, and then just add this exponential here. And then just add this exponential here. So that's quite nice. Yeah, and like I said, this enables seamless integration into deep learning frameworks. But that's more of a curiosity. You could also do that by using Splines again and implementing them in a deep learning network. It becomes more powerful if we talk about something called time-dependent Cox models. Cox models. So there are different types of time-dependent Cox models. Here I want to talk about time-dependent coefficients, Cox models. And the change here is that you also have the T here in the coefficients. And people have implemented that in different ways. And I haven't found anything in the literature that guarantees that you have still this monotonically decreasing property. So I was quite annoyed by that. So we came up with... Quite annoyed by that, so we came up with our own version that has, yeah, that guarantees that it's monotonically decreasing. Um, and um, I don't want to talk about this in detail, so you can easily check this fulfills those properties, but what you essentially have here is you have this network, and that basically takes the power that you can use any kind of features X here, for example, images that you usually couldn't utilize. That you usually couldn't utilize in classical machine learning because you would hopelessly overfit, but you can use this here in the deep learning setting. And yeah, so you basically have the power of deep learning and generalization of deep learning in here. And yeah, here, maybe for interpretability. So this O here marks the point of least risk. So if your features have the values O, you're pretty safe. And the further you go away from this O, Further you go away from this O, the more in danger you are of having this adverse event, for example, death. Okay. Now let's talk about the loss function. So we talked about the models. And so I want to talk about two different loss functions. So there's first a sumo loss function that was published by another paper that also published the SUMO network. And I want to talk about the binary. And I want to talk about the binary cross-entropy loss that we came up with. So let's focus on this one first. In case that you see that the adverse event happens, so the patient dies, for example, then what you can do is you say, okay, I want to maximize the probability that I predict with my curve that the patient dies at the point of time where the patient actually died. Died. What that means is if you have, if you interpret this as a probability function of the probability of death occurring at time t given those features, then you can say, okay, I want to maximize this log likelihood. So minimize it. So we have minus 0. Or alternatively, if you've seen the patient being censored, then you want to say, okay, I don't want to increase. Okay, I want don't want to increase the probability that the patient dies at that point, which we can interpret as the curve is going down as quickly as possible. But what we want is that the curve stays one. So we want to keep a one there. Yeah, okay, so you basically have a switch here. So, and here you have more or less a probability density framework that you're thinking in, and here you have a class. Framework that you're thinking in, and here you have a classification framework that you're thinking in. And that switch, um, um, yeah, we didn't want to make this switch, so we formulated it all in the classification framework. So what we do is we have our time record for time t, and we look at the times before that and the times after that. And for the times before that, we say, okay, we want to have the curve to stay at one, so we want to push this curve up. So, we want to push this curve up. And for times after that, we want to push this curve down. So, this is just binary cross-entropy. And we sample different time points after the record and before the record. And we don't care, we don't really sample time points after the time record if the patient hasn't died or if we don't know that the patient has died. So, we only optimize that the curve goes down if we've seen the patient die. So, those are two different philosophies, I would say, to think about what survival. Philosophies, I would say, to think about what survival analysis is. And they give you two different loss functions. Okay. Let's look at some numerical results. And maybe we can also recap basically what these loss functions mean in the context of these. So we have time here. And this is based on a data set that analyzes recurrence of muscle soreness. Recurrence of muscle soreness in patients. And we have our different models that predict what's going on. And when you have the vertical red line, that's basically when the patient, yeah, when the event occurred. The patient didn't die, but in this case, the patient experienced muscle soreness. And so what the sumo loss function does is basically it wants to increase that the curve is getting steeper at this point. Curve is getting steeper at this point. And what our loss function wants to do is that the patient, that the curve is staying at one here and it's going to zero here. And here we have a couple of different models. Those are basically the classic or the real deep learning models. This Cox model here is also a Cox model that is. That is, yeah, that is a Cox model, but is implementing the deep learning framework. And the Kepler-Meyer model is a feature-independent model that just tries to predict the average patient, how that will behave. And we have the log-normal model, which is what's the best of all the classical models that we tested on this data set. The other case that we have is that we have some sensoring. So, I marked this with a blue line here. A blue line here, and um, so here we wouldn't uh optimize this part for our last function, we would just optimize this part to be up there. We wouldn't push anything down here. And what you may already spot here is that if you look at the deep learning models, they usually experience or they display much higher curvature than the other models, which if you think If you think about, for example, the Fischer information, then if you think about curvature in that way, then at least the models think that they are doing, that they know something that the classic models don't know, because they change much quicker to certain points in time, or at certain points in time. Another example for a type of cancer or for COVID here in this case, and here we also see. And here we also see that, for example, it can actually happen that if you have a patient who has that event really early, maybe on the first day, then this sumo model, not the sumo plus plus model, that one basically doesn't really fulfill the property that we want to have, that we have a one at time zero. Okay, another thing that I want to point out is you can also use this for Like to point out is you can also use this for regression analysis. So if you, for example, want to predict the housing prices of houses in California, that's the follow-up data set to, there was another one. I think the Boston data set, this is basically the data set that you now should use instead of the Boston one for some reasons, I don't remember. Then you can also try to predict the housing price. So you can always ask, or you can change the question. You can stop asking, or instead of asking, what's the price of the house? You can ask, is the house cheaper than this? Or is the house cheaper than this? And that basically turns the regression problem into a survival problem. And the nice thing about this is. The nice thing about this is, or the benefit that you experience is that you are not just getting the estimate of what the housing price is, but also by looking at the derivative of this, you can formally connect this to the probability distribution of what the housing price probably is for this house, which is quite nice. So you are basically predicting the whole posterior, not just some point estimate. And here are the promised image data points. So here we have an analog clock. Here we have an analog clock. So these are example images. We have an analog clock. And what you can ask is: yeah, what time is it? That's a regression problem. Or you could also ask, is it already that late? And then you have a survival problem. And that's deterministic. So we should see a sharp peak for most models. And we actually see that. And here we also see that the deep learning models definitely can approximate this sharp change way better. Sharp change way better than the classical models, which is not surprising because they are not that flexible. Okay, we looked at a couple of graphs and it's kind of hard to say, okay, which one is actually better. And there's something called the Breyer score. So what the Breyer score does is it looks at here's an example where we have the event occurring and censoring. It basically looks at And censoring it basically looks at these data points that we actually have, and the classical Breyer score just looked at the L2 error. So, what's the L2 error between what the prediction is? Let's say we pick the red curve here. And this one should be zero at this point. You can take the L2 error. In this case, it should be one because this is censoring. So you make the L2 error to one. But you have to be careful that you're not just doing that because since you. Because since you have censoring, you're losing information or you bias your data set over time so that it's no longer looking like the population. So you have to adjust for that. And that's how you can come up with a brief score. And you can do that not just for L2, but for lots of classification metrics also, which wasn't usually done before, but that's a whole other discussion that we have in our paper. And that gives you, yeah, in this case, average prior. Yeah, in this case, average prior scores. And what basically is the take-home message is here from this numerical section is that the deep learning models are almost always better than the non-deep learning models. And you don't lose anything by really enforcing that you have this, all these guarantees. So the SUMO plus, the SUMO model. And it actually, you could probably argue that it works better. And And yeah, there are some interesting effects that you could observe based on whether you just do a regression or whether you actually have a data set with censoring. But I'm running out of time, so I won't talk about that. And so I'm going to the conclusion. I think the main point, or one of the main points I want to make is I think survival analysis is highly underutilized in machine learning, deep learning, probably because we don't know about it. Probably because we don't know about it. I didn't know about it at least, but it's proven to be really useful. You can look at it in different ways. So you can even look at regression as the survival analysis problem. And that gives you also different loss functions if you think about it in different ways. And the model that we came up with, the SUMO plus plus model, is a universal and exclusive approximator of survival curves by which, I mean, we can approximate any survival curve that there is, provably, and we can Provably, and we can, whatever we approximate, it will be a survivor curve. So it's not possible that we get something that's not a survivor curve with our model. And yeah, this all integrates really easily into a deep learning pipeline. And it also enables you to even have classical models like Cock's model in your deep learning pipeline. Okay, here are some references also to the data sets. Are there any questions?