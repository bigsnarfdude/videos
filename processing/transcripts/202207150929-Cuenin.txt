It's been really a very nice experience, so I appreciated a lot uh the invitation and the opportunity to speak here. And so I'm going to speak about Schrodinger operators with complex potentials. I wanted to squeeze in here. So I want to talk mainly about complex potentials with some additional structure. And I'm going to talk about spars and random potentials. Because I wrote the Schrodinger operator. Because I wrote the Schrodinger operator so big I didn't have any space to put this. But I'm going to recall some results about the unstructured case, and then I'm going to speak about the structured case. I'm going to spend most of the time on the random case. Because that's, in my opinion, the most interesting one. All right. So the setting is very simple. So everybody knows. So we just look at minus Laplacian plus La Potang. Look at minus Laplacian plus a potential and L2 of Rd. We assume that the potential is a complex value, and we usually assume that the dimension is bigger or equal to 2. Okay, so I'm going to mention some results in one dimension, but for most of the talk, the interesting things will higher dimensions. And we're also going to assume that the potential decays at infinity in an average sense, so that it's in an LQ space for some Q which is For some q which is strictly less than infinity. And we will see that there is some transition between short range and long range potentials, what we call short range and long range in this setting. And so short range potentials, they will be characterized by q less or equal than dimension plus 1 over 2 and then long range. And then long range, everything else. Okay, and you will see that this exponent will pop up a lot and is connected to some harmonic analysis, which I cannot go into. All right, so you can think of to motivate this nomenclature. So, for example, if Q is 1, then the potential has to decay very rapidly for it to be integrable. Integrable, if q is infinity, it has no decay whatsoever. So here is just a picture of sort of the generic situation. So we want to study the discrete eigenvalues of this operator. And because this is not self-adjoint, obviously these eigenvalues can be anywhere in the complex plane and they can a priori accumulate anywhere on the positive real axis, which is the essential spectrum of the free labels. Of the free line velocity. Okay, and I also want to say that in this setting, where the non-self-adjointness just comes from a potential which is decaying at infinity, this is sort of a mild self-adjointness compared to a lot of the other talks. So in this case, actually, there's no distinction between various definitions of essential spectrum. They're all the same. But we will see that even this sort of very mild self-adjointness can have some. Some very unexpected, surprising special behavior. So, I don't have time to mention all the works that are related to this talk. I just made a partial list here of some people who have contributed to this business. And I also left out many people. I apologize for that. In particular, I left out all the That. In particular, I left out all the contributions that deal with operators different than the Schrodinger operator. So the list would be three times as long at least. So let me just set the stage and start with what is known for real-valued potentials. So most of you probably know the so-called Turing inequalities. These are inequalities on sums of On sums of eigenvalues of Schrodinger operators, and the inequality tells you that you can bound this sum in a scale-invariant way by an integral of the potential. And I abbreviated this a bit, so I left out the case equal to one, which is also interesting. But I said we want to focus on the higher-dimensional case. And so this inequality is true for all positive gamma if the dimension is at least two. At least two. Because it's also true for gamma equal to zero if the dimensions at least three, in which case you should think of this as being the number of eigenvalues. All right, but so for non-self-general operators, for complex potentials, we are far away from this kind of precise result. So all we can say is something about the single ethanol. So I will not speak on results dealing with. Speak on results dealing with the distribution of eigenvalues because these are at the moment not very well understood and they will take a separate talk. So I will only speak about bounds for individual ideologies. And again, this is the important thing is that this is scale-invariant. And so you can ask the question. The sum is missing or each individual there is no sum here. There's no sum here. So for a given j, you have this in a call. I could just evaluate the j. I just put the j because there was one here. All right. And yes, it's certainly not true if you put the sound here. It's not. And what is maybe more surprising, okay, so first of all, Rupert Frank proved that this inequality in this sort of analogous form, the only difference, you place the negative part of the potential. The negative part of the potential by the absolute value because V is complex now. So Rupert Frank proved that this inequality continues to hold for gamma at least up to one half. Okay, and at the time when this was proved, it was not clear or whether one actually has the same thing as in the real case. Alright, and in fact, before In fact, before Rupert Frank proved this, there was a paper by Lochtev and Sofronov, and in that paper, they conjectured that an inequality like this should be true up to t over 2. So they already had the intuition that this inequality cannot be true for all gamma, and their guess was that it could be true up to this number here. And I think their intuition was based at least partially. Based at least partially on this Wigner von Neumann potential. So the Wigner von Neumann potential is a real value potential which gives rise to an embedded eigenvalue. And so this potential decays like 1 over x and in the L P scale this would correspond to an exponent d here, which means gamma. Here, which means gamma equal to d over 2. Okay, so they had probably this example in mind. And so, recently, together with Sabine Pagli, we proved that actually the bound that Rupert Frank proved is already optimal, and this is false. And the counterexample is not this, so it's not the Wiener von Leiman potential, it's something which, again, sort of comes from harmonic analysis, which is sort of a Schrodinger merge. Which is sort of a Schrodinger version of the NAPI example. Because this is what is called the NAPIC sample in my analysis. The difficulty was sort of to find the precise potential that gives you a contradiction to this inequality. And in fact, if you wish, this is still an open problem because we didn't manage to find the explicit potential. We were able to say that there exists a potential. Say that there exists a potential which has this property. So the absolute value of the potential is bounded by some constant times epsilon times the characteristic function of this long thin tube. And this has characteristic scales here. I just made a picture in 2D, but you can think of the higher dimensional case, which is quickly instead of a rectangle, we take a tube. Instead of a rectangle, we take a tube here. Okay, so d minus one directions which have length epsilon to the minus a half, and one long direction which has length epsilon to the minus one. And then you can do a simple computation. So we proved that this potential produces an eigenvalue on the order of one plus eigen epsilon. And so if you let epsilon go to zero, then the modulus of this eigenvalue. To zero, then the modulus of this eigenvalue becomes one. So you have a one on the left-hand side. And if you compute this norm, you don't need to know what v is precisely. So this sort of size and width is enough to compute the norm. If you plug this in, you get epsilon to a power which becomes positive when Q is bigger than plus one over two. So I mean just to go from here to here, we just do a change of variance right to We just do a change of variance, right? So Q is d half plus gamma, so d plus one over two corresponds to gamma being one half. Okay, so that was the counterexample, and we had to work quite a bit. So you can see the conjecture was formulated in 2009, and then this was made popular by Ruben Frank in a previous installment of this conference series. Conference series, so that's maybe a good advertisement for these open problem sessions. Yeah, so and this is from last year. So it takes quite some time. Okay, and what is interesting is that this counterexample, this simple counterexample, not only disproves this Laptop-Sophron conjecture, but it also shows optimality of many of the known bounds. Many of the known bounds. So it shows that the bound of Rupert Franc that I showed you is optimal, first of all. And there is also another bound which he proved for the long range case. So if you compare these two inequalities, so this is the short range case, this is the long range case, you see that, so they are both scale invariant inequalities. If you count the dimensions on the left-hand side, this is q minus d over. The left hand side is q minus d over 2, and here this is q minus d plus 1 over 2 plus a half. So this adds up to the same, but the difference is that this is in general smaller than this. So this inequality sort of becomes worse once you cross this threshold. You cannot control the left-hand side by the absolute value of the eigenhale, but only the distance to the central spectrum. Again, so this inequality follows from the technology. Inequality follows from the techniques used to prove this inequality together with a very simple interpolation argument. So basically, you interpolate with the standard inequality for a self-adjoint operator. The norm of the resolvent is one over the column. The norm of the resolvent is one over the distance to the spectrum, so I was quite surprised that this inequality is dark. And also, I proved a version of this long-range case, which is sort of a local version, in the sense that you don't have to take this distance here, you just have this part, but then it's not the true LQ inequality anymore. The true LQ inequality anymore. So there is an LQ norm of the potential here, but there's also this exponential. And this exponential depends on the eigenvalue, right? It depends on the measuring part of the eigenvalue, or the square root of the eigenvalue. And what it does basically is it localizes the whole integral. So in the sense that if x minus y is much less than 1 over the imaginary part, Then one over the imaginary part of square root of e, then this is basically negligible, right? So you only have to measure the size of the potential in balls of this size. Then you sort of go around and check where the biggest ball is. Okay, and so these three things basically give a very satisfactory description. Description and what you can say about eigenvalues for general complex LQ potentials. So I was interested in studying what happens when you have a certain structure. So in which case do you have improvements over these sharp bounds? Okay, so before I come to that, let me also mention some optimality results in 1D. So this is the only place where I will speak Is the only place where it will speak one-dimensional things. So, in this case, there are also simple examples like this. So, the same kind of example, you take a potential which is just a characteristic function of some interval in this case, of length r, and you put some constant in front of it. And with this, you can show in one dimension that certain bounds on sums or number of eigenvalues, which I Sums or number of eigenvalues, which I didn't show you, but they exist, are optimal, or in this case of Sabine and Frantyshev proved that some conjectured found on the sum of eigenvalues is false with this example. So here's an example of an inequality that you can show is optimal in this case, and this is due to Frankfort-Franoff. So the number of eigenvalues is bounded by this quantity here. Here. And this is quite an interesting observation here: that the power here is two. So, this, if you remember what I told you about, the number of eigenvalues for real potentials, so for real potentials, you can bound, at least in three or higher dimensions, you can bound the number of eigenvalues by an integral of the potential. By an integral of the potential. And then this inequality is also, there's also a version of this inequality in higher dimensions, which I didn't write. But the main thing I want to emphasize here is that this is integral, right? The power of the integral is 1. The power of the integral here is 2. Okay, so this is a sort of a non-local thing. If you take the potential and chop it apart and move the pieces apart, then this could grow much faster. then this could grow much faster than this. This is invariant on this, however, you chop up your potential. And this is sort of a manifestation of non-locality in the non-submitting case. And so surprisingly, this is optimal. Okay, so I want to prove to speak about improved bounds. So first I'm going to talk about sparse potentials and then About sparse potentials and then about random potentials, and perhaps some more interesting cases which we haven't studied yet. First, let me also say that if you assume that the potential is radial, then it's actually known that the Laptop-Sapronov conjecture is true. This was proved by Frank and Simon. So as we saw, this counterexample is sort of the opposite of radial. So radial potentials in this setting are not Yemen. Okay, so sparse potentials, what do I mean? I mean potentials that are sort of sums of individual bumps that are spaced very far away from each other. Okay, and so Sabine probably was the first to consider this. I mean, she didn't call it sparse potentials, but she constructed examples that are basically sparse potentials. So, where she proved that every point on the Prove that every point on the positive real axis is an accumulation point of the Schrodinger operator with a potential that you can make as small as you want. So given epsilon, you can make it its size less than epsilon, and you can have eigenvalues accumulating everywhere. So that's what I meant when I said that this very weak sort of non-self-joint perturbation, you know, a small norm and decaying at infinity. Small norm and decaying at infinity can have a very wide spectrum. And actually, if you take this newer counterexample that we proved together, you can replace the d here by d plus 1 over 2, which is better. Okay, and then what can we say about the upper bound? Let me just summarize it by saying you have sort of almost orthogonality in this sense, right? In this sense, right, so the bumps don't talk much to each other because of the exponential decay of the Green's function. There's some small tumbling which gives rise to errors, but if you take the spacing large enough, then you can sort of deal with this. And so, roughly speaking, you can replace the integral of, so you can replace the RQ norm of the whole potential just by the biggest RQ norm. So you only have to look. The biggest function. So you only have to look at the biggest bump and take that. You don't have to take all of that. And of course, there are some assumptions here that I suppressed. So the assumption is that basically if you look at the spacing of these pumps, here's a pump and here is a pump. And so there is some, let me just draw it in 1D here because it's simpler. So you have some separation, you call it Lj, and then you assume that this is large enough. So Lj to the power minus L. L j to the power minus epsilon is finite to reach epsilon. Something like that. Then we can prove this. Okay, so now for the last bit, I want to talk about random potentials. So what we're doing here is so we're chopping up the space or the support of the potential. Let's say the potential lives in a ball of radius r, a ball in the sense of q. Of cube. And I want to randomize the potential on the scale h. So I chop up this ball into small cubes, and on each cube, I put the plus or minus sign with equal probability. We could do something more general. For example, it works for Gaussians, but in the interest of simplicity, let's just look at this simple Bernoulli distribution. And the little V change. The little VJ's, they are just so the LPU norm of this is finite, right? Okay, so here is a theorem, and this has been proved recently together with Konstantin Metz, which says that you can double this exponent here. Okay, so this short-range exponent, you can double it. Numbered. So short range for random potentials is sort of twice what it is for general potentials. And so each eigenvalue satisfies the same type of bounds. So there's something here which you don't need to read. So there's some very weak dependence on the diameter of the support. So there's an R here that comes with a lock. So it almost doesn't matter. And there are tricks that. Right, and there are tricks that um there are tricks that allow you to remove this if you go to uh strict inequality here. Okay, but the point is that so you have this inequality and so you can put some constant, let's call it m here, and then the inequality holds except on a set with measure which is very small, so x minus some constant to the power m squared. Okay, as I said, if you're willing to sacrifice the endpoint, then you can remove the convox support assumption. This is something called, this is a technique in harmonic analysis called epsilon removal. This is sort of quite standard there, but we used it in this setting to remove this compact support assumption. And in particular, if you look at this quotient, then this will be finite almost surely. Most surely if Q is smaller than e plus one. Okay, yeah, I have a couple of minutes to sort of give some ideas of the proof. So what you do is you start in the standard way, you reduce the eigenvalue problem to a problem for a compact operator by writing the resultant identity. And there is a slight twist here, so it's not just the Bermuda-Schringer principle, it's a bit more than that. Schwinger principle, it's a bit more than that. It's sort of a multilinear Bernstringer principle. So, what you do is you write out this expansion sort of as a multilinear, you write a multilinear expansion of that. And then, because of this special structure, so that the potential is random, you cannot put half of the potential left and right because you destroy the randomness in this case. You cannot take absolute values of things. So, you have to be a bit more careful. Um but uh okay anyway, so so that's uh that's what you do and and you basically have to look at at these pieces, so these blocks consisting of uh these three operators, half of the resolvent potential, half of the resolvent, right, and everything that is sort of on the left and on the right doesn't really matter, because at the end you take you know the 1 over nth root, and anything that you get from here sort of just an alien if you take nth. It's just a nalweight that if you take n to infinity. Okay, so what is this r0 to the one-half? It's a Fourier multiplier with this symbol. And if you look at the previous slide, so you see that each resolvent is surrounded by potentials, and the potentials have compact support. So they localize left and right in position space. And because of this localization, Space and because of this localization, by uncertainty principle, you can blur out the multiplier. So you can afford to add one over r here. So this is actually less singular than this. So the localization x gives you a smooth meaning something. And then the next reaction is, well, use the spectral theorem for the free resolvent, and you write this in terms of the spectral function, and you express the Function, and you express the density of the spectral function in terms of what is called the Fourier extension operator. So here's the Fourier extension operator. So this is a simple calculation in Fourier analysis. The extension operator is something that takes functions living on the sphere and sends them to functions living in the whole space Rd. So we take a density and you take the Fourier transform of this measure or the inverse Fourier transform. Transform of this measure, or the inverse of a transform. And here's just a side calculation. I don't know if you can read it on its head. So, you know, if you integrate this with this, because of this one over r here, you can always bump this by the log. So morally, the resolvent and this EE star thing are the same up to logarithmic losses. And we don't care about logarithmic losses because we're going to lose them anyway. So once you lose something, you can. So, you know, once you lose something, you can afford to lose many things. Alright, so up to this factor, it's sufficient to study this operator and finish. Okay, and then there are sort of a couple of reductions. The most important one is that you discretize space and that you pass to a discrete version of the extension operator where instead of taking functions. Instead of taking functions on the sphere, you only take sort of vectors of order r to the d minus one points, which form a one over epsilon net, and you look at this object here. And okay, maybe let me skip this slide. So the crucial step, okay, I didn't want to write this slide by hand, the crucial step is that we use an entropy bound due to Jean-Baugan. Due to Jean-Baughlin. So, once we have done this finite-dimensional reduction, we can use this entropy bound, which gives us a control on the entropy is the minimum number of balls that you need to cover the image of the unit ball in some Hilbert space under this discrete extension operator. And the upshot of this is basically that, first of all, it's independent of the dimension of. First of all, it's independent of the dimension of this Hilbert space. It doesn't matter if it's finite-dimensional, we can always pass the limit. And it's also very weakly dependent on the dimension of this space. There's only a log. Okay, that's why I said we don't care about losing logs because we're going to lose them here anyway. So, all right, I should finish. So, this is too much. And thank you for your attention. Thank you for the talk. Uh other questions, comments? Can we look at the random potential again? So is there something about the argument that really requires jumping in this way, one negative one? I mean, or could you do something really truly more verified? Yes, of course. Yes, of course, you can. I mean, just take, for example, you can take mean zero Gaussian. That's one of the illustration. Yeah. And you can also, you don't have to take this particular form. You can start with an LQ potential, and then you can just randomize it in this way, by cutting up into cubes and multiplying by this omega j in each cube. So what I did here, I just took In each cube. So, what I did here, I just took the potential to be constant in each cube. But you don't have to do this. Further question, comment? Claude, what you're saying is that in this regime, the larger the dimensions, the more allowable are the potentials, right? You mean because of this. So you're losing the two, right? Yes. Yes. Yes, so they're not allowed. Yes. And I mean, you should think of this as a sort of like a central limit theorem. It's like a square root cancellation. Instead of taking the L1 norm of something, you take the little L2 norm of something. Other questions from online participants maybe? Let's thank the chance more again. Do we have a coffee?