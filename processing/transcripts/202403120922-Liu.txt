Academy of Mathematics and System Science. It's my great honor to talk in this wonderful workshop. So today I will talk about covariance-based activity detection in cooperative multi-cell massive memo. So in particular, I will talk about two technical pieces of this problem, which are scaling law and the efficient algorithms. So I believe you have known the You have known the background of this activity detection problem from Professor Yu's talk. So, there are some keywords like massive random access, massive memo, coherence-based approach. So, in my talk, I will mainly focus on coherence-based activity detection in multi-cell method memo systems. So, here is the outline of my talk. Talk, I will first talk about the problem formulation. So let us consider a cooperative multi-cell memo system. So in this system, there are B cells, and each cell contains a B station equipped with M anteners, and also N single antenna devices, and key of them are active at a time. So we assume that the So we assume that the signals received by all of these B stations can be collected and jointly processed at the central unit. To formulate the problem, we needed to introduce some notations. Actually, these notations are very similar to those in Professor Yu's talk. So we use this SBN to denote the unique signature sequence assigned to the device N in cell B. To the device N in cell B. The channel between device N in cell B and B station J is given by this. Here, the J is a large-scale feeding coefficient, and H is a really feeding coefficient. And we use this binary variable ABN to denote the activity of device N in cell B. So if the device is active, then ABN is equal to 1. Otherwise, it is equal to 0. It is equal to zero. And WB here is the AWGN noise following this distribution. So with bow notations, the received signal YB at base station B can be expressed like this. So where we assume that the signature sequence matrix SB and the large school feeding coefficient. And the large school feeding coefficient matrix J are all known. And the matrix H and the noise W are unknown, but their distributions are known. The device activity detection problem here is to estimate the activity of all devices here from the received signals RYB. So similar to the way that formulates the problem in Professor Yu's talk, so given a deterministic activity pattern A, we can compute the distributions of the columns of received signal YB. In particular, each column of YB is a Gaussian vector with zero mean. With zero mean and the covariance sigma b is given like this. So to recover the activity A, we can formulate this maximum likelihood estimation problem. Here, the sigma B height is a sample covariance matrix. So now we can present our interest covariance-based activity detection problem. Activity detection problem is a multi-cell meso-memo system. As you can see, that this problem looks very similar to the problem in Professor Yu's talk. The only difference here is that we have a submission term here due to the multiple cells. So I want to remind you that we use the A to denote the activity of all the devices. And sigma B and sigma B height here are the true code. Here are the true covariance matrix and the sample covariance matrix. So, throughout my talk, I will focus on this maximum likelihood estimation problem formulated here. So, we will consider the following two, we think, important problems about this problem formulation. Question number one is about the skilling law. Both scaling law. So, what's the detection performance limit of this problem formulation as the number of n ten n goes to infinity? And in particular, we are interested in answering how the number of the cells B and the associated multi-cell interference affects the detection performance. Question number two is about algorithms. How to design efficient algorithms for solving this problem to achieve accurate and fast Achieve accurate and fast activity detection. So, do not overlook the summation here due to the multi-cells, which will result into a highly non-linear objective function and causes computational challenges. And this issue will become more clear later on. So, let us first answer question number one: the scaling law. So, our scaling law result is Result is based on this consistent result by Professor Yu and his students. So, basically, this lemma says that the necessary and sufficient condition for the consistency is that the intersection of these two sets, N and C, is a zero vector. So, here are some remarks on this slide. Here are some remarks on this lemma. This condition implies that the likelihood function is uniquely identifiable in the feasible neighborhood of the true activity pattern. So there is generally no closed form characterizations of the intersection of these two sets. So our interest of scaling law analysis is to characterize the feasible set of system parameters, such as the number of the devices. The devices, the number of the active devices K, the signature sequence length I look, and in particular, the number of cells B, and which this sufficient and necessary condition holds true. So recall the definition of this set N. So we can see from its definition that both the signature sequences S and the logic go feeding coefficient. And the large scale feeding coefficients GB appear in the definition of this set. So, therefore, both of them will be critical in our scaling law analysis. So, in this slide, let me first state our assumption on the signature sequences. So, the signature sequence matrix S is generated from the following two ways, and the corresponding signature sequences are called type 1 and type 2 sequences. And type 2 sequences. Type 1. We draw the components of ice uniformly and independently from this discrete set, which is equivalent to draw the columns of ice randomly and uniformly from this discrete set. In type 2 signature sequence, we draw the columns of ice uniformly and independently from the complex sphere or radius or square root of L. So as you can see, Of L. So, as you can see, that both types of signature sequences are well normalized with the length of square root L. So, in fact, the type 1 signature sequence is better than type 2 in terms of the complexity of generating and storing the signature sequences. So, this result says that for both of the type 1 and type 2 sequences. Type 2 signature sequences are in assumption one, they will enjoy very good statistical properties. I just skip the details. So now let us state our assumption on the path loss model. So this is a quite standard assumption. So we assume that the large scale feeding components are inversely proportioned to the distance to the power gamma. To the power of gamma. So the gamma here is a pass loss exponent, which will play a very important role in our analysis. So if the exponent gamma here is greater than 2, then we have this good property. I don't want to go into details. So I want just mention that this condition actually is a sufficient condition for this property whole 2. And this And this gamma is greater than two is typically hold for most channel models and application scenarios. And here is our main scaling law result. It basically says that for both type 1 and type 2 signature sequences in assumption 1 and assumption 2 with gamma greater than 2, this scaling law holds with high gamma gamma gamma Stealing law holds with high probability. So, from this result, we can conclude that the maximum number of active devices key that can be detected correctly in each cell increases codely with add and decreases logarithmically with the number of cells B. So, the implication here is that the maximum number of active devices that can be correctly detected in each cell. Directly detect in each cell in the multi-cell scenario is almost identical to that in the single-cell scenario. So, I want to repeat this. The maximum number of active devices that can be correctly detected in each cell in the multi-cell scenario is almost identical to that in the single-cell scenario. This is because the number of cells here B appears in the log term inside of the log. Log term inside of the log. Okay, let me recap what we have done so far. I have presented two types of signature sequences and show that both of them have very good statistical properties. And I have also shown you a scaling law results and some model assumptions. So the next question is how to achieve fast and accurate activity detection based on the maximum likelihood estimation formulation. Likewise estimation formulation. So, in the next few slides, we show design efficient coordinate descent types of algorithms for solving the maximum likelihood estimation problem. So, let us first recall our interest non-convex optimization problem, this problem. So, this is a box constraint optimization problem. We actually can define the following non-negative. Define the following non-negative KKT violation vector like this. So, this is the gradient of the object function. This is a projection operator onto the box, and this operator is a component-wise absolute value operator. So, as you can see, that with this notation, solving the above optimization problem actually is equivalent to finding a point that satisfies its. Finding a point that satisfies its KKT condition, like this one. So, our goal in this part is to find a feasible point A with a sufficiently small KKT variation, like this. Now, let us first have a quick review of a state-of-the-art CD algorithm called random permuted CD. So, at each iteration, the algorithm So at each iteration, the algorithm randomly permutes the indices of all coordinates and then updates all coordinates one by one according to the order in the permutation. So the iteration here refers to the update of all coordinates. So for any given coordinate Bn of device n in cell B, this algorithm needs to solve the following one-dimensional optimization problem in terms of D in order to possibly update the coordinate. Update the coordinate ABM. Unfortunately, unlike the single-cell case, this problem doesn't admit a closed-form solution. So in order to solve this problem exactly, we need to find all roots of a polynomial of degree 2b-1 with a complexity of b to the cubic. So here are some remarks on the SOTA CD algorithm. On the SOTA CD algorithm. First, the total complexity of updating one coordinate is like this. This complexity includes the complexity of solving the sub-problem for D and the complexity of updating all the inverse covariance matrix using the rank one updates. Another issue associated with this CD algorithm is that this algorithm might become This algorithm might become inefficient and involve many unnecessary coordinate updates when B and N are large. What I mean here is that there are many sub-problems with the solution might be very close to zero. So next we shall propose two simple acceleration techniques to overcome these two problems. These two problems. And these two acceleration techniques will lead to two accelerated CD algorithms, which are inexact CD and active set CD, which I will present one by one. Okay, so let me first introduce the in-exact CD album. So this is a one-dimensional problem, one-dimensional sub-problem that we need to solve in order to update each coordinate. So the motivation here. So the motivation here is that we want to update the coordinate ABN, this coordinate with a lower complexity by solving this problem 8 in exactly, but in a controllable fashion. So here are two important observations. The first one is that the large scale feeding coefficient G and the variable D, they appear together in this sub-problem. In this, in this subproblem, okay, you can see this, and the second one is that the large skill feeding J Bn is much larger than JBN for all G not equal to B due to the pass loss model. So our idea is to construct a simple yet tight approximation of the sub-problem it by keeping the B's dominant term unchanged and the linearizing. linearizing all the other terms, which will give you the following approximate problem. So you can see this problem, this is the base term unchanged, and this is the linearized version of all the other terms. And this term is introduced to control the step size of D. And the parameter mu here is chosen such that the solution of this problem satisfies the following sufficient decrease condition. So I don't want to go into details. Go into details. So, the good news here is that this problem, the approximate problem, can be solved very easily by a cubic formula with a very low complexity. So, here is the CD algorithms. There are two variants. The first variant is that we solve the sub-problem exactly by using the root finding algorithms. The second one is the inexact variant. We use the cubic formula to solve the approximate sub-problems. After we get the Problem. After we get the D, we update the corresponding coordinates, and then we update all the inverse coherence matrix until we find a satisfied point A satisfying this condition. Now let me introduce the active set C D algorithms. The motivation here is like this. So both of the C D and inexact C D algrams might do many unnecessary might do many unnecessary coordinate updates, where several problems are solved but the corresponding coordinate almost doesn't change, and therefore the objective doesn't decrease sufficiently. So this corresponds to the case that the solution D of the subproblems is approximately equal to zero. So the active set idea here is to select and update a subset of updates a subset of coordinates that have the most potential of decreasing the objective to reduce the number of unnecessary coordinate updates. Here are some details. So at each iteration, the active set CD algorithm selects an active set of coordinates and then updates all coordinates in the select active set. So the proposed selection strategy for the active set A key can be expressed in A key can be expressed in this equation. Here, the omega k is a properly selected threshold parameter at the case iteration. As you can see in this equation, we only select those coordinates that have a sufficiently large variation. So, the intuition behind this selection strategy is that updating the coordinate ABK with a large variation. With a larger variation, it is expected to yield a larger decrease in the objective function. So, do not treat all coordinates equally, as in C D. Also, I want to mention that the paramet omega k here plays a crucial role in achieving the balance between decreasing the objective function and reducing the cardinality of the active set. So here is a proposed active set CD algorithm. So at each iteration, we select the active set first, and then we update all coordinates in the selected active set only once by using the C D algorithm. So it is worth mentioning that the violation vector Vek will become smaller and smaller as the iteration in Smaller as the iteration index increases. So, therefore, the cardinality of the selected active set, this one, is expected to be significantly less than the total number of the coordinates Bn and gradually decreases as iteration goes on. I would also like to mention that the active set strategy can be used to accelerate both CD and the in-exact CD algorithms. So, with the proper choice of the threshold parameters, The threshold parameter, we can have this convergence and iteration convergence, iteration complexity guarantees. I skip this page. So now let me use this table to summarize what we have done so far in the algram parts. So we have these CD algorithms. There are two potential problems with this algorithm. One is that the complexity of updating one coordinate is high. Updating one coding is high, and another one is that there might be too many unnecessary coding updates in this algorithm. To overcome these two problems, we have proposed two acceleration techniques. One is to solve the sub-problem inexactly, which gives us the in-exact CD algorithms. Another one is the active set strategy, which leads to the active sets. Which leads to the active set C D albums. Of course, we can combine these two acceleration techniques, which will give us the active set in exact CD album. Okay, I needed to be quick. So let me quickly show you some simulation results. So we have two goals in this part. I want to show you the detection performance comparison of different Performance comparison of different types of signature sequences. Second goal is: I want to show you the computational efficiency of proposed inexact and active set CD algorithms. So here is the detection performance comparison of different types of signature sequences. I hope you can see this figure clearly. So we can see from this figure that almost the same detection performance can be achieved. detection performance can be achieved by using type 1 and type 2 signature sequences type 1 and type 2 signature sequences. So the detection performance of type 3 signature sequences, which are generated from an IID complex Gaussian distribution, is obviously worse than the other two. So the key message here is that normalization of the signature sequences is crucial to the detection performance. Detection performance, especially to the false alarm error performance. So I don't have enough time to go into details. So I hope you can, if you are interested in this, you can check our paper on more simulation results and explanation on this point. And now I want to show you some comparisons of our proposed algorithms in exact CD, active set CD, active set in exact CD with two benchmarks. With two benchmarks. The first one is a vanilla CD proposed by you, Professor Yu, and the other one is a clustering-based CD proposed in this paper. So actually, this algorithm utilizes the signal from T dominant positions for a given user and completely neglects all the other terms. So, this is a rough idea of this clustering-based CD algorithm. So this This figure plots the comparison of the probability of error of all compared algorithms. We can see from this figure that the active set in exact CD is the most efficient one. Clustering-based CD is also very efficient, but its probability of error is worse than the other algorithms due to the coarse approximation. And this figure shows the average running time comparison of Time a comparison of all compared algorithms. So, from this figure, we can show that as the number of the cells B increases, all algorithms that solve the subproblem exactly are much more efficient. On the other hand, when the number of the users in each cell increases, the algorithms that use the active set selection strategy are significantly more efficient. And this figure shows the comparison of the number of updated coordinates of vandala CD and active set C D over the iteration. So we can see that the number of updated coordinates of active set C D at each iteration is significantly less than that of vanilla C D at each iteration. And moreover, the overall number of iterations of active cell C D is only slightly more than that of the vanilla C D. That of the vanilla CD. So, okay, let me quickly skip this page. Actually, there are many interesting practical issues and extensions of this problem. And I'll also skip this page. So, I would like to take this opportunity to thank all of my collaborators on the massive random access project. So I started this project with Professor Yu and two of his PhD students when I visited the University of Toronto in 2018. This is Julian Engineering is now with Huawei and this is Fuad. He's now with Nokia Lab. When I come back, I continue working on this problem with my PhD student Wang Ziyu and many of my colleagues. Many of my colleagues. I would also like to thank Professor Yui for inviting me to attend this wonderful workshop and for long-term help and support. Here are some related references and the slide here is mainly based on this work. Okay, many thanks for your attention. 