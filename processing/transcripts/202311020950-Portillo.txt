All right, good morning, everyone. So, I'm excited to share some of my research. Again, all about analyzing pixelized astronomical images, and a lot of my work is specifically looking at point sources. So, sources that are either so small or so far away that theoretically they should just look like points in our images. And Connor did a great job introducing. Connor did a great job introducing how images are typically used in astronomy. And the fact that these beautiful images that we get fundamentally are just giant arrays of numbers that tell us how many photons have hit each pixel in our camera and maybe as a function of whatever color filter we've put in front of our telescope. And, you know, often when an astronomer says they use a data set like the Sloan data. A data set like the Sloan Digital Sky Survey. This is an image from that. What they really mean is that they've used the catalog from that survey. And so we build these numerous software pipelines whose job it is to go through these images algorithmically, identify sources, and then measure their positions and fluxes or brightnesses. And in some applications, we'd also like. Applications, we'd also like uncertainties on those positions and brightnesses. And a lot of astronomy is done in a paradigm where, you know, the questions we want to answer about stars, galaxies, whatever, we don't go to the images to answer those questions, but instead a lot of our questions are answered through this catalog, this list of objects and their positions and brightnesses. But there are situations, and I'll talk about a couple of And I'll talk about a couple of them today, where it really helps to go back to the images and to forward model the number of photons in each pixel in the image. And there are certain scientific questions where the catalog, at least as typically used, isn't good enough. And we do need to go back to the images. And the first, well, before I go through the first example, to introduce the details of this forward modeling, I wanted to go over. Modeling, I wanted to go over what astronomers call point spread function photometry, or how you measure the position and brightness of a single point source. And so here I've got a 10 by 10 pixel image, completely simulated, fake image of a single star. And an important piece of modeling this image is to use the point spread function. And so this takes into account things like what the atmosphere does to our Like what the atmosphere does to our light, what the optics of the telescope do to the light. And even though this source is so far away, it should just be a point, the light from that point is going to get spread out in a way that we can mostly characterize empirically and that we've got some theoretical handling of. But the way I'll present it today is that we've got a parametrized model. If I know I have a point source at some x, y position, XY position and it has unit flux, I can calculate what fraction of its photon should land in each pixel of my image. And so I can model the expected number of photons I should have in my image, given a single point source, by just putting some position into this point spread function and multiplying by the flux or the brightness of this source. In a lot of astronomical applications, we can make the assumption. Applications, we can make the assumption that the number of observed photons, given a certain expected number of photons, is basically Gaussian. There are situations where you want to use Poisson, but Gaussian works a lot of the time. And so if you want to fit this image and get your best estimate for position and flux, you maximize that Gaussian likelihood and you just end up minimizing a chi-square. So, great, we can get an estimate for the Get an estimate for the position and brightness of this object by moving our PSF around until it matches the image. Alright, so how do things get harder? So the first case I'll talk about is moving objects. And one class of moving point sources that I've gotten into with collaborators at the University of Washington and the DEEP survey. Looking for asteroids, the kind of traditional Asteroids, the kind of traditional way that you find asteroids is you take an image of the same part of the sky over multiple different nights and you look for something bright that's in multiple images but moves relative to the background stars and galaxies. And, you know, maybe you put in some constraints that, oh, this movement is consistent with a solar system object orbiting the sun. You know, boom, you've got an asteroid. But the disadvantage. But the disadvantage of this traditional method is that your source needs to be bright enough to show up as a detection in multiple nights. And, you know, astronomers always want to find more objects, more asteroids, so we can learn more about the solar system. And so at the University of Washington, they're interested in how do we find the faintest asteroids that are possible to find with the data. And so if our asteroid is not bright enough to show up as a detector, Is not bright enough to show up as a detection in each of our images, you know, how can we go about finding them? And so there's this technique called digital tracking or shift and stack. And the idea is if you know the velocity of your asteroid, you can shift your images to cancel out that velocity and to hold the asteroid stationary while blurring out the background stars and galaxies. And after you do this correction, you can co-add your images. You can co-add your images and basically reinforce the brightness that you get from your asteroid. Now you're making it stay still while you're blurring at the background, and you can boost the signal-to-noise of your moving object. The catch seems to be, you know, how do I know the velocity of this object if I don't know it exists in the first place? The answer is: use a GPU, guess every plausible, well, guess a grid of plausible. Plausible, well, I guess a grid of plausible velocities and get candidates out. And it turns out this actually works, which is lots of fun. And then where I stepped in at University of Washington is thinking about, all right, great, you've used a GPU to find these candidate objects, but we want to measure them as best as we can. And so I went back to the original images. And so here's a real object that we detected, and 25 images that. And 25 images that are centered on it. If you squint, maybe you see it in a few of the images. It's like signal noise four or five. But really, we only know this object is there because of taking this set of images as a whole. And we can just extend this forward modeling framework. The first thing we want to change is, okay, let's add another dimension to our array. I've got images taken at different times, t, and observation conditions. And observation conditions may have changed over time. So I've got a different point spread function, different things happening, but as long as I can model that, I'm okay. I've also got movement. And so my object's position is changing as a function of time, but we know how solar system dynamics works. And in fact, over a short enough period of time, saying that this object is moving in a straight line with constant velocity is actually a good enough model. Actually, a good enough model. And so the parameters that I've got are just an initial position and an initial velocity on the sky for this object. But given that parametrization, now you can Ford model a position in each image. You lay down the point spread function at the corresponding position in each image, maximize the likelihood, and you've got the same sort of framework. But now, taking advantage of these multiple images, you're You're able to find asteroids that are a lot fainter. And so here is our characterization of the completeness of this detection method with magnitude on the x-axis, completeness on the y-axis. And this first vertical red dotted line is the twenty-five percent completeness magnitude for a single image. And so that's around twenty-third and a half magnitude. 23rd and a half magnitude. But with this stacking procedure, we're able to push that complete lissim element down almost three magnitudes. I forgot to calculate what flux ratio that is, but I think that's almost the power of, that's more than 10 times fainter. And so, okay, great. Combining images, we can find fainter objects, but also by going back to the images, our characterizations of the position. Of the position and velocities of these objects take advantage of the signal-to-noise of all of the images as a whole. And because of that, even though these are really faint objects, really far away, we actually have enough information to constrain their orbital parameters, things like semi-major axis, eccentricity, and inclination. And it actually turns out to be very important to go back to the covariance matrix. To the covariance matrix that you get from fitting position and velocity, you propagate those errors forward to the orbital parameters, and we still have enough precision to kind of talk about the dynamical classes of these different objects, which I think is really cool. So that's the first case I wanted to talk about, which is moving objects. Another case that's very near and dear to me are crowded sources. So sources that are Sources. So, sources that are very close together, like in this real image of a star cluster. So, I've got another 10 by 10 pixel image, and when you fit multiple sources, what most codes tend to do is they'll run some sort of peak finding algorithm first to give an initial guess as to how many sources there are and their positions and brightnesses. And we can use the same Ford modeling. And we can use the same Ford modeling framework. Now we've got multiple point spread functions. We can just add them linearly. That's fine. And we can go through the same procedure of we've got a parameterized model. We've got a likelihood. Maximize that likelihood to get your best fit estimate. And refine this initial guess that you've gotten to get positions and brightnesses. But when you've got sources that are this close together, such that they're Close together such that their point spread functions overlap, you run into a lot of issues. One issue that you run into is that your uncertainties on sources that are close together are no longer independent, and the errors are covariant on neighboring sources. Maybe that's okay. Maybe we just replace the error bars in our catalog with some giant covariance features. The tougher thing to deal with, I think, is the fact that what astronomers call deep blending. That, what astronomers call deep blending, that is figuring out how many things are in the image in the first place, is hard to do. And if you're uncertain about that, that affects your measurements of all these other sources. So maybe we think that there's this dim fourth source right there. If we include it in our model, that changes the best fit parameters of the other three sources. So even if we don't care about that fourth source, because it's If we don't care about that fourth source because it's too low signal to noise, it affects the brighter sources that we might care more about. And this is kind of an odd dependence that our uncertainty depends on like the number of sources. That's not really something we deal with very often. And this star cluster is kind of an extreme example of crowding. I've got lots of stars really close together. But as we build more sensitive instruments, like at the root, Sensitive instruments like at the Rubin Observatory will be seeing more objects in the same area of sky and will be limited by our atmosphere in terms of angular resolution. And so by making our telescopes better, we're making our images more crowded and these sorts of issues will pop up more often. And I've been working on basically putting this forward modeling framework into a Bayesian framework. And so we've got some. And so we've got some familiar things. You know, my model image is a bunch of point spread functions added together. I've got some likelihood. But now that I'm being Bayesian, I put priors on my parameters, and maybe I've got physically motivated priors. But I think an interesting thing that you can do is to treat the number of sources itself as an uncertain quantity that we want to estimate, and that is a parameter. And that makes your inference. And that makes your inference transdimensional because each source has a position and flux associated with it. So if you add or remove sources from your fit, you increase or decrease the number of parameters that you have. But, you know, in the statistics literature, reversible jump, you can deal with this sort of inference, and you can still sample from your posterior, like with a typical Bayesian MCMC analysis. And in this context, And in this context, each of those samples from the posterior you can interpret as a catalog that plausibly fits your data. And because the number of sources is treated as uncertain, your catalogs can have differing numbers of sources. And you can actually capture the fact that your best estimates for positions and brightnesses depend on this deep blending, depend on how many sources you actually think there are. You actually think there are. And so I think this is actually a natural way to capture this weird uncertainty that we run into. To show that this works on real data, I did a cross-check with comparing this image taken by the Sono Digital Sky Survey with the same area of the sky observed by Hubble. Because it's in space, it gets a much better view, so I used Hubble as a ground truth to compare to. Truth to compare to. And sorry that I flipped black and white here, so dark pixels now have more photons. So I compared with kind of a standard pipeline that's used called DAOFOTE that looks at this 10 by 10 pixel patch and says there's three point sources at those locations. But if you look at the Hubble image, this is not close to the right answer, right? But the brightest Hubble stars in bright green here. Stars in bright green here. And the DAO folk catalog is doing a lot of blending, which can be problematic. Like if I've got a red star beside a blue star, but I blend them together into a yellow star, that's a completely different physical scenario. And so this blending can be problematic. And you might say, okay, why bother looking at the ground-based image? We should just use the space-based image. But it turns out, using just these pixels and These pixels and Bayesian probabilistic cataloging, you can actually get something that's pretty close to the Hubble answer. And so these red crosses are just from this 10 by 10 pixel image. And because we've got a Bayesian method, we can also get a distribution of samples. Looking at this more quantitatively, this method's able to find stars that are over a magnitude fainter or two and a Over magnitude fainter, or two and a half times fainter. And we can also incorporate multi-band imaging, combining images taking the multiple colors to go even fainter. And oh, I need to show the fun animation, if I can. So each of the frames in this animation. Oh, come on. I think this is a Windows Mac thing. Find me offline, and I'll show you a fun animation of all of the different samples from this catalog. The last thing that I'll just give a preview of is I've talked about moving objects, crowded objects. The next thing I'm kind of thinking about is crowded moving objects. I'm looking into binary asteroids that are orbiting each other and whether we can find them. Each other and whether we can find them using methods like this. So, with that, I'll take any questions.