Um okay, so um I will talk uh about things uh what we did during the pandemic with a particular focus on forecasting and predictivability. And I will try so I'll describe what we did and describe a few of the a bit of the research that we did on top of that on forecasting and predictability of Forecasting and predictivability of models during pandemics, and then conclude and try to draw some conclusions for the next pandemic, whenever it may come. And so there's been a lot of focus during the pandemic on mechanistic models and the use of mechanistic models, which have huge value in supporting causal understanding of what drives an epidemic. And here I'm showing you. So, this, for example, here, I'm just going to find my mouse. So this for example, here I'm just going to find my mouse. So this is one example from one paper by Keeling et al., a paper that was used in the UK for quite extensively to inform policy and also for short-term forecasting, in fact. And so you get from mechanistic, the use of mechanistic models allows the creation of counterfactuals of scenarios and through these things can enhance our understanding of something like a pandemic. Of something like a pandemic. Although I would argue this also comes with difficulties, and I don't think there has been a kind of truly mechanistic model applied at any time during the pandemic. And because of that, generally, they have had fairly poor out-of-sample properties, and therefore had to always be interpreted with caution. But the other thing, so I already mentioned short-term forecasts. Already sort of mentioned short-term forecasting, and as probably you all know, you can also use models for purely for predictions. And these predictions can have a value in their own right, even when they don't come with a mechanistic understanding. And this is what I focus on here is purely on predictions based on data that is available at different times of the or was available at different times of the pandemic and focusing on predictions. And focusing on predictions comes with one benefit, which is that it's relatively easy to assess their quality by comparing them to what later happened and comparing them to the data as it later turned out to be. So here this one example from the CDC Forecast Hub and the kind of national level CDC forecast that I'm sure most of you are familiar with. Okay, so and I just want to motivate a little bit this Want to motivate a little bit this statement that I just made that predictions can have a utility in their own right. And I'll talk about three projects that I and people in my group have personally been involved in. One shown here and described in a paper led by Flavio Finger was on a diphtheria outbreak in a Rohingya refugee camp in Bangladesh. And so working at the time with WHO and WHO and MSF, and using a model fit to this data as it emerged, making forecasts at four time points with a particular importance to this first forecast because there was a decision that had to be made on expansion of hospital capacity and making more beds available. And in fact, this is something that we have quite often, and I will get back to this, is that the early work Is that the early work has the greatest value and much greater value than later work? So, in this case, the value of this prediction, in this case, of case numbers, was that it allowed to anticipate healthcare demand. A second use is it can inform interventions even when we can't necessarily make counterfactuals. So, this is another piece of work we were involved with on cholera in Yemen when In Yemen, when there was an ongoing cholera outbreak and a decision to try and make vaccines available from the WHO stockpile, and ourselves and various other teams were working with the World Health Organization on trying to prioritize area into which a limited number of cholera vaccine doses could be deployed. And again, in this case, In this case, models could inform, the different teams came up with rankings, priority rankings, and therefore models could inform an optimal deployment of vaccination. In the end, unfortunately, the vaccination campaign didn't happen, but it just shows that I just want to use this as an example to show that predictions can inform interventions. And then lastly, and this is perhaps And this is perhaps less often the case, but sometimes also predictions can inform clinical trials. And so we were involved in a project in the Ebola outbreak in the Democratic Republic of Congo, trying to predict the spatial spread of Ebola then. And this was to inform, there was an ongoing vaccination program in response to the outbreak, but a second vaccine was to be trialled, and therefore And therefore, it was important to find potential trial sites where currently there was no vaccination but a high likelihood of seeing cases. So, this was about trying to predict where there might be cases that there haven't been any yet. And we did this on a week-by-week basis. In the end, it didn't come too much because the outbreak was fairly geographically static and didn't expand very much. But it just hopefully serves as an example that, in principle, Hopefully, it serves as an example that, in principle, real-time predictions of outbreak trajectories can support clinical trials or vaccine trials and site selection. Moving on from this to COVID and what we did during the pandemic, so we got, like many other modeling teams, involved in the response to the pandemic fairly early on, so starting in February or even January 2020. February or even January 2020. And something we started doing fairly early on, and then many others did the same, was tracking reproduction numbers. And we're tracking reproduction numbers with the aim of making short-term forecasts. And something that is often done for short-term forecasts, and we did it in that case, was to assume that the reproduction number would stay constant in the future. So, in some sense, it's a So in some sense it's a counterfactual of no change in reproduction number even though we know that in reality in outbreak situations usually we have changes in the reproduction number. But anyway so we started from this renewal equation. I think there's already been a lecture on the renewal equation yesterday which I unfortunately wasn't able to join. But so I won't go into detail here, but what we did initially is we used a commonly available tool in form of Commonly available tool in the form of an R package called EPS Tim because we were aware of its existence and it was readily available, which uses observed case numbers and an assumed or estimated generation time that is fed in from the outside to estimate this time-varying reproduction number. And the problem then is that you have some delayed reporting, also some truncation usually in the time series of In the time series of reported cases. So, we did some backcasting of cases and filled in the more recent cases with a pretty poorly informed probability distribution of delays. So, we tried to do this back. We did a backwards convolution to get this infection curve. So, as you can see here, this is from a later iteration, but just to illustrate, you have the case numbers here as bars, and then we estimate an infection trajectory from that. We need to do some From that, we need to do some now casting here where there's truncation going on and then use that in FPSTIM to estimate the reproduction numbers. All of which we discussed, sort of, well, realized over the coming weeks and months is a fairly flawed way of going about this. And where in reality a much better way of doing this is to have a generative model in which the infection time. Rate of model in which the infection time, the infections and reproduction numbers are inferred jointly because the reproduction numbers affect themselves, affect future infections. So estimating infections first and then using that to estimate reproduction numbers would lead to biased estimates. So by July or so, we had a method implemented that did that using stand implemented also in an R package and by that time we were publishing all of these on a website. Publishing all of these on a website and updating them daily. And so, both national and sub-national reproduction number estimates and short-term forecasts based on this pretty simplistic assumption. So, this is all described in Abbott et al., whereas in the Gostig et al. paper, that describes many of the mistakes that we initially made in estimating reproduction numbers as well as others that can be made. And we ran that for about two years, and it was used by various organizations. We also used the same method to produce sub-national estimates in the UK. And it turned out to be very useful to have these estimates always available because we produced them daily, we published them on a public GitHub page, and they were used in a range of other analyses, both by other teams, including one. Including ones that didn't always get in touch with us about it, and where we weren't entirely sure always whether they were fully aware of all the limitations in these estimates. But also, we used them in some of our own analyses, for example, when it came to estimating transmission advantage of new variants or changes in reporting or changes in the infection fatality rate. And we have, if people are more interested in the kind of experience of interaction with Experience of interaction with or the experience of this public website and the interaction with the general public on this. We also, on our website, on the EpiForecast website, about two months ago, we put a blog post where we try to go a bit into detail into the lessons learned from that and the experience of doing that. I don't want to go more into that now. Instead, I want to mention just two other bits of work that we did. One is that we use the same model for short-term forecasts in the UK. Short-term forecasts in the UK, and as part of the efforts to inform policy there, and there were a bunch of other models as well. We published some evaluation of the quality of the forecasts that were made, especially early on in the first wave. They were generally pretty much all over the place, as you can see here, and not particularly impressive, which is largely owed to the fact that people were scrambling to put their models together, and this was one of many. With their models together, and this was one of many, many things that people were trying to do with their models. And we, somewhat interestingly, about six months into the pandemic in the UK, there was a decision to no longer ask modelers for short-term forecasts or publish them, but instead ask for what were then called medium-term projections. The difference being that now modelers were no longer asked to try and predict the future. Asked to try and predict the future trajectory, but this assumption that I mentioned earlier of a constant reproduction number into the future was made explicit. And so models were asked to include that assumption. And at the same time, models were asked to then run their models further into the future. So the longer horizon, which was then up to a month or two, was considered to be more useful for planning purposes. For planning purposes, whilst the explicit assumption of constant reproduction number was considered to be easier to communicate. And so, this is one example of this. There's no timeline, unfortunately. The axis is cut off, but these are the sort of longer-term hospital admissions. What if the reproduction number stayed constant from here onwards? Which, of course, in reality, it never did. And it usually fluctuated throughout the pandemic in the UK and pretty much everywhere. And the last one. And the last one is, I don't know if people are aware of these. I think the US COVID-19 forecast hub has been fairly prominent and it was done in collaboration with CDC. And about a year into the pandemic in 2021, we started a similar one, which was funded by the European Centres for Disease Control, ECDC, who were keen to put a similar project Project together in Europe. So, this was in 32 European countries: forecast of cases and deaths. And the idea here is that forecasts from multiple teams of cases and deaths are being collated, and an ensemble is created from them in order to come to improve the reliability of forecasts that are then communicated to policymakers. Especially with this spanning the whole of Europe, there was also an element. The whole of Europe, there was also an element of capacity building and being able to include countries with less modeling capacity that could benefit from the forecasts made in this pan-European project. But then also, as in the US forecast hub, there's also an element of trying to gain insights into which modeling approaches do well, holding models accountable, assessing the reliability of forecasts for different measures of. Of forecasts for different measures of disease severity and then creating a community of infectious disease modelers with an open science ethos. So, this was all done in the public, and here this is one example here from Portugal with a bunch of models making forecasts and an ensemble created from them. And then there's a nice comment led by Nick Reich on the, well I think it's nice, but I was involved in it, on the kind of function and setup of these hubs. Okay, so three So three coming to work that we did on the back of that, or some more or the analysis that we could do based on this. And again, as I said earlier, I want to focus here purely on this idea of making predictions and predictive ability. So I will touch upon in the next 10 minutes or so three questions, well, two questions really, and then I'll try to get to conclusions. But the first one is how good the The first one is how good those forecasts were that we and others made, and the second one, some attempts where we looked at how forecasts could be improved. And then I will come to drawing some conclusions from this. Okay, so how good were these forecasts? Well, we can compare forecasts. This is something that has a history in other fields, but we can use these so-called proper scoring rules, such as here the continuous ranked probability score. Continuous ranked probability score, which assesses not just how well a model captures the future data in its central tendency, but also how well it quantifies uncertainty. So this here, X, is the predictive probability distribution. Well, in fact, it's a sample. It's independent samples from the predicted probability distribution from a model. Little x is the data as observed. Little x is the data as observed, and so this expectation quantifies how close the predictions are to the data. And then there's a second element, well, and in fact it also captures an element of how widely spread this is around the data. And then this is just the expectation with two independent realizations from the predictive probability distribution of basically the spread within. And basically, the spread within the predicted probability distribution. So, we can use this to compare models. It's a proper scoring rule, which means it incentivizes honest forecasts. The true data generating model would always have an optimum score. And doing this, applying something, not exactly this, but a related score that's an approximation to this on the models from the forecast hub, we found, and this is completely consistent with what. And this is completely consistent with what has been found in many other fields as well as in other projects in epidemiology, in the US Forecast Hub, to no surprise whatsoever. And yet, it continues to somehow amaze me just the degree to which a simple ensemble from the contributed models outperforms usually each and every individual model. And so we used a median ensemble. So it's simply taking the median at each quantile from each of the contributed models. And you can see here in red and blue, this is for cases and deaths in the different countries. The spread of the models, and this is a relative score, so it's negatively oriented. One would be equal performance with a baseline model, which I'll get to in a moment. And the ensemble is the And the ensemble is the asterisk and it's usually at the bottom. So it's usually either the best performing model or amongst the best performing models for either cases or deaths. So that's fine. So we can say the ensemble performs better than everything else. But these kind of proper scoring rules really only tell us about the relative quality of forecasts. So we can use these to score models against each other, but it doesn't tell us whether that was actually a good forecast. Was actually a good forecast. And thinking about how we would tell whether something is a good forecast, I will indicate three ways amongst many in which we could quantify this, and I call this the absolute quality of forecast rather than relative quality. The first one being comparing it to a baseline model. And here you can see this for the 32 countries. The baseline model that we used as default in the Default in the forecast hub is one of a flat trajectory. So we're assuming the future is the same as now, but not in the sense that I mentioned earlier, where the reproduction number stays the same, but where case numbers or death numbers stay the same. So you can think of this as really one of the simplest possible ways in which one could try to predict a future trajectory of these epidemiological quantities. It's not completely ridiculous. It's not completely ridiculous because these are highly autocorrelated, so constant behavior you would expect to have some quality, but at the same time it's extremely simplistic and a model of any quality should be able to beat that model. And in fact, what we find for the ensemble is I show you some parts of this. So here this is the relative scale versus the baseline. So again, less than one. Baseline. So, again, less than one means the model beats this baseline, and in this case, it does so for, I think, in this case, one week-ahead forecasts of cases. One can think of other potential baseline models, and other things that people have used is, for example, a baseline model of a linear extrapolation or maybe an exponential extrapolation. But it comes down to what you consider a minimal form. Minimal forecast that your model, if it's worth using, should be able to beat. And one somewhat different baseline model that we looked at was predictions made by humans without explicitly using a model. And so one of my PhD students, Nikos Bosser, set up a website where people could Where people could essentially draw, and this has been done before in the US, also in the Delphi project. And the idea is that people can essentially draw a future trajectory on a plot that shows the data up to today. And we did this, and we invited people to take part. In the end, the people who consistently took part were mostly people from within our own research group, two or three people, committed people, which is actually an Which is actually an interesting finding in itself. That it turned out fairly difficult to motivate people to do this on a regular basis. It took five minutes or so every week to do this, but clearly that was too much to ask from any. But what we found, so here you can see four models here in the four different colors. Let's forget about the first one first, and let's focus here on cases. And let's focus here on cases where what we call the crowd forecast, even though it turned out to be not a particularly large crowd, is in whatever this is, violet colour. In orange you've got the Hub ensemble and in blue the renewal equation model I showed earlier. And it turned out that for cases, the crowd forecast, which is two or three people drawing a line on a plot, for most Plot for most horizons, this is one week ahead, two weeks ahead, three weeks ahead, four weeks ahead, and again the lower the better. Ignoring for a moment the different shades in there, the human forecast performed better than the Hubble ensemble. Whereas the Hubble ensemble itself performed better than the renewal equation model, which is no particular surprise, given that we know the Huban sample generally beats individual models. For deaths, this was no longer the case. This was no longer the case, so we felt that was quite interesting that humans seem to have an advantage over the models for cases, but not necessarily for deaths, which we interpreted as cases being very difficult to forecast for a model in general, given the non-stationarity of the system and changes in behavior, as well as policy, which are difficult, if not impossible, to encode in a model and therefore would usually require some form. Therefore, would usually require some form of human intervention in order to be recognized in a model. So, for cases, humans perform better, but then for deaths, that is more of a statistical task, where deaths generally should be well informed for the four weeks in advance that we looked at by what has been observed already in terms of cases and hospitalizations with some. Some scaling factor and some forward convolution. That said, it's not that humans performed terribly, but humans no longer outperformed the hub ensemble. The other potentially interesting thing is just here at the bottom right is an indication of the standard deviation of scores. And generally speaking, Generally speaking, well, it's just actually I'm going to skip this because I think it's I'd rather move on. Okay, so this particular baseline model, a second baseline model of human input turned out to be impossible to beat, or fairly impossible to beat by the computer models. Okay. A third way of assessing the absolute quality of models is. Assessing the absolute quality of models is to consider their calibration. So, presumably, some of you will be familiar with this, but ideally, so this is looking at these plots here. This is again for the ensemble here in where we call this color, and all the other models here in blue, so that the small bars are the ensemble model. And if a model was well calibrated, which is really the minimum you should expect. Really, the minimum you should expect from a model that you trust for forecasts, then 50% of future data points should lie within the 50% prediction interval and 95% within the 95% prediction interval. And what we found is that for cases, this was the case for one week ahead, but then very quickly stopped being the case, both for the 50% and the 95% coverage or prediction interval, whereas for deaths it was more stable. Whereas for deaths it was more stable. And again, this mirrors what was found in the US Forecast Hub, which is that models tend to be calibrated, well calibrated, only at the shortest time horizons, really. And at least for cases. And it becomes very difficult to trust forecasts of cases for more than one, maybe maximum two weeks ahead. And the situation is slightly. And the situation is slightly better for deaths, but then generally speaking, policy is rarely made based on deaths. So, arguably, this indicates that the value of these forecasts for informing policy is really fairly limited. Okay, so that brings me to the second part of just indicating some insights into these forecasts, which is looking at different ways in which we could potentially improve forecasts. Potentially improved forecasts. And this is looking at different possible predictors. And one obvious one that we studied was using cases to predict hospitalizations. So this is from a paper looking at this at the local level in the UK, and this is yet another piece of work that was used to inform policymaking in the UK was the low. Was the local, sorry, hospital by hospital forecasts of admissions. And what we found is, well, it confirmed what we knew before, which is that the ensemble performs best. And then this is using multiple models, including some time series models, so that ones that purely consider the time series of hospitalizations up to a point, and then two regression models that use cases as predictors. As predictors. And these performed not particularly well. In fact, they didn't perform noticeably better than models that didn't consider cases at all, which are up here. And the reason for that was that because of the short delay between, relatively short delay between cases and hospitalizations, to make good hospitalization forecasts based on cases, you really need good case forecasts. And as I showed earlier, that is generally has proven illusion. That is generally has proven elusive. In fact, if we, rather than trying to make case forecasts and then using them in the forecast models, we later with hindsight use the observed cases in the same models, they perform much better. So now the convolution model, which is a simple forward convolution of cases to osmolizations with a given delay distribution, and the regression model that uses cases as a predictor perform much better, especially at the longer time horizon of 14 days. Sebastian, are you still there? Can you hear us? Yeah. No, that'll be on his end. No, that'll be on his end. Maybe we should be doing this all the time at the moment. Hello. Okay. You're back. Yeah, sorry. Not sure what happened there. I'm back. Yeah. Right, I've got another five minutes or so, so I'll just go through that, share my screen again. Okay, so the second predictor was looking at contacts in the so-called COMIC study, which is used to track how many contacts people made during a day over the course. People made during a day over the course of the pandemic. So, this is shown here at the top. So, you can see first lockdown, some level of contacts, which took quite a while to get back up after relaxation in the summer of 2020, then started to decline even before another lockdown was enacted, and then was at a particularly low level at the third lockdown here in early 2021. 2021 with the alpha wave. And so, we so, what we did is we created a forecast model that would take this into account and to see whether this could act as an early warning signal or give some early indication of changes in the trajectory of the epidemic. And it turned out it didn't. So, sorry, there's a label missing here, so I can't really see anything. But trust me if I tell you that we found that this We found that this generally did not help make case forecasts and improve predictive skill. And a third one, however, where including some additional information does and did improve forecast skill is a fairly obvious one, which was the emergence of new variants. But this is also a case where in the forecast hubs where a lot of the models are run automatically and are based on either time series. Based on either a time series model or a semi-mechanistic model, but some fairly automatic processing of the data could clearly miss some trends, such as here in the emergence of Omicron in the UK, where overall case numbers appeared fairly stable up to around here and then increased. But anyone looking at, in this case, SG and target failure as an indication of the Omicron variant would have at this point clearly picked up that this was exponentially increasing. And a colleague of mine and collaborator, Johannes Brafe, did put a model together that he submitted to the Forecast Hub, which ended up performing amongst the best models and clearly outperforming the ensemble during that time of overlap. Okay, so these were just a few snapshots of what we try to learn from these forecasting efforts, and that brings me to That brings me to what we can sort of carefully conclude for the next pandemic and what I think is needed to improve preparedness. To summarize what I've talked about, I think forecasting, especially the early signals such as cases in an outbreak is fairly difficult and because of that, also not particularly impressive, I would say. Generally speaking, the ensembles perform best, but they can also be difficult to interpret because we But they can also be difficult to interpret because we don't know exactly what assumptions went into the or we can't interpret it in light of the assumptions that went into the model as we could do with an individual model. And you get this, we had many discussions about this in the forecast, how we get into the situations where because essentially nowhere did we see sustained periods of exponential growth to stratospheric levels, because there would either be a policy intervention or a behavior change or something generally. Or a behavior change or something. Generally, models perform better that had some kind of fudge factor that would tend towards the lower estimates of cases. But then obviously that becomes difficult if policy is based on that and the model predicates policy. So these kind of individual assumptions that go into models get lost if we focus on an ensemble. But then at the same time, I think these kind of methods and techniques Methods and techniques will remain relevant. And so, this is from this is a plot from a report that appeared on, I think, end of last week on monkeypox by the UK Health Security Agency. And these are cases of monkeypox in the UK by day of symptom onset. And what you can see is what we always see, which is they go up and then they go down because a truncated time series, and cases are reported with a delay. And there's some so this is a in the first. So, this is in the first instance a now-casting problem because then you look at this and you ask yourself, what is the curve trending down or not? Which you could to some degree solve based on previous observations of delays, but for that we need methods for now casting. And so again, Johannes Bacher, colleague I mentioned before, has been putting together this now casting hub in Germany, comparing the performance of different models, and I think Different models, and I think what we then need is to go from this to the kind of the next step, which is we have this prospective evaluation of methods, we need to learn which methods work or don't work, and so we can, I think an agency such as the UK public health agency needs to be in a position where tools are readily available and it's clear which methods work and have which limitations in order to analyze a time series like this, which I don't think is a situation that we're in now. At the same time, there's At the same time, there's a bunch of new initiatives to create these kinds of tools and applications of methods and capacity for doing this. I think it's great. And I think there's a huge risk that there's a bunch of new projects being created and a lot of effort being put into this, but without necessarily useful outcomes that will help us combat the. Outcomes that will help us combat the next epidemic. And I think, in order to get to useful outcomes, what we need is really collaborative efforts where different teams that work on these methods, be it for estimation reproduction numbers or forecasts or nowcasts, collaborate on things like these hubs or in other ways using standardized data sets to rigorously compare methods in a prospective manner as well as generate sustainable tools. Because if research on the method Research on the methods doesn't result in tools, essentially, this becomes this won't be used by public health agencies. And with that, that leads me to thank the people who have contributed to the various bits of work or created the bits of work that I've presented here, people in the EpiForecast group, as well as collaborators and funders, and you all for listening.