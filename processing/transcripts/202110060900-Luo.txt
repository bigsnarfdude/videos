So, thanks for the opportunity that I can share with you some of our work. This work is about contextual dynamic pricing with Professor Wells Western and Professor Yufanvio. And so, here we say that it is distribution-free. free it basically means we want to we will learn the learner cdf a non-parametric cdf of the market noise uh so uh why we should why should we price thing dynamically because maybe the market environment or the customer customer characteristics always change with time so we want to like adjust our prices according to these Our prices according to these features. And we will receive sequential customers. They will arrive one by one, and we want to decide our future prices based on their reactions. So there are many application fields like in the bookings, we want to book some hotels or in the online advertising. I think it is the pricing aspect. It is the pricing aspect of the advertising, which is the topic of our workshop, and also when we need to buy tickets or like call a Uber or Lyft, the prices are always adjusted to trying to maybe maximize the revenues. Also, there are some fairness issues we need to consider, but which may be a very hard future direction. So, because we want to, we are actually doing some price discrimination, but we also want some fairness. So, in dynamic pricing, the data comes from large number of products in online market. And in our setting, each time one customer arrives and he or she is checking a product. So, from the customer side, he has evaluation of this. Has a valuation of this product. So it will come from a valuation model. And from the seller's side, the seller will post the price for the product based on his pricing policy. So based on these two things, which is valuation model and pricing policy, we will have a feedback from the customer. So we call it the customer choice model. So basically, it depends on the feedback depends on the comparison. Feedback depends on the comparison between these two prices. One price is the seller's price. If the seller's price is larger than the customer's valuation, then the customer won't purchase the product. So we receive a binary, maybe zero feedback. But if the seller's price is smaller than the customer's valuation, then the customer will certainly happily maybe buy the product. So we receive a binary one. Receive a binary one feedback. So it is the specific forms of the valuation model and the pricing policy that determines the mechanism of this sequential process. So in our setting, the valuation model is called a linear valuation model. So the valuation model at the valuation at T from customer, which has a characteristic or Characteristic or the feature XT will have a valuation like this. It decomposes to two terms. The first term is the main part, which is the linear inner product of an unknown coefficient theta zero and the customer characteristics XT. And also, XT can involve the market environment or the product features. And also, the second. And also, the second term is a random noise. And this noise plays a very important role in the feedback structure. So we will assume later that this noise will be IID from a CDF, which is if. And that if is also unknown in our setting, in our problem. So basically, we have two things unknown. The first one is this theta zero, which is Is this theta zero, which is a linear coefficient, and we also have a C D F that is unknown, and we will assume that it is kind of not parametric. And our pricing policy, PT, is currently, it can be, it is determined by the seller, and we basically want to design such a policy based on this valuation model and the interactions. So this So, this P here I just write this Pt as the function of the feature at time t and the history, which is a future history at t minus one. So this means that we will decide our price, decide our price at time t basically based on the feature in the key stream. And the customer choice model maybe is Also, is a key factor is a key factor. So, the YC, which is the positive or negative feedback from the customer, is very simply determined by the relationship between the PT set by the seller and the valuation VT from the customer. So, if the, I just want to maybe emphasize if the price is smaller than the But uh, smaller than the valuation, then the customer will purchase. So, we will see uh, like see a binary uh and positive one feedback. And but if the PT, you set a very high price, so the customer won't buy it. So it will be uh so we code it into the YC to be minus one. So this means the customer just leaves without purchasing the product. In this dynamical pricing algorithm, because it's sequential or kind of online, so it is always going with a loop of the information. So, and it falls into the category of online decision making. So, actually, there's not a starting point of these four aspects, but let's get started. Let's get started from the price setting. So, after we set a price, we will observe a feedback because we don't know the valuation of the customer, which we can just see their binary feedback, whether they purchase or not purchase. So, we will observe the feedback. And these are the data and the new information. So, from this data, we can update our so first update our data, and then we can learn our parameters. And then we can learn our parameters, the unknown parameters. So, with a better learned parameter, we can set a better price for the next customer, for the next product, for the next marketing environment. So, these four aspects compose this loop. So, our goal is from the firm's aspect is to maximize the cumulative revenue over the time horizon. Cumulative revenue over the time horizon. And as this ZT is the random market noise in the customer's linear evaluation model, so this ET we will assume that it follows a distribution F, which is the CDF of that distribution. And so there's the first calculation part here. Calculation part here. So, the purchasing probability of a customer at time t is the probability that his valuation is larger than the seller's set price pt. So the valuation, since it takes the form of this linear product and the random noise, it further derives as this term. So, why is it a probability? Because dt is a random noise, so we can. So, we can we will find this f here. So, this probability just would just be the y minus this C D F with P T minus X G transpose theta zero. So we can see that if this P T is very large, then this inner term will be larger. So, the F term will be larger. So, the probability will be lower. It matches our inner. It matches our intuition. So, yeah, it's just a check. So, for us to understand this model better. But, like, if the valuation linear product, which basically composes, forms the customer's valuation is higher, then it contributes to the reverse side. So, this will be higher. So, this inner term will be lower. So, the probability will be higher. Higher. The expected revenue is simply a multiplication of the prices you set and the probability. So there is a trade-off here. If you set a very high price, the probability will be low. If you set a very low price, then the probability will be high. So we can expect there's an optimal price there. So, but we can calculate. Can we calculate the best price if we have the knowledge of this f and theta zero? However, we are not at the beginning, we don't know these two parameters in this problem setting. So the key challenges is that we have the unknown set of zero and unknown non-parametric if. So how should we design the pricing schedule or pricing? Schedule or pricing policy to maximize our revenue. And also, we have a task of learning this theta zero and AF. So these are kind of the main goal is just one, which is maximize the revenue, but actually there's a deputy goal, or there's a second goal, which is to learn the CTA0 and if. These two goals are not fully. Are not fully contradicting to each other, but they are not exactly the same. So they tangle with each other. So this is the main difficulty that we should counter. And also because it's an online decision-making problem, so the data may not be IID, so this is also a problem. Maybe any questions? Maybe any questions from our problem setting? Yes, thank you. Does your model allow the possibility of a loss leader? The way, for example, Uber will give you a $35 discount on food. But the expectation is that you'll continue to use Uber in the future and they will make up what they lost at the beginning. So, maybe can you repeat the first part of the sentence? I didn't hear clearly. Yeah. It is not unusual for a company to offer something for much below the actual value in order to attract customers. And then they hope that they will retain those customers in the future and make up. Oh, I see. Okay. So, you mean maybe there are. So, you mean maybe there are like old customers? Yes, a new customer might be attracted to go to Uber because they get a very steep discount on what they pay for the first month. But after that, they get charged more. But Uber hopes that they will remain loyal to Uber and stay with them. And so Uber thinks that the optimal policy is to take a loss at the beginning, but then make it up later on. Then make it up later on. Yeah, that very makes sense. And I think it's a very obvious strategy from these kinds of companies. But in our problem setting, we don't kind of incorporate such a maybe such a, I don't know if it's just about our, what the customer thinks. So like they will get loyal if like the the the if the first price is. If the first prices are very low, we don't see, we don't incorporate such a structure and we don't find these phenomenons in our problem. I think maybe because we don't consider the customers, maybe so they're like the customers can be the same from time to time. We just assume that the customers are like represented. Like represented are identified as their just features. So there's not a like old customer or new customer, this kind of feature, this kind of information incorporated in our problem. Thanks for the question. Another thing that might be interesting to think about in the context of this problem is instead of having dynamic pricing that Dynamic pricing that depends upon the time, you might have dynamic pricing that depends upon the time and the individual. For example, in old days when people would haggle in the marketplace to buy, you know, a loaf of bread or something, each person would get their own price depending on how hard they bargained. And I could imagine that Uber decides that, well, if Bill Gates wants to ride an Uber car, we can charge him more. In an Uber car, we can charge him more and he won't argue about it. But if a poor person needs a ride, we pretty much just have to get back the cost of the transportation and maybe a couple dollars extra for the driver. So in principle, you could have a model for individualized pricing. Yeah, I think so. I think that's a individualized pricing. The individualized pricing or like personalized pricing is very common in the in the in the just in the title of these kind of these streams of literature and and in our setting uh it's also kind of personalized because we incorporate the features and it's just like uh yeah because maybe the bill gates uh if the bail gate if the bill gates uh comes then we can We can like in this XT, which is the feature of Bill Gates. The one feature would be maybe he's like he has like billions of fortune. So like it's one billion in, there's a one billion in this XT. But like if that's the and also maybe there's a I don't know, there is a category feature to indicate that he is a he is a very rich person, maybe just rich. Person, maybe just rich. Or if that's like another person with another job, so that job doesn't pay very well, then it can also be incorporated into this XT feature. So I think our model kind of can describe such a phenomenon and handle such a personalized pricing problem. Personalized pricing problem. It's just that it may not be very, it can be so general. So it may not be very specific to some very specific problem, maybe just about their fortunes. Yeah. Yeah, thanks for the question. So I think this is a topic that can incorporate many aspects of. Aspects of thinking, like to find the our intuity, find our intuition in the mathematical or statistics forms. And maybe we can have many interesting results or interesting results through these incorporations. Yes, thanks for the question. So there is actually some examples. There is actually some existing works that also adopt the similar model. For example, in this 2019 paper by Jamad and Nasazada, Naserza De. So maybe, I'm sorry, maybe my pronunciation is not correct. So they proposed RMLP and RMLP2, which is two policies. So the first one handles the First one mainly handles the known and low concave F, and the second one handles the case that F belongs to a known low concave location variance family. So in both the cases, we have much knowledge about F and also maybe it's in a parameter family. There is also a very recent work by Professor Fan Jingqing and his students that states that if States that if Kendall states that if is has a M's order derivative and for our case we kind of develop a policy maybe called dip so it can handle unknown non-parametric af only required to be lipships continuous so we allow much for example allow much more flexible shapes of a f beyond log just a uh maybe log concave or and also we don't need no And also, we don't need to know if or know that it belongs to some parametric family. So the noise distributions for so the noise distributions, does it do very like vary a lot with each other? So we kind of test on a real data set. It records many autoloan applications. Any auto loan applications in this data set, the prices, the seller is the lender, and the buyer and the customer is the borrower. So the set prices are like the monthly rate or monthly payment required decided by the lender, which is the seller. The purchasing decisions is the acceptance or decline from the borrower, which is the customer. Uh, the purchase of the customer or the purchaser. So, the contextual information involves includes the borrower's personal information, like his age, age, gender, but actually we should have some fairness on the gender. Yeah, so but there's also many other information that we can share the use. So, after we fit out this data with our model, This data with our model. So, actually, maybe this data doesn't follow our model, surely, because our model is a specific one. But if we fit our data with this model, we can estimate these ifs through all the data. And the noise PDFs for like four of these US states look like this. So they are not actually follow a very clean shape or a very common shape. All a very common shape. So they are so the F, so the PDFs are kind of surely not concave. So this is one motivation for us to consider some more complex f's of the market noise. This is for the Florida and Pennsylvania, Texas, and Virginia. So then there's So then there's one theme of our work, which is we have to do with handle the exploration and exploitation. So and also exploration and exploitation are just the tag, the main tag for the bandit algorithms. So first let us like recall our model, which is the valuation model, a linear product plus the voice following some unknown F. And this zeta zero, blue zeta zero. Zeta zero, blue zeta zero is also unknown. And the feedback YT has the form of the indicator function of the comparison between the valuation from the customer and the price from the seller. So our data is an adaptive stream of these three things, which is the context, context, and the prices, and the feedbacks. Feedbacks is binary. So the XT is contact information, TTS pricing decision, basic basic. TT is pricing decision based on the history data and the current context the firm observes. And also, YT is the customer feedbacks, which has a distribution, but only distribution because it's binary with the purchase probability of Y minus this F. So there will be a optimal price based on the context at time T and this F and the theta zero. So So, the optimum price should certainly give the highest revenue. But when we apply our policy, we don't know this FMC to zero. So there will certainly be a regret, which is the loss of the optimal revenue because we can only post this price PT, but not PT star, which is the optimal. We don't know the optimal. So when we can accumulate these. So, when we accumulate these all of these regrets in this horizon, we will get a total regret, which is RT is the expectation of the total loss from time one to time t. Note that the optimal price will change across time because the context also changes across time. So, there will be an exploration, exploitation trade-off for the exploration side. For the exploration side, we want to learn of zeta zero and f. And intuitively, we should randomly pick a wide range of prices. So that gives us a more complete image of this CETA0 and F. But for revenue accumulation, which is the exploration side, on the exploration side, we may want to set a line or a curve of near often prices. Of near optimum prices because the best price PT might be an implicit function of this context XT. So we should set very restrictive prices according to this XT. Then we can set our price very close to this PT start, the open prices. But in this sense, our prices are kind of more restricted in choice. So, this means we may not learn the CT0 and F very well. So, these are two kinds of tasks, and they are not the same and almost very in a sense, they kind of contradict with each other. Any questions here concerning the idea of exploration, exploitation, trade-off? Promotation trade-off. Okay, thanks. I can imagine that that balance might evolve over time, and that at some points more exploration is wanted, at other times, more exploitation is wanted. Is that something that your model could account for? Yeah, but I think it's also the very main part. Main part of the algorithm design. For example, in Professor Fan's paper, which also considers a very similar problem, they like kind of split the exploration and exploitation phase very, they split them very completely. So, for some kind of consecutive time periods, they were just employed exploration, and for another period, they were just And for another period, they were just for another epoch, they were just applied exploitation. But in our algorithm design, because we will use use very use the UCD idea, which is the upper contidence bound idea, which is a very common approach in band-aid. The exploration and exploitation will kind of mix together, but surely the exploration will be more emphasized. More emphasized at the beginning, and the exploitation is more applied in the end or in the later time periods. That makes perfect sense. Thank you so much. Okay. Yeah. And I think this kind of mixed balance is very well addressed by the UCP algorithm. Addressed by the UCP algorithm itself. Yes, thanks. So the pricing plays a core role because the pricing gives us our data and the data can help us learn the CTA and learn F. But we know that CTA is a linear coefficient, but F is a non-parametric CDF. So they are kind of different learning objective. So like if is also a linear coefficient. Like, if it's also a linear coefficient and they are kind of symmetric, then it will be like, like they will be both like triangles, but now they're kind of different. And also pricing gives us, directly gives us the revenue. So there are two, so it is the core of our problem. So this is just a recall of our problem setting. So if we have too much emphasize on maybe it's just another kind of saying of like the balance. kind of saying of like the balance. So if we too much emphasize on exploration, then the CETA learning and if learning will be very good, but we will be far from the prices that gives us very good revenues. So this will like can't support a very large revenue. But on the other end, if we want to, if we just set the prices to very accumulated revenue, we may Accumulated revenue, we may be far from the object of CETA learning and F-learning. So, our F-learning and CTA learning will be poor, so they can't actually support a good revenue accumulation. So, there we will require a balance between the revenue accumulation and CTAF learning. So, for example, the one-step level exploration and exploration trade-off is that we may want to choose the next price to best help with the estimation of CTA0 and F, or we may want to choose the next price. Or we may want to choose the next price to maximize revenue based on current estimates. Then I want to introduce our global algorithmic design. So we will split the entire horizon, final horizon into episodes according to a trick called doubling trick, which is very common approach in bandit literature. So in actual So in episode one, we will just price randomly, which is which we could see it as a pilot episode. And in episode two, we will first using the data collected from the previous episode, which is episode one, to do the CETA learning. And then we use this given CETA to guide our exploration-exploitation balance of the pricing and F-learning. So basically, we So basically, we won't do the entire circle of CETA learning, pricing, and F-learning, but we first estimate CETA and then guide the online learning of pricing and F-learning. And after we do this pricing process, complete this sequential pricing process in episode 2, we do the same thing at the episode 3. At the Episode 3. So we will do the SETA learning first using all the data, pricing, and feedback data from the Episode 2. And after we get this CETA, we will guide our further pricing and learning loop in Excel 3. So the first talk is how can we use the pricing data from the previous episode to estimate the CETA0. S minus is theta zero. So, our observation is that our feedback about whether the customer will purchase the product is a binary feedback and it follows this probability. So if we write it out, then it determines the probability will be larger or equal or smaller than one over two determined by this linear, actually linear linear. linear linear equation. So we can formulate a linear classification problem with responses yt and the covariates one, xt transpose and pt. So the base distribution boundary of this classification problem will be this line, which involves the theta zero, which is the unknown linear coefficient. So we just adopt the logistic regression to estimate Estimate this classification problem and the linear boundary so we can have an estimate of this zeta zero. So record this process after we have the theta, estimate the theta from the pricing data from the previous episode, we will guide the like simultaneous pricing and if learning. So this is the this this one may be the This one may be the core part of our algorithm. So, this task is joint pricing and if learning. So, this involves like we will learn if and price at the same time, and there's a balance between exploration and exploitation. So, our input is the zeta hat, which is the maybe zeta hat k minus one, which is the estimated zeta from the k minus one episode. So, in the exploration and exploitation trade-off, we want to map. Trade-off, we want to maximize the revenue, so we want to set prices with higher values of the expected revenue. Uh, although we don't know C0 and F, and also the other aspect is F learning, which is operation. So we will set the price PT. When we set the price PT, we will invoke a binary response. So this binary response will correspond to knowledge of F on this point, which is Pt minus XT transpose zeta zero. So like YC is one, which will Is one which will give us a positive indicator, give us a positive indication that this one minus this probability is kind of high. So this F on this point is very low. So by set some prices, we will have the information of F. So one key difficulty is that we only have an estimate zeta height of zeta zero. So we do not know the exact place PT minus D62 transpose theta. Minus this Xt transpose theta zeta zero. So we only know their shadows, which is pt minus x t transpose zeta hat. So this gives us some difficulty on the knowledge accumulation of f so we don't know where exactly we accumulate knowledge of this f, but we only know like we could accumulate the knowledge around some point, around that point. Then we will apply the ideas of ideas of Ideas of descriptization, candidate sets, and upper confidence bound. So, this is a picture trying to illustrate these all three concepts. Basically, because F is, we assume it to be, basically we will trying to try not to estimate the whole F, but focus on some points of F. And so this So these points, these like these small balls are just some prices, not just prices, but the prices minus these linear evaluation. That point will be distributed to these three points, which we want to know more knowledge of on. And they will not be exactly placed to the middle. To the middle, these middle points because we only know their shadows, but we can like fix these shadows to a certain point, then their true place will be around that shadow. So we can see that these points, these balls, they will not be concentrated to a single point, but they are kind of close to each other. So this is enough for us to accumulate some kind of knowledge of F around these points. Knowledge of F around these points. So there are more details about this process. So this is the key difficult we have already discussed before. So like we only know their shadows. And if we have the evicures assumption, then the shadow and its true place are kind of bounded by this one norm of the zeta hat, your estimation of the linear coefficient. linear coefficient so in this in this quantization um we will just uh we will we will only need to focus on a range of f with uh which is like this thing and we kind of disc discriminate this arrange into these sub-intervals and create uh and set out midpoints as m1 to mb we just want to concentrate our final resources on these midpoints so On these midpoints. So to get knowledge of F, what's the value of F on these midpoints? So this will further guide our pricing process. So maybe this is kind of detailed things and maybe a little bit technical, but so I will introduce it, like introduce the candidate set, which is a second opinion. So since our here so since our when we set price pt we have some we will anchor our this point on anchor the f on this pt minus x transpose zeta zero so we will have a feedback yt following this distribution so uh we will have further knowledge on this pt minus x2 transpose zero zeta zero but we can only control pt minus x t transpose zeta hat because we don't know the real value of zeta zero so our strategy is concentrate these uh Is concentrate these shadows on the midpoints. So, this will invoke the candidate set ST, which is the price we can only choose from. So, because we concentrate this on the midpoints, so our prices can only be of this discrete side, which is the MJ plus XG transpose theta hat. So, this is called the candidate set at time t. So, when we choose any. When we choose any parts from this candidate set, we will concentrate this shadow on these midpoints. So the true value will be around the midpoints. And this will give us some knowledge of F on these points because from all these 0, 1 binary feedbacks. So then we can accumulate knowledge of F near these MJs. So then there's the one core which is the There's the one core which is the UCB-based optimism algorithm. So maybe consider those prices, then we will have the binary feedback Y is. And this will give us the Bernoulli of Bernoulli distribution like this. So because the Zeta hat is close to zeta zero, so this term is very close to the midpoint. Is very close to the midpoint. So we will have more knowledge of this if on these midpoints. So for past data, we can construct upper confidence bound of this one minus fmj for one for one month fmj. So the upper at the upper competence bound because from these all these binary feedbacks. So its usage is help us to select price because they expect. Select price because the expected revenue when we set price PT from these candidate sets will be this term, which is the PT multiplied probability. So if we have the upper confidence bound of this probability, we can use the very common optimism in the face of a uncertainty principle invented in literature. So we will just choose the price PT that max maximize the optimum. Maximize their optimism expected reward. So there are certain, there are maybe 10 prices in the candidate sets. We just choose a price that can maximize this optimism expected reward. So we will choose the price in candidate sets that maximize this thing, which is the optimism expected reward. So the question reduces to how can we Reduces to how can we construct this UCB? Because we have a perturbation here for each midpoint. They are not exactly one minus Fj. So they are kind of spreaded. So how can we construct a UCB from these perturbed, maybe perturbed binary feedbacks? So the UCB construction will be discussed later. So it should adapt to the, in general, the UCB construction. In general, the UCB construction should be attached to the rule structure. Actually, our single episode pricing problem can have a formulation of a perturbed linear banded formulation. And an appropriate UCB construction will emerge naturally from this TLD formulation. So, our key tool is the perturbed linear band aids. Perturbed linear bandits. So the bandit is a general framework for reward maximizing with unknown action reward structure. So for linear bandits, for any the action reward structure is in this formula. So the reward zt will be the linear product of some linear coefficient and the action vector plus some conditional mean zero. conditional mean zero conditional mean zero uh noise so and for our perturbed linear band aid the with some perturbation cp we say that the cassiet the linear uh coefficient will change over time and these also these uh cascades are unknown but but they they satisfy that um any of any two of them are close to each other so the close measure will be the cp and also Cp. And also, this is equivalent to there is a central linear parametric size star, which satisfies any linear coefficient is close to this cassioid star. Xdp equals to zero, then this reduces to the linear banded where all the linear coefficient across time t is the same. So our briefing will be, there will be three equivalents. So our single episode pricing problem will be equivalent. Single episode pricing problem will be equivalent to a PLB perturbed in a bandit, and the optimism algorithm with a special UCP construction will be correspond to a M in UCP algorithm applied to the designed for the PL perturbing abandoned. And also the discrete part of regret, which we'll introduce later, will be correspond to the regret of the PLB itself. So, but there is also a continuous part of regret. Also, a continuous pattern regret, we need to further bound. So, in the PLB, so this is just to discuss the PLB formulation of the single episode pricing problem. So, the casai T has this form. So, then we can write the expected revenue as the disk IT, the inner product of this cascade and the QTPT, which is a vector with all zeros except for the J term. And the J term is determined by. And this term is determined by PT, where PT is just this quantity from the candidate set. So basically, we kind of write the single episode pricing problem into a petrolinear vantage problem, and the central parameter will be the true 1 minus f values on these midpoints. But why it is perturbed, the true linear. It is perturbed, the true linear coefficient is perturbed because we don't know the zeta zero. So there is zeta hat at each time t, and the perturbation can and it's not the same because the xt contexts are different. So it's just correspond to the different balls, like placed on the place close to the midpoints. So the lemma one is they are, it is our single active problem is equivalent to a perturbed band date. Equivalent to a perturbed linear band date with the perturbation constant, which is Cp, is proportional to the L, which is the Lipschitz constant, this Lipschitz constant of F, and the zeta hat, the estimation error. So if our estimation error at the beginning of each episode, which is estimated from the data from the previous episode, is very large, then the perturbation will be large. So the larger perturbation will indicate larger regret, as we'll see later. As we'll see later. And also, the Lipschitz constant is also proportional to the Lipschitz constant. So if the high perturbation, the balls will be more distant to each other. So this will concentrate less information because they are, because we only want to like we want to concentrate our resources on the middle points of it, but actually they are distant. They're distant, so distance to the middle point. So, this will give us less information and a larger regret. So, this is I don't need to interrupt you, but I want to make sure that you know that you have about 10 minutes left. Okay, thanks. So, it's just, yeah, maybe more details. So, thanks. So, for the second equivalent lemma, it's just our the M-link UCB for the perturbing linear bandit can invoke a UCP construction like this, and it is kind of Like this, and it is kind of reasonable when we check each terms. So, this is the main term, and this is basically the uncertainty term. So this is the relationship. And also, the regret can be decomposed into two parts because we have a tentative set. So, there's a discrete best price from this tentative set in the overall best price. So, there's a continuous part and discrete part regret. So, there's also regret equivalence between. Also, a regret equivalence because of the two equivalence we constructed before. So, the general PLB regret is a lemma which has this form. It's order with basically the square root of t, but plus a linear tail, which depends on the perturbation. The perturbation is larger, then the regress is larger. So, this also corresponds very similar to the misspecified. Very similar to the misspecified linear bandits. So, because the regrets equivalence, so we can bound using the PLB regret result to bound the discrete path result, discrete path regret. So the linear TL is proportional to the estimation error and the basic from the perturbation constant we discussed before. So for the, so then. So then, if we further bounce the continuous part regret, we can bounce the single episode regret. So, it has the form of the square, t to the two over three order, and plus some plus some extra term proportional to the estimation error. And we have some so and then after we dislike. And then after we construct the regret for each single pricing episode, we can merge them together to get the overall regret rate, which is T to the two over three plus some extra terms, which depends on the estimation error. The problem is we are not able to really constrain these, theoretically constraint these extra errors of the estimation of this zeta hat. Of this theta hat. So, but we actually observe the estimation errors into the total regret bound. So, if the estimation error has this kind of rate, then the overall regret will have a corresponding rate, which is t to the one of alpha. So like if the so the second term will have this rate, so the overall regret rate will have this t to the two over three and maximize, maximize with one minus alpha. Maximize with one minus alpha. So we discussed some simulation settings basically from all different noise PDFs have multi-models and our regret result is just because comparing with the RMLP and RMLP2 from JavaBad's paper. So because it's kind of nature because we can handle like more complex shapes of the CDFs. Complex shapes of the CTFs. So, and we are aware of that complexity, so we can have a better performance. And also, we test our data, test our method on the auto-loan data set. So, this is the instruction is the same as before. And our regress, we also kind of estimate the noise distribution from these polo data sets for the entire US. Data set for the entire US data or just for California. And our record results is better than RMP2, which is the more robust one in Jamma's paper. So yeah, so and also just summary, so we can handle both unknown linear parameter and unknown market noise distribution f. Distribution F. And we design an algorithm. We say no operative linear bandicoke core and provides theoretical grants, guarantees. So it has some competitive performance on simulation and real data. That's it. Thanks. Thank you so much. Gosh, in terms of the non-parametric estimation of the CDF, I would think there would be a lot of Bayesian. I would think there would be a lot of Bayesian prior information about F that would be available. Is that something that you could use? Yeah. Yeah, I think thanks for the good question. I think we can certainly, so the answer would just be. Certainly, so the answer would just be it. So the answer, in my opinion, would just be we can use this extra information. It's just I'm thinking about how we can incorporate this information into our framework, into our algorithm framework. Maybe it should have some interaction with our CLB preservine advantage. A possibly embedded problem. So, if like we have some BNC prior for this unknown, these Kasai Ts. So how we can, yeah, I think that's a good question. I actually don't know in the Benti literature how we, although in the, although in terms of Also, in Thompson Sampson, which is also a very common approach and have a very good performance for banding problems, they actually using the basing framework to estimate this size. So I think we can incorporate the basic information for size. information for cases because casi is just the middle points, the middle points values of f. So this casai star. So I think we can incorporate the bins in prior informations. Thank you. At the beginning of your talk, much of what you said reminded me of things that I think Y Guo is going to be talking about later today. But then you took a totally different direction. Direction. And so maybe it's not as relevant as I thought it would be at first. But if you have time, I hope you'll sit and stay and listen for E's talk. Yeah, maybe I didn't remember incorrectly. It would be at maybe 1.40. Yeah. I'm looking forward. Yeah. Excellent. Okay. Good. Good. Anybody else have any comments or figures? Else, have any comments or suggestions? The dynamic pricing issue is, I think, an intriguing one. It's certainly something that drives, is relevant to a lot of computational advertising. But I also sort of like the idea of personalizing it and sort of letting each person haggle for their own best price. For their own best price, which is not something I've seen happen in computational advertising. But maybe. Okay, so you mean that maybe it is not very closely kind of integrated into the online advertising, like the personalized pricing. Personalized pricing idea. Dynamic pricing is absolutely integrated into computational advertising. And as your examples at the beginning with Uber and Lyft made clear, that's certainly part of the strategy.