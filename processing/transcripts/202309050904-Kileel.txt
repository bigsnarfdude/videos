An issue came up from me, and I can't leave the US until that's solved. So, in any case, I'm very happy to be joining. And I'll talk about diffusion maps for analyzing shapes, collections of shapes. So, there's basically two main issues with analyzing collections of shapes and variability within them that I want to address in this talk. Within them, that I want to address in this talk. And the first is that we probably should not be measuring distances between two shapes using the Euclidean distance, using L2. Transport-based distances capture more intuitively what we might mean by a distance between shapes. And there are actually various fast approximations to optimal transport-based distances. Okay, so I'll sketch a theorem related to that. The other main issue I want to address is that shape data can often come with arbitrary rotations unless we've aligned or registered the data. And so I'll describe, I'll sketch at the end of the talk a principled way to perform infinite data augmentation. Augmentation when in such a situation, okay, to deal with the rotational ambiguities. Okay, so there's these three references. The first two relate to manifold learning with optimal transport-based distances, and the last is submitted presently, and it's about dealing with the rotational ambiguities. Dealing with the rotational ambiguities within manifold learning. Okay, motivation. So I'll take cryo EM here as motivation, but the techniques I'll describe are actually very, very general and maybe applicable to other shapes-based analysis settings. So CryoEM, if you're not familiar with it, is a microscopy imaging technique. A microscopy imaging technique that seeks to determine the 3D shape of protein molecules without crystallizing the molecules. So here you have a cartoon of the CryOEM experimental setup. A bunch of sample molecules are placed in a thin aqueous solution, which is flash frozen. And then an electron beam produces these, produces noisy tomographic images. They're actually much noisier than the. Images. They're actually much noisier than the ones indicated here. And we seek to get the 3D structure back. So, in this, you can already see the two issues that I mentioned coming up. So, the molecules may not be identical to each other. There could be some conformational variability in the sample. And also, each image here has attached to it a rotation. So, there's also a rotational ambiguity. So there's also rotational ambiguity here. Okay, so CryOEM was recognized by a Nobel Prize six years ago. And it delivered a 3D, the first 3D view of the COVID-19 spike protein in 2020. Okay, so I want to stress that cryo EM can help reveal a continuum of different conformations of the same molecule because we don't crystallize the molecule. So it's an appropriate setting for manifold learning. Okay, so just as a cartoon, how should we understand a molecular conformation space? This is sort of the applied. Information space. This is sort of the applied goal of this line of work. Okay, so here's some ATP synthes. It rotates around this axis. We'd like to say something like qualitatively, this set of confirmations is a circle because the top part rotates and there's just this one degree of freedom. Okay. Okay. So. Okay, so diffusion maps gives tools for making statements like that more rigorous. Okay, it's a general nonlinear dimensionality reduction technique. The input are n data points, x1 through xn in Rd. D is typically very large, like say corresponding to the number of voxels in a volumetric array. Volumetric array. The goal, roughly, is to find just a useful embedding of the points into RE, where E is much, much less than the original dimension D. All right, this is the goal of dimensionality reduction. So, for example, in CryoEM, the XI could be low-resolution reconstructed 3D volumes, in which case D may be a D may be a million or more. Of course, PCA is one approach for dimensionality reduction, but it's not particularly helpful if the points aren't well modeled by a linear space. And in alternative is diffusion maps, this is working well when the points roughly lie on a low-dimensional manifold, which could be curved. Then we get natural coordinates via diffusion maps. Okay, so just a quick reminder on diffusion maps. The input is the data points. One chooses a base metric D on the data points and an affinity function K sigma from non-negative reals to non-negative reals. This often will be a Gaussian kernel. We then form We then form the a weighted graph on the data points. It's really an n by n symmetric matrix, Wij, and its ij entry is the affinity function k applied to the distance between data points xi and xj. Okay. One then can talk about the graph Laplacian corresponding to this. Corresponding to this, this weighted the weighted graph on n nodes with the weights wij. This has this formula. And to express it in terms of the weight matrix, it's w with changes at the diagonal. And so as I've defined it here, the Graph Laplacian is negative semi-definite. It has a negative eigenvalue. It has negative eigenvalues or non-positive eigenvalues. One of them is zero, corresponding to a trivial eigenvector, which we discard. And otherwise, we get eigenvectors phi1, phi2, and so on, corresponding to the ordered eigenvalues lambda1, lambda2, etc. So diffusion maps uses the eigenvectors of this graph Laplacian to produce the low-dimensional embedding of the data. Produce the low-dimensional embedding of the data points. Okay, so the first eigenvector gives one coordinate and so on and so forth. Standard theoretical justifications prior to our work for using these eigenvectors relied on the metric being the Euclidean base metric. And as far as I know, and the Know and the typical traces of the affinity function, like I said, could be the Gaussian kernel or maybe an indicator function. Classic sort of guarantee for why you might be interested in these eigenvectors is that if the Xi are IID uniformly sampled points from an embedded compact manifold M in Rd. Then Then this matrix, the Graph Laplacian, converges in some sense to, in probability, to a natural differential operator on the manifold, namely the Laplace-Beltrami operator. And moreover, the eigenvectors of the matrix converge to the eigenfunctions of the operator. Okay, so this is to say that we're getting natural. That we're getting natural coordinates on the underlying manifold. If the points are coming from an underlying manifold, we get natural coordinates in this way. Okay, just to highlight a technical point, within the classic proof of this, it's important that the distance is actually the Euclidean distance because the Euclidean distance. Because the Euclidean distance approximates the geodesic distance on the manifold to second order, whereas like other choices of norms do not. So Belkin and Nyogi in their important, one of their early important papers made a point of this approximation, which holds for L2. Okay, on the other hand, intuitively, non-Euclidean metrics are going to be Euclidean metrics are going to be better for analyzing shapes when the axi are images or 3D volumes. So the earth mover distance, to define it informally, is the minimum work required to move shape one to shape two, where the shapes are considered as piles of earth. So we have to convert them technically to probability. Convert them technically to probability measures. The drawback is that the earth mover distance or this optimal transport can be expensive to compute if we want to compute this exactly, the minimum work exactly. It's a linear program, but it's equivalent to a weighted L1 norm in the wavelet coefficient coefficients. So there's a wavelet earth mover distance, which can be computed in linear time, and it's just a Linear time, and it's just a you take the wavelet transform of the of the two signals and then a particular weighted L1 norm of their difference. Okay. Also, what about rotational register? What about registration? So, when data points are volumes or images, we could get unwanted variability in the collection of them just coming from rotational ambiguity, right? If each volume is specified with an arbitrary rotation, then this will sort of enlarge the data manifold M in a way that we don't really care about. So, sorry, one could try to register all the data points or align all the data points, but in practice, this might be expensive for subject to errors. Okay, so that's the setup. Some results. So, we applied diffusion maps. We applied diffusion maps, forming the graph of Placian and extracting the leading eigenvectors with this weighted, with this wavelet Earth mover distance, and compared it to the diffusion maps with Euclidean base metric. This was in a simulation based on that ATP synthas molecule I showed before. So the data points here were various 3D conformations of that ATP synth. Conformations of that ATP synthase molecule, which has a rotating top part. So we expect something like a circle to come out. And in the bottom here, we have the number of data points n. The top row shows the, we take two eigenvectors here with both choices of base metric. The top row shows the embedding with Euclidean base metric. The bottom With Euclidean base metric, the bottom row shows it with wavelet earth mover distance. We see that we get much better results with fewer samples with the wavelet earth mover distance. Okay, so this isn't with Euclidean, it's not really resembling a circle until we have something like 800 confirmations. Whereas already with 25, we get reasonable results just with the transport base distance. Transport base distance. Okay, so we have some analysis on this. So this is joint with Amit Moscovich, Amit Singer, and Nathan Zalesko, who's a student. So if, like I said, the classic analysis does not apply when there's a non-unital A non-Euclidean base metric. However, you can sort of go through the proof strategy and make a number of changes and show that with a non-Euclidean base metric, the graph Laplacian does converge to a second-order differential operator. And here we're describing it. And it's not. And it's a, it's a, it's not as simple as the Laplace-Beltrami operator, but at least it's a self-adjoint second-order differential operator. Okay, so I won't go through this technical statement and instead just contrast what we get with the Euclidean case. So with a non-Euclidean base metric, there is still a second-order stelfa joint limiting operator. One can show that sort of our formula does reduce, in fact, to the Laplace-Beltrami operator when the base metric is Euclidean. So that's a good sanity check. However, in general, it includes a first-order term as well. And moreover, the operator is extrinsic, meaning that it changes if we re-embed the manifold isometrically. Okay, so just to show quickly a small example: a circle in the plane. Circle in the plane. This is really a very toy example with a weighted L1 base metric instead of Euclidean. What do you get? Turns out that this is the second order differential operator one obtains. So it has a linear term and sort of the coefficient function on the linear term is actually has this sine component. So it's not even. It's not even smooth. Hey, and the formula indeed from our theorem actually does agree with numerics if we sample points on the circle and extract eigenvectors. Okay. The other main issue I want to address is symmetries in data and shape data. So images can be rotated in plane. In plane, and for volumes, such as protein molecules, can be rotated in 3D. So again, one could try to align all of the data points before passing through the spectral embedding technique, but this could be expensive. And also, the alignment may not be so easy to solve, like if you have two. Solve, like if you have two significantly different confirmations, there may not be a very prominent global minimum for the alignment problem in that case. So how do we deal with this and how do we actually even exploit this symmetry? Well, again, going back to classic results with, for example, Belkin and Nyogi, there's a convergence rate for the spectral embedding. For the spectral embeddings. And the main, again, the main thing I want to highlight is just this big O term. This is the error in the convergence, and it is controlled here by the dimension little d, which is the dimension of the manifold M, the data manifold M. Okay, so this epsilon here tends to zero. Epsilon here tends to zero. It's the bandwidth of the affinity function. Previously, I was calling it sigma. So this is a small quantity here in the denominator, close to zero. So for D larger, we need a very large number of sample points, capital N here, to make this error small. We need a larger number of sample points. Larger number of sample points. So, larger dimensions for the data manifold means more data is needed. This is the sample complexity of this method. It grows with the dimension of the data manifold. So in new work, we proposed this variant of the graph Laplacians, where effectively you try to add in all rotations of every data point at once. Once. So it's not the same as just say picking like your favorite 1000 rotations and appending them to the data set. Rather, this is like simulating, or this is sort of mathematically equivalent to adding in all the data points, all rotations at once. And we need to introduce this function. this function this this this function the the this space which is uh the data set times the group k and then uh strict we we define an operator that that operates on functions on this space so the the functions actually take a data point and also a rotation okay um Yeah, so there's an analogous way of defining a graph Laplacian that operates on functions which accept a data point and a rotation. Here's a formula for it. This is a way of carrying out infinite data augmentation in diffusion maps. Data augmentation and diffusion maps. The main result I want to show is that doing this improves the convergence rate for graphloplastins. So this here is again the convergence rate, but now where we perform this infinite data augmentation and we're working in this new setting. Then the Setting, then the sample complexity is controlled by the dimension of the data manifold D minus the dimension of the symmetry group. So for rotations in the plane, the dimension of the symmetry group is one. For rotations in 3D, this is three. So there's a significant improvement in the sample complexity that comes about because we add in all, we effectively enlarge our. All we effectively enlarge our data set by adding in all the rotations. Okay, we have some numerical results confirming this. In the interest, so more details on this or in this recent paper of ours. I'll just go to the conclusions in the interest of time. So there were two main ideas here. were two main ideas here. One is that transport-based distances are what we would like or what we should use when I claim when analyzing shapes. And also another important point is that shape data can come with arbitrary rotations. A standard way to deal with it is through registration. But alternatively, though, one can perform data augmentation. And we sketched here an And we sketched here a way of doing this by adding in all rotations at once to get game. So that's all. Thank you. We have three minutes for a question. Yes. Thanks for the talk. I have a question. Let's say that we know that the molecule has some symmetries. That the molecule has some symmetries, or it's a trimer. So, can you put this as a constraint in the construction of the operator such that your eigenfunction would be so degenerate? It's a very interesting question. So, yeah, I haven't thought about how symmetry is a Thought about how symmetries of like symmetries of the molecule would influence the operator. Um, so yeah, I mean, one could potentially leverage that to improve the convergence rate even more. I'm not sure. But on the other hand, if you just use like our existing procedure, you will still. Um, you will still um gain improvements over the um usual diffusion maps, but yeah, thank you. Right, I would just refer. Let's say I want to uh I want my operator to, at the end of the day, have those symmetries. So that is my goal. Uh, so again, the question: can I constrain the construction of the operator to have such property? Yeah, so uh, like. So, like, if so, in this case, like, I think it may come out that the operator you compute by our method will have those symmetries automatically. If the data has those symmetries, then the data manifold M also has these symmetries. So, it should come out. Yeah, because it will not necessarily come out. It will not necessarily come out. There will be a little noise if this is so to like enforce the symmetry as a hard constraint. I'm not sure how to do that in the construction. It's a good question. Thanks. Another question, Jeff? Just to kind of go to the very beginning and the justification of using UGMAP to get the manifold. When you think a practitioner would want to know, like my manifold has this type of shape, versus just getting high-quality samples along that manifold. A lot of structure drawing papers, they look into a lot of details about specific regions and this specific human fragment here. And so, yeah, have you thought about like Thought about like what types of setups where people want to know how to order their states on a manifold? You might have samples and want to order that. I'm just wondering, like, once this is all, this machine is working and it's efficient, how can it be quite right? So initially, it's going to give qualitative information. So it could shed light on sort of how many degrees of freedom are there in the confirmation space. Of freedom are there in the confirmation space? What is the dimension of the confirmation space? This might be of interest. Another classic use of diffusion maps is denoising. So we can use these eigenvectors to denoise volumes. That might be useful. So if one had like a low-resolution volume, you could project it onto the space. You could project it onto the space spanned by various eigenvectors of the graph Laplacian, and that could denoise. Yeah, I don't. It's also a good question. And I don't know if this method is going to deliver for you high-resolution samples. So it's doing something different. Sounds good. Thank you so much for the talk and pointing. For the talk and for answering the questions. Thank you. Okay, so I'll switch off. 