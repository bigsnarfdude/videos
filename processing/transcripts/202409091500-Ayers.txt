So, I'm going to tell you something, but I think I can start with this quote that I have: that one learns more from defeat than victory. I think it's something that probably most of the people in this field hope is true. Anyway, and I will tell you, and maybe we will learn something, but we will start off by saying that it's a defeat, right? So, this is the type of talk sometimes. I tell my students that there's two. My students, that there's two types of research: there's research that works, and there's research that's a saga of failure. This is a saga of failure. Okay, so everyone who doesn't want to learn about that can fall asleep. It's nice snap time anyway. This grew out of, I was in Chile, I was visiting Carlos Cardenas, who's the next speaker, and I was telling Jose Luis about a neat trick that I had learned. And he said, Oh, you should try this. Said, oh, you should try this on the gradient expansion. It was like, sure. And at that point, there was a student, Zhao, who was from Mexico and in my group. And I came back and I said, Zhao, let's try this on the gradient expansion. And that's where this research came from. So to start off, when ChatGPT came out last year, the very first thing I did is I typed this question into ChatGPT. Question into ChatGPT. I thought this would be a really good test. And I checked it again last night. And I can tell you that ChatGPT knows no more helpful things now than it did when it first came out. It's very amusing. I will say that now what it says sounds somewhat more intelligent, but there's no insight there. So we're still at a point where human intelligence is beating artificial intelligence, at least for orbital-free DFT. And And let's just think about why this is hard. The first thing, and one thing we're going to see a lot in this talk: so, this kinetic energy is a big number, right? So, we got away with a lot of stuff in normal DFT because we were approximating small numbers and a relatively big relative error and a small number is less important, right? Not the case here. Is non-differentiable essentially everywhere, right? So, you're gonna build building a functional. You're going to build building a functional that doesn't have any derivatives. That's going to be hard, right? And we understand that if this doesn't happen, you have catastrophic behavior, like your diatomics associated to things with fractional charges and weird stuff like this, right? So we expect sort of catastrophic chemistry unless we can somehow build in some non-differentiability. It's actually discontinuous, so it's at best lower semi-continuous. At best, lower semi-continuous. You can find points of discontinuity anywhere you touch the boundary of the acceptable sets of densities. So, not only is it not differentiable, it's not continuous. Okay, now everyone who's trying to do machine learning, your universal approximation theorem just fell flat on its face, right? So it changes discontinuously with molecular geometry, which is unpleasant because you might say, I just learned the kinetic energy straight. I just learned the kinetic energy straight from my positions of my nuclei. It changes discontinuously. In this paper with Mo Levi and some other friends, we have explicit counterexamples for that. So explicit examples you can check. It's inherently non-local. Also from this paper, you can sort of set up an equation that has your kinetic energy on one side and something that's explicitly non-local on the other side. And so you go, oh, well, it must be. Other side, and so you go, oh, well, it must be non-local too. So, local functionals are also sort of doomed to have the right wrong structure. And the really unpleasant thing about that is as soon as you decide you want to be non-local, well, that was the only thing that we were sort of avoiding, right? The kinetic energy as a function of the density matrix is a function of R, and we don't need to consider S very, very big, right? R and a little bit further away, right? And so. Right, and so already we were sort of, and that's the trick for linear scaling DFT, right? So, non-locality is already getting us edging us towards Colin Cham DFT. And Sam alluded to this equation, or an equation like this, earlier on today. So, these are the inrepresentable equations. It's something I did with our chair Shibundu. This is the bad thing. This is the bad thing is you can prove, and it's done in this paper, that if you violate this constraint, there will always be a system for which you have an infinitely large error, right? Or an arbitrarily large error is a better way of saying it. And this is seemingly an innocent constraint. This essentially is minimizing a non-interacting energy. The catch is that it has to be true for every possible potential, and this zeta is like inverse effective mass. Inverse effective mass. So it has to be true for everything. And whenever something has to be true for everything, then it actually becomes a strong constraint. And Sam alluded already that this isn't satisfied by a lot of things. And when it doesn't, then you'll get variational collapse, for example. And I will go on and say, I know variational collapse is a thing. And when I show you data, we're going to be just doing it post-SCF. I believe that unless I can get something useful post-SCF, there's no point in seeing whether or not I can. There's no point in seeing whether or not I can optimize it, right? We do post SCF first. Okay, so now what's my personal saga failure? So the very first thing I did was also something I did together with Shubin back when I was a graduate student. Is they said, ah, well, we know it needs to be non-local. Moments of the electron density are non-local. Let's write kinetic energy as a function, now a function of the moment. Now, a function of the moments, which are functionals of the density. And this is beautiful, right? You can actually write down a formula for the exact functional. You can write it down, right? And it's done in this paper. I'm not saying it's practical. I'm saying it's beautiful that you can write down the universal functional exactly, right? It's true for other functionals, not just kinetic energy. Energy. But in practice, the results are disappointing because, of course, you don't take your exact functional and it isn't size consistent, or at least it's really hard to impose size consistency on moments. That's sort of the drawback of this. So the next thing I did is I said, ah, let's look instead of the density, let's look at the pair density, the three-electron density. So I put in some non-locality just through that. And I thought, oh, well, that'll be a good way of dealing with this. This. They're beautiful mathematical results. You get exact bounds on the kinetic energy. The generalization of the Weiss-Hecker bound, in fact. Again, you see, results, disappointing, right? Just didn't work all that well. I think if Carlos talks about what is in his abstract, then his next talk will talk some about trying to put in some information from the strictly correlated limit and the exact form. Limit and the exact form of the kinetic energy functional and the strictly correlated limit. The other thing that I've tried was tried using the weighted density approximation. So again, moving towards a non-local functional. And the reason for that was you could put in these exact inrepresentability conditions. You could put in the non-differentiability. Still, disappointing results and expensive, like slower than Khon-Sham DFT. Than Colin Sham DFT. You had to determine the right reconstruction of the density matrix, even though it was approximate, it was expensive. So now we're back to when I was very depressed visiting Carlos in Chile and was already thinking maybe we should go back to the very beginning. And the very beginning is a gradient expansion. So here's the gradient expansion. I've just written it down. Already you can see. Written it down. Already you can see that the fourth-order term is a little bit unpleasant. There's a sixth-order term, and everyone can guess why I didn't bother to write it down. We did code it, but I'm not writing it down. And it's even worse than that, right? We often leave off this Laplacian term here, but there's a bunch of other stuff in these expressions that technically is zero when you integrate it, but is not zero locally. So if you're going to play games locally, So, if you're going to play games locally with convergence of the grading expansion or try to resume or try to play games, really you can argue that you should be including all these terms, including the one that's fallen off the bottom of the screen here, right? Okay. So we're going to, in fact, I'll show you results for both with and without these extra terms. One thing to keep in mind is sometimes results with the extra terms are bad, right? That doesn't necessarily mean they're really bad. It can also mean that because Bad, it can also mean that because they have just gigantically high derivatives, that it just was numerically bad. And in some cases, we just didn't nail down every last little digit to precision there. Okay, so now I'm going to explain to you what we did. All the results I'll show you are PBE. They're post-PBE results. I used UGBS Spaces. And we did atoms, hydrogen through argon, and Atoms, hydrogen through argon, and these small molecules. Okay, the data, the results I show you, the errors will be just for the molecules. And that's because every now and then I'll get one parameter that will show up. And when I have one parameter, I fit it to the atoms. I show you data for the molecules, right? I'll allude to at times, but what I can promise you is there's no, I'm hiding nothing by giving you the molecules. This is pretty good for the atoms. In no cases are the In no cases, are the worst mistakes where you might think they would be the hydrogen or the argon, right? So it's not that the worst mistakes are at the ends of our atomic sequence, right? It's a fair comparison. In fact, the errors get better if we average over the atoms. I can't tell you why, but the errors will, trends will be the same, but the errors will go down a little bit if we included the atoms. But in the spirit of not testing against the same set I fit to, I'm not going to do that. The motivation for this protocol is basically that if I was doing a basis at limit calculation, then you do a basis set limit calculation. It doesn't matter if it's an approximate functional, but you have a density to kinetic energy mapping, right? And UGBS is close enough to exact basis that certainly for the types of errors I'm going to show you, it's close enough. Now, we've also everywhere did HartreeFock 2 because a lot of these people who do Because a lot of these people who do, when we do post-SCF, we do post-HartreeFock instead of post-DFT. And I thought it was interesting, given that post-HartreeFock isn't really rigorous because it's not a local potential to see whether or not it would work. I can tell you up front, nothing changes, right? Maybe some digits change by a tenth of a heart tree, usually less. Nothing changes. So no reason to do that. We coded our arbitrary order derivatives. So, some people are saying they don't have code for arbitrary or high derivatives. This is a code for high derivatives. I'm not suggesting doing it with a Gaussian basis set, but it does work, right? So, we've tested up to 30th derivative or something like this. There's a normal version of the code, and then there's a CUDA version of the code that's, you know, a thousand times faster. The CUDA version does not do to arbitrary order, it does as much as we need. Order. It does as much as we need it. It doesn't go beyond that. We have, we integrated it with our in-house package. We use our in-house package for parsing the data and things like this. So these are sort of the key things that are there. I've linked to all of them. But if you just go to qcdevs.org, you'll see links to all of that, all of them there. So easy to remember. And in fact, a lot of the publications have just come out. So that's our rough protocol. So let's start by remembering. So let's start by remembering where we are, right? So here's the gradient expansion. This is mean absolute error on those molecules in atomic units. So Thomas Fermi is bad. Second order gradient expansion is sort of remarkably good. We've seen that several times today. And then adding the fourth order getting worse, the sixth order formally diverges, right? So I'm getting a huge number here. That's just the number. Number here, that's just the number my integration grid gives me that approximates infinity, right? If I did this, including argon, it goes up to something like 10 to the 10. And in fact, we can see it here. Here's my gradient expansion for neon. And you see, for instance, that the T6 does something very, very strange, right? And the main thing, and this is something that we've already seen, is it doesn't really look like this expansion is. Really look like this expansion is converging. Maybe it's converging in this sort of physical region here. Maybe it's true that T2, you'd expect these terms in the Taylor series to be getting smaller and smaller and smaller, and maybe they're getting smaller and smaller and smaller. But certainly it isn't true asymptotically, and in fact, they have some weird behavior. So when you see this sort of thing, then you say, well, I maybe don't want to just use my Taylor series. My Taylor series, my series expansion and the gradient, maybe I should be using some sort of way of intelligently dealing with this series, right? And so the first thing you'll learn if you start looking at series is they say, well, this is an asymptotic series if the point where it stops converging, at least maybe it's asymptotic. And so maybe what we should do is we should try to stop at an optimal order. And the classic way to stop at an optimal order is you say, as soon as you're Optimal order is you say, as soon as your correction starts getting bigger instead of smaller, then maybe your corrections aren't corrections anymore, right? So your correction should be getting smaller. So this is exactly what we do. We say, whenever one of the terms is bigger than the term that came before, we're just going to stop. Now, we're doing this locally at every point in space, right? And if you do that, then all of a sudden now these numbers get a little bit better, right? And in fact, Better, right? And in fact, the T, you know, the sixth order now gives you something, right? It's not absurd. You can do it with or without the extra terms. This is a case where I think with the extra terms, it's probably a numerical issue that I'm just not converging it as tightly, but I wouldn't rule out that these are real numbers. I haven't beaten these numbers hard enough to promise you that they're trustworthy. So, one problem with this, and maybe go back. One problem with this, and maybe go back, is everywhere this condition changes, so when you've satisfied this condition, you drop a term from the series, which means that this local kinetic energy is not continuous, right? And so the obvious thing to do is that you put in some sorts of step functions. So here is a step function basically, and we just turn on the t2 term at zero. So we say, okay, if it's negative, we don't include it. If it's negative, we don't include it. And then we say, and if it gets bigger than T0, then we don't include it. And we just smoothly turn it off. This is a Fermi smoothing. We tried six different smoothing functions, right? Qualitatively, all the same, different numbers, right? So I'll just show you the results for one of them here. And it actually was getting worse when I smoothed it. I don't know why. There's one parameter in the smoothing functions. All the smoothing functions we used had one parameter. We used had one parameter and we optimized it on the atoms. I'm showing you the data for the molecules, right? And you can include the extra terms, and now with the extra terms included, of course, it's different now, right? These terms, they integrate over all spaces zero, but now I'm not including them over all spaces. They're being included some places and damped some places and not included other places, right? And the results are, yeah, better, but still not great, right? Okay. Okay, so then you can try to say, well, why don't we resum it? The trick we use for resumming it was we rewrote our Taylor series in this form, and the reason that is we make sure we get the coordinate scaling right. If you don't play some sort of game like this, you might not get coordinate scaling right. So this guarantees the right coordinate scaling. And then we fit it to a Padé approximant. And there's at least one precedent in the literature for doing this sort of Padé approximant, right? It's sort of an obvious thing to do if you have a great answer. Sort of an obvious thing to do if you have a grade in expansion that isn't behaving nicely. And now you'll see this approach works better. Once you get to sixth order, there are two different choices for the PAD, and I'm showing you results for both of them. I mean, it doesn't really matter. This is a result that's been in the literature before, is this specific one? Adding in the extra terms, again, this may be there's a Laplacian here, getting the Laplacian at the nucleus right. Plausian at the nucleus right or near the nucleus right can be problematic. That could be that issue. Again, I haven't tried to stomp out every last little numerical error. But basically, okay, but nothing to write home about, right? Nothing that makes me super excited. I mean, these are errors in atomic units, right? So these are not, I mean, absolute errors that make me excited. In the same way as before, this term ends up. Same way as before, this term ends up having some weird stuff. Anywhere your denominator gets very close to zero, which can happen certainly, you get weird things happening. And so the logical thing is that we smooth it out. So again, you have this sort of form. And basically, this is saying anytime this term went negative or anytime this term got bigger, so it's almost like the super asymptotic approach, you're going to turn it off. We do the same thing with, I mean, slightly. We do the same thing with, I mean, slightly different forms, obviously, for the Pades associated with higher-order expansions, right? And we smooth them out. And smoothing does help us, right? And smoothing with the extra terms also helps us. But again, is it wonderful? No, right? I mean, progress maybe wonderful? No. Okay. So this is the thing that I. So, this is the thing that I was actually playing around with when I was in Chile, and Jose Luis was there. And this is a different way of dumming. And one thing about the Pades is they can deal with very simple branch cuts, not the form I was using, but you can deal with very simple branch cuts, and you deal with poles in your Taylor series, but you don't really deal with the full complexity of Taylor series. And so there are tricks for using more flexible functions than rational functions or quadratic functions. Than rational functions or quadratic functions. And the nice trick that we were playing with was hypergeometric resummation. So I think that was an approach that was exciting to me because I know, in fact, that the gradient expansion, the coefficients or the size of these can diverge at a factorial rate. And when something converges at a factorial rate, the first thing you should do is you should make the Borel series. So that just basically says take all your coefficients, divide by infinite. Says, take all your coefficients, divide by n factorial. And if you think about what that's doing, well, one over n factorial is the Taylor series for the exponential. So you're basically putting in an exponential. And then to reverse this, you do a Laplace transformer. You have to undo the exponential you put in, right? So, I mean, on this slide, I've done nothing. I've just said that you can start with a function and you can get back a function, right? But in practice, what you do, of course, is you take this series. What you do, of course, is you take this series, you truncate it at some order, you do the Borel summation, then you have to approximate the Borel summation with, say, a Padé approximant. That would be classic. And then you do the Laplace transform, and now you have an approximation of your function that's been resumed, but not just like Paday. You resumed the Paday of the renormalized series. But again, the PAD can't deal with a branch cut. So what we do instead. So, what we do instead is we fit that function to a hypergeometric function, and hypergeometric functions can deal with all sorts of things, including poles and branch cuts. And then, due to a miracle of mathematics that I deserve no credit for, the Laplace transform of this sort of hypergeometric function can be done analytically, and you get this Meyer G function. Okay, so this is the protocol, and when you do it, then here are the first two orders. It then here are the first two orders of myer G. Okay, we did this for the six-order term, too. The sixth-order term was much bigger than this one, unsurprisingly. And I think we have a bug, or maybe it just is really bad. But either way, I'm not showing you data for that. And so I'll just show you that these are the MyRG results. And they're, again, they're a little bit like our smooth. Again, they're a little bit like our smooth potties, which are the best results we had so far. But nothing really great. One thing that is consistent, and we just observed it everywhere, is with the MeierG, it really didn't matter at all whether you included the extra terms that normally integrate to zero or not. I have no idea why that's true, but to within a few hundredths of a heart tree, it never changed anything. Okay, so that's my saga of failure. I'll wrap it up at this stage. Is there any hope, right? That's the question we have. And I'd say mathematically, I mean, we have a lower semi-continuous functional that's non-differentiable. Yeah, mathematically, I think most mathematicians would tell us we should give up, right? At a practical level, We should be skeptical, at least at using it in this way. I know some other renormalization techniques that are suitable for even worse behaving series. Like this is a really bad one, right? Because in fact, it just gives infinities at some order of the Taylor series. And there are renormalization techniques that are suitable for that. And so I can try some of those. It's on the list. We'll see if and when I ever get to it. We'll see if and when I ever get to it. I think one thing is that some of the bad things we understand don't happen everywhere. So in condensed matter, you never get the density becoming vanishingly small. At least at that point, our functional should be continuous, right? Well, yeah, it should be non-differentiable, but it should be continuous. So all the discontinuities we know of of the function. Discontinuities we know of of the functional happen when you're on the boundary of the set of allowed densities. That means either densities that are very rapidly oscillating or densities that are infinitesimally far from zero. So maybe condensed matter is a good way that would sort of support a lot of what we're seeing here also. And I mean, there is the other type of orbital-free DFT, which is you just do DINC matrix-driven, concham, and do linear scaling, con-sham. And that's another practical way going. And that's another practical way going forward, right? And with that, that's my saga failure. I'll be happy to answer any questions.