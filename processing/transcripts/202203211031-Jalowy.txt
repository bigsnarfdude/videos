To present my work here, which is somewhere between random matrix theory and optimal transport. And if you want to put me somewhere in the map that Martin just advertised in the very beginning, I'm, as you said, with the empirical measures, and I will also show you some links to the PDE part, so to say. And let me apologize, my video is not working. I don't do that on purpose. I would have been happy to so that you see my face, but apparently my computer doesn't want to show that. Computer doesn't want to show that. Yeah, since this is the first time I'm talking to an optimal transport audience, I would like to motivate the research that I was doing differently. So let's start with putting IID points into a complex object. Now, in this case, we will study the unit disk in two dimensions, which is also the complex plane. And we put NIID points into this complex plane and would like. This complex plane and would like to study what is the distance between this empirical distribution to the reference measure, which in this case, of course, is the uniform distribution on this complex disk. Then the classical optimal transport problem without any additional constraint, as we have been seen maybe in the previous talk, is the intrimum overall couplings or transport plants, so the so-called Wasserstein distance. Called Wasserstein distance for some order p, and I will be mostly interested in p equal to one in this talk here, where the cost function is then just the distance between the points. So this optimal transport between then one discrete and one non-discrete measure might look like this. So for p equal to one, you see here different types of cells, different types of areas surrounding the points, which Surrounding the points, which is exactly the product of the point and the area surrounding the point is exactly the support of the optimal transport plan that appears here in the Passerstein distance. And there are several results from this area. Maybe the most important one is going back to Ashtai, Komlosh, and Tusnadi, who proved first that the Wasserstein distance is. First, that the Wasserstein distance is of order one over square root of n up to one logarithmic factor. So there's an upper and lower bound here. And then there was a breakthrough result very recently by Ambrose, Joska and Trevisan, who proved that for these IID points, the expected squared Wasserstein distance actually has a very precise limit, and they managed to calculate the constant in this limit. Then it was later generalized by Ambrosio Goldman-Hevisan, for example, also covering areas like this ball and not just. In areas like this ball and not just the unit box or the other compact money box. Now you see here that this logarithmic factor appears and it looks like that some of these shapes in the optimal transport plan here have a very let's say ugly shape, which comes back from, if we look at the point process here, comes from the fact that there in this The fact that there in this Poisson point process there is some kind of clustering, and also we have some areas where there are big gaps. And it would, of course, a natural question to ask what happens to other point processes. And is it possible to maybe remove this logarithmic factor? Well, of course, the one over square root of n is very natural for the typical distance between two points in a complex plane. And in order to, let's say, And in order to, let's say, construct a point process that has some dependency structure between the points, in particular, some repulsive structure, I will now tell you a bit about the model itself, which comes from non-Hemitian random matrix theory. So let's look at the random matrix that is just a collection of IID entries. So the entries are independent and centered and normalized. And let's say they're complex. Are complex, then random matrix T is mostly concerned about the study of the eigenvalues of these matrices. And if I scale everything down by one over square root of n, you see that the spectrum of, let's say, 100 times 100 matrix looks like this. So the eigenvalues really lie also very close to this unit disk. So here again, we look now at the empirical distribution, but now it's the empirical spectral distribution because the Distribution because the Dirac masses lie in the positions of the Riescale eigenvalues of this matrix, and these eigenvalues have some kind of repulsion term like I wanted the model to satisfy. And we will also see this repulsion term later appearing in the formulas. Now, for this eigenvalue process here, it is also known that this empirical spectrum. It is also known that this empirical spectral distribution converges weakly to the uniform distribution on the complex disk, meaning that again this converges to mu infinity, like we had before. So we can ask the same question like before, for example, about Wasserstein distance between those distributions. This circular law has a very long and actually very interesting history going back to Jean-Geniebre, who proved this in the 60s for random matrices where the entries have a very particular Where the entries have a very particular distribution, they are standard Gaussian distributions. And that's why these kinds of matrices for Gaussian distributions are called Ginibre matrices, and they will also play a particular role in this talk now. Then later, Gierko had an ingenious idea how to tackle more general ensembles, and this circular law under the optimal two-moment conditions is due to tau and blue. Now, let's have a look at how the optimal transport plan looks for. Transport plan looks for also for p equal to one for this eigenvalue point process. And you see, in particular compared to the previous picture, that the cells here are looking much more regular. It's a quick picture. So this was for IID points where you had like weirdly shaped star domains. Then these cells here on the first side at least look much more regular, and we could hope for removing the logarithmic factor maybe. Factor map. Results for in this direction are rather sparse. So I will just recall a bit what is known so far for the Ginibre matrices, meaning for matrices with Gaussian entries, there was one result due to Meckles and Meckes who proved that the Wasserschland 2 distance is of a non-optimal order n to the quarter. And then there was a result by Shafay, Ali, and Maida proving a Coulomb gas concentration. Concentration inequality, which also entails that the Wasserstein one distance is of order one over square root of n. So this is nearly optimal, up to still this logarithmic factor appearing here. And I need to remind you that Coulomb gas for a very particular inverse temperature beta equal to two is exactly the eigenvalues of this Geneva matrix here. Then there are even more results maybe known for Komogovo this. Maybe known for Khomoga of distances, even for products of Geneva and more. But the most important part here is that all of these results still include the logarithmic factor. And the result that I would like to advertise is that for Geneva matrices, it is possible to remove this logarithmic factor by a very explicit constant, which is like close to four here. So the expected optimal transport cost between the empirical spectral distribution and the uniform distribution is of order one. And the uniform distribution is of order one over square root of n. Yeah, then at the same time, I heard from a PhD student, Maxine Prodom, from Max Fati, who proved a very similar result for the Wasserstein 2 distance up to an unknown constant. And if time allows, I will tell you a bit about the differences and the similarities of both grooves here. On the other hand, if you look at random matrices that do not have a Gaussian distribution, but let's say a different Have a Gaussian distribution, but let's say a different distribution. So let's fix a different setting, a random matrix with independent entries which are centered, normalized, and have finite moments. Then there is one unknown polynomial rate known from Tau and Wu. Then Sean Rogue and Noah Williams have been generalizing the result by Meckers and Meckers to non-Gaussian entries. Non-Gaussian entries, and in my PhD thesis with my supervisor, we consider Kolmogorov distances. And the result in this direction is also very similar to the one before. So that for non-Gaussian distributions with this assumption, we have that the Wasserstein distance is also of order one over square root of n up to a factor n to that silon. And this is how a leading phenomenon occurring in random matrix theory that Random matrix theory: that you first prove something for Gaussian random matrices, and then later you realize that actually this holds in a much more universal setting. So, that the entry distribution of the matrix actually does not play a role, which is called universality. At least this is close to being nearly universal in this sense. Let me tell you a bit more. Let's go a bit into details. So, you all are very much familiar. So, you all are very much familiar with, and we also saw that in the previous talk about the wasteland duality, where you look at the supreme overall test functions, which satisfy some consistency inequality. And in particular, you can, let's say, fix one function f and the best function g that you can take is by just putting the, let's say, the f here to the other side. And then it is the definition of the C, the constant. C a conjugate of the function f. In particular, for p equal to one, this then gives back the Kantovich-Obenstein duality, which is an integral metric by looking at the suprema overall Lipschitz functions and testing against the difference of these measures. And maybe that why am I telling you this? This gives you a very simple lower bound of order one. simple lower bound of order one over square root of n showing that this one over square root of n behavior is optimal by just putting one particular function f into this duality formula for example a bump function that is just bumps around all the eig waves on the other hand if we change the viewpoint again and look at some fixed function g and the dual function f, then this fixed function g just has finitely many, takes finitely many values. So it's actually just a Takes finitely many values. So it's actually just a vector, which then entails to a semi-discrete optimal transport problem. In particular, this integral can be split into the cells. These are the colorful areas down here. So a cell is defined as all the points in the complex ball which are closer to one of the particles lambda j up to some dual weight than they are to all the other. They are to all the other particles. Now, this is exactly the J where this infimum is attained. So, if we put the function f here, like the dual, and the function g in here, you see that this is actually the same as above. In particular, by looking at the maximum, you will also see that by differentiating with respect to g, that all these cells, of course, have equal mass. So, it's also a fair allocation of the mass to the Fair allocation of the mass to the points. You can also call it Apollonius diagram if p is equal to one. So there are several names for that. Also, you see from this definition here that the particles lambda j lie into the in their own star domains. And you could rephrase the result that I just told you about in the following way. So this is what I told you about in the previous slide. What I told you about in the previous slide. You can rephrase it in the following way by looking at this, by pulling the sum out of the expectation, you see that this shows something like the typical cell, so the CJ here, the shape of the typical cell, in particular, the spread of the typical cell is lower than the spread of the typical cell for IID points, which is exactly what the picture showed you. Showed you, yeah, so the spread of these typical cells here are somehow more regular in some sense, which is, let's say, one viewpoint from random geometry on the same problem. This duality formula also gives us already an idea of how to prove this result. So, I told you that the Wasserstein one distance is by Kanto-Rogenstein, something like the supremo over Lippen. Something like the supremo over Lipschitz functions tested against the difference of the measure. And one particular very important tool from non-human random matrix theory is the so-called logarithmic potential, which basically is nothing but the measure convoluted with the logarithm. So it satisfies the distributional Poisson equation. And if we just naively plug in this distributional Poisson equation into this difference here, we can make the following formal. Formal calculation. So we put this distribution Poisson equation in. We throw one of the derivatives to the test function f, which is integration by parts. Now you see that the derivative of f, of course, is bounded because it's a Lipschitz function. So by Cauchy-Schwarz and another integration by parts, you arrive at some interaction energy term. And our aim would now be to show that this is of order one over square root of n. Maybe some remarks on this. Remarks on this approach that also appeared in several other works. For example, if you want to bound the Kolmogorov distance, which is not, of course, an optimal transport problem, then you would maybe want to use integration by parts once more and try to bound the Laplacian of some function that looks very close to the indicator function. Also, the Coulomb transport inequality by Shafayardi and Maida is in some sense. In some sense, using similar steps, like here. And maybe most importantly, here, Ambrosio, Stra and Trevisan in their work with using the PDE techniques for the optimal matching, they used some, let's say, unknown solution U to exactly the same distribution Poisson equation with Neumann boundary condition on some box and used the fact that the Wasserstein 2 distance squared is bound. 2 distance squared is bounded also by something like a Dirichlet form of the function u here, very close to this guy here, which on the other hand is also can be rewritten as a dual Sobolet norm. Now, in some sense, you might want to see this logarithmic potential U here as, let's say, the solution to the same problem here on the box, which is infinitely big, because the derivative, of course, of the logarithm for large values is also close to zero. Values is also close to zero. And you might want to hope for using similar ideas for the logarithmic potential, but the problem is that this constant C here blows up with the lower bound of the measure mu. And of course, you will not be able to find a lower bound of some measure on which or some probability measure which lives on the whole complex plane. So something like this will not work in this sense. On the other hand, this logarithmic potential, as I told you, is a very important. Potential, as I told you, is a very important tool in random matrix theory. I will tell you in a second why. And maybe the other nice idea is that it's a very explicit function where you can make very explicit calculations. Another idea that Maxime Pradom used to prove the Wasserstein two rate for the genie eigenvalues is to cut everything down into mesoscopically big boxes, and then each box uses. Each box uses the same PDE approach from Mancrozius Kai and Trivesan, and then by sub-additivity arguments, summing up all the errors on each of these boxes, which is also why the constant there is not explicit. So, let me now tell you a bit about what I mean by this formula calculation here. So, this is just the last. This is just the last page summarized. Gerker semitization is the idea of how to use this logarithmic potential. So the logarithmic potential looks like the sum of the logarithms, which by the way is also the same as what we just heard in the question to Matthias from the audience a second ago. It's also the trace of the logarithms of the corresponding shifted matrix. But in this sense, Gerkosymmetrization trick just gives you somehow a bridge between. Just gives you somehow a bridge between non-Hermitian and Hermitian random matrix theory that I don't have time to explain now, but it is the main tool to study such things. And the logarithmic potential looks like this. So it is a function that has logarithmic singularities at all the positions of these random eigenvalues, which of course makes analysis very hard and also makes this right-hand side up here actually infinity. Here, actually, infinity. So, u n is a sum over these logarithms which we evaluate at the positions of the singularity, so that's actually infinity. So, the best idea we can do is to regularize everything, for example, by modifying or making this cutoff here and hoping that as n tends to infinity, this logarithmic potential is very close to its limiting object, the logarithmic potential of the circular law. Which, so, what we could hope for is that for the regularized measures, That for the regularized measures, such an inequality holds, which then makes the formal calculation from the previous slide actually rigorous. So for matrices which have independent entries, so that let's say more general statement from the beginning, we know that logarithmic potentials converge at an order of one over n, which is a very strong result due to Alt, Erdős, and Klugel, and by some technical random grid approximation. Some technical random grid approximation, we can make that uniform. If you now put in this uniform bound into the idea up here, which is now just repeated down here, you see that the Wasserstein one squared distances of order one over n after this regularization, which you can basically choose for free, and you end up with the one over square root of n behavior up to the epsilon factor. Now, of course, it would be interesting to remove this epsilon factor, which Epsilon factor, which is possible for Gaussian random matrices. But for Gaussian random matrices, many tools are very different. That is because the Ginebra eigenvalues, so eigenvalues from Geniebre matrices, form a determinantal point process. In particular, their joint probability density is known and all the correlation functions are known as well. For example, the density of the averaged measure is given. Averaged measure is given by this exponential here times the cutoff exponential series, and the two-point correlation function looks like the product of the one-point correlation function minus some repulsion term. And so here you see that the two-point correlation function basically behaves like something like independent particles, product of densities as independent particles, minus something like a repulsion term, which Something like a repulsion term, which is why this model is a good way of modeling what I said in what I motivated in the first slides, a model where the particles are repulsive. Now, for these Geneva matrices, you can very easily show that the mean empirical spectral distribution mu and bar is very close to the uniform distribution. To the uniform distribution by an order of one over two n. So it's even faster than the non-averaged distribution, which of course is basically a proof of analysis because you have a very explicit formula here. And using duality and Laplace method, it's easy to find that here. And of course, keep in mind that for IAE points, of course, mu n bar is just equal to mu infinity. So there's nothing to show. Mu infinity, so there's nothing to show like this for independent particles. Now, for the proof of the theorem without where we removed the logarithmic factor, our idea would now be to look at the regularized empirical spectral distribution and compare that not to the limiting distribution, but to mu and bar, which helps in some sense to kill the mixed terms that appeared in. In terms that appeared in the calculations from the previous slide, so in some sense, we can compare that to the logarithmic potential integrated with respect to its own regularized measure minus the average parts. And our aim would be to show that this is of order one over n up to some constant, which then leads to the proof of the theory. So, here you see that the logarithmic potentials are just the sum of these logarithms, and the empirical measures are just the sum of the Dirac measures. So, you pull out both sums, and sometimes both sums have the same argument, which is this diagonal part here. So, that's not the logarithm to some power, it is the logarithm that is regularized at the scale to R. This superscript is the regularization parameter. And if you have different And if you have different particles getting each other in the sum, you end up with expectation of the logarithm of the differences here. And now, since we do have the two-point correlation function, you can plug in the formula from up here and realize that these independent particles from the first part of this two-point correlation function exactly cancel this term in the end here. So, what remains is the Remains is the repulsion term in the logarithmic integral here, which then, in the end, after some calculations that I will spare you the details with, ends up also canceling the logarithmic part from the diagonal part of the regularized logarithmic potentials here. So, in some sense, and that's maybe an important part, you see that this logarithmic term coming from the This logarithmic term coming from the diagonal terms is cancelled only because we have repulsion of our particles. So, even on a, let's say, very mathematical and very technical level, you see that the repulsion of our particles is basically the reason why we can remove the logarithmic factor in the results that appeared by Ashtai Komlos and Tusnadi and by Ambrosius Katrivisan, where the logarithmic factor. Where the logarithmic factor cannot be removed because there is no repulsion. Yeah, that is all that I would like to want to tell you about. So I thank you very much for your attention. And here's some summary slides. And if you have any more questions, I would be very happy to make more connections in the map that Martin said in the very beginning. Thanks a lot for this nice talk. Any questions? Hi, I might start it today, but I think there was a recent paper of COVID-19 which put the AKS using Fourier analysis exactly in the two-dimensional case. Also work here? If I remember correctly, they also are like a smooth movement argument. I have just briefly reading the paper, but I think all of these arguments use some smoothing techniques, right? So that regularization that I showed you here is very specific regularization, which is just like, let's say, smoothing by the uniform distribution on the sphere in the paper by Ambrosio St. Here in the paper by Ambrosio Stratrivizan, it is a smoothing by the heat kernel in the paper by Leidou for Gaussian samples that I did not advertise here, and maybe I should have. It is also regularization with a different kernel. And yeah, so it's necessary everywhere, yes. But in the Fourier analytic method, that does it uses something which is inherently It uses something which is inherently independent. So, Fourier analysis is helpful, maybe for, let's say, sums of IIDs, while the, let's say, Steels transform, for example, is very, is the analogon in Hermitian random matrix theory, and the logarithmic potential is the analogon for non-Hermitian random matrix theory in the following sense. So, if you want to prove, let's say, a CLT or a law of large numbers. Say a CLT or law of large numbers for IID points, you use the Fourier transform in the sense that something converges weakly if the Fourier transform converges pointwise. The same holds for logarithmic potentials. If this converges pointwise, then you have weak convergence. And that is how you usually work with non-Hermitian random matrix theory, with non-Hermitian random. Theory with non-emission random matrices. If you want to prove something about the measures in non-emission random matrix theory, you use this logarithmic potential. So, this is maybe the most natural candidate to replace the Fourier transform to eye values of random matrices. In my opinion. Okay, so there's another question by Simone. There's another question by Simone. Yes, thank you very much. It was really very interesting. So my question was on: so what is your feeling? One, so two questions. One is what is your feeling about a general random matrix? You expect still to have one over square root of n, or you expect maybe something else, depending maybe on the distribution. on the distribution and the other the other question is about so if you expect the epsilon to be zero actually and the other question instead is about so a different generalization of Genum in the in the direction of you know more Coulomb gases so instead of looking at the eigenvalues of matrices Looking at the eigenvalues of matrices, now let's look at interacting particles in R2 or Rn, let's say. And then you look at the probabilities at the given temperature of what is their distribution. You expect, in general, in that case, that some techniques like yours can lead. Can lead, okay, in particular in R2, to better convergence in Bassestine to the uniform case, or in that case, to the homogeneous case. Yeah, first of all, thank you very much. Wonderful questions. First, regarding this epsilon here, I very much believe this epsilon is zero. I cannot prove that, and I don't think that. And I don't think that my methods will be able to remove that in some sense. You would have need to come up with something better. What I'm quite sure of what you can do is removing the enter epsilon here or reducing it to some logarithmic factor. But below logarithmic is basically impossible because at some point I use a convergence rate for the logarithmic potentials, which will never be of order one over n. There will always at least be a logarithmic factor due to this Gaussian-free field. Due to this Gaussian-free field analog on. So you need to do something different then. Maybe you can, I mean, you can remove, you go down due to some logarithmic factors, but maybe not more. And I very much believe that the epsilon should not be there in the limit. Regarding Coulomb gases, maybe here. So for Coulomb gases which have a different beta, I expect something like this behavior. So it should be 1 over square root of. So, it should be one over square root of n, and this should be something that behaves like a logarithm of beta. So, logarithm of beta in this sense that if we send the temperature to infinity, so we recover the Poisson point process case, which then leads to Ambrosios Kartribisan, we should somehow retrieve the logarithmic factor again here. And this is how I hope this will work. And I have some, let's say, ideas how you should be. Ideas: how you should be able to prove it. It is though quite a different idea from here because here I very much relied on the fact that we have correlation functions, while for Coulomb gases in general, there is no correlation function. So you would need to estimate this interaction energy in a different way, which is future work. Yes. Okay. Thanks a lot. So there's another question by Daniel. By Daniel. Hi, and thanks for a fascinating talk. I'll keep this quick. I don't know if we have a lot of time left. But let me, okay, let me start with a comment to set up the vague question. And the comment is that I was a bit puzzled naively by the intuition, which would suggest that more repulsion means points are farther apart, not closer together. But I guess this is countered by this very strong confinement potential scaling in the degree ensemble. But I wonder then if some. I wonder then if somehow this phenomenon that things are closer together is more to do with some notion of rigidity in the corresponding point process. Is this at all feasible or do you have a general picture of when and why this kind of thing might occur? Wonderful question. Yes. So the confinement, of course, just pushes everything back, as you said, and this definitely helps. Rigidity is hard to I mean rigidity is a a fake phenomenon. Fake phenomenon which appears in one dimension for angles of emission matrices in a very precise way, but it's hard to phrase it for two dimensions maybe. But the maybe better concept would be hyper-uniformity. So a point process is hyper-uniform if the scaling of the variance is somehow lower than that of the Poisson point process. And this might help to prove. And this might help to prove something like this for general hyper-uniform processes. And then it might be hard to use techniques like logarithmic potential, but maybe it does. The uniform processes should be the way to go. On the other hand, maybe to link the last two questions, there are not many. There are not many examples for hyper-uniform process processes. So there's this Geneva eigenvalues that I showed you, but Coulomb gases, no one knows if they are hyper-uniform. So you would have an you would need to come up with a cool example. All right, okay. So thanks a lot for this very nice talk. So Simono put another question in the chat, but we might also postpone it to further discussions after the next talk. Further discussions after the next talk or other coffee breaks. And it's about time.