That must be one of the most fantastic bases on the Eurost. I think enjoy it very much. So the title doesn't look very much related to machine learning, but I will say just a little bit of motivation because we have a long-term perspective that this is a reason. One of the main reasons why we do this is that we want to apply this in a setting on neural networks. So let me just, yeah. So here's the outline. I give this multiple. The outline. I give this motivation that I just mentioned. And then I start to talk about stability in a certain sense. It's a certain form of non-linear stability, which is called instability. And this is very well known in Euclidean spaces and has been since the 1970s, maybe. And then what is new is that we will put this notion of non-linear stability, V-stability. Non-linear stability, V-stability of Riemannian manifolds. And then we will talk about numerical integrators of manifolds and give first few results on the stability properties set in the Riemannian setting. And then I will show you some, for me, at least quite surprising results about this, things that don't follow what we know from Euclidean space. What we know from Euclidean spaces. And then maybe I will finish with I will talk a little bit about S2, which belongs to a model space in a sense for a certain class of Riemannian manifolds, and then the symmetric positive definite matrices, which is an example of negatively curved manifold. So let's move on to it. Just a little bit of motivation first. A little bit of motivation first. So neural network, very briefly, can be thought of just as a sequence of maps, right? So that you have an input space x0 and then you apply some layers or some maps between layers, and depending on the previous layer and some set of parameters that Parameters that you have for each of the layers through the network. But in our case, we would like to think of these spaces that we map between to be Riemannian manifolds. This is something which is useful, for instance, in computational mechanics, where you have typically you can have data that belongs to some animal, to a leg group, for instance. If you look at For instance, if you look at elasticity, rods, and so on, this is often the case. So, many results for the, if you want to control the, so as you say, that you want to control the evolution through these feature spaces. And there's a lot of work on that going on now. And I think maybe, I don't know if Tao is going to talk about it, but there is some recent result about it. What you really want to do is to. What you really want to do is to control the growth through the layers. That's important. Here, the D is some metric on your manifold, and the one without tilde, then you can think of it as two nearby data points that you start with, and you would like to keep the distance sort of controlled so that data which are almost the same should not deviate too much through the network. Some ways that this Some ways that this, or one way this is done, is first to design classes of vector fields. So, now I am in the neural ODE setting, I should have said that maybe, but in the neural ODE setting, what you can do is to design classes of vector fields whose flows can be controlled to be non-expansive or neutral or expansive. This is something which is already done. And then you have to apply numerical methods which inherit the same properties so that. With the same properties, so that if it's non-expensive, then you would like your numerical method also to be non-expensive. So, this is one motivation for this talk. The other one is the case where not the feature spaces is necessarily a manifold, but the space to which the parameters of the system belong. So if you look at the standard A standard gradient descent on a manifold. So, this just if you just do this in a manifold setting, then what you might want to do is to use to follow a geodesic along the negative gradient of the cost function evaluated at your point. That would be kind of the generalization of the standard gradient descent to remain manifold. You can also think of a Manifold. You can also think of a type proximal gradient type algorithm where you start at the arrival point, in a sense, the way you write it, and then you follow an omnigodesic to arrive at the previous point. That is like the implicit Euler method, also known as that. So let's go on. Let's do a little recap of what is feastability in case you have. In case you haven't seen it or in case you have forgotten, because this is not something that is so much talked about maybe these days. So, in a classical setting, we have an ODE on some Hilbert space that could be Rm, and we have an inner product and an OR. And then we consider a differential equation, a vector y dot is equal to x of y with some initial value. And then we have to be a little careful because Because if you want to make this rigorous, what you want to say here, it's important, first of all, that you want to restrict to some subset of Rm and it should be convex and connected. And then it is important for the definitions to make sense that the vector field is forward complete, is what we call this, that it exists for all time, and then that it's invariant, that the set U is. invariant that the setu is invariant under the flow so so for all t it's it's like that if you want to consider unconditional stability you have to you have to to impose these two points and then comes really what we used to see is that you have a so-called monotonicity condition on your vector field so you assume that there is some constant nu such that the inner product between x y and x of z for any y and x of z for any arbitrary chosen points y and z with y minus z is bounded by some constant times the the norm of the different squares. And this should be the case for all y and z in u. If this happens it's a very easy it's very easy to show that the norm of the difference of the solution at time t for these two initial values y0, z0 is bounded by the exponential of nu. Bounded by the exponential of nu times t and the norm of the initial difference. So here you have a continuous stability estimate that you get as a direct consequence of this monotonicity conditions and the proof is very easy. And notice also that this new here, as opposed to the standard Lipschitz condition, which is always positive, then or non-negative at least, then the new can be negative. At least, then u can be negative. So, this means that you can have actually here also a decay in some cases. So, you can capture the case where the solutions are contracting. So, let's just go on to the next here. So, now comes what to think about the numerical methods in this sense. So, suppose that we have such a system which satisfies these conditions. These conditions, and if we have a one-step method, a numerical one-step method, then we have to do a similar set of assumptions that the method is defined for all positive step sizes, that it is in the variant set U with all step sizes. And then, if we then have that, when we take one step, the norm of the difference is bounded by the norm of the difference. By the norm of the difference of the initial values for any pair of points in U. Then we kind of inherited the monotonicity of the continuous system and then we call the method B stable. If this always happened when the system is non-expansive. So if it holds with nu is zero, then this condition holds when the monotonicity condition holds with. monotonicity condition holds with nu equal to zero, then we call the method d stevel. So this is nice because you can use this for many things. You can use it to derive global error estimates for your numerical solver. And you can use it sometimes to ensure that implicit methods have unique solutions and so on. And in general to control the growth of your solution. But some known facts that I would like to remind you for later also is that if you look at the implicit Euler or the backward Euler method and you compare also with the implicit midpoint methods, both of these methods have the property of being v-stable. So these are methods which are V-stable. And if you are used to stability, where you use the dog with the equation that we heard before, it is also easy to show that To show that B stability implies A stability. So this is not so strange that this happens. And for instance, an example of a method which is A stable, not B stable, is the trapezoidal rule. That's one such example. Alright, another thing I would like to mention also is that for both of the methods, the implicit equation you need to solve in each time step has a unique solution when the When the problem is non-expensive. Now, let's move to the Riemannian manifolds, which is really the subject of this talk. Now we start with, so we have a manifold M, which is a connected Riemannian manifold with a certain metric that I sometimes draw as G and sometimes just with angle brackets. And you know that whenever you have a metric, so On a remaining metric, there is also a connection, a unique connection, which corresponds to this metric, which is called the Levy-Chivita connection. So we will always refer to the Levy-Chivita connection, but when we use Nabla in the rest of the talk, it will always be the Levy-Civita connection. So then now you can define the length of a curve just by taking the tangent to the curve. the tangent to the curve and and takes take its norm with this with this so the norm is is given by the by the by this metric and you integrate it between two points. So this gives you the length of a curve gamma between A and B. And then you get from there also a distance measure between points on M by taking the infimum of all curves in the homology class connecting X with Y. So all curves that connect X. With y. So all curves that connect x with y, you take the infinum over the length of all of those, and then you get the distance between x and y. What is our intention now? This may be obvious, but rather than using the norms as we did the inner product norm that we used in the Euclidean case, we would like now to bound the difference between two solutions and also two numerical solutions in terms of the. Empirical solutions in terms of the difference between x0 and y0. And the only difference is that we're using the Riemannian metric, the Riemannian distance measure, and not the that we don't have anyway on Riemannian manifold. So again, we need to repeat some of these conditions that we made. So we would like to consider a subset of our manifold. In fact, we have to do that. I'll maybe get back to To do that, I'll maybe get back to that in a moment. But we need to be able to restrict to a subset, and then we need this subset to be geodesically convex. And that means that if you take two points inside this subset, it must be connected by a unique minimizing geodesic, which is completely contained in the set U. So that is geodesic convexity. And then we have the same thing of forward complete. This is just saying that when we This is just saying that when we solve our differential equation for positive times, it should be defined for all positive times, in fact, for starting values in the set. And then the forward invariant thing we also had before is that when we start inside the set U, then we remain inside the set U for all times with the solution. So then now we are ready to state what is going to replace the monotonicity. Going to replace the monotonicity condition that we had before. And this is again, I just call it again the monotonicity condition. We suppose that we have a constant now such that for every vector field defined on this set U, then we need that so x is given. So for any y, the Levitica connection, double y x in the product with y. Wyx in the product with y is bounded by nu times the norm of y squared. And that has to happen for every point in this subset u. So this is actually the equivalent of the monotonicity condition we had before that looks now a little bit different but it is well defined as it says here. So now we will say that, make it short, that we will say that a system is We will say that a system is not expensive if for all of these points 1 to 4 holds, all of these bullet points hold, and we need the new in the last point to be a non-positive number. We call it contractive, it is strictly less than zero. That sometimes makes some more of a difference. Difference. And now we have a theorem which is again an analog of what we had in the Euclidean case. Is that if it is non-expansive, so we have such the conditions 1 to 4, then the difference between two solutions is bounded by again exactly the same exponential of mu t times the distance at time zero. So this is something that is not so hard to prove. Something that is not so hard to prove, and it's actually done here. But I think that maybe, in the interest of time, I will skip that now. You need to use the properties of the Lebitibida connection essentially, and then you can prove this result. So, I should mention a couple of things. One thing is that if you have a contraction system, so this is if Î½ is strictly less than zero, then Less than zero, then you will always have there will always be a unique equilibrium point in the set U. This is not so difficult to prove because it's just a matter of with the conditions we have of defining a contraction mapping that you can use a fixed-point theorem to show this unique equilibrium point. This is maybe a little bit harder to prove than the next one here is a little To prove the next one, here is a little bit more difficult to prove. It's a topological result, really. And if you have a compact manifold, you cannot have a contraction system with a negative nu. So this also may be you cannot have a contraction system on all of m. So that kind of underlines what. kind of underlines what why it is so important that we restrict the to a subset of the manifold. So that we would like to deal with with the compact manifolds. Let me say just now a little bit about this is well known, everything I'm going to say here, but just to remind you, there are many numerical There are many numerical integer just designed for manifolds that are sort of intrinsic, and they don't depend on how you have embedded the manifold in some ambient space, and it does not depend on any choice of local coordinates or anything like that. And just to mention a few of them, there's something called Crouch-Grossman methods and commutative-free methods, and they are designed by defining some finite. Some finite dimensional space of vector fields on the manifold, and then to design the methods by doing compositions of flows of these sort of elementary vector fields that you have. So, this is one type. Another way that was invented by Hans Mythikos, I think, is to impose a group action, a Lie group action on the manifold. If you have homogeneous manifolds, and then you can also find, there are also ways to. And you can also find, there are also ways to derive such methods. Tangent space parametrization methods, retraction methods, and so on. And yeah, and there's been a lot of work on this. So all the classical integrators have a kind of a counterpart on manifolds. So also PDF methods, for instance, and for linear problems, you have also stuff, variational legal. You have also stuff variational lead group integrators, you have almost what you like. What we are going to focus on here, which is natural, as a first step when you work in Riemannian manifolds, is to consider numerical methods that are just based on geodesics, because geodesics is a very natural object on a Riemannian manifold. So, then our model methods here that we will discuss is the geodesic implicit Leo method, I will present it on the next slide or so, and then geodesic. Slide also, and then justig implicit midpoint method. Let's see if they are defined here. Yeah, they are. So now, when I use the XP, I did this on the motivation slide also. This is the geodesic exponential. So when we consider the curve gamma of T and we write it X P of T V, this for a point P of a manifold and a V in the tangent space over P. And a V in the tangent space over P. This is geodesic emanating from the point P in the direction of B. So then we can write the implicit Euler method. And this method is defined like this. You see that now the initial point of the step is called just y and the arrival point where we go to is called y one. And I define it implicitly by saying that y should be the what we get if we start at the arrival point and go in the The arrival point and go in the negative direction of the vector fields away is minus h of the vector field evaluated at the arrival point. So this would be if you look at the Euclidean case, this would be just the standard backward Euleri method or implicit Euler method. The geodesic implicit midpoint method is similarly defined. Then you need a midpoint that we can call y bar. And so if you start at y bar and you move So if you start at y-bar and you move in the negative direction, half a step size, then you get to the initial point. Whereas if you go the other way with the opposite sign, distance h half, you will write at the arrival point. So this is the implicit midpoint rule. So now with this, we can define, we can get back to the story about B stability, and we can define what we mean by a B stability. We can define what we mean by a B-stable numerical method. So then we start with some non-expansive system on M, and then if we have a method, a one-step method, phi, the step site H, applied to vector field X, be a numerical method such that we need this that is forward complete on U so that it's defined for all H positive, and then forward invariant so that it remains in U for every In U for every step size. If we then have that, the remaining distance between the arrival point is bounded by the remaining distance between the initial points for any pair of points in U. Then we call the method B stable. This is a very natural extension of the B stability in Euclidean spaces. So of course if we choose the metric to be the Course, if we choose the metric to be the Euclidean metric, then we get exactly the same. So let's go on to. So this is one of the maybe most important results we have so far about the stability on Riemannian manifolds. If we take the geodesic implicit Euler method and apply it on the Riemannian manifold, we find that it is. We find that it is B stable if M has non-positive sectional curvature. So, this is also known as a Habermar manifold, this type of manifolds. So, I think the importance of this theorem is not really so much what it says, but it's what it doesn't say. Because we are now restricted to a certain class of manifolds which are negative declared. And And I have to say that I have been doing numerical analysis for many, many years, and I never ever heard about any situation where the implicit Euler method doesn't fulfill a stability definition of some kind. And the stability condition you can think of almost is too stable. It's too stable. But apparently, here we have a problem, and I will get back to that in a moment. Why? Because here I just Moment. Why? Because here I just assert that if you have negative equivalent spaces, then everything is fine. But what about positive equivalent spaces? That is going to be the get back to that soon. So some remarks. Why this can be a problem is that if you have a simply connected compact manifold, then it cannot have a so there is no simply connected compact manifold. Simply compact manifold that has a metric of a non-positive sectional curvature. So, like the sphere and so on, the compact and connected ones, they are usually positively curved, or they are positively curved. So, this theorem doesn't apply to that case. In fact, if it's non-positive, it's also diffeomorphic to Rm. And one important example, if you are not used to this, then the the space of symmetric positive definite Space of symmetric positive definite m by m matrices is a negatively or a negative recurred space. So let's move on because let's consider some, now I'm mostly going to do numerical experiments for you in the rest of the talk. And let's look at S two, which is the sphere. So we have some questions. Yeah, I'm sorry. Sorry, bef before you you go uh can you specify the metric for the last example? Specify the metric for the last example. So you said the positive definite column is hard marked. Yeah. Under what metric? So the metric is I think is something like this. If you evaluate, if you if this is a positive definite matrix, so this is the root point, and you take two tangent vectors which are symmetric matrices then, I think it is something like the trace of Like the trace of is something like A inverse X, A inverse Y or something like that, I think. I think this is the... But I think that I have this on a formula later on, so if we'll get a confirmation later in the talk, I think there's something like that. Okay, so let's look at S2, the two square. Since we tried a little bit to prove to see if we could extend the proof to S2, but we didn't manage to do that. And then we thought we have to see if we can prove that it's not true then, numerically. And what do we do? Well, we take the vector field, which is just barely non-expansive in the sense that it is not contractive but non-expansive. And that you can do by choosing a so-called Keeling vector field. Using a so-called Keeling vector field. An example of a Keeling vector field is just to take the unit vector in the z direction, cross-product with y, if you represent points on the sphere as three vectors with norm one, with length one. Okay? So then you have this, and then this is a contraction is a non-expensive system, I suppose, on the open northern. So this is a contraction strong, non-expensive on the northern hemisphere. And then if you look at, so this is one. And if you look at so, this is one result that we found a very easy result because now we can use a Rodriguez formula to write down how the implicit Euler method looks. Because the exponential, the geodesics are known on this one sphere. So you get something like this. And if you look at the third component, you see that it's decoupled because it's zero here. And so it's completely decoupled from the rest. And then the last component is this. Last component is this, and then if you make h, the step size bigger, for small step sizes, this is a unique, you can solve a unique solution for y3 prime. But if you make h bigger, then something bad happens. You get a non-unique solution. And it is interesting because in the Euclidean space, you can find that the implicit Euler method has always a unique solution for non-expensive solutions. Unique solution for more expensive systems. And here you see a bifurcation diagram. So here the step sizes increasing to the right. So you see when you are here, if you go vertically, you cross only once. It's a unique solution. But when you come here, you cross one time, two times, three times, three solutions. And it goes on and on. You get more and more. Here you have one, two, three, four, five, six, seven, eight, nine. So this is getting worse and worse. So you lose the non-uniqueness of. So, you lose the non-uniqueness already of the solution to the implicit equations. Let's look at the B stability. We still have maybe a small hope that it could be B stable. We haven't proved this, that it isn't B stable on a positive recursive basis. But here you see a counterexample from a numerical example. Here we take the Keeling vector field, and now it's important to remember and take the step size here. You have to remember that this curve is not. This curve is not the numerical that we trace out the numerical solution for many steps. What we do is that we take always one step and we plot where we end up with one step with increased step size. So we increase h and then we get this. And then if you measure the distance between this, that is done in this plot, you see that for small step sizes, even the distance between these two two curves is increasing up to a certain point and then it's Up to a certain point, and then it starts to go down. So, this proves that we don't have the stability for S2. Okay, let's see. Sorry? You've got plenty of time. Plenty of time. Okay, thank you. And then if you look at the midpoint version, this is a little bit nicer because at least it is so the exact solution is going around on the Is going around on a constant latitude around like this. So, this is the method. I didn't say anything about this, but this is a variant of the midpoint method, which was developed by McLaughlin and Modina Vedia, I think, some years ago, which is called the spherical midpoint method, and it's actually symplectic on the sphere. And this is the one we presented before. And this is the one we presented before. But here, even if they follow the right trajectory, sort of, if you look at the distance again, that should be constant. You see for the midpoint method, it increases in the beginning, it's even worse, and even worse for the spherical midpoint for increasing step sizes. So it's also not be stable as opposed to the case in Euclidean space. To the case in Euclidean space. Oops, let's uh yeah. So uh let me now uh uh finish uh more or less with some uh with some with the case with negative recurrent manifold. So then you can have a manifold of symmetric positive definite. And here is what you you asked uh before that that uh you have uh the metric is I think. metric is I think it was what I wrote more or less yeah so then and then so this is the metric you are using there and then used Riemannian distance is you can compute it's not hard it's simple linear algebra to get to this lambda i is the eigenvalue of this matrix and um and you take the log square and the sum and you and take the square root and the remaining exponential has also an explicit um uh expression. Expression in terms of square roots of the matrix A, which is positive definite, so it's well defined. So, this is how it looks there. And then we try to run examples on a there's a problem called a Cauchy mean, which is you are given as data k matrices y1 to yk symmetrical to definite, and what you look for is some. And what you look for is some matrix X star, which is a Cartier mean, which is such that the sum of the distances between this point and each of the data points is minimized. So it's sort of a mean between all these data points, y1 to yk. And this you have to do numerically. There is no uh um generally no uh uh analytic uh or known solution for this uh uh exactly known solution for this. Exactly, no solution for this. So, what we do when we solve this is that we calculate, we think of this as a cost function, and we compute a negative gradient of this cost function, and we use the gradient descent. So, that is a way to try to solve this. And we are not so interested in the calcium in itself, but more about how the different methods behave. And that, I think, is shown here. So, you see the red curve is the exact solution. The red curve is the exact solution that it's going down. We start with two positive definite matrices and look at the difference and it goes down like this. And you see also the implicit Euler goes down as you would expect. The implicit midpoint method is going down and then up, but we have no reason to believe that it will ever pass above the. The initial point here, and that is what would happen if it would be not contractive. So you see here it stays for the moment below. And I think we have sort of an idea that this could be also a B-stable method, but we have not proved that, so we don't really know. Here you see, just for comparison, the You see, just for comparison, the geodesic explicit Euler method, as you expect, it will not be contractive. It will go down for smaller step sizes, but then at some point it will start growing. So the explicit Euler method will not be contracted. Alright, some recent developments. We are working together also with some people in China at the Chinese Academy of Science. And we think now that We think now that there is a proof we have a proof that this geodesic implicit Euler method has a unique solution for negatively curved manifolds in general. So this is, I don't know how much this helps, but this is at least something. And things we are working on is to extend to other methods. So we'll be looking at this JIP method. We are not yet there. I suspect that you need to restrict to symmetric Riemannian spaces. Remaining spaces, the remaining manifolds, if you want to get it, but this remains to be seen. Then, conditional stability will be important when we apply to neural networks so that we can find bounds on the step size where we get contractivity, because that's usually what is done, as far as I can see in the literature for neural networks. And we have some ideas also because we are not sure that it is such. Sure, that it is such a big damage for positively curved spaces that we get this for small step sizes, this increase, slight increase, slight increase in the distance. So, maybe by considering several steps, that we can do something to overcome the barriers that we have seen in numerical. And then, of course, we want to derive stable methods in deep learning applications. So, I think that was it. So, here are a few references, and I think that. A few references, and I think that I will stop here actually. So thank you for your attention. Lots of time for questions. Thank you for an offer. Very clear talk. Yes, sir. Brian, sorry, I've been asking a lot of questions, but I thought I should volunteer. Oh, yeah. Out of the way. So, how do you ensure that your next steps stay on a manifold? How do you ensure that your next iterator stays on a manifold? Oh, yeah, so that lies in the construction of the format of the integrators. They are defined so that just in terms of intrinsic operations. So that in some sense, you can say that the method knows no other world. You can say that the method knows no other world than what is on the manifold. It's not inefficient. It's not inefficient. It's not like of course often in practice you will do that, but what Maylin says is correct. It doesn't even assume that your manifold is embedded inside Euclidean space or anything like that. For the numerical experiments that you have shown regarding the positive definition. Regarding the positive definite matrices, what exactly are the metrics? Yeah, so what so in that case I think we model the positive definite matrices as n by n matrices. But all the operations we do ensure that all the steps we take and so on are already by definition managed. By definition, it makes the next step also symmetric also definition. I don't know if I missed your question, but I think you're answering it. I'll ask you later quick. Yeah, okay, all right. So, what about order of your schemes? I mean, is there a nice notion of order? I think you're I think that's another interesting uh thing to consider. I know that uh Hans Mutukos now uh the same place as number one there is uh is published. Hans Mutukos published a paper about numerical integrators for symmetric spaces and I think this is Riemannian symmetric spaces he does there and and there I think he proposes a format. I think that he's using geodesics in this essay. I look at Elena because maybe she wrote the red. Luke Katela now, because maybe she wrote the red paper. But I think maybe they are using geodesic, that he's using geodesics in a way. And we know for sure that there are ways to do this, because already many years ago, we proposed some retraction that is generated just from the geodesic just from the use of geodesics. And with the retraction, you can get arbitrary high order if you like. Arbitrary high order if you like. So we know that it's possible anyway, but maybe there are better ways than let me know. And other retractions? I mean, the exponential is very universal, but I mean... Okay, maybe you need an embedding there. But I mean, if you do something like for quadratic D groups, the KD transforms, it's much cheaper, then, right? That can be much cheaper, yeah. So, so far, we have been really focusing on the very, there's something. Focusing on the very, there's something nice about using on the geodesics also because it means that you don't add any structure to your manifold, right? You use exactly what you have and nothing more. But of course, to get efficient methods, you should also consider other cases. I completely agree. In fact, Evan and I, it's like many years ago, had this apariant interpolation method. It's like symmetric spaces. And here it uses local interpols between interpolable systems. It's like an sort of space which is analogous to sort of, you know, it's like canonical coordinates in the first canon. That sounds a little bit similar to what Thomas is doing in this. So it's sort of like the finite analog of the Cartan decomposition. Yeah, yeah, yeah. Any more questions? Any more questions? Okay, well I think