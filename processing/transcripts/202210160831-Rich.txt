Multistability and dynamical systems modeling epileptogenic neural sects, please. Awesome. Thank you guys so much for having me. Albeit virtually, hopefully one day I'll be able to participate in person. So yeah, so today I'm going to talk about multi-stability and dynamical systems modeling of elliptogenic neural circuits and basically sort of motivate why computational neuroscience. Computational neuroscience, and particularly more mathematically motivated computational neuroscience, is such a powerful tool in studying epilepsy. As a postdoc on the job market, I was particularly happy early in the conference when you guys were all excited about computational neuroscience and its applications to epilepsy. So, hopefully, I can keep that excitement growing with this talk. By means of an outline, I'll start by going into a little bit of mathematical background. Little bit of mathematical background given the interdisciplinary audience here talking about stability, bifurcations, and dynamical systems and what these terms mean in the particular context of our studies. I'm then going to walk through two works that I published during my postdoc that sort of use these terms and these concepts in slightly different ways. The first uses this idea of bi-stability as a more descriptive tool to understand what dynamics we're observing. What dynamics we're observing in simulated computational networks, particularly of inhibitory neurons prior to epileptogenesis. And then the second work actually uses a little bit more mathematical detail, a slightly different type of model where we can use mathematical analysis and the tools and theory of bifurcation theory to actually understand what the underlying differences are in systems that may look more like. Systems that may look more like epileptogenic circuits than not. And then to conclude, this session sort of ends up being a mini Cremble session. My supervisor, Dr. Valiente, is giving the next talk on a work that we recently published in Cell Reports. So I'll give a little bit of a transition into his work and serve as the opening act a little bit and show you how the ideas that I'm going to talk about in this talk feed in a little bit into the work that he's going to talk about. The work that he's going to talk about. So, in order for everyone to be on the same page here, I know it's such an interdisciplinary audience, and some people may not have thought about differential equations in a long time, depending on their background. So I wanted to give a little bit of a, you know, a brief update on some of these terminologies. So, when we talked about dynamical systems, we're quite simply talking about any sort of model of some physical phenomena. Typically, these are Phenomena. Typically, these are differential equations. So, the simplest toy example, which I'll use to illustrate some of these things, are, it's a differential equation, which is a one-dimensional position in space. So if I give you a differential equation dx dt, telling you how a ball in one dimension is moving at a particular time, I can tell you that as a function of both x and t, as a function of both its position and also whatever. Its position and also whatever the time is of the simulation. Now, one of the first things we do when we're classifying these types of systems is we look for specific special solutions known as equilibrium solutions or fixed point solutions. I may go back and forth between those terminologies a little bit. And those are any positions that will persist for all time in the absence of a perturbation. Essentially, it's any value of x such that the differential equation defining the Differential equation defining the dynamical system will be equal to zero, meaning that there is no potential for the differential equation to change. The rate of change, dx dt in this case, is equal to zero. So in this simple toy example, if dx dt is either x or negative x, and I plug in x equals zero, in both cases, dx dt will be equal to zero. But they actually represent completely antithetical phenomena. In the former case, In the former case, it's essentially like you have a ball perfectly balanced on top of a hill that's sitting very, very precariously, but happens to just be perfectly balanced. The second case is a ball sitting at the bottom of a valley, which is something we would more intuitively think of as something that's sort of sitting in stasis. Now, we obviously want to be able to differentiate these two phenomena because they are so antithetical. And this is where the terminology of stability comes. Terminology of stability comes into play. So we call one of these solutions unstable if any perturbation is going to force the system away from that equilibrium. That's the example where you're sitting on top of the hill. If I nudge that ball in any direction, it'll go rolling away. In contrast, we call the equilibrium solution stable if any little perturbation will cause the ball to sort of fall back down towards that equilibrium. Towards that equilibrium point. So, again, if the ball is sitting at the bottom of the valley, I perturb it up the hill a little bit, it rolls back down and eventually settles back at the equilibrium solution. Now, as I mentioned, these are both potentially the simplest differential equations and dynamical systems I could give you. In reality, and especially when we're modeling epileptogenic neural circuits, we're going to have a much more complex picture of stability of these models. These models. Oftentimes, these can exhibit multiple of these equilibrium points, and multiple of these equilibrium points can be stable. We call these bistable systems. So this is an example where depending on where I start the simulation or the system, what types of perturbation I give the system, we may tend towards one of two or even multiple stable states that the system will persist in in the absence. Persist in the absence of some other external perturbation. So, in this toy example, depending on sort of where I drop that ball initially, we'll determine whether I settle down into position two or position three. Now, things get a lot more complicated when we're actually modeling things of interest as opposed to one-dimensional toy examples. And one of the ways these can get more complex is if we have a model parameter that can actually change the stability. That can actually change the stability structure of the system. Whenever that stability structure changes, it yields what we call a bifurcation. Now, this is something where the mathematics becomes a little bit more complex. There's a whole field studying these bifurcations and what their effects are on dynamical systems. But one thing to note in the study of epilepsy is that it is well known that certain types of bifurcations where you transition between multistability. Between multi-stability or bistability, and then go into a system where you actually have an unstable fixed point, those can actually yield oscillatory dynamics. Now, obviously, this is where we sort of leave behind this one-dimensional toy example that I've shown you here, because obviously we can't sort of, you know, oscillate in one-dimensional space. But this is an idea that I'll come back to a lot as I start going through the examples from our recent publication. Examples from our recent publications. So, with that basis in mind, I want to talk a little bit about the role of inhibitory neurons in epileptogenesis and in seizure onset. When I joined the lab, there was a recently published paper by a grad student under Dr. Valiante's supervision, Michael Cheng, which showed something somewhat counterintuitive. It showed that in hippocampal slices treated with four amino acids. Slices treated with 4-aminopyridine that make these hippocampal slices epileptogenic or models of epilepsy. What you can actually do is selectively optogenetically stimulate inhibitory neurons. In doing this, counterintuitively actually promotes seizure-like or icto-like dynamics in these slices. And this goes somewhat counter to our baseline understanding of epilepsy as a disease that unhappy. As a disease that unbalances the EI balance in the brain. We often think about inhibitory cells as restraining seizure-like activity and preventing it. But what Michael showed is that it actually might be a potential promoter of ectal activity. The analogy I always like to use here is that if your brain is a car, an excitatory neurons are a gas pedal, and inhibitory neurons are a brake, essentially what Michael showed. Essentially, what Michael showed is that in some scenarios, you can slam on the brakes and actually cause your brain car to go craning off at high speeds, a la seizure. And if that happened to your car, you'd be really, really confused. You'd probably take it to a mechanic and try to figure out what's going on under the hood. So that was something that we started the process of in this work. And what we proposed was what something we termed the GABA ergic initiation hypothesis, which was a sort of multi-hydrogen. Hypothesis, which was a sort of multi-step pathway by which we thought we might be able to explain the tendency for inhibitory activity to actually trigger as opposed to restrain seizure. Now, in order for this hypothesis to be in any way viable, we needed to understand that sort of first initiating step. We needed to see whether intraneurons in epileptogenic settings would be more prone to undergo a sudden transition and give. Sudden transition and give us a strong inhibitory signal that might trigger the remaining factors in this process. So, what we did in order to do this, and I'll mention here that this was actually the first work I published in my postdoc done with two of my co-supervisors, Dr. Francis Skinner, as well as Dr. Valiente. And then my once-and-always collaborator, Dr. Homero Morati-Chame, provided the experimental results. Experimental results. What we did is we used Dr. Morati Cheme's experiments from a single intraneuron, both before and after treatment with 4-aminopyridine, to constrain models of intraneurons reflecting how 4-aminopyridine makes these hyperexcitable. And the main differences between these models are the shift in their rheobase and the increase in spike frequency adaptation exhibited by the four immunopyridine-treated interneurons. Pyrrhine treated interneurons. Now, as I said, we treated models of each of these cases. We did this using the Isikiewicz formalism. For those who are more mathematically minded, you've probably heard of this. This is a relatively simple system of two ordinary differential equations, six parameters. What's really nice about it is, despite being relatively simple from a mathematical perspective, it can actually replicate a wide range of neuronal activities ranging from Activities ranging from simple spiking to complex bursting. So it's a very commonly used model in computational neuroscience. Now, with these models in hand, we essentially created two inhibitory networks. One network where we had inhibitory neurons that were all controlled. One where we had all of our intra neurons, all being the 4AP model. So again, two networks, only inhibitory signaling. Only inhibitory signaling, where all of the neurons were either our control model or our four-aminopuridine model. And we gave these neurons atomic external input, so there would be some spiking, and tried to understand what the differences between the dynamics solely responsible for by the treatment of foraminopyridine might be. And initially, we didn't actually find any differences that were all that interesting. were all that interesting. It wasn't until we started thinking about things in the context of biasability where we found something quite interesting. So what we did is instead of just thinking about how these models behave from randomized initial conditions from a random starting point, we looked at this differences between network dynamics from these randomized initial conditions and then after a synchronizing current pulse. Synchronizing current pulse, a high amplitude, short duration input identical to every neuron in the system that essentially instantiates a single instance of near perfect synchrony. And the idea being, if anything was going to force an asynchronous system into a synchronous state, it would likely be a perturbation like that. Now, in the control settings, we found more often than not, systems that were asynchronous from random. Systems that were asynchronous from randomized initial conditions would decay back into asynchrony following that perturbation. Thinking back to our toy model, essentially what we did is we found ourselves in a valley representing asynchronous sparse firing. We tried our best to push ourselves out of that valley and over the hill, but we were unable to. We were unable to determine whether there was any sort of other stable dynamics that these systems could exhibit. Systems could exhibit. Contrast that with what we found in 4AP-treated networks. Then, this synchronizing current pulse was much more likely to yield stable, persistent oscillatory dynamics following the perturbation. Essentially, we pushed our ball out of the asynchronous stable state and into a new synchronous stable state that would persist for long periods of time. And not only did this synchronous state state state state state And not only did this synchronous state obviously yield oscillations, but it also yielded increased neuronal fire. There's a rich literature on the computational study of inhibitory networks, part of which shows that inhibitory networks have big fire more often when they are in these synchronized states. So the results of this were interesting in two different ways. First, relative to our GABA ergic initiation hypothesis, Initiation hypothesis, we essentially showed why an epileptogenic system would be more prone to seizure triggered by this pathway. It's because hyperexcitable for ammunopyridine treated interneurons are more likely to be in a bistable state, more likely to suddenly transition into a hyperactive state, in a hypersynchronous state as well, that might provide this inhibitory bolus. What's also interesting about What's also interesting about this is that it is known experimentally that intra neurons exhibit increased firing in the moments prior to seizure onset. And this is often contextualized as the brain sort of trying its best to put the brakes on a seizure event. What we've done here is we've shown A, how this can happen in hyperexcitable interneuromal networks. And we've also contextualized this where maybe this isn't always. Where maybe this isn't always interneurons trying to put the brakes on the seizure. Maybe this event in itself is actually contributing to the seizure-like activity. This is something that there's been some exciting more recent experimental studies that dive into this more and basically showcase how epilepsy isn't as simple as an EI imbalance. There are more complex and convoluted pathways to seizure-like events that may be contributing to seizure. May be contributing to seizures under the wide swath of disorders characterized as epilepsy. Now, you've probably noticed when I discuss this study, I've talked about biostability as a descriptive term. I haven't gone into the guts of the mathematics. And that's by design. The Isikiewicz neuron, as powerful as it is, isn't particularly amenable to mathematical analysis. In this next study, we chose a different neuron model. We chose a different neuron model specifically so we could do these types of precise mathematical analyses. And what was of interest here was an interesting phenomenon we observe now in networks with both excitatory and inhibitory cells, where depending upon the amount of intrinsic noise in the system, systems would be less prone to sudden transitions into hypersynchronous, hyperactive activity that echoes seizure. That echoes seizure. And what we found is that increasing the level of noise in the system made it so these systems were less prone to these transitions. Now, how do we understand these? Well, first we need to look at our model. And I'll mention again, this was work that was actually performed with all three of my, I finally refer to them as my Hydra of supervisors, Dr. Lefebvre, who will also be talking momentarily, Dr. Valiente and Dr. Skinner, as well. Valiente and Dr. Skinner, as well as Dr. Axel Hutt, who is a frequent collaborator of Dr. Lefebvre's. Now, this is a more complicated model. There are both excitatory and inhibitory cells. If you look at this model, you see a lot more Greek letters that might scare you off. So I won't delve into what all these Greek letters mean. I'll just mention briefly that if you are more mathematically inclined, this may look familiar to you as similar to the famous Wilson-Cowan equations. That is by design. Equations that is by design. This is essentially a spiking network equivalent of the Wilson-Counten equations that facilitates our later simplifications. And I'll also draw your attention to the fact that these neurons have noisy input. This is something that was absent in our previous model. These neurons themselves are also stochastic in how the spikes are determined. But I won't go into the mathematical details of that process here. So, what we found. So, what we found when we simulated these networks, and again, these are EI networks, there's 800 excitatory and 200 inhibitory cells, is that dependent upon this term D, the amplitude of the intrinsic noise, we would see different responses to an increase in the input to excitatory cells. When noise is low, this increase would yield sudden transitions into hypersynchronous hyperactive activity. Hypersynchronous hyperactive activity. When this noise was high, you would see a slight increase in activity, but nothing even remotely looking anything seizure-like. So it seems that increased noise mitigates phenomena that echo seizure-like activity. The question is why? And because we chose our model in this informed way, we were able to do mathematical simplifications that allow Simplifications that allow us to directly assess the stability structure of the system. We use something called mean field approximations, mean field analysis, to simplify our system from a system of a thousand differential equations, one for each neuron, into a system of two differential equations, one representing the mean excitatory activity, one representing the mean inhibitory activity. And I won't go again into the details of these things here. Into the details of these things here, I'll just mention that the noise amplitude D does play a pivotal role in the dynamics of this system. It's just buried a couple layers deep in the definition of this function rho that in turn determines this function f, which factors in directly to the baseline mean field equations. Now, what we found is something completely different about these systems in the presence of very low. Systems in the presence of very low noise, we would see multi-stability, multiple fixed points in between this pre- and post levels, the 0.0 and 0.1 levels of I naught. So essentially what would happen is this explains the sudden increase in activity following this perturbation. In contrast with high levels of noise, these changes are very gradual, and that explains why we see a much And that explains why we see a much more, you know, understandable gradual change in the activity following this perturbation with high noise. If we delve deeper into mathematics, we actually see this phenomena I foreshadowed earlier in the talk. When D is very low, what happens to this system is we initially have a single stable fixed point. The fixed point is stable because its value alpha is less than zero. We then have this period where we have multiple fixed points. And once these multiple fixed points go away, what we're left with is now a single unstable fixed point. And it's unstable because this alpha value is above zero. So we've undergone a bifurcation, specifically what's known as a saddle node bifurcation, from our single stable fixed point to a single unstable fixed point. And what this yields. Point. And what this yields is that oscillatory activity we saw on the network. This is a well-studied, well-understood property of saddle node bifurcations. Contrast that to when D is high. Now we see just a very gradual increase in all of these elements of the stability structure. In particular, we don't see any bifurcations. We don't see any change in this stability. This alpha value remains below zero. Below zero. Now we took this one step farther, and you notice the title of this paper is Neurostimulation Stabilizes Spiking Neural Networks. We asked ourselves how this might inform treatment paradigms and neurostimulation paradigms to prevent seizure. Most often, these devices currently use what's known as a biphasic pulse, just a sort of square current pulse that's a negative followed by a positive square pulse. Positive square pulse. And that's just because of engineering restrictions and the history of these devices, et cetera. But what we thought was that if internal noise stabilizes this system, it would make sense that externally applied noise might have a similar effect. And lo and behold, it does. If we model neurostimulation as just a noisy process, that eliminates this transition into hypersynchronous, hyperactive oscillation. Hypersynchronous, hyperactive oscillations. And so, again, this models the prevention of seizure-like dynamics. Now, I'll leave the details of this to the rest of the paper. We go into a little bit of what the properties of the noise might be, why these different properties would have different propensities for preventing seizure. But this is, we think, something very exciting for the development of next generation neuromodulatory devices where you might be able to fine-tune what. Might be able to fine-tune what that neurostimulation paradigm is to optimally prevent seizure. So, as I mentioned in this next talk, Dr. Valiente is going to go into a paper we recently published in Cell Reports, which is a really cool synthesis of some of the ideas I've talked about so far. In particular, we built off the sort of architecture of the network we had in this scientific reports paper. We synthesized some. We synthesize some really exciting experimental data with it, and we use that network to ask a slightly, to answer a slightly different question about the role of neural heterogeneity in these circuits. But we were still able to analyze that both with spiking networks and mathematical analyses in a similar fashion. And I won't do anything to spoil Dr. Valiente's talk, so I will cut off my foreshadowing there. My foreshadowing there. I'll just briefly thank the various sources of funding for my postdoc, both through my supervisors as well as awards directly to me. I'd love to talk to you guys further, especially since I wasn't able to join you in person. Please reach out to me either. You can find me on Twitter. You can find my personal information on my personal website. I would love to talk to any and all of you about this work more. And with that, I'll open things up for questions. I'll open things up for questions. Okay, let us thank this video. Stop our talk.