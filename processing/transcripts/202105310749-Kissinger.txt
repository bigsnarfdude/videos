Good. Okay. Thanks very much to the organizers for inviting me to this workshop. I'm sad that I'm not skiing in the mountains with all of you, but hopefully next time. Yeah, so I'll start with saying what the maybe what the general setting or the idea of what I'm trying to accomplish is here, which is something like black box. Which is something like black box causal reasoning. So, what this means is that we have some agents, probes, moments in space-time, something like that, that have some labels. And then we want to ask a number of questions about them. So, can we model, observe, or prove properties about causal relationships between these things? Okay, so the thing that distinguishes, say, black box causal reasoning from From something that's more concrete, I don't really necessarily want to say what these processes are. They could be quantum processes, classical processes, or some generalization thereof. Okay, and there's at least two interesting settings to look at here. And one is the shared device kind of settings. So here you can imagine I have some agents, A, B, C, D, E, and they each have some settings that they can put into a box. Settings that they can put into a box, like measurement settings, and then they get outputs. So you can imagine this is like a bell-type scenario or some generalization where there's some causal relationships between these parties as well. Another kind of setting, which you could see is the kind of dual of that setting, is rather than a shared device, a kind of shared environment setting. So here we've turned everything upside down. Everything upside down. Now, rather than putting an input to the box and then getting an output, you get an input and produce an output. So this is an interesting setting if you want to vary, for instance, the causal relationships that the agents have been placed into. Okay, so for example, we could have something like here, here Alice is before Bob. You could imagine the kind of wiring between Alice and Bob. kind of wiring between Alice and Bob as hiding in this box. So I could have some output here which is wired into some input there and this composes Alice and Bob's processes in this order. It sounds like somebody's not muted. I'm hearing some kind of printing type noises. Yeah, that seems to be gone now. And you could, so you can imagine that if I put this wiring in this box, I That if I put this wiring in this box, I could consider different wirings, right? So, if so, here if Alice is before Bob, I can think of the box as being wired up this way. And if Alice is, if Bob is before Alice, I can think of the box as being wired up this way. But, of course, since that's in the box, you could imagine more generalized things, such as mixtures of these causal orders, or superpositions of causal orders. So indefinite causal structures, like Julio was talking about before. Okay. Okay, so yeah, I guess most people here are kind of familiar with this idea that it's quite an interesting thing to look at, these indefinite causal structures, which arise basically from these shared environment situations where you might have something like a superposition going on inside the box. Okay, so the setting for this is process theories. So a process theory in In categorical language, is a symmetric monoidal category with an extra thing called discarding. So, if you don't know what a symmetric monoidal category is, it's basically just a minimal setting for plugging stuff together. Okay, so you can have processes, so things which have an input and output type, which I'll represent as a box like this. And I can have boxes on joint systems which look like this, boxes with no input. Boxes with no input are called states. You can think of this as the process which prepares something. Boxes with no outputs are called effects, which you can think of as something like a predicate. And then you can compose things in sequence and in parallel. The order of these things can be sort of varied, which you can draw as crossing wires and so on. And these things all kind of fit together in a sensible way, and that's called a symmetric monoidal category. Symmetric monodal category. So the one thing that makes a process theory different from just a symmetric monodal category is this special process called discarding. So discarding is just a distinguished effect, which intuitively we think of as just taking a system and ignoring it or throwing it away. Okay, so this thing doesn't really satisfy any interesting properties on its own, but as soon as you have discarding around, you can make interesting definitions. Make interesting definitions with respect to causal structures. So, the most basic one is something which is just called causality. And if you're familiar with operational probabilistic theories and this kind of Pavia notion of causality, this is exactly the same thing. And the way it's drawn in this process-theoretic picture is a process, say phi here, is causal if discarding its output. If discarding its output amounts to discarding its input. Okay, so the intuition here is if the outputs of a process are ignored, it doesn't matter which process actually happened. So there's not some sort of hidden side effect of doing this process. Everything that that process does is reflected in its output. So a consequence of that is causal processes only affect the processes which consume their outputs. Which consume their outputs. So, only things which are plugged into the future of this thing, so things that lie in what you could call the causal future. So, if all of your processes satisfy this causality condition, then I can draw a diagram of such processes, and I can see the wiring as effectively the causal ordering between those things. Okay, so some special cases. There's only one causal effect. So, if something has no outputs, then discarding all of the outputs amounts to discarding all of the inputs, which just amounts to saying that that effect is already discarding. So you know that there's only one, for instance, there's only one CPTP going into the trivial system, which is just tracing out the input. And causality for states is just normalization. So if you think Normalization. So if you think a generic state is something like a positive operator, then a causal state is a density operator, so a trace one positive operator. That's the kind of image you should have in your head of these two special cases. Right, so in addition to this basic causality idea, you can also express signaling conditions using discarding. So one-way signaling of a byte. So one-way signaling of a bipartite process is just if I discard on the right, it kind of falls through. It kind of disconnects. And now I have just a wire from A to A prime. So the way I can read this picture is from, say, Alice's perspective. So taking Alice's perspective, then there's no influence from Bob's input to Alice's output. Okay, so Alice can send a message to Bob, but not vice versa. Bob, but not vice versa. Right, and the non-signaling is just a symmetrized version of that. So neither Alice nor Bob can send messages to each other. Okay, or Alice and Bob can't communicate, though they can share correlations. Okay, so in the classical case, now I could imagine taking some concrete process theories and instantiate these definitions and get familiar things. Familiar things. So, in the classical case, I'll take the kind of simplest thing I can think of to do something like probabilistic reasoning. And the simplest thing I can think of for that is I just take a process theory whose objects are natural numbers, like a dimension, and the morphisms, or the processes, are matrices of positive numbers. Okay, so in that picture, discarding is just a row vector all of ones, and that's set up such that if I And that's set up such that if I discard a state, if that equals 1, well, that's just saying that the state, which is a column vector, its entries sum up to 1. Okay, so this is set up such that your states are probability distributions. So my states on an object or on a system n are just probability distributions over n values. Okay, so the causal states, probability distributions, causal processes are things that preserve. Are things that preserve probability distributions or stochastic maps? And the signaling conditions are just the conditional independences that you would expect if you read about signaling conditions. So here, marginalization is discarding. So here I've discarded B prime, which means I've taken that marginal, and I see that it doesn't depend on B anymore, and symmetrically. The quantum case is very similar. The only thing that's different are my basic processes. So now, rather than just matrices of positive numbers, I take the quantumized version, which is completely positive maps. And then in this picture, discarding is the trace, and causal states are just the things which have trace one. Okay, so this thing is a CP map from a trivial system to some Hilbert space, which is just a positive operator. Which is just a positive operator. And then tracing at equals one says it's a density operator. Okay, so causal states are density operators. Causal processes are completely positive trace-preserving maps or channels. And the signaling conditions are just the usual ones. So if you know what a non-signaling quantum channel is, there should be no surprises to what this kind of amounts to. Okay, so what about all this? Okay, so what about all this higher order structure? So we see that there's all these interesting things you can do with indefinite causal structures and so on, but we really need super maps. So things which take maps to other maps. Okay, so one particularly simple way to think about higher order maps is to use compact closed categories. So what's a compact closed category? So, what's a compact closed category? Well, it's a process theory which has a kind of abstract version of Choi-Yamikovsky isomorphism. So it has a way of treating any process, so something which goes from A to B, as a state on a joint system. So maybe a state on A dual tensor B. Okay, and the way you can picture this is just by bending the wire, so by putting a cup before a process. Okay, and that's the intuition, but actually, the way that this is formalized, as I say, kind of abstracts the Chojamikovsky isomorphism. The way this is formalized is really you just have two processes. You have a state, which we draw as a little piece of cup, and an effect, which we draw as a piece of cap, like this, satisfying these two equations. And that's enough to guarantee that this thing is indeed an isomorphism. That this thing is indeed an isomorphism. Okay, so why does this help you deal with higher-order stuff? Well, it's because I can treat, as soon as I can treat processes as state spaces, then it's very easy to think about something which maps processes to processes. So I let this kind of A lollipop B thing be defined this way. And then something which And then something which maps a map from A to B to a map from C to D looks like this. So it's something which takes an A star tensor B as its input and produces a C star tensor D as its output. And then it's a little bit more intuitive to deform this picture a bit, and you start to see these kinds of Pac-Man shapes, like you would have seen also in Giulio's talk. So you now this really captures this intuition that. Captures this intuition that this second-order process wants something to get plugged into the mouth of Pac-Man, something which goes from A to B, and then on the outside here I have something which goes from C to D. Okay, and all of this categorical machinery, compact closed categories and whatnot, is basically just in place in the background to make sure that if I draw these kinds of pictures and I plug wires however I like, as long as the time Plug wires, however, I like, as long as the types match, then it makes sense and I get a well-defined composition of these things. Okay, so that's kind of what makes these categories so simple. The fact that anything I plug together makes sense, makes these things simple. But it also causes a problem. So, this problem is. So, this problem is rooted in the fact that if I think about A tensor B, and if I take the dual of that, well, in a compact closed category, that's the same thing as A dual tensor B dual. Okay, which seems sensible. If you think about vector spaces and how duals interact with tensor products, that is kind of a sensible condition. But it's kind of disastrous for higher-order things. Okay, so if I wanted to find, for instance, this thing which takes a For instance, this thing which takes a map from A to B as its input and gives me a C as its output. Well, in a compact closed category, that just amounts to this, using this definition of this mapping thing. So it's an A tensor B star tensor C. All that's really remembered is that B acts like an input, and A and C act like outputs. So if I shift things around, I see that this type is actually exactly the same. Type is actually exactly the same as this type. But now this thing took a process as an input, and this thing is all about just systems. It takes a B as its input, and it produces an A and a C. So this means that all of this beautiful higher order stuff all just collapses to first order. And the reason this happens is basically because of this equation. So this distributivity between the tensor and the star. So if we throw away this condition, So if we throw away this condition, you get a different kind of category which is called a star autonomous category. And really the only difference between a star autonomous category and a compact closed category is that this isomorphism is not true anymore. So if I take the duals of two systems and I put them together and I take the dual of that, then that's not just the tensor product again. But interestingly, this gives me something new. Gives me something new. So I can actually take this new thing I get, which you can see is a kind of De Morgan dual of the tensor product, and I'll give this another name. Now, this is called the par in the kind of linear logic parlance. But you can think of it as just a different way of forming a joint system, which is related to my tensor product by this equation. Okay, so I'll say a little bit. Okay, so I'll say a little bit more about that, but first I'll tell you the kind of recipe that can be used to get a star autonomous category out, which lets you say interesting causal stuff. So the basic idea is I take a raw materials category, say matrices of positive numbers or CPMs, completely positive maps, and then I perform a construction on this category. On this category or on this process theory to get a star-autonomous category which has inside of it a kind of logic of causality. Okay, so if I do this to matrices of positive numbers, I get higher order stochastic maps, which actually theoretical computer scientists already have a name for these things. Sometimes they're called probabilistic coherence spaces. So that's the classical case. And if I do this for CPM, I get higher order quantum channels. I get higher order quantum channels. So, this is where things like quantum cones or process matrices all live in this category. And you could imagine taking some different kinds of raw materials, okay, so maybe box world or something, and then I get, and I apply this construction, I get a new kind of non-standard model of causality, which I can study and play around with. Okay, so before I give an idea of how this construction works, I'll tell you what the star is, because kind of all of the interesting stuff is kind of hiding in this star. Well, first, I can say what it means to define the star on a set of states. Okay, so if I take a subset of all the states of a system A, then the star of that set is just all the effects, which is normal. Just all the effects which are normalized for all of those states. So if I took, for instance, all of the states of a system, then C star would just be discard. The only thing which sends every possible state to one is just discarding, is just the trace. But then I can get more interesting things by looking at smaller and smaller subsets of this thing. Okay, so this star is a useful operation. One thing is, if I do the star twice, I get states again, and I can think of this as a kind of closure operation. So without even knowing anything about my theory, I can say what it means for a set to be closed, and I don't really need any convex structure around to talk about this kind of closure. Okay, so with that star around, I can just say what this new process theory, cause of C, is. So its types are pairs, A with a C of A, so that's just a kind of carrier system, like a Hilbert space, and then a closed set of states of A, which is called the causal states. And then the processes are just the things which preserve the causal states. So something which goes from A. States. So something which goes from A to B, if there's a causal state in A, it should be a causal state in B once I map it over. Alex, you have five minutes still question. Okay, great. Okay, and that's it. That's basically it for the definition of this thing. And now this isn't a compact closed category anymore, it's SARA tonomous. So the one kind of connected, the tensor product, actually. Connected, the tensor product actually becomes three things. I have the tensor, which I can think of as the little joint system of a pair of systems. I have the par, which I can think of as the big one. And I have the lollipop, which I can think of as the maps between two systems. Okay, so so I have I have now at least three basic ways to combine my systems, and these are all actually or can be all different from each other. Can be all different from each other. So if I look at my first-order systems in this new process theory, then these are systems where I take my carrier and I take the star of discard. So this is taking my carrier with all of my causal states, so all of my normalized things. Okay, so this is just like a normal state space. In the classical case, it's all probability distributions. In the quantum case, it's all probable. Probability distributions in the quantum case, it's all density operators. And then if you work this out, you see that actually tensor and par are the same as one another for first order. Okay, so you can think, well, actually, maybe this is the reason why if you crack a textbook on quantum mechanics, you will never see something that looks like the par. Because if you're just thinking about states, there's no difference between the tensor and the par. There's no difference between the tensor and the par. But if I think about processes, well, then if I take a system here, which is the processes from A to A prime, and I tensor it with the processes from B to B prime, the thing I get out is causal non-signaling processes. Whereas if I take the power of two such things, I get all processes. I get all processes. Okay, so this is kind of what I meant by you should think of the tensor as the little one and the par as the big one. Okay, so this is a fairly small space of maps, just the non-signaling ones, whereas this is a huge space of maps where A and B can communicate with each other arbitrarily using such a channel. Okay, so you get a kind of hierarchy appearing. You get the non-signaling things which embed in all Signaling things which embed in all processes. And in fact, you can define types for each of the kinds of one-way signaling things. So here's Alice before Bob, and here's Bob before Alice. So I get this whole hierarchy, which you can then prove all these embeddings in a star autonomous category. And then if you hit this whole sort of diagram of embeddings with the star, well, all the arrows flip around. And up here on top, you get all the correlations. And up here on top you get all the correlations. So here's all the processes where Alice and Bob just share an entangled state or something. In between those are all the two combs. So this is the environments where Alice is before Bob and Bob before Alice. And then at the top of this thing are the process matrices, where Alice and Bob are put in an arbitrary causal and possibly indefinite causal structure. Okay, and you can actually see in all of these studies of indefinite causal structures, you always see an appearance of this non-signaling type under a star. Okay, so the study of indefinite causal structures is really the study of these kinds of types. Okay, so the last thing I want to say is a bit about how these things compose. So, composition at the first order is kind of easy because first order processes have well-defined parallel and sequential compositions, which suffice to build any kind of composition you like. Higher-order processes, it's not so simple. So, if I take the tensor product of two things, like a process matrix, this is typically not a thing again. It's typically not another kind of process matrix. And there are many ways to plug together higher order things, and some are well-defined, and some are not. Okay, so here's the thing I said about process matrices. If I put two of these together, then the resulting thing is not a process matrix. I'm sorry, I'll have to go a little fast here, but basically, the reason that it's not is because you can introduce tiny moves with a pair of process matrices and you can get contradictions. Okay, so how can I tell you? Okay, well, I'll try to finish up in a minute or two. So how can I tell if a composition is allowed? Well, in a compact category, anything goes, okay, so I can plug anything into anything else. In a star autonomous category, I've got this kind of category In a star-autonomous category, I've got this kind of logic of composition, where I can read each of my connectives as these kinds of logical statements. Okay, so tensor is like and, par is like or, implies not, so on. And the rule effectively is that a composition is allowed only if it corresponds to a tautology in this logic. Okay, so this thing, which doesn't So, this thing, which doesn't make sense, I can read it logically as A implies B or B implies A, which is not a tautology. But this thing, which does make sense, I can read it as A implies B or not A implies B, which is like P or not P. So this is indeed a tautology. And this is a, okay, yeah, I'll go a bit quick. I'll go a bit qui, I'll just kind of jump to the punchline. This is a certain kind of logic which is actually well understood called MLL. It's been studied basically since the 80s, and we even have tools for studying such logical statements. And bah, sorry about this. And this actually gives a this gives This gives nice, sufficient conditions for when things are causally consistent. And if we can sort of promote this to necessary conditions, then this gives us very efficient ways to understand the way these things compose with each other. And well, I'll just go to here. This gives us very efficient ways to understand compositions of these processes and could give And could give us a very logical understanding of questions like consistency of causal structures and coarse-graining of causal structures entirely in this picture. Okay, so apologies, that was a bit rushed, but I think I'll finish there. And thanks. Thanks very much. Okay, thanks, Alex. We have time for. For one quick question. Maybe. Does anybody have a short question that they could ask? One quick question. Does anybody have a short question? Okay, I guess Lucy and Alex, right at the very beginning, you have to get. It looks like this. Maybe Robin, if you mute for a second. So right at the very beginning, you talked about this theory with the discard. Just sort of in light of the previous talk or other work that's been done on time surrounding, the discard is a time-asymmetric addition. And it seems sort of against the spirit of the general stuff that you've been doing. So I wonder if you. I wonder if you've thought about relaxing that constraint. So, for example, in work I've been doing recently, you have sort of discards in both time directions, or you could call them ignore, because so you ignore the future or ignore the past. I wonder if you thought about relaxing that and see if it might make a difference to your overall categorisation. Well, so the disc card itself is an asymmetric operation. But this whole process theory, so this causes C process theory is symmetric. So any system also. system also has the time reverse system so so for instance there's there's a kind of if i have an a then i have another a star uh whose only state is the maximally mixed state um and maps from a star to b star are the same thing as channels going from b to a so in that in that So, in that sense, the discard does have a kind of it's it's maybe it's it's more sense that it gets used in this kind of symmetric way. But yeah, I'm not sure. I'm not familiar with this ignore thing. Does that sound like the ignore, or is that, or is the ignore really a different kind of beast from discarding? No, it's the same. It's the same thing. It's just you think discarding sounds like you have to do. So, you think discarding sounds like you have to do something in a particular direction in time? It sounds like it's, whereas actually, all you're doing really is ignoring. You're ignoring the future, or you can ignore the past. So, just to use a word that's more time-neutral. Alex, you're muted. Yeah, I think that's once you pass this kind of this This kind of this cause of C process theory, I think this sort of agnostic to the past or future way of thinking of discarding or of ignoring is the right way to think about it. Okay, I think we're going to have to stop here. And we'll, in the interest of getting almost back on schedule, we'll take a 10-minute break and reconvene at 10:30 Eastern Daylight Time. And you can rescale that. You can rescale that.