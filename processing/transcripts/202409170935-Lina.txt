Was against me, and I was forced to stay in Muslim for many reasons. So I am very jealous indeed. But anyway, so my background is I am a physicist. So I have a you hear me? Yes. Because there is some feedback. Anyway, I am a physicist and I'm working in neuroscience and neuroimaging. So of course, at the very beginning, I'm a bit At the very beginning, I'm a bit far from the topic, but I'm working with Roxanne, with Eric, with Jacques Belaire. And of course, the data that you are collecting and analyzing has some connection with what I'm doing in practice with my own data that are more related to this kind of system. But in fact, we share a common perspective to discover, in fact, the hidden variables. The hidden variables that we can infer from the data we can collect. And I'm working in sleep studies, and we are collecting data on the scalp with some channels. This is our observables. And we try to find the Latin variables for us, are the regions, or well, we can cluster, but mostly it is the region of the brain that are responsible, the sources, the Latin variables. Sources, the latent variables of the observation. So, this is, of course, a very well-known problem. And what is interesting is that, in fact, with the algorithm that we had done with Roxanne and Eric, not the algorithm that Eric just spoke about, but Eric often asked me to make this kind of connection and to understand where the disciplines were sharing the same kind of perspective. And of course, on my side, it is more the It is more what I will talk about about the Kalman filters that are quite related to discover the dynamics of the individuals that I'm using currently. But there is, of course, a link with the structural equation modeling that Eric just spoke about. The dynamical structural equation modeling with a natural extension when the time is involved in the data. Is involved in the data. And finally, the common filter that I will speak about. So let me start with, of course, as a physicist, I have to, my language is more with equations. I have absolutely no result to show you. I'm sorry, but well, you will see some results with a talk with Roxanne during the day, I guess, with a little different perspective, but very close to what Callman filters. Very close to what Kalman filters are giving to us. There is a Bayesian technique, in fact, behind all those techniques. Maybe not the structural equations, but still, there is a sort of Bayesian inference behind all of that. But in any case, you start with this kind of measurement model. You have observables, you have some data as Eric just spoke about, and you have a model that are generative of those observed variables, the data. Observed variables, the data. And to modelize, to have this kind of model, you have the Latin variables that are much more important for us, for sure. And you can repeat, and you have individuals that will produce this kind of observables, an array of data. And there is this latent variable that you don't observe, but you want to infer about all the things that you want to modelize. To modelize, and you want to learn about your data is the parameters. Sometimes some parameters can be known. For us, for example, the load factors, what you call the load factor, we call that the lead fields, they are known. They are known, but sometimes there is also some error on those variables. So maybe we could correct them, but in general, we know them. And you have the measurement noise with some variance that you can know if you collect some data. You collect some data from the system itself, or you don't know those parameters. So, you have to, of course, as you know, you have to make the list of what you know and what you want to infer beside the Latin variables as the parameters that you have in front of us. And we have also the structural internal model where the latent variables are related to each other to this regression factor. In fact, the In fact, a way to request the formalism is to say that the latent variables is a filter one minus gamma of s, one minus the regression parameter inverse, times the noise or some randomness that related to the fluctuation of your Latin variables. And at the end, your model will give you, of course, Our model will give you, of course, the expression of the variance, the population variance, and the mean of the population related to the parameter of your models. And the log likelihood mentioned, of course, the formal is you can compute the log likelihood of your system. And what you want to know, of course, it is all the parameters of your model that describe the your model that describes the latent variables and you will have of course your parameters to see to to be the parameters that will maximize your your luck likelihood so there is many ways to to to to find this uh maximum or the the parameters that will maximize the luck likelihood the mcmc and gibb sampler and other diff or other techniques numerical techniques that will embrace the full model The full model, the full system to find the parameters that make your system, to explain your observable thanks to the hidden variable, the Latin variable that you put in your system. So this is a classical SEM formalism. It has been extended to incorporate the time. And for us, for me, for the people that are in neuroscience, of course, the time direction is something very specific because you have this correlation. Because you have this correlation, you have this temporal correlation, the causality of the data that are now involved in this perspective. And the DSM is, for us or for you, are the formalism that will describe the evolution of the latent variables across the time, along the time. The data are multivariate from multiple individuals collected at many points. Individuals collected at many points in time. And if we have to make a difference between the previous scheme or the previous description, even if you have some repetition in the SEM, now the repetition are not given without any order. There is a temporal organization of the data that you want to account for. And now you have this description, and Eric also probably mentioned that before. probably mentioned that before that you have to you can write your model your observations in terms of the between and within variables the between variables will capture what is specific to the individual the traits and you will have also the between parameter able to capture what is time specific events that may be optional of course That may be optional, of course. You can put that in your model or not. And more importantly, you have the deviation that describes the trajectories of the individuals. The deviations of the individual i at time t, it is much more important for us, it is a model that will give you the within part of the model. And for all those categories of process, you will have the structure, the structural equation that gives you the relationship. You the relationship between the latent variables and the observer or the component of the observation and the intrinsic or the internal model for the latent variables. So this is a very general framework. In fact, it is exhaustive and you can modalize what you want in some sense in this kind of framework. Framework and to handle this the complexity of the entancy longitudinal data and with this within and between distinction between your components, of course, you can introduce randomness on the model. And this is interesting for on the point of point of view of the Kalman filtering that I will discuss later. I will discuss later, you can introduce randomness and to account for fixed or random effects. It will increase for sure because you will handle this globally all the complexity of your data, the longitudinal data, it will increase the computational load on the side of the between level because, for example, what it is illustrated in front of us, if we introduce in us if we introduce in the parameter of the within model the measurement model in the within if we introduce the the fact that the the capital G are no more fixed but they can be random they can be changing with individuals suddenly in the between model you will have also the equation to account for this fixed and random effects relatively to those parameters. Relatively to those parameters. So you will have this kind of mixture to describe the complexity of your data and potentially to have also this kind of complexity for the model itself with respect to the data. Of course, your variables, you will have also covariates. So of course the covariates may be part of the latent variables that you have in front of us. That you have in front of us, of you. What is interesting also, you have what Eric introduced in his model, the Latin transition model. That means that you will have some transition, the timing, the time will evolve here in this regression with respect to the t and t minus one. So you let also the system to be able to deal with this transition model. Model. So, what Kernman introduced a couple of years ago, it is something very close to what we have in this structural equation modeling that I just described briefly for you. But it was more emphasis is more related to the dynamic on the time series perspective. On the time series perspective. So you have the, you still have this observation of the equation that governs the observation, you have observed data, and you have a series of latent variables that you cannot observe as usual, but you are interested in inference on those data. And you still have the measurement noise with some variance. In general, the Kalman filter. The Kalman filter, all those techniques has been done for unique individuals, but in fact, you can extend the model to deal with a series of individuals. This is not a big deal, but it is not dedicated at the beginning to deal with a multi-individual, but the story will remain the same. You have the state equation. People will speak about the state in this uh in this framework, but it is it follows exactly what we had before. Follows exactly what we had before. We have a notore regression parameter or model here that governs the dynamic. And I will write the dynamic with only one delay. So it is an autoregressive model for order one, but we can extend that with more data or more memory in the process. It will be the same. But in any case, you have the structural equation here for the latent variable, again, with a round. Again, with a random fluctuation that will describe this internal dynamic. And for us, it is a dynamic of remember the sources that generate the potential that I will measure on the scalp of the individual. So this is only the framework that in which I'm working, but it is exactly the same kind of framework that Eric and others were describing for you just before. But here there is a change of perspective in terms of the variables that I will now show. Of the variables that I will now introduce in terms of the prediction and the estimation. So, suppose assuming that you have an estimation of the latent variable at time t minus one, the idea is to have the best prediction coming from this estimation. In fact, it is only the expectation of your state equation that you can use to define. To define what is your prediction. The prediction is written as with the t vertical bar t minus one. So this is the best prediction you can do from the estimation at time t minus one, which is eta i, i was the individual that you have in hand that you are dealing with, t minus one bar t minus one. And this from this From this estimation, you can use your state equation or you can make the only expectation of this equation to find what is the best prediction that you can do. In fact, in the same, it is not very complicated to use the same equation to have also the variance, and this is of course important for us to have to control about the variance of the error of the prediction. And you have an equation that we Addiction. And you have an equation that will give you this variance capital P T bar T minus one. Again, this is the variance of the error of the prediction. And you can relate, thanks to your structural equations, your state equation that you have above, you can relate this error or the variance of the error with the variance that you have at the level of your estimation. It is given by. It is given by this summation. Of course, you have the variance of your fluctuation that you have at the very beginning in your state equation corrected by the variance of your estimation, your previous estimation. So if you have this in hand, it is the graphic that you have just in front of you from an estimation, you can infer. Estimation, you can infer some prediction for the next time. Now, you have some observation. So it is incremental in time. You have the observation at time t, and from this observation at time t, you can correct your prediction to have an estimation for the next time. So this is what it illustrated here. So a prediction again at time t coming from the previous t. From the previous T-1 event, you will have an expected observation. The expected observation is nothing but the observation equation, the expectation of course, of this equation that will give you capital G, which is the model of your generative model of your data, times the Latin variables, or more precisely, the estimation, sorry, the prediction at time. Prediction at time t from t minus one, and you will compare this generative model or the generative production of your model with the real observation that you have in hand. And this is what we call the innovation. So the innovation is a discrepancy between the observation and what you can predict from your prediction on the latent variables. Latent variables. And now, Kalman filter is only to say that if you assume this innovation, that the innovation in the data domain can be filtered back. So we are very close to the learning approach, in fact. If you assume that this innovation, the discrepancy between the data that you have collected, the observable, and what you have predicted at the level of the data coming from the prediction of your latent variable. Prediction of your Latin variable. If you predict, or if you assume that there is a linear correction that you can correct your Latin variable to make an estimation for the next time step, then you have this kind of equation. You don't know what is capital K at time t, but this is exactly the purpose of the Kernan filter is to find what is the best linear correction to make this improvement of your prediction. So you can So, you can look at what you can do, and this is exactly what Kalman did, in fact, is to compute the variance of your estimator. And so you have just on the top the estimator at time t. This is the estimator of the Latin variable. Again, it is your predictor, the prediction that you have done, plus capital K times the innovation. times the innovation. And you can compute explicitly the variance of this, the variance of the error, sorry, the variance of the error of your estimator at time t. This is the expression that you have in front of you. And the Kalman vector is nothing but the value that will minimize this variance of the error of your estimator. So you will minimize the error of your estimator with this Kalman filter. And the solution is just in front of you. It is capital K times Capital K times you have two terms, and you have a party in particular this term that you have to invert. Notice that you will invert this problem at each time step. At some point, when we look at the DSEM, just the structural model that I spoke about, you will have to face this kind of inversion. You have to face numerically an inversion of a large. A large matrix, in fact. And this here, it will be reduced because, in fact, you will do it a lot of time, but the problem, the size of the problem, will be reduced. So, this is maybe one difference between the Kalman filtering among others and the DSEM. In any case, what is interesting here is that the Kalman filter, what we call the Kalman filter, which we What we call the Kalman Victor, which is in fact this expression of this operator, it is a matrix, in fact, that you have to compute. It is also related to what we call the pseudo-inverse. In fact, this operator that you will use to act on the innovation to modify, to correct the prediction of your latent variables, this operator is close to be the Is close to be the end version of the model, the generative model. GK, and it is easy to check that GK times the Kalman factor is close to the identity. It's not equal to the identity, but it is close to identity. It is a regularized, in fact, pseudo-inverse. But if you permute those two operators, it is not the identity. And if you permute those two operators, it will give you another operator and this new. You another operator, and this new operator will give you the posterior error variance of the error given the previous variance of the error of the predictor. So, you have a relationship now between the variance of your predictor, of the error of the predictor, and the latent variables, and the variance of your estimator of the latent variable. And the Kalman gain will give you this expression, in fact. So, in summary, this is what we have. This is the Kalman filter algorithm. We don't use it like that in neuroscience. It is something, well, I'm speaking about neuroscience, but it can be done, it can be used in many circumstances. It starts to be used in your domain, in fact, given the data collected. In fact, given the data collection of the data, to infer the dynamic of the latent variable, and I think this is the most important point: the emphasis about the dynamic itself of the latent variables. And given the very first time, you initialize your algorithm, you make the prediction from t minus one to t, you make this prediction about your latent variable, you make also the prediction about... You make also the prediction about the variance of the error of this prediction, and everything is known in this vertical here. From this result, you will have two things to compute. The innovation, this is where your observation will come in the framework. And you will compute also the Kalman gain or the Kalman filter. It is an expression by definition, it is a result that we had just before. It is always that we had just before that we can call also the grand matrix. This matrix is, there is, you see, that there is the generative model times the transpose of the generative model. There is a correction with the variance of your generative model. The generative model is the model that produces the data from the latent variables. So there is currently a lot of work. A lot of work to understand or to control or to understand the structure of this gram matrix. There is a lot of work to control the regularity and the inversion of this matrix, because indeed, to compute, and it is what it is written in red, the filter, the Kalman filter, you have to use the inversion of this matrix. So there is many, many ways to control. many ways to to to to control this animation to make like numerically stable the the algorithm and then once you have the gain filter the kalman filter then from the prediction you can update now your prediction to have this estimate of the latent variable at the next time and you have also the variance of the error of your estimator of the latent variables and then you can make a loop and go back at the very beginning and you Back at the very beginning, and you will progress in time by making this prediction, the Kalman gain correction. And each time you will use your observation to make this integration across the time. I'm speaking about integration because it is very close to the fact that you want to integrate the dynamic of your Latin variables. We spoke just before with Eric about the trajectories. Here, you will not find the statistical trajectory from your data, which is. Trajectory from your data, which is of course a very important approach. Here, you will construct the trajectory of your latent variables across the time. Of course, you can do a little more. You can also learn your parameters in this model. If you are not sure that the parameters are regulatable from the beginning, what you can do is to cumulate the likelihood of your model at each time step of this. step of this algorithm. So at each time step, you will compute the correction of the likelihood of your innovation in fact. And then this cumulative likelihood, if you maximize it with respect to the parameter of your model, you will be able to modify the parameters and then you can go back in the process and do it again and again conversion stability. Conversion stability of your estimation, on of course, of your parameter of the model. So there is room to learn the model also in order to stabilize, to have this dynamics of the Latin variable in perspective. So this is essentially what I wanted to describe to you: the fact that the Scalman technique is available and And there is no at some point, people were thinking that the Kalman filters were more general than the DCLM. And people, of course, on the side of the structural modeling was believing that it was more general. Actually, no, today, probably it is more open, it is more a matter of the perspective of what we want to do in these models. So the structural equation models and the kind. The Schrittel equation models and the Kahneman state space models are not in competition. I think they are complementary. It is only two paradigms to handle the dynamics of the individuals and the inter-individual difference. So what are the same, what are they sharing, and what is the difference? Of course, DCM, it is a global, very exhaustive statistical approach for sure. Approach for sure. Simultaneous software relationship among the latent variables. And the previous example was quite eloquent at this point. And the Kalman is more appropriate to capture the intra-individual dynamics. So it is clear that it is a matter to see how much what is the dynamic and to follow the dynamics of the individuals across the time. I would like only to mention that. I would like only to mention that today, and this is an idea also that could percolate in the social science, is to introduce also the notion of graphical model, the graphs. There is already graphs in the perspective, but not in the term of the dynamics of the graph. So, this is a kind of phrase that we have produced a couple of years ago and on which we are working today. Working today, but in terms of the inference, so not only make some inference about the dynamic of the individuals, of course, we can also have the dynamics of the clusters of the group, but also to see how much we can have some dynamics in terms of the graph across the group. And this is something that we have in mind also with Roxanne, with Eric, with Jacques, is to make the Is to make the groups and the dynamics to make to have some interaction. So, here it is interaction that are during time in the brain, during sleep, in fact, and in a very short term. We have one second between the two graphs here and effort for us in this domain, but I think it is also something that can be done in the data that we are looking with Roxanne and in the group. And in the group, also to make the inference in terms of the graph and the in the graphical model, and to have the dynamic of the graph themselves. So, I think that, oops, I'll make a little more, but this is mostly the thing that I wanted to share with you. Again, it is a pity that I'm not in presence with you because it would have been a good opportunity for me. Been a good opportunity for me to collect information and to share with you my perspective in terms of how to solve this, what we call the inverse problem to make this inference about the latent variable. So it is for us an inverse, ill-posed inverse problem. For you, in social science, we don't speak about this kind of inverse problem. It is exactly the same kind of perspective, but with a statistical tool to embrace. With a statistical tool, so embrace all those data in one shot. For us, it is more to follow the dynamics of the Latin other box that I wanted to discuss with you. Thank you so much for that talk in this second speaker.