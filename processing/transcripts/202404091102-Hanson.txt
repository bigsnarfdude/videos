Thank you. Yes, thank you to the organizers for putting on this great workshop for inviting me to give this talk. So yes, I did put the slides in the folder already so that if you want to take notes on the margin of the slides instead of on your own notebook, feel free to do so. Just to tell you a little bit about what my goal is for this talk. So one is I want to talk about how similar TDA and FDA finite dimensional algebras are, and the answer is. On a dimensional algebra, and the answer is you just take this right there. They're very similar. That's the point. They're very similar, right? But we're going to talk a little bit about some differences, some similarities. I'm going to give you an overview of a bunch of stuff without really giving precise theorems. If you want to know precise statements, ask me now, ask me later, ask Thomas now. I know he's organizing, but like the Burr staff is actually organizing because he's got plenty of time. Got plenty of time. I'm kidding. Yeah. Ask the other people in the room who have worked on homological invariance and persistence modules. We can all give you the precise statements of anything you want to work on. Okay. So the plan, I'm going to talk about, Delores gave a very nice talk and talked about homological variants at the very end. I'm going to talk about a slightly different perspective on those, two hominar variants. I'm going to talk about finite and infinite posets. I come from finite dimensional algebra. I come from finite dimensional algebras. Originally, we like finite things all the way. TDA likes infinite posets. So we can actually talk about how can we go back and forth, how can we move results about finite dimensional algebras up to infinite posets. I'll talk a little bit about invariants from post-set embeddings, which is some new work with Claire Meow and Thomas Brusla, and some related stuff on bases and side decompositions. I'm not sure if we'll get to number four, but that's okay. I'll be happy with you. But that's okay. I'll be happy even if we don't have a poster. Okay, so our setting. So P is a post set, any post set for now. When it matters whether P is infinite or finite, I will tell you that I'm assuming P is infinite or finite. Ref P with the lowercase R. It's the category of finitely presentable P persistence modules. Okay? So already we get to the first time in this talk where we have a difference in vocabulary. Because if I tell you what is a finitely presentable Because if I tell you what is a finitely presentable persistence module, I'm going to tell you it's a co-kernel of a map between projective modules. If I ask somebody in TDA, what is a finitely presentable persistence module, they might tell me it is something that has a presentation, a free presentation, by free persistence modules. This is one thing to keep in mind. I come from fine-dimensional algebra, I like the term projective, okay? But people in TDA like to use the word free, the same thing, a free model. To use the word free, the same thing, a free module. Okay? This is fine. It's just two words for the same thing. I mean, that is an important difference because in setting all representations, all projections are for finite. Can you say that's the important thing? That's why the definitions are different, but the definitions are different, but they coincide. Definitions are different, but they coincide over finite places. Sure. Yes. Okay. So S, a subset of P, is going to be an interval if it's convex and connected. Another way you can think about this, it's an intersection of an upset, something closed under going down. Larger elements or postet with P downset. Okay? And I used this math boldface I sub S. Now that I look at it, it looks like row numeral 2 sub s. So I mean the indicator module. To make the indicator module over an interval S. So I take an interval in my postet, but I put a one-dimensional vector space at every point, and every possible map is going to be the identity when I'm inside that interval. In theater module, or interval modules, as they've been called before. Okay, so we've got some special classes of intervals. Okay, so whenever I draw pictures, I'll draw them in R2. So the first is upsets. I just kind of talked about what this is. Upsets. I just kind of talked about what this is. So, what's the defining condition? You take x and u, you take x less than or equal to y, that means y has to be in u. So, here's what a typical picture looks like. This doesn't cut off here, it keeps going on forever. All right, we take some kind of base points, a1, a2, a3. I take everything that's bigger than any of those points. All right, I write it kind of in this funky way, right? So, I write it like a bracket, and then my base points: a1, a2, a3, and infinity, and then. An infinity, and then a backwards bracket. I'm a little bit more motivated later. Whatever, that's just the notation. Okay, another one is the principal upset. Okay, so principal upset, I sub A to infinity, where instead of a set, I just have one thing here, so I don't draw the set brackets. Pick a point, go to everything bigger than it. P is finite, and we look at this as like a finite dimension in algebra language. The finite-dimensional algebra language. This is the module of PA that Sir talked about. This is the free functor at A. If you're thinking about this as functors from a post-set, a vector space category. There's a lot of different things. Okay, the next one is a segment, which in post-set theory, we would maybe call a closed interval. We'll call it a segment. So we have one, everything between A and B, where A and B are related to my postet. One warning. One warning: if we do the indicator module for a segment, I don't know why such ABDEXY there, but I did. This is not generally going to be finitely presentable over an infinite post set. Okay, so these seem like really natural choices of modules, but for finitely presentable modules over infinite postsets, it turns out these are not good at all. Okay, so they will show up, but it's just something to be cautious about. A single source interval, this is something we're not going to define it, I'm just going to kind of explain it. Where I'm not going to define it, I'm just going to kind of explain it. We take an interval where there's kind of a unique smallest point. I call it A, there's maybe a couple points where we've cut it off. So here I use this notation where like I do bracket A, bracket B1, B2. So this is, you can think of it as a difference. You take the upset at A, you take the upset at B1 and B2, and you cut that off. A special class are the hooks. This is kind of where this notation originated, this A, B, where you take two elements, you're present. Where you take two elements here present that are related. You take the upset at A, you cut off the upset at B. If I draw it, I mean, I didn't draw it, but you just have one thing there, still a little stare. So let's kind of look at it. Okay. Great, so let's talk about Jim Homin variance. So I'm going to fix X a set of modules. Usually we just say a set of indecomposable modules because everything is added. Is going out there. I'm not worrying too much about that for now. And so instantly, we get a Dim Hom invariant, I call it. Alright, so this K0 SP of P, this is again something that Dolores introduced. She talked about growth and D groups relative to some exact structure. This is what's called the split growth and D group. Okay, so I'll just write it in. This is the free billion group. Generated by isomorphism classes of indicomposables. Okay, so if we think about like one parameter persistence, the barcode, this is where that barcode lives. It's just like you are like thinking, like, look at the direct sum decomposition. I have this many copies of this indie proposable, this many copies of this new proposal. I can think of that as an element of this group, where the basis are my indecomposable modules. So then an invariant in this sense is just a group homomorphism to some other group. Here it's the Freiburgian group with basis x, my set of modules. And what it is, is the coordinate for a specific module x just records the dimension of the Hom space from x to n. Okay? So I get a nice invariant. I take my m, I. M, I map every possible module in X to it. I get a bunch of numbers recording the dimension of those homes spaces. That's Jim Hum. Okay. Examples. All right, so if I take a point x and p, we have actually like a natural homomorphism here. m of x is isomorphic to, so m of x is like the vector space at vertex x. At vertex X. This is isomorphic to POM of this principal upset module, or this indecomposable projective module, or this free module, whatever you want to call it, to M. Okay? So not only are the dimension of these the same, they're actually naturally isomorphic to the vector spaces. So what this says is, again, we have a change in terminology here. What I grew up calling the dimension vector, what some of you in this room probably grew up calling the Hilbert function, is a Tim Homin variant. Jim Homme variant, and the set that you take for your X to realize it is the indicable projectives, Riemodrums, principal upsets, whatever you want to call it. Okay, number two, Botman, Oppermint, and Noudeau studied the Rank invariant from the perspective of relative homological algebra. And in particular, they showed that the Rank invariant is a DeLam invariant where x is you take these principal upsets and you add to them these books. And so, really, I And so, really, I'm using an abuse of notation here, right? Like, these hooks are intervals, they're sets, they're sub-posets, right? They define a module, so this is the close-corresponding module. And why is that? Well, if I take two related points, the null space of that map, m x to y, is isomorphic to the Hom space from the hook to m. Like, here's your little picture, right? I'm mapping this into my module. I'm mapping this into my module, right? I have what's in between here. I have one dimensional there. So when I want to choose a homomorphism, it's enough to just choose a homomorphism here at the base point. I'd send that to some vector in m of x. The definition of a morphism of representations will determine the rest of the morphism. And in order for it to be well defined, by the time I get to y, I need to be zero. So that's just exactly the end of the multiple. That's just exactly the right thing. So, you can compute the rank and variant this way. Yes? Sorry, this is just a stupid question. Is the interval in the because you've got this interval, i sub x, y is that finally presentable? Yes, this is finally presentable. It's um it's the projective at X modulo the projective at Y. X modulo, the projective at Y. Yeah, do you get others as a finitely presentable? Yeah, like if I go back, this one here would be finitely presentable. It's the projective at A mod the sum of pieces of. Good question. Good question. Okay. So a third one from one of my joint papers with Ben Lachette and Thomas. So we have this exact structure fix that was just introduced in the last talk. Structure of x that was just introduced in the last talk. If it has enough projectives, what does that mean? That means that these approximations exist, these add x approximations. And in addition, every m in rep has a finite x resolution. Then we get what was brought up at the end of this talk, this homological invariant. What it is, is for each module, it gives you a representative of this relative growth indicator coming from the relative protective resolution. You record that element of the group as a homologous. Element of the group is the homological invariant. This is actually like a dim Hom invariant, as long as you have this finiteness assumption, and it's the same x. So when you go to compute this, as long as you have this theory about the exact structure, having projectives, having finite global dimension, when you actually go to compute these homological invariants, you don't need to compute resolutions, you just need to compute Hom spaces. Pete Hom spaces. The rest is kind of theory in the background. Okay, so in particular, as a special case of this, if P is representation finite, so in particular that means finite, or P is totally ordered, then the barcode or the direct sum decomposition, this is again when restricted to finitely presentable only, is a Dim Homin variant or is equivalent to a Dim Homin variant, meaning you can compute one for the other. Compute one for the other. For X, the set of all, and equals. So we have this finiteness, right? We want this, you know, we want enough projectives. We want a finite X resolution. As Laura's pointed out, if P is finite and X is finite, this enough projectives property is just free, right? We don't have to worry, nothing to worry about. Nothing to worry about. So let's assume that for now. P is finite. This slide. Let's look at where do we get this finiteness of this global dimension problem, that all resolutions are finite. Okay, so in our paper, we proved this if every x is the module coming from one of these single source intervals. Not an arbitrary interval, but an interval with like a unique smallest point. Okay? Special cases of this were done by other people, Bought and Oberman dosed co. Bought an opermint dose of cola. Heard about this for hooks. The group from KTH heard about this that says hooks. That should say segments. They can cut up the global dimension for segments. There's some older work by Chino Shi called representation dimension that also considered segments. So there were special cases of this known elsewhere. We also then considered Ezra Miller also. He has Miller also in a slightly different setting. He deals with modules that aren't necessarily finitely presented, but a lot of his proofs tapped it right over that if every X, if every module in our set is an upset module, then we also get this finite S. Okay. And then after we put our papers out, we couldn't do it like with all our proof didn't work for all integral modules, but does this one? All interval modules, but does this one or not? And Emerson and co-authors, Shiva Nakashima, Yoshiwaki, proved that yes, if you take all indicator modules, all integral modules, that also gives you this finite global dimension. They used the theory of quasi-paradetary algebra. It's a very, very, very meaningful argument. Yeah, Lucia. Just as a comment, Esther is not dealing with relative homological algebra. He resolves by absence, but not in the sense. Results by absence, but not in the sense that dollars is bank. That's correct, yes. You can deduce what we need, which is actually about asymplicity of the Haume relation from his results, which is why his name is here. Okay, that's interesting. Thank you. And another comment is in the work by Bartner and Amber, Manuel Dor and myself, we are dealing with an infinite positive. That's the interest of it. It kind of follows from what you're doing. Okay, good point. Yes, you're right. Okay, so I'm signing. You're right. There it's okay. So I'm citing a result that's more general. It deals with infinite process. Thank you. Plus infinite. Good. Okay, now let's talk about infinite post sets since Luis just insists. Okay, we like infinite post sets, right? Like R is our indexing set. Okay. Like, R is our indexing set. Okay, so here's the problem. All right, and Dolores mentioned this very briefly: infinite post-sets do not have Oslander writing theory. Okay, it's worse than that. Like, the injectives aren't even finitely presentable over an infinite postset in general. Okay, so Ocean-Wright in theory, if you've never seen it, it is like the most beautiful, powerful tool in finite-dimensional algebra that you can possibly imagine. Like, I don't know what to do. Let's use Ocean to Wright in Theory. That's like a very good instinct if you're trying to solve a problem. Instinct, if you're trying to solve a problem in finite-dimensional, all right? It deals with these almost like sequences, right? They're built out of a duality between projectives and the injectives. If you don't have that duality, you have no chance of developing out-sender rating theory. I mean, there are settings in which you can get almost what sequences still, but not the same robust theory that we're used to. Sorry, because, but there is another way to put it in that there is an almost always if you have a You always have a multi-screen symbol ending at a module with your current morphism. Yes, excellent. So I am underselling it a little bit. Yes, it is more general than just finite dimensional algebras. Thank you. It does not work when you do representations of all of R, for example. You can find all these exceptions. Rating a little bit. Okay, so many of these classical results, like this beautiful formula we had in the last talk, that the relative projectives of this exact structure f of x is x union the regular projectives. That uses outstanding writing theory to prove that. The paper that's usually sending. So if we want this, over an infinite post set, we have to have a different argument. We can't use the We can't use the classifier. Alright, so for now, let's go back to a different setting. So P is going to be Rn, n greater than 1. I don't want to be totally ordered. Q is a finite n-dimensional grid that I'm thinking of as sitting inside. So here I just drew like a 3x3 grid in orange sitting inside of R2. Alright, I'm going to think of things I call F mapping Q to P, which I'm calling in a line. Mapping Q to P, which I'm calling an aligned grid inclusion. Writing down the precise definition, I think, is more tedious than looking at the picture, right? It's a grid that I stuck in R2, right? Axes go to axes, spacing is even, et cetera, et cetera. There is a precise definition of why. Alright, so we get a bunch of functors from this. We get, and I'm going to talk about these. So some of them have come up. We have the restriction functor. Functor, which we'll talk about. We have the induction functor, which is also like a left con extension type construction. We have an extra one that's kind of weird called the contraction functor, but I'll define it in a few slides. So I'm going to tell you about these functors. There are adjoint relationships between them, which is very important. Okay, so let's start with restriction. Okay. Erica have a question. Sorry. So they they are uh young fairs. Young pairs? Or are young pairs? Yes. Um, I'll get to that. Yes, yes. There are some injunctions here, and I will get to those. Okay, so let's talk about restriction first, okay? So I'm taking M in rep P. The restriction, you can think of it as like a precomposition, right? So, like, what is it? You know, you go over a smaller postet and you just move it up to your big post set, plug it in. Here's an example. Plug it in. Here's an example. Alright, I take this integral module in purple in R2. The post-it I'm restricting to, I just ignored commas, so like 0, 0, 0, 0. It's just these four points. So if I restrict, like the restriction name is telling you what you need to know, right? You just look like if it's, and this should, this is actually just taking, sorry, it should not have D in N. Great, so if I look here at my points and my subposet, my module in purple is supported at B and C. So we should have a direct sub Okay, so when I restrict, I have one-dimensional vector space here, one-dimensional vector space here, those points are going to be and c. I have a zero-dimensional vector space there because I've cut off. Vector space there because I'm cut off along the line x equals 2 and a zero-dimensional vector space here. Alright, so when I restrict this particular integral module, I get the direct sum of two integral modules. Okay, so that same thing is in this corner. So Just to remind you what the restriction was. All right, contraction. This one's weird. Okay, so it's another way to go from modules over my big post set, my infinite one, to modules over my finite postset. And so the problem is you have to restrict your family of modules. So I said now I'm starting with the representation of P. So I said, no, I'm starting with the representation of p, f plus. Alright, so what is f plus? It means if a vector space is non-zero, then I have to have a lower bound in my embedded postset. So if m of x is non-zero, there exists some y in q such that f of y is less than or equal to x. Okay, so here I've included q as these four points. So this postet is fine because 0, 0 is less than 0. Is less than or equal to every point at which this module is supported. So it satisfies this. It's in M, it's in rep of p to F plus. But if I slid this module over, like let's say I moved it just over one unit, then I have a non-zero vector space at like negative 1, 2. And that has no lower bounds in the image of Q, because that would not be in rep of P, F plus. Okay? So now Okay? So now this construction, it's a co-limit construction. So I take Z and Q, my sub-poset, and what I do is my contraction along F of M at the vertex Z is I take the co-limit of all points in my postet, where F of Z is the greatest lower bound when I look at the image of Q. Alright, so let's look at that. Here's our example, here's our module. So for 0, 0. For 0, 0, 0, 0 is the great, uh, the greatest lower bound of everything that's like in this square minus the boundary of that square. All right, so I'm looking at the co-limit of everything that's like here. So, even though there's no vector space at 0, 0, right, it's a zero vector space in my original module, this co-limit, it's pushing me towards the outside of what's happening. So I'm going to pick up a vector space at A because of the points here that have a non-zero vector space. Here that have a non-zero vector space at non-vector. All right? Then we get like the same, like the reverse type of weird behavior, like two, zeros. So two, zero is the greatest lower bound of everything in like this infinite rectangle here. So even though there's a vector space at two zeros, the restriction would give me a vector space at two, zero. In the colimit, I end up up here, where the vector space has gone away. So there's nothing in that colimititimit. So there's nothing in that colon. The collimate is going to be zero. So when I actually, and then like at zero, two, it's everything in this infinite rectangle, which the module is supported everywhere there, the colon will be keg. 2, 2, there's no vector space there or anywhere past there. So I get supported only at A and C. So it's weird. I've moved the support from here to here, kind of, in some sense. It's a weird functor. It's a weird functor. Takes some getting used. For this to even be well-defined, you need this aligned grid inclusion property. Otherwise, these co-limits aren't guaranteed to have finite. Of course, you could construct an infinite co limit, but the finite co-limits are not guaranteed to exist without some assumptions. Okay? And so this is just to highlight, like, I do get something different than the restriction, right? The restriction was the two segments, the direct sum. This one's a Their direct sun. This one's a single segment of lithium. All right, one more functor: the induction. The fastest definition of induction is that you preserve the projective presentation. You can also compute this using floor functions. So if I start, now I'm over my small postet. So I take like I sub AC, right? This is a segment. I wrote it as a segment because to me this is familiar, this is safe. I like writing like, oh, it's the points between A and C. Remember, like back before I said, when you go to the infinite post set, the segments aren't finitely presentable anymore. So I actually don't want to think about this as the segment from A to C. I want to think about this as the hook, where I took the upset at A and A and I cut out the upset at B. So I started with everything and I cut out this stuff. That gives me this notation. And then from there, that tells me I'm going to take the projective at A, mod the projective at B. I now apply A, projective at f of A, mod the projective at F of B. So this becomes the hook from 0, 0 to 2, 0, which is kind of a degeneric hook, it's an infinite rectangle, right? But I took. Rectangle right, but I took everything above super zero and cut out everything above super zero. So the same projective presentation. Okay. So, fact, we have some adjunctions, all right? What does this mean? If I take hom over q of u to the restriction of m, that's the same as hom over p of the induction of u to m. If I have hom over q of the contraction of m to u, that's the same. Q of the contraction of m to u, that's the same as hom of m to the induction of u, but this only works if m is in this subcategory rep of p, f plus. So one of my functors, the domain, is not all of rep, it's rep of p, f plus. When I set that up, I still get an extra junction. And in fact, every m in rep is of the form into f of u for some f. Okay, so everything comes from applying an induction function. Applying an induction function. And this is like kind of like one of the things that I like to say about finitely presentable persistence modules, right? They're all coming from finite postets. You're taking a module over a finite postet and you're moving it forward by induction. This is a tool that'll let us do a lot of things. So, why do we care? Okay, so this is in one of my papers with Thomas and Ben. So, if we take some. And then. So, if we take some set x with good properties, I don't have time to go into those. So, given in m in refp, we can use the functors to build a candidate of an x approximation or fx pre-cover, whatever you want to call it. We get a nice candidate. This candidate is the induction of something, or that it's coming over some finite poset. And we can guarantee this factorization property when we are restricting. When we are restricting, I put Q plus here, I should have put F plus. When we restrict ourselves to rep of P F plus. So we're trying to approximate by X. We've gotten an approximation that works if we restrict to F plus. But if we leave F plus, all guarantees are out the window. We cannot guarantee the representations that aren't in this replica P F plus 3 F factor. So examples from this. So, examples where this isn't a problem, if we take x instead of all principle, principal, shouldn't say principle upset, sorry, the standard projectors, this isn't a problem. If we take x, the set of all hooks, if we take x the set of all single source intervals, this is not a problem. Examples where it is. In fact, at least for x equals upsets, we have an explicit example where fx does not have enough projection. Example where fx does not have enough projective, so you don't get an approximation by an upset module. We don't have that worked out. That's really for this one. For intervals, the proof fails for the same reason, but we don't have the explicit construction worked out. So for upsets and intervals, you can get things that aren't in this replica F plus that break the fact that your candidate approximation is actually approximating. And it gets yet worse. If we replace Rn with zero to infinity to the N. To infinity to the n closed at zero, the problem goes away. So, to me, morally, representations of Rn and representations of the positive orthant should not be different things. But the existence of this unique base point completely changes the homological behavior in terms of the existence of these approximations. Yes. I want to clarify what this is the problem with. So, the problem, yeah. Um, so the problem, yeah, so I can use this sketch here. Yeah, so you can't build. So, here's my uh I'm doing a resolution by upsets here. I'm trying to do an upset cover. So, here is my persistence module, it's this rectangle. All right, I can find upsets that surject onto it. Here's one, this orange one. Ooh, this orange one that I drew here. This is an upset module that surjects onto the purple thing, but I can. But I can take this point here and I can move it as long as I don't pass this green dotted line. I can keep moving it down and to the left. And I can go actually onto the green line. So left is fine. I can only go so far left. But then I can keep moving it down. And as I move down, the order of factoring is that the higher endpoint one opposes to the lower endpoint one. So in order to actually get an approximation, I have to. So in order to actually get an approximation, I have to send this endpoint all the way to minus infinity, which I can't do while maintaining finitely presented, unless I am here, in which case I'm rescued by this minimum element that I send this point just to zero, where I'm cut off, and now, okay, I'm still kind of presented. Yeah, Louis. If you add minus infinity to R, that's also fine. If you add minus infinity to R. Also, fine. If you add minus infinity to R, that's the same as this. Right. Yeah, so that would be fine. Sure, yeah, I got that. But there was something about contractions in the previous. I thought that was really the problem or a solution for it. Oh, no, so the contractions, you can play with adjointness. And so what you do is you start upstairs over a big post set. You start upstairs over a big post set, you restrict to a sufficiently large, finite postset, you build a cover there, you build a cover there, you use induction to go back up. Now, when you want to test your factoring, the way the proof goes is you take something in add x and you apply contraction. The problem is you can only apply contraction if the thing in add x is in this rep of p, f plus. So, what saves you in So, what saves you in these three cases is that things that aren't in rep of p, f plus just don't map to m. There are no maps. So you say, well, thankfully there are no maps, I don't have to worry about these things. And then you can use contraction and a junction to show you have an approximation. But in these two cases, you cannot rule out maps from things that aren't in this F plus, so then you can't use the contraction functor. Okay, so that's all I want to say about infinite postsets. All right, now I'm going to go to finite world for a little bit. All right, so P is going to be a finite postet. F mapping Q to P is going to be a postset embedding. What this means is it's not just that, I mean, you could think of it as a full sub-postet. This is what it was called yesterday, Everson's presentation. So x is less than or equal to y, and q, if only if f of x. To y and q, if you only have f of x, it's less than or equal to f of y and q. Or in other words, take a subset of p, you have an induced post-set structure there, this is what that is. Recall this adjunction, or hom of u to the restriction of m, is the same as hom of the deduction of u to m. Okay? So, from this, we can do two constructions. This is what I did with Clarion and Thomas Brussela. So, first, we fix Q representation finite. Q representation finite. We fix E, a subset of the embeddings from Q to P. So we might have multiple ways to embed Q into P. The easy example is take Q to be the postet with one element. Then I can choose any element of P, this gives me a different embedding. I, the set of all indicosables of Q's. That's why we're assuming Q is representation finite. We'll be finally many indecomposables. Then we can define two invariants. The values are in the Freight Building group generated by each. Are in the free billing group generated by E cross I and they're equivalent. So one is this restricted multiplicity invariant. So we take F in embedding, we take U, a module of Q, and we say we record the multiplicity of that U is a direct sum end of M when we restrict it. All right, when we take like U to be like a very specific model. Like a very specific module, like the say more about this on the next slide. We get like something that Emerson was talking about yesterday, where we restrict to a subhost, we look at the multiplicity of some module, and this gives us things related to the generalized ranking variable. Here we're looking at all indecomposables. The reason we want to look at all indecomposables is because we can also look at the dimension of the Hom space from u to the restriction of To the restriction of M, which using a junction is the same as the Hom space from the induction of U to M, which means it's a Dim Homme variant, right? This tells me it's a Dim Homme variant. I'm recording Homs of something to M. And these two are actually equivalent. So in other words, we can look at our pro set Q, we look at all ways to restrict to things that are isomorphic to Q. We look at direct sum decompositions there. This is required. There. This is recordable as an ATM homogeneous. Okay. So for some, this top part is just on the street slides. So here, the new thing here is the examples. Take Q to be one point, E to be the set of all embeddings. We get the dimension vector, right? Look at a way to embed a point that should be scrapped in vector space at that point, the direction of decomposition. At that point, the direction of decomposition is the dimension of the vector space. Take Q to be a two-element postset, E to be all bettings, we get the rank invariant. I'm not going to explain exactly what's going on, but the proof there is not too bad. Interestingly, going to higher linearly ordered things doesn't buy you more. Like doing A to B to C means you do more computation, but get no more information. You do more computation but get no more information, which is always fun. Do you get the ranking or more nulls? Oh, this is a good question. So the question was, do you get the ranking variant or do you get the null spacing variant? I guess the null spacing variant, if you're talking about like the specific number. To me, I consider those invariants the same since they carry the same information. But you're right, like you'd have to do a computation to switch this to the ranking variant as. Switch this to the ranking variant as defined. Yeah, so I was confused about the same point. I mean, the multiplicity of AB is the number of variables there, which isn't the same. They're equivalent, right? You can compute one for the other. One and two are equivalent, but maybe not the same. One and two are not equivalent. Because this is just. Sorry, on the previous slide, I mean, you had the two things. Oh, here. Yeah, these are equivalent. Yeah, these are equivalent. Yeah, right. It depends on what you mean by same, right? There's an equivalence relation on invariance, which is if invariant A can determine these two things, then so can invariant B and vice versa. They're the same under that equivalence relation. So it really depends. But if you mean like it's a list of numbers that you store as a vector computer, then vector vector. Yes. Feel free to tell me that this is a technicality that you don't want to use. This is a technicality that you don't want to deal with. This is a technicality that shouldn't be allowed to be infinity in 2 to actually recover the ranking variant? Because you also need the point-wise dimension. Oh, oh, so you get the point with dimension. So you're not only look... This is a very good question, actually. So the question is, should I allow B to be infinity? So no, because you're not only looking at the module that's like dimension, non-dimensional. module that's like dimension one dimension one of the identity you're looking at all modules so you can get the dimension vector from looking at like just the simple module that's one dimensional year zero there if you're not looking at all modules then yes something goes wrong yeah thanks okay these are good questions thank you okay so then we do a second construction all right because it turns out like as this kind of indicates the fact that we add a third vertex and we get a bunch more complicated Third vertex, and we get a bunch more computation. We go from three indecomposables here to six here. So we have twice as much data that tells us no more information. Like even here, we have a bunch of data that's not telling us more information. This like as a like vector, like I'm recording this as an element of a Freebeling group. This is a bigger Free Belium group than the rank invariant needs. So I have extra information sitting somewhere. So, one way to get around that is we said, all right, let's now take a bunch of postets. And a bunch of sets, right? So, Q1 to Qn are postets. E1 to En is each one a set of embeddings from QI to P. P is the union. Then we get this, we called it mult-F. So for each F, it's in some EI, and then I record the multiplicity of this interval module. Of this interval module supported on the entire postset QI as a direct sum end of the restriction. So I'm taking one special indecomposable over QI now and looking at its multiplicity as a direct sum end. Examples, generalized rank invariant that we heard about yesterday, the compressed multiplicities that we heard about yesterday. If you don't remember what those are, we don't have to go into them. It's just that. Are we don't have to go into them? It's just that just trying to say, like, these are things that have shown up before. Okay? So, for the last part, I'm going to talk a little bit about what this gets us in terms of bases and sine decompositions. Alright, so here's a more technical definition. Alright, so we take an invariant. It's a finite invariant. So, n is a finite number. That's what I mean by that. A basis is a set B of indecomposable representations. Of indecomposable representations such that this invariant yields an isomorphism. Here's the image of C, so you look at all possible outputs of the invariant. Here, this is the subgroup of this split growth in E group generated by the modules in V. Right? What this is saying, if we go to the second one, we're talking about these like signs in an approximation. Given a basis. Given a basis, you take a representation, you have these unique integers, alpha u for u and v. And what you can do is you can you take u to the alpha u, you collect your positive terms here, you collect your negative terms here, right? So we're looking at what, so this is the module, right? And you can express your invariant at m as a sum of the invariant of something in add of. Something in add of your basis, add V minus something else in add of your basis. Alright, so we call this the minimal C decomposition with respect to the basis B. So examples. This is a very classical result. We have the dimension vector slash Elbert function. It has the standard projectives is one basis. This is because finite cosets have finite global dimensions. And that's the simple. Finite quantal dimension. And that's the simples is a basis, right? The dimension vector, like the basis of the simples, is just like the multiplicity of i sub xx is the dimension at x. This gives me two bases in which these are bases for the classical growth of the group. That's really what they are. All right, the paper by Bond and Opering Redo. The Rake invariant has the hooks plus the projectives as one basis. This comes from the relative homological algebra perspective. We also have this beautiful result about the sign. This beautiful result about the signed barcodes, which in my language is that the segments are basis. They have a lot of good stuff on infinite post-literation, I'm restricting it with finite for now, just to make things easy. So the segments, everything is, the rank invariant of any module can be expressed as the difference between the rank invariant of two things that are direct sums of segments. This is not directly coming from relative homological algebra because the projective Algebra because the projectives for the rank invariant are the hooks, not the segments. So that's a different result, it's a different basis. And this felt like magic to me the first time I read the paper. Like, I understand every step of the proof. It's a very well-written proof. I'm staring straight at Steve right now. The proof of his uncomfortable is possible. But I was like, I get the proof. I understand why it's true. But this is really surprising to me. It felt like kind of matter. There. So we proved something. Still feels kind of like magic, but now it feels like magic with a proof that uses homological algebra. So what we did, we took F, this family of postets and sets of embeddings. We assume that each of the QI has no chain of length greater than or equal to 3. The reason is essentially because of this, like, the 3-element chain being the same as the 2-element chain. Like, you get all this redundant information. Information. Then there's R here for rectangles because, like, rectangles are another name for segments. But we say you take the indicator modules where you take what's called the convex hole of the image, right? So if Q is embedding as an interval, that's just Q, or the image of Q, if it's embedding not as an interval. This is, I think, my last slide. You take all things in between. So here's the formula. Great. So here's the formula. It's a nice paper by Yeoki Desuga Hitata that explains where this comes from. It actually comes from looking at the induction functor and the dual and co-induction functor simultaneously. You get a nice basis, okay? We have a dual result, or not a dual, a complementary result. If you take one representation, finite postet A B, a set of embeddings, such that P is covered by these embeddings, that's what this says. Everything is in the image of some embedding. Image of some embedding. Plus, some technical stuff I didn't write down, then H, H stands for hooks. The induction functor applied to your indecomposables is giving you another basis for this DH. What I don't have here is the corollary, which is that I could take Q to be the two-element poset, and you take here Q1 to be the one-element coset, Q2 to be the two-element posset. To be the two-element closed set, you can make these invariants equivalent, in which case this is the segment modules, the rectangle modules, this is the hook modules. Okay? So it does extend this result about an operative bill. Okay. So that's where I'm going to end. Thank you very much. Thank you, thank you very much. Very much. Are there your questions? Does every basis give you a dimensional homogeneity? Does every choice of indie composable modules will give you a Dim Homo invariant with that set as a basis. But in general, if I take an existing If I take an existing invariant and find a basis, it will not necessarily be a DIM HOME invariant with that basis. For example, the rank invariant is not equivalent to the DIM HOME invariant with the segment modules being your family. That gives you a different invariant. You want me to call on people? So, you mentioned this lack of projective covers for upsets and for intervals, and I know. And for intervals. And I know that we discussed this, but now I feel that for intervals there are enough projected covers because of what Emerson said: that the cover, the interval cover, the summons are always sub-modules of the module that you're resolving. So that should mean that when you enlarge the posit, the cover doesn't change. Yeah, that's a good question. Statement. My question is whether it's true. Sorry. It was a joke. Yeah, you're right. So I might need to be way more careful on that slide. So certainly the upsets do not have the projective covers. The intervals might. Yeah, they might still. That would be a good argument to look at. Tunes the existence of the system. Right, right, but that's a very important thing. But yeah, I feel like it should be okay. Would be cool if it were true. Yeah, I mean, that would be nice if that exact structure has enough projections, even if the contraction functor is not the way to see it. And for the questions, let's thank everything. And before you run away, I wanted to talk about the structure of this afternoon very quickly.