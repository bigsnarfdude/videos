So I would like to share our recent work on sharp openity for testing covariance and precision matrices. So this work is collaborated with my advisor, Pishi Daddy, Sung Chi Cheng, and Bingu and Shui. I'm an academic brother and sister. Okay, so the outline of the talk is that we will talk about what is a sharp. Talk about what is a sharp mid-max result for common testing and then a multi-level thresholding test. So, this is very related to basically motivated from the high-quisition test by Donald and Jean in their 2004 paper. And we demonstrated this is more powerful than both L2 and L maximum, maximum type test for detecting fast and weak signal. Detecting fast and weak signals. And we also discuss testing cooperative for precision matrices. Okay, so first of all, some background. So we will talk about in this talk about signal detection, basically whether a block of covariance metrics is zero. So this is basically the global noun that Professor Ling talked about. So this is different from signal identification. So signal identification basically try to identify where are the signals. Where are the signals? So, this is related to multiple testing problems. So, both signal detection and signal identification has many applications. So, signal detection can be considered as the first step. So, if the non is rejected, then we move on to identify where are the signals. So, it's related to applications in gene expression network, brain connectivity, or omics data analysis. Or omics data analysis, where we are given one type of variable. For example, we given a microbiome and study the gene expression network. So that relates to conditional association. Okay, so let's first talk about a very simple hypertesting problem. So this is one sample covariant testing. We just want to test whether the covariance is a diagonal. So basically, all the variable Basically, all the variables uncorrelated under the noun. Okay, so this procedure can be extended to testing sub-blocks of the covariance or testing two covariance being the same. Okay, so let's review two existing minimax results. One is by Professor Tai and Ma in their twenty thirteen paper. 2013 paper. So they consider testing covariance being identity under the Gaussian distribution. So right now we all talk about under Gaussian distribution because we want to derive the minimax result. So they consider this alternative covariance class. Okay, so this basically says that the distance under of the sigma of the covariance matrix and the identity And the identity is measured by the Fulbright norm, and the Focus 0 is larger than rate P over N, square root of P over N, times the constant B. This shows that, so minimize over this alternative class, U1, at a given B0, so the maximum power, so this is the power maximized over all alpha-level tests. Okay, so this is the maximum meaning. So this is a maximum mini maximum minimum power, max mini power. So if we take one minus, this is a mini max type 2 error. So this power will not reach 1 as mt goes to infinity for a given b naught. So if b naught is small, which means that the distance between sigma, the covariance and the identity is small. So then we cannot separate. Okay? So this is referred to as the minimax read optimal result. Minimax read optimal result because the expression of the constant B now is unknown. Okay, so the other result is also by Professor Tsai and his collaborators. So they consider, so this hypothesis slightly different. It's a two-sample hypothesis, but the technical is the same. Okay, so they consider the sparse alternative where the number of unequal Where the number of unequal covariance is modeled by p to the power kappa. Kappa is constant smaller than 1 over 2. Okay, so it's specified the number of unequal covariances under the alternative hypothesis and then use the Fubi's norm to describe the distance between the two covariances. Okay, so this MA is basically the number of unequal covariances. The number of unequal covariances. Okay, so basically it says that there are MA unequal covariances and the strength of the signal is at the order of square root of log p over n. So they show that there exists, say now, exists a constant if the say is small, so the distance is small between the two covariance. Then the maximum meaning power will not reach one. Again, this is the rate optimal result where the constant. Result where the constant is unknown. So this is what we kind of summarize for the existing results. So they are a medium X rate optimal result, but the tight bound on the mediumax power is unknown. And on the other hand, if we look at the least favorable prior under these two cases, so the Cases. So the time the mass paper considered a non-spass regime where the number of signals is at order P. Okay, so MA should be at order. So if we think about the number of three parameters in a covariance is at order P square. Okay, so now the signal is at order P, so if P over P square, so this related to the dense signal regime. Okay, and the Tai and Niu and Sha considered the Liu and Xia considered a highly sparse region where the number of signals is less than a square root of p. Okay, so the moderate sparse region is unknown, the result. Okay, so before we talk about our mean-max result, let's first review the detection boundary discussed by Donald Hohen and Gene 2004. So this detection boundary is actually first shown by Ingister in his By Ingister in his 1997 paper, and then Donald Hander Jean, they showed that high-present tests can achieve this detection boundary for test mean. Okay, so they considered the problem for test mean, and the non-zero signal is denoted by mu A is at the order of square root of 2r log P over N, and the proportion of signal is P to the power minus theta. So the Minus beta. So the beta is a specific parameter. The larger the beta is the smaller, the fewer are the signal. And then the smaller the R, so the smaller are the signal strengths. Okay, so the hypothesis, this is kind of a basing risk type of hypothesis. We want to test the non-hypothesis mu or equal to zero. And the alternative is that there is one. Is that there is a one minus epsilon proportion of the components without signal? So nu zero means this is a point mass distribution at zero. So basically meaning to zero. And then epsilon proportion, epsilon equal to p to the power minus beta, proportion of signal equal to mu a and the mu a is this signal strength. Okay, so under this setting, the detection boundary The detection boundary for this testing program is given like this. So it has two range, so separated by three or four. So if the signal strengths are smaller than the dating boundary, then the type 1, type 2 arrow will convert to 1 for any test, which means that no test can separate the non-authority hypothesis. If the signal strength is. If the signal strength is above this detained boundary, then there exists some test, basically the high-criting test can achieve this detecting boundary such that it's a type 1, type 2 error convert to zero. Okay, so this is the visualization of the detected boundary. So beta minus one over two is for the range 0.5 to 3 over 4. And then from 3 over 4 to 1, it's To 1 is the square root of 1 minus beta. And this is a detained boundary to separate the signal space. And the similar detailing boundary also exists for classification problem that Professor Fan has studied and Tricey also studied this problem out. Okay, so let's come to our question. Okay, so let's come to uh our uh current testing. Um so we want to mimic uh the testing for mean uh setting. Um so uh we are so we first uh define Q equal to P times P minus one over two, so basically the number of three parameters in the off-diagonal covariance, okay, and then Q to the power one minus beta is a number of non-zero covariance on the off-diagonal for beta from R1 over two R21. So this So this range corresponding to the sparse signal region. And then the non-zero covariance is equal to square root of 2 times RJ1J2 times log Q over N. So this is basically similar to the mean setting. And then we consider this alternative covariance class, which says that there are Q to the power 1 minus beta non-zero covariances and with strengths given by this. And we have a lower bound R null for R G 1 G2. R naught for RJ1G2, basically R naught is the smallest signal strength. Additionally, we have a tau parameter to upper bound the variance on the diagonal. And then the last part is the kind of technical assumption which prevents the variance go to zero. Okay? So this is our covariance class which basically specify Class which basically specify the minimum signal strengths and the number of signals. And we can show that, so this detection boundary for mean is still the detection boundary for testing covariance. So if R now over tau square, so this is the standardized signal strength. If this standardized signal strength is smaller than the detection boundary, so then So then the maximum power, the minimum power over this commerce class be defined given R naught and the tau and the maximize over all alpha tests, this maximum minimum power will not convert to one. Okay. So compared to the existing results, we derive the detecting boundary, which basically Boundary, which basically tells us what is the minimum signal strength we can separate the non-adultive hypothesis. So we're just talking about the detection lower bound and then we will talk about the upper bound. And if the lower bound and upper bound match, then we say that the non-analteric hypothesis can be separated. And this detection boundary is a tight detection boundary. Okay. Okay. Okay, so this is one page to briefly introduce the proof. So the key step in the proof is to specify the least favorable prior under the alternative hypothesis. Okay, so because our alternative says that there are non-zero covariance on the off-diagonal, so we just put this non-zero cognitive So we just put this non-zero covariance in the first of the diagonal. Okay, so this basically is a two by two block diagonal structure. And then we shuffle this, shuffle the row and the coordinates of this structure to get kind of randomized signal location. And then we need to find the energy distance of the density of the data under the none and under the list. And under the least favorable prior. Okay, so we talk about the detection normal bounds, detection normal boundaries, whether we can achieve this detection boundary. Okay, so this basically is kind of a variation of a high-precision test. Okay, so we build the test based on sample covariance and we get the standard. Covariance and we get the standardized sample covariance if under the normal distribution assumption. So, this standardized sample covariance is basically the sample correlation. If it's under non-Gaussian data, so then we need to find this theta height, which estimates the variance of the sample covariance. So, the idea is basically I need to doing a kind of thresholding. Doing a kind of thresholding or a regularization on the statistics. So we just summation those large standardized sample correlation. So MJ1J2 basically square of the sample correlation. So we summarize, we sum those squared sample correlation for those larger than a threshold uh named S. And named S is at order uh Q S dog P uh for some thresholding parameter S. For some thresholding parameter S. So comparing with the existing result, so the maximum statistics basically take the maximum of the standardized statistics. And the L2 statistic basically the summation of all the standardized statistics over all components. So maximum test is powerful if the maximum signal is very strong. And the L2 test is powerful. And the L2 test is powerful if there are many, many signals, okay, for basically the dense signal case. But they will lose power if the signal are both as fast and weak. So we will show that later. So this slide basically derives the mean and the variance of our thresholding statistics under the null hypothesis. So the mu now 2 times sigma 2. Now, Tilta and the sigma tilde is the mean order of the mean and the variance, which has a close form expression. We can show that so the standardized thresholding statistics minus its mean under the non-hypothesis and the standardized by its deviation is asymptotical normal. Okay, so there are a little bit of additional constraints that we Additional constraint that we need to pay attention. So basically, this constraint, this condition is on the threshold level S. Okay, so for testing for mean, if under the Gaussian data, so the S can be any value between 0 and 1. But here, so if the P is growing at expression rate of N, then we need to choose the threshold level larger than 0.5. If N is growth at a If n is growth at a polynomial rate, sorry, if p is growing at a polynomial rate of n, then the threshold can be choose slightly smaller than 0.5. Okay, it's kind of anything larger than 1 over 2 minus casi over 4. Casi is basically measure the growth rate. Okay, so the intuition of why we need a threshold lower bound is because for test. Because for testing for mean, most of the work is focused on a very ideal dependence structure among the variables. But here for sample covariance, so we know that the sample covariance, MJ1, J2, and MJ1J3, as long as they share a common index, they are dependent because they are using the same variable. Okay, so it's a kind of a circular dependence. Of a circular dependent structure for our thresholding statistics. So we need larger threshold level to help to reduce kind of the dependence among those components. So this threshold NOR bounds will also affect the detection boundary of our test. Okay, so then based on this asymptotic normal. Then, based on this asymptotic normality result, we can build this single-level thresholding test, which basically rejects a none if the thresholding statistics is larger than its mean under the none, plus the upper alpha quantile of normal times the standard deviation. Okay, but how to choose the threshold S? So, this threshold S is a hyperparameter. So, as a way to choose this, So, the way to choose this is basically by the high criteria. Okay, so we get the p-value for a given S. Okay, so this is the p-value of a single level threshold in test. And then we just choose the minimum p-value. So, this is the most significant threshold. So, this is equivalent to consider the maximize of our standardized thresholding statistics over the threshold range from S now to 1. From S naught to one. And the choice of S naught is basically determined by the growth rate of PO with respect to sample size. Okay, so we can show that this mati threshold statistics is the gamble. So that's why Professor Lin in her talk says that the hydrogen type statistics converge slowly because it converges to Because it converts to extreme type distribution. Okay. But in our simulation, we found out for testing a covariance being diagonal, so the size actually is okay. Okay, so the multi-level solution loading test has this rejection criteria based on the gumbo distribution. Okay, so next. Okay, so let's look at the power of our thresholding statistics. So we can show that this db star, which determined by both beta and casi, casi is basically the measures of dimensionality, have this form. So the optimal determinant boundary is only the second and third line of this function. Okay, so the first line is actually cannot achieve this determination. Cannot achieve this detailing boundary beta minus 0.5. It's actually a little bit higher, which I will show by a graph later. Okay, so this part is basically caused by the threshold level cannot be too small. Okay, so we show that if so this RJ1J2 over sigma J1J2 sigma sorry RJ1J2 over sigma J1J1 sigma J2J2 is a standardized signal strength at the component J1J2. At the component J1, J2. Okay? So we take the maximum of this standardized signal strength. Okay, yeah, sorry. Yeah, if this maximum of standardized signal strengths is smaller than the stating boundary, then we can show that the power of our test will converge to zero. On the other hand, if the minimum signal strength, standard signal strength is above this dating boundary, we can show that our test Our test, the power of the test will cover to one. Okay. Okay, so this is basically the graph that I just mentioned. Okay, so this dashed red line is beta minus 0.5. We can see that at this the far left end, so it's Left end, so it's the different boundary of our stretch coding test is slightly larger. So this three line, the blue, green, and the black line is corresponding to different cases. If casi is closer to two, which means that the dimension, the p is at the order of n square, then this, our detecting boundary is become closer and closer to the optimal, closer and closer to the detection lower bound. Bound. Okay, so on the other hand, the maximum test has power compared to one if this maximum standardized signal strength is larger than four. And the L2 test is powerless if the beta is larger than 0.5. That's our signal strength. Okay, so that is the reason why the proposed test is more powerful than the maximum and AO2 test under this regime. Region. Okay, so this theorem basically summarizes our detective boundary, tight detection boundary results. Okay, so if it's exponential growth rate between P and N, so then if beta between 5 over 8 and 1, our test can achieve the detection boundary. So if the polynomial grows and the beta is between five. And the beta is between 5 over 8 over 1 over 6, sorry, cosi over 16 and 1, then our thresholding statistics can achieve the optimal detection boundary. So if cosi equal to 2, so then this 5 over 8 minus casi over 16 is basically 0.5. And then this basically matches the detection normal bound. Okay, I'll very quickly go through. Okay, I'll very quickly go through the simulation. So we consider three tests: the proposed test and the L2 test and the maximum test. And under the alternative hypothesis, we set all the signal on the first off diagonal. Well, despite all the signal are on the first off diagonal, but a after we generate the sigma, we permute the row and the corner. Might be permitted to the row and corner. So basically, the actual signal can be anywhere in this covariance. Okay, so this is the size. So our size is slightly smaller than 0.05. Okay, but generally it's okay. So this is a diagonal of the covariance equal to 1. This is variance equal to 0.5 and the variance equal to 0. Variance equal to 0.5 and the variance equal to 2. Okay, so the maximum test has kind of a size distortion, but if we use a Gaussian approximation to correct the rejection criteria of the maximum test, then we'll have very precise size. Okay, so for the power, so this is the case that we fix sparsity level 0.5, 0.6, and 0.7 in the core. 6 and 0.7 in the corner of the panels. Larger the specity level, the smaller are the number of signals. So the power will be smaller. And on the x-axis, we increase the signal strengths r from 0.5 to 1. So we can see that under the most dense signal regime, so our test has a similar power performance as the L2 test. The L2 test, the QC is the L2 test, but more powerful than the maximum test. But on the sparse signal regime, so our test is more powerful. How many times I have? No? Okay, thanks. Okay, so I will skip on this another simulation setting. Another simulation setting. I will also skip the two-sample covariance testing setting. Okay, so I will quickly talk about testing for precision metrics. Okay, so because the precision matrix is related to conditional association, so under the Gaussian distribution, if omega J one J two is zero, so this means that the variable J one and the variable J two, they are conditionally independent. So that's why it's important to study the Gaussian graphical model. To study the Gaussian graphical model. Okay, so we consider the hypothesis that for a given block of a variable A1, we want to test for any J1, J2 from A1, so their partial coordination is zero, as long as J1 not equal to J2. So this is basically the testing a block of the precision matrix being diagonal. So this is related to evaluate the spatial dependent structure. The spatial dependent structure using the concept concludes. So if they have a Fourier neighborhood in a lattice, then there are two concrete. Given one conclusion, the variable and the other conclude are all conditionally independent. So this similar to covariance case, so we So we use a similar metrics class to study the minimax lower boundary. So the MA again denotes the number of signals. It's specified by Q1 to the power 1 minus beta. So Q1 is P1 times P1 minus 2 over 2. So basically the off-diagonal elements in the block A1. Okay, so beta is again from 0.5 to 1. And omega J1 J2 is And omega J1J2 is specified as square root of 2R J1G2 times NOC Q1 over N. So basically, this is the same setting as the covariance case. So the class we consider is there are MA non-zero precision coefficient with strength larger than R0. And again, the upper bound the diagonal value by tau and the lower bound by a small constant. So we can show that this detection boundary. At this detection boundary, dB beta for mean and for the covariance metrics we just talked about is also detecting boundary for precision matrix. So now R over tau square is the new standardized signal strength, but here the tau is not the upper bound for the covariance, it's upper bound for the diagonal of the precision matrix. So we have an additional constraint which restricts beta larger than 3 over 4 minus a cassette over 4. So this is an additional condition. A cassette is still the parameter to denote the relationship between N and P. Okay, so we consider a polynomial, we only consider a polynomial growth rate here. Okay. Okay, so as long as Okay, so as long as the standard signal strength is below this detained boundary, so then the max and minimum power will not converge to one. Actually, it's basically no power at all. So how to achieve this detailed boundary? Okay, so we take the nodalized regression approach. So this approach, using this approach to do inference for precision metrics. Do inference for precision metrics is proposed by Ahuidon Leo in his 2013 paper. Earlier, there are several papers, including Professor Zhu's paper, using this approach to select the non-zero component in the precision matrix. So a lot of regression has a very nice property. So first of all, the regression coefficient is proportional to the precision. To the precision coefficient. And the second, the covariance of the regression error is also proportional to the precision coefficient. So therefore, based on this result, if the omega j1, j2 is zero, so then the regression arrow from node wise regression is uncorrelated. So this is related to our first, the previous result of the uncorrelated test for covariance. Test for covariance. But now we just need to estimate the residue from the model wise regression. Okay, so we estimate this by using NASO to get the residues and then form debias estimates of the so we had J1J2 basically is the debiased estimates for the covariance between the regression error. So then we just build this thresholding statistics on VG1G2. VG1G2 basically is the estimated partial correlation based on the residual from the model-wise regression. So the detention boundary for this case is more complicated. Complicated, a little bit more complicated. So, this is related to not only cosine, but also gamma. So, gamma is the size of the set of variable we are interested in. It's at the order of p to the power gamma. And then an additional parameter nu is the sparsity level of the precision matrix. So, we can show that. So, we can show that. So, if the standardized signal strength is above the boundary, then the power of this thresholding test will convert to one. So, different from the covariance testing problem, so the first part of the decent boundary is different, but the second and third part is the same. So, the difference is mainly caused by the cutoff value. Caused by the cutoff value, which has expression given earlier. Okay, so this C is related to sigma, gamma, and new. Okay, so to sum up, so this talk we consider the tight mediumax result for testing covariance and the precision matrices. And we propose a sharp optimal multi-navoral straight. Multi-naval stretch holding test. So here the sharp optimal probably has some code because it's a sharp optimal under some conditions. For example, the casi equal to two. So then under this case, we can achieve the whole detection normal bound. If the casa is smaller than two, we can only achieve this detection or bound for a particular range of the signal specific. Okay, so we demonstrate it's more powerful than L2 and L maximum test for sparse and the weak signals. Okay, that's all. Thank you.