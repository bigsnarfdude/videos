I actually got my PhD in 2015. I've been around for a little bit longer. So yeah, thanks a lot to the organizers for inviting me and giving me the opportunity to present our work here. So the title of my talk today is A Landscape of Peaks. Okay, so the intermittency islands of the Stochastic Heat Equation with Levy noise. And this is joint work with Peter Kebe. With Peter Keve from the University of Security. And if you're interested, you can find the preprints of our papers on archive. So there are two papers, and I will describe in a minute what each of them is about. So let me start right away with the equation that we want to consider in this talk. So here we are really interested in studying properties of the solution to the stochastic heat equation. To the stochastic heat equation, um, you know, driven by a levy noise. So, I think all of you are, you know, familiar with what is the stochastic heat equation. So, you know, that's this part over here. We were studying the heat equation in D dimensions on free space, so on Rd. Okay, so the normalty in this talk is really that we consider a non-standard noise. So, instead of putting a usual Gaussian space-time white noise, we're interested in the case where the noise is less. In the case where the noise is levied, so that's this guy over here. I will recall in a minute what it is, but let me for now mention that we're considering the situation where we also have a coefficient in front of the noise, so sigma of u, where sigma is a globally Lipschitz function and u0 is some initial condition. Okay. Now, as I've mentioned in this talk, you know, I will only focus on the case where this L here. This L here is a Levy noise. In fact, I will only restrict myself to the case of a pure jump-levy noise. In that case, the noise L can be obtained in the following way. So let's take a Poisson point process. Okay, let's call it omega. That's where the randomness comes from. With intensity dt in time, dx in space, an additional measure lambda dz on R, which is essentially. Which is essentially governing the distribution of the jumps of noise. Okay. Now, given such a Poisson point process, what you do is the following. So, for each point of the process, you put a delta function at time point t and at space point x, and you weight it with z, okay, which is the third variable that comes from the points on point process. So, you know, in a certain way, if you have never seen, you know, if you if you have never seen you know levied noises before you can really view um a levy noise as a sum as a weighted sum of dirac deltas um you know at random space-time locations with random weights okay and the um you know the space-time locations are uniform in time and space the weights are chosen according to this lambda over here um for technical reasons what happens is actually that you know um a lady noise may have Know a leavy noise may have a lot of small jumps, okay? So many small jumps that it may fail to be summable. Okay, so that means in order to get a meaningful definition, what you usually do is you first truncate, okay, this sum that I've just described at a level of little a. So, little a, you should think of something very small. So, no, we're removing all the jumps less than or equal to a in absolute value. Um, then You. Then this sum would always be, you know, well-defined, it would be always summable. And then what we do is we center this sum. So we subtract essentially the expectation of the small jumps. So Ka is the average or expected value of the small jumps between A and one. And once you have centered the small jumps, and then you take the limit A to 0. And then you take the limit A to zero, you can actually show that this will converge to something meaningful in L2, in, say, the space of Schwartz distributions. Okay, so that's the usual way to define a levy noise. So you truncate the small jumps and pass to the limit after normalization, so to speak. Okay, so that's essentially the noise that I'm going to consider. Throughout this talk, let me also say that. Throughout this talk, let me also say that we actually only consider two different functions sigma, either a constant, and that's what we refer to as the case of additive noise. And we will also couple that case with the initial condition zero. And the other case is where sigma x is proportional to x. So for simplicity, let's say just x. And in that case, we will take one as initial condition. That I will refer to as the case of multiplicative noise. The case of multiplicative noise. So, really, I will only talk about these two cases in the talk. And that's also, you know, how the two papers that we have posted on archive are related to each other. So the first one here that you can see is about the additive case, whereas the second one is about the multiplicative case. Okay, so I've chosen the title of the second paper, but I would talk a little bit about both cases in this talk. About both cases in this talk. So, before I get to the properties that I want to talk about, first of all, I think I have to recall the notion of solutions and conditions for solutions to the stochastic heat equation with Levy noise. So I think that's something that most of you will be familiar with, but let me nevertheless recall it. So a random field is called a mild solution to the stochastic heat equation if it's a predictable random field such that Random field such that for all t and x, u satisfies this, you know, stochastic integral equation. So u appears here and here. Okay, and you know, the first part just comes from the initial condition, the deterministic thing. The second part is the convolution of the heat kernel, which is given here, with the noise point, essentially. You should forget about the beta, that's a typo, there's no beta here. Beta, that's a typo. There's no beta here, okay. Um, of course, implicit in this definition is that you know you really want that all these integrals here are well defined, okay? So that's part of the definition. So, you know, u has to be in such a way that these integrals are well defined and you have, you know, almost sure equality for all fixed t and x. Okay, so that's the notion of a mild solution. Um, and as you can probably see, if sigma is just a Can probably see if sigma is just a constant, okay? There's nothing to be solved. You just take the right-hand side as a definition, and the question is: does the right-hand side exist? Is it well-defined, et cetera, et cetera? If sigma is equal to x, there is actually an equation to solve. All right. Let me tell you right away, you know, the result about existence in the additive case. So it's actually quite easy to. So, it's actually quite easy to show that in the additive case, you will have a mild solution in the sense that the right-hand side of the equation that I've just shown you will be a well-defined object if and only if you have this condition, okay, and one further condition, which is the first line if d is two, and the second line if d is greater or equal to three. Line if these greater or equal to three. We do not need any further conditions in dimension one. And if you, you know, if you have not seen Levy noises before, you may wonder what these conditions mean. So let me quickly describe a little bit what they really stand for. So it's always important when you deal with Levy noises or this Levy measure lambda here to distinguish whether you're looking at behavior around infinity. That's this part. Okay, so you're looking at really So you're looking at really jumps of size bigger than one, big jumps, or whether you're looking at small jumps, so jumps smaller than one in absolute value. So the first condition is really a requirement that the noise have moments, log moments of order d over 2 for the big jumps. So that's essentially requiring that the noise has a Has a logarithmic moment of order t over q. Whereas the second and third condition here are conditions about the small jumps. So it's about the summability of the small jumps. So please note that you always have, you know, integral z squared lambda dz to be finite. That's part of the condition of a levy measure. That's true for every levy measure. So in dimension two, it's a very mild condition. We just need an You know, condition we just need an additional log z here, whereas in dimensions three or greater, you know, it's you know, this exponent is really smaller than two, so you know, that becomes you know a condition, okay? Um, I should also mention, I've not said that before, but it's written here. Um, in this talk, I will only look at noises with positive jumps. So I will not consider the case where you have negative jumps. That's why I start everything from zero and go to. Start everything from zero and go to infinity. Okay. And that result is actually not difficult to prove. You just use Ogachfoot and Kosinski's conditions on the existence of deterministic integrals with respect to lady noise. And you just need to verify their conditions in the present setting. And then that's it. So if you move to multiplicative noise, things are much more subtle. So now I've written. So now I've written this here, so I assume the case we have no negative jumps. Okay, so in order to give you a flavor of the results that are known in this case, let me denote by UA, okay, the solution to the SHE with the truncated labyrinth noise LA. So remember, LA is the noise where I remove all the jumps of size smaller than A. Okay, so only jumps bigger than A are retained in this LA here. In that case, In that case, you can show that the multiplicative, I should have said it here too. So, this is about the multiplicative case. Okay, so in the multiplicative case, if you do this truncation, the stochastic heat equation with this truncated Levy noise will have a finite solution, UA, if and only if, again, you have this condition. So, this is the same condition as before, the same as this one. So, and we need that here too. So, and we need that here too. As you can imagine, we cannot have fewer conditions in the multiplicative case. So, this condition here will guarantee that these truncated solutions will exist and are finite. Now, of course, the question is, what happens if you send A to zero? That's how you hope to obtain the solution for your original noise L. Now, here's what happens. Now, here's what happens. In dimension one, oh, I actually didn't write it, so I should have written it. But in dimension one, okay, you just set A to zero, and you would get a finite, strictly positive limit, which resolves the equation. So there are no further conditions you need in dimension one. Okay, so in dimension one, no, that's it. If you send A to zero, you will get convergence to a solution to the SAG. In dimension two, In dimension two, however, you do need a condition, which is this one. And that's exactly the same as we have seen in the case of additive noise. So the same condition. And this one, we guarantee that you will converge to a finite striply positive limit as A goes to zero. Okay, and that's an if and only if condition. If that condition is not satisfied, if it's violated, you can show that UA actually goes to zero. Okay, so that's not a meaningful solution. Okay, so that's not a meaningful solution. In dimensions three or greater, it's a little bit trickier. So we can show that UA converges to a finite strictly positive limit, U, if you have this condition here. So please note that this condition is a little bit stronger than in the additive case. Okay, so in the additive case, we didn't have the log z in dimensions three or greater. Here we do have a log z. And this is. A log c and this is an almost necessary condition, too. We actually believe it's also necessary, but we cannot prove it. What we can, what we actually can prove is that if you have this, so okay, log C, but then you have a log, no, correct it by the log, log factor to some power, okay? If that is infinite, you can show that UA actually goes to zero. So these results are proved in the paper by In the paper by Continent Perg√©, Yupa Lacroix, and myself in a separate work. Okay. So maybe, you know, one thing that I want to point out is the interesting fact that in dimension three or greater, because of this condition here, you really have cases, okay, not so many, but you do have a few, where you have a solution for the stochastic heat equation with additive noise, but no solution for the, in the case of multiplicative noise. Okay, so. Of multiplicative noise. Okay, so that's exactly when the integral of z to the power one plus two over d is finite, but this guy with the log z divided by log log z is infinite. Okay, so that does happen, which is, I think, kind of interesting. All right, so that's about solution theory. Of course, you know, I should say that under the previous sufficient conditions, the limiting process used, so if you send eight or zero, well, that's first of all. Well, that's first of all, just some limit. But that limit you can show is indeed a mile to the ocean to the SHE with multiplicative noise. And under some slightly more restrictive assumptions, but they would cover all alpha stable noises with alpha small than one plus two over D. You can also obtain uniqueness. And of course, I should also mention that we are not the first one to study those questions under stronger assumptions on the noise. Stronger assumptions on the noise, okay, um, existence, and sometimes also uniqueness were obtained in previous works by you know several people: Song of Bobier, who's, I think, also in the audience, and myself in 2017. I should also mention that Carl and Leonid also have studied the SHE with Levitic noise, but no, with a different non-limit Lipschitz nonlinearity. Okay, all right. Um, so I think that's you know about the existence of solutions. You know, about the existence of solutions. Let me also mention the following result, which is about moments. So you can show that if you have those conditions that I've mentioned, okay, so they are now assumed throughout the remaining talk. But if you have, in addition, the following integrability condition for the big jumps, so the P, Pth order integrability condition for the big jumps, for some P between one and one plus two over D, then your solution would. Then your solution would also have a finite pth moment. Okay, and please note that this restriction that p is less than one plus two over d is something that I, you know, it's not innocent because you can actually show that the SHE will never have a finite one plus two over D moment, neither in the additive nor in the multiplicative case. Okay, so even if your noise has as many moments as you wish, so for example, if you take a Poisson noise, which is you know bound. You take a Poisson noise, which is bounded by one and hence has moments of all orders, your solution will not have more moments than up to order one plus two over D. So that's something that you have to keep in mind. So in dimension one, you can will have at most, you know, three minus epsilon moments. In dimension two, you will have two minus epsilon moments at most. In dimensions three or higher, you know, even fewer moments. Okay. So the solutions that So, the solutions that we are looking at getting here are really heavy-tailed in all cases. Okay, so even if you put in a light-tape noise, you will get a heavy-tailed solution. So, now let me come to the question that I actually want to study in this talk today. So what I'm interested in today is to study the behavior of the solution for a fixed time. Okay, so you take a snapshot of your solution for at a, you know, at your Delusion for at a you know, at your favorite time. And now you study what happens if you go to infinity in the spatial direction. Okay. And if t is large, you know, this type of questions is, of course, related to the notion of intermittency, where people have shown, and actually there are many conjectures also around this topic, that the solutions will develop high peaks on small islands. Small islands are small in between. And so, what we're trying to do is to quantify how big are these spatial islands here when you take a picture. So, I'm going to describe these peaks in a minute. So, that's what this talk is about. All right. Now, before I discuss the Levy case, I think I should really mention and discuss the details. Mention and discuss in detail what we know about the Gaussian case. In the Gaussian case, of course, you know, you only have solutions in the multiplicative case and function value solutions in the additive case if dimension is one. And in that case, you know that, well, if you look at the behavior of the solution in the additive case, well, then UTX in X essentially behaves like. Essentially, it behaves like a stationary Ulenbeck process. In other words, the spatial maxima will be of order log square root of log x times the constant. In the case of multiple predictive noise, the spatial peaks are much larger. That is possible because you have the effect of intermittency. And in that case, log of u, okay, so log of u behaves. So log of u behaves like a constant times log x to the power two thirds. So u itself, u tx will be exponential of constant times log x to the power two thirds, which is of course much larger than square root of log x. Okay. That has been shown in you know in earlier works by Conuse and I, for example, from 2013. All right, so that's the Gaussian case. Now keep that in mind. Now, keep that in mind. Yeah, may I ask you a question? Sorry, may I interrupt? Yeah, so if sigma is one, you have a pi appearing as a constant. If sigma is x, why there is no pi appearing for the right-hand side? Yes. So wonder, what's the reason? Oh, okay. So these constants are, of course, you know, due to certain explicit calculations related to the heat kernel. And I don't think I can. And I don't think I can give you a reason why there's a pi. Yeah, we imagine we have a pi, right? Yeah, we expect for the heat kernel we have a pi, but if sigma is x, why are the pi disappear? Okay, I have to first admit that, you know, I mean, I copied this from the, you know, from the, from the Berkeley, so but I actually don't, you know, know right now whether I made a mistake in copying the results, but you know, whether there's a pie or not, you know, in the end, you know. There's a pie or not, you know. In the end, you know, for this chart, what matters is that this is a constant. Okay, so that this number is really giving you the rate. Now, for the values of the constant, of course, I should mention that these are not, well, in the additive case, it's easy to obtain, but in the multiplicative case, it's actually not such an easy thing to obtain the exact value of these constants. So, you know, please do recheck with the results in the paper whether it's correct. The paper whether it's correct. Actually, it's written like it's just this work here that showed it. But the way this constant or identity is proven, it has a long history. So many people have contributed to that asymptotic. And this is the paper where people have finally managed to show it. Yeah, so please do verify whether there's a pie or not. Thank you. So, um, that's a Gaussian case. Um, now let me go to the Levy case, and you will see in a minute the Levy case is fundamentally different from the Gaussian case, and there will be quite a lot of cases. And in order to really focus on what is important, I will only consider one family of jumps, and that's the case where lambda is essentially giving you Pareto distributed jumps. So it's this. So it's this particular Levy measure here. So you have a constant times z to the minus one minus alpha, and you only have jumps of size bigger than one. So no small jumps. And by making this restriction, you know, I can really focus on what is important. Please note that in the papers that I've mentioned, we actually consider general levy noises, so not just that example. Not just that example, okay. So, much more general lady noises, but all interesting phenomena already occur in this example here. That's why you know I restrict myself to this example here. You should view alpha as the measure of how heavy-tailed the noise is. Now, of course, depending on your definition of heavy-tailedness, okay, but under the usual notion of heavy-tailedness, all these measures should be viewed as Measures should be viewed as heavy-tailed because they only have a certain number of finite moments, not finite moments of all orders. So they're heavy-tailed in all cases. But alpha is a measure of how heavy-tailed it is. If you take the limit alpha to go to infinity, alpha very, very large, that would correspond to something that's like a lighted noise. By making this restriction, I can also. By making this restriction, I can also avoid discussing technicalities due to small jumps, which actually you can show play no role for the macroscopic behavior. So the macroscopic behavior is really determined by the big jumps only. The small jumps will only have an effect on local behavior, and that's not the focus of today's talk. So, no, that's why I've removed them altogether. So, we will only consider this. So we will only consider this family of jumps in the talk, in the remaining part of the talk. Okay. So I will distinguish three cases, light, medium, and heavy tail. Okay, so and that's essentially about how big alpha is. Okay, light tail is when alpha is very big, bigger than one plus two over d. Medium tail is if it's you know between two over d and one plus two over d and heavy tail is when alpha is. heavy tail is when alpha is less than two over d okay so that's my notion of um you know how heavy tail the noise is and you know you have three types of three types of behavior um in those three cases okay so let me start right away with um the case of light and medium tailed noise so not too heavy tail okay so alpha is bigger than two over d let's assume that for the moment um when you can Um, what you can show is now the following. So, take your favorite non-decreasing function f, okay. Then, almost surely, if you take the spatial supremum of the solution up to x, okay, and you divide by your favorite function f, then this will either go to infinity or zero, okay, according to whether this integral here is finite or infinite or finite. Okay, so if it's infinite, then this will go to infinite. It's infinite, then this will go to infinity. If it's finite, then we go to zero. Okay. So this is actually quite some surprising result because that really shows us there's no proper normalization. So you cannot find a function, a proper gauge function, such that this ratio here converges to something non-trivial. Remember, if I, you know, if you allow me to go back a few slides, in the Gaussian case, you know, you were able in both additive. You were able in both additive and multiplicative case to find a function, a gauge function here, so that after normalization, you will converge to something, to a constant. So the news here is in the Levy case, you do not have such a thing. There's no function that you can put here. You will always either get convergence to infinity or zero, depending on the behavior of this integral. So that's already quite some different. Already, quite some difference to the Gaussian case. So, you have an integral test instead of an abrogate function. Okay, what is more surprising, in my opinion, is the fact that these peaks, okay, the largest peaks are of the same size, no matter whether you have additive noise or multiplicative noise. Remember, in the Gaussian case, you can have different behaviors for additive and multiplicative noise. Here, you really have the same. Here, you really have the same largest peaks, okay? The same integral test, both for additive and multiplicative noise, which is, you know, I think against all intuition, because, as I've mentioned, in the multiplicative case, you have the phenomenon of intermittency. So the peaks that you should observe, well, that's what you think, should be larger than in the additive case. But it turns out, no, that's not what's happening. So you have the same size for the largest peaks. Size for the largest peaks if you have laby noise, no matter whether you put it additively or multiplicatively. So that's a result. The high-level explanation for this last point that I've made is the following. So the reason why the largest peaks are of the same size is because they are caused, and that's something I would elaborate later on, by single closed jumps. So by a single jump that occurs very Single jump that occurs very, very closely to the space-time point that you're looking at. And you know, you do not need a multiplicative cascade of points to build up your largest peaks. A single jump is enough. So, you know, and you have a single jump both in the additive and in the multiplicative case, that's why they are of the same size. And we call this effect additive permittency. Okay, so because you know, the largest peaks are really caused by single isolated jumps. That's it. Okay. That's it. Okay. Now, you may, of course, ask the question: so, you know, are the solutions with additive and multiplicative living noise qualitative the same? So in the Gaussian case, we know they're not, but here it's kind of seems that, you know, they are the same, right? Well, the answer is still no. And that's, of course, good news because you do expect that mitochondriative noise gives you something different than additive noise. So let me. So, let me explain a little bit how to distinguish two now. So, of course, what this tells you is it's not by, you cannot distinguish the two by looking at the largest peaks, okay, because they are of the same size. What you should really do in order to distinguish the two is to look at a second layer of peaks, much smaller, in fact, which occur, you know, on a grid, for example. So, if you restrict yourself to the lab. yourself to the lattice Z D. So you're now no longer walking in continuous space, you only hop from side to side on a grid, on Z D. And now you observe the biggest peaks on this grid. Then you will start seeing a difference between additive and multiple noise. So you have the second layer of peaks, and that's how you can distinguish the two. So in the additive case. Sorry, can I interrupt? Sure. Sure. Yeah, I was wondering if you could see more differences if you could include the small jumps. No, no, no. The picture would be the same. Would be the same. So, okay, I should say the following. So, if you include small jumps, there are two ways of doing it. So, there's, you know, if you put a lot of small jumps, a lot of small jumps, okay, then what happens is that the solution locally, it will be. Locally, it will be unbounded. Okay, and that means you really have oscillations between infinity and finite values infinitely often on the bounded domain. In that case, of course, all these questions do not make sense because the supremum here will be just infinite, both in additive and the multiplicative case. So there's nothing to study in the continuous case. If you put that many small jumps, the question is not meaningful, I think. Is not meaningful, I think. If you put fewer small jumps, still, no, still, you know, you can have infinite activity, so infinitely many small jumps on the bounded domain. You can still have that. If you put, but not too many, then in order to ensure that this thing here is locally finite, as soon as you have that, you will have the same picture. So no difference. No difference. The small jumps? Sigma equals identity and x, right? No, no difference. Excellent. No, no difference. Yeah. So the small jumps will either destroy everything, making everything infinite, or not changing the picture. But it cannot distinguish the two cases. Okay. So if you want to distinguish, you have really have to go on a grid or something like that. And that's what I'm going to describe here. So same setting, alpha bigger than 2 over D. Now you restrict yourself to Z D. Okay, supremum over Z D. You get an integral test in the additive case as before. Test in the additive case as before with a slightly different factor here. So before it was minus two over T, now it's something different. That's already something. So these peaks are smaller, okay? Because of what? Because of this new integral condition here, it's smaller. And if you go to the multiplicative case, oh, that's what you have. Now, you have two cases. And that's already interesting. So if you're in the medium tail. You're in the medium tail case, medium tail, so that's alpha between 2 over d and 1 plus 2 over d. Essentially, you know, the largest peaks are of the order x to the d over alpha times exponential of a constant times log x raised to some power. Okay, so that power is something that depends on the dimension and your alpha. Okay, so in that case, I should mention that we do not know. I should mention that we do not know. Okay, so we know that for some m, this goes to zero, for some m, this goes to infinity. We do not know whether there's an optimal m so that you get, for example, convergence to something that's non-trivial. So that's an open question. But essentially, what this results tells you is that this is, if you do not mind the constant, that this is the, you know. That this is the size of the largest peaks. And that one is different from the additive case. In the additive case, you get essentially x to the d over alpha, maybe some logarithmic terms in addition. But here in the multiplicative case, we get really something much more, much larger than a logarithm. So an exponential factor, very similar to the case of Gaussian noise. The case of Gaussian noise. Okay, so it's an exponential of a logarithm to a fractional power. Okay. So, is there any like an estimate for the M and the substar and M and superstar? Yeah, yeah, yeah. I think from the proofs, you can get get no bounds, depending on all the parameters. But we have made no efforts to make them optimal. Okay, so I think our proofs will not allow us to make them equal. Allow us to make them equal because there are steps where you know you get you know a constant that's not optimal. I see. Um, so that's why you know we haven't you know made efforts. But you know, if you follow the proofs, of course, you can find some bounds, okay, on these two things, depending on everything. Thank you. Thank you. Right, so that's medium tail, okay. Now, let me tell you what happened to the lighthate case. That's even more interesting, in my opinion. So, look at that one. So, same. That one. So same story, but here now the function of interest is x to some power d squared over 2 plus d. And then this very, very strange looking thing. So it's expansion of a constant times log x times log x divided by log log x. So we were really surprised to find this thing. So we're not aware of any other natural, you know, probabilistic model that would have something like that in their asymptotic behavior. But that's what we can. Symptotic behavior, but that's what we get. Okay. And here's a high-level explanation of why in the multiplicative case, though, sorry, why when you walk on a grid that you do see some other behavior. So it turns out that in the multiplicative case, you have a second layer of peaks that are caused by a chain of jumps. And you only have a chain of jumps, of course, in the multiplicative case. Multiplicative case. And that's why we refer to this effect as multiplicative intermittency. So that's the usual type of phenomenon people are interested in studying because this is what distinguishes the multiplicative from the additive case. What is interesting is, however, that the peaks caused by multiplicative intermediary are smaller than those caused by additive intermediency. That's why if you just look at the largest peaks, you won't see those coming from multiplicative intermediency. You really have to restrict yourself to You really have to restrict yourself to a grid or a discrete sampling to see those multiplicative peaks. Okay. The results are pretty interesting, amazing. You have used like a triple logarithmic function divided by true, all right? Yeah, this is true. When we figured that out, we were really like, okay, how the heck did that happen? Yeah, but that's what you get. So that's a very interesting function, in my opinion. A very interesting function, in my opinion. Um, yeah, that's what we get. So, um, let me mention the heavy tape case. In the heavy tape case, things are much closer to what we expect things to be. So, in the additive case, you still have a grotesque. Okay. In the multiplicative case, now you have this thing again. So, that's something we have seen already: x d over alpha. The d over alpha, uh, you know, e to the constant log x to some fractional power, and here already, so this is not on Z D, that's on whole R D. Okay, so here, in the heavy tape case, if alpha is smaller than 2 over D, the largest peaks are already different. So, that's the usual picture, right? That in the mighty big case, you have taller peaks. And that's something that happens here already if you look at the largest peaks. If you look at the largest peaks. And the same remains true if you restrict yourself to Zd. So you will not see different peaks on Zd compared to Rd. So that's the case is much, much closer to the case of, you know, to the type of behavior you expect to see in the Gaussian case, from the Gaussian case. So I've tried to summarize that. Okay. So I've spent an hour or two yesterday drawing this picture here. So let me. Drawing this picture here. So let me walk you through what you see here. So let me see whether I succeeded in drawing these things properly. So, okay, we have three cases: light tail, medium tail, heavy tail. We have additive noise or multiplicative noise. Okay, so I included these three cases and the Gaussian case. And the black lines are the sizes essentially of the peaks on Rd, so that's the continuous supremum. The continuous supremum in red are the discrete peaks on Zd. And in this picture, I try to show you whether the peaks are of the same size and how they differ between light and medium shade, etc. And the way you should look at it is you progress from top to bottom and then to top and then you do a six-second. Okay, so you go down and then you go up, you go down, you go up, you go down, you go up and down. And as you do. Down, and as you do so, you know, you essentially move from additive to multiplicative noise, and then from some lighter tailed noise to some noises with heavier tails. Okay, so in this direction, you go from additive to my predictive. In this direction, you increase the heavy tailedness of your noise. And as you can see, each time you move, peaks become a little bit larger. Okay. In that block, so in the In that block, so in the middle panel, as you move, you will not change the size of the biggest peaks. Okay, it's always roughly this, but you do change the size of the discrete peaks, okay? And that's the type of behavior you see. So, this is, I think, a good summary of all the theoretical results that I've shown you on the previous slide. So, yeah, if you want to take. um yeah if if you want take take take a look at at this um later on okay so um what i want to do in the last uh well how many minutes i have maybe uh 50 minutes or so um i want to give you not really sorry not really 15 this is including questions oh sorry so how many how how much time do i have we had already some questions so up to 10. up to 10. okay so then i know probably i know Okay, so then probably I won't be able to finish everything, but I can give you some ideas about the proofs. Okay, it's just some random ideas that I think are interesting or enlightening. Let's see. So essentially, the first observation is all the results about these spatial supremum, whether it's continuous or discrete, actually follow from upper and lower bounds of the tailbilities of the supremum of the solution. Probabilities of the supremum of the solution and of the solution itself. Okay, so the tail of the supremum over a compact set will give you the tail of the continuous supremum of the solution. Whereas the tail of the solution itself will give you the tail, will give you the behavior of the discrete supremum. That's something you can show using some general arguments. So in the following, I will only focus on discussing tails for these two guys here. Here. So, some heuristics in the additive case. So, if you that's for the continuous supremum. So, what you can do is, of course, you know, if you take the supremum of this illusion, you, well, that's the first inequality here. You remove all the jumps that are far from the point X that you're looking at. Okay, so you choose some X in a unit bot, and now you remove all the contributions Y that are outside of a unit bot. Okay, that's something you can do. Unit mod. That's something you can do, and that gives you a lower bound. After you've done that, you realize that this is exactly just a finite sum of points. So the tau i, eta i, and zeta i are the points from your Levy noise, and you can write it as a finite sum because there are only finitely many jumps in zero t times a compact z. Okay, of course, you know, a sum is lower bounded by the largest contribution. So we bound the lower bounded by the maximum over I of this thing. Okay. And then what you see is you can compute this explicitly. So rho is the heat kernel. If you take this supremum in X, that's precisely attained at the point where X is equal to add I. So you only get the You only get this part from the heat kernel, and that's a lower bound of the continuous supremum. And now you know the following: well, these guys have explicit distributions, right? So n t is a Poisson distribution. Tau i is uniformly distributed on 0 t. Eta i doesn't appear anymore. And zeta i is Pareto. And all things, every variable, all the variables are independent of each other. Okay, so what you can do now is you can. So, what you can do now is you can use standard results about regular variation in order to figure out the tails of this object. So, under standard results of regular variation, you know that the tail of zeta is r to the minus alpha, of the contribution from the heat kernel is r to the minus two over d. So, if you take the product of the two, okay, if you take the product of two regularly varying functions and they have different indices here. Uh, you know, indeed says here the one with the you know smaller index will dominate. So, you know, if it's either this or this, depending on which case you are in, that explains already the two cases. And once you have that, if you take the maximum of a random number of such random variables, and this random number n t has a finite expectation, and in that, in our case, it has a finite expectation because it's possible. Has a finite expectation because it's possible distributed, you can show that you will keep the same, you know, index of regular variation. Okay, so that actually essentially shows the tail of the continuous supremum. At least it's a lower bound. I should also mention that I will usually only discuss either the lower bound or the upper bound, depending on which one is more interesting. Depending on which one is more interesting, okay. So, here the lower bound is more interesting, so that would give you the lower bound. Okay, now when you go to a lattice, okay, that's the next thing that we want to study. You really want to only look at you, not the supremum of you, just you. If you just take you, no supremum, you do the same, you get the same argument. You get the same argument. You can use the same argument, except that you do not have the supremum. So, this supremum here should be removed. Supremum here should be you know removed. There's no such a thing, okay? It's a typo, so you have no supremum, and what and the only thing that changes is the index of regular variation of the rotor of the heat kernel part. Okay, so now it's no longer r to the minus d, it's r to the minus one plus two over d. And that would create this difference here. But you know, otherwise, you know, conceptually, it's the same. And because of that, the reason I'm showing that is this tells you that. That is, this tells you that why we have these three cases: light, medium, and heavy tail. That's because you know you have a phase transition at 2 over D because of the continuous supremum, and another one at 1 plus 2 over D because when you look at the behavior on the lattice. That explains the three different cases. Now, I want to talk a little bit about the multiplicative case. Bit about the multiplicative case, also, because I think that's interesting. So, let's see how far I get here. So, now what I want to show you is an upper bound, okay, for the continuous supremum in the light and medium trade case. Why upper bound? Well, the lower bound is easy. The lower bound, you just use lower bounded by the additive case. Because remember, this is the case where I claim that the multiplicative peaks are of the same size as the additive peaks. Okay, so the Peaks okay, so the trivial lower bound is you found the solution of the multiple equation by the additive equation solution, so that's a trivial lower bound. Um, so here the interesting part is the upper bound, okay. So, um, well, that's just rewriting the notion of mild solution, okay? Um, the first equality. The second one is to observe the following. So, you know, if you remove the jumps that are far from x, okay, x is From X. Okay, X is in the unit ball. You remove the jumps that are, you know, that have a real distance from X. So you're outside the ball of radius 2, for example. Then you can show that this will have sufficiently many moments so that it will not contribute to the tail. So the main tail will really come from the jump setter close to where you are, X. So that's the first observation. And now what you can do is you can put it inside, obviously. It's pretty inside, obviously, right? That's an upper bound. In that way, you get only that part. Again, after doing that, we have a Leby integral on a bounded domain. So you can write it as a discrete sum using the points of your Poisson point process. And you get a similar thing as before. So you have something coming from the heat kernel. The noise variables themselves, zeta. noise variables themselves, zeta i. And now that's new to the multiplicative case, you also multiply by u evaluated at tau i eta i. Okay. Now we've already discussed the tail index of this part and this part, that's as before, so minus two over d so it's regularly bearing with index minus two over d regularly varying with index minus alpha. The new part is really this guy, you, okay? And well, we know that something I've mentioned briefly. That's something I've mentioned briefly at the beginning: that the solution will have moments up to order one plus two over d, and you can actually show that you know it has its regular varying with index minus one plus two over d. Okay. Of course, what makes things tricky? Of course, no, okay, what I want to say now is, okay, if you look at these three guys, okay, this one is the largest, you know, has the heaviest tail. So that's why this guy will dominate and give you this tail. Okay, that's what I want to. You this tail, right? That's what I want to say. And this argument is, of course, perfectly fine if things were independent, right? You know, if everything is independent, we know product of independent regulatory variables. The detail of such a product will be determined by the heaviest component. And if you sum independent things, you know, it will not change the regular index of regular variation. That's true. That's true. But the thing is, in our case, we have you, and you creates dependence between these guys and also between different eyes. So as you vary eye, you're no longer summing independent components. Okay, so well, that's just a fact. And now, of course, you have to come up with techniques to get around. Okay. And we manage, and in the end, you know, you get still the same. And in the end, you know, you get still the same, you know, index of fragile variation. But, you know, this part is actually something you have to spend a little bit more effort on. But yeah, that's a high-level explanation why you get the same tail in the multiplicative case. That's really because the heat kernel part here, the heat kernel part, this guy, dominates everything else. It particularly dominates the solution. So no matter whether you have a U here or just one, that would be if you have additive noise. One, that would be if you have additive noise, right? Just one instead of you. It doesn't matter because this guy dominates. Okay. So that's the reason why, in this light and medium tail case on Rd, you see the largest peaks, you see largest peaks that are the same among additive and multiplicative noise. So, Anika, how much time do I still have? These were the 10 minutes. Oh, okay. Then I'll stop here and thank you for your attention. So I have a few more slides on a lower bound, okay, for the, you know, to explain why you get these, you know, strange looking functions such as this guy. Okay, but you know, you can study them. I will post the slides on, I will send the slides to the organizers. If you are interested, you can take a look at them or get back to me for. Look at them or get back to me for questions. So I will stop here and thank you for your attention. Thank you very much. Carson, so we have some time left for questions. Please just raise your hand or unmute yourself. I will try to keep track here of the list. I can start if you want. Yeah, I had some questions about, I mean, you're not using many of the techniques we're used to for Gaussian, the Gaussian case, right? Like, do you have a Freyman-Kat representation? Is it useful? Yes. Yes, so we're not using the Feynman Katz representation. So, what we do use, of course, is in this part that I've skipped, okay, which is where you get these interesting additional functions. What we do use is, let me see here it is, is a chaos decomposition. Okay, so you write a solution. Okay, so you write a solution as a series of multiple integrals of different orders. So that's something we do use, but we're not using a Feynman-Katz representation here. But of course, what is similar to the arguments that people have used in the Gaussian case is translating these tailbounds, translating these tailbounds to statement. Statements about the behavior of the supremum on RD versus Zd, these techniques are somewhat universal. So we are appealing to the same type of arguments. So what is really different... Those rely on, I mean, when I've worked on those kind of things, this relied on Holder continuity properties, right? Not so much, in fact. So once you have these bounds, you can. Bounce, you can go from here to here by essentially using Boret-Conterdi's lemma and some independent, you know, local asymptotic independence properties. That's in the in the discrete case, right? But if you. No, it works in both cases. It works in both cases. Okay. So, yeah, essentially, what I'm saying is, you know, going from these bounds to these things, I mean, there's some technical difficulties, of course, but. Technical difficulties, of course, but the main ideas and the main steps are similar to papers by Dabar, Kunwo Kim, and Yiming Xiao on these type of questions. So the main novelty, in my opinion, is really about the upper and lower bounds on the tail properties. From here to here, it's rather standard arguments. Okay, then maybe. Sorry, I haven't had another one. So there are some works by Peshat and Zapchik on stochastic heat equation driven by Levy noises. How do you relate to those? Okay, so they are okay. So they have considered different versions of Levy noise. So if they consider, I think they If they consider, I think they call it impulsive Levy noise, okay, you will get um, they essentially work under an assumption like this, okay? So if that's what they call impulsive Levy noise, so a Lebesgue white noise, they, I think, in their work, they assumed a finite pth moment of the solution, and then they derived existence and uniqueness. That's something you can do, and that will give you, you know, necessary sufficient conditions in a special. Well, sufficient conditions in a special case. But in addition to that, they have also considered cases where the noise is not white. Okay, so I think the main body of their work is really about noises when we go to the first slide, where LD, you know, L is a, let's say, Hilbert space value Leady process. What that means is, you know, you have independence of increments in time, but then, you know, your But then, you know, your noise, at every jump time, you will add a Hilbert space function. So, you know, in particular, you will not get something that's wide in space. So it will be very much correlated. So I think their work is, you know, the main body of their work is about these Hilbert space value Levy processes. So it would be white in time, but very colored in space. In that case, of course, things are totally different, right? Things are totally different, right? No, you would not expect the same type of things that I've described here. Thank you. You're welcome. Are there more questions? Oh, yeah, I have a question that the intermittence basically has something to do with the growth of the moments. So I was wondering: is it possible to get the Freyman-Kauf formula for the moments of the solutions? Formula for the moments of the solutions so we can study the moment growth of the eighth moment. Very good question. Um, I have to disappoint you. No, so let me explain why. Here's the problem: where is it here? You only have at most one plus two over D moment. Oh, yes. In dimension one, you have moment of order one and two. In dimension one, two, you only have one. In dimension one two, you only have one moment. And in dimensions three, you also have only one moment. So, you know, there's nothing you can do. In most cases, you even don't have a finite second moment. So no, no. If you cannot use second moments, then all these things break down. So yeah, you will never have high order moments. That's a problem. I see. Okay. You're welcome. Yeah, may I ask a question? So, you want to study the probability of those and the peaks, those are like similar solutions. Physically, what do those peaks tell us? Sorry, can you repeat your question? Right, so I didn't get your question. Okay, so and my question is that physically, what do those peaks tell us? Those are singular solutions tell us physically. I see. So, um, you see. So, you see, these peaks should be really viewed in the context of intermediate. So, can you give us a concrete example to illustrate those peaks physically for those kind of heat equations? So, I'm not sure if that answer will satisfy or address your question, but no. In the context of particle systems, it's very important. Systems, it's very important to study the phenomenon of intermittency, which is to try to find equations that generate a lot of mass concentration on small areas. And the heat equation with multiplicative noise is one such a system where people have shown that whatever white noise you put in, you typically can expect that the solution will concentrate on very small. Concentrate on very small areas as time progresses. And that's how you should really view these peaks. So the physical meaning is that you have a lot of concentration of the mass, of whatever mass mean. If you look into examples in biology, it means all your individuals concentrate around one area, or if you look at other things, but it really means mass concentration. You have a lot of mass concentration on very tiny islands as tiny islands. As time progresses. So that's the physical meaning of these peaks. Yeah, I think I got you. Yeah, there are some other studies for those like concentrated energy within a very small areas. Exactly. Exactly. That's it. So you should really view, you know, u, you know, as some sort of a density. Okay, so utx, no, you can view that as the probability at time t to find your individual at place x. Okay, and if that has a peak, it means you know you're very likely to find. It means you're very likely to find individuals at position X and very unlikely to find individuals at the valleys. I think that Carl is going to talk about in this conference too. Yeah, so that's intermittency. Yeah, and then I will keep asking another question. So as T and goes on, but you are sort of a fixed tier, right? You look at the behavior for logic X. What if T goes on, and what's the behavior of those peaks? Ah very good question. Peaks? Ah, very good question. So, I would recommend you to take a look at this paper. So, that's another paper by myself and Contra Bexi and Iber La Kuan. And here we really study the behavior of the solution as t goes to infinity. But can you just summarize in one sentence what would be the behavior as t goes to infinity for those people? If t goes to infinity, so in the Gaussian case, you know, it's kind of well known that you have intermittency. Known that you have intermittency, you always have intermittency in dimensions one and two. Okay, if you know in higher dimensions, of course, you have to do some modifications of the noise, but if you do that, you will always have intermittency dimensions one and two. And in dimensions three or higher, you will only have intermittency if the noise is strong enough. Okay, so in dimensions three or large, you have a non-intermittency and an intermittency regime. That's what you know from the Gaussian case. In the Lanny case, what we could show in that. In the Levy case, what we could show in that paper, and that's the main result, is that you always have intermittency in all dimensions, no matter how strong your noise is. So, in particular, in dimensions three or higher, you do not have a weak disorder regime. You always have intermittency. So, no, that's the difference from the Gaussian. Thank you. Thank you very much. Thank you. You're welcome. Hello, it's very good talk. I have a quick question. I have a quick questions. So I'm wondering if you consider the both the multiplicative, so you consider the general case of multiplicative, like you have AX plus B, this alpha noise case. So which will, so this behavior will looks like the your cases, the you, the signal, the diffusion trial. The diffusion trial equals to x equals to u, this case, or kind of like the additive noise case. Sorry, I didn't get your question. Can you repeat it? I'm sorry. So my question is that if you your diffusion coefficient is like AU plus B is this case. So sorry, what do you mean by diffusion coefficient? Yeah, sigma is equals to equals to for example. Equals to, for example, A times U plus constant. So, if it is the case, does this additive behavior dominate the asymptotical behavior, or it's more likely to the case, the sigma u is close to you? That's a good question. Let me think about it. So, well, I do think that it will be a mixture of the two. It will be a mixture of the two. Well, we haven't proved it, but no, everything I'm saying is conjectural. But you know, what you do expect is that you still have the additive peaks, no, they will always be around, no matter where added four multiplicative noise, you will still see them. But I expect that you will also see the multiplicative peaks on this second layer. So it will just be a combination of the two types of effects that you see. That's what I guess. That's what I guess. I guess. That's what I guess. Thank you. You're welcome. One last chance to ask a question. Maybe I may ask a question, which is probably just a stupid one. You have phase transitions at 2 over D and 1 plus 2 over D. What happens at exactly these points? Ah, yeah. So Uh, I think we have no. I thought you always had strict inequalities, yes, yes, yes. So, what happens is in those cases, no, uh, let me just explain, but in those cases, you will typically get a logarithmic correction. Um, you know, it's related to, you know, if you have two heavy tail random variables of the same, you know, index of variation, if you multiply them, you will get a tail which has the same, you know. A tail which has the same, you know, if it's alpha, you get r to the minus alpha, but times a log. Okay, so you know, you get an additional log, and that's what essentially happens. You get some small logarithmic correction. Okay, thanks. You're welcome. So time is over. We had no problem for discussing now 15 minutes. That's very good. I'm very