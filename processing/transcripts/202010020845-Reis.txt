Backspaces, we are looking forward to your talk. Sketch is yours. All right, thanks for the introduction, Christian, and thanks to Hue and Matthias for the nice talks. If I can share the screen, all right, I hope you can see this. Right, so I'm gonna talk a little about work that I did this year with my advisor Thomas in vector balancing. In vector balancing. So, first, we're going to start off with some quick warm-up. There's this very old result of Minkowski that tells you that if you have any symmetric convex body in Rn, if it has large enough volume, like larger than 2 to the n, it's going to contain an integer point that is non-zero, right? I think it's a very nice result. I always like this, to see this. And the proof is not constructive. It uses some pigeonhole argument. Pigeon pole argument, and it doesn't give you any guarantees on how large the coordinates of these points are. So, while it's a very nice result, it's very difficult to apply it computationally. So, there's another result that is also non-constructive, but it gives you a little bit more control over the coordinates of the points. So, here it's a result of Gluskin, and it says that basically if the volume of the body intersects, If the volume of the body intersected of a cube is large enough, then well, you can find some point that's not going to be like it's not going to be integer, but it's going to have many coordinates that are integer and small. So like a linear number of coordinates is going to be in plus, okay, it's going to be integer actually, but you know, it's going to be 2k and you're going to have a linear number of coordinates that are plus or minus 1. Of coordinates that are plus or minus one. So there are two issues with this theorem: is that the first one is that it's not polynomial time. We can't have an algorithm to compute these point. And also, it's kind of hard to deal with this volume of k-intersected with the cube. So, a way to get rid of this last problem is to use a different measure instead. So, we can So we can work with this Gaussian measure that I denote by gamma n. And there's a result of Cantor that shows that basically we can lower bound this volume of k-intersected with the cube by this Gaussian measure up to a factor of 2n. And so as a corollary, if we just have a measure lower bound on this Gaussian measure, then you can have the same result. But again, it's non-constructive. We don't have a polynomial time algorithm yet. So there's a result of my advisor six years ago that kind of gives you this polynomial time algorithm as long as this Gaussian measure is extremely large. So if it's larger than 0.999n, then we can actually find this point. It's not going to be an integer point anymore, but you know, it's going to have a linear number of coordinates in plus or minus one, which is pretty good for our applications. And it's going to be in police. And it's going to be in polynomial time, so that's nice. And the algorithm is quite simple. I can fit it in the slide. Basically, we just sample this point from a Gaussian and we project it onto the set. There's still one issue with this algorithm is that it requires this very stringent upper bound, lower bound, I mean, that is 0.999N. It's kind of hard to prove sometimes. So, what we did in this work, one of the things we did. Work one of the things we did is to give an improved version of this algorithm, which can take any lower bound of the type c to the n for c between 0 and 1. And it's going to give you not a point inside the set, but inside a scaled version. So instead of this y being inside k intersected with plus or minus 1 to the n, it's going to be inside this 1 over epsilon k, where epsilon is just like a polynomial in the c. In the C. And here in the stock when I write poly C, I just mean like some positive small polynomial. So like C to the 10 or something. And the algorithm is quite similar. So we're just going to sample a point from a Gaussian and project into this minus epsilon to epsilon to the n. And we're going to output the scaled version of the projection. And if you look at this, you might think, wow, that's basically the At this, you might think, well, that's basically doing the same thing as before, but I would argue otherwise. The issue here is that if we try to just scale the set and then apply the previous algorithm, we do not necessarily have the Gaussian measure lower bound of 0.999 to the n, because Gaussian measure does not scale as nice as volume, for example. So, here the assumption is much less stringent on the set k. But the algorithm is quite similar and I will just present the main technical lemma in the analysis, like a brief overview that is different from the previous analysis. It is basically saying that if you have this body with a large Gaussian measure, well, not that large, but at least c to the n for some small c, then you can have an upper bound on the average distance to this set, k. To this set, k. So if you imagine just the distance to zero, that would be something like square root n. And here we can get a better than square root n for the body k. So it's like it's getting large enough that the average distance is better than the distance to zero, kind of. And the overall picture is we're going to take this set K, set Q I mean, which Set Q, I mean, which is the intersection of K with a moderately sized ball. I don't tell you what C prime is, but it's a function of C. And we're going to look at the distance, we're going to estimate the distance from X to K by looking at the distance from X to this point lambda Z, which is kind of like, so the Z is the argmax of this inner product inside Q. And we're going to take somewhere in that segment, we're going to take some length. In that segment, we're going to pick some lambda and we're going to upper bound this distance with the distance from x to that point. And if we pick lambda carefully enough, we can get this result. So I guess the proof is not really here, but the next step would be to use Jensen's inequality to upper bound this distance. And also we're going to have to use something called Urisson's inequality to estimate the mean width of the body. Of the body Q. I'm not going to get into too much details. I just wanted to give you a high-level overview of what's the different part in the analysis from the previous algorithm. And I just have a little side note. If we want to go beyond Gaussian measure, you might wonder whether this condition is actually necessary. We could argue, okay, maybe. Could argue, okay. Maybe someone's talking in the chat. Let me see. So, can I also bound the expected distance from x to k squared? So, yes, I guess the proof does do that. Like basically the Jensen and then we upper bounded quantity, yes. Okay, so okay, so yeah, if we have a vote Okay, so yeah, if we have a weaker volume bound, like if you just assume that volume of k is larger than c to the n, well, that's really not enough, right? You could imagine you have like a very thin set that's infinitely long and it's going to have infinite volume, but we might not have a partial coloring. So yeah, so it's not enough, even if the volume is infinite. But you might okay, might argue something different. If we just have the volume of every coordinate section. The volume of every coordinate section is large, then we actually this suffices. I'm not going to give you a proof of this, but what I'm going to say is that this already implies that the Gaussian measure is not too bad. And the proof is a little outside the scope of this talk. It uses some complicated things. So if I already lost you by this point, we're going to start with an application which is quite different from what I've been. Is just quite different from what I've been talking about. So, I guess if anyone has other questions about this part, otherwise, I can just move on to applying this result. Okay, so we're going to think about how to apply this to vector balancing problems. In particular, we're going to apply it to vector balancing and LP norms. So, if you recall, the L P norm is defined in such a manner, so we just sum up the In such a manner, so we just sum up the p powers and take to the power of one over p, and this is all known to be a norm for all p between one and infinity. And here we're just going to focus on the regime where p is from two to log n. And for the purpose of this talk, the log n is going to be the same thing as infinity because if you think about it for a few seconds, the log n p norm, p is log n is up to constant factor. Is up to constant factor, it's equal to the infinity norm. So, the problem here we're going to study is if you have this endpoint in Rm with bounded L P norm, the question is what is the minimum constant Cp for which you always have these signs plus or minus one for which the norm is bounded by the Cp of Mn. So, that is the notion of vector balancing because you're trying to balance like. Vector balancing because you're trying to balance like a sum that for each other is small in this norm. So, yeah, it's sufficient to take m at least n. I think if m is less than n, then Bonash Kosek has shown that for actually any norm, not just a p-norm, the bound you get is up to a factor of two as worse as the m equals n bound. So that's nice. So that's nice. And also for Pico's 2, it's quite easy. You can just take random signs and you get that this constant is actually equal to square root n. It's tight because any orthogonal basis, orthonormal basis, will give this square root n. And more generally, if you take random signs, you can prove this upper bound of square root p minus 1 times n. So if you imagine p equals log n, which is kind of n n n n n n n n n n equals log n, which is kind of infinity, that is off by a factor of log n from the above bound. So there's also this theorem of Spencer that upper bounds this c to infinity constant by square root n log of m over n. So if you imagine that m equals n, that is matching the square root n that we had before without paying this square root log factor. So here the question is So here the question is whether this Cp upper bound can be improved similarly to what Spencer did. Spencer's theorem is, the way he proved it at least, is very specialized for the infinity norm, so it's not quite obvious how to do that. But our theorem here is that we can in fact get this upper bound, which nicely interpolates between the p equals 2 case and the between the p equals 2 case and the p equals infinity case. So when p equals 2, we match the square root n, and when p equals infinity, we match Spencer's theorem. And this is tight when m equals n because you can take the columns a1 through a n to be, if you imagine a half the Martin matrix, this will give you the square root n lower bound. So we're going to talk about the proof in a high level. About the proof in a high level here. I guess we're going to get into a little bit of detail, but if you don't fully understand, it's okay. So, the idea here is going to pick this P0 that is going to be actually at most P. And it's easy to see that we can upper bound the P0 norm of these vectors as well. And after that, we're going to combine the Kinchine bound of the random signs with another bound. random signs with another bound that's going to be a p to infinity kind of bound with this interpolation inequality which is actually quite nice i think this interpolation inequality that i wrote here so the bmp i didn't define but it's uh it's the lp ball so it's the set of points for which the norm the p norm is at most one and this interpolation and natural that i wrote here it's basically saying that if you Here it's basically saying that if you take two values of p and you can scale by a factor that is not depending on the dimension, then you can actually fit that intersection in the intermediate value of b the proof is quite simple of this inequality, but it if it's it's something that if you haven't seen it, it's quite uh hard to come up with, I think. Um I'm not gonna prove it, but um I'm not going to prove it, but we're just going to see how the Lp to infinity measure bound goes, and later we will combine this with this interpolation inequality to get the above theorem. Okay, so let's get to it. The kilometer is going to be this measure bound from Lp to L infinity. And what it says is that if you have this matrix, This matrix A, which encodes the columns, the input to our problem. And we can define this symmetric compact set K. And the result is that the measure lower bound is nice. It's like C to the N for some constant C greater than zero. And this will suffice to apply the theorem we had in the first part of the talk to find a partial coloring. Well, that's not exactly what I had promised. I promised a full coloring, but Promised, I promised a full coloring, but you know, it's not too hard to iterate that by paying only a constant factor, it's like a geometric sum, and then you get basically the same result for full coloring. So, how would we show this key lemma? Well, the idea here is that we're going to convert this information that we have about the columns. We know that the columns have L P norm at most one. We want to convert it to information about the rows of the matrix. Information about the rows of the matrix. Because this set K is kind of defined by inequalities. It's like a polytope. And it's defined by inequalities that are depending on the rows. So we really want to have information about the rows here. And once we do that, we can apply Sidak's lemma, which is saying that if you have a strip and you intersect any convex symmetric body S with that strip, the Gaussian measure is nicely lower bounded by the product. Bounded by the product. Once we know this, we can lower bound the measure of any strip. There is this nice inequality here that is true for any vector v. I'm not going to prove it, but it's like an elementary one-dimensional kind of thing. And the key inequality we're going to use is actually the next one. It is saying that we can nicely relate this to the power of p here with this previous one. With this previous one. So, if you just see this the first time, it's not gonna make any sense, and I'm not gonna prove it directly, but I will give you a picture. The key inequality here. So basically, we want to upper bound this weird function in the left side by a nice monomial. And we have to pay this factor of p to the p over q here, which is kind of like where the square root p is coming from after you take powers of 1 over p. Of one over p. Okay, so I'm not going to prove it, but I hope that this plot will vaguely convince you that something like this is true. Okay, so while we're still not done yet, we only got something like a square root p, whereas I promised you to have like a minimum of square root p and the log of m over n term. So we still have to do a little more work. So, we still have to do a little more work. And the idea here is that we took the liberty of choosing the speed zero, which was less than p, and we can actually optimize for it. So here I introduce this factor of m to the one over p0 minus one over p, because you know the assumption on the AI is that you only have p norm at most one, but the p0 norm is at most this m to the one over p0 minus one over p here. So we can show this result by combining. So we can show this result by combining this previous lemma from the p to infinity to the p to p. I didn't really write what Kinchine's inequality is, but it's just like a basic result for random signs in the p norm. And the idea here is that we can choose this p0 carefully, depending on the value of p, and that will nicely come out with this minimum of p and log of 2m over n. It's just some algebraic manipulation. It's just some algebraic manipulations that allows us to do this. Okay, so let's see what we conclude. So at first I wrote just a theorem from P to P, but actually we can get something a little more general. So here we have a main theorem that is saying that from P to Q norms, we can get this partial coloring is the main theorem, which is optimal in the sense that the Hadamard matrix. In the sense that the Hadamard matrix also satisfies this for any partial coloring of the Hadamard matrix, you need something like the right side. And if we really want to have a full coloring, then well, we're going to have to pay a little extra factor of this one over two minus one over p plus one over q term. And when p over q, that is just a constant. But if you imagine that p is like going low, like Like going low, like near q, and q is going to infinity, that's going to be a problem for us. So, really, the main uh main improvements here are when p equals q or p is like moderately small, like four, and q is equal to infinity. That's also okay. So, we're gonna see an application of this result to backviala. So, there is this old conjecture of So there is this old conjecture back in Fiala that says that any matrix with each column having at most t once, if it's a binary matrix that is, it has discrepancy at most square root t up to a constant. That's a very old conjecture. It's still wide open. But what we can get here is that if the value of t is at least n, so you could imagine that m is very large and n is kind of moderate in that setting, then yeah, we can. In that setting, then, yeah, we can get the conjecture. And it's quite simple. We just take p equals 4 and q equals infinity. 4 is just an arbitrary number, it could be like 3 or 5. And we get this nice bound. Well, you could ask, okay, what if t is less than n? Then, you know, you have to pick p a little more carefully, depending on the value of t, and you can get this upper bound of square root of t times log of n over t, roughly. And if t is very close. And if t is very close to n, that is an improvement. But if t is not that close to n, it's not as good as the other known upper bounds. I think it is known that you can get square root t log n, where log n is inside the square root. In particular, it would be quite nice if someone knows how to remove an extra factor of square root log of n over t from this result. I thought about it for some time and I don't know how. I don't know how. Okay, so here's some open problems. We have this generalized Humless conjecture. Well, I didn't talk about even the original Humless conjecture, but that is basically the case where P equals 2 and Q equals infinity. In that case, we expect to have just a constant upper bound. That's a very like a generalization of Bachphiala, actually. And here we propose this general version. And here we propose this general version of Comlish for any P and Q. I wrote it from Q to infinity, but it's not too hard to generalize from 1 to infinity. I just want to make it a little easier to read. So basically here, it's kind of similar to what we had, but it's not paying the factor of one over one over two minus one over P plus one over Q that we had from the no, I didn't really say where that was coming from, but it's basically. But it's basically you have some geometric sum that you have to pay that term based on the exponent of the geometric sum. It's not entirely interesting, but we hope that that is actually not there and we can actually remove it. And another open problem here is this. Actually, the reason I started working on this project was to solve this matrix Spencer conjecture, which is a generalization of Spencer. Which is a generalization of Spencer, not for LP norms, but for Schotten norms. And here we also expect something similar to HOD. It's a little more complicated. We don't expect quite the same result. In particular, we know we do not have the same result for matrices, but we expect something similar to hold. In particular, when m equals n, most of this mass disappears, and the right side is only square root n. Square root n. If you just look at it for a few seconds, you can see that when p equals q, we really do expect just to have a square root n on the right side. And the issue here is that the similar techniques don't seem to work. We might still want to use the first part of the talk, where if you have some kind of volume lower bound or Gaussian natural lower bound in the set defined by these matrices, then well, we can hope to get this result. Can hope to get a result, but so far we don't know how to show measure lower bounds for matrices as well as we do for vectors. I should also say that the COMLS conjecture does not hold if you just replace the vectors by matrices directly. There's a counterexample. Someone asked something. Let's see. Oh, yeah. So the major. Oh yeah, so the matrices, I wrote the same notation. It's a little bit of an abuse of notation, but when I write for a matrix big AI, the p-norm is basically the p-norm of the vector of eigenvalues or singular values, if you will, if they're not symmetric. That's like the shatter norm. So I mean, when AI is diagonal, we get the same result for vectors. So that's why it's like another. Factors. So that's why it's like a nice generalization. Okay. Yeah, that's it. Thanks for your attention. Thank you very much, Victor, for your talk. Again, questions, please use the chat or use the raising your hands button if you have any questions. I have a question. I just don't know how to raise my hand here. Yeah, go ahead. Okay. So very nice talk. So very nice talk. So I have a question. So you you have this C P Q and and I'm saying let's think of Q being infinity. Okay, so you just want to get an infinity bound. So the Communist conjecture is really about P when P is 2 and you expect to get a constant. And the original Beck Fealer result is is effectively with P being one and you get a constant. My question is, does your technique give you anything when P is between one and two? anything when p is between one and two and q is infinity leaving a bound that's better than the root log n that we know. So the question was whether for p equals from one to two we also get something and I think we do but it's not quite as good as Beckfiala original theorem. So if you look here in this second theorem for full coloring yeah if you imagine that p equals one then this max will be zero and the one over q will be kind of like And the one over q would be kind of like a log n when q equals log n. So we're paying a log n factor in that setting as well. So it's not improving the result for backfial. Okay, basically if p is very small and q is very large, we might as well just do whatever is known for backfial or commercial. I see. Okay, thank you. Other further questions. I could ask something. I also somehow lost where the raise hand button is. Sorry, I should know this. Why do you expect these expressions with min to be the right answer rather than, I don't know, maybe there's something more continuous here that's the right answer. Yeah, so the question is: why do we have this min like why is mean like why is that the right answer? Well for m equals n we know that it is the right answer because we have a lower bound for the Hademard matrix. In general if m is like n squared I don't know if that's the right answer. We tried for a while to construct a general lower bound. It doesn't seem to be that obvious but at least for m equals n, we know it's the right. It's the right result. Right, but then it's when M equals N, the whole main thing is just a constant, right? So it's okay, I see. Yeah, yeah. Yeah, I agree. So I guess like when Q equals infinity, we also know it's tight, right? Because I guess P equals infinity and Q equals infinity, we also have this log factor there. But yeah, it's a good question of like why is Yeah, it's a good question. Like, why is that the correct way to interpolate between the two regimes? It's not entirely clear to me. Yes, thanks. We have time for one more question, if there is one. If not, I would have one question, and that would be: do you get the same constants as in Spencer's sixth standard? As in Spencer's sixth standard deviation theorem, or do you get better or worse constants? So in the original theorem, Spencer, at least for m equals n, he did a lot of very careful calculations and he got the six square root n. I think in general, if m is greater than n, at least I could not find in his paper an explicit constant. I don't know if there is an explicit constant known in general. But here we are not too much concerned of that. We are not too much concerned with that. I think if we just actually compute it, it would be much worse than what Spencer got. Okay, great. So, thank you very much once again, Victor, for your very interesting talk today. Thank you. And I'll stop the recording.