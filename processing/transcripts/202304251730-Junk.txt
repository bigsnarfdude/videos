So in our question. So this talk will be a bit different from the talks that you've already had. I'm not going to talk about theory. I'm not going to talk about methods. I'm not going to talk about anything other than a challenge that we're going to give you where you That we're going to give you, where you come up with the methods and try to give me the answers, and I know the true answers, and I'll go grade your assignment homework and see if it's right. So, what I'm going to do first, where go down all the time, go down. So, okay, before I dive on into the challenge, since this is all about systematic uncertainty. Is all about systematic uncertainties. I have to define my nomenclature, which is a little bit different from Becca Sinervo's taxonomy of good, bad, and ugly, which was repeated by Nick Wardle yesterday. And in fact, I'm going to do something that's more like what Sarah said about the division of errors into statistical, systematic, and what she call irreducible, which is a funny word because we use the word irreducible in a different way. We use the word irreducible in a different way in physics, but it also doesn't mean that you can't reduce it, which is one of the jobs of the challenge, in fact, is to try to find a way to reduce the irreducible uncertainties. So the good ones are essentially statistical uncertainties that have been classified as systematic uncertainties. We physicists know that we're doing this and we're using sloppy words. So if you can constrain the Of the values of the systematic uncertainties with the data, then they have statistical uncertainties, and those are good. The bad ones are the ones where you have to come up with guesses, priors, and in this case, I've actually called what Frank calls ugly as merely bad, since if you go back to Pekka's taxonomy, it has, say, model assumptions and things that you have to guess. I actually call the values of theory nuisance parameters model assumptions. Parameters, model assumptions, very similar to model assumptions. That's bad in my taxonomy because there are things that are even uglier than that. And what's uglier than that are things that we have forgotten or neglected or have dismissed incorrectly. And that gets really, really ugly. And me as an experimentalist, I have to worry about designing detectors. It's my new job. I used to work on colliders. I now work on neutrino experiments that we haven't built yet. So we're prototyping. So we're prototyping them, and right now we're arguing back and forth in our reviews and with our funding agencies about ugly systematics. What have we forgotten? What have we not included in our estimations? So I was very ambitious in the challenge, and I wanted to include good, bad, and ugly components. But let me talk a little bit about the history so that this isn't the same challenge as you've seen before here at the bann meeting. Here at the band meetings. So, this one, the first challenge, which I'll call band challenge one, which wasn't called that at the time because they didn't know that we would have a couple more, Joel Heinrich, that's him over here. And I'm actually standing next to him in this slide back in 2006. So this was a simple on-off problem. It's very similar to the one that Bob was describing yesterday. It actually has one extra piece. You've got the main measurement. Piece. I've got the main measurement where ni is the observed number of data counts. It's Poisson distributed. It's got a signal in the background. The background just adds to the signal, and the signal has a detection efficiency epsilon. The background and the detection efficiencies were nuisance parameters, and they were constrained by subsidiary experiments, meaning that they are good uncertainties, and that Joel or And that Joel provided, what did Joel provided? Joel provided the measurements in the main experiment and the two subsidiary experiments, and he also provided these extrapolation factors, T and U, which related the number of events in the subsidiary experiments to the background and to the efficiency, respectively. So, this is already an issue because we have two good systems. Because we have two good systematics, but we also have a TNU that we know exactly. And there's no uncertainty on TNU, and that's unrealistic in a real experiment when we only know approximations to TNU, and often we rely either on theoretical calculations of Monte Carlos or data ratios or somethings to try to get the T's and U's, but Joel just gave us the T's and U's. So that made the problem much easier, and he was able to boil it down to essentially statistical. Boil it down to essentially a statistical exercise without worrying about the complications of systematic uncertainties. So, Joel was able to calculate everybody's coverage by giving us lots and lots of five tuples and then asking whether people could set upper limits and bounds on the parameter of interest. And it's Poisson, so we have these things called the dinosaur tail curves for coverage. And I won't really get into that because it's all described in Joel's. And Joel's write-up. His big conclusions are, of course, bugs are you bit with this problem. This is a computer exercise. Whenever anyone codes anything up, there will be bugs. So that's actually one of the values that are added by statistics committees on big experiments and reviews is that you find bugs in a lot of things. Coverage is well defined. Bayesian credibility is not so well defined. Is not so well defined. This was one of the things that Joel actually produced as a deliverable from his output. And of course, Bayesians need to worry about coverage, and frequentists should worry about credibility. People are afraid of empty intervals, for instance, which have no credibility. So there was a lot of discussion along those. Zero length is different from empty intervals. Sorry. Zero length. Thank you. Okay. Very good. So enough of bench challenge one. So I didn't want to repeat bench challenge one because even though it had nuisance parameters, it's not really some of the things that we've been talking about here. Bench challenge two actually looks a whole lot like things that we saw this morning. So Chad's talk and so there were three talks that So, there were three talks that essentially had plots that looked just like this. And this was a challenge that I put together back in 2010. So, this was two years before the discovery of the Higgs boson, and we already knew what the discovery of the Higgs boson would look like because I've been working on CDF, and we already had lots of data in these channels. And we were looking for Higgs to gamma-gamma. Of course, we didn't have the energy or the luminosity to do it, but we knew what the plots would look like. The plots would look like. So I put together a challenge that looked like that and challenged the people in different regions of this histogram, some which had lots and lots of events, some regions which had very few events, and I would put the signal over here, put the signal over here, and ask people if they could find it. There are actually two problems in Van Challenge 2. One was this bump on a falling exponential background, and this was another simplification because I told people that the background Because I told people that the background was exponential. So Roger tells us that, yes, we should not fit only polynomials, but try other functions. But I told people what the function was, which was a little bit simplification. So, in fact, I'm very glad that I didn't take this problem. I was considering taking this problem and generalizing it, making more complicated functions than simply exponential, and I'm asking you guys to figure out what those functions are or put uncertainties on it, because that's what you guys are. On it because that's what you guys are talking about this morning. But it turns out you have the data, which is much better than anything I could come up with. So let's not have me come up with new functions for fixed again and gamma-backer. Another thing that, the other problem was a more arbitrary shape. So this was meant to simulate an analysis where you have scores from, say, a neural network with a decision tree that separates signal from background. And the idea was some of these looked. Was some of these looked, this one has a lot of signal, a lot of them had very little. And the question was: could you calculate the right p-value when you didn't have the functional form, but instead had a Monte Carlo distribution? Again, I gave Monte Carlo distributions, and the coverage for that second exercise was not all that great. So, for challenge two, people should provide the power of the test, how often you discover and p-values for each data sample that I gave. Each data sample that I gave, yes, no discovery claim, location parameter, where's the bump, and how big is the bump. So, yeah, I already covered this, is that I gave people the functional form, which is something that I will not be doing in VAMP Challenge 3. And I made some of the signals, in fact, most of the data sets had no signal in them because I wanted to check the type 1 error rate, which means a lot of null hypothesis data had to be generated. Data had to be generated. So there were 20,000 unbinned data sets. And that's actually important for Banfie Challenge 3 because the whole scale of the problem is a bit different. So I can't actually ask for type 1 error rates with the way that I've constructed Bamp Challenge 3's data because there would be too much data required. But back to Banfield Challenge 2. So we made the score plots. How often did people get the right answer with the type 1 error rates? Type 1 error rates, the discovery probabilities, and the coverage for the location parameter in the signal string. So I'd like to thank Offer Dutels for summarizing all these things that I calculated. And I already mentioned that the second problem had a lot of undercoverage in the results. So instead of giving people the exact form, I gave them some Monte Carlo. And this caused some trouble because people parametrized that. They smoothed this function with arbitrary shapes, got it wrong, and some of these numbers. Wrong, and some of these numbers were undercoverage. Okay, so I guess I already told you how to win, bam, challenge two was that you had to have the right coverage, or the type one error rate could not exceed what I asked you guys not to exceed, and it was 1%, or the people in 2010, I asked them not to exceed 1%. So, among those, the ones that had the highest discovery rate would be the winner. So, something similar for challenge three. So, something similar for challenge three is in order. So, I didn't want to just repeat challenges one and two, and I wanted to have something with nuisance parameters that explore some of the techniques like template morphing and whatnot that can be applied this time. So, all the systematics and challenges one and two were good. I wanted to include some bad and ugly ones in order to mimic what we actually do as particle physics. What we actually do is as particle physicists. But I don't want to introduce a whole lot of domain knowledge. So, this is an exercise also for statisticians, and I don't want to have to describe all the physics processes. Instead, I need to boil it down to some abstract distributions with names like x and y. So, the idea is to give a two-dimensional feature space parameterized by x and y, so each collision in our detector. In our detector, it produces an X and a Y value. There's a background distribution for one process that say we're not interested in. And there's a signal here. And we're interested in this process. We're interested in the rate. So if you have a mixture of signal and background, what we want is how much signal there is in a sum of these. So that's what a sum looks like, where the signal is over here and the background is many other places. Many other places. So, this background, maybe I should motivate what this is, where the physics motivation comes from, is the W cross-section from way back in 2003. In fact, this is what Heka's note used as an example there. So the idea is that a W boson that's produced in a hadron collider sometimes will decay into a lepton and a neutrino. We measure the electron, we do not measure the neutrino, and there's some hadronic recoil. And there's some hadronic recoil. So each event kind of looks like this: where we see a very high-energy, say, electron or muon. We see missing energy, it recoils against nothing or very little. We see a bit of hydronic reclil, which has less energy. So the idea is that the background is jets. And a jet can fake an electron. And if you have a spray of hadrons in here, then the electron candidate will have other hadrons. Electron candidate will have other hadrons around it. So, one of the variables that we use to select electrons is whether there's extra stuff around the electron in this blue cone. If there's little stuff, then it looks signal-like. If there's a lot of stuff, then it looks less signal-like. So let's show you that plot again. So here's what the background looks like. This is the amount of stuff that's near the electron. This is the amount of missing energy. So if you've got a W boson. Got a W boson, then the missing energy should be high, and there should be very little stuff around the lepton. In jet events, you have less missing energy and a lot more stuff. So that's the physics motivation. But you don't need to know that in order to solve the problem. Can you just confirm that the funny vertical horizontal bands in the background pop that there's any artifact or something? That is a screen artifact. Okay, so that's not. That's an aliasing of the projector and like pans. Couldn't be if you zoom in on it. So you got different bandwidth. Zoom in on it. So you got different bandwidth. Okay, so just for the theorists, this is what, or theorists and statisticians, this is what a detector looks like. We measure the electrons and the tracking and the calorimeter, and we don't measure neutrinos at all. Okay, so what do I provide? I provide 100 sets of unlabeled data. Each set has about 100,000 XY pairs in it. They're of order 100,000 interaction, but they're Thousand interaction, but there is a big, broad variation from data set to data set, so you can't use the total amount of entries in a data set to tell you whether there's a signal present or not. Some of the data sets do not have any signal in them whatsoever. Originally, I wanted to do some p-value testing, but it turns out that 100,000 entries in these data sets prohibited giving you enough data sets to test the p-values well. And the reason that Oh, well. And the reason that there are 100,000 entries in each data set is because I want the systematic uncertainties to dominate over the statistical uncertainties. And we heard from earlier yesterday, or maybe even today, that if the statistical uncertainties are big, then people can stop estimating them working on their systematic uncertainties because they don't matter. And I want them to matter. So I'll give you a whole bunch of simulated data. That's why it's so big. So I give you not only. Whoops. Not only simulated data, but simulated Monte Carlo. So the Monte Carlo comes parametrized in terms of nuisance parameters. I don't give you all the nuisance parameters, there are some hidden ones, but there are some ones that. ones, but there are some ones that I give you alternate Monte Carlo sets. So there's a central Monte Carlo generator sample for the signal and one for the background, and you get three pairs of up and down systematic samples for each one. So these are kind of like two-point samples, but you've got up and down variations for Newsom parameters and no in-between, which means you may have to do some template morphing LL Lydia. So also not provided is I don't tell you what the true rate or shape information is for any. True rate or shape information is for any of these data samples, but I give you some Monte Carlo samples which are representative, draws from the Newsent parameters some of the Newsence parameters, others are actually ugly and hidden. And don't ask me. Okie dokie. So here's a list of the Monte Carlo simulated samples that I give. Actually, it's missing one. So background sample or background nuisance parameter one also has a minus one sequence. One also has a minus one sequence. So they come in pairs, except for this one. This one represents an alternative generator. So that's like a two-point, these are like three points where we've got the central sample sandwiched in between plus and minus variations of each of these Nuzence parameters. And then there's one that's just a one-sided probe alternate generator. All of these functions, by the way, are functions I just made up out of my hat. So they don't actually correspond to different generators. I'm just throwing them in. On the different generators, I'm just throwing random numbers. So, participants should provide point estimates for the signal strength for each data set and a 68% confidence level interval for the signal strengths. You shouldn't use anything from one data set to interpret another data set. They're generated from different values of the parameters. In order to win, your confidence intervals. Your confidence intervals should cover the true value 68% of the time. And I have a list of what the true values are on my laptop. And of those kind of, yeah, don't steal my laptop. And of those who cover the ones who have the shortest average interval length in the 100 data sets is taken to be the winner. So in a real experiment, what we do is we use the Real experiment: What we do is we use the Monte Carlo as an a priori estimate of our power of our techniques, and then the expected interval length is estimated from Monte Carlo rather than data because you only get one data set, and we want to know what the, say, not a random sample from actual data, but something that's average predicted. So, we usually use the Monte Carlo. But in this case, the Monte Carlo, you know what the true rate is. So, you can quote arbitrarily small intervals where the Monte Carlo. Small intervals for the Monte Carlo samples that I give you because they're labeled. You know which ones are signal and which ones are background. So that would be unfair. So I'm actually giving you the hidden data samples as the metric for calculating the average interval length in order to win the challenge. Can I ask a question? When you say the average interval length, since as far as I understand, you have different single sides per sample. Is it the absolute interval length or the relative? The absolute interval length or the relative interval length? I think it's going to be absolute. So, given that the statistical errors are going to be small, the intervals actually probably won't change too much in size from signal strength to signal strength. So, there's no root end in these. It should be all systematic. So I got a quick sorry. Sixty eight percent will nobody will give you sixty eight percent. So we need seventy s sixteen percent. People may have to be conservative. If you don't know the truth, you may have to guess. And that's what we do in real life. So we discard the solutions that have 67% of our Yeah, you know, for Bath Challenge 2, we were very generous with our awards. We said everyone did a real good job. When you say require 68, it sounds like you were saying requires at least 68. Yeah, at least 68. Yeah, if you do more than that. Am I going into the point? Do more than that. Why do I need a point estimate? You have to start the end of the beginning of the. Yeah, we're not grading you on your point estimate, but we'll make a plot of them. So you provide much more variations of one generator than the other. Is that right? Or did you already try? So the background. Chrome table. Yeah. Or is that not true? Okay. Yeah, that's true. So should I infer that the data was generated from generated functions well generator they're just made from different parameters settings and the I'm throwing random numbers from from functions so this other generator is really just another parameter setting for a parameter that's not varied in these these other things now it could have provided more example so the data comes from the generator yeah with all the parameters including maybe more right Maybe more. Now, maybe it's unidentifiable. Maybe that's the word if I have additional parameters that I don't sample here even discreetly. Okay, so some ways of looking at the data. I must confess I haven't actually tried the challenge myself because I know the answers, so it would be a little unfair, but it would be nice for me to actually at least try to do a close-minded. To do, close my eyes to the answer and see if I can measure some signals. But I can actually tell you that I can look at some of these histograms and tell that there are signals in them. So you can see the signals by eye. And one way that you can look at the data is just to make, say, a scatter plot of X versus Y, you make 2D histograms. Here are projections. There's a distribution of X, there's Y, and this is the average Y as a function of X. So, oh, I should tell you, because I didn't already, is that X and Y. Already, is that x and y in these samples are approximately uncorrelated. They're independent from each other separately for the signal in the background. So the idea behind the isolation and the missing energy is that they're supposed to have very little to do with each other for individual processes. Now, sums of distributions or mixtures of distributions that are uncorrelated can be correlated because if your signal is clumped up over here and your measure. Over here, and you measure a y down here, then you're more confident that x should be small if there's a mixture of signal and background. Now, this is just a background distribution. So, in fact, the average y is a function of x. I could have plotted the average x as a function y, would be flat because these distributions, in fact, are independent. And this is something that you can use to help separate signal from background and try to constrain these systematic concerns. This is also true for signal. This is also true for signal: is that the x and y values are independent from one another, but they have very different distributions from the backup. So here's a sample where there's a lot of signal in it. In fact, it's so much signal that the average y is a function of x is a big steep function. And if you look at it, there's a big spike near zero in y. So it's pretty clear from this sample that there's a big signal that... A big signal that stands out enough that you can measure. And I don't know what the right uncertainty to assign to that is because I haven't tried to challenge myself. Okay, so here's one way of measuring this. This is how CDF did it back in 2003 for Pekus Note, and that is what's called an ABCD method. If you've got two variables that are separately uncorrelated for the two processes, what you can do is you can chop up your space into four. Is you can chop up your space into four samples and a few extra padding regions in between because they might have mixtures of signal and background that you're not interested in. So these A, B, C, and D regions are meant to help control the background in the signal region. So the idea is that if there's very little signal in A and B and C, and all your signals over here, then what you can do is you can use the ratios. Do I have those ratios? Do we have those ratios? We should have those ratios. Yeah. So the idea is, okay, yeah. So A over B plays that role that Joel had for T. This is A over B, whoops. A over B should be. That A over B should be C over D for the background only. If A and B and C have mostly background, almost all background in them, then this ratio A over B can be applied to extrapolate the background rate from C down into the signal region. So that was the argument for using ABCD, is that if these are purified in background, you can use them to estimate the background here without knowing what the shape of the background is, just that it's independent in these two variables. Just that it's independent in these two variables. Now, there might be some signal that leaks into region B or region C, and that's why I'm providing these Monte Carlo samples. And one of the reasons why these cuts are placed the way they are is because this region here between B and D may have some signal in it as well as background. So these ratio method relies on the purity of background in there, and you'd actually have to subtract off some signal if you To subtract off some signal if you predict some signal in there using the Monte Carlo. So I provide some Monte Carlo, and you can use either KBCD or Template Fits or any technique that you choose. I do not recommend that you slavishly follow the almost the 20-year-old method that was used there, but use more modern techniques in order to estimate the signal and the uncertainties. Okay, so I've already described all of these things that are on this slide. All of these things that are on this slide. And I should mention that trivial multiplicative uncertainties are not part of the challenge. So you're supposed to figure out how much background is underneath the signal. But things that are typical in a real particle physics analysis, like the efficiency and the integrated luminosity, have systematic uncertainties that come from someplace else that are merely multiplicative factors. They would be just propagation if I added that to the challenge. So I'm not asking you to measure the effect. So I'm not asking you to measure the efficiency unless you place cuts, of course, on your analysis. Then you have to estimate the efficiency of your cut. Okay? Domain knowledge is crucial for real analyses. I boiled the problem down so that you don't need to know any domain knowledge, but a lot of real experiments require you to investigate other samples and think of other ways to get properties of the events to constrain the systematic uncertainties. Sometimes you actually have to go down. Sometimes you actually have to go down and wiggle the cables. And I'm not asking you to do that. If you're reading Pekka's note carefully, Louis reminded me that I should point out that Pecka's uncertainty formulas contain mathematical errors. He's treating the uncertainty on the background as a multiplicative uncertainty, when in fact it should be an additive uncertainty, and this term is also incorrect. So don't pay attention to that part of Peckett's term. Okay, so one of the concerns I had for the challenge was that it's either too easy or if it's too hard. So it might be too easy, but a p-value calculation would make it more interesting. So Pam Challenge 2 had a p-value calculation, but I couldn't figure out how to provide the requisite data sets. I could provide histograms within it, and that would make the data set smaller. If you guys want to do that, then I can provide many more random. Many more random data sets, if that's requested. Something with a bit more domain knowledge. So, for instance, the WMS measurement for CDF was an interesting recent example. We still don't know what's at the bottom of that discrepancy. And you already heard from Frank about the Z as a calibration sample for that to the P T spectrum of the Z. And they extrapolate that to the W. And I can't say that as well. I can't say that as well, as frankly. Here's a W mass measurement. We didn't actually, I didn't give you the W mass as a challenge exercise because there's a lot of domain knowledge and a lot of guessing that goes along in there. And I didn't want this to be a total guessing game. So the challenge data sets are on my Google Drive, as is the note, and the note is also uploaded to the Banff file list. File list. And okay, that contains documentation with instructions. They're tarred up with bzip2, so use this command on Unix or Mac to untar it if you're using Windows. I don't know. And good luck. Have fun. So what are you going to tell us about this connected cable today? So I can only speculate. I mean, I can tell you stories where people would say that in the elevator at Fermi Lab, you would have theorists and experimentalists, and the theorist was all excited about neutrinos that go faster than light and opera, and was about to write preprints explaining this model and why that could happen. And the experimentalist said, well, they should just check their cables. And I think one reason why a loose cable can make neutrinos look like they're going faster than light is because there might be a lot of light. Going faster than light is because there might have been resistance in the connection, and that RC time constants take time. There was an optical cable? Oh man. The cable connection either connects or it doesn't. Yeah. An optical cable if you plug it in and don't push it hard enough to get micro, but not enough. Well, if you have a resistive connector, it's very deadline. Yeah, that was the question earlier. Yeah, that was a question earlier. We gave something like six months or so for bank accounts too. And it took two years before I think we haven't set the deadlines. Let's have it by the end. I take Lego submissions, extra credit. I respond to Flat. My response is Flapp. My response is flat.