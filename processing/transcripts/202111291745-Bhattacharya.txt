Thank you very much for the invitation to speak in this workshop. So, I'm going to talk about recent work on coupling Markov-Chin-Monte Carlo algorithms for high-dimensional regression with shrinkage priors. This is joint work with Niloy Bishfas, Pierre Jacob, and James Chandrow. James Chandrow. So, just to provide some motivation, I've been in general recently interested in developing and fine tuning aspects of computational aspects of high-dimensional shrinkage priors. And in this work, in particular, we explore convergence diagnostics for some of the Gibbs samplers we have developed in the past using. We have developed in the past using coupling techniques. So, a very quick review of Bayesian regression, high-dimensional Bayesian regression with shrinkage priors. So, we shall concern ourselves with the case where the number of predictors is possibly much larger than the sample size available. And for simplicity, we shall consider the high-dimensional Gaussian regression model. Dimensional Gaussian regression model. So, our object of interest is this p-dimensional vector of regression coefficients beta. So, among various Bayesian approaches for conducting sparse regression, one prominent direction is using continuous shrinkage priors. So, these shrinkage priors were originally proposed as a continuous relaxation of A continuous relaxation of spike and slab mixture priors. Over time, with a lot of investigation into their empirical and theoretical properties, they have assumed their own identity. And now there is a substantial literature devoted to this so-called global local shrinkage priors. So the main idea is in setting up the Idea is in setting up the prior distribution, each component of beta has its own local precision parameter eta j, and also there is a global precision parameter xi. And the idea being that the global parameter controls the number of non-zero signals, whereas this low parameter signal is not non-zero signals, whereas these local shrinkage parameters eta j's provide the identity of the non-zero signals. So in particular, we shall concern ourselves with the half t family of priors on the local scale parameter, which is one over square root eta j. The usage of these hafty shrinkage priors dates back at least to Date back at least to Gelman 2006, but subsequently with the advent of the horseshoe prior, which corresponds to the case where the degree of freedom of the t-distribution is equal to one, they have been primarily used as the default choice. And one of the takeaway messages from this talk towards the end. This talk towards the end would be to suggest possible alternate values of for this degree of freedom parameter nu. So in particular, if we look at these prior distributions marginalizing out the local parameter, they are characterized by this infinite spike or pole near the origin and also. Near the origin and also heavy tails. So, this is how the marginal prior distribution of beta looks like, of any of the beta j looks like. And both this infinite spike near the origin and the heavy tails, so polynomial tails are important in delineating their optimal frequentist concentration properties, as has been found in the literature. In the literature. So, however, the presence of this infinite spike near the origin, as well as the heavy tails, brings some unique challenges to the computation. So, just to highlight that, here is a stylized example where we just have three predictor variables. So, the corresponding coefficients beta one, beta two, and beta three. One, beta two, and beta three. Now, if we plot the bivariate posterior distributions, the pairwise bivariate posteriors, we see some interesting features. For example, in each of these cases, we see that near the origin, the infinite pole, the infinite spike that was present in the prior distribution that's retained in the posterior, which of course can be is holes in general. In general, and in also some cases, the marginal and joint posteriors can be bimodal. For example, in the second panel here, so there is a mode at the origin and the second mode away from the origin. So, if you, in particular, looked at the marginal posterior of beta one, that would be bimodal. And these features are a very important thing. Features, I should also mention here that when p is greater than n, you can also show that there would be directions where the posterior distribution also retains the heavy tails of the prior distribution. So, this infinite spike near the origin and the heavy tails are translate from the prior to the posterior, and the necessitate some care in developing. Necessitate some care in developing computational algorithms. So for the horseshoe prior, which again corresponds to the case where the degree of freedom for the local scale parameter equals one, one prominent approach has been to develop blocked Gibbs samplers. So here it's a good. Here, it's a good point to just remind ourselves what all the state variables are, the parameters which we are going to sample. So, we have our p predictor variables, so the regression coefficients, the accompanying local precision parameters. So, again, P many of them, the global precision parameter, and the residual variance. So, at the least, there are The least, there are these 2p plus 2 parameters. Now, if you have more complicated models, for example, there are hierarchies or you have to augment latent variables that, of course, adds to the number of state variables in your Markov-Chin-Montekal algorithm. But at the least, there are these 2p plus 1 parameters that you have to be able to sample from. So, in a recent article that appeared in JMLR last year, we developed a blocked GIPS sampler extending earlier work by Nick Paulson and co-authors, where we developed a two-block Gibbs sampler. So, the whole idea was to block as much as possible to improve the mixing. In particular, the way we do the block. In particular, the way we do the blocking is we keep the local parameters in one block and put everything else in the other block. And this would be the sampler that we shall be focusing on throughout the rest of the talk when we develop coupling diagnostics. I'll just briefly go over the steps of the sampler. So, for sampling the local precision parameters, something very convenient. Something very convenient is that this factors as a product of, so that they're conditionally independent, the coordinates are conditionally independent. And now there are a number of possibilities. For example, Paulson and co-authors recommended a component-wise slice sampler. In this paper that I just mentioned, we developed an efficient rejection sampler to do the same. Can to do the same, and you can also actually carry out perfect sampling using inverse CDF. The inverse CDF CDF isn't a nice function, but the inversion can be carried out efficiently. So there are a number of possible alternatives that we can undertake for sampling the local parameters. And for sampling all the remaining parameters, we go through a sequence. We go through a sequence of marginalizations. So, for sampling the global parameter, we marginalize everything else out, beta and sigma square, and then perform a metropolis within Gibbs. And this, in particular, turned out to be quite practically useful in terms of improving the mixing behavior and limiting the autocorrelation in the chip. Limiting the autocorrelation in the chain. And then also when sampling sigma square, we marginalize out beta. So since this is a Gaussian model, all these marginalizations can be carried out in closed form. And finally, when sampling beta, you condition on everything else. And now this is a big p-dimensional multivariate Gaussian distribution. And Distribution. And although this is the covariance matrix is non-diagonal here, so you have a dense P by P covariance matrix exploiting the structure in the covariance matrix or the precision matrix, you can carry out this sampling exactly in order n square p steps, which is quite useful when p is much, much larger than n. And the details for that are And the details for that are in this previous paper from 2016. Okay, so that's a quick recap of the sampling algorithm. Now, just to show a highlight, so we carried out a number of simulations looking at various aspects of this block sampler. So I'm just going to show one aspect here where we look. One aspect here where we looked at the median effective sample size for a subset of the parameters while we crank up the dimension. And we found that they were fairly stable across increasing dimension and ranging from somewhere between 5,000 to 50,000. So for fairly large dimension, we're still getting. Dimension, we're still getting consistent effective sample sizes and also looked at statistical properties of the empirically statistical properties of the posterior mean and everything. And all of that looked good. Okay, so now I shall move into talking about coupling this Gibbs sampler, beginning with a bit of motivation and some background on coupling. Some background on coupling-based diagnostics. So, even with a lot of careful numerical linear algebra and optimizing all the steps of the previous sampler at GWAS scale, where you may have 100,000 predictors or much more, per iteration of the MCMC algorithm can take. The MCMC algorithm can take several seconds. So, any non-asymptotic insight on the number of iterations we need to perform is going to be highly valuable, particularly so that we do not waste computation even when the chain has converged or base analysis on chains that have not converged. And these, of course, depend on a number of different things: the signal-to-noise ratio. Things, the signal-to-noise ratio, the size of the data set, the sample size, and various other things, the prior choice. So the question is, how do we diagnose our MCMC path and the approach we are going to take in this talk, which is one of many possible. Which is one of many possible strategies for MCMC diagnostics, is these coupling-based strategies, which go back to work by Val Johnson in the 90s and has recently seen an re-emergence based on this work by Pierre Jacob and co-authors. And in particular, we shall be using this 2019 NIPS paper by Niloy and co-authors. Co-authors, which provides total variation bounds using coupling techniques. So, quick recap of some relevant definitions. So, a joint distribution is a coupling of distributions P and Q if marginally X follows P and Y follows. Marginally, X follows P and Y follows Q. And an important inequality is the coupling inequality, which tells us that the total variation distance between P and Q is always founded by, is less than or equal to the probability that X naught equals Y. This is true for any coupling of X and Y. And in particular, a maximal coupling achieves this bound. Maximal coupling achieves this bound. So, a maximal, what is a maximal coupling? Well, here is an illustration when P and Q are both continuous densities. So, this shaded area in green in between, this is one minus the total variation distance between P and Q, the area of the shaded region. And the way a maximal coupling proceeds, or one way to maximally couple them is. One way to maximally couple them is to draw both x and y and set both x and y to be equal to z, a random variable z drawn from a probability distribution h, which is basically the minimum of p and q normalized to be a probability density. So you can immediately see that when you sample from the maximal coupling, the probability that x equals y equals y is one minus alpha because the remaining thing they have equal zero probability of being exactly equal. And therefore, the probability that x is not equal to y equals the total variation distance between p and q. Okay and there are various ways of actually sampling from a maximal coupling. Here is one option where One option where all you need is the ability to sample from one of the two distributions and to be able to compute the likelihood ratios. So that, but that's just one way of sampling from a maximal coupling. Okay, so having talked about coupling of probability distributions, we shall now move on to talk about coupling of Markov chains. Of mark of chains. So suppose we are given a Markov transition kernel script P, which in our case, for example, could be the mark of the Gibbs sampler for all our state variables. And in this context, coupled chain refers to two chains, Xt and Yt, so two Markov chains, X T and Y T. Proceeding jointly, so that one marginally they both follow the prescribed MCMC algorithm. So, if I looked at any one of these two chains in isolation, there would be a perfectly valid sequence of samples from with stationary distribution or target distribution. And crucially, Crucially, there exists some random time tau such that the chains agree, the chains meet at this random time tau, and then they stay the same. So the two chains meet after a random number of iterations, and after the meet, they stay, they evolve together for all subsequent times. Okay, um, so here is an illustration of coupling Metropolis-Hastings algorithm. So, suppose in particular, I'm my target density is pi and we're using a proposal Q. So, one option, again, there are multiple ways of doing this. One way of coupling Metropolis-Hastings algorithm would be to maximally couple the proposals. Maximally couple the proposals, for example. So suppose the X chain is at X T minus one, Y chain is at Yd minus one. You maximally couple the proposals. And then at the accept-reject step, you, of course, need to compute this likelihood ratio, this acceptance ratio. And then what you can do is you can sample a single uniform random number and use that single uniform random number to decide accept or reject for. To decide accept or reject for both of these chains. And if we follow this prescription, it's very easy to see that since we are first proposing using maximal coupling, there is finite, there is a positive probability that x star is equal to y star. That's a property of the maximal coupling. And then at the next step, if both proposals are accepted, which again is being decided by the same. Which again is being decided by the single uniform random number u, then xt and yt meet, and once they meet, they evolve together. Okay, so that's an example. Now, couplings have, of course, been hugely successful as a theoretical device to characterize rates of convergence. And there is really large literature on this. I've just mentioned some. Uh, mentioned some representative papers here, um, and the key idea there is the coupling lemma, which tells us that if we begin with two starting measures, mu and mu, after as many steps, the total variation distance between the two chains can be bounded by the exceedance probability, so the probability that you haven't met until time S. Haven't met until time s. So, in particular, if we set the second one of these to be the invariant measure so that all subsequent iterates stay at the invariant measure, this gives us a total variation bound between the invariant measure, the target distribution, and the S-step transition kernel in terms of these exceedance probabilities. Now, this isn't immediately practical because to be able Practical because to be able to use this bound, we need to initiate the second chain at the invariant measure, which of course is what we are trying to sample from. So instead, there have been various approaches based on coupling, but relaxing this assumption that you need to initiate the second chain at the invariant measure to obtain practical upper bounds on the total variation distance. On the total variation distance between the Markov chain at some finite time and its stationary distribution. And the approach we're going to take is based on using these lag chains. In particular, this work by Nilo and co-authors that appeared in NeoRIPS in 2019. So in lag chains, the idea is you run these two chains, but at a lag. So you can think of your So, you can think of your time counters for these two chains are not set at the same time. So, once you have run the first chain for L many iterations, that's when you start the second chain. And then they just proceed like any usual coupled Markov chains. And using these lag chains, you can derive upper bounds on. Upper bounds on the total variation distance between the chain at time t and the invariant distribution, which is then, of course, can be immediately used for diagnostics purpose. And I won't go into the details of this, but what appears in the right-hand side here is again the meeting time. So the time when the first time when these lag chains meet. Okay. So now coming back to our setup, the blocked Gibbs sampler that I talked about can be with some work adapted to the entire half T family of priors on the local scale parameters. The question is, can we devise an efficient coupling strategy? Something that we can use that hopefully would meet relatively quickly so that we can then use that for. That we can then use that for diagnostics purpose. Okay, so I'm going to present two versions of coupling algorithms. So the main challenge here is that because we have this very high dimensional state space, so again, we have two times p plus two parameters where p could be easily in the tens of thousands or hundreds of thousands. So the main challenge is first being able. First, being able to come up with a coupling strategy that meets in a reasonable amount of time. So, I'm going to first present one such coupling strategy, and then we shall see, look at some of its theoretical properties and also see how it performs empirically. And looking at the empirical performance that would motivate sort of a more carefully designed coupling strategy, which Coupling strategy, which we shall call a two-scale coupling strategy. Okay, so some notation. So we shall call our entire state vector, so all two times p plus two parameters to be c, and we shall denote the two copies by c and c tilde. And remember, the way our sampler works is it's a two-block Gibbs sampler, which alternates between sampling the eta vector, which is p-dimensional. eta vector which is p dimensional and everything else which is p plus two dimensional and we shall call this everything else so the regression parameter sigma square and the global precision parameter to be z okay so um here is how the one scale coupling works well recall the sampling The sampling, the local scale parameters, given everything else in our sampler uses component-wise independent slice sampling. So, well, a natural idea is, well, now let's couple these, maximally couple these slice samplers or couple the, I shouldn't say maximally couple, we couple the steps, the pieces of the size slice sampler using maximum. Slice sampler using maximal coupling. And for the rest, again, sampling the global parameter, we use tupled metropolis Hastings, like the one I showed you a few minutes, a few slides back, sample the sigma square, the error variances from a maximal coupling of inverse gamma distributions. And so here. So, here is something interesting. We have these three-dimensional parameters which are sampled individually from multivariate Gaussians. And one of the things we've initially tried was to come up with the optimal Wasserstein coupling, but we weren't able to carry that out in N square P with N square P. With n square p computation, we're needing p-cubed computation, which would be prohibitive. So instead, we just used common random number coupling, meaning there are pieces here where we sample multivariate Gaussians, and we use the same set of p plus n independent Gaussian variables to sample both beta and beta tilde, the two copies. So this is how the coupled How the coupled the one-scale coupling works. A couple of important comments. So, one, we do this in a way so that the order n square p overall computing cost is maintained. So, so that the computing cost for the couple chain is roughly twice the cost of just running an individual sampler. Also, Also, since the last step uses common random numbers, an upshot of that is the two chains are allowed to meet exactly with positive probability at each step. However, that probability might be very small, and I shall come back to this point. And with some additional work, the slice sampling and metropolis hasting steps can be replaced by perfect sampling. By perfect sampling, but that adds to the computational complexity. So, some theory for this coupling strategy, looking at the marginal, so since it's a two-block Gibbs, two-block sampler using existing results, it's we're gonna have to look at the marginal Z chain. And we were able to show. And we were able to show that it has exponential the coupling times of exponential tails. This is important for us to be able to use some of these results from the lag chain literature. So, and the way this works is primarily by coming up with a drift function or a Lyapunov function for the marginal chain. And as an upshot of this analysis of the milling times of the coupling, you can show that the chain is actually geometrically ergodic, which is nice result for this class of half-T shrinkage priors. And this adds to a growing body of literature on geometric ergodicity of base shrinkage procedures, including a recent parallel work on horseshoe and regularization. Horseshoe and regularized variants. Okay, so with all that, now it's time to look at some simulations. So we set N to be the sample size to be 100, 10 non-zero signals, so everything else is zero, and we vary the dimension P and we set the signals according to a schedule so that they decrease. They decrease in magnitude. So, some of them are above the detection threshold, and some of them are around the detection threshold. And then we simulate these meeting times by cranking up the dimension. And something that we immediately observe is that these meeting times are, first of all, the mean or the median is shifting, so they're increasing, and also the tails are getting heavier. Tails are getting heavier. So, already at dimension 150, the meeting times are substantially right-shifted compared to at dimension 50. And this is not entirely surprising because even though we have previously showed exponential meeting times, it might have been there is little. There is literature, recent literature showing that this constant may have poor dimension dependence. And we are actually seeing that in practice in looking at this distribution of meeting times. By the way, these are based on 100 independent simulations. Right, so beyond, so scaling this up further, of course, further. Further, of course, further shifts the meaning times, and this becomes difficult to simulate four dimensions, thousand or higher. But we really want to do this in very high dimensions, much higher dimensions, at least tens of thousands or hundreds of thousand dimensions. So we need a better strategy. And to that end, what we end up proposing is something we call a two-scale coupling. A two-scale coupling, and as we shall see, this is significantly better empirical performance. And just to set ideas, I'm going to just focus on the two p-dimensional quantities, beta and eta, and assume the other two quantities, the two scalars, to be fixed. Although our actual implementation, of course, does update all the parameters. Okay, so. Okay, so there is only one modification we make to our one-scale coupling to get what we call our two-scale coupling. So the coupling for the beta, the regression parameter part remains exactly the same. So we continue to use common random numbers. However, when coupling the local parameters, etas, we take a different strategy. So I'm going to introduce a distance functional. So I'm going to introduce a distance function or a metric D. So if this metric between beta, between the two copies of beta exceed a certain threshold, instead of doing the one scale coupling that I showed previously, we just use common random number coupling. And when this distance falls below the threshold, we shall use our old one-scale coupling. So this is why. Scale coupling. So, this is why, because we are using two different coupling strategies depending on this threshold, we call this a two-scale coupling. So, natural questions, why do this and the choice of the metric. So, the main idea of doing this is when the two copies of beta, so again, remember these betas are p-dimensional quantities, and when these two copies are far. And when these two copies are far away, if we make an attempt for the local parameters to exactly meet at the next step, meaning all P of them meet, that's going to be wasteful because that has very small probability, something which goes down with increasing dimension. And in the one-scale coupling, the components that fail to meet evolve independently. So if If there is a failed meeting, the components just evolve independently, and this continues until all the coordinates meet, which again has small problem. So instead of trying to meet exactly or taking the gritty approach, what we do is when the copies are far away, we just use common random numbers with the hope that the two chains get close over multiple steps. And we don't have a Steps, and we have we don't have a complete theoretical understanding about this, but we do have some understanding, some theoretical backup for this. And only when the two copies of beta are closed, we attempt to meet exactly. And the choice of the metric, the metric we use is, again, we looked at various different possibilities, but ended up using the probability of an exact something that depends on the. An exact something that depends on the probability of exact meeting. So you use this d to be the probability that at least one coordinate of eta differs from eta tilde. And by the way, this quantity can be efficiently computed. So computing this in our setup at least is not a big problem. Although in situations where you cannot perform as You cannot perform as much analytic marginalization as we can. In this case, this could be more complicated to compute. And then, since this is a probability, choosing the threshold, well, we only can choose it between zero and one. And I'm going to show you that the choice of the threshold doesn't make a huge difference here. So we just use the default 0.5 cutoff. So here is the same setup I showed you a few slides back, but now we are showing the distributions of the meaning times from the one scale and two scale coupling. The two scale are this, the ones, the solid, the gray ones at the one scale and the other one is the two scale. And immediately we see that these are much more stable across increasing dimension compared to the Across increasing dimension compared to the one-scale coupling. So, this is encouraging. And also, by looking at different thresholds between 0.1 to 0.9, we see that the meaning times are, their distributions are very, very similar. So, this shows that the choice of the threshold isn't super important here as long as you don't choose it too close to zero or one. zero or one. So I'm going to skip some of these and move on to well just briefly mention that we do have a variant of the two-scale coupling that avoids an explicit distance metric. And it again shows very similar meeting time distribution. Time distribution to the two-scale coupling. But I won't get into details of that. So, one advantage of this alternative two-scale coupling is that we could prove exponential meeting times for this modified two-scale coupling, whereas for the original two-scale coupling, we have been able to show exponential meeting times. Able to show exponential milling damps. Okay, so in the remaining few minutes that I have, I'm going to come back to the point I made at the beginning of the talk about the choice of the degree of freedom. So again, the horseshoe corresponds to the degree of freedom being equal to one, and it has been overwhelmingly used in the literature. I think partly because there hasn't been a strong reason to There hasn't been a strong reason to use any value of mu other than one. So the theory, so there, the theory, the posterior concentration theory fails to distinguish between mu equals one and nu greater than one. And empirical performance for the horseshoe and degrees of freedom slightly greater than one are very similar and something that I'm going to show you in a few slides. However, something we found interestingly here. We found interestingly here was that, at least based on our approach, there seems to be a substantial difference in the meeting times between new equals one and values of new greater than one. While for values of new slightly greater than one, the empirical performance is essentially the same. And this motivates using values of mu slightly bigger than one in practice. One in practice. Of course, this needs much more careful study because everything we have are upper bounds. So there may be other couplings that work much better for the horseshoe corresponding to new equals one. But at least under our current setup, we found a big difference and some kind of phase transition phenomenon going on at new equals one versus new greater than one. Than one. Okay, so to illustrate that, we conduct this simulation where n equals 100, p equals 500. Once again, we have this 20 non-zero predictor variables and their magnitudes have this decreasing schedule. So some of them are well above the detection threshold, whereas some of them are very close to the detection threshold. So beta one, which is the largest coffee. Which is the largest coefficient, the value is four. See, if we look at the posterior marginal posterior densities across different values of nu, looks pretty similar. But now when you go to beta nine, which is a coefficient which is near the detection threshold, so the posterior shouldn't be sure about whether to classify that as a signal variable or a noise variable. And you expect the posterior distribution to be bimodal. The posterior distribution to be bimodal. See that as you increase the degree of freedom way greater than one, so maybe around four, six, eight, this second small mode at zero, sorry, this mode away from zero is not being picked up. So increasing the degree of freedom well beyond one has adverse statistical effects. But on the other hand, for values of slightly bigger than one, the posterior densities, the marginal posterior densities look fairly similar. Okay, but now we also look at MCMC aspect. So we use the coupled chains to diagnose convergence, and we get this upper bounds to the total variation distance using the technique in this Neurips paper. In this New RIPS paper by Nilo and co-authors. And very interestingly, what we find here is that the upper bound for new equals one sticks out from the rest. So for all values of new even slightly bigger than one, the degree of freedom, we see much rapid convergence of the total variation distance. So it gets smaller much faster, whereas for nu equals one, it goes down much slower. And that's a consequence of the meeting. a consequence of the meeting times for new equal one being much bigger on an average compared to new values of new greater than one. So one thing again to keep in mind here that this total variation distance here, these are what we are plotting here upper bounds. These meeting times are exact, but this does not rule out a different coupling strategy which would give better meeting times. But at least under our setup we see this Under our setup, we see this phase transition at new equals one. And for example, if we pick a value of new such as two degree of freedom, and we now crank up the dimension way higher, so even at 20,000, the meeting times, the distribution of the meeting times remain surprisingly similar across increasing dimension, which is clearly not the case for nu equals one under our. The case for new equals one under our setup. So, summary: new greater than one gives order of magnitude improvements in meeting time for our sampler, for our coupling strategy at least. And if you now compare statistical performance, you see that something which is expected as new gets. Something which is expected. As gets much bigger than one, the MSE starts deteriorating, which is not surprising. But for values of nu between one and two, the MSE seems fairly stable. So it's almost negligible difference between nu equals one and nu equals two. So this at least indicates that we should be considering, we should not just be limiting ourselves to the default choice. The default choice of equals one, but at least explore values of nu around one and try to understand better what's going on. And just as a final point, we applied this to a real data set, a GWAS data set, where we wanted to see if what we are seeing in the simulations is being replicated for the real data set. So this has almost 100,000 covariates. 100,000 covariates and roughly 2,200 samples, so much bigger than the simulations we conducted. And even at this scale, we find that at about 750 steps, the upper bound of the total variation distance is very, very small. So this is an optimistic message that even if Message that even in very high-dimensional problems, we may not need to run our Markov-Gen-Monikarl algorithm for an enormous number of steps to get close to stationary. So, of course, there are a number of interesting future directions here. One thing particularly interesting would be to see if the two-scale coupling can now aid the theoretical analysis and give us favorable. Favorable dimension-dependent geometric convergence results. Also, the two-scale coupling seems generalizable to related contexts and because the general strategy isn't really dependent on the specifics of the Of the particular algorithm here. The main idea is to do this, follow, do different strategies depending on where the regression parameters are, how far they are. And so that should be generalizable. And also investigate alternative coupling algorithms. So there is, of course, always scope for improvement in devising better coupling algorithms. So that's all. What I talked about today. The what I talked about today is mostly based on this paper, which is on archive, and there are some other related references. Thank you for your attention. Thank you very much, Anne Barbara for a very interesting talk. So we have one question here. I wanted to read for quite a while, but you didn't get to. Your point in using common random numbers to get near a coupling made me wonder if you could. Made me wonder if you could go directly to use quasi-random numbers until the point you reach coupling and then get back to regular random numbers so that you would speed up, you could possibly speed up the time of coupling by using quasi-random numbers. That's an interesting point. I haven't thought about that, but so when you say quasi-random numbers, what do you mean exactly? Generators, but I mean, maximum discrepancy series to randomness, well, to eliminate randomness. Yeah, that that that's uh that's a that's an That's a very interesting point. We haven't thought about that, but certainly, I mean, since the main goal there is to suppress the random behavior, that certainly seems something to investigate.