And I'm a first-year PhD student at University Paris-Dauphin under the supervision of Tony Cornelas and Cl√©ma Royer. Today I will be talking to you about my topic of research. So positive spanning sets and in particular positive case spanning sets. So the angle that I will have is more a theoretical angle. So the U is in DFO will actually be in the end of the presentation. Okay, so I've decided to split my presentation into three parts. To split my presentation into three parts. So, in the first part, I will talk about this. Sorry. Okay. Okay. So, in the first part, I will talk about post-displaying sets in general. So, I'm sure you all already know what they are, but I will still make some reminders about them. And at the end of the first part, I will actually show some less. Actually, show some less known properties about them. Then I will introduce the notion of positive case spanning sets. This notion, I think basically, none of them must already know what they are, or at least I hope so. And then I will introduce a new tool that is kind of similar to the cosine measure that can be used to study positive case spanning sets. So, but let's get started with positive spanning sets. First of all, what are they? Well, they are just sets of vectors that can span the whole inner space. Vectors that can span whole linear space with non-negative linear combinations, and a positive basis would just be a minimal positive spanning set. So, you have three examples on the slides, two positive bases and a PSS that is not minimal. And already there are some weird things happening because the two positive bases, they don't have the same cardinality. And we also have a PSS that is not minimal, yet it has the same cardinal as a basis. So, this never happens in linear algebra, but I guess in positive linear algebra. Positive linear algebra, things like this can happen. So, first thing we need to do if we want to understand positive spelling sets and positive basis is to understand how their cardinality works. So, a known fact about them is that PSS needs to have a cardinal of n plus one or more. This is kind of obvious because it is obviously larger than a linear basis. And then you can easily build a PSS of size n plus one. So, this is. Uh, this is uh, this bound is tight, and also you know that PSS can have an arbitrarily large size, but if you are looking at positive basis, the size cannot exceed 2n. So it is a bit harder to prove, so I will not go into details, but it's known for what it has been known for at least 60 years, 70 years, was proved by Davies, and I think it was even proved before him. More recently, we had a proof using linear programming that was made by Charles Aude. Maybe by Charles Odi. Okay, so now that we've looked at the cardinal, I will just show you a characterization of positive spanning sets. This based on geometry. So I will use hyperplanes. So the idea is that a set of vector is PSS if and only if it is well spread into space. So to say that more formally, you just say that for every open half space, you have at least one vector pointing in in this open half space. In this open half space. So I have two sets of vectors here. Well, clearly, the first one is not a PSS, and the second one is. But if I want to prove that the first one is not a PSS, I have a no certificate. And this certificate is given by this red hyperplane, because on the left side of this hyperplane, there's no vector. For the second set, well, I cannot conclude anything. I can just say it might be a PSS, I'm not sure. An equivalent characterization. An equivalent characterization is to just consider the normal vectors of these hyperplanes, and then I would just say that I am a PSS if and only if there are no vectors that have positive dot products with all of my elements. So here, this first blue vector has a negative dot product with all of the elements of the first state. So this is a certificate that this is not a PSS. And for the second set, well, I still cannot conclude anything. If I wanted to prove Conclude anything. If I wanted to prove that the second set is actually a PSS, I would need to use this property several times. Okay, so this is not the best way to prove it. Okay, so this is basically the end of what I wanted to show you about PSS that you already know, I think. And then I enter into the territory of things that are less known, I hope. So, first of all, PSS, they are linked to graphs. And how can you make this link? And how can you make this link? Well, to make this link, I need to define some notions about graphs. So I will define what is a strongly connected digraph. And a strongly connected digraph is just a directed graph in which there is a path, an oriented path between any pair of vertices. And it is minimally strongly connected if erasing any edge makes it lose this property. So you have two examples of minimally strongly connected. Of minimally strongly connected digraphs. Clearly, I can go from any vertex to any other, and if I delete any edge, I cannot do it anymore. Okay, the thing is that these graphs, they are well, they have already been studied in the past. And for instance, we already know their cardinality. So a minimally strongly connected digraph cannot have less edges than vertices. And the number of edges cannot be more than two times the number of vertices minus one. So why am I talking about this property? I'm talking about this property? Well, because this property should remind you of some other properties that I showed you earlier about positive basis. This is kind of similar to the cardinality of a positive basis. Just replace the E by the cardinal of the positive basis, and then you just replace cardinal of B by N plus one, and you get the exact same formula as the formula for the cardinality of a positive basis. So is that a coincidence? Is that a coincidence? No, it's not. And it is due to the fact that you can create positive phases using minimally strongly connected diagraphs. So, how do you do that? Well, the best way to explain is to show you an example. So, here on the slide, I drew a mini, many strongly connected digraph. And what I did is that in red, I put some edges in red, and these edges, they form a spanning tree for this graph. Tree for this graph. So this spelling tree can be seen as a basis for my graph. And since I can see it as a basis, well, I will represent it as an identity matrix, which is why I get this identity matrix on the right. And then also I have two other edges, which is why my matrix actually has two other columns. And these two other edges, if I want to represent them via the matrix, Via the matrix, I need to wonder how do I go from vertex 2 to vertex 5 only using red edges, for instance. Well, I don't need to use the red edge going from 3 to 4. I don't need to use the one going from 2 to 3, but I do need to use the two other red edges, but in the wrong direction, which is why this first edge, 2, 5, would be represented by a vector which has two negative components and two zeros. And it's the same idea for. And it's the same idea for the other edge. It only uses three edges, three red edges in the wrong direction. So I represent it with zero and three negative components. So the fact is that since I took a minimally strongly connected digraph, the metric that I created is positive basis. If I had taken a strongly connected digraph, not minimal, I would have gotten PSS, not minimal. And if I took any other graph, I would not get any. Will not get anything in particular. Okay, so this explains the link between graphs and positives. However, you still have to be careful because some positive spelling sets cannot be associated with thigh graphs in such a way. So here you have an example. But that's not really the point that I was trying to make here. I was not trying to tell you that graphs characterize PSS. I was more trying to show that studying the properties of graphs can be a good way to have some ideas on the properties. Have some ideas on the properties of PSSs. Doesn't mean that the proof, that finding a property on a graph immediately makes it true for PSS, but it can give you some idea on how to prove the property if it turns out to be true. So maybe you are not already convinced, but then I can give you another property of strongly connected digraphs. Basically, you can characterize them by saying that they are the only graphs, only digraphs that have what we call a near decomposition. What we call a near decomposition. An ear decomposition, I won't detail what it is, but if you try to translate it in linear algebra terms, you can actually adapt the proof and get results on the structure of PSS, which I put here. Basically, any PSS or any positive basis can be rewritten in this form. So I have an identity block, and then I get I get a special matrix n which has a negative Echland colon form. So I put the definition of negative colon Echelon form. It's a bit long, but to explain it, the best way to explain is to show examples. Basically, it's a matrix where every colon has three blocks of elements. In every colon, you have a negative block. It's going down from colon to colon. Above the negative block, you get whatever he wants. Whatever you want, it does three matter. But under the negative coefficients, you always have zeros. So every PSS can be, every positive basis can be put in this form, identity, and then a negative version column form. And if you have a PSS, well, you still get this form, and then you get some arbitrary columns under X. Which blocks? I mean, oh, yeah. Which blocks? I mean, yeah, the arbitrary blocks could be empty, potentially. The zero blocks could also be the only block that is not allowed to be empty is the negative block. Yeah, because the first colon, for instance, needs to start with a negative component. So there's always nothing about it. Same for the last colon, it needs to start with negative coefficients. So it cannot be, there's nothing behind it. Oh, underwritten. Underwritten input. So basically, we have characterized the structure of PSS now. So I think we are, I'm mostly done with what I want to talk about on PSS. So now I will introduce the notion of PKSS, positive case spending sets. First of all, what are they? Well, they're just a special kind of PSS in which you are allowed to delete, to erase K minus one elements, and you still have PSS at the end. So you have basically too many. So, you have basically too many elements. You have a PSS with too many elements, and you can delete some of them if you want. Um, and if it's minimal for if your set is minimal for this property, then it's called a positive k-basis. Okay, there are two ways that I can show you a positive two-basis. The first one, but I guess it's a bit shitting. I just take a positive basis and I duplicate it. And then, okay, I get a positive two basis. But I guess that's not the most interesting example. So, another one is just to consider a pentagon. Another one is just to consider a pentagon. A pentagon in R2 will always be a positive two basis. So, what I mean by that is that you put the origin at the center of the pentagon, you draw vectors from the origin to the vertices of the pentagon. Also, yes, a P1SS is equivalent to a PSS. So, I will show you some characterizations of positive case banning sets. The first one, well, the first one is actually quite similar to the Well, the first one is actually quite similar to the one we saw with PSSs. So every open half space must have at least k vectors of the PKSS on each side, in it, I mean. And okay, a similar characterization with normal vector to the hybrid plane. No vector must have a positive, must be close to having a positive dot product with all of the elements of your set, basically. Sets basically. So these two sets are positive to bases. So this is the pentagon I was talking about earlier. Now, if I want to show that they are positive to spelling sets, I will not be able to do it quickly with this characterization, but it will still be useful to illustrate how it works. So here I have two hyperplanes. Clearly, there are two vectors pointing on the left of the hyperplane and on the right. Here, it's the same, two above and two below. Two above and two below. And here I have two blue vectors and like have two elements of my first set that have negative dot products with this blue vector. So I do have, I didn't prove that I have a P2SS, but at least this is not a certificate that it is not a P2SS. Okay. Now, of course, PKSS can also be linked with graph theory. With graph theory. So, in basically the same way as PSS, just now you just instead of considering strongly connected digraphs, you consider k strongly connected digraphs, which are graphs for which there are k different paths between any pair of vertices, k disjoint paths, I mean, between any pair of vertices. And you would create a PKSS using such a graph in the same way that I created the PSS using a strongly. That I created the PSS using a strongly connected digraph before. Okay, so now if you want to know the cardinality of positive case spanning sets, we might be tempted to look at the cardinality of minimally strongly connected K strongly connected diagraph. So we know the cardinality of such graphs. The number of vertices is in between, the number of edges, I mean, is in between k times the number of vertices and 2 times k times the number of vertices minus 1. So an immediate corollary of this. So an immediate corollary of this is that if your PKSS is associated with such a graph, then its cardinal is in between Kn plus 1 and 2 Kn. There are two problems with that. First problem is that not every PKSS can be associated with digraph. And second problem is that we already know that the lower bounds do not work for all of the PKSS, since the pentagon already does. Since the pentagon already does not respect this term. So we have to change, if we want to find the bounds on the cardinality of positive k basis, we need to change the lower bound. So maybe we can find a better candidate, and we actually can. So if we have a positive k spanning set, we know that it spans, it linearly spans, I mean, the whole space. So I can already take n minus one linearly independent vectors of my set. I will consider the hyperplane. I will consider the hyperplane that is induced by these vectors. And then on every side of this hyperplane, I always have at least k vectors pointing out. So I know that I have, at the very least, 2k plus n minus 1 elements in my set. So this is a better lower bound, and it is actually tight. It's not that easy to prove, but it was proved by Marcus in 1984. So as for the upper bound, well, this Well, this 2Kn seems actually to be a very good candidate to be the upper bound for several reasons. First of all, because it works for positive k basis related to digraphs. Second of all, because if it's not correct, it's very hard to find a counterexample. And third of all, because we have a similar result that is actually true with what linear k basis. I won't detail what they are, but basically there's something similar. They are, but basically, there's something similar to positive k basis for which this works. So, there was a conjecture that was made, which is that a positive k basis always has a cardinality of 2kn or less. Problem is that this conjecture is actually false. It was disproved in 2009. Actually, there's like it was made by Marcus and he kind of did a fair, he kind of did the same thing as Fermat in a way that's. Did the same thing as Permat in a way that he wrote a paper about this conjecture? He said, I believe it is true. And then, when the paper was published, just before it was published, he added a sentence saying, Oh, as this goes to press, I realize that this conjecture is actually false and I have a counterexample. He did not give it. And then he died. And for 25 years, no one knew what was the counterexample. So, someone finally found it. So, that's great. Yeah, so I so the counterexample is really hard, so I won't give it to you. If you want to find the counterexample, the smallest one that is known, I think it's in dimension like 36 with k equals 18 or something like this. Of course, I cannot write it down. But if he wants to prove that this connector is false, I won't prove that it is false, but I can give you the tools that were used to prove, to disprove it. Tools that were used to prove to disprove it. So, the tools that were used are linked with Polytop theory. So, first of all, I need to define what is a Gale diagram of a matrix. Well, a Gale diagram of a matrix is just the transposed matrix of the basis of the kernel of the matrix. That's simply that. Okay, and now if you take, that's a bit weird. This theorem is a bit weird, but if you take the vertices of a convex printup and you represent it as a matrix, then the gay diagram of this matrix will always be. This matrix will always be a positive two-spanning set. So, this is a way to construct actually any positive two-spanning set. It can be constructed this way. And the fact is that, okay, maybe you could create, I guess, PKSS by just duplicating the elements of the P2SS that you create, but it's not very interesting. You can do better than that. You can just choose your protop wisely with some nice properties, and then you will get PKSSs or even positive K basis. SSs or even positive K basis, if you want. If you want, for instance, a positive two basis, you need to choose a polytop that has no universal vertex. Then you're sure that you will have a positive two basis. And if you want positive K basis, the properties to verify are a bit harder to explain, but we know them. So using this and using some known results about polytops, what's not was about to prove that was was about to prove that was managed to prove that the connector was false so he constructed a polytop of sorry he constructed a positive k basis of size I think it was n squared time k but he didn't prove that it was the best that we can do he he just proved that there was actually an upper bound a positive k basis cannot be arbitrarily large and the upper bound is quite huge compared to the size of the the positive k basis that you construct The positive k basis that he constructed. So we have k n squared, and he proved that it was not better than he constructed one of size k ah we have k n to the k and he constructed one of size k n squared. So probably this bound can be improved. But still, this theorem is actually correct as opposed to the conductor. These bounds are correct, just the upper one is probably not tight. Okay, now that I've Okay, now that I've talked about positive K spanning sets, we want to have a nice way to use it in direct T3 optimization. So, to do that, we want to use the cosine measure. So, what's the cosine measure of a set? Well, basically, so this is given by this formula, a min-max formula. Basically, this the cosine measure computes the distance between your sets and the vector that is the furthest away from your sets. And if your cosine measure is positive, then you are a positive spanning set. So you have three examples here where I computed the cosine measure of sets. There are some known results about positive about the cosine measure. And we know that for a positive basis, it cannot exceed one over square root of n if the positive basis is of maximal size. If it's minimal, it cannot exceed one over n. One over n, but and we also know that it's better to use uh PSSCs whose cosine measure is close to one if we want to use them in derivative pre-optimization. However, probably the cosine measure is not the best thing that we could use to to measure the efficiency of a positive case spanning set, considering that it at no point does the cosine measure take into account the fact that maybe the set is a PKSS. Fact that maybe the set is a PKSS. Like a PSS and a PKSS could actually have the exact same cosine measure. So we need to introduce a new tool, which is what we did. We introduced the K cosine measure of a set. So this is given by this ugly formula. Basically, a set is a PKSS if and only if the K cosine visual of the set is positive. And what it means, this formula. And what it means, this formula, is that you are a positive k-spanning set if and only if for every vector the k-c closest elements of your set are not far away from you. So this means that even if you delete k minus one elements, you still have one element, at least one element left that is not so far away from you. So that will still remain a good approximation of you. That's the idea of this formula. The idea of this formula. I know that this formula is a bit, okay, let's say, frightening at first sight. So I put an example of the computation of the two cosine measure of a set. So let's pretend that we don't know that this set is not a P2SS. It's kind of obvious that it's not a P2SS, because if we delete any colon, then we don't have a PSS anymore, clearly. But let's just pretend that we want to prove it formally using the two sign measure. Using the two cosine measure. So then we start by drawing the subsets of D of size two, all of them. And then we are trying to find a vector that is far away from all of these sets. Well, not really far away, but I mean, we have to find a vector for which in every set there's something far away from it. So, okay, here my blue vector is stuck to these first elements, but it is actually far away from this other element, so that's good. And if I so And if I so if I try to compute the two cosine misery sets, so I will actually realize that the minimum is realized by this vector V that I drew and the minimum is realized on which subset? Well, on several subsets, I guess, but on this one, for instance, and yeah, this one and this one. Well, I just chose one randomly, so this one. If I look at the distance. If I look at the distance, I mean, if I look at this set, I can deduce that the two cosine measure of my set is just zero, or at least it's zero or less, I guess, because I didn't prove that this vector v is actually the best way, one you can choose. Okay, to conclude about the K cosine measure, I can show you the K cosine measure of some sets. So, this is a tool that we introduced recently, so we still have a lot of things to discover about them. So, I just computed the K cost me. I just computed the k-cosameter of some sets in R2, but I believe that this could be easily generalized to Rn. Maybe I'm overconfident, I don't know, but we will see. And okay, basically, that was the point of my talk. I wanted to introduce, to talk to you about positive case spending sets and this new tool to measure their efficiency. So now, if I had to conclude my talk, and I will talk about the relative optimization here. The relative optimization here. So, what you have to remember is that positive case spanning sets are just a generalization of positive spanning sets. We don't know a lot about them because they have not been studied yet, but we can already guess that graph theory and especially polytop theory would be a good way to understand more about the properties of these sets. Also, if you want to use them in their So if he wants to use them in a derivative pre-optimization context, then maybe the cosine measure is not the only tool that you could use. Probably new tools should be introduced, for instance, the K-cosine measure. And so what would be the use in derivative pre-optimization? So this is just a perspective. We have not really thought too much about it yet. But we believe that, for instance, if we want to compute, if he wants to evaluate, if he wants to, sorry, I'm losing my words right now. My word right now. If you want to optimize a function by doing parallel function evaluations, then you would use several processors. Maybe some of them are strugglers. And the fact that you use a positive case planning set means that you don't really need to wait for the trackers to give you the evaluations of the function value. You can just wait until you get the first, well, let's say that the size of Well, let's say that the size of, well, you would just need to wait until you have n minus k plus one elements that are returned before you can conclude and go to the next step of your algorithm. Basically, that's all I wanted to tell you about.