Okay, I want to thank all the organizers for arranging this very nice conference, which I'm enjoying, hearing so many nice talks. I also apologize for not having made very beautiful slides, like almost 90% of you. And even those who wrote out these slides, I mean, they had this very nice way of doing things. Nice way of doing things, and I suppose I'm still in the last century when I have to write things and use a document camera. So, I'm sincerely apologizing. And also, since we are getting to the end of this conference, I did not want to, I suppose none of you want to see too many estimates and this and that. So, I have tried to keep this talk as simple as possible. And so, if some of you find And so, if some of you find this very elementary, then again, I'm very sorry. But this is what I am doing here. And so, this is a joint work with Alex Loganov, Alexandra Loganov, Eugenia Malinikova, and Dan Manguvi. And it's about a problem that I think goes back to about 30 years ago. Go. And I have been thinking on and off about this problem for that many years. And now I want to tell you what has been achieved. So I want to just tell you a few basic things about the Laplacian. I'm sure a lot of you already know these things, but it's helpful to recall some of these things. So, what I'm talking about here will be Will be smooth. So the operative word is smooth as opposed to real analytic. This is the key. And in effect, I don't really need smoothness. I mean, some C2 or something is going to work and all these theorems are going to remain valid under this sort of assumption. So I have a smooth compact manifold. And as you can see here, that I As you can see here, that I have no boundary, and I'm endowing this manifold with a smooth metric as opposed to real analytic. Because a lot of the theory that was done was for a real analytic metrics. And there is a very key difference between smooth and real analytic for multiple reasons in this subject. Okay, so as I say here, you don't need smoothness, some C5 is good. As some C5 is good enough, I think even C5 is even asking too much, you can do much better than C5, C3, or something. So, now what is the most important object on a compact Riemannian manifold? Most important object, I mean the fundamental operator. And the fundamental operator is the Laplacian, which in local some system of local coordinates. I have written it out for you. Written it out for you. Now, there are some important facts about this operator that I need to recall because that proves a little useful for our applications and for the corresponding theorems that I have in mind. And one of the most important results is proved by using either the heat kernel and its trace, or there is another approach by using the Approach by using the wave trace, which is pioneered by Hermander. So, the heat kernel approach is a Meenakshi Sundaram Pleel, which is a very important result because it also has connections with invariant theory and so on, and it has all sorts of things. And it leads one to what is called the Y law. Okay, so what is the Y law? It is a statistical law. That is, if you look at law that is if you look at uh the the uh the eigenfunctions uh associated to uh the eigenvalues associated to this laplace operator then they satisfy uh some sort of counting formula which says that lambda k less than lambda then you have some sort of relation like this omega n is the volume of the unit ball and then there are lower order terms and then none of this is of interest to to me as far as Interest to me as far as this talk goes, but I just want to point out there are lower order terms. And the behavior of the lower order terms essentially depends on ergodic properties of what are called billiards. And these sort of these asymptotics were investigated by Victor Ivry many, many years ago. And in particular, however, I just want to make two comments which might be useful for us, and that is the Lambda case. The lambda k is if you look at, so you take this to be equal to k, and then you solve, and you see here that the lambda k is given by means of this in the leading term. It's really given by k raised to the 2 over n. And then there are some other error terms which we don't bother. So this is when k goes to infinity. To infinity, and in particular, this formula when you look at dimension two, then you have this sort of relation. Okay, this is the formula when n is equal to two. I just want to point out that there is a lot of number theory also involved with these sort of results. And I just want to make a plug-in for a paper that will appear in the notices. It's by It's by my colleague Fioralva Ciacconi. And we had written an article for the volume for Carlos Kenning, which ties in number theory and this sort of relations and what are nowadays called transmission eigenvalues, which is a very important subject that has occurred in worse category. And we find that there are most of the results on transmission eigenvalues. Results on transmission eigenvalues were done on Rn. But in our work, we show that these things also have origins in number theory and on hyperbolic surfaces. And there is a relation between the scattering matrix and the counting and so on, and various number theory relations. An overview is to be found in the notices AMS article that will appear with Ciacconi, Colton, and others of our paper. So the last section is. So, the last section deals with it. So, this is just a plug. It has nothing to do with what I'm going to talk about about nodal sets. Okay. So, all right. Now, so now, so I want to define what are nodal sets, nodal domains, and so on. So, the point that I am going to, I'm looking at are actually these, yeah. Actually, these. Yeah, so I'm really thinking of k going to infinity, not the low eigenvalues. So I want to understand the behavior for large eigenvalues. This is really the focus of this talk. As a rule of thumb, the large eigenvalues have statistical properties, while the lower eigenvalues, geometry plays a very important role. As you know, that they are. Important role as you know that there are bounds for the first eigenvalue and so on, which rely very much on the geometry on the reach curvature and so on and so forth. So here what I'm looking at will be in principally less of this, but anyhow this is connected to what I'm saying. This is the nodal set. So it's the zero set of the eigenvalue of the eigenfunction. And also I'm Function and also I'm interested in the nodal domains, which will play a very important role in what I'm going to talk about. And these are the connected components of the set where the eigenfunction does not vanish. So those are called the nodal domains. And these are very delicate objects. And one should not think that by perturbation you will get everything. So I just want to remind you of a very important result of Karen Ullenbeck. Of a current Uhlenbeck, that if you sort of perturb the metric, so you perturbation means that you know, in some CK topology, you perturb, then there are some properties which are generic. So the perturbation can actually destroy certain things. So the nodal set is a smooth manifold. And the second point being that the eigen function is a Morse function, these are generic. So that means the metrics for which these means the metrics for which these hold they are essentially of the second category this is and what she and this is the way she calls this second category is she uses the word residual in her paper so so so yes it is true that most metrics the the more eigen eigenfunctions are Morse functions and for most metrics generically nodal set is smooth manifold there will be intersect Is smooth manifold, there will be intersections, of course, but otherwise, uh, in general, I mean, if you want to look at you might, I mean, just to say something is generic does not mean anything because you know, you might, if you are sort of parametrizing these things, might be there is like the rationals and you might, you know, which are which are also nowhere dense in some sense. Then you might hit something, some metric where things are not very dense. Things are not very good. So, perturbation is not the way you have to deal with nodal sets and nodal domains. They are very delicate things and they involve in most cases some difficult analysis. And so, they are very sensitive. This is my key point here. Okay. So, what is it I'm So now let's talk of some global result. And this is important because in some form, these global results already are making use of ideas which will play a prominent role later on. So I arrange my eigenvalues in this way. And so I have, these are all discrete. We know that because it's compact. Because it's compact, so compactness allows us to tell us that these eigenvalues are discrete. And then the key point here is that if I look at this ordering, then there is the notion of the kth eigenvalue. And associated to the kth eigenvalue, you have the kth eigenfunction. So there are multiplicities. So here you can just make something here, like you know, call it k. If there is a multiplicity, you can. K, if there is a multiplicity, you can just make some order. So, the key result here, the global result, is that if you look at the eigenfunctions uk corresponding to the kth eigenvalue, then this one will have at most k nodal domains, no more. And the proof is very interesting because of two points. So, the first point is that it uses a mini-max principle. It uses a mini-max principle and it proceeds by contradiction. But then there is one other important ingredient which shows up much later in the works of Pfefferman and Donnelly that I'm going to explain in a little while. And it is the notion of unique continuation. So unique continuation plays a very important role in what I'm going to say. And in effect, I mean, when people say Donnelly Pfefferman and When people say Donnelly, Pfefferman, and so on, I would like to point out that unique continuation was already used by the Quran. The point of Donnelly and Pfefferman is not that they are using unique continuation. They are using a quantitative unique continuation. But historically, unique continuation already appeared. So when people say, oh, they use unique continuation, I mean, I have to disagree. I have to say, no, this already appeared. So to emphasize this, So, to emphasize this, I want to go through this proof and to make sure that we understand that this has already this notion has already appeared. Okay, so yes, there are versions that I will talk about, but there are more refined versions of unique continuation. So, what is the proof of Quran? Okay, so the proof of Quran is that let's by contradiction. So, they assume they are k plus one nodal domains. Okay, so. plus one nodal domains okay so i'm going to label these nodal domains as omega one up to omega k plus one okay and now i take this kth eigenfunction and i restrict it to this nodal domain so i get a function which is u jk now let's say this is a nodal domain i'm restricting it to restricting this eigen function to this omega j so this is my ujk and so you see these are the nodal nodal domain Are the nodal lines or the nodal set which bounds this domain? So, in other words, this function is zero on that, and it is the same sign on each one. So, this ujk is the first eigenvalue for that nodal domain, because it's the first eigenvalue for that nodal domain. And so, so on the because it's a zero on the boundary. On the because it's a zero on the boundary, all right. First Dirichlet eigenvalue. So, what can I do? So, the proof is you form a linear combination of these ujks. Okay, so I make this function, I take these functions and such that I choose the first k of them. I know they are k plus one, I ignore the last one. So I look at only those on the first k. So, there are here. k so there are here there are only uh k um there are uh there are there are there are k um coefficients that i have here and so how do i determine the quick k coefficients i choose these k coefficients such that this function is orthogonal to the previous k minus one um eigenfunctions so they are k minus one equations and there are k coefficients to determine so there Determine. So therefore, I can find non-trivial values of aj for which all of this happens. Okay. And then there is a little bit of work. I don't want to get into it. So I just want to just hand wave that this H that you manufacture is actually an eigenfunction. And so it satisfies this relation. Okay, so there is some work, which is this is what I meant by the minimax that sort of argument that, and then there is some work you might say, okay, there are. There is some work you might say, okay. There are, you know, how do you extend it by zero and so on, so forth. There are some regularity issues to deal with, but more or less, this is the idea. So if you have an equation like this, notice what has happened. This function h that you constructed is actually zero on the k plus first nodal component because you see these functions were zero on the k plus first nodal component. And number two, it satisfies. And number two, it satisfies a PDE of this form, and therefore, you can apply unique continuation because it vanishes on an open set. So, you are actually applying the weakest form of unique continuation, which is really called weak unique continuation, as opposed to strong unique continuation, where you assume that it vanishes to, say, infinite order at a single point. You are actually here. The data is that the function is vanishing on an open set. And so you conclude. An open set, and so you conclude that this h has to be exactly zero. So you see here, unique continuation has already appeared in the proof of the Quran theorem. Okay, so now, of course, you want to make some of this very precise. I mean, you might want to say, do you really need vanishing to infinite order, or do you really need that it should vanish on an open set? After all, you ask this H satisfies an equation. Maybe. Maybe it doesn't need to vanish to infinite order. Maybe there is a certain finite, I mean, there is a certain finite order to which, if this vanishes, then H vanishes. And the question is, what is that finite order? And that, what is that finite order? If you think about it, has to do something with like, say, if H were a polynomial, it would mean that it should vanish to a little bit more than the degree. A little bit more than the degree. So, if you pretend that this H is a polynomial, and then the question is: what is the degree of that polynomial? Is it related to the vanishing order? And how do I detect that degree? And so these are the questions that I will answer because they have very important consequences for what I have to say. Okay. People nowadays call this quantitative, they have given it a fancy word, they call it quantitative curve. Word, they call it quantitative Carnival inequalities. Okay, so I don't know about these names, but you know, this is Carnival inequality, and it is some sort of precision is there, no doubt. Okay, so the next remark is that you see this, I said that this Quran theorem says that the number of nodal components should not exceed k. And what Archpliel observed. And what Arke Pleiel observed is that the number of nodal components is strictly less than k as k goes to infinity. I mean, you can never achieve k. It has to be strictly less than that. And so let me show you the proof because the proof is not hard. I mean, it's a proof that I can show you in 2D because it uses some very nice ideas that some of us have used. And I think even in the And I think even in the talk of Rupert, he mentioned something about symmetrization. And so that sort of appears here in the form of what is known as the Faber-Prand theorem. Okay, so let's say you have these nodal domains, and I pointed out to you with the reason that each nodal domain is the first, the lambda k for each nodal domain becomes. For each nodal domain, becomes the first eigenvalue for that lamp for that nodal domain. So now, if you apply Faber Kran to each nodal component, so what is Faber Kran going to tell me? Fabrican is going to tell me that the first eigenvalue, which is the lambda k, must be bigger than equal to the first eigenvalue for the ball with the same volume as the nodal domain. And so what that statement of Faber Khan simply translates. Of power trans simply translates in 2D. So, this I'm working in 2D to pi times sigma squared over omega j. So, what is this sigma? So, this is the this sigma has to do with the first eigenvalue of the unit ball in or the unit disk in 2D. And so it is the smallest positive zero of this Bessel function of order zero. That's that that first eigenvalue problem for the disk. Okay. For the disk. Okay. And then you have to divide by the volume of the omega j. Okay. So now, so now you sum up, you assume that there are k components, like curant, you assume there are k components. So you sum this nodal domain by nodal domain. So when you sum it, and this summation is also important because it comes up again in what I'm going to tell you. So if you sum up here, you're going to. So, if you sum up here, you're going to get the volume of omega. So, I'm assuming here that my domain is actually omega in R2. So, I'm not in a manifold or anything. I mean, even much more simpler situation. So, move this this side. This comes downstairs. Sum up. The sum over the volumes of omega j is bounded by the volume of omega. And here you have lambda k. And here you have pi sigma square k. Okay, so this is what you get when you. Okay, so this is what you get when you sum these inequalities up for every component. And now you use the Y law for lambda k. So the Y law, if I recall for you, was given by, so here's the Y law. So the Y law tells you that it's 4 pi K over volume of omega. So that's lambda K. And I'm assuming K is big. So now you see on the right side, you summed up all the K nodal components. You summed up all the k-nodal components, so you got kÏ€ sigma squared, and you do all the algebra, and you end up getting that this sigma squared has to be less than or equal to this four because everything else cancels. But this sigma is 2.4, as you can see by tables or Watson's book or whatever. So then that's a contradiction. So that means that in general, you cannot have K achieve. K achieve for at least domains in R2, and more or less, this is true for manifolds. And in fact, there are even more dramatic results of this nature. So what is the more dramatic result of this nature? Well, I want to remind you that there is a theorem of Urakawa, which says on S3, there is a metric so that if you look at lambda 2, which is the next one after 0, 0 is the lowest one on a compact manifold. The lowest one on a compact manifold, then this lambda two has multiplicity of eight. So, so in you know that in for the standard one, it has four because you already use these sort of things in cast down warner, but here I'm in S3. So, here are the ones for the on with the standard one. But here, this one has eight. And this Yurakawa result is related to this other very striking result of Berard, Bergeri, and Burgigno that says that on the That says that on the on compact man, there are compact manifolds with metrics so that if you go far enough, then the number of nodal components is exactly two. So instead of having k or less than k, you actually have only two nodal domains for all the eigenfunctions. So this so this is the sort of behavior that you can have. Okay, so now I come to start. To I come to start coming to what I really am interested in, which is the local Quran theorem and Yaus conjecture. Because this Yaus conjecture plays a very important, in some sense, a role in what I have to say. So here let me remind you again that NK is the nodal set. And so the Yaus conjecture says that there exists C1, C2 positive, finite. Finite, but I'm not sure whether the way Yao's conjecture is stated, whether he meant that this C1 and C2 are dependent on dimension. I think he does not say anything because this is still a problem whether these constants depend on the dimension. Because as far as I understand, the way people have obtained results in this direction is that C1 and C2 depends on the injectivity radius of the manifold and so on. So, and yeah, I don't think. So, and Yao does not, I don't think the way I have checked whatever places he has written this conjecture, and he does not completely say what this C1 and C2 should be, whether they should be just dependent on the dimension or not. But the conjecture is that the n minus one dimensional housed of measure of this set is bounded above and below by root lambda t. This is the conjecture, and so the idea is that if you take, for example, this square. example this square side length pi then here are here is some some eigenfunction I have written down and then you can see that the the the the the the lengths of of of these components you see uh so you see there are there are there are l of them here and k of them on the other side so total length is l plus k and you can see the eigenvalue is l square plus k square so one of them is bigger than the other so you and Is bigger than the other, so you and so, therefore, you see where this conjecture is coming from. So, incidentally, here you see a little bit of also the number theory because here the eigenvalues are exactly which numbers can be written as the sum of two square. So, you see, number theory is tied in already at some elementary level, because this has to do with the primes and so on, with primes of the form 4k plus 1 or whatever. And the number of ways, the degeneracies, which what physics call the multiplicity has to also deal with how many ways can. To also deal with how many ways can you write a number as a sum of these squares, okay? So, uh, so, and so, so, an important remark is that you know, in the previous example, you know, you might have, you saw the nodal domains and nodal components, but when you write down a linear combination, then it is possible that the number of nodal domains of this linear combination is more than the number of nodal domains of any particular member of this sum. This could happen, okay. Now, what is Okay, now what is known about Yaus conjecture? So, the point here is that if M is real analytic and the metric is real analytic, then everything is true. Okay. Now, for smooth M and G, there is a result by Bob Hart and Leon Simon, which is that this thing is like lambda to the lambda to the lambda. So, for most practical purposes, all you can, this is saying is that it's finite. Is that it's finite because this is a gigantic number which is which is nowhere anywhere near what this conjecture is actually stating. There are some special results in 2D. For example, Nadirashvili showed that this is no more than lambda k to the three quarters. And also, I want to point out that in 2D, you can also bring into play complex analysis. Like even in our results, Like, even in our results, you can into the use some complex analysis. People had tried to use it in the past, and you can get some results, but in higher dimensions, no, I mean, you cannot bring in any complex analysis there. Okay, now comes this very striking works of Loganov and Malinikova, who managed to show that this n minus one-dimensional house-to-measure is bounded. Hausdorff measure is bounded, you get the lower bound exactly as like Yao said, but this is for smooth case. So I should say here M G is smooth. Smooth does not mean you really need smooth. You can actually do with the C some C2 or C force regularity. But the main point is that they don't need real analytic. So if you, so in other words, look at the upper bound. This is this is a control. This is a fantastic improvement over what Hart and Simon have done. And in fact, in the lower bound, they actually achieve whatever has been conjectured. So now there is still a question that remains. And that is, are nodal domains thin and narrow? So you can look at one nodal domain and I will make these pictures a little bit more clear. And this is one of the reasons I decided to write this out because too many pictures here. Because there's too many pictures here. So, when you ask this question, that are nodal domains thin and narrow, this leads you to what is called a local Puran theorem. So, let me explain in the one-dimensional case. This is the easiest case. So, here you have the classical Stern-Leobill problem, and you know that this is sine kx. And here are here is the graph of your function. And if you look, there are indeed k nodal domains. But now, let's look at this little piece here. This little piece here. When you look at this little piece, you only see two components. So, indeed, globally, there are k, but if you look locally, you only see two. Okay, so you say, oh, this is a 1D, and maybe it's nonsense. No, it happens in higher D. And if you are careful enough, that is the situation in all D. Okay, so now in 2D. So, 2D, you just look at this problem. You just look at this problem, you plug in this separation of variables, you get this nice equation. Everybody knows it's the Bessel function equation, and you get some equation like that. And then you get this classic Pizza Pie picture. So here you will notice that if you take this point, then if you have lambda k is like case k squared, right? So you are going to get here that the number of nodal components. Here, that the number of nodal components is roughly of the order of the square root of the eigenvalue. So it is roughly the number of nodal components is square root of lambda k. And so what is this disk that you are looking at? Well, roughly this disk is of the size 1 over root lambda k, which is exactly what you saw here. Because you see, this thing here, this distance here, is less than 1 over k. So because here's K. So, because here's the distance between the peaks or the nodes, so you have to be inside this distance to see this two. I mean, if you take bigger than that, then you'll see many more. But so you have to be inside that distance, which is one over root lambda k. So, inside this ball of radius one over root lambda k, you see square root lambda k. Not a constant anymore, but you see square root lambda k. Okay, so there is. Okay, so there is some formula like that which works. So, what is going on? So, this sort of question was posed by Donnelly and Pfefferman 30 odd years ago. What is the local picture of the Quran theorem? So, I'm going to, I'm going to, so, so what can happen is that here is what I'm asking. So, here I have a disc, okay, and I have the middle half of this ball on this manifold, and I On this manifold. And I take a global domain or other domain. So any global domain can, in principle, go in and out, in and out, in and out many times into this disk. So you see, here you see these components. So I'm interested in those components which go and hit the middle half. Because otherwise, if you adjust this ball so that it just meets it like that, then you see it just meets it like that. Then you see, it just meets it like that, and you're looking at this. Then it's nonsensical. So, I'm interested in those components that go and hit the middle half of this ball. And I want to know what is the volume of this. Is it substantial or is it like thin and narrow so that you don't get enough volume? So, so I want to say that no, this is a really fat piece. It's not going to be something like this. It's not going to be like, even if it goes and hits the middle half, And it hits the middle half, it's not going to be like that. There is going to be substantial volume for those nodal domains that go and hit the center half of this. And this sort of result that there is substantial volume, then you add up all these volumes. If you add up all these volumes, then you would be able to compare it with the volume of the whole ball, just like the sort of thing we were doing in the player result. We summed up. And then you will be able to get how many components. Be able to get how many components there are. And that is exactly my goal here. What is really the result? So, this is what Donnelly and Fairforman did in 1989, 1990. But I'm going to tell you a result that was proved soon after theirs. And then I'm going to tell you what they did. And then you can compare. So after their result, we, Mackenhauf and myself, we proved that if you have, now here everything is smooth from now on, not real analytics. Down, not real analytic. So, and also when I say smooth, I don't need C infinity. I mean, I can get by with C2, C3, and so on. So, if I take one of these domains, okay, which intersect the middle half of a ball, so B is any ball on this manifold, then I find that the ratio of these two volumes is bounded below by this number, lambda k to the minus 2n squared minus n over 2 log lambda k. So, you can forget this log lambda k. So, you can forget this log lambda k because this really is the dominant term. Okay, so now I do what player did and I sum up this over all these components which hit the middle half. Let's see what happens. Okay, so I sum up, okay, and that's controlled by the volume of the ball. On this side, I sum up, and this here is the n, the number of components. Remember, this was omega j divided by b. Divided by B. So you see omega j divided by b so and I sum up the inequality. So this b that you see on the right side is is is coming from this theorem. Okay. And now you see this is reminding one very strongly of Leil. Okay. And then you see that the number should be this. Now, if you look at the y law, you plug the y law in, you see the number of components locally is growing like k to the Is growing like k to the 4n. It's not growing like k. I mean, so it's worse than the global Quran, and it is, but definitely there is a control that you cannot locally have more than this. Now, what did they do? They did k to the n squared. So there is an improvement from their result to our result from 30 years ago. And that is from n squared, we went to n. Okay. Okay, so okay, that's so so what is it that one needs in these proof? So, in the proof, I'm talking about our proof, one needed a quantitative Parliament inequality, okay? Because this, as I said, has to do with the order of vanishing. And at least to my knowledge, there was some combinatorial aspect that came in. And this combinatorial. And this combinatorial aspect that came in was in the form of a covering lemma. So, we all know what covering lemmas are. I mean, the most useful one that we know of is the Vitali covering lemma. And in fact, what we needed was an improved version of this very simple measure theory, Vitalik covering lemma. And it's quite interesting to notice that in Loganov and Malinikova's proofs, you also have lots of combinatorial tools. Lots of combinatorial tools in addition to analytic tools. So it is not just analysis that is helping you. I will talk a little bit about some of these tools, but at this point, let me just point out that the quantitative Parliament inequality looks like this. This is not the quantitative Parleman. This is a consequence. This is a corollary. That is, so this should be UK. So you have the same function. So the eigenfunctions, if you integrate. functions if you integrate in L2 on a ball of this radius, delta is a ball, and you see this one over root lambda k, which runs to this theory. So this on a bigger ball is controlled by the L2 norm on a smaller ball. So essentially, I'm pretending, one can, I'm not pretending, it's good to pretend that eigenfunctions are essentially polynomials of degree root lambda k. This is a very good. Okay, this is a very good model to think about eigenfunct. Okay, and in fact, this model can be made very precise in the real analytic case, and it is also now made very precise in the smooth case. Now, what is this covering lemma I was talking about? So let me just tell you this covering lemma. It was improved in the thesis of my student a tiny bit, but that improvement did not help in the K to the 4N. In the k to the 4n, it may have given k to the 4n minus epsilon, but the k to the 4n remained for at least a very long time since the 1990s. Okay, so the covering lemma is that you take a set E and you cover it by balls, okay, M balls, then you can extract a sub-collection Dij, so that you now can want to increase the radius by one plus delta. In the classical vitality, you increase. In the classical vitality, you increase this by five or six or something. And then you say, okay, these balls can be made disjoint. But here, because you have only increased it by one plus delta, you cannot anymore select the balls to be disjoint. You can have bounded overlaps. And this tells you, the next statement tells you that the number of overlaps is controlled by delta to the minus n. And then the question is: is this best possible and so on? And my students said, And so on, and my students said, Well, you can do delta to the minus n plus one or some such thing or plus epsilon. Gauge and Lu, he, and that was part of his thesis. That's his name. Gauge and Lu. Now, what is the nature? Why is why are we doing all of this? So, as I said, the reason we want to do this is that we want to expose that essentially eigenfunctions are poinomials of degree root. Are enomials of degree root lambda. So that is, you know, so in other words, if you vanish faster than root lambda, then your eigenfunction vanishes. This would be the way you would use quantitative Parliament inequalities. So how do you detect something is a polynomial of degree m okay, so here is a detection technique that I can think of. What you do is, let's say you have a polynomial, so pretend it's some monomial and it has a certain degree in this case. And it has a certain degree, in this case, root lambda k. So you want to detect this. Well, one way to detect it is to take the logarithm. If you take the logarithm, then this comes in front and you're left with a function log x. So what's a nice space to detect something is a function like log x. Well, if you're a harmonic analyst, you say, hey, the only function I know, which is a very nice function in BMO, is log mod x. So you would say, okay, I take this and I compute the BMO norm. This is the Compute the BMO norm. This is just a constant, and this thing is a standard BMO function and whose norm is like 100, something like that. So that means what? That means you go back to your eigenfunction, you take the log and compute the BMO norm of that. And whatever BMO norm you see, that should be this guy here. And so in our paper, this is the main result that we proved, that if you take the BMO norm of log of the eigenfunction, then this is exactly what you're going to see. Then, this is exactly what you're going to see. Okay, so this is the heart of the proof, and then there is something else, and then, and so then, and then that gives you the result. So, okay, this was all fine and good, but then, you know, if you talk to Charlie about it, Feverman, the first thing he would tell you is to make you extremely depressed. And the first thing he said, well, okay, but then, you know, this is still dependent on the dimension. So, okay, you got it down. So, okay, you got it down from n squared to n, but you know, so Charlie said to me, I had a conversation, he said, Well, you still don't have it independent of n, you know. Okay, so there, so that was it. And the subject essentially, as far as I know, went into a coma because it was just too hard for me to do anything. I tried other things. So, let me tell you some things I tried, and of course. And of course, I knew it's related to Yao's conjecture. So, one of the things I tried was to do some LP version of Carl. Why LP version? Because I had success using an LP theory of Carloman to solve Calderon's inverse problem. So that was the success I had in 91 when Gunther Ullmann had done V L infinity to detect through the Dirichlet-Neumann map the potential. Dirichlet to Neumann map the potential, and by using Lp Carliman inequalities in the spirit of Jerryson and Kenny, I could get down to the optimal theorem V Ln over two. So I said, hey, everybody is using L2 Carliman because why? Because they can do integration by parts and Arenshein's method to get the Carliman. But now, you know, as a harmonic analyst, we know much more. We know all these restriction theorems and so on. So maybe I should be able to apply LP Carleman to this problem. Carliman to this problem, and because there it got to the optimal one, maybe it was. No, it didn't, nothing happened. You know, you can do all of that, but it's not going to do, not going to do anything. And then there's a reason because I should have noticed that combinatorial ideas are also important here. It's not just doing the Carliman improving it. So, this is the reason. So, and so essentially, And so essentially, I mean, I had no ideas for all these years. And then comes the work of Lavonovo and Malinikova, which is actually a major breakthrough. So now, what? One can use whatever their tools are, and you can then prove the following result. And here is the theorem. Okay. And the theorem is that if you The theorem is that if you take omega i, which is a ball, which is in a ball of radius 2r, so this is one of the nodal domains, connected components, and you assume, as I said, that omega i intersects dr. Then you can prove this lower bound for the volume, okay, for that component. And now, if you plug this in player style, you have to be a little careful here. Why? Little careful here. Why? Because if you just do it the way I told you, you will pick up a log factor. So if you want to remove this log factor in this conclusion, then you have to a little bit more clever of going from here to here. So I don't want to get into it. If you are interested, you can read the paper, which has been on the archive since August. And you see that the number of components is exactly that. And here is the And here is the transition to the full global Courant theorem. And that's optimal because you plug in n equals one, you see constant. You plug in n equals two, you see root k. You see everything, all the, and you do it, you do it for any dimension. That's it. That's the correct value. Okay. And again, this c1 and c2, these constants are independent of k, of course, otherwise it's stupid. All right. It's independent of the ball. Otherwise, if you It's independent of the ball, otherwise, if it's not, it's a stupid theorem, but it does depend on M as far as I can see. So it depends on all this injectivity radius and so on. And so as I again point out, that you need a trick to avoid the log factor. So if you if you do it, so the way it is done is that when we wrote down this result, we actually did it with the log and the paper was circulated amongst people. And the fellman at Tel Aviv University. Uh, Tel Aviv University pointed out that maybe if you do this and this, you should be able to get rid of the log factor, and then we tried it, and it worked. So the first one version of the first version has this log coming from there. All right, so I want to say two words about tools and so on. And I mean, if you wish, I mean, I don't even want to say that if you think it's too much, you know, it's end of the. Think it's too much, you know, it's the end of the conference. I could just stop here because it's already getting close to time, a few minutes left. I would want to say that we don't quite use BMO this time. It probably can be used, but we have something which is very, very similar, which is this thing here. This is the sort of thing you will also see in the Mackenhauf paper, and it has something to do with Mackenhauf A infinity waves. so the so we we we sort of uh and the key point is that if you if you go through the the the tools here you can actually improve the BMO norm to get to here and as you said as I pointed out to you that this is the right right growth bound that you should expect so so it is really an improvement in that in that in that bound from lambda to the n to square root of lambda and root of lambda and and so that leads to things like that i mean this is not surprising once once you have this uh control uh yeah then then you can you can you can more or less think that the game is now almost over and what you will see in the markenhow paper is something like this you will see uh the race so this is mod u square but this is with this is exactly what is called an This is exactly what is called an A infinity weight in the Muckenhub theory. And so you see here that if you know this ratio, then you can control the ratios of arbitrary sets. This is really the E, as you can expect, is one of those tubes. It's one of those nodal tubes to which we are applying this sort of work. So one of those nodal components. So this ratio controls the ratio of the nodal volume to the volume. To the volume of the ball. Okay, so thank you very much for your attention. I hope I tried to give you some idea of what's going on. And I tried hard because I'm the second last speaker before this beautiful conference ends. I tried not to say too many equations and so on because people, I think, are very tired at the end of a long conference. Thank you very much. End of a long contract, so thank you very much. Thank you, Sagun. I think you have succeeded. Thank you for this beautiful talk. And then I guess we have time for questions. I have a question. So hi, Segun. That was a very nice talk to do with me. It was very nice. About the Donnelly Pfefferman result, that this domain cannot be too thin if it touches the interior. Be too thin if it touches the interior ball. Yes, yes. You said it uses the proof, uses the Carleman inequality, or the Carleman inequality came later. Yeah, so the Carleman inequality, let me point out. Like I was saying, so here is what the Carleman inequality has a consequence, and so it uses the And so it uses this sort of, you end up with this corollary. Okay, it's a corollary. But yeah, this is not the Carleman inequality. This is a corollary of the Carleman inequality. The Carleman inequality will have the Laplacian on the right side and the function on the left and some weights. And this is used to prove also. Sorry. And you use this inequality to prove a Donnelly-Fafframan's result, or it follows from. Or it follows from no, no. Who's I mean, Mackenhauer and me? Is that what you're asking? No, the result on page 13 about this, the thickness of the domain. Yeah, so here we need two ingredients. We need some version of the car. Of the Carleman inequality, and we need this extra ingredient, which is definitely not in Donnelly and Pfefferman, which is the covering lemma. The covering lemma is not in their paper. The covering lemma is a new ingredient in our paper. Okay, so they didn't have, they had something, but obviously, you know, if you follow that, you'll get k to the n squared, which is their theorem. To the n squared, which is their theorem. In our theorem, it's k to the 4n. So you see, there's a substantial reduction from n squared to n. Okay. But, you know, Charlie was, you know, he has high standards. He said it's great, but I mean, I remember him telling me that if you get it independent of the dimension, that would be major. So he did not. So, he did not even care for K. I mean, you know, K would be the same as the global Quran theorem. If you got K to the four, then he would think that, oh, now there is something important. But at the end of the day, it's not even K to the N, you see, or K, it's even less than one. N minus one over N is strictly less than one. See, locally, you're doing much better. Where is that slide? Where is that slide? Where we have? Oh, look here. Look, this number is smaller than one, so it's not, it's not even K. I mean, you're doing at a local level, you're doing better than the global Purani, which you saw in the stumbling example. You saw two. Two is certainly better than K. When you looked at the pizza, then you saw root lambda k. Root lambda k. root lambda k root lambda k you know if you plug in with all the while formula you're going to get square root k because n is two square root k is better than k so so you see on the local level for small enough balls because this is the one that's saying that there is a transition so at certain time this this gets much bigger when this gets much bigger and dominates that you see the k appearing okay so if this term dominates that's the term dominates that's the global guy at work when if this is small like this ball is very small then this is going to dominate then you see the local version appearing if the local version is appearing you see this number is strictly less than one okay so you see you see there there is a transition zone so you know you you beat the global theorem you believe you beat the global theorem if you're on a small enough ball You're on a small enough ball. That's what you saw. That was the point of my writing down that 1D example, 2D examples, so that you can see, you know, you because I mean, you need models. And if you don't have models, you don't have anything to work with. So here it's not even dimension independent. You are strictly less than one, and that's optimal. The n minus one by one n is optimal because I showed you only examples in one. Showed you only examples in 1D and 2D, but in our paper, in the introduction, you see that we have the example in NED. If you read the introduction, you see we write it down for NED. That's it. You can't beat that number. So it's not like, oh, in higher D, maybe these guys missed it. No, that is the number. That's the right number. I hope, I mean, did I answer your question? I mean, yes, yes. I'm not sure. Did I answer your question, what you asked? Yeah. In fact, I don't know if there is time. Just to be sure, the dimension comes only in the covering argument. Yeah, no, no, no. We don't use the covering argument in the new proof, by the way. The covering argument was used in the Was used in the proof with Mackenhau. So, the covering, as I pointed out, I mean, this covering lemma is only used in the paper with Mackenhau, but it can only do so much because you see, you cannot improve this anymore. So, it cannot go further than improving the K to the 4n result. Okay. So, it's a dead end. I mean, that's what I'm trying to. I mean, that's what I'm trying to say for 30 years. I mean, you know, so I knew that you cannot improve. So, where is the other way you can improve? You can improve by trying to prove a much more refined Carnivan, but not using L2 theory like here, L2, L2, but using LP. Because by that time in the 90s, you know, people knew how to prove these LP parliaments. Up to then, everybody was doing. Because up to then, everybody was doing L2 theory. And one of the important things of the LP parliament, well, two important things, was that Koenig and Jerrys managed to get very precise, unique continuation results. Secondly, I could use this to push down Kunter Ruhmann's result with with the with Nachmann and and Sylvester from bounded potentials to all the way to optimal L n over two potentials. Optimal Ln over 2 potentials to do the colour and problem. So, this was two instances. So, I said, oh, look, it's doing that. So, maybe that should be useful here. And I tried for many, many years to get a version of LP Harleman, just like Jarrison Kennig and Sawyer and myself, we also have LP Carleman to apply to this problem. And every time I hit failure, no, of course. Hit failure. You know, for many, many years, every summer, I would compute and it wouldn't work. This is, this cannot improve anymore. That's it. So, you see, you needed new ideas. And the new ideas, I mean, have come from Lagunov and Malinipova's work. Okay. So you can see now. I mean, this is all hindsight. Everything always, you have a better view on hindsight. Hindsight, you know, where is it that I was doing wrong things? But that's that's after the situation. You know, it would be nice if it's before the situation, but it's always after the situation we discover all the dead ends you were doing, stupid things you were doing. Okay, well, there is three minutes for the next talk. I don't know, maybe you have time for one short question. Maybe you have time for one short question. May I? Go ahead. Hi, Segun. Hi. Is there a conjecture about equidistribution of the novel sets? Sorry? Is there any conjecture or results about equity distribution of novel sets? So if I take a great question. Yeah, it's a great question, but I don't know the answer. It's in fact something that warrants. That that warrants attention, right? In this statistical properties, right? Because you see, I mean, I said Ullenbeck, so you know, there is this some sort of, you know, you saw in a generically, so you want to say something like that. Yes, yes, yes. So, so that is clear. I mean, you want to, I mean, you want to say, you know, like in the spirit of Like in the spirit of the while law itself. Yeah. So while law is a statistical information. So you want to say something about it. Statistically what happens. Yeah, I don't know. I don't know the answer. Maybe somebody has thought of these things. Okay. Thanks. So maybe. Maybe I would like to thank Sagun again for this next talk, and then we have one minute for Jeffrey. So, thank you. Thank you.