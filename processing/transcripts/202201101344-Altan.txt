Um okay, there we go. The brain computer interface is a rehabilitative technology that converts neural activity into control commands to allow paralyzed patients to operate assistive devices or even their own limbs through electrical activation of their own muscles. In my lab, we implant 96-channel multi-electrode arrays in the hand area of the primary motor cortex. The hand area of the primary motor cortex of monkeys, which contains information about the motor behaviors that involve the use of the hand, and we collect these signals as the monkey engages in motor behaviors. One of the behaviors is picking up treats from a board, which we'll see in the clip on the middle panel. And what you're hopefully about to hear is the neural activity on one of the 96 channels. And please pay attention to how the firing rate changes during different portions of the motorbike. During different portions of the motor behavior. But let's give it a try. So as the monkey was making movements, especially reaching and grasping, you could hear that what we call the spikes were becoming more and more frequent, which is indicating that these neural signals are directly related. Indicating that these neural signals are directly related to the behavior itself. And finally, on the rightmost panel, we'll see a paralyzed patient, a human patient, controlling a robotic arm using her brain. And the patient can translate and rotate different portions of the robotic arm to complete this, to pick up this cup and stack it on another stack of cups. And in my lab, we don't ask the monkeys to operate robotic arms, but instead, Robotic arms, but instead we relate their neural activities to their muscle activities associated with moving the hand, wrist, and the arm in hopes of eventually creating a brain-machine interface that can allow patients to reanimate their own limbs through electrical activation of their own muscles. Many people talked about this today, but I'll briefly talk about it too. It's the idea that neural activity is constrained to low-dimensional Activity is constrained to low-dimensional neural manifolds. In this example, I'm showing recordings from 100 neurons. And if we suppose we wish to plot the collective activity of these 100 neurons, in that case we can construct a 100-dimensional state space where each axis corresponds to the activity of each neuron. And then we can plot this collective activity of all neurons across time. And it turns out that the neural activity does not span the 100-dimensional space, but instead... The 100-dimensional space, but instead is constrained to a low-dimensional subspace that we call the neural manifold, shown in green here. In this toy example, the manifold is a plane, so it is a two-dimensional structure. And low-dimensionality is quite helpful if we wish to interpret these neural representations. For example, earlier today, Manuel and Nick talked about the relationship between low-dimensionality and generalizability of these neural representations. Of these neural representations. One aspect that I'd like to talk about today is why we might care about the low dimensionality is that correct inference of dimensionality goes hand in hand with low error in decoding the behavior. So let's unpack this statement a little bit using the following figure here. And this is a figure from a simulation study by Surya Ganguli's group at Stanford. And they wanted to understand the relationship between inferred The relationship between inferred dimensionality of M1 or simulated M1 signals and how the estimate of the dimensionality was related to the decoding error. The neural data that they had lied on a 20-dimensional manifold and it was embedded in a much, much higher dimensional space. And then they sampled up to 500 neurons and up to 500 trials from this neural data. And they wanted to infer. And they wanted to infer the dimensionality and decode the associated behavior. And the main takeaway is this black line here, which indicates the boundary that describes either sampling from a few trials and many neurons, or many trials and a few neurons. And the right side of this boundary is tied to low decoding errors. So this is why it's essential to be in a regime where you can infer the dimensionality correctly, which means you can decode. Dimensionality correctly, which means you can decode the behavior correctly. So, one glaring problem that you might think about here is the multi-electrode arrays that we're talking about, is that we can sample approximately from 100 neurons. While it may look like the arrays are incapable of sampling enough neurons for accurate inference of dimensionality and low decoding error, this is only due to this number here, that their simulations were 20-dimensional, which is relatively high. Which is relatively high when compared to the dimensionality estimates that people see of the primary motor cortex, as Tatiana alluded earlier to. People often get much, much lower dimensional neural manifolds. So primary motor cortex is low-dimensional, certainly much lower than 20, according to many studies. And we can infer the dimensionality of these neural representations and decode behaviors using these UTA rays. Behaviors using these UTARs, the multi-electrode arrays. And we can even decode single trials. So the questions that I'm interested in are, or to take a step back, there is a problem with this previous statement that N1 is low-dimensional. And the problem is that primary motor cortical manifolds have only been identified in laboratory settings. So we take the monkey, put them in a chair, we usually restrain their movement so that they can only do what they So, that they can only do what we want them to do. And these behaviors are very stereotype, simple behaviors like grasping, different types of grasping, or manipulating a handle to move a cursor on the screen. And then we conclude the dimensionality is low, and then we can decode the behavior. But more interestingly, what is the dimensionality of M1 in more natural settings? And there is a possibility that the underlying simplicity suggested by the low dimensionality of M1 is simply a byproduct of. Is simply a byproduct of the constrained laboratory environment. And if the monkey were free to do whatever they wanted to do or were behaving in a more free environment and doing similar types of tasks, maybe this dimensionality number would be way high. And conversely, if the dimensionality is high, maybe it will be difficult to decode these natural behaviors. So the next question that I'm interested in is how well can we predict the muscle activity from M1 manifolds in natural settings? And answering the second question. And answering the second question could be important for translating brain-machine interfaces from constrained laboratory settings to more natural settings. There is another problem. There were some discussions earlier too about how to pick the correct number of latent dimensions or estimating dimensionality for short. And this is a difficult task. When I first started this journey, I would try a bunch of off-the-shelf algorithms on my neural data, and they would yield very different results, and it was really Very different results, and it was really hard to interpret. In addition to the abundance of dimensionality estimators, these methods are affected differently by the linearity and nonlinearity in the data, whether the intrinsic dimensionality data is high or low, whether the variances of the features are different or similar, or the amount of data or the amount of noise. So there are all these different factors that precluded me from understanding the correct dimensionality. Understanding the correct dimensionality of the neural data that we collected. Some of the methods, so we could classify these methods broadly into two categories. The first set of methods would be those that are limited to linear manifolds, and the second set would be the methods that can also identify the dimensionality of nonlinear manifolds. So let's focus on the left side of the presentation here. I'm sure many of you are familiar about I'm sure many of you are familiar about PCA with a variance threshold. Many of you use this method today in your presentations. And the idea is that we create this scree plot and count the number of leading eigenvalues that lead to a certain threshold of accounted variance. And as you can see, if you were to pick a different threshold, you would get a different estimate of dimensionality depending on the curvature here. There are more principled ways of using PCA for dimensionality estimation. Two of such methods are. Two of such methods are participation ratio and parallel analysis. Participation ratio is a way in that you don't need to set an a priori variance threshold, and it is a measure of how it's a measure of determining dimensionality based on how the eigenvalues collectively contribute to this curve. Similarly, parallel analysis doesn't require a variance threshold, and this method works by creating null distributions for each eigenvalue. Null distributions for each eigenvalue by shuffling the features independently and doing PCA many, many times over. And through those null distributions, you compute the significant eigenvalues. On the flip side, there are these methods that are not largely known to the neuroscience audience, but they're being well studied in the machine learning domain, specifically image processing domain, where people deal with extremely high-dimensional image data and they want Image data and they want to understand how this data is represented in their neural networks. These methods, instead of fitting a model like PCA to your data and projecting it to a low-dimensional space to infer dimensionality, these methods look at the data points in the high-dimensional cloud of data. And by looking at the neighborhood properties and comparing those properties to data sets with known intrinsic dimensionality, they infer the dimensionality. They infer the dimensionality of the data that you're looking at. So, for example, this is a toy example of a Swiss roll where the data is embedded in a two-dimensional Swiss roll. If you were to apply PCA to this data and use these methods, you would get that this data set is embedded in a three-dimensional space, but you know by visual inspection that it's two-dimensional. And these methods would either look at the distribution of distances or separability properties and how these properties. Separability properties and how these properties change as you change the scale of the neighbors that you're looking at, and infer the dimensionality directly. So the rest of the talk would be structured into two parts. The first part is in the challenges that I just described to you, how can we estimate dimensionality? And we devised a pipeline for estimating dimensionality. And I'll talk about the pipeline that we have come up with. We have come up with. And in the second part, I will talk about the application of the pipeline to actual neural data in hopes of answering the questions that I laid out earlier. So the first part is the pipeline for estimating dimensionality, which we recently published in PLOS Computational Biology. And the first step is to simulate neural data with known dimensionality, non-linearity, and noise level. And to do this, we first generated latent signals, low-dimensional latent signals. Latent signals, low-dimensional latent signals, and our generation method was we sampled actual experimental recordings and then we did random sampling so that our latent signals were uncorrelated to one another. And then we embedded these low-dimensional signals using a mixing matrix in high-dimensional space. And in some simulations, we added non-linearity, some form of an exponential function, so that we were able to play with. And we were able to play with the degree of nonlinearity in these simulations. We were able to add noise when necessary and change the signal-to-noise ratio in these simulations. And we had a playground, so to speak, to generate data sets with varying intrinsic dimension, nonlinearity, and noise level. And you could imagine that in this toy example of the Swiss roll again, certain features that you Features that you initially set for your simulation might change by changing some of the parameters of the simulation. For example, a 2D Swiss roll, as you add more and more noise to it, might start looking like a three-dimensional blob. So how would these methods operate on these different conditions? Was our initial question. One of the key results was that in the ideal scenario where we didn't have any noise in the system, the Any noise in the system. The linear methods failed on non-linear manifolds. So on the top panel, we only looked at linear data sets, and on the bottom panel, we looked at nonlinear data sets. And the true dimensionality of these simulations was shown in the dashed line. And the linear methods based on PCA, these first three, really overestimated the dimensionality of the nonlinear manifolds. Whereas those Manifolds. Whereas those non-parametric methods, so to speak, that I described earlier were successful in estimating dimensionality of both linear and nonlinear data sets. In addition, noise causes a lot of overestimation of dimensionality. When we change the signal-to-noise ratio such that we injected more and more noise to the system, the estimates of dimensionality The estimates of dimensionality grew when we looked at these nonlinear methods. I'd like to highlight that parallel analysis, which was a method based on identifying significant eigenvalues, was robust to change in the noise, but it wasn't robust to the nonlinearity. So, in this framework, we didn't have a way of knowing the true dimensionality of a highly nonlinear and a highly noisy signal. So, we wanted to reduce these unwanted effects of noise, and our approach was the following. Let's say we have our signals, and the first step here is to estimate the upper bound dimensionality. We'll call this capital D, and for this purpose we used parallel analysis, which happened to be the most accurate linear method we tested in our simulations across all the conditions that I laid out earlier. And once we have this D-dimensional upper. D-dimensional upper bound estimate, what we wanted to do was project the data to a d-dimensional subspace and reconstruct it. And one obvious way to do this is using PCA. You could retain the principal components and reconstruct your data. Another way to do this would be something that we've come up with called the joint auto-encoder. This is a dual network design for denoising, and we divided the 96-dimensional data into The 96-dimensional data into two 48-dimensional partitions, blue set and yellow set. These partitions were each mapped by the compressive halves of the respective auto-encoders to the D-dimensional subspaces here. And these compressed subspaces were then used to obtain the reconstructed versions of the blue and yellow partitions using the expensive halves of the corresponding autoencoders. And the cost function that we use to train the joint autoencoder. That we use to train the joint autoencoder network not only minimize the reconstruction error of the blue and yellow partitions, as we do for vanilla autoencoders, but also the difference between the D-dimensional latent signals shown in green. So this design assumes that each of the partitions contains the information necessary to robustly identify the underlying D-dimensional signals, but not the independent moist components that would differ between the two partitions. That would differ between the two partitions. So once we denoise the data, either using PCA or this method, we can finally estimate dimensionality. And it turned out that denoising significantly improved our dimensionality estimates. Here we're looking at the dimensionality estimates of denoise data sets, linear data sets on the left and non-linear data sets on the right. And we can see that in the linear case, we can get to the correct dimensional estimates. The correct dimensionality quite reliably. And in the nonlinear case, we can't really get to that line exactly, but I wanted to remind you that the estimates were on the order of 30 to 35, and we can bring it way closer to the true dimensionality of the neural data if we do denoising before we do dimensionality estimation. So, in conclusion, the pipeline is the In conclusion, the pipeline is the first we obtain an upper bound estimate, then we denoise the signals using linear and nonlinear approaches. For this we used PCA and joint auto-encoder to reconstruct the signals from a d-dimensional subspace. Then we compare the reconstruction accuracies, the variance accounted for, to establish the linearity of the data. And finally, having access to the denoise data and having some understanding. Data and having some understanding of its linearity, we can use a dimensionality estimation method depending on the expected linearity. The second portion is about the application of the pipeline to actual neural data. And the questions that I laid out earlier, I just wanted to remind you again, are one, what is the dimensionality of the primary motor cortex in natural settings? And two, how well can we predict muscle activity from Can we predict muscle activity from the primary motor cortical manifolds in natural settings? To answer these questions, we collected data in two settings. The first setting was the classic stereotype laboratory setting, where the monkey engaged with devices that were tied to a computer, and the monkey was able to control the movement of a cursor on a screen as he did power grasping and precision grasping tasks. Tasks. More interestingly, we have a wireless recording setup in our lab where the monkey can do whatever he wants to do in a big plastic cage, and we can collect these neural and muscle activation electromyogram signals in the bottom that you're seeing here wirelessly as the monkey is crawling, picking treats, or foraging for toys and doing grooming and whatever else he'd like to do. And this He'd like to do. And this, Shuen, who's a postdoc in our lab, has been leading the efforts of obtaining these signals across contexts. So the first question, finally, what is the dimensionality of M1 in natural settings? And it turns out that manifolds in the primary motor cortex are low-dimensional in both contexts. Here we're looking at, we collected our neural data, we applied our... We collected our neural data, we applied our pipeline for estimating dimensionality, and we're looking at estimates using parallel analysis, our most accurate linear method in the simulations, and also two nearest neighbors are most accurate nonlinear method in the simulations. And what you can see here is that warm colors indicate laboratory tasks and the cold colors indicate tasks in the cage that we collected wirelessly. And the dimensionalities are comparable. And the dimensionalities are comparable. Using parallel analysis, we get dimensionality estimates between 7 and 13 for the lab tasks and roughly 8 and 15 for the cage tasks. And more interestingly, when we apply a nonlinear method, these estimates all shrink to a very similar level. So the estimates are on the order of 5 and 6, which is indicating that the true intrinsic dimensionality of these signals. The true intrinsic dimensionality of these signals is roughly 5 and 6, and the linear methods can only identify the embedding dimensionality, which is roughly at most 15 dimensional. So the second question is: how well can we predict muscle activity from these primary motor cortical manifolds? And to answer this question, what we did was we Did we decoded the 16-dimensional muscle activity from either using all of the neurons, approximately 100 neurons, or from low-dimensional embeddings of the my headset just turned off? Can you hear me? Thumbs up? Okay, great. Yeah, so we either looked at the decoding. Either looked at the decoding accuracy from all neurons or decoding accuracy from low-dimensional embeddings of these neurons. In the left case, we compared the accuracies. The low-dimensional embedding that we used was the method to get to that embedding was PCA. And just as a reminder, our linear estimates were around 7 to 15 dimensions. And we're seeing that all of the points here. And all of the points here lie nicely on this unity line, meaning that we don't really lose much information with regards to the coding accuracy if we go from all neurons to this 7 to 15 dimensional PC space in an unsupervised way. Similarly, we did the same analysis, but instead of PCA, we looked at UMAP embeddings. And here, UMAP embeddings were 5 to 6 dimensional, so way lower than these. So, way lower than these estimates here. And we see the same scenario. And hopefully, these results are convincing that the dimensionality estimation approach that we have is able to capture the essence in the data. So, in conclusion, low-dimensional neuromanifolds exist for both constrained laboratory tasks and natural motor behaviors, where the monkey needs to incorporate different sorts of sensory stimulus. And proprioception, balancing himself. So he can do whatever he wants to do, yet the M1 activity is still low-dimensional. So it's not simply a byproduct of making the monkey sit in the chair, but it's actually reflecting something regarding the computations that happen in the brain across contexts. The neural manifolds may be nonlinear. Our nonlinear estimates of dimensionality are lower than Dimensionality are lower than the linear estimates consistently. And the decoding accuracy of EMGs from low-dimensional neural manifolds is similar to that from all neurons. And finally, the decoding accuracy of EMGs from low-dimensional neural manifolds across settings is also comparable. And with that, I'd like to thank Sarah, Lee, and Eric, my advisors, and the rest of the Miller-Limblamp. And with that, I'll also take your questions. Thank you for listening. Also, take your questions. Thank you for listening to me. All right. Thank you, Ege. Questions for Ege? I have a quick question. When you apply this pipeline to the naturalistic movement, Uh, the naturalistic movement, what was like, how did you segment these movements entirely? Like, and how was how sensitive were your estimates to the length of these snippets in a sense, right? Like, because you would imagine if you stuck a bunch of them together, then your dimensionality is going to go up. So, did you look at this segmentation issue? That's a very good question because, as I mentioned earlier, the amount of data that you The amount of data that you have in your analysis is very critical for you to be able to correctly identify the dimensionality. So, what we did was we, using a video recording system, carefully inspected the portions of behavior where the monkeys crawling or picking up treats and spliced those data sets together across time and created a five-minute long data set where we did this analysis. Did this analysis, and obviously, there was some cross-validation for finding the low dimensions and decoding. But the main data set that we worked in was approximately five minutes of data, where all the trials are concatenated across time. And your first comment was that, or the second comment is that the dimensionality might change in those snippets. Although I've done that analysis to look at how the dimensionality might change. How the dimensionality might change because it may not be stationary as the monkeys having slightly different variations of movements since these are not stereotype trials. I'm reluctant to rely on the variance of that estimate because we are in a regime where we may not have enough data for stable estimates of dimensionality. Thank you. So, I'll ask a question. So, you had this plot where you were showing overestimation of dimensionality with different methods. And I think Fisher separability underestimated in some cases. Is that typical? And do you know why? Fisher separability. Fisher separability has a pre-processing step where you need to whiten the data before you can use it, and it also has a bunch of other hyperparameters that you need to tune, such as after you do the whitening step, you still need a low-dimensional system, otherwise, it's not a feasible problem to solve computationally. Computationally. So people often pick a high number of PC components to retain and then do their dimensionality reduction, say, to this 15-dimensional space, and then apply the Fisher separability. And I think combination of these hyperparameters result in an underestimation of dimensionality. I was able to play with those hyperparameters and get a good estimate, but for real data, we don't really have a way of verifying. Of verifying what to do since we don't know the intrinsic dimensionality and what those hyperparameters might be without looking at the behavior or some other anchor, so to speak. I see. So usually the behavior guides your hyperparameter selection in that case. Not really. In the case of Fisher separability, it was just the experiment that I did to satisfy. To satisfy my curiosity, basically. I wanted to understand why this underestimation happened, and I think it's due to poor default hyperparameters for the case of neural data. But it works well for many other data sets, such as Mobius strips that are intertwined in 60-dimensional space. You know, all these crazy data sets, it works quite well, but not for neural data, at least at the Well, but not for neural data, at least at the default hyperparameters. Gotcha. Yeah. Looks like Jay has a question. Yeah, go ahead and mute. Yeah, thanks. That was a really cool talk. The big curious with the parallel analysis, it sort of seems like it's swapping one threshold for another, in this case, I guess the significance threshold. Is that something that you? Is that something that you also perhaps correct for as you for like multiple observations, and would that decrease the dimensionality as a result of such a multiple comparison correction, for example? That's a good point. So there is an implicit significance threshold there, so you're right in that sense. But what I'd like to highlight with that method is that the method itself is way less sensitive to that threshold. Less sensitive to that threshold. So, even if you were to pick a 90% threshold or a 95% threshold, you often get the same answer. Whereas, if you were to do that using PCA or the variance threshold of the eigenvalues, you're almost certain that you're going to get different results in the two scenarios. But for most of our analyses, we kept the significance threshold at Significance threshold at 95%, which is how people use this method in different fields as well. And if you do increase it to like 99%, you would then expect, you know, perhaps not by much for the reason that you just gave, since it seems what you would expect. I would expect some change, but not as drastic change, so to speak. Well, thank you. Thanks. Any other questions? Okay. Well, thanks again, AJ, for a nice talk.