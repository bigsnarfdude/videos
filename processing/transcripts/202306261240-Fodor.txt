Okay, so thank you. Yeah, so thank you very much for the introduction. And I'd like to thank the organizers for the invitation so that I can give this talk here. So I'll be speaking on central limit theorems for random polytopes. Limit theorems for random polytopes and the geometric ingredient in them that makes it possible to actually prove such a theorem. So, this is the first thing I'd like to say that this is a joint work with my PhD student, Daniel Popvari from the University of Seged. So, maybe a short overview before actually we get to the substance of the talk. I'll be talking about random polygons rather than random polytopes, so only in the two-dimensional. Random polytopes, so only in the two-dimensional plane. But let me give you a very short, slightly more general introduction. So, let K be a convex body or a convex disk if it's in the two-dimensional plane. In our case, this will be restricted to bodies with a C2 plus boundary, which means two times continuously differentiable with a strictly positive Gaussian curvature. So, these are quite nice smooth bodies. Bodies. Now take N IID, identically distributed independent uniform random points from K, and take that convex hole. This is just for right now, this is just the usual convex hole. Generally, this would be a random polytope in the dimensional space. If you're in the plane, like in this thought, then this is a random polygon containing k. And of course, there are many possible questions that one can ask regarding these objects. Regarding these objects. Obviously, the behavior of the area, perimeter in higher dimensions, the intrinsic volumes or number of k-dimensional faces, the F-vector, etc. These are all subjects of interest, these random variables. In this particular talk, I'll be concentrating on the area of the random polygon, or more precisely, The of the random polygon, or more precisely, if you look at it in this fourth bullet point, this is not quite the area of kn, the random polygon, but the uncovered, the area of the uncovered part. So this is sometimes called the missed area of Kn. So the area of the part of K, which is not in Kn. Now, of course, we would be interested in the distribution of this random variable, but in this talk, I'll be speaking about expectation, variance, maybe some. Expectation, variance, maybe some higher moments, and some limit theorems in an asymptotic sense, which means in our case that n tends to infinity, so the number of points tends to infinity. That is, asymptotic investigations of random polytopes, and particularly in the plane random polygons, started with the famous papers by Rainy and Sulanka in the 60s. Actually, there were three consecutive papers, but I only listed two of them, one from 1963. Them one from 1963, the other one from 1964, in which they actually determined the expectation, the asymptotic expectation for this missed area under different circumstances. In one of the papers, they actually determined it for like us, for convex disk K with a C2 plus boundary, and then for the case when K is actually a polygon. It's quite opposite. Now, Now, later, of course, these results were extended to higher dimensions. But this is only the expectation. I did not really put here the formula because it's not going to be essential for us. The next question is variance. In 2003 and 2005, in two papers, Matthias Reitzner approved matching lobrand upper bands. I mean, these are again asymptotic lobby and upper bands. Lower and upper bands for just the orders of magnitude or the variance of the volume. And here I denoted this, you know, the D-dimensional volume by Vd or this random put of Kn. Although, like I said, we are working in a plane in this talk, but the original statement was for general D, so I decided to put here the general version. And then this made it possible to actually prove a central limit theorem for the Central limit theorem for the volume. This was done by Matthias Reitzner in 2005. So the CRT is for central limit theorem. But this is a really complicated business in some sense because when you look at the V D of Kn and these random variables are not independent, of course. I mean, you're adding extra points. Whatever, you know, the random political of Kn plus one will be, in some sense, depends on Kn. So these are dependent. So, these are dependent random variables, which complicates things. The way Matthias Reisner proved the central limit theorem was through an extra step. He considered another random polytope. We took a pi as a stationary Poisson point process with a particular intensity, which is adjusted in a way that the expected number of points that fall expected. That fall actually into k into this body is going to be n. And he proved a central limit theorem for this particular object that I denoted k sub n super pi, which is a Poisson random polytope, and then made a transition to this classical random polytope. This is a fairly difficult process. And then later, not much later, Wu proved strong concentration estimates and there were further results. Estimates and there were further results. And actually, an interesting idea came about in 2018 in a paper of Taylor Turkey and Wespy, in which they actually gave a very direct proof for this uniform distribution, a random polytope, I mean, the essential limit theorem using a different method, something called Stein's method. This is a very clear, very direct. Very clear, very direct, very to the point proof, which actually makes it possible to prove such theorems with much more ease than if one would have to go through this extra step of using Poisson polytopes. So today what I'm going to do is I'm going to show you how to prove such a central limit theorem in the case when we slightly modify the idea of convexity. This topic is loosely This topic is loosely related to what Carli Beslek was talking about in his talk. Actually, he introduced some of the definitions I need, but just for the clarity, I'll repeat them. So, what we're going to be considering is the following. First of all, if we take a set X in the plane and fix a number small R and take all the intersection of all radius R closed circles, or in higher dimensions, these are, of course, closed balls. Course, closed balls. But like I said, today, everything is in the plane. The intersection is going to be a convex set if, of course, X is contained in at least one of these circles. If that's the case, then I denote this intersection by this bracket with a subscript R, indicating that this is like the convex whole, except, you know, instead of using closed health spaces or health planes in the plane, what we take is the intersection of all. What we take is the intersection of all radius R closed circles that contain this set X. In the special case, when the points are not contained in such a circle, and then their whole is defined to be the whole space, but that is not going to happen today under these circumstances. So we say that a set K in the plane is R-spindle convex if you take any two points from K. If you take any two points from K, then this hole, this XY bracket sub R, is contained in K. This is what Carlo Besla called a spindle today because of the shape. It looks like a lens in two dimensions, of course. I mean, higher dimensions, you know, in three dimensions, it's like a classic spindle and it has rotational symmetry, but in two dimensions, it's just a lens. So, a set K, spindle convex, if together with any of If together with any of its two points, it contains this spindle spanned by these two points. Sometimes it's called the R segment. It plays the role of the segment, essentially. Now, in our case, one can easily prove, it's not really a difficult thing, that if the boundary of K is C2 plus, so this is K is a convex disk, compact convex set with no empty interior, and its boundary is C2 plus smooth, so twice continuously differentiable with strictly positive curves. To differentiable with strictly positive curvature, then actually k is r-spindle convex for any r that's bigger than this r sub m, which is the reciprocal of the minimum curvature. This is the maximal radius of curvature. So if r is at least as big as the maximal radius of curvature of k, then it's going to be spindle convex with that radius r. In particular, if one takes only a finite number of radius r. Radius or circular disks and intersects them, then the intersection is called a disk poly or disk polygon. This is pretty much like a convex polygon. It's a convex object. It's like a polygon, but instead of having segments as sides, it has circular arcs of radius r as sides. Now, okay, so our modified Randall model is the following. So we assume that K has this C2 plus boundary, and suppose that Boundary and suppose that the small r, the radius of spindle convexity is bigger than or at least as large as this r sub m. Take n IID random points, these are x1 through xn, and then take their spindle convex whole. So the intersection of all radius are circles that contain these points. Now, under these circumstances, this is, of course, a meaningful definition. And what you get is going to be an R-disc polygon, a convex. Ardis polygon, a convex or disc polygon, and we are interested in the usual things, would be interested in the number of vertices, missed area, etc. But we will be concentrating on the missed area today. Okay, some existing results in this direction. In a joint paper with two of my colleagues, Peter Kevé and Victor Weig, we actually proved that if K has this property and the small are the radio suspindal convexity, The smaller radius of spindle convexity is strictly larger than the maximum radius of curvature, then the expectation of the missed area is asymptotically equal to this quantity that's on the right-hand side. What's really important here is the order of magnitude in n. You can observe that it's n to the two-thirds. And then there is some absolute constant, and then there is a constant which somehow depends on the k. Here, kappa denotes the curvature. Kappa denotes the curvature. So here, you know, one has to integrate kappa minus one over r to the one-third, so the cube root of kappa minus one over r with respect to the arc length. This is very much, this very much resembles something like affine arc length, although its meaning at this point is kind of unclear. As a next step, in another paper with Victor Wieg, we proved an asymptotic upper band. Asymptotic upper band on the variance of this random disk polygon. And just last year, with a paper published with a graduate student of mine, Bolaj Grinfelder and Victor Rieg, they proved a matching asymptotic lower band for the variants. Now, in this talk, this lower band will be an interesting one. Upper bands usually lead to strong loss of large numbers. However, in order to prove a However, in order to prove a central limit theorem, you usually need a lower bound. So now we have a lower bound. Now, this sign, this asymptotic inequality, hides the constants. We do not care about actual constants here. So here, this means that the variance is less than or equal to some absolute constant times n to the negative 5 thirds for all n. So that's what this notation means. It really is very convenient in the sense that one doesn't. Convenient in the sense that you one doesn't have to deal with the constants and can be very generous with them. Okay, so now the main result. We're going to be proving a quantitative central limit theorem, which means that not only establish that the area in some sense, centralized and normalized area of the random disk polygons tends to a standard normal distribution in. Standard normal distribution in distribution, but we also have a sort of estimate on the speed of convergence. So, okay, so how do we do this? Suppose that we have two random variables, x and y, on the same probability space. We're going to use the so-called Wasserstein distance to measure their distance, which is just the supremum of the expectation of one variable minus the expectation of the other variable. The expectation of the other variable such that you apply all possible Lipschitz functions with Lipschitz constant one or at most one to the random variables. So this is a standard notion of distance and actually one can show that this is really a metric, etc. Okay, so the main theorem is the following. If the radius of spindle convexity small r is strictly greater than this magic number small r sub capital N. R sub capital M, which is the maximum radius of curvature, then the following holds. If you take the centralized and normalized area function of the random R discolygons, then this tends to a standard normal distributed random variable in a sense that their Wasserstein distance is. Distance is less than some absolute or some kind of a constant times n to the negative one six times this log factor, but a constant only depends on the on the on the convex disk. So this is essentially tells us that you know this is a central limit theorem. This random variable tends to the standard normal distributed variable. And here on the right hand side, this term, Term whether this expression in n measures the speed of convergence. Now we suspect that actually we know that this is not optimal in n. So there should be actually a larger power here, probably n to the negative 5 sixths, but we cannot prove that. Okay, so how does this work? So we're going to be using Stein's method. I very briefly tried to explain what it is and What it is, and I'll show you how this can be used for the proof. So, suppose that E, the capital E in this first bullet point, is a topological space. In our case, this will be the interior of K, where K again is in the plane, a compact, convex set with non-empty interior and with the C2 plus boundary. Now, this capital X, I switch to capital X because this fits more the notation that comes later. This is a random vector, actually, X1, X2, X2. Vector, actually x1, x2, etc., xn, these are iid uniform random points from e. So this is a random vector from e to the n. Suppose that x prime and x double prime, these are independent copies of the random vector x. Now, what we're going to do, we're going to cook something new from these independent copies. We say that another random vector z, whose components are z1, et cetera, zn, is so-called a recombination. called a recombination of the random vectors x x prime and x double prime if each component of z is one of the corresponding components of x x prime of x double prime so in particular if you take pick an i between one and n then z i is one of x i x i prime x i double prime so it you know you take one from each okay so suppose that oops sorry suppose that f is now a measurable function a symmetric A measurable function, a symmetric measurable function. Symmetry means here that this is on essentially e to the n. But just for convenience, I list it here from k from one to n, union of e to the k. But essentially, this is going to be in our case a function on e to the n. Symmetry means that you can, of course, switch, swap coordinates, and it doesn't affect the value of the function. The value of the function. Now, okay, a notation. This is a common notation. Take a random vector x with components x1, x2, etc., xn from e to the n. And this x superscript not i means that this is going to be a random vector in e to the n minus 1, which you get from x by actually deleting the ith component. So it's all the way x1, x2, xi minus 1, and then delete xi. i and then xi plus one etc xn um if you have two different indices i and j with i less than j then of course you can delete x i and x j both components which is denoted by this way x super not i not j This is a random vector from e to the n minus 2. Okay, so why do we do need this? Because we introduce the first and second order differential operators. We have this function that we are examining, f of x. Of x. Here, x is a random vector. It's kind of an awkward notation, but there's a proliferation of x's and so one would have to simplify. So this actually this function depends on x1, et cetera, xn. These are the random points. di, f of x, is denoted by f of x minus f of x not i. So this measures the change of the function if one actually drops one of the points. Drops one of the points. In our case, f will be the area of the random R disk polygon. So this will essentially measure something like how much the area changes if you remove one of the random points. So di f of x measures the effect on the functional f when x1 is removed. Of course, now you can do this twice. Take two indices i and j, and then d sub i j f of x is when you first take is when you first take dj of f of x and then apply di on it. So this is a second order difference operator and one does the algebra then you know you can write this out this way. Now this somehow will measure how pairs of points in a sense interact. I mentioned that here the actual A, the areas belonging to K and R are not independent. This somehow measures the dependence of this random variable. Measures the dependence of these random variables. Okay, so now one can create from these quantities four quantities here on this slide. These are quite complicated, so I would probably in this talk only concentrate on the third and the fourth ones. Those are sort of easier to see, easier to understand. The third one is just simply the third moment of this first-order differential operator of this function f. And the fourth one, b4, is the fourth. And the fourth one, B4, is the fourth moment. B1 and B2 somehow measure interdependence of the random points. Okay, so now, okay, so why do we need this? Imagine that we have a function w of n, which is in our case just our function f of x1, etc., xn. But for convenience, we assume that it's centralized. So it's expectation is zero for all n. Expectation is zero for all n, and that it's normalized in the sense that its variance of second moment is equal to one. If this is the case, then there's a theorem proved by Lachisj and Percati in 2017 that says that under these assumptions stated above, if g is a standard Gaussian random variable, then the Wasserstein distance of this function w of n and g is not larger than some combination of these functions. know some combination of these four quantities that involve powers of n in each term and some of the bi's first one b1 second one b2 etc so if actually one can somehow calculate these magic quantities b1 of f b2 of f etc at least bound them asymptotically from above in terms of n, then one would get actually a central limit theorem which is quantitative in the sense that you have even an upper bound. Have even an upper bound on the speed of convergence. Now, in our particular case, this function f of x1, etc., xn will be the centralized and normalized area of the random disk polymer. So we're going to apply this theorem of Lushi-Sré and Pacati to this particular random variable. So, okay, just a short note. Okay, so what do these difference operators mean? I told you that, you know, if you apply the first I told you that if you apply the first-order difference operator to this particular function, it simply measures the effect on the area, on the normalized and centralized area function by removing a particular vertex xi. So, in particular, if xi is inside the convex hole of the other points, then nothing happens, and this difference operator gives you zero. But if this xi happens to be But if this XI happens to be outside of the convex or the spindle convex hold of the other points, then of course its removal will remove a positive measured part of the hold, in which case this difference operator will be positive. Now, how about the second-order differential operator? Now, that's a lot trickier. Actually, if particular two random points form an edge of Edge of this random disk polygon, then it's easy to see. This is a homework to check, that the second-order difference operator is going to be non-zero with probability one. In this case, we say that xi and xj interact. However, this is, like I said, it's a bit trickier because the converse is not true. It may happen that the second order differential operator is non-zero, but the actual points xi and xj do not span an edge. This actually happens. edge. This actually happens when removing one, say xi, so xj may be not a vertex of k and r, but xi is. So when you remove xi, it may happen that xj becomes the vertex of the smaller object. Then the second order differential operator will be non-zero as well. This is the other case, but the argument contains also, it covers also this case. So this is sort of a simple homework to check that this is exactly. To check that this is exactly what can happen, okay. So, now, how does one calculate these quantities? And this is the point when I have to talk about a little bit about floating bodies. Yeah, classical floating bodies are well known. You have a convex body, you take a point in the convex body, and there is always a minimal volume cap containing that convex body. That's because of continuity regions. Also, if you take a convex Also, if you take a convex body and prescribe a particular number t, which is smaller than the volume, or maybe half of the volume of the convex body, and cut off all caps of volume t, then the intersection, whatever remains, is called the classical convex floating body of parameter t. And whatever is cut off is called the wet part of that convex body. Okay, so in our case, We're going to be modifying this definition. We're going to introduce the spindle convex floating body. So let this function vr be defined as follows. So for a point x in k, vr of x is just simply the minimum of the volume of caps cut off minimum area of caps that contain x. That contain X. Now, these are not usual caps cut off by straight lines, but caps cut off by radius or circles, like here, this shaded area, for example. And if you do the same thing, you take the level sets or super level set of such a function, then that will be the R-spindle convex body of parameter T. So take the level set T. This is the R-spindle convex floating body. This is the same thing as in the classical. But this is the same thing as in the classical case. What one does one specifies a particular T and cuts off all circle caps or disk caps from the original convex disk of area T. Whatever remains is convex, actually it's R spindle convex and that's the R spindle convex floating body. Okay, so what do we know about this? One important thing, in analogy with the classical case, the following lemma is true. Case, the following lemma is true. If n is sufficiently large, then our random disk polygon will contain the R-spindle convex floating body with parameter c times log n over n for some suitable constant with high probability. This is an important piece of information. This is what actually makes it possible to calculate those quantities that lead to the essential. Those quantities that lead to the central limit theorem. So formally, it says that if you pick any R, radius spindle convexity bigger than Rm, and your choice of beta, positive beta exponent, and there exists a constant depending only on beta and R, positive constant, that this statement is true, that the probability that the random R-disk polygon contains, it does not contain this floating body with parameters C to With parameter c times log n over n is less than n to the negative beta for sufficiently large n. So this means just if n is large enough, then with high probability, like in this picture, the random disc polygon will contain this floating body. This is an analogous statement to a similar theorem of Barani and Dola from a paper of 1997. The reason why one can prove this, one The reason why one can prove this, one can prove a connection between classical floating bodies and spindle convex floating bodies. It's possible to estimate one with the other. One can be sandwiched in a suitable way between two copies of the other type and vice versa. Okay, now, okay, so what we need is we need to estimate these difference operators, the first and second order difference operators. I only talk about the first order difference operators because the second order ones are more. The second-order ones are more complicated. By scaling, one can always achieve that the area of k is one. Then we simultaneously scale everything, including r, the radius of spindle convexity. Let this kn minus 1r denote the r spindle convex hole of the random points when you drop x1. So this is from x2 to xn. And let small capital A1 be the event that this Kn minus 1R continues. That this k n minus 1r contains the spindle convex floating body with parameter c times log n over n. We have just seen in the lemma that for sufficiently large n, the probability that the complement of this event happens is less than or equal to n minus 1 to negative beta. We can choose beta to be anything we like, then adjust the constants. Now, of course, this is less than or equal to some c star constant times n to the negative beta. The negative beta. Okay, so how about this first-order difference operator? Now, if x1 is in kn minus r, then of course the difference between the areas of k and r and kn minus 1r is zero. So x1 does not really contribute to knr. On the other hand, if x1 is outside of kn minus r, then the conditional probability of this event with the condition a1 is just simple. With the condition A1, it's just simply the essentially the area, the missed area. Don't forget that now we are using this normalization that the area of k is equal to one. Now, one can always use the fact that if you take the classical convex floating body and the spindle convex floating body of the same parameter, then classical convex floating body will contain the R spindle. Floating body will contain the R-spindle convex floating body. So I can further oppressimate this quantity by just simply the area of the classical. Sorry, in the first step, I use the condition A1. So Kn minus 1 contains the. Yep. And in the second step, let's see. Yeah. Okay. Yeah. In the second step, yeah, this is when we use the condition A. This is when we use that in the condition A1. Then, this Kn minus 1 contains the R spinal convex floating body. And then that can be oppressimated by the missed area by the classical convex floating body because it always contains the R-spinal convex floating body. And this area is known from a result of Barani and Larman. It can be expressed by the parameter. This is log n over n to the two-thirds. n over n to the two-thirds. So that's that's a known quantity. Okay, so now finally, okay, so I need one more notion: the R-spindle visibility region. Suppose that we take a boundary point. So here's the floating body, or this is a classical floating body on this picture, but imagine that this is an R-spinal floating body. And take a boundary point Z. Then the visibility region of this particular point is all the points in K. In K that are visible along a radius R circular arc from Z, avoiding this pindoconvex floating body as an obstacle. So here this is the shaded region, except that it was easier to draw straight line segments. Of course, this would be the classical visibility region, but imagine that these radius are circles. So this is some sort of a curved object. Object. One can prove, this is a standard thing, that if t is small enough, then the supremum of the area of these visibility regions as z goes over the boundary of k is linear in t with some constant that only depends on c. So this is essentially the size of the shaded region. If t is small enough, then the maximum of this is linear. Okay. Later. Okay, so now one can use a simpler notation. We're going to take this R-spinal convex floating body of parameters C times log n over n, and instead of writing this c log n over n in the belly of the visibility function, I just put n here. This is just a simplified notation. Formulas are shorter. So now take a boundary point Z of K. We define. K, we define another notion, sort of a hat defined by Z. So suppose that we have another spindle convex body, another R spindle convex body containing K, and we have this boundary point Z, and take their spindle convex hole, which would be L augmented with Z in a way that there's this kind of a curved hat on top of it. And then finally, cut out L. So whatever is left is. L. So, whatever is left is this red region. This is denoted by delta of Z and L. Okay, so why do we need this? Because if this event A1 happens, then actually for a boundary point Z, this corresponding delta, this hat produced by a boundary point Z and Kn minus one. This is Kn minus one here, except with a little longer notation. A little longer notation. This is part of the visibility region. Okay, so this is part of this visibility region. So this corresponding this red part. So its area can be actually oppressimated by the visibility region. And so the absolute value, the absolute value doesn't matter here, really, because this is a non-negative function. Or the first order difference operator is now less than or equal to the supremum of the areas of this visibility function. The areas of this visibility function such that z runs over the boundary of k times the indicator function that x1 is outside of the floating body. And so now because we know that this area, this supremum is linear in t therefore it's essentially of order log n over n. Okay, so if what happens if the complement of a1 happens, then one may use the trivial essay. Then one may use the trivial estimate for the difference operator that the contribution of any point cannot be bigger than the area of k. And surprisingly, this is a good enough estimate. So, putting everything together, we need the pth moment. We just needed the third and the fourth moment that one can do this for general p of the first order difference operator. That's right here. We condition on the event A1. Okay, so here's an indicator function of. So here's an indicator function of a1. Here's the indicator function of a1 complement. The first one we know, we just agreed that that stuff there is the pth power of log n over n times, of course, the pth power of the indicator function, but that's just the indicator function itself, plus the other part where we estimate the first-order difference operator by the pth power of the area of k, but that's just a constant. The area of k, but that's just a constant. But because of our lemma regarding the R-spindle convex floating body, the second part is negligible. If n is large enough, this is really negligible compared to the previous part. And so whatever is left is just the previous part in which we substitute everything. So we have log n over n to the power p, and then the area of the missed part of this R-spin log convex floating body. Floating body, you know, that's also log n over n to the two-thirds. And so you just add the exponents, you get that this is less than or equal to log n over n to the times log n over n to the p plus two thirds. Okay, so how does one put this together? Okay, so now here's the point when one uses the lower band on the variance. This was about the difference operator on the area function. Now we need to apply this to the apply this to our function f, which is normalized and centralized. And therefore, one would have to divide everything with the variance, the pth power of the square root of the variance. So that's why appeared the variance to the negative p over 2. And then one puts everything together, does the arithmetic, then you get this formula. And if you substitute p equals 3, p equals 4, then you get a particular size quantity that are listed. Quantity that are listed down here. So, this is n to the negative 1,6 times this logarithmic factor in both cases. And if one calculates the other two quantities that I didn't discuss, really, those have the same order of magnitude and one puts them together, then by the theorem of Lushi's J and Pakati, you get this upper estimate. And we have proved the central limit theorem, quantitative one, with this order of convergence. Okay, so a few. So, a few final remarks and notes. One is that the order of magnitude is probably not optimal in the normal approximation band. This mostly comes from the original formula, probably not the geometric estimates. We expect from the various same bands that probably the correct order of magnitude should be the square root of the variance, which is much bigger than n to the negative. Uh, which is much bigger than n to the negative one-sixth. Well, we do not know anything about higher dimensions. Uh, some of the results, actually, estimates, geometric estimates used here, work only in two dimensions. Especially, we only have a lower band for the variance of the area in two dimensions. That has specific reasons. Also, other distances, the Kolmogorov distance, for example. Of course, the Wasserstein distance gives an upper band of the Kolmogorov distance, we take the square root. Distance, we take the square root, but that's usually not sharp. So this method doesn't work for the colour of distance. That's still an open question. And finally, a last remark, generalizations. This was just the simplest sort of generalization of the classical version for spindle convexity when one generates the convex hole by taking the intersection of equal circles. This can be further generalized. This can be further generalized. For example, one can take a fixed convex disk L and take the intersections of the translates. But it can be even further generalized, as it was done by Kabluch Komarinch and Morochanov in a recent paper, where they take a fixed closed and convex set and H, a subset of the affine group of the D-dimensional space, and they introduce a notion of LH convexity, in which case one Case one requires that a set which is LH convex is equal to the intersection of all sets of the form H times L plus V. V is a vector, represents a translation, and all these transformations H. This gives rise to a new notion of floating body, but essentially, as far as I know, nothing is known about this. Okay, so thank you very much for your attention. Okay, so thank you very much for your attention. Thank you very much. Thank you very much, Perenz. Any questions first, people in the room? No questions here. Then onliners, please just shout, turn on your microphone and shout. Don't raise a hand or anything. Don't be nice. Okay, so then we'll move on to the So, then we'll move on to the next question. Thank you again. Thank you very much for the talk. I try to stop sharing. Okay. Yeah, you managed. Thank you.