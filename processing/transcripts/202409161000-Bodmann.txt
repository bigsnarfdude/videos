Thank you so much, Thomas. Let me start by repeating my name. I am Bernard Bodman, and I'm secretly a data scientist. And so today, I'm going to talk about a topic that I think is somewhat still in the background, and I believe it's interesting to look at. And I have sort of a take-home message that may. That may be more general than what the focus of this talk is on. So, sparse recovery, I think everybody has seen. It's been around for quite a while. I got sucked into it, and then I also got sucked into graphs, I think partially when I was here in 2019. And I again want to say it's so great to be here. This is such a great place, this nice part of Austria. This nice part of Austria, where we are in the middle of the birth of data science, right? And so this is work that I did with my former student, Jennifer May, and I acknowledge the support of the National Science Foundation. So the overview is I'll talk about sparse recovery. I'll talk about sparse recovery for linear combinations of heat kernels on graphs. And again, you know, I touched space with graphs at some point, but then I realized that much of the stuff that I had seen elsewhere was possible to transport in the graph setting. And I do think we still have things to learn, also in the context of networks, even if things are nonlinear. It's not as scary as it may seem. As it may seem. And so, here I'll emphasize again the role of fuel certificates for types of convex optimization problems that relate to sparse recovery. And so then, right, I mean, there's two situations. There's the ideal world in which there's no noise, and there's the noisy setting. And the noisy setting really is where you have to start thinking very carefully about how. Thinking very carefully about how to solve a problem. And then in the last part, I will talk about a generalization of what I'll present here, which is a relatively old problem, but this portion here will relate to disease dynamics. And you can think of when or how that was inspired. And so here, here's the big picture take-home message, right? There are some people here. There are some people here who have dabbled in geometry of sorts, right? And you may think of algebraic geometry or Riemannian geometry as very far away from graphs. But that's actually not true. You can take a lot of ideas that are known in the context of manifolds, and you can import them in the discrete geometric setting and make them work there. And so, There. And so, in more detail, I could say the results that I'm presenting here are, in a way, inspired by a paper by Candes and Fernandes Granda, and then also by Rest and collaborators. And what we are doing in this context here is we're just taking what, let's say, in the Candes Fernandana Skranda paper, was worked out with methods of hard harmonic. With methods of hard harmonic analysis, say the decay properties of the Fayer kernel. And we're replacing this with bounds on the heat kernel. So if you want to do harmonic analysis on manifolds, then much of what you will be doing could be built on decay of heat kernels. And so the message here is: if you live on graphs, you can use that very same technology. So, in the absence of symmetries, right, heat kernels. Right, heat kernels are your friend. So let me leave it at that. Okay, so just a background, I mean, maybe just to fix notation, right? Everybody's seen this, but what is sparse recovery? I don't want to just talk about recovering the signal. I want to talk about how I can recover the constituents, right? So how much of a certain dictionary is contained in a signal and which In a signal, and which portion of my dictionary is active. Right, and so then in general, the setting is that maybe this here could be a low-dimensional projection, but then I have noise that is thrown on top of what I'm allowed to observe. And so, if the dictionary were an orthonormal basis for, let's say, a finite dimensional Hibbert space, right, then this is precisely the setting of compressed sensing. Precisely the setting of compressed sensing, but you can also ask this type of question here in a much wider setting. For example, low-rank recovery would be the same sort of thing with, let's say, Schotten 1 instead of L1 norm, but then you're losing the fact that your space has this discrete orthonormal basis. Okay. And so again, examples of sparse recovery. Of sparse recovery that we were inspired by in the context of harmonic analysis. Let's say, here, right, is this paper by Candes and Fernandez-Granda, where you have these signed measures that sit on a circle, and you're only allowed to see the lowest portion of the Fourier coefficients, right? And from there, you're supposed to be able to say where those guys are and what the weather. Are and what the weights are. And of course, this doesn't work quite as well, right? Because if you're only looking at total variation norm minimization there, then moving this guy here by just a little bit would be discontinuous for the total variation norm. And so you have to design some sort of a recovery procedure where you give yourself a little bit of slack, you give yourself some wiggle room, and this is what is. And this is what is in the title here, right? So, a super, quote unquote, super resolution is really you give yourself sort of a level of resolution at which you want to be accurate. Okay, and then Benrecht and others have a similar idea that they worked out in parallel that they call compressed sensing off the grid. Okay, so here the emphasis of these papers was you're Of these papers, was your dictionary is infinite. And then you may think that you're lost because of this logarithmic term in the compressed sensing literature. So as the dimension blows up, you give up. But that's not true. So there are ways in which, even when your dictionary is infinite in size, even if the vector space that you're working on is infinite dimensional, you can still make the process. Make the process or the idea behind norm minimization in order to do sparse recovery. You can make that work. Okay, and so now let me bring you into the world of graphs. And so as I said, this is a very old problem. I'm just going to reformulate it, and then later on I'll explain how it relates to a generalization that, in a sense, you can get for free. Sense you can get for free when you solve this type of problem. So here's the thing that most people who do a harmonic analysis on graphs work with. It's the graph Laplacian, right? So you have some sort of weights that sit on the edges, and you can think of those as maybe as indicating how close the vertices are, right? If you make this guy big, then you force the functions if you minimize, let's say, a type of Sobla. Let's say, a type of Sobolev norm, you force the functions to have very values that are very close to each other on these vertices that are adjacent. Okay, so this thing here, this Laplacian, generates a semigroup, and then I can think of this here, the associated quote-unquote integral kernel, which is called the heat kernel. You can always think of this as the analog of the Gaussian. So, for anybody who lives in the Euclidean. Right, so for anybody who lives in the Euclidean world on the graphs, that's your friend. That's the Gaussian. Okay, it may not have the same type of nice decay properties, but at least it sort of behaves in very similar ways. Okay, and so the simplest possible problem that maybe you may want to think about in the context of sparse recovery is you have a sparse signal, meaning a signal that has very small support. Small support, and then you only observe the blurred version of it, right? Okay, and as I said, this has been thought about for a long time. T is fixed, correct? Yeah. And so then you can do this either with or without noise, right? And so without noise, there's a very simple answer, but in a sense, it's deceptively simple because once you turn it, Simple, because once you turn on even the smallest bit of noise, you'll see this type of problem becomes very, very difficult. And I will show you in practice how difficult it is. Okay, so what we just talked about is nothing other than a standard example of deconvolution. The screen resolution is not good. Okay, too bad. On my computer screen, this looks Computer screen, this looks a little different, but that's okay. All right, this is, yeah, I know it's reconvolved. So you have to take it on faith. This is a picture that I took outside of the window of the plane last night as we were landing. And so there's these blurry things, and I can show you. Well, no, I won't. I have this very old cell phone, right? So my wife always complains that I can't take. Wife always complains that I can't take any good pictures. And so then, what I tried to do is I ran it through a standard deconvolution algorithm. And so what you see here on the right should be significantly better, but maybe not quite perfect, right? It's not quite. So what do you see? Well, these things are really not point-like, which maybe they should. Really, not point-like, which maybe they should be, right? There are things here that were lower-dimensional, the singular portions of the image that got now broken up into these spots. And if you start looking at the clouds up here, then you see that really you're going to pick up noise, right? So the smooth, nice-looking, fluffy clouds have turned into a very bristly pattern. Okay, and so this. Pattern. Okay, and so this is precisely the problem. When you try to do this type of deconvolution, any little bit of noise could possibly be amplified to such an extent that it really distorts what you were trying to gain in the first place. Okay. And so just to give you the theory right behind the easy and the hard portion of this type of problem. So in the noiseless case, not a problem, right? Not a problem, right? The Laplacian here is, in this case, this is the version where the Laplacian is negative semi-definite, right? So this operator here is actually invertible, perfect, right? So you apply the inverse and you're finished. Well, the problem is that this guy here may have a large eigenvalue, sorry, large negative eigenvalue, right? So now you try to invert it. So now you try to invert it, that inverse the operating norm may be huge. So, what people do in order to avoid this type of problem, at least if they want to stay with linear, which we're going to leave behind in a moment, is they invoke a regularization term. They say, I'm only interested in the spectrum up to a certain point. I'm going to cut off there, and then I'm going to invert. So now you're going to have two pieces. You're going to have two pieces. There's an error that I make here, which relates to the distortion that I introduced by this projection for the original signal that I'm really interested in. And then there's the other part here where I have the noise. And in the worst possible case, this noise lives at the band edge, right? And then I'm amplifying it. And so the problem here is that as the T grows, this thing grows exponentially and kills everything. Okay. Okay. And so somehow, when you insist on linear type of recovery strategies, you have to balance those two parameters. And maybe it's not immediately clear how to balance those contributions. And so, what I want to present here is that there's an easy or relatively easy recipe that you can use, which will give you. Which will give you geometric conditions that you can apply that will give you recovery guarantees, where the balancing, in a sense, shows up in a very natural way. And so that's why I like dual certificates. And, you know, also, as a comment, right, they exist in the discrete as well as in the continuous world, meaning if you're interested in low rank, then you can apply these types of concepts. You can apply these types of concepts equally well. Okay, so we're doing the usual linear programming. I have this type of known accuracy goal that I want to achieve, or let's say a bound for the level of noise. And here, this is essentially just some kind of a nicely refined version of Poder, right? So if there is an L infinity function which has nice properties, then Has nice properties, then I can use that to certify that what I just found is an optimizer for the norm minimization problem. And if I look a little bit more closely, then I see I can also add an additional requirement that will give me that this optimizer is unique. Okay, so there's no noise in this portion yet, right? So I'm just looking at the noiseless case. So here, all I need is a strictly less than one of the true support of the The true support of the signal. And the message is that the guarantee for being able to recover a signal comes from implicitly the question, can I solve a certain type of interpolation problem? An interpolation problem, which is, right, so at the support points, I want to get the sign of the signal, right? And off the support, I want to be strictly below the L infinity norm of the signal. The L infinity norm of the certificate. Okay, and now I'm going to get to the conditions that we find when we try to realize this type of strategy in the graph setting. And as I said, so there's always some kind of a balancing going on, but I claim in this situation here, this is a fairly natural type of. Type of trade-off between different quantities that you will face. Okay, so of course, sparsity is going to appear, that's clear, right? Then there is the question of the heat kernels, right? Until what time would I be able to perform this type of linear programming strategy and get recovery? And get recovery because if I let this thing here run for too long, everything washes out, and then I won't have a chance anymore to solve the interpolation problem that I need to solve in order to certify. And then the last one, and this is the same type of idea that shows up in the super resolution paper. For example, I need to have a minimum separation between. Have a minimum separation between the supports, right? Okay. And so all of these three quantities are going to show up. And what's really interesting is, at least to me, so there's one researcher who for quite a while looked into the question of how can you best bound heat kernels either for manifolds or for graphs. For graphs. That was E.B. Davis. And he has this really neat technique in order to get almost state-of-the-art type of bounds. And all that involves is changing a weight on the Hilbert space. And so what's behind the strategy that he was using for these bounds is the question: what is a natural distance that goes with the Laplacian? And so here's a kind of a And so here's a condition that is essential in Davis's work. So the natural distance would satisfy this. So if you took the weights that appear in the Laplacian to divide the distance squared by those weights, then you would have one candidate for these things, which in the Euclidean case would be what you actually have. But this is a more general type of condition for a metric. So in a sense, right, so if somebody gives you Laplacian, then you should ask. Then you should ask: What is my distance on the graph, right? And this here appears to be a very natural candidate for that. Okay, and so then we formulate the result in this type of distance setting, right? And so here are the conditions. It may look a little messy, but I just want you to notice, right? So as t goes to 0, t is the time parameter or the upper bound for the time. The right-hand side here goes to 1, the left-hand side goes to 0. One, the left-hand side goes to zero, so there's a crossover point where you're good, right? Okay, and so then these things here are similar to the terms that appear in the super-resolution paper as well. Okay, so again, this was just the noiseless case, but it's kind of maybe an instructional example, right? This technology will work. Technology will work. And to make it work, right, we had to ask the question: what is the right notion of distance in this type of trade-off situation? And then we built on heat kernel balance that were developed by Faltz, which follow from Davis's work. Okay. And so this was the noiseless case, right, which really was more like a warm-up. Um, a warm-up for the true problem, right, in which you introduce the noise, and so then the hope is right that if you have everything in place, you won't have to adjust much more in order to get what you want, right? And so it's true. So the same type of assumptions that gave us this perfect recovery in the noiseless case now give us accurate recovery in the noisy case. In the noisy case, and I have to be a little bit careful. So, right here, I'm talking about signal recovery. But if you want to estimate constituents, et cetera, right, then this norm bound here is going to be your friend because you're going to say, well, if there are contributions to the signal which come with a total mass that's above some sort of an error level, then I know that I'm going to identify the support correctly, and I have an additional norm bound for how far I'm off with the coefficients. Off with the coefficients. And as you can see here, everything scales proportionally to the measurement error, which was the goal of the whole thing. And then also, the same type of term that showed up in the conditions for recovery is now this multiplicative constant that shows up in front of the air. The error. And unfortunately, well, I've tried to get rid of that thing, but I could not. So, not with these techniques. So, the problem here is this is, in a sense, the same as the deterministic RIP stuff. And so, there I, you know, I have that square root. Yeah, yeah. So, if some genius comes along and has. Some genius comes along and has another idea how to prove it that just does not use heat kernel bounds, right? I think I can come up with situations where this is this is the right upper, I mean, this is a bound that's the relevant bound. And the reason for that, as I said, is I can also come up. As I said, I can also come up with deterministic settings for measurements where the measurement is stupid, right? And I have the same type of bound. Okay. Yeah. So in any case, it may not be as exciting as it could be, but this is the best we could get. Okay. And then, as I said, this was originally motivated by another question that was. Another question that was a little bit more general. And that question is: so think of this deconvolution problem as being, you know, a physical problem where you have a pot of water and you put little droplets of ink on the surface, right? And then you watch them diffuse. And you want to ask or solve the problem of where did these guys come from and how much ink did I inject, right? Well, it could be that these. Well, it could be that these ink droplets were injected at different times. And so then I would want to ask myself, can I solve the same type of problem under this more general setting? And again, it's really nice that the warm-up rounds that we have done, they work and everything goes without much modification. Okay, so we can solve this type of problem in the same way. Problem in the same way as we solved the preceding problem. Okay. And so the only problem is that in this context here, I would like to get away with the technology that I've already developed. I don't want to have to solve a new interpolation problem. And so the idea that we use is just, well, you had a dual certificate before. You had a dual certificate before, or the problem that came with one fixed T, just use that same thing. So now it's not the dual certificate that you want, but at least, well, it's an approximate dual certificate, right? And again, there's another paper that can help you use that. Okay, and so there's a little bit of a price to pay, right? So now we have an additional condition here. But again, I mean, this is. But again, I mean, this is similar to other results in the literature. And then, as you can see here, right, this constant goes up. But okay, I think we can live with that as long as it's proportional to the measurement error. And so, again, to summarize the main results, so dual certificates are useful. So, dual certificates are useful also in the setting of graphs. They give conditions for recovering sparse components or sparse signals where you have, in my opinion, a fairly natural condition that comes in about admissible sparsity patterns, right? If your scale of distances is allowed to vary a lot, then maybe. To vary a lot, then maybe you do not want to allow every possible so-and-so sparse signal in your setting of problems where you want to recover. And then, so what we used here, essentially, as I said, the central piece of technology is these heat kernel balance, because there's no symmetries, right? You can't do, I mean, well, so you can think of the eigenfunctions of the Laplacian as sort of the, oh, this is. As sort of the, oh, this is like my Fourier transform, right? But really, only, I mean, the only thing we know there is the lowest portion of the spectrum, how that relates to the graph geometry. And so the heat kernel bounds here are, in a sense, universal. They go across a broader range of scales. And we use those in order to construct the dual certificates for the recovery problem that we're after. Okay. Okay. And so just to give you again, we've kind of gone backwards in time. The problem that really motivated the whole thing, and then it was, you know, boiled down to a sub-problem and then boiled down to a sub-problem. This here is 2020, a few months after our workshop here in 2019, right? And so there you see, this is the United States. See, this is the United States, which you can think of as a big graph. And so there's just a few points that light up on March 15, and then you see things spread. And you see things spread. And then if you go two weeks further, it's all over the place. And so in this type of situation here, what you're really interested in is the question, where are the new hotspots? And so for that, right, what you would do is you run your data. Would do is you run your data, you let it diffuse, and then you compare with what you observe. And maybe you also have a table of community events, right? So there was the choir meeting or the Harley-Davidson Club or whatever. And they met at this time at this location. And you want to find out, was this meeting significant for spreading, right? And in that moment, you solve a sparse recovery problem because you want to find out, was there an added hotspot that appeared here and here and here? That appeared here and here and here, right? And if your time intervals are sufficiently small, then the hope is you can identify those, then perhaps that might help with the next pandemic. Thank you so much for your attention. In fact, Ben and I were on the same fight yesterday and at some point during the fight you still designed, please pass through the feet and hold on to the that's something you usually don't understand. That's something you usually don't already understand because then I of course was not the only cell phone in this day. That's right, that's any questions. Yes. Oh, that's an interesting question. My name's Dustin. I don't know. I should have invited you to the defense. Yeah, no, it's a good question. You're right. So, yeah, there is an information theoretic limit. I have no answer right now. Sorry. Yeah. But good question. Related to sort of the question. Related to sort of the question discussed in Thomas, in one natural information theory for the fixing theory. Mixing time is of course very important. So you want to think of this thing now as a dynamical system? Yeah, sure. Sure. Yeah, again, I'm drawing a blank. So the main point, right, is, I mean, so maybe we came from a much more pedestrian point of view, which is why I ignored these sorts of questions in my head. The main point was, you know, there's this earlier paper on Euclidean spaces where they use the decay properties of the Fayer kernel in order to establish these bounds. In order to establish these balance. And so here we're just using the decay properties of the heat kernel, right? So I am, in a sense, closer to still to the harmonic analysis context than to the information theoretic context. But also a good question. Thank you. Okay. Well, thank you very much, Perla.