Thank you very much. It's great to be here, team of Dr. Bradley. This work is joint with my PhD student, Dagos Dell. I'll be talking about differentiability of shape functions and regularity properties of shape functions are Properties of shape functions are not very well known. The general setup is like this. This is pretty generic. So we are in R T and we are looking for an optimal path connecting to points X and Y and there is some action or energy or cost assigned to each path like that. And this may depend on the geometry of the path, and it may depend on. Path and it may depend on the interaction of the path with the environment, which is usually assumed. Well, in the process that I'm talking about, it's assumed to be stationary with invariant on our shifts. And so to each path, we assign some action, uh and and then we minimize the action over all admissible paths. And admissible paths can be, you know, different from uh from problem to problem. So From problem to problem, so lattice problems may be something in continuous space, maybe directed, so there may be a notion of time, maybe not. And in all these problems, essentially subductivity implies that you can define the shape function, which is the average growth of the cost. Try to connect zero. To connect zero, well, any point, but you can choose zero to a distant point. So you take, so v is a vector, and you multiply this vector by a large number t, so you call it in some interaction v, and then you divide by the same constant, take the limit, and the subversion of Sabette-Pergoric theorem will imply that this limit exists, and it is a term. It is deterministic, and in most situations that I'm gonna talk about, it's convex, which is usually which is also a simple argument where you just, if you can concatenate paths, then you can prove x. And so explicitly, shape functions have been computed for a number of models. So this is a list that I compiled. So this is a list that I compiled. At least some of these papers compute shape functions explicitly. Some of them can be just interpreted as calculations of shape functions. This is usually due to either some symmetries in the system, maybe there is some group of mice transformations, or maybe there is even an exactly solvable situation. Solvable situation. And I'm sure that this may not be a complete list, but you know, let me know if there is something important missing. So for this talk, most relevant actually, so these are my papers with Eric and Cost and Costia and also with Liam B here. Uh with L and B here. Now in all these situations shape shape functions are known to be to be convex. So this claim was a little sloppy in all explicit examples differentiable and strictly convex. Of course, if you don't require a finite dependence range, everybody knows that you can create corners and flip edges. And also there is this issue that And also there is there is uh this issue that um uh you can you can have a precolation code. So these are sort of exceptional situations that I I don't want to talk about but it's it's understood that in in all these in all the situations where understood it's it's conjectured, it's really natural to conjecture if you look at all these explicit examples and simulations and all that. And simulations and all that differentiability and strict convexity, first of all, it calls for a broad class of situations. And also, this is associated with the KPZ universality. So this local quadratic behavior of the shape function is somehow related to KPZ, although I don't think that there is any explicit theorem that says this. Now, strict convexity, uniform A uniform curvature bound would imply existence and uniqueness of Lewismann functions with each isymblatic slow one-sided geodesics, in positive temperature case solving polymer measures. But even even differentiability, as we know from the work of these esteemed people. These esteemed people. Even differentiability allows to make pretty strong claims about that. So you still can claim that for most points you just have in the zero temperature situation. We do have one-sided geodesic. You have to say to be more careful about flat edges, but you still can make these claims if you have differentiability or shape functions. For the lattice model. Of shape functions. For the lattice models, that's what these papers are about. Even differentiability at one point, well, two points at the edge of the percolation code for these models is a notable result. So in this talk, I will talk about a few models, well, actually classes of models. I will just give you some assumptions. I will just give you some assumptions on ingredients, and these are pretty broad. We have the shape function. We don't know what it is precisely, but I can prove differentiability of the shape function. And so these situations that we have written have completed theorems. So these situations are continuous based on direct polymers at both zero and positive temperature. And also, so this is the paper that we just. So this is the paper that we just uploaded to DART came out today last night So that's about Hemingway equations with dynamic elements and you will see from my story that the method should apply to even broader class of objects. I will start with talking about this older older case where Older case where we were in the in this paper with Eric and Koista, we considered Gurgi's equation with Poissonian noise. And so it's the situation where we have this kind of quadratic Lagrangian. And the story was possible, you know, the story with constructing Constructing one-sided minimizers and Busimon functions and global solutions of the Gergers equation, developing the ergodic theory for all this. So it was possible because due to this special, because quadratic Lagrangians are special and you can derive at the model with shear invariance. So shear transformations play an important role in this. So what is this model precisely? So, what is this model precisely? So, there are you have space-time. First of all, this is in dimension, this picture is in dimension one plus one, although it's possible in higher dimensions as well. And you have Poissonian points in space-time, and each sort of carries weight minus one. And to each path you associate this quadratic kinetic energy of the path, and each Poissonian point you visit gives you a weight minus. gives you a weight minus one. So the best path, so it tries to go through a lot of these awesome points, but you just can't. You have to take into account that you cannot develop too high speeds. So you want to minimize this. And so there is a shape function that is precisely quadratic. And in this case, it's due to this essentially. Is due to this essentially to this simple computation based on the shear test formation. So, this is just the same action, I'm just copying it. You can start with a path that starts at zero and terminates at time t also at zero, you can add zero to zero. And then you can apply the shear transformation to it. So, shear means that you add, I mean, in this space-time, it means that you sort of add a constant velocity, so this was the original path. So this was the original path, then you apply this information, and it requires this additional slope. And so what is the kinetic energy of the new path after the transformation? It's just the S because the velocity of the path increases by, well, changes by V. And then you just do the simple algebra, you expand this. So you get this term, which is the energy of the original path. Original path. This is zero because your path connects zero to zero. So this term doesn't contribute anything. And then there is this v squared grade that from 0 to t and you get this term. And if you're interested in the shape function, well, first of all, it's clear that to minimize this plus Plus, well, plus the contribution from plus seven points is the same as to minimize just this term. So it means that the shape function means that the shape function for v, for v is obtained from the shape function at zero plus v squared. So what I forgot uh to mention is that if you must also apply the shear transformation to the Poissonian points and The Poissonian points, and the trick here is that the Poissonian point process is invariant on the shear. That's why this calculation is the same in the original environment and in the sheared environment. I mean, they're both shears of each other. So this works extremely nicely in the quadratic case. And if this is not quadratic, then Is not quadratic, then you know. So, for years we didn't know what to do with this, but at least now we can see something. Now, all this works for a variety of other models where there is something quadratic. So, for example, you can work with a model like this. So, this is the kick potential, right? So, the time is discrete, but the pattern. And but the paths the paths are in the contin in the continuous piece. So this is still the kinetic action, and this is what the path takes from the environment. And so that's what's first considered in this paper of mine. But you can also consider a model where on each of the horizontal layers that are enumerated by integers, you just consider Poissonian points. You just consider Poissonian points and you just declare that your admissible path must go through one well, so a Poissonian point on each layer. And then you just compute the kinetic action. And this is also shear invariant, cle clearly. So when you apply the shear transformation to this, you just shift all these Poisson elements and they are being independent from each other from layer to layer. From layer to layer. And there are other models that you can create for the library. Say comets in Yashida Brownian polymer, where polymers pass through this like Poissonian. Each Poissonian point is equipped with a horizontal window. It is also here. And you can attach other things to it. It's a whole class of models. So, this is maybe a little distraction, but I can't resist doing this. So, this is a simulation of these minimizers in this Poissonian, right, for this model. And so, this shape function is quadratic, but I just recently realized that if you want to interpret this in terms of not just shape functions, but limit shapes, Unit shapes. If I construct, if I look at all these points whose action is less than a large number, and I part them, and then I let this number go to infinity, and if I re-normalize it, it's a totally obvious calculation from the shape function. But it turns out that the limit shape is is an ellipse, which is which is kind of cute. So I I it was it's really tempting to show this feature. It's really tempting to show this feature. This is a distraction, but there's no new content really. I was giving a talk for high schoolers, and this is a teacher. So what if the Lagrangian here is not quadratic? So then we still can prove this. then we we still can prove this so this is this is a new result with uh with Douglas Dell so the model is once once again we have these layers and I'm interested in a in a path that optimizes this action. So the action is composed of two parts, so there's this generalized kinetic action, or Lagrangian L that depends on the increment. On the increment, and also F is the external potential that path is going to pick from the environment. We don't assume much in this. So for F, we assume that, in this paper, we assume it's sort of white in time, IID in time, so they're independent from layer to layer. It is stationary in space. Um we require it to be continuous, but it's not a real uh just for some measurab measurability um considerations. We needed for this paper we decided to take it rebounded from below, which is because we wanted to to to write I mean it's easier to check the stability of ergodic theory. That's that's not really So notice that it's not it's not even assumed to be to be like ergodic in sp in space. Like ergodic in space. So, this is a very broad framework of potentials that you can use. Now, the L here, it doesn't even assume it is not assumed to be convex, but we do require certain regularity. So, L is C2, it should go to infinity as the argument goes to infinity, and there is a requirement. And there is a requirement on the second derivative. So the second derivative is sort of controlled by L itself. And so, you know, so any function that looks like this works. So, for example, like polynomials, like this. Even power plus some smaller term. And under these assumptions, we prove. Under these assumptions, we prove that there is a okay, so deterministic and convex shape function, well, that's all. So the new thing is differentiability, which was not the one. This is the definition of the shear function. So again, so this is the old thing. So we take m to infinity here, connect these two points to the optimal action. Of optimal action. So, this is the old standard definition of the shape function. The formula that we prove, though, is that the derivative of the shape function is given by this. So, we take the optimal path connecting 0 at time 0 to point N B at time N. And along this path, you compute all these derivatives of L and take the average. In particular, we show that this limit exists. That this limit exists. This equal to this limit. And the proof, it's a soft proof. I'm going to show it to you. It's also based on the shear transformation. And this is the formula for the shear transformation, and I will I'll be applying it to the I'll be applying it to the points in space-time, but I will also apply this to paths. It makes sense, it turns out, to consider paths that connect zero to zero and cook all other paths from those using the shear transformation. So, in a sense, what happens is that, I mean, this is really like the old. This is really like the old formula for the, you know, for the original action. But now instead of phase path, I consider paths connecting zero to zero. And so if I want a path that connects zero to N V, I need to add add V to each step. And because the potential is IAD, The potential is IID in time and stationary in space. Again, under the shear trans under the shear transformation, the distribution of thres doesn't change. And so if I'm interested in computing the shape function, I can use only paths that go from zero to zero. I must compute the action that depends on v now. Instead of going to a different endpoint, I will. going to a different endpoint uh I will I'm changing the the kinetic kinetic actually um and I can so it doesn't matter because this distribution these two things are the same I can I can compute the limiting well the shape function so this is the old definition but I can also compute like this and it turns out to be I mean It turns out that it's a super useful point point of view because now I can use the same path here for different V's. And that's the whole idea of the proof. Now, so there's this claim here that actually the way B depends on V is much nicer. On a V is much nicer than dependence of A on a V. And so here are some simulations actually before we go to the proof. So these simulations, instead of this continuous F, so this is done for this Poisson point process. And this is uh this is the graph of of the of the of the original action. The original I remember is A. So I just choose the endpoint N. So I take N, and then for various V, I'll just compute the optimal path and plot its normalized average cost. And it's a, I mean, probably you've seen it, I mean, if you ever tried doing this, it's a. Now, in terms of D, this is what you get. Right, so I choose Portic. So instead of quadratic for the Burger situation, I'm using Vortic here, but you will see essentially the same picture for whatever you choose. The reason for that is that, so this plots all optimal paths. So if v ranges from minus one to one, From minus 1 to 1, and I'm plotting optimal paths that realize B and R B. So, what should strike you in this picture that there are not so many of them? What happens is that the same path is optimal for a range of feeds. Because the it's the same in the same environment, right? It's it's lit literally the same path connecting zero to zero works for the whole um uh range of these. Range of this. Now, had this been quadratic, you would have seen just one optimal path that works for all in this picture. Total shape, precise shaving variance of that. So here you have you have some, like a few a few if a few paths. And so that's this why the the the um Why the problem now is easy to believe that it's really smooth, the shape function. So, this is the proof. So, take the optimal path for the n of b, okay? So, in this proof, all these paths now they connect zero to zero only. So, I'm working I'm working so you know, maybe later I I will want to like shear to it to go back to the the original setting. for better the original setting. But here I'm I'm taking the optimal path that goes from zero to zero, but I'm computing the optimal action associated with v. But I'm using this action to estimate the best action associated with a different slope or different parameter of the action that is closed. So this is smaller than this, right? So because this is the optimal action. This is the optimal action, and this is just some path, some, well, it's optimal for V, but it's not optimal for W, so you have this inequality. And then you Taylor expand this. And so this is the zero-order term, this is the first-order term, and you just differentiate L spectral, all those grave lines. So this is the quadratic term. Now Now for a second you may choose, you may, I mean, for the sake of this presentation, you can think about maybe functions L that have bounded second derivative, but still a large class. But we actually prove that for functions from this class, this is also bounded because you cannot, I mean, L second derivative, it's controlled by the L. It's controlled by L itself under L assumptions, and you can't accumulate two large values of L. So this is bounded. Okay, so how does it work now? Take W minus V positive. So W is on the right of V. Then just move this to the left-hand side and divide by W minus V, because it's positive, the inequality is preserved. And you also divide by M. And you also divide by n. And on the right-hand side, what you get is you divide this by n. And I'm just saying that this is bounded by constant times W minus V, which is certainly true if L has bounded secondary rates. But something like this is also true in the general case. And then what you have to do is, you know, you take n to infinity and you take limin of both sides. Of both sides. Well, we need to mean on the right-hand side, and that's what it is written, but on the left-hand side, we know these things actually converge. This converges to lambda of w, this converges to lambda of lambda of v when you divide by m and now we know we're here and the next thing is to take w to v and under this assumption And under this assumption, so it means that w approaches v on the from the right, and so this, because we know that lambda is convex, it converges to the right derivative at this point at point v. And the right derivative is bounded by the limit of s and so this this term this term disappears. Alright? And then you do the same on the left. So now you see W. It's a bit strong, right? You see, but you don't even need a second derivative. Right. I mean, make this is an overkill. But you know, you need some assumptions, and this is already pretty general. So you can do this, the same thing to estimate the. Is the same thing to estimate the left derivative. So there, so now W is less than V, so you're approaching V from the left, but now when you divide it by W minus V, the sign of this changes, right? Because this was negative. So now this is an estimate from greater than. But you do the same thing, and now it equals soup calls. Sides and the right on the right side, you know, you get what you get. On the left-hand side, you have this converges to this shape function at W, this converges to this shape function at V. And then you take W to V on the left, and you end up with this inequality, that the derivative on the left is greater than min Su. Combine these two. Combine these two things and you will get so the right derivative is less than limf, which of course is less than C, which is less than less. But my function is convex. So convex function, the left derivative, is smaller than or equal to the right derivative. So it means that all these things are equal to each other. And so both derivatives on the right and on the left. Derivatives on the right and on the left, they inside, and also automatically proves the existence of this limit. That this limit actually computes this derivative. So we have all that. Derivative of lambda is given by this formula. So this formula is in terms of those paths that go from 0 to 0, but if you go back to the But if you go back to the original coordinates, maybe it's wrong to use the same gamma or something. But this is what you can do with this argument. It looks very basic and soft. So you don't use anything for F, except for stationary. You said the words had to be smooth. So all that was really, I mean. I mean it it takes more more pages to really to make sure that the you know subjectivity works. Oh, that's fine. For the proposal case. So I mean all this theory and this one and the other two cases that I'm going to show you basically you say so I mean you would it would be easier to say that suppose shape function theorem holds and then you need just a couple of assumptions. And then you need just a couple of assumptions some some additional assumptions on on L and practically nothing. Yes? Does it work for F equals one, identically? F equals one. Optimal paths are s straight, straight lines. So it's so you just i the regularity is the same as uh as uh as as L. So the environment is not providing the regularity. Can you go back just to the VN? You need things to be shear-invariant and if you go back to the formula for BN for BN, the definition of BN, like after you did. Of EN. Like after you did it shit. Yeah, so see, the dependence on the V now is on the L. The L is deterministic in differential, so PM over L. That's always the shear that took the V from the F to the V. Again, I still wanna. Okay, wait a second. I'm not done. You know, we're wasting my time. So the same thing calls for positive. thing holds for positive temperature. So this is the partition function for a Polymer model. So the same energy, maybe there's a temperature in front, I don't care. Now this is the end point. On the right it appears, this is the small y here, but so it's a point-to-point polymer connecting zero to point y at time, at time end. And so for this you need a little more requirement just because we Because the polymer measure needs to be well defined, and so we need a little more here. But the same theorem holds. So, for the positive temperature version of the same thing, the rate of growth of the free energy, which is minus one over n logarithm of the partition function, it is also smooth in space and In space, and the derivative is given by this. Now, this is averaging with respect to the polymer measure. Now, the proof is really the same. I mean, it looks more complex than it should, but it's actually very easy to decipher what happens. So, w is close to b, so this is what the definition is, but then you rewrite this. Definition is, but then you rewrite this using the Taylor expansion, using yourself binding to value v, and you have some contribution from the second derivative. But then you treat this formula as you can rewrite it in terms of integration with respect to the polymer measure connecting 0 to V M. And once you do it, I mean, we can check all this, but let me keep going. But let me let me keep going though. It's probably believable that it works. So, the paper that we uploaded just a couple days ago, it actually deals with continuous time and only stamina-Nerikovi equations. So, if you watch, if you looked, if you've been following, so shear invariance is still an important ingredient in this. So, the model, it's not shear invariant because Is not shear invariant because L is not quadratic. But still, the environment F is shearing shear invariant. So this is a step away from that. So now the time is continuous and I'm computing this kinetic energy of the entire path, whatever you accumulate from the environment, now in continuous space-time and in multiple dimensions, it doesn't matter what. I mean, that. I mean, that argument doesn't care if it's one T or multiple functions. But we no longer assume that F is widening time, so instead we have a convolution and we have Poissonian points, again, in space-time. But we convolve them with a kernel, with a kernel of phi that is extended both in space and time. In space and time. And actually, we equip each Poissonian point in space-time with its own kernel. So it's like it says something, a smooth version of the Poissonian point field. And it's not white anymore, right? Because you can all in each Poisson point comes with some kind of shape around it. I mean, so they have to be, it still has to be. So they have to be it still has to be uh C2. So it's not that like these boundaries are brought. And you know, for years uh I've been asked like can you do anything for non-white noise? I would answer no and I would say well because if you have a dependence like this under the shear transformation you will get will get these shapes instead and so this they're not the same so this is not shear invariance and so what can I do? This is not shearing variance, and so what can I do? But we can, it turns out we can use the same trick. We can apply the shear transformation to these. So we obtain a whole new ensemble, a whole new potential that is based on these sort of shared versions of this. But then we can consider the original well, the same the same field, but sort of centered in those. At those. Then we can track how these how these kernels, how they get perturbed under these shear shear transformations. So you only need small shear transformations. Yeah, right, right. You don't need from here to V, you just need around V. Right, so these are local, uh local pert perturbations. I mean, this is this is exagger exaggerated, so we're just getting s small perturbations. Um yeah, I'm just so you Yeah, I'm just so you just so for this because we want to interpret this in terms of Kenneth-Micov equation and the existing theory, so we assume that L is combat, less differentiable, coercive, and we still need the control of the second derivatives. But under these transformations, we can obtain this, we obtain smoothness, and we also can interpret this smoothness. Interpret this in Hamilton-Nicobi terms because this is a fundamental solution of this Hamilton-ecobi equation where this Hamiltonian is the general transform of this cell. I think there is a mistake because there should be a minus here or something like that. So the cost, the optimal cost given by this, solves the Himbler English equation with these initial conditions, and you can. We can give a theorem that in the context of Hemingkobi equations, shape functions are called effective Lagrangians. So the effective Lagrangian, so the fact that it exists, it's not super new, but the fact that it's differentiable, there is also a formula that looks trickier. So that's our new result. Now the literature on Maybe I should really stop. I mean, in terms of Hamilton equal equations, you can say that homogenization holds and the limiting Hamiltonian is the transform of the effective Lagrangian and smoothness of this. Smoothness of this means strict convexity of the effective. So the literature exists on these models. So there are some zero viscosity results and positive viscosity. So we have written this only for zero viscosity, but it should work for all kinds of other situations. So maybe this isn't I mean I hope this is a complete list, but maybe not. So tell me if if you know more. So tell me if you know more. So I was planning to participate in the open problem session, so there are some questions that you can ask. I mean, this should work also for continuing. These first two, I'm pretty confident that we can do it easily. On general potentials, I mean, it would be nice to go completely away from shearing variance because it still sounds. Shouldn't variance because it still sounds somehow fair with the Poisson points. Um but does this say anything about lattice models? Um or of course the it would be nice to use these formulas for the shape function that we get to get strict complexity. It is sort of plausible. I thought I had a slide about that, but maybe I just skipped it. Or maybe I should just pass postpone this to the following session because I'm not thank you. So I guess the argument wouldn't work if you examine the second detention. But this estimate that that I'm giving you, it's a one-sided estimate. And so in for the convex function, So, for the convex function, I mean, we already know it has support. So, lower estimates are sort of given. And what this estimate does, it provides a matching, we call this condition something like approximate local linear domination of something, right? So, if you have a condescend, if you want to show differentiability, the fact that you don't have a corner, right? You don't have a corner, right? So, what you prove is no corners. So, you know, you already know that from below you have all this estimate, but you need a matching estimate from above, that the same linear function estimates one dp on the right and on the left. So we sort of do these approximations. But in terms of second degree, this doesn't say anything. I mean, we tried, but it's useless. Why do you have to use the voice, right? Yeah, it also doesn't work for smoothness of the Hamiltonian somehow. I mean, it would be nice to prove smoothness of Hamiltonian. Smoothness, I mean differentiability, and then it would imply supinaxity of the Lagrangian, which would solve obvious problems. But that doesn't work somehow. Yeah, so your very nice proof you have this up on the right and the left and it works out. In higher dimensions, this seems like it is more complicated, but differentiability of a convex function in multiple dimensions still means that the sub-differential consists of unique points. Essentially, the same proof can be. Yeah, it's it's the same sandwiching you just show that the same linear functional the same Taylor expansion gives you an in a processing with upper upper bound linear upper upper upper bound. So for the background what you kind of need is that if you do the shear transformation around V then your background should change too much right don't even well I mean you you need to apply the shear also to maybe Apply the shear also to in the case of this continuous Hammers and Flooding collision. So you have to do it everywhere. Yes. Right. So it's not that you mean when you buy the shear when you apply the shear to your right, so to go from this picture to this picture. So this is now not a realization of your original environment anymore, right? But you can effectively very nicely. But you can effectively very nicely cut it to a distributional copy of the original. These shapes are... So that's not so obvious if you don't have this invariance somewhere in your original. Well I mean I look I have an idea but I know I mean currently there's nothing I can tell with how it does. Why do you do this, right? Just to show you variance. Just your shear invariance. That's all you need. Well, so right, so if we right, I mean, for the AD, exactly, right, right. But you can, I mean, it's clear that this argument applies to all kinds of interpolations between these various settings that I told you about, right? I mean, you can have white noise for the Hamilton-Nicode equation. I mean, you have to The equation. I mean, you have to make sense of what the path really, what it means for the action. But it's harder to deal with a non-white matter. There's this new element that you don't have precise shearing variants. It really changes what the stuff. So, you know, we we had to make some decisions what to write for for you know when it was clear that this argument works. I mean you can apply it to I mean you can apply it to