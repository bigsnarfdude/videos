Okay. Thank you everyone. Thank you for being here. And thanks to the organizers for this wonderful event. So I'm going to be talking about learning in strategic environments and specifically I'm going to talk about calibrated agents and at the very end I'm going to connect it to general information asymmetry in games. And my talk will be reminiscent or will have a lot of things that will remind you of the talk. That will remind you of the talks of Semina this morning and also John Schneider. Okay, so repeated styleberg games, why do we like them? Well, there are many real-life problems that we can model as a repeated styleberg game. There are two important applications. One is styleberg security games. Okay, cyclebrick security games and also a more recent version of a type of cycleberg games for strategic classification where we are trying to create a classifier, a machine learning model, that will take as input different features for individuals and will produce an output. Saying that the output is going to be the probability that the person will repay a loan. And people try to strategize in order. And people try to strategize in order to cross the threshold given the data that they have. Now, for the purposes of this talk, I'm going to be focusing on Stykelberg games for discrete action spaces. So I'm mostly going to be playing around with Staglberg security games. But everything that I say generalizes to continuous action games. Okay? Good. So, what are repeated styleberg games? Let's see an example of an instantiation of a Example of an instantiation of a security game. Assume that you have an area that you're trying to protect, and specifically, there are two targets. The targets are the elephants and the chimpanzees. And there's a game that happens between the defender and the attackers. The defender wants to allocate resources optimally to protect the targets. And the attacker wants to optimally identify which targets to attack, given the defender's strategy. The defender strategy. To be a bit more formal, and again, I'm still at standard Steitelberg Gamesland. To be a bit more formal, we are going to assume that we have N, agent actions, and N, and K, principal actions. Oh, by the way, throughout this talk, orange corresponds to things that are principal-related or defender-related, and blue will correspond to things that are attacker or agent or problems. Agent, I'll call them sometimes deleted. And of course, you can also have randomized strategies both for the agent and the principal. So, what is the protocol of interaction for these games between the principal and the agent? So, there's a repeated interaction that happens over a sequence of capital T rounds. At the beginning of each round, the principal decides a strategy to play H T. This can be from the randomized strategy space. The strategy space and announces the strategy to the agent. So the agent then observes HT and plays an action YT that belongs to the best response polytope that corresponds, sorry, YT is the best response for the agent given the principal's action HD. So this is the best thing that the agent can do at this point in time given that they know that the principal played H D, committed to H D. HT, committed to HT. Then the principal will observe the agent's response, YT, and then calculate their utility. And to see an example of how these things can be computed in real time, so let's assume that the HT that we have is, you know, with 30% probability covered the left target, and with 70% probability, cover the right target. Then the expected utility from the agent side is going to be, you know, the expected utility for the agent. Expected utility for the agent, given that the principal chooses a strategy from HD, and you know, they're going to calculate what is their expected payoff if they attack the chimpanzee, and also their expected payoff if they attack the elephants. They're going to see that attacking the chimpanzee is better, so then they're going to choose to attack the chimpanzee. That's what it means to be a best response action to the principal strategy. Okay? And then for step number three, the principal will observe that, you know. Will observe that the agent attacked the chimpanzee, and the principal will update their learning problem where they're going to say, hey, the utility, the expected utility that I got from deploying HD is 2.8. Cool. So what is the goal in these games? The goal is to take the principles perspective and try to approximate what is called the Stackelberg value here. And this is similar to the Statlberg value that Semina talked about earlier. Value that Semina talked about earlier this morning. So, V-star is going to be the best that the principal can achieve, the best utility that they can achieve, had they committed to this utility, to this strategy, and then allowed the agents to best respond to it. And the goal is for the difference between what the principal gets from their learning algorithm to be close sublinearity to be start, the optimal strategy. Uh, to this start, the optimal strategy, the optimal uh cycle profile. Cool. So we love these games. Uh, we have worked, a lot of people in this audience have worked extensively on Rupidian Cyclebrook games, myself included. There is one key problem with these games, which prohibits them from being actually super applicable in real life. And the problem is here that we assume that the agent has to know exactly. The agent has to know exactly HT and best respond exactly to this HT that the principal commits to. So we introduced the setting of calibrated cycle regains where instead of saying that the principal will report exactly their HT, we're saying that the agent is going to build calibrated predictions PT, and then the agent will best respond to these calibrated predictions. These calibrated predictions, and what they play is this best response to these calibrated predictions. And how do they build the calibrated predictions? Well, at the end of each round, we can assume that the principal reveals fully HT that they deployed at this round. So, you know, for next round onwards, the agent can use this knowledge and adapt. Or actually, we can even assume that the principal reveals an action sampled from HD, and this the results of these papers hold. The results of these papers to hold. Okay, and our goal here again will be to take the principles perspective and try to approach the static work value. So one thing to clarify here is that my benchmark will be again with regards to the best response of the agent, have they known exactly the principal strategy, but the algorithm that I'm playing as principal does not use the fact that I'm committing to something. That I'm committing to something. Is the setup clear? Yes? The sample that they, like, you mentioned that you can also observe a sample in Park5 instead of just HTTP. Is the sample the same one that the principal or the agent gets their utility from, or is it an independent sample? Ah, good. Good question. So the agent will get their utility according to that sample, yes. Okay, to the same one they have to. Yeah. But on the previous step, they only. But on the previous step, they only see their utility. Okay. So, again, as a reminder, this is the difference between regular cycloberg games and calibrated cycloberg games. Basically, in regular cycloberg games, the agent best responds, VR, throughout this talk, is going to correspond to best response to the principal's action, as opposed to calibrated cycle per games, where the agent best responds to the calibrated prediction of the principal's action. Of the principles action. So, what are the questions that we want to address in this setup? Question number one: I mentioned that there is this benchmark that essentially is the benchmark for a setting where the principal can commit to an H star and the agent can observe this fully. So, what is the relationship between this V star and the principal's optimal utility that can be obtained in these calibrated cyclical games? Calibrated cycling of the games. Question number two: Are there natural agent forecasting algorithms that satisfy a desired notion of calibration for this setting? To say this differently, I told you that we are going to assume that our agents collect information for previous rounds and somehow build a calibrated forecast of the principal's actions. And I'm going to tell you in contribution number two, essentially. Contribution number two, essentially, that we provide algorithms for the agent to do that. And just to maybe reveal some of the results that I'm going to talk about today, we actually show in that paper that the principles optimal utility in calibrated style over games essentially converges to V star for question number one. And for question number two, we're going to provide parameter-free forecasting algorithms for. Algorithms for the agents to build adaptively calibrated forecasts. And I'm not going to have time to talk about this part of the work, but feel free to ask me afterwards in the TE session. We're going to talk about the first one. Okay? Questions? Yeah. So, before I jump into giving you the intuition behind how we obtain this convergence to the optimal utility, to the VISTA. Optimal utility to the V star. I want to give you a primer on two things. One is a quick primer on calibration, and two is a quick primer on how do you solve standard style games before you have any assumption in calibration and whatnot. Okay, primer number one. Very quickly on calibration. And it's very nice that I'm following Rato's talk. So roughly, as we saw in the previous talk, forecasts are calibrated if they are. Forecasts are calibrated if they are unbiased according to their own predictions. And to be a bit more formal here, and by the way, I'm going to be using the definition found in Foster and Hart, but I have adapted it slightly in order to match the notation that I have on his talkflow. So we are going to create what is called a binning function. A binning function is a function or maybe like a set of bins essentially. A set of pins essentially for all the X's. Oh, you cannot see what I'm pointing at here. Okay, so for all the X's that belong in the randomized strategy space for the agent, I'm going to create essentially a bin. Okay? And this bin is going to be activated every time that the P, the prediction, equals X. Okay? And Okay? And of course, when the x's can be long in a continuous space, this means that the binning function essentially creates infinitely many bins. But let's roll with this for now. Good. So, after we have defined the bidding functions, we can define calibration error as follows. The calibration error is going to be defined between an actual principal chart. An actual principle strategy and a prediction, a set of calibrated forecast essentially. Okay. And it's a summation over all the possible bins and over the average number of activations per bin. And it's multiplied by the L2 norm of the average forecast for activating bin X minus the average true strategy. Minus the average true strategy that activates bin X. Okay? And then we say that a sequence of predictions are calibrated when the calibration error is small, as defined by this equation here. Now in our work, we're going to define something slightly different. We're going to define adaptively calibrated forecasts. So the first thing that we need in order to define adaptively, So, the first thing that we need in order to define adaptably calibrated forecasts is a binning function for games. So, when we talk about games, what we're going to do, and again, I'm instantiating this in the language of Steichelberg security games, we have a set of targets, target one through target N. So these are, again, the targets that we had from the previous example, and I've also added a keepo at the very end. And every time that a target is attacked, we are going to add one. We are going to add one to the bin for that specific target. Note here that there are many potential calibrated forecasts that can induce the elephant to be the best response. To say this differently, it may very well be the case that you predicted that PT is, let's say, 70% and you attack again the elephant, but it also may be the case that PT is. But it also may be the case that TT is like 75%, and you still have to attack the element. Okay? We're going to constrain the number of bits to be the targets. Bless you all. Okay? Good. Okay. We're back in business. Cool. We have, so now we have defined n. So now we have defined n pins according to the number of targets that we have. So the calibration error in this work is going to be defined per target, and it's defined for a history, a sequence of principal strategies versus a sequence of forecasts. Note here that I'm constraining the interval. I'm going to require something that is stronger than standard calibration. This is similar to what we saw before. Is similar to what we saw before. Now it's the average activation for the bins, and this is the error, the distance between the average forecast that induces target I as the best response and minus the average true principal strategy that induces target I as best response. So we're gonna say that a sequences that we have like low calibration, that we have Low calibration, that we have good calibrated forecasts when, with high probability for each of the targets, the calibration error is upper bounded by calibration of rate plus some small epsilon. And throughout this work, we are not going to assume that we know the algorithm that generates these calibrated predictions. We're only going to assume that we know the calibration rate. Okay? So, similar to things that John was talking about, we're not going to assume that someone uses like To assume that someone uses, let's say, multiplicative weights. We're just going to assume that they satisfy this calibration forecast problem. Okay, very quickly, to give you an idea of what is different between adaptively calibrated forecasts and calibrated forecasts, when we talk about calibrated forecasts, the calibration interval is just the whole time from zero to capital T. When we talk about adaptively calibrated forecasts, Actively calibrated forecasts, we require this calibration property to hold for all small sub-intervals. And I'm happy to point you where in the proof we require this. When we talk about calibrated forecasts, the binning function requires an infinite number of bins, which is what we said because the x's have to be all of the randomized strategies. However, when we talk about adaptively calibrated forecasts, we only require Forecasts: We only require a number of bins that is equal to the number of targets. And the calibration error itself, when we talk about general calibrated forecasts, is the summation of all bins. But in our work, we are going to require that it holds for the max over all bins. Okay? Yes? So it's like uh so it doesn't reduce it to the button doesn't reduce to the top right. The button doesn't really touch on that. No, if you could lev if yes, like in no, it doesn't, like, in general. You know, it's like the max calibration error, that's a stronger condition? Well, it's unclear, right? Because this is like, this is the thing that really kills you here. Right? Like, if we change this to be equivalent and only require it for like zero to capital T, this is a different though. Okay? Okay. Questions? Okay. So project number two, I promised to yeah calculation is related to online learning. Is adaptive calibration related to adaptive learning? Yes, so as we show in the and I will touch upon this in a second. So adaptively calibrated forecasts are a stronger requirement essentially to know squat regret. You know, like this adaptively grid notion. If I have an adaptively grid algorithm, does that imply that I have an adaptive calibrated? We do use the adaptive learning of algorithms to build the adaptively calibrated ones. It's not direct. But by the contribution of the work, actually. It's actually, yeah, if you leverage the adaptive algorithms in order to build adaptively calibrated ones. Okay. Okay. So let's. So let's try to see what happens. How do we learn the optimal strategy for the principal in standard cycle book security games? And standard means that, you know, again, the principal just comes out of every round and says, hey, that's the strategy that I'm playing. You can best respond to this strategy. So if the agent utility were to be known, so we are in a no-learning setting right now, we assume that everything is known and we're just trying to identify an equilibrium strategy. Identify an equilibrium strategy essentially. Then, what happens is that if you think about this space of like the principal strategy in terms of a simplex, you know, we have like principal strategy one to protect only the elephants versus protecting only the gorillas. What you actually need to learn is to solve multiple LPs. Why is that? Because for a set of For a set of principal strategies, it is the case that the best response will always be the Qing Feng Zhi. And for a set of principal strategies here, it always is going to be the case that the elephants are going to be the best response. So why do we say that we need to solve multiple LPs in order to identify the optimal strategy? And by the way, this is called the best response polytope for the chief 1Gs. Because essentially, what you should do in order to identify Essentially, what you should do in order to identify the optimal H is identify the H that maximizes the objective function for the principle, given the constraint, which essentially just says that for this region, any strategy that you choose gives greater utility to the agent for any other strategy. Okay? So essentially, the So essentially, the constraint just says that the changing is the best response for this polyto. Cool? So, how do we solve that, or how do we leverage this idea of solving multiple linear programs when the agent utility is unknown? Well, when the agent utility is unknown, you do not know this boundary between the polytopes. You do not know where exactly the agent switches their best response from attacking the chiponies. Best response from attacking the chip on Gs to attacking the elephants. So, the way to learn that is through queries. And what exactly do you do through queries? Well, the principal tries a bunch of different strategies. Let's call them, you know, HDPI. And for each of these strategies, they observe whether the agent attacked the chimpanzees or whether the agent attacked the elephants. And then the name of the game is to identify algorithms that explore the. Algorithms that explore the space of strategies for the principle such that they can identify the breakpoint efficiently. Okay? Yes? So I was wondering I can say it's very helpful for you. Sure, it can be such a case, but this is not going to be covered by this example here. This adds an extra constraint on top of the constraints that we have currently. Okay. Okay, so we are going to try to apply this idea of using queries in order to identify where the breakdown point is. So, before we do so, I want us to all think about a problem that exists in calibrated stackover games, which we didn't have before in standard stackover games. So, we said that when the utility is unknown in stackover games, the regular ones, then you can look. Irregular ones, then you can learn through queries. And there are many papers, both for continuous action spaces and discrete action spaces, that show that despite the fact that you don't have access to these utilities a priori, you can use algorithms, querying algorithms, such that you can ultimately converge to B star, which is the strategy. Again, the standard work value. This is the strategy that if you knew everything a priori, this is the strategy that you should have committed. Strategy that you should have committed to as a principal and allowed the agent to best respond. So, note that when the agent now responds to calibrated forecasts, when the principal plays a strategy HT prime, it's no longer the case that we are sure that the agent is best responding to Ht prime. In fact, the agent may be responding to something that's actually far away from Ht prime. From HTY. And of course, there is a good case scenario where the calibrated prediction induces a best response, which is the chimpanzee, and it's actually the same best response that the actual principal strategy would have induced for that round. So that's a good case. Now, a bad case is if you have a forecast which is relatively close. Which is relatively close to the actual strategy, but it induces a vastly different response from the agent. Okay? And so what do we think? Do we think that the principles utility in calibrated style web games should be larger or smaller than p star? Like, I mean, I have kind of given away the answer because it's. I mean I have kind of given away the answer because you're talking about the button as the paper is. But like I still think that it's a kind of like tricky question if we think about it for a second because what is the tricky thing here? So the tricky thing here is that when you start introducing calibrated forecasts, it may look as if you are making the agent strictly weaker. So the principal should be able to obtain So, the principal should be able to obtain something that's much larger than V star, which is what they could have obtained if the agent could best respond exactly to what the principal was doing. But on the other hand, when the principal can commit to an HT, they can actually steer the agent one way or the other. And one of the first results of that paper is to show that, in fact, the principal's utility in calibrated stackover games. In calibrated Stackelberg games, it is always upper bounded by a V star, the Stackler value that could have been achieved if everything was known a priori. And given some of the results that also John talked about earlier today, is that really interesting or surprising or contradictory to some things that have been proven in the literature about no regret agents or no swap regret agents? It is not contradictory. So if the agent who's no regret, So, if the agent holds no regret, which is a different condition than being calibrated, then papers that John has been involved in have shown that the principal's utility can be lower bounded by V star. So, what is happening here? What is happening here is what I talked to Basilis briefly about earlier. Adaptively calibrated agents as a condition is a stronger condition, and it essentially And it essentially equates no swap regret agents, which again it was shown in prior work that is upper bounded by V star. The value that the principle can achieve is upper bounded by V star. Cool. So let's see the other side of the principle's utility, whether we can provide algorithms that approximate V star sublinearly, up to a sublinear factor in time. And as a reminder, I'll keep this image here. This image here, just to remind you that you know, when we are learning regular Stack Rover games, the agent can see exactly what the principal is playing. So the principal has a good feedback model of what the agent is best responding at, one second, as opposed to a calibrated cycle by game where the principal may be playing HT prime, but the agent is actually better. prime, but the agent is actually best responding to Pt prime. Positively? So there's another result, I don't know the one that you're showing in the slide prior to this. There's another result we put from previous work that says no swap regret principal's utility less than V star? Greater than V star. No swap utility? Greater than. No swap is otherwise. Yeah, I flipped it. I flipped them over, I think. Wait? No, I haven't flipped them over. I haven't flipped them over. Say that again? Like, when the learner is like no swap, then you can't get more than your stack or vertical stock. Yes, so you can't get more than the upper bound. I think that's what we're supposed to do. Yes, did I misspeak? So the no-swap regret condition shows the same upper bound as we do. Upper bound as we do. What's the difference between the statement about no regret versus the statement about no short regret? No regret is weaker. Is the conclusion the same? Like if no shop regret, if agent was no regret, principal's utility is greater than this start. If agent was no short regret, then the agent's utility, the principal's utility is upper bounded by this. That flips, okay. Yeah, it flips. And that was prior work or this one? No, no, it was prior work. No, no, it was prior work. We obtained a different proof through adaptive calibration, but we also have an explanation for that, which is that adaptive calibration is a stronger requirement than no swab requirements. I think the other, something like the other statement also below. Okay, thank you for the feedback. I will appreciate it. Reminds me of my calls. I wasn't expecting to be put in this situation again, but it never ends. Thank you, thank you. The game never ends. Thank you, thank you for the feedback. Yeah, yeah, yeah. Okay. Don't worry, yourself, cool. So, okay. Okay, so what was I doing here? I had the principles algorithm. I will try to explain the principles algorithm, or to say it better, I will give you an intuition about what is hard about designing such an algorithm, and this will also give you the key technical pieces of that work. So, if we try to replicate how we learn in regular style beginnings, we would try to do. Can I have three more minutes? We would try to do an explore then commit type of algorithm. We would try to find first the epsilon optimal strategy for the principal and then commit to it for the rest of time and just pick up like a small epsilon term for every round afterwards. Now, challenge number one here: when we have calibrated. Here, when we have calibrated stackliber games, is that you need many more rounds when the agent doesn't exactly create respond to your strategy as a principal in order to converge. And right, we've seen that time and again with these like polytopes here. If your HT is not the thing that the agent sees, but the agent has instead a prediction that can induce a completely different attack target, like it will take you longer time to identify. It will take you longer time to identify whether you are actually in a region where the elephant should be attacked or whether you're in a region where the jifajis should be attacked. The second problem, the second challenge, arises from the commit phase, where it's kind of similar again, because of the fact that you cannot control how the agent perceives exactly your age, your actual strategy, because you don't control their algorithm and you don't know their algorithm exactly. Algorithm exactly. You need to try to be robust with regards to how the agents could have best responded or how their prediction, their calibrated prediction, could have appeared. And so what you need to do is to identify a strategy that is not only epsilon optimal, but it also lies robustly within the best response quality. And the last thing that I want to say. And the last thing that I want to say is that if we think about this setting more from the abstract level, what did we assume? We assumed that the agents are not myopic, and we assume that when responding to the principal's action, the agents do not know the principal's action itself. And instead, the assumption that we worked with was that they use a calibrated forecast algorithm and the best response of this algorithm. And the best response of this algorithm to choose their strategies. And when this interaction was happening, we could show that the principal's optimal utility essentially converges to the principal's style work value. Okay, now there's a meta question here, more like a set of meta questions that I have been avoiding to allude to. Question number one is I placed the assumption in the agents being calibrated. In the agents being calibrated. But would that be the best thing that the agents could have done? Like, if the agent just knows that I'm going to play a certain learning algorithm A, is it in their best interest to form calibrated forecast? Or should they use this information somehow differently in order to change my ability to converge to the optimal utility? And this is the second part of the question: where, sure, even if you fix two algorithms. Even if you fix two algorithms and how they are playing with each other, it's unclear. I only show that the principal's optimal utility converges to V star under this very specific setting of learning algorithm for the principle and calibrated algorithm for the agent. And we addressed both of these meta questions in the recent paper where we show that an uninformed player that wants to learn to attain That wants to learn to attain the value of her cycle work outcome through repeated interactions alone will not be able to do so when the algorithms that the principal and the agent are playing are in a nash equilibrium in the algorithm space. And I'm happy to talk more about that during the coffee break or in the questions. That was all. Thank you. 