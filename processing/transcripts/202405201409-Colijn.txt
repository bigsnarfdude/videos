This is Carolyn Gillen, also from San Francisco University, so I'm excited to hear. Hi, everyone. Thanks so much. I'm Carolyn from San Francisco. I'm going to talk. The title you may recognize is a bit of a riff on Maria Servedio's title and paper. She gave a talk about not just a theory, about proof of concept models. And I thought there was a lot more to say. And of course, even after this 20-minute talk, there will still be a lot more to say, but it's a topic that I'm interested in, especially coming out of the pandemic. Out of the pandemic. So I think modeling has its enthusiasts, and I think we are all among its enthusiasts. And it also has its detractors. I've got a little cartoon of someone who some of you ran for sure. Anyway, so we see this for sure, so we see both of these extremes. But I want to take a bit of a step back and think about what do we really mean by Step back and think about what we really mean by the word model. Model can mean many things, it can mean a physical thing, like an actual physical model train set or a model skeleton. It can be something like a mouse model, a model system, a cell line in which we study disease. This is the OMG observatory of microbial growth device at Harvard with Michael Bames group, looking at resistance as antibiotic concentration increases as you go in from the outside, and resistance evolves. A model could be a fictional object. A model could be a fictional object with a kind of metaphysical status of something like Sherlock Holmes, or maybe you would think of the Bohr model of the atom as sort of a fictionalized or idealized object or a Wright-Fisher population, maybe a bit closer to home. And we might want to think about attributes of Wright-Fisher models, not because we think they're representations of the world, but because we're interested in their properties before the spice is added. Philosophers of science have constructed ideas of what models are as kind of set theoretical. Of what models are as kind of set-theoretic structures, which I won't talk about. I think the idea of a model that we probably all share here is a stylized description of a target system, usually in mathematical or formal language of some kind, which is really just an analogy. We're using models as analogies to help us think about the world. So, it's a form of analogical thinking. So, I'm going to focus on mechanistic, mathematical models, and this talk. I think that's probably mainly where we're coming from. Mainly, where we're coming from. I think in the pandemic and before, we can encounter two extreme views about what models can offer. We sometimes see, you know, oh my god, this thing has time on the x-axis and that goes into the future. So that's what's going to happen. So you're telling me that next week or in six weeks we're going to have 972 COVID cases in hospital? So probably everyone here is aware that models will not crystal balls. That models will not crystal balls. And then on the opposing side, models may be seen as nothing but a fun little game you play on your computer. You don't need them. They offer us nothing because they're not, after all, crystal balls. Now, I should say, if you have a crystal ball, go ahead and use that. That would be a great thing to have. You don't have that. Okay, so models, of course, I think are somewhere in between. And there's a rich space in between. I have this kind of diverting slide, you know, because this is a society. Slide, you know, because this is a society for modeling and theory, I thought maybe it's, and because I riffed on the title of not just a theory, and are we a theory, are we more than a theory, are we less than a theory? I would say a model is not exactly the same thing as a theory. A theory somehow is a bigger set of ideas than one model. The same theory could admit many different models focusing on different aspects of the theory. And models can be essential elements of a theory, and models could, as a suite of models, might. Could, as a suite of models, might you might be able to construct a suite that sort of comprise all the important elements of your theory. Why do we want a theory? Probably you all have as rich a set of ideas of why we might want theory as I do. And of course, it's very brief to list them off like this. But models and theory can help us understand a system and explain phenomena and can help us to reason better. They can help us to formalize what we're really saying as we do these proof of concept models. We do these proof of concept models. They can help us categorize phenomena to generalize and for many other purposes. So, we may want theory, and models I think are not the same as theory, but they can be part of theorizing. This is my first ever infographic. I made an infographic for this talk. It is, again, not exhaustive, but I think it's informative, and I think it brings in a few very different uses of models that sometimes get conflated. And so, when we find that a model for all these models for X are not useful. For all these models for X are not useful, we may be actually thinking they're not useful for a completely different task than the task for which they were designed. So, one example is virulence transmissibility trade-off models might not be useful for a particular system unless they've been contextualized to get those contingencies. Statistical modeling is, of course, huge and it has lots of points beside it, and I won't really talk about it, but I will go through some of these other, I think, very distinct uses of models because I think some. Models, because I think some of them are a bit new. And I think sometimes we can fail to do any of these useful things. So that's not even a theory part of the talk. Okay, so that's the infographic. Okay, so what I've called a theory, I think, just for convenience for the talk, I'm going to have that section be kind of this slide or a few of these ideas that are really classic uses of models. So we have a statistical model and it describes our data, or maybe it describes Describes our data, or maybe it describes some mechanisms and then gives us insights about a distribution from which we expect our data maybe should come. And then we can do hypothesis testing or whatever. So I won't go into too much statistical modeling. Maybe what's a bit more relevant to this audience is classic kind of quantitative modeling, quantitative prediction and scenario modeling. Get an idealized description of your system that you think captures the most important fits. Fit it to some data, parametrize it, dig into what's the severity. Tries it dig into what's the severity, what's this, what's the rates, and these things. What are the, if it's a stochastic model, what are the probabilities or distributions? Then test and validate with more data. Does it predict the past? Do you just leave some out, et cetera? Then, you know, does it predict the present? Does it do now testing? And then maybe use the model to forecast given the current state or to explore scenarios, impact of policies. This should be sort of bread and butter modeling stuff that I think is very common and is probably one. Very common and it's probably one of the most commonly understood uses of modeling. And some people would even say, Oh, what's a model that can't make a prediction? Okay, there's a lot more, and of course, Maria's paper and talk outline one of these ideas, but I'll just go through these briefly. I think models, in addition to doing the sort of quantitative stuff in the previous slide, models can really be a form of evidence synthesis. They can bring together data from very Bring together data from very disparate types of inquiry and types of experiments and observations into a coherent picture. And that, of course, was very appealing in the pandemic because policymakers and the public wanted a coherent picture with time on the x-axis and some scenarios shown on the y-axis. And being able to bring together a lot of information in that storytelling and narrative way was very helpful. If you've read Maria's paper and worked on that, you know, about the proof of concept models. You know, about the proof of concept models in which we can identify a verbal hypothesis or verbal narrative that we can formalize in models and use those models to help explore under what circumstances or how robust that verbal hypothesis really is, under what circumstances it plays out, and really interrogate it further. I think there's another use of models that is really in what I would call qualitative or could also be quantitative exploration of a phenomenon. So there may not be a previously existing hypothesis. Be a previously existing hypothesis, but you may want to still explore what could happen in models on a certain kind of baseline and then add your spice later, but you may still want to understand phenomena through the lens of modeling without necessarily really being a proof of concept. And then finally, I think models can highlight connections between very disparate things through having the same formalism. So I'll talk about examples of each of these. Examples of each of these. Evidence synthesis: here's one of my own models. COVID, this is in, I think, in BC, and it was the time when alpha was going to come, the B117 or alpha variant, and we knew that it was more transmissible because of gold standard amazing data from all over the world. Integrating what we know from population data, from testing and contact patterns, including international, from sequencing surveillance. From sequencing surveillance data, from contact tracing data around how fast is transmission, what are the serial intervals, and so on, from virology data, from epidemiological data, all of that gets pulled into these kind of scenario models and integrated. And it is a form of evidence synthesis for that. Proof of concept models, just to say a little bit more about them, they test verbal hypotheses or verbal theories. They're almost parallel to model systems. Parallel to model systems in a laboratory, and modeling can be a form of experimenting and a form of setting up a system carefully and clearly to answer a question. In Maria's paper, she said, these models don't really need validation or to be tested against data. And in her talk, she said, you get these, she didn't put it this way, I'm going to be more pejorative than she did. These sort of cantankerous, maybe people who are more empiricists, you know, who Who are more empiricists, who say, well, what does your model really mean about my data? What does it mean about my fruit flies? How are you going to validate your model? And to me, I think the concept that these models are special, they don't need any validation, I think that goes a bit too far. I think we still need high-quality analogies. We need the appropriate contingencies. We need to be thinking about this. And I don't think the empiricists have nothing to offer there, but I see the point about you know, we don't need to make a prediction for these particular fruit flies. Make a prediction for these particular fruit flies. Okay. So I'm going to pause here and ask you three questions because I'm really curious what this audience will say. I'm hoping this will cause discussion through the week. So question one, I'm just going to show, I thought about doing a polling thing and then I thought, I don't have time. Could there be a good, useful model that does not make a prediction or cannot make a prediction for any specific system? So yes or no? Who would say yes to that? To that. Most of you, and I think I agree with that. Okay, number two: so, could there be a good useful model that does not need to have any data to develop it? Yes. Okay, you think there is? Okay. So, and then three, does a useful model have to have relevance for any specific system? Yes or no? So, here, yes, there could be a good useful model. So, does a useful model, I'll just say yes or no as it's written there. I'll just say yes or no as it's written out. Does a user model have to have relevance for any specific system? Who would say yes for that? And who would say clearly no for that? Specific specific system. This is where I think, yeah, like there's some notion of relevance, I think, that is broad enough that I could say, yes, a useful model has to have relevance for some system somewhere under some notion of relevance that is not as strong. Of irrelevance that is not as strong as, like, I'm going to make a prediction for the system future in time. But somehow, we need, if we're in the zone that came up at the end of the previous talk where we think that ecology is so contingent that there will never be anything generalizable that you can ever learn from one system to another, then theory and modeling will be very diminished. But I don't think that's the case. But anyway, so these are the. The second problem is with the word any in that last question. I know, yeah, okay. Some. For some, yeah. Some. For some, yeah. Some. Right, it doesn't have to have relevance for every system. It has to have relevance. It has something. Oh, yeah. Is it just on A or is it the backwards E? Yeah. Wow, a math person should know. What a rookie mistake. Okay, there we go. Okay, so here's an example of models as qualitative exploration. This paper is famous in Is famous in epidemic modeling. If you have a scale-free network, if you have a very long-tailed degree distribution, actually, the epidemic threshold vanishes. So, in contrast to the well-known and overdone SIR plus plus suite of epidemic modeling, these have a basic reproduction number R0, which is a threshold. And typically, if R0 is less than one, you don't get an infectious spread. In these models, with this very wide scale-free power low-DV distribution, no matter how low the transmission rate, you don't get a rumination. Low the transmission rate, you don't get elimination of the infection. So that is interesting. I don't think it is relevant for a particular real-world system because we have financed and financed populations and scale-free and the durations and concurrencies of contacts and so on. But I think it is an interesting point and it's an interesting hook to hang things on and think about how epidemics change on networks of different degree distributions. I think it's useful and good, even though it doesn't do all of those things, that it's not a proof of concept. All of those things that it's not a proof of concept and it doesn't do quantitative prediction. Okay. I think power law distributions, much as I'm skeptical of them, and they were way overplayed in the physics land of the 2000s when everything had to be a power law distribution and it was how nature works. So much as I'm skeptical of that, I do think they are helpful in identifying conceptual connections between very disparate things through modeling and through formalism. And I think that's a role that modeling can and should play. Modeling can and should play when we identify the same formal structures in very disparate systems, and we can learn about the world. Models are great, okay? We look models. But I think there is also a large volume of modeling work that does not succeed in any of the functions of modeling, despite how wonderful modeling is. And I hope that we as a society can think through what modeling can and should be and avoid doing things that are less good science than we could be doing. Than we could be doing. So I think this happens because you know these four reasons, which you've probably read now, so I won't read them out. And instead, I'll move on to some examples. I've taken examples of useless modeling from my own papers typically. This is a great paper. I love this paper. These folks reviewed and implemented 312 published TB transmission models. And Ted Cohen is on this paper. It's led by Nick Menzies. Only 60% of the modeling results were. Only 60% of the modeling results were within a factor of two of either empirical point estimates. So I had two numbers here, folks. Estimates are all over the map. That's this big spaghetti line. Key parameters aren't really known, so modelers make assumptions, which is strength of modeling is you get to play with the consequences of your assumptions. But a potential weakness is you may end up in a land where a substantial proportion of models adopt assumptions that are incorrect, and that has downstream impact on real policy questions, which Brandon also highlighted. Brandon also highlighted. It really matters because it will impact the subtle assumption inside these TB models about progression to active TB after infection and what portion of people get TB. That's here. You can see this on log scale, so they're per thousand. So they go from 100% to zero. We'll get active TB and this whole spaghetti zoo of models. So it can make a big difference. I'm going to have some cartoons. I think that we are generating a lot of these models. Most of the 320. Of these models, most of the 312 models were pretty recent, and then so enough in one year as since 2000 to that year, I forget which year it was. So we have sort of sweatshops of people generating, you know, this majority of this is actually from one of my papers, these matrices and bifurcation analysis and so on. And I think there are some challenges here. And of course, there are, you know, like the happy families and the Tolstoy. There are so many ways to be wrong. And I will not start enumerating. Ways to be wrong, and I will not start enumerating all the ways to, you know, any endeavor worth doing. It's possible to do it poorly, and we can't enumerate all the ways, like, oh, don't do it poorly this way. But I'm going to highlight two things that I think are challenges in infectious disease modeling and in modeling in biology in general. So problem one, illustrated with this chasm between a modeler with his asymptotic stability and a TB epidemiologist who asks, you know, gee, well, it's nice to know that that model would be stable if I could reduce the. Model would be stable if I could reduce the transmission rate. And you know, if I could do that, I would do it. Like, we know that we would have done that already. Kermack and McKenzie really knew that. But, you know, how much resistance is acquired versus transmitted? Why do we see isoniazin resistance before rifampicin resistance? When are multi-droid-resistant strains likely to acquire enough fitness advantage to overcome fitness costs of resistance? And so you have these army sweatshops of people generating these models, and they're not answering questions. These models in, and they're not answering questions that are relevant beyond the model itself. And I think that happens partly because the model is a poor analogy. So we're doing analogical thinking. We just have a good quality analogy that helps us answer our question. And I think sometimes it's sort of a lack of humility on behalf of modelers that, you know, we like models and we look at their properties and we may forget that they are not the world and that the questions that they are. The world and that the questions that they are amenable to are not the questions that maybe matter the most scientifically. No, different things matter to different. Okay, so then problem two, I think, comes and is the reason I talked about Nick Minzi's paper is partly that I wanted to use that slide here, and I wanted you to know what this big zoo of model plots actually meant. But I think models may not be correctly set up to ask a relevant question or test a relevant hypothesis. Question or test a relevant hypothesis. And one reason for that could be, in the case of the TB models, that the underlying data are just not strong enough to constrain models, and so people make assumptions. That's okay, we can make assumptions. That's the strength of modeling. But when those assumptions actually drive the model answer to the question that we're asking, and we're being kind of subtle about it, so those assumptions are not really readily examined or interrogated, and that can happen easily in nonlinear dynamic models, then I think we really have a scientific weakness. Then I think we really have a scientific weakness because we're assuming the answer to our question, but we're not honest with ourselves always about that fact that we're assuming that answer. So, modeling can't answer how much TB is from rapid progression versus reactivation of an early infection without assuming something about the rates of reactivation. And if you don't know those, well, you're going to end up the assumption you make is just going to shake the answer. Again, I think we need humility as modelers. We need to think about what the limitations of the models are. You know what the limitations of the models are, that they are not the world, what the assumptions are, and I think that will help us. So I think that, you know, although I'm critical of some of the anti-modeling colleagues that we've bumped into in the pandemic and before, I do think that we can do harm. And Brandon mentioned this too, and I really appreciate that: that people, people being end users, policymakers, people. Policymakers, people who are working in the fields that our modeling is nominally supposed to be about can see a zoo of hundreds and hundreds and hundreds of poor quality models and think that's what modeling is and what it can offer. So when we do that, when we produce volumes of this, we can lose engagement or prevent engagement of people that we actually really do need to be talking to. That can then reduce people's motivation to share data with us or to gather data that is relevant for modeling. And, you know, if we're making TV policies, And you know, if we're making TB policy based on models, and it's been 100 years of research, and we still don't have the key numbers that actually shape the outcomes of those models, that's a problem. And we should maybe be, you know, if modeling was seen as the strength that I think it is and I think we all see it as, maybe we could motivate studies that actually measure the parameters for models without being kind of apologetic, like, oh, you know, no models are cheap compared to data. But actually, if we're going to use these models, we need data for them. Them. And at worst, I think it can give credibility and talking points to people who are skeptical of models. And then the default is: well, if models can't help us, what are we going to do? We're going to use this guy's intuition, right? Which is also a model. It's just a model that you can't interrogate against data. And it's a model that you can't test and you can't formalize because you don't know it's this guy's intuition. So here's this guy with his intuition. So we have this is the same story, but with a cartoon, then I just take the cartoon. How much time? Take three. How much time do I have? Five minutes. Okay, great. So here's our cartoon, you know, more applied person with eye roll at these patient analyses and these zoo of things. And then, you know, the conclusion can be, well, models are crystal balls. So we should just do what we've done. We know what to do. We already know what to do. Let's do that thing and let's continue to believe what we believed without the models. And I think that's a real loss because I made a whole infographic about how awesome modeling is and how many distinct uses. And I think for each of those, Distinct uses, and I think for each of those arms on that infographic, one had Maria's paper about it, the proof of concept models and statistics. Many of these have been written about. But most of the others have not actually had a comparable paper written about them, like conceptual connections or qualitative exploration or evidence synthesis. So I think a lot of that can be developed, but I think we lose that opportunity. So, concluding ideas here: I think modeling, of course, played a huge role in the pandemic. We had RT. Role in the pandemic. We had RT as a household phrase. People were talking about RT and serial intervals and estimates, and everyone, you know, journalists were calling us three to ten times a week for two years asking about modeling results. We can, and I think we should, articulate and communicate the uses of models and the data needs of models and how much data do you need. Sometimes you don't need any data. We all, everyone agrees, sometimes you can just explore your concepts without data. But sometimes, if you want to answer a question about what's the best Sometimes, if you want to answer a question about what's the benefit of closing restaurants versus closing schools, well, you know, models can't help you very much unless you have some data. So, I think we can communicate strengths, uses, data, needs. I think modeling's many purposes, and I illustrated them on the infographic, and there are many more. You should tell me if you have more, add arrows to that thing, or I can. But these purposes are often poorly understood by people who do need to know. And Maria had mentioned tenure committees at the start of her talk. Committees at the start of her talk: policymakers, funders, data controllers, data would-be data sharers or data non-sharers. Modeling isn't good science, and I think because there's so much kind of volume of, I would say maybe modeling that doesn't succeed in doing any of these purposes out there, I think we as a society should, or we have an opportunity to think through how to do this clearly and communicate how modeling can be good science and can do a lot. It can do a lot with some things. So, just for connecting, Harnish Netzer is our next speaker. We have to go a little bit quickly, so while he sets up Joanna. So, one more comment that I think RT being a household phrase is an example of longs gone wrong. You know, RT might be a household phrase once you're vaccinating, but at the time that RT became But at the time that RT became a household phrase, RT was poorly estimated and most of the irrelevant one made it in the curve of an alpha DNR. And so I think it was epidemiologists had not fully understood models and kept emphasizing, you know, we didn't really care what RT was, what we cared about was, you know, R. Growing or not growing really was what mattered. Growing fast or not growing, from the point of view of public engagement. At what rate you're going? Yeah. At what rate you're going? What is hospital capacity going to be at a certain point in time? And R is much better than RT, and it was better estimated because you don't need the additional assumptions. So I think it's a good example of something that was invented for, you know, well, it was popularised for vaccination purposes. And then somehow people with half-understood models have thought, oh, this is really important, we need to communicate it. And it's like, no, there are many things you need to communicate, but that wasn't. Many things you need to communicate, but that wasn't one of the. Yeah. Maybe a very quick question from Grandma. Well, I mean, this is for the afternoon conversation. That was a great talk. I think what you did is set up the need employs like a society for like an ethics statement, really, right? But that articulates a lot of what you said.