I'm very sorry I couldn't come there. I see a very nice group of people, and I would have really liked to see you in person, but okay. So in any case, I'm very honored that you invited me to speak in your workshop. So I will talk about a piece of work that I have been doing with my collaborators, Jocelyn Carnier from Eco Polytechnique, Alex Mamonov. Polytechnique, Alex Mamonov from the University of Houston, who's there with you, and Jorn Zimmerling, who used to be in Michigan, but now he's in Uppsala. And I also want to acknowledge collaborators that helped to lay the foundation of this methodology, who are Vladimir Drushkin from Worcester Polytechnic Institute and Mike Zaslavsky, who's now in Southern Methodist University. Okay, so we We are concerned with an inverse problem for the wave equation where we want to estimate a medium which is modeled in these slides by the unknown wave speed in the wave equation here. Okay, so this problem we call the waveform inversion problem. And in order to find this wave speed C, we are going to use data that is gathered by Data that is gathered by an array of M sensors, which we assume play the dual role of sources and receivers. Okay, so the sources emit signals. The sources are modeled here as localized at excess. So this is the S source. Okay, so its support is modeled by this Dirac delta. It emits a signal F. Depending on the application, F could be a pulse or F could be a pulse, or in radar, it could be a chirp, which is a long signal. And then the generated wave, which is denoted here by P index S. So S reminds us that this is for the S source, is measured at the receiver XR. And we take this wave and we convolve it with a time-reverse version of the emitted pulse. This is especially beneficial. Especially beneficial. So, this is done routinely in Radar when, again, the signals that are emitted are usually long signals of captures. So, this kind of processing is known as pulse compression because it compresses those long signals into something that looks like it came from a pulse. Okay, so this is the array response matrix. It's a matrix with M, so it's an M by M. So, it's an M by M matrix, which depends on time, like that. So, our problem is to find the wave speed from these measurements. So, now there are many, many methods to do so. I would say that most of them are PD constraint optimization methods that minimize over some That minimize over some search velocity space the data fit. And this can be done. Usually, this is done in the L2 norm, but of course, it is better to use other norms, like, for example, the vast magnetic. So, Bjorn did an excellent job giving us a broad overview of the field. So, okay, so those are data fitting methods. Now, what we have Now, what we have tried recently is we have tried to change a little bit and we introduced data-driven reduced order model methodology, where we looked for a reduced order model of the operator, which is essentially minus C squared Laplacian. And we showed that this reduced order model can be used to estimate the wave speed. And it looks like, you know, it overcomes. You know, it overcomes issues such as cycle skipping and so on. And for that, I invite you to go to Alex's talk, which will be tomorrow. So what I want to present here is another idea that stems from this reduced order model methodology, which can be used in conjunction with any data fitting. And so, what we are trying to do here is we are trying to go from is we are trying to go from the array response matrix M, which I explained to you how it's collected, to a reduced order model. And use the reduced order model. In fact, I don't even need all of the reduced order model stuff, but okay, I will need it, need to go through it to explain what we are doing. Okay, so we go from the data to the reduced order model and then to an estimate of an internal wave. What we call internal wave is a wave. We call internal wave is a wave evaluated at points inside the medium to which we don't have access. Okay, and we know, of course, in inverse problems there is a lot of talk about internal solutions and so on. There is all these hybrid methodologies and all that. And what we are trying to do here is to use the reduced order model methodology to get an estimate of this internal wave. Now, with such an estimate, what we will do is we will use it in the form Use it in the forward model that relates the wave speed to the wave measured at the receivers, and this forward model is given by a Limmer-Schringer equation. And by using this internal wave there, we essentially kind of linearize that equation, and this way we can sort of aid the inversion. Okay, so that's the plan. So, in order to explain how we do that, I have to go a little bit through the To go a little bit through the reduced order model construction. Those of you that have seen it, I apologize for the repetition, but I will try to keep it as brief as I can. So first, in order to do this, for our analysis and also in the computations, of course, it's beneficial to work in a bounded domain. So I'm going to consider the wave equation in a bounded domain, which is called omega. Omega, and the boundary of this domain, we always model it as a reflecting boundary. Now, this boundary can be there. You know, we could work in a waveguide or in a cavity or some, or we could work in an open environment and introduce the boundary using the hyperbolicity of the problem, the fact that waves propagate at finite wave speed and we record over finite time. Finite time. So that is, if the boundaries are far enough, so the waves don't sense them during the recording time, we can just put them there mathematically and model them however we please. I'm going also to assume that the medium near the sensors is known and has constant wave speed, which I call C bar. Okay, now. So the first step in our construction of the register. Of the registrar model is to take the wave equation which has a force in it, which is the excitation from the source, and transform it to a wave equation which has zero Rihanna size, so a homogeneous wave equation, with initial state that is determined by the source. Okay, so in order to facilitate that transformation, we are going to take the wave P, which is the source. The wave P, which is the solution of the wave equation, and we are going to transform it to this new wave, which I call W, which is defined like this. So, first of all, you see, so the convolution with the time reversed emitted signal, that's just because that's how we model the measurements. And I introduce here a scaling. You can view it as a similarity transformation. So, it's a scaling of the speed at point X. speed at point x divided by the known speed near the sensors and here i add the wave not at negative time now if you remember my wave equation here i have a zero initial condition for time t that is uh smaller than you know the support of this okay so this wave is going to contribute only within the support of this emitted signal Support of this emitted signal. And okay, so that's what it is. Now, the reason I do this transformation is because using some straightforward but long mathematical manipulations, you can show that this wave is in fact the solution of a homogeneous wave equation, which can be written using operator calculus this way. Okay, so here A. Here, A is the operator which is given here, which is obtained from the z-squared Laplacian because of this similarity transformation. So this is the operator A that is a nice positive definite self-adjoint operator. And here I introduce the cosine of A, which is defined as is usual in functional information. In functional, in functions of operators using the spectral decomposition. So if A has eigenvalues, let's call them lambda j, and eigenfunctions, let's call them yj, then this is the operator with eigenfunct with the same eigenfunctions and eigenvalues cosine t square root of lambda j. Okay, and similarly, I have this operator here where f hat is the Fourier transform of the emitted signal. Okay, so this Form of the emitted signal. Okay, so this expression looks quite peculiar. Believe me, it's right. It just follows after some calculation. Okay, so now this is the solution. So this models the solution of the homogeneous wave equation with initial state, which is given by this f hat square root of a absolute value squared acting on delta xs and the evolution of the wave. And the evolution of the wave is dictated by this cosine. Okay, now, what we call data are measurements of this wave, a WS, which can be obtained from the measurements of our wave P as shown here. Okay, so you either measure for negative time if you can, if you cannot, then assuming that the medium is known near the sensors, near meaning within a long enough, within a big enough. Long enough within a big enough set so that the waves over the support of the signal that we emit don't you know don't go outside that support, then you can actually compute that if you cannot measure. Okay, so this is our data. Now, the reason we do this transformation, so there are two reasons. The first is that this data can be written in a very useful symmetric inner product expression. Useful symmetric inner product expression, which we use a lot in our reduced order model construction. And here is how it goes. So, this data is the way that I wrote on the previous slide evaluated at the receiver location XR. Okay, so I wrote it this way using the Dirac delta. Now, using the fact that I have functions of the same self-adjoint operator A and such functions commute, I can Such functions commute, I can rewrite this expression this way. And now you see what I did here, I introduced a new function which I call u0, u0 of r, which is defined like this. So we call this a sensor function. So this is a sensor function associated with the receiver. Why we call it like that? Because we can prove that this is a function that is supported in the immediate vicinity of the Vicinity of the receiver XR. Well, immediate vicinity depends on what kind of signal we have. Okay, so this is the sensor function. And then here we have on the right another sensor function which corresponds to the source. And we have the cosine acting on that. So this wave, which is given by the cosine acting on this sensor function, we call it US. Okay, so this is the wave that we like to work. Is the wave that we like to work with. And because it gets tedious with all these indices, I'm going to write everything in block form. So I'm going to take all these waves for all the sensors from one up to m, and I'm going to put them in this row vector field like this with m components. Okay, so this is the wave u, which is the wave with initials. Which is the wave with initial state, which is given by this. Okay, and this initial state is the components, each component is localized, is supported near the corresponding sensor. And this wave is evolved from this initial state according to this operator cosine. Now, the fact that we have this cosine is very important because we can use trigonometric identity to tie the cosine. Identity to time for the cosine to time step the wave exactly. Okay, so what we're writing looks like a finite difference scheme for the time for the time derivative, for the second time derivative, but in fact it's exact. And that is why we need this cosine here. Okay, so we are going to time step the wave on some grid with time step tau, which has to be chosen carefully using the And carefully using the Nyquist criterion and so on. And when you do that, you obtain this scheme. So, this is an exact time-stepping scheme, which takes the wave at time instant j plus one. I mean, it computes the wave at time instant j plus one given the wave at the two previous time instants using this operator p, which is called a propagator operator, because it just sort of like propagates the wave from one time instant to the next. To the next. So, our reduced order model is obtained using a Galerkin projection of this discrete time-stepping scheme. And the Galerkin projection is done on the space which is spanned by these solutions, which we call snapshots, up to time n minus one. Okay, so this is the Galerkin space. And the Galerkin projection is done as is usual. The Galerkin approach. The Galerkin approximation is given as a linear combination of the functions in the space, and the coefficients in this combination are called gj. So they are matrices that are computed such that when you stick this into there, you obtain a residual which is orthogonal to the space. Now, one thing that I want to mention is that because we have an exact time-tempted stepping scheme, and because And because of our choice of the space, it is pretty easy to see that, you know, so we achieve zero residual for this first instance. So the first Gallery coefficients are trivial. They are just block columns of the identity matrix. Okay, now, of course, so if I take now, if I write this orthogonality relation explicitly, I obtain the time stepping. I obtain the time-stepping scheme in the Galerkin space, where this matrix M is called in Galerkin jargon. The mass matrix is just a Gramian, and S is the stiffness matrix. And the big point here is that the mass and stiffness matrices can be computed from the data, even though, because of course, we don't know this space. We only have the data, which are measurements of these waves essentially at the sensors. And this can Sensors and this can be done in a data-driven way. So, I will show you that, in fact, everything in this equation can be computed from the data. So, the matrix is M and S, and therefore the coefficients G. Okay, so here is the calculation for the mass matrix. I'm sure Alex will show it again. So if I go fast, that will be, you know, the second time around. I'm sure you'll get even better. So, here I write the one block of the mass matrix. The one block of the mass matrix, which is given from the definition of the mass matrix as this product. So these are row vectors. Remember, the u's are row vectors. So this is now a column vector field. And here I just wrote the expression of this wave. I use the fact that this is a self-adjoint operator and I get this expression. I use the trigonometric identity of the cosine. I get this expression. Of the cosine, I get this expression, and then I recognize that this is nothing else but what I measured. And a similar calculation applies to the stimulus matrix. Okay, so the orthogonal projection, so the ROM is given by the orthogonal projection of this equation, and the orthogonal projection is done using an orthonormal basis, which is stored in this vector field phi, which is computed using the Gram-Schmidt orthogonalization. Using the Gram-Schmidt of analysis. Okay. And the point here to notice is that, of course, u is not known, v is not known, but r can be computed because it is just the Cholesky square root of the mass matrix. Okay, so this is what it is important. And the reduced order model snapshots. So the equation that I had, so if I take this Galerkin equation. This Galerkin equation, and I multiply it on the left by R inverse transpose, where R is the square root of the mass matrix, which is the same R that appears in here. I obtain this time-stepping scheme in the reduced-order model space, where the reduced-order model snapshots are given by the projection of the Galerkin approximation, which is given by R times J. Okay, so that's all you need to know. Okay, so that's all you need to know for this talk. Now, how do we use this for the inverse scattering? So, for this particular approach that we propose. So, what we wish to do is we wish to invert the map from C to the data. And to write this map, because I have this similarity transformation and I have this operator that is C Laplace and C acting on the wave, I introduce this new function rho, which is Function rho, which is defined in terms of C easily like this. And so basically, what we want to do is we want to invert the map from C to rho and from rho to the data. And this is what this map is. And what this is, is nothing but Lieb and Schwinger's equation. Okay, so here, ref refers to fields calculated in our best guess of the wave speed. So we are going to do an iteration. So C ref is the current estimate. C ref is the current estimate of the ref of the wave speed. So, this is Liebanschinger's equation, massaged a little bit in order to be, you know, sort of like fit our framework and be beneficial to our calculations. So now, of course, so this mapping from C to D is non-linear, and not only because rho depends non-linearly on C, but this dependence is kind of trivial, but because we have this way which is shown here. We have this wave, which is shown here in green, which is the internal wave. And this internal wave is the wave corresponding to the unknown median. Okay, so that's what makes this mapping complicated. So our proposal is to estimate using the reduced order model this internal wave. Now, recall that this wave, so what we can do is we can take an interpolation in time of An interpolation in time of the snapshots. Now, the exact snapshots are just the components of this U that we used to define the space on which we did our Gallery projection. And of course, U is considering the Gram-Schmidt orthogonalization. So here what I wrote, I just wrote the Gram-Schmidt orthogonalization where V is the orthonormal basis and R is the block triangular matrix obtained as the Cholesky square root of the mass matrix. Cholesky square root of the mass matrix. So, of course, we don't know V, we don't know U, but we know R. So, what we want to propose is to, instead of using the internal wave U, is to use this wave, which is given by R, which we can compute from the data, and the orthonormal basis computed for the reference wave speed. Okay, so why is this a good idea? Why is this a good idea? So, here I have basically an explanation. So, this is a theorem or a proposition in our paper. So, the first thing to observe is that if you look at the initial state of the estimated wave, that is exactly the same as the true initial state. Why? Because V is causally constructive. This is, you know, Gramsci. This is the, this is, you know, Gram-Schmidt. It's a causal procedure. And we assume that we know the medium near the sensors, okay? And so that means that the first snapshots are known, and so will be the first components of the orthonormal basis, because they are determined by the known snapshots in a causal way. Okay, so the initial state is correct. Now, using the fact that our Using the fact that our data matrices are given by this expression, so I already showed that in the beginning. So you see, so this we recognize that in fact is nothing else but the first row of the mass matrix. And now I wrote the mass matrix using the Cholesky factorization, which is R transpose R, like this. And now I can also write it like this. I can also write it like this, where I use the orthonormal basis that we compute in the reference medium. Of course, this is an orthonormal basis, so this integral here is just the identity. And so we recognize that, in fact, our estimate of the internal wave satisfies the data, which is not the case for the wave calculated at the reference wave speed that is typically used in iteration. And so this is for the first n minus one, for the first n snapshot. one for the first n snapshots. For the later snapshots, you can also prove that our estimate, so you can prove that our estimated wave fits the data up to the instant 2n minus 2, which is the amount of data that we use to construct the estimate. Okay, so now the important thing to notice from this calculation that I showed over there that what is really important But what is really important in fitting the data is not the orthonormal basis, it's R, the square root of the mass matrix. Okay, so this matrix contains all the information that we have in the data. The problem is that it's in this algebraic ROM space. And what we want to do is we want to put it in the physical space. And that is what we don't know. You know, ideally, we would like the true orthonormal basis, which we cannot compute. And so we use the reference. And so we use the reference basis. So, what this really means is: I will show you now with an example. What this really means is that this internal wave that we estimate has all the arrivals, all the multiple scattering arrivals. They may be misplaced depending on how badly we know the kinematics. So, here is an example that I want to show. So, this is a median that we want to recover. This is the wave speed, and you can see the And you can see the values in the color bar. I guess they are in kilometers per second. And here I'm plotting the internal wave at this point, which is shown here with a cross. Okay, so this is the true internal wave. This is the wave that you would compute in a usual iteration corresponding to the reference medium. So this is just for one iteration for. Is just for one iteration for we start with constant wave speed. So, this is the internal wave that you would compute with the constant wave speed, okay, in the usual approach. You see the direct arrival, which is this hyperbola, and then these are just reflections from the side boundaries. Okay, this is the estimated internal wave that we propose. So, when you look here, you compare it with this, you see that this estimate has This estimate has all the arrivals, they are just misplaced, okay, because we have the wrong kinematics. So, for example, this wave, this wave front here, which is the wave that goes straight from here over there, okay, because we don't have the wave speed, the correct wave speed in the inclusion is slightly delayed, okay? But all the arrivals are there, okay. And now, of course, as we iterate. And now, of course, as we iterate, our idea is that as we iterate, we improve the estimate. And sure enough, in this case, we get to the true wave via iteration. And here is the result of the inversion. So this is the true C that we wanted to find. This is the ideal reconstruction that was obtained by cheating by putting the internal wave calculated with the uncomputable orthonormal. Orthonormal basis in V. Here is the result that we get by iteration. So the first wave was this one that we used, and then we iterated a couple of times, and this is what we achieve. And this is what full waveform inversion with least squares data field gives. So you see that we get a bunch of noises over here. And here is a plot of the data feed. So the one So, the one in deep blue is the one where we cheated, where we use the true internal waves. The one in black is the one where we iterate using our estimate. So you see that we are approaching the optimal fit and the full waveform inversion is stuck in a local medium. Okay, so to end, so what I want to say is that this approach, first of all, so First of all, so it's based on the raw machinery, but in fact, we don't really need to compute the registrar model. Actually, this approach is very cheap. All you need is the mass matrix whose blocks are computed very easily from the data. It's also very robust because the internal wave estimate, all it needs is the calculation of the Cholesky factorization of the mass matrix. So if you have noise, you have to regularize it a little bit to make sure that it's symmetric and Make sure that it's symmetric and positive definite. And the advantage of this is that it can be used, of course, with any data fitting method. So for example, I hope that Bjorn and Kui and all you, Yunan, maybe you get interested in using it for the fastest dynamic fit. Okay, so that's all I wanted to say. 