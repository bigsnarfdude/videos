Yep. So thanks for the introduction. So I'm going to speak about two subjects today. The first one is recapping some results on, I would say, entropic regularization, of course, because it's the topic of the conference and in particular what we call synchron divergences, which was introduced by Od during her PhD. And I will explain a bit why, I mean, some recent results in particular that you've heard yesterday. That you've heard yesterday, the talk of Luca, of Lukatamanini, not Luca, that could be used here to analyze what we did with ODA. And then also to introduce the problem of chroma-versernstein, which I think is not so well studied and that probably deserved more attention. And I'm raising this for you guys to work on this type of problem, which seems interesting. All right, so let's start by motivation. So one typical motivation of optimal transport for machine learning is to train. For machine learning is to train a generative model for dormitor fitting or generation of data and many tasks, where the goal is to fit some template model alpha that is parametrized by some parameter theta that in practice is typically a deep network. And the goal is to make this template alpha as close as possible from the data beta. So you'd like to warp somehow the templates. What would be the traditional way to approach the problem is to form what is often called macro. is to form what is often called maximum likelihood. So to maximize the likelihood, where you somehow assume that all your template alpha, they have a density rho with respect to some fixed measure dx. That could be, for instance, the Lebesgue measure. And the reasoning is independent of the reference measure dx. Then what you maximize is rotata evaluated at the xi or the log or you minimize minus the log and somehow Minus the log, and somehow the intuition that you can get is this type of maximum likelihood in the large sample limit is exactly the same as minimizing a relative entropy, which is often called a Kullback labeler in statistics, between the measure of observation beta and the model. So somehow you see that what you would be tempted to do is to minimize a relative entropy. And the bottleneck of this type of approach is this requires some strong assumption. In particular, this requires Assumption. In particular, this requires all the model alpha theta to have a density, which is not often the case if you are in high dimension. And the typical case of failure for the MLE is the generative model setup, where you assume that, I mean, you force your model alpha to be a push forward by some function g of theta, which is typically a deep network. So the way you model your template is you say, I will draw some sample in low dimension. Draw some sample in low dimension, let's say according to some distribution over a latent space Z, and then you push forward or you send your sample to high dimension using a deep network to generate, for instance, images. And you see that in this framework, the problem you have is when you modify your deep network, you have some kind of a surface, which is a support, if you want, of alpha that will move around, that will be deformed by your deep networks. So it's totally realistic to think that you could have. Realistic to think that you could have a fixed, I mean, a fixed reference measure for every model. And typically, the support is very thin, it is located on some kind of hyper surface. So, you would be facing some problem if you directly apply maximum likelihood. So, this has forced people to introduce some regularization, so some regularity in order to somehow modify the maximum likelihood to make it well-behaved. And another way to look at the problem is which is somehow very similar to regularizing the likelihood, is to Regularizing the likelihood is to find another metric to replace the relative entropy by a metric that would be typically continuous or smooth for the weak topology, for the topology, which is natural topology, which is the topology of the Wasserstein distance. So this is really what is motivating people in machine learning to move away from classical statistical approach toward more metric approaches that take into account some metrics, some notion of weak regularity. Weak regularity. So, this is just somehow making this a little bit more precise. And the problem you will have with the relative entropy is that you are somehow using the strong topology. And the way to think about the strong topology is this is the topology induced by the total variation, the L1 norm between the two measures. And the weak topology in sharp construct, the topology of the convergence in law, would be, of course, the one that you obtain by looking at the That you obtain by looking at the convergence through the prism of a continuous function, you ask that all possible expectations converges. Okay, so this is really a weaker way to think about the problem. And of course, I mean, most of you know about this, but the way to really visualize what's happening is if you look at just a single Dirac, if your model is the most simple model, just a Dirac, and you want your model to move to another one, to another Dirac, for the total variation norm, this would never converge. Norm, this would never converge because the variation between two Dirac is always equal to two. And in sharp contrast, you would like something where you could measure the convergence. So, how do you measure the convergence? Of course, you can do this with Wasserstein, and this is why we are here today. But in order to make the connection to the synchron divergence, and also to maybe speak about something that is not so well known, that is basically super simple, what is the simplest way to measure or to measurize we? measure or to measurize weak convergence. I think the simplest way is to simply replace the L1 norm by a L2 norm if you want. It's to replace the L1 norm by somehow an Hilbert norm between measure. Of course you cannot directly use the L2 norm because the L2 norm of a Dirac is plus infinity. It's not defined. So you need the smooth L2 norm, you need a smooth Hilbert norm. And this is exactly what you obtain if you use what is often called the MMD norms, maximum mean discrepancies, which are kernel. Maximum mean discrepancies, which are kernel norms, which are basically the equivalent of Euclidean distance, but in infinite dimension over the space of measure. So, how do you measure an Euclidean norm between two measures? Well, you make the difference, alpha minus beta, and then you simply integrate twice your difference against your favorite kernel. So, your kernel k of x, y plays a role, if you want, of a positive matrix if you were in finite dimension. Except here we are in dimension infinity. In the dimension infinity, so you have to replace your favorite matrix by your favorite kernel. But that's pretty much it. I mean, you see, it's very simple and it's very intuitive. Now, the question is, what should be the kernel or what is the advantage and the disadvantage of this? Well, I guess the most famous kernel or the one that you would pick at first choice would be the Gaussian kernel. But I think the Gaussian kernel is not so good in high dimension because the Gaussian kernel is decaying very fast. So it's not necessarily the one that people would favor. The one that people would favor in high dimension, but I think you get the idea. You take your kernel and you integrate it twice. What are the good news? First, the good news is this matrix is a weak convergence. So I didn't even put this here in my slide, but alpha converges weakly to beta if and only if your MMD norms converge to zero, at least if your kernel is reasonably well-behaved, if it's a universal kernel, like a Gaussian kernel. So it's a way to measure how much too much too. A way to measure how much two distributions are close one from each other. The other semi-good news is it's quite fast because if you have discrete measure, the number of operations is just the square of the number of the arc. So of course it's good or bad because it's still quite expensive if n is very large. I think what is a bit lacking here in this is it's the price of simplicity. It's a very global notion of comparison because you have two measures to compare and you really compare everyone, every Dirac against every Dirac. Dirac against every Dirac, which is, I think, the price to pay for simplicity. And this comes, for instance, at the price that's typically, if you look at what happened, if you compare two Dirac, delta X and Delta Y, according to a kernel, you see that you typically have a distance that is saturating. And if you have a Gaussian, you would see that the distance is saturating to one when the two Dirac are to two. If the two Dirac are very far away from each other. So somehow, this is good locally. So somehow this is good locally to compare two distributions, but if the distributions are quite far, you might expect bad behavior, non-convexity or something like this. And also the algorithm will have a hard time to fit a template if the template is very far away. So you'd like something that has a better behavior. And this is exactly what optimal transport is made for. Optimal transport is really a way to replace the global comparison that you have with kernel method, with opinion method. kernel method with Euclidean method with a non-Euclidean method because optimal transport is non-Euclidean that operates by by matching or by similarity of pairwise similarity so it's it's my intuition my intuition is although it's very simple to use maybe you'd like something a bit more refined which is why we are here today and why we use optimal transport instead of just like simply computing Euclidean distance and this is the main motivation so I will go very quickly just to state the notation on Monge and Contourovich problem On Munge and Contorovich problem. I would describe everything with direct masses, but of course, everything extends to arbitrary distribution. Just for the sake of simplicity, I would consider a discrete distribution with a pointer xi and the weight Ai for the first distribution. And for the second one, the point will be YJ and the weight BJ. Then I will describe my optimal transports through a coupling capital P. And a nice way to write down the marginal constraint. Write down the marginal constraint is that you want P multiplied by the vector one to be equal to A and P transpose times the vector one to be equal to B. It's a very convenient way to write down the affine constraint of optimal transport, conservation of mass. And then you minimize the solve the linear program, which is here for simplicity. I assume that the cost is some power p of the distance, but most of what I would say works for any cost. So you have to face a linear program, which is once again both the good and the bad news, because Again, both the good and the bad news because it's convex, it's nice, but it's quite costly, of course, to solve if the number of points is very large. Just to make the connection with MMD, of course, what is nice is it's the same as MMD, basically, that not only it's a distance, but it's a distance whose topology, whose induced topology, the topology of the convergence in law. So if you are, let's say, on the compact domain, you converge in law if and only if the Washington distance goes to zero. So somehow now you have to. Distance goes to zero. So, somehow now you have two different ways to measure the convalescent law: which is MMD norms, so Euclidean norms, or Wasserstein distance, which have the same topology, but of course, they are like very, very different. Okay, it's in dimension infinity, they are not comparable with each other. And I think it's a good question to ask when should you use one and when should you use the other. And to the best of my knowledge, there is no clear analysis up to now of some problems. Or you could say, okay, this is better, this is worse. Could say, okay, this is better, this is worse, and so on. So, so it's still, I mean, in its infancy to understand what is the best distance for some statistical estimation frame. I mean, maybe in some cases you could do this, but it's still lacking a global understanding of what is best between MMD, Versailles, and maybe some other distance that you like to introduce. So, I've already said this. The bad news is it's quite complex to solve. So, MMD was L square, but optimal transport is L cube. Square, but optimal transfer is n cube because you need to solve typically a linear program, so you would have one order of magnitude slower methods for optimal transport. So it's a bad point, I would say, for optimal transport. But of course, there is some hope because, of course, in statistical method and in machine learning and data processing, you don't really care about computing optimal transport very accurately. You could use a lot of tricks, a lot of acceleration, a lot of approximation to... Approximation to make the complexity much cheaper in practice and also in theory. So, I think the key question that people are trying to address now is: what should you do in practice and also in theory to reduce the complexity while still having something that is efficient in terms of comparing two distributions? And also, and this is also something that is important, is also to understand if this still works in high dimension. What happened if you, instead of having alpha and beta, you have discretization. beta you have discretized the discretization discretization error uh what should you do okay this is typically the question uh i want to study and to understand um one one last uh one last slide before before uh getting more into the mathematics uh just about the application so i've told you about uh density fittings what do you do in practice um and i don't really explain all the practical details and how to implement deep learning and deep architecture but uh but Architecture, but uh, but well, this is like depicting uh what you need to do. So you have inputs, um, data sets. Here is a collection of images, a huge collection of images, digits, and you train a network G, parameterized by theta, so that theta are the weight of the network, so that when you take random noise in low dimension, V, then you apply to this random noise inputs, your deep networks, you get nice images. And when I say nice images, it doesn't mean that Images just, it doesn't mean that each image should look nice, although it's better if they look nice. The question is that the collection of the images, so here the red samples should globally be consistent with the blue samples. So the research line distance should be as small as possible. Okay, so you see it's really what people would call, if you want, a bi-level problem or a nested problem where you minimize over data, but the Wasserstein itself is an optimization problem. Okay, so you really have Okay, so you really have something that is a bit more tricky to optimize, and it's just not, I mean, there's still not so much theory actually about how to minimize with a guarantee this type of energy. But okay, what you use in practice is typically gradient descent and it seems to work reasonably well. And it works well for simple images like this one. But if you try this on very complex images, or at least images that don't look like binary black and white images, it's a disaster because. Faster because if you compare images using the Euclidean distance, if you use Vassarstein 2, it simply does not work. Give you very blurry images that look like nothing. What makes this type of idea works for complex data is this idea of Jan Goodfellow and collaborators that basically you need to learn also the metric. Okay, and this is not going to be the main topic of my talk today, although Gromo of Washington is very similar to learning the metric, but we should keep in mind. The metric, but we should keep in mind that for complicated applications of optimal transport, you also need to optimize the cost itself because you typically don't know what cost you should use. And the way people have learned the cost in deep learning is through what they call adversarial learning, which means that they minimize over the generator G and they maximize over the cost. They look for the worst possible cost to distinguish the two distributions together, to distinguish. Two distributions together to distinguish the blue, which is the true distribution, to the red, which are fake distributions. Sorry, I see there is a question. Yes. I can wait for you. Especially in early iteration, it's absolutely not clear. And it's one of the many open problems. Because in practice, when you train this by gradient descent, It's not at all clear that you actually minimize really carefully the Vassarstein distance because you draw random samples. So I didn't explain how you do, but of course, you cannot work with alpha being a continuous distribution. You need to draw a random sample. So you already introduce a huge error, a huge sampling error. And if you would really want this error to go to zero when the number of, I mean, iteration of your algorithm goes forever, we don't know how to do it basically, because there is some bias and it's not clear how to kill the bias. And it's not actually how to cure the bias, I mean, due to some sampling. But of course, if you really would like ultimately to minimize the bias social distance, you would need the inner iteration, the minimization over your transport plan to be more and more precise. And it's totally open problem. I mean, if you would like to state some tolerance, I would say yes. And in practice, this is what they do, where they run more iteration, for instance, for the iteration for instance for for the for the discriminator uh but to be honest i have no clues i mean it seems to be like black magging in practice to make this work uh so no no sorry i don't really know how to answer your question i would say uh yes if you would like to apply some theorem uh but i mean if you if you replace the vasostrain by mmd well you could have unbiased estimator uh yes i you i mean probably people could answer to your question that you would need People could answer to your question that you would need more and more iteration or more and more precise estimation of the MMD norm, more and more sample in some sense. But for Wessenstein, it seems, at least to the best of my knowledge, it's so we have got another question. So does the transport course have anything to do with the kernel? When you speak about kernel, do you speak about the MMD kernel or something else? Yes, yeah, yes, so this is a connection I will. I mean yeah, so this is a connection I will I will explain later. Later, I will explain basically, I can tell you the answer now, is if you introduce entropic regularization, as epsilon goes to plus infinity, the kernel that you obtain is minus the cost. Okay, so I would come back to this, but exactly what you say is true, meaning that the entropic regularization is a way to link Versa Stein to MMD, and the link is exactly this, is the kernel is minus the cost. So, and I would come back to this. So, maybe you could re-ask your question. Come back to this, so maybe you could re-ask your question if things are not clearer when I will explain this because this is basically the conclusion of my section two. So, section two is about shrding your problems. So, I would, of course, not teach you about shrining your problems, just to state the notation. The notation is epsilon is the temperature that you put in front of the Shannon-Boltzmann entropy. So, you penalize the optimal transport with the entropy. And, well, I'm sure you know because we are here for this, but in case it's Are you here for this? But in case it's one of the first time you hear about this, the intuition you should get, which also somehow answers at least in part, you know, the question of SUMIC is as when epsilon is small, you have optimal transports. And if you have the same number of points in the red and blue distribution, you get a permutation. You get Birkhoff-Franemale theorem tell you that the optimal transport is a permutation. And then epsilon increases if you draw a little segment each time there is a non-zero entry that is larger than, let's say, 10 to the minus 3. Let's say 10 to the minus 3 in your p-matrix, you see more and more little segments. So you see more and more connection. And when epsilon goes to plus infinity, everybody gets connected to everybody. Okay. And then the first term, the linear term, will just become an MMD term. So this is really the interpolation is on the left, you get sparse connection, and on the right, you get very dense connection. And ultimately, you just connect everyone to everyone, and you sum globally the interaction of everyone. The interaction of everyone with everyone. So you get basically a quadratic function. This is the intuition. I would not also explain in detail SYNCORN, which is the most basic algorithm, but yet surprisingly powerful. I mean, the way I would derive SYNCORN from someone that don't know it is just writing down the first order condition for the optimization problem, introducing language multiplier, basically UNV. Multiplier basically u and v for the two constraints. And you would see that u multiply the rows and v, multiply the column. And then what you end up solving is a celebrated matrix scaling problem, which is even older than Schrödinger. It's a question of you give me some matrix. So here the matrix in question is capital K the Gibbs kernel. So exponential minus the cost divided by epsilon. And you ask the question, can you always scale a positive matrix, kernel matrix, capital K, to a big? Capital K to b stochasticity if A and B are one, for instance, and the answer is yes, because it's a convex problem and it's a strictly convex problem, so there is a unique solution. But what is even better, of course, is you have a very simple algorithm that you obtain by simply revisiting the two constraints in diagonal form over the U and V variable. So, if you write down what does it mean to satisfy the mass conservation in terms of u and v, it simply means two quadratic equations in two unknowns. Okay, so simply of course. To unknown, okay. So, simply, of course, it's a matter of taste, but basically, solving Schrodinger problem is just solving degree two polynomial equations, and this is hard because of the presence of the capital K matrix. So, when I here put a dot with a circle, it's just pointwise multiplication, and k times v is really matching the vector multiplication. But of course, you can solve them iteratively. So, if you give me v, I can compute u by division, and if you give me u, I can. Division, and if you give me u, I can compute v by also almost the same formula. And the magic happened is that SimCon proved in 64 that this converges. And the proof of convergence of Syncon is exactly the same as the proof of convergence of power iteration if you prove Peru Frobenius' theorem. So really what is beautiful is also there is a division from the perspective of the proof of convalence is exactly as if there was no division. As if there was no division. And the magic is that you have to use a very specific metric to prove contractance of this iteration, which is the Hilbert metric. So this is a proof. If you know the proof of pero-frobenius, the same proof gives you convergence and convergence speed, linear convergence speed for Synchorn. And up to now, it's almost state of the art. I mean, there's very little improvement that has been made over Synchorn original proof. I'm simplifying a bit, but let's say that the analysis is really beautiful. In terms of complexity, also, it's beautiful because this is. Complexity also is beautiful because this is now quadratic complexity n square, because the complexity is only matrix vector multiplication, which is n square. So, really, somehow Syncorn is opening the way, paving the way towards algorithms that have quadratic complexity. Of course, you might have to do a lot of iteration, and the algorithm could converge a bit slowly if epsilon is small, but at least you break the curse of a cubic complexity. And the last point, which in my view is the most important one, is you could complement. Important one is you could implement this on GPUs, and this goes, I mean, fast because GPUs are really efficient to handle matrix vector multiplication. And if you want a very efficient implementation, a state-of-the-art implementation, if you're interested in having really fast methods, I advise you to look for K-Ops, which is a library developed by Jean-Fedi and collaborators, which is really accelerating a lot kernel multiplication, k times v. Multiplication k times v. If you implement it on PyTorch, it will be fast. And if you implement it on K-Ops, it will be even faster by a factor up to 10 to 100. So it's really something that would make Syncorn almost, not exactly, but the same ballpark as other much simpler methods like MMD. So for me, it's a game changer. And you could have a lot of tricks and lot of approximation and also theorems about even going below n square by doing convolution approximation. Convolution approximation, Laurent approximation, random sampling approximation, and there's a lot of things you could do. But let's say for the basic implementation, it's already working reasonably well. If, of course, if you implement this using an efficient library. All right, so I would not say much more about this. Of course, if you have questions, feel free. No need to go through all the detail, but I can stop at any time. At any time. So, what is the practical use of this in high dimension? And honestly, it's a bit limited by the first limiting factor, which is the fact that you have this epsilon entropy penalty, which basically makes this not a distance. So, w epsilon, let's say the sink on cost or the Schrodinger cost between alpha and itself is non-zero. And if you find, if you take a reference measure beta and you look for the alpha that minimize the cost. That minimize the cost. It's not going to be alpha itself, beta itself, sorry, because you have some diffusion if you want introducting your problem. So you always have to diffuse a bit, the solution. So you would find the optimal alpha, which is somehow a frank version, a contracted version of beta. And if you are a bit nasty and look at the large epsilon limit, you see that the optimal alpha is converging to a Dirac at basically the barycenter. So you see, it's very bad if epsilon is very large and I would. See, it's very bad if epsilon is very large, and I would argue later that in high dimension, you need epsilon to be quite large. So, this is not okay. How can you cure it? A simple but nice way to cure it is to do what people would do in kernel method to normalize kernels, basically. It's like a polarization formula. It's to subtract one half of the diagonal term, one half of the Schrodinger cost between alpha and itself and beta and itself. So, somehow this renormalize the kernel or this shift the kernel if you want. This shift the kernel, if you want, or the cost to make it zero when alpha is equal to beta. But well, you might raise the question that it might be problematic because of the minus sign. Okay, and in fact, it's not. I mean, at least in some reasonable case, although there is minus sign, it still behaves very nicely. And a way to get some insight about why this normalization makes sense is to first look at the asymptotic, and then we'll also look at a bit more refined, I mean, Taylor expansion as we. Fine, I mean, Taylor expansion, as we all love here to do Taylor expansion because it gives you insight. But let's look at just the limits. And the limit when epsilon is zero is very classical. It's the one that's been studied by Christian Leonard in details in 2012, but he has several papers about this. But really, I mean, you can analyze exactly what's happening. So Sumic has to deal with in more details this asymptotic more precisely, and you converge to optimal transports. And you converge to optimal transports, and you could even quantify how fast you converge. Now, what is a bit more surprising? I mean, once you look at it, it's obvious, but when you first see this, it's surprising is when epsilon goes to plus infinity, this converges to an Hilbert norm, to an MMD norm. And this answers the question of summing that the large epsilon limit of this is the MMD norm, the kernel norm, associated to minus the cost. So you might raise a question that you have a minus sign. So you might be a So, you might be afraid that this might become negative. And it might, I mean, for generality, we don't know. But be careful that here we are integrating minus the distance on the difference between two measures. So, alpha minus beta is not a positive measure. And the question is, when is this quantity positive, which is actually have been addressed in the literature? It's true, basically, if and only if the kernel is what is called conditionally positive. Okay, I will not explain what it is, but let's. I will not explain what it is, but let's say that at least there exists some kernels for which this limit quantity is a norm, positive, and matrices the weak convergence. The most famous one is if you take the cost being the Euclidean distance. So if you look at syncorn on Euclidean space, the limit defines the norm. The most celebrated one, I think, is when p is equal to one, but you can take p between zero and two. Two is degenerate. It's positive, but it's not definite. It's not defined, definitely definite, so it's a bit degenerate, uh, but but it's a norm, and um, and yes, exactly. I mean, if you are in a Euclidean space, you can see this just by computing the Fourier transform of your kernel, and it's a norm if and only if the Fourier transform is positive. And you can compute the Fourier transform of this kernel, I mean, quite easily, at least if you have Wikipedia with you. And you would see that if you take this MMD norm with cost minus With cost minus the Ecladium distance to the power p, it's exactly the kernel associated to a functional Laplacian. So, basically, what we have in the limit is basically a dual sub-olf space that I guess most people would like and love. So, you see this really interpolating between on the left-hand side optimal transport and on the right-hand side, some sub-bolefs, an inverse subolef phase. So, I'm not saying one is better than the other, but at least to give you some insights. Sense. Let's, I mean, I would not work in more details this, but let's at least speak about the question of sample complexity, which is what happens if you discretize your problem. Okay, if you replace alpha and beta by discretized version alpha at and beta at, which are obtained by random samples, n random samples, you know, I mean, probably that optimal result is very bad in high dimension because it pays a price that is exponentially bad with the dimension. The error you make on average is one over n to the one over. Over n to the one over d. And there is a very nice paper of Jeanette and Witt and Francis Bach that recap everything we know and much more and derive very sharp estimate for this. So it's really well known, well understood, and not so good in high dimension for optimal transport. Just a little remark that we have been working on with Linaj, Pierre and Flavion, and Flavion will give a talk on Friday, not about this, but on Schrodinger problem, that it's not actually optimal if That it's not actually optimal if you know that alpha is different from beta. You can show that actually the exponent should be two divided by t. So we gain a factor two, which is good, but unfortunately it's still fairly bad because if d is there, it's just not possible to use it. It just simply means that in high dimension, if you don't have extra hypothesis on your densities on your distribution, there's no way you could use optimal transport. Even if they are just sampled on cube, by just looking at the optimal transport. By just looking at the optimal transport distance, it's going to be so noisy that you could never even tell that this is drawn from a uniform distribution or machine. On sharp contrast, if you do MMD norm, kernel norms, because kernel norm is simply, I mean, estimating kernel norm is just replacing basically integrals by sum, somehow the price you pay is exactly the same that you would have with the Monte Carlo method. It's a bit more tricky than the Monte Carlo error analysis, but it's roughly the same idea. And the price you pay is. idea that the price you pay is just one over square root of n. It doesn't mean that it's necessarily better than optimal transport. It just means that this quantity could be estimated in high dimension, in sharp contrast to optimal transport, which basically cannot be estimated. If you look at Schrödinger problem, at the cost induced by Schrodinger problems, then I mean it's to be expected that you would interpolate between the two regimes. Between that as epsilon is strictly positive, the convergence speed is one over square root of n, but there is no free launch. And but there is no free launch, you would have a constant, and then will grow exponentially bad with the dimension. So you have some trade-off, and unfortunately, the trade-off, of course, is not so good. And we knew beforehand that there is no method. Actually, it's known that the one over n to the power one over D is actually optimal. There is no method, even clever algorithm, that could do better if you don't have extra hypothesis. Okay. So, once again, we are not. I mean, it's not to argue that SIMCORN is better than optimal transport, it's just a quantity that you. Optimal transport is just a quantity that you could estimate in high dimension, and maybe we could even make this even shorter just to nail the point and to say that synchron is good because it's fast, but synchron is not better as a way to get better estimation. I mean, up to my knowledge. I mean, we wrote a paper with Renee to explain this, and we were blocked. I mean, we cannot really improve a lot over just the classical estimator. And you can really see this by reusing sumic analysis. Sumic analysis first, where you first see what's happening to optimal transport, where you introduce a bit of entropy, you immediately see that you have a bias, and this bias is caused by first, I mean, epsilon log epsilon, which is just the fact that you don't normalize basically your kernel. So this is easy, of course, to remove. But the nice thing about the sumic result is you see that this first order error is also very nice because it's separable in alpha and beta, which means that if you remove the two diagonal terms, remove the two diagonal term, you would kill them, I mean, immediately. So if you go to sink on cost, to the sinkhole divergence by removing one half of the Schrodinger problem with alpha and itself and beta and itself, the first term and the second term, which are separable, will be killed, which I think is fairly nice. Of course, you could, I mean, I've guessed this, but it's fairly nice. And then you could even be better and more clever by reusing, I mean, by using the result of Lucard. The result of Lucard that was presented in a much more refined way yesterday, but basically, this is the basic result: is that the epsilon square term you also know it, and the bad news is it's not separable, so it's not trivial to remove. And it's built on the Fischer information. So, well, if you would be able to estimate the Fischer information efficiently, you would be more than welcome to try to improve over SIMCON by removing it. But up to my knowledge, there is no so simple. To my knowledge, there is no so simple way to estimate the feature information. We tried to do this with some kind of a bootstrapping method that I won't not describe in the Norib's paper with Linalik and Flavier and Pierre. It works on simple problem, but in practice, it's still, I guess, an open problem to just improve over basic synchron divergence. And to wrap everything that I've said before, what is the bad news? The bad news is first you know the stochastic error you make, you know the Know the stochastic error you make, you know, the random error, you know the variance term by the result of the previous slide of ODF. Now you know also basically the Taylor expansion through the result of Lucas that is epsilon square. You could just balance the two errors together. You could balance the epsilon square with this epsilon minus blah blah blah blah term. And if you balance them together, you see that you need to select epsilon as n over minus one over d and if you put this together. And if you put this together, you find an error that is exactly, unfortunately, n to the power minus 2 divided by. So you see that the error you make through the synchron divergence is exactly the same, unfortunately, and we were a bit disappointed, as the one you get by the vanilla estimator, which you could be using just Samplex algorithm. So in terms of error, synchron development is not better, but in terms of speed, of course, this is much better, because of course, this comes with an algorithm that is much faster. An algorithm that is much faster than Snowplex. So, well, bad news is you're not better. Good news is you're faster. So, it's half empty, half full glass. I think this is, yeah, just to conclude on this and also to give some hope, is what would be the ideal, I mean, one result that would be interesting. Of course, not the ideal result because it comes with very strong hypothesis. Is for instance, if you know that your measures are very smooth, like they are very smooth densities, you would, I mean, you. Densities, you would, I mean, guess it should be possible to have better than synchron, to have something that is as efficient as MMD and put scale as one over square root of n with a fast algorithm. And the bad news is Synchron is not able to do this. And I've told you before, I mean, there is a strong bottleneck. And somehow, to get the intuition of why Syncon is not doing the good thing to exploit smoothness, is to see what you should do. I mean, what should you do if you know that your measures are smooth? You should just. Are smooth. You should just take your empirical measure alpha at and beta at and just smooth them because you know that alpha is smooth. So you should just smooth alpha at by a Gaussian kernel. Evola, you would get an optimal algorithm and not optimal algorithm, but an optimal method, and you would really exploit smoothness. But of course, the difficulty is in high dimension. It's not possible to compute the convolution with a Gaussian. I mean, this could be immensely costly. So this is a theoretical method where there is no algorithm to compute a Gaussian smoothie in high dimension. Gaussian smoothing in high dimension. And somehow, Synchon is a little bit like doing Gaussian smoothing, but not exactly, because basically, Synchon is smoothing the transport plan, but it's not smoothing alpha and beta. Synchron is not exploiting the smoothness, basically. So it's not, this is why it's not working. And something that probably came to your Hada recently is this paper of two teams that merged together. One team is Vashi and Diallar who worked on this and independently. Who worked on this and independently a second team of Musele Krudi and Bach worked on this? And then they emerged and they produced a very impressive paper that showed that you could basically leverage smoothness and you need to replace linear programming by SDP programming. So, which unfortunately it's quite hard to to make it work in practice. It's still in its infancy, but I think it's very promising. So you see unfortunately Syngon it's seems to be reaching uh some limit here. To be reaching some limit here. Let me conclude by some kind of open problems section on Gromov-Wasserstein. So I would be quick on this because I don't have much time. Gromov-Westerstein is basically a quadratic version of linear problem. In optimal transport, you solve a linear problem. In Gromov-Westerstein, what you want to do is to compare two different spaces together, and on each space, you have a distribution, alpha and beta. And the way to make this as an optimal transport-like problem is to just Transport line problem is to just replace the usual optimal transport cost where you just move Xi to Yj and then you pay a price. It is to look at pairs of points. So you look at pairs Xi and Xi prime. They are mapped through your coupling to Yj and Yj prime. And you just ask that the distance, the two distances on X and Y should be as close as possible. And this formula here that I've wrote for you is exactly encoding this. You multiply Pij by Pi prime. Pij by Pi prime j prime. So it's small when both are small. And you multiply this quantity by the discrepancy between the two distance. And you see that what you obtain here is instead of having a linear function, you get a quadratic function where capital Q is basically your quadratic form. The good news is it looks simple, but the bad news is it's very hard because this quadratic function is typically non-convex. And the simplest way to convince you that it's non-convex is what happened in Euclidean. Is non-convex is what happened in Euclidean space. If your two measures are on two different Euclidean spaces, you see that, I mean, it's easy to show that capital Q is actually concave. So, Gromo-Washerstein, you should typically think of this as minimizing a concave function under a convex set, a polyanalysis. So, this is not as bad as if it were fully non-convex, but of course, it's not so good. The bad news, but it's mostly of theoretical antalysis, that is define a distance on metric measure space up to either. On metric measure space up to isometric, meaning this is a distance, and if it's zero if and only if the red and the blue space are isometric. Some application, and I would not speak a bit a lot about this, but this could be applied, of course, to shape registration, comparing two shapes together. This is the obvious application of this, and there's a lot of paper on this. Another type of application is to compare molecules, because it's very natural to look at molecule up to isometries, and so it's been started. So, it's been starting to be used to do machine learning in quantum chemistry. And the last application, which is also quite similar and very promising, is for genomics, where you record cells in two different space and you like to compare the cells together. I mean, you know they are probably the same cells or very related cells, but they live in two different spaces because you have two different types of expression, chromatin and RNA, for instance. And chromovasartine could be a way to basically compare the. To basically compare the two together. For you guys or girls, some motivation of trying to work on this is you could also use orthopic regularization. So you could somehow do a Schrodinger problem for this quadratic problem. You could try to find a solver for this by basic successive linearization, and it works great. So it's something that works great in practice: to linearize a problem and do synchron. There is very little guarantee, so you can show like local conversion, but. So, you can show like local convergence, but it's not clear, it converges globally. And you really see, uh, and in practice, I mean, in theory, there's no theory, in practice, it works great. So, there is kind of a huge discrepancy with respect to the little we know about the theory, but how much it works well in practice. It very often converts to a global minimizer, or something that has quite a low cost. But it raises a lot of questions. It raises a lot of questions about convalence of the method, about existence of MojMap, about apply. Monge map about, I mean, applying the theoretical Schrodinger problem to this quadratic problem. So I would conclude. So maybe I can just speak about the last item. So the two items are just like recapping what I've said before. I think it's important to understand what's happened in high dimension. For the last item, I think two very interesting questions that should be studied is the existence of Mojmap for Gromo-Versonstein. Can we show some kind of a Brunier Macan theorem for Gromo-Versonstein? And it's not so. For Gromov-Sachine, and it's not so clear. I mean, I think you can see this even here when you linearize the costs, when you linearize the objective, you obtain some kind of a cost function. So it's like solving Roma-Westernstein is like solving optimal transport between two spaces, where the cost is very weird, because the cost depends itself on the two spaces and on the coupling itself. And it's not clear at all that these costs satisfy the twist condition and things like this that would be useful to show Brunette Toronto. So proving existence of Monch mapping. So, proving the existence of Monchmap is interesting. And another thing that is interesting is studying the Taylor expansion as epsilon goes to zero. So, extending the result of time and di and so Iqbal and so and others to this quadratic function, because this would be useful to motivate synchron divergences and so on. And I would conclude, I'm already very late. Thanks a lot for attending this talk. Thanks a lot for that great talk. So, since it's a bit over time, we have less time for questions, but please go ahead. I think Shomik has his hand up. I think Shomek has his hand up. There is one question of Marin Fazel. I think it's about this one. So, this one is just, I mean, very simple thing that you could do is you have a huge collection of shapes. You could compute their Gromo-Vasserstein distance and then try to do visualization by somehow projecting into low dimension the Gromo-Vasserstein space. So, you take the pairwise Gromo-Vasserstein distance between your shape, and then you visualize them in low dimension by doing usual. By doing usual manifold learning or dimensionality reduction. And here's just to display that you recover the movement of the horse. You have a collection of horse that are working, and you see the periodicity of the movement here in Gromov's sustained space. It's just, I mean, some illustration. But the idea is to do machine learning, where you replace the classical notion of distance by the Gromov-Sustain notion of distance to do interpolation, prediction, regression, or here, visualization. I hope this answers all the questions. Gabriel, can I ask one question? In that, I didn't understand that part about where you smooth the data distribution with, let's say, the Gaussian kernel and you compare the W2 and you say, yeah, exactly, this one. It's a bit puzzling to me because, I mean, there's, of course, something like the kernel density estimator, which takes the discrete data points and spots out things. So, are you saying that the kernel, like, do Saying that the kernel, like doing the Washington analysis after doing the kernel density estimate is bad? No, it's good. No, it's good. It's good. I'm saying that if you, of course, if you have no hypothesis, it's not necessarily good, but if you know that the measure you're looking after are smooth, what you should do is you have end sample, you should do a kernel density estimator to have some kind of a smooth version of your densities. And then you do Wasserstein, not on the original sample, but on the kernel estimator. Original sample, but on the kernel estimator. And there is a paper of Comte Alberté, and I don't know if Nathan is an old co-author, I mean, of Filipino, probably, that studies this kind of estimator, and they show that it's optimal. So they should even more complicated estimator because they like to make things complicated using wavelets and so on if you want more complicated smoothness level. But basically, if your measure are uniformly smooth, that they have d-derivatives, I think, I mean, the way to go would. I think, I mean, the way to go would be to smooth your densities. But you see that the problem is this is nice on the paper, but it's very hard to do numerically. Like cannot density estimator in high dimension is not going to be feasible. I see. Okay. So you can see this as just denoising. You have your samples that are noisy. You denoise them by smoothing and then you compute the Walsenstein. And the smoothing kills the noise. And then you do your Wassenstein and you're very happy. Except you recover an exponent. You recover an exponential dependency in the so you kill the exponential dependency in terms of convalence speed, but in terms of complexity of the algorithm, it's well explained in the paper of Canton that you have an algorithm that would then introduce complexity in high dimension. You take much more samples. And to the best of my knowledge, this paper of the Vashi and collaborator is the first one that was able to have both an efficient estimator in terms of precision and also Of precision and also in time of algorithm.