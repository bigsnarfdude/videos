Thank you very much, Ahunja, for the introduction. I'm very grateful for having this opportunity to present my work. So, the main focus of my research is to develop methods to analyze data generated from genetics and genomics studies. As a method developer, my work is mainly driven by the development of technologies. In the last few years, we have seen that the method of the year selected by nature methods. Selected by nature methods has shifted from single-cell sequencing to single-cell multi-omics and more recently to spatially resolved transcriptomics. In my talk today, I'll share with you my recent work on how to analyze the spatial transatomics data, and in particular, how to integrate information from histology images and the single cells. I hope my work in this area will help other researchers to have a better understanding of the tissue microenvironment. Environment. So there are many technologies for spatial transcriptomics. My work is mainly focused on the VCM platform, Bitanx Genomics, because this technology is already commercialized and is readily accessible by most research labs. In the VCM platform, the spatial location information of each spot is recorded in the spatial barcode. And within each spot, the gene expression across all of the genes. Expression across all of the genes in the transcriptome are simultaneously quantified by RNA sequencing. And what is exciting about the vesium data is that, in addition to high-throughput gene expression measurement, we also have a high-resolution histology image generated from the same tissue section. And because a gene expression histology, they simply reflect the different aspects of a tissue, an ideal method for spatial transforming data analysis should be able to incorporate both gene. Be able to incorporate both gene expression and histology image information. So, in my talk today, I'll discuss some of the problems that I worked on and some of the problems that I'm still working on. And I will specifically discuss how to perform spatial clustering, how to integrate gene expression histology for high-resolution tissue annotation, and how to recover the spatial location information for cells in sequence and RNA sequencing. Ourselves in sequence and RN sequencing by leveraging information learned from spatial transcatomics. First, I will discuss how to perform spatial clustering. Clustering analysis is a very important task in spatial transplatomics data analysis because results from this type of analysis can tell us the spatial organization of a tissue. Although many methods have been developed for single-cell clustering, these methods do not account for These methods do not account for the spatial dependency of gene expression. In spatial transcomics, spots that are physically close and share similar histological features are also expected to have similar gene expression profiles. So this means that these spots are more likely to belong to the same cluster. But if you were to analyze the data using methods that do not account for the spatial dependency of a G-expression, you may misassign these spots into different clusters. Clusters. To account for the spatial dependency of gene expression, we recently developed a machine learning method called SPAR-GCN, which is based on graph convolutional network. I'm very glad that we have already provided a very nice introduction about the GCN. So this graph-based approach is very flexible in modeling the spatial dependency for data that have irregular shifts, which is exactly the case for the spatial transcommunications data that we are analyzing here. That we are analyzing here. A critical step when building the graph is to define the adjacency matrix, which describes the degree of similarity between every two spots in the data set. To consider both spatial location information and distorted image information, we first define a distance metric between two spots represented by U1V here. And this distance metric is determined by the spatial coordinates of By the spatial coordinates of u and v represented by x and y, and the histology pixel intensity value represented by z. By considering the histology as the third dimension in the Euclidean space, we can calculate the Euclidean distance between the two spots. And the edge weight between these two spots is then simply defined as an exponential decay function of the Euclidean distance. So, this way we can ensure that supposed We can ensure that spots that are close to each other will have higher weights, but the spots that are far away from each other will have lower weights. And we do this type of edge calculation for every two spots in the data set. And after that, we can obtain the adjacency matrix that describes the graph structure of the data. And after the graph is built, we then perform graph convolution to aggregate the gene expression information across neighboring spots, and then the graph convolution. Spots and then the graph convolutional layer is connected to an iterative clustering layer in order to get a cluster assignment of the spots. We call each cluster from this analysis as a spatial domain. To evaluate the performance of our method, we analyzed many data sets. Here, I'll first show you the analysis of pancreatic cancer data that we analyzed. So, we focused our analysis on a tissue section with the pathologist. Tissue section with the pathologist manual annotation available. So, here, if you look at this picture, you will see that the red line indicates where the cancer region is located in this piece of tissue. For this data, we performed spatial clustering analysis using SD-Learn and the Bayes space. Both are recently developed methods that are specifically designed for spatial transpotomics. And in addition, we analyze the data using Lovain, which is a Using Lovain, which is a commonly used classroom method for single-cell RNA sequencing data analysis. If you compare the three classroom results, you will see that the cancer region detected by these three methods do not match well with the pathologist annotated cancer region. And surprisingly, ST Learn and the basis space, they performed even worse than Lube. For SPAR-TCN, we can adjust the weight given to tithology image. Uh, given to histology image when detecting uh neighboring spots for each target spot. So, by specifying the weight value by the default parameter in SPARGCN, we are not able to detect the cancer region well either. But when we increased the weight value to a larger number, we start to see that we are able to identify this cancer region. And this cancer region that Region, and this cancer region that we identified matched almost perfectly with the cancer region annotated by the pathologist. So, this example indicates the importance of incorporating histology image information in spatial clustering analysis of spatial tectonics data. So, we also analyzed another data set that is generated from the posterior brain of a mouse. And here are the classroom results obtained from Love Bay, ST Learn. From Lovain, ST-Learn, and the Bayesis space, and the SPART-CMEN. So we see that Lovain's clustering result is in general similar to the other three methods, but the spatial domains detected by the other three methods are more spatially contiguous because they have the ability to account for the spatial dependency of gene expression. Now, after seeing this result, we next ask the question of what is the ability of each method in detecting Of each method in detecting more refined tissue structure. To answer this question, we performed a sub-clustering analysis for spots in domain 5 detected by SPARGCN. And this is highlighted by the spots within this red box, which corresponds to the cortex in the brain. This plot here shows the subdomains that are detected by SPARGCN. We can see that it agrees very well with the It agrees very well with the cortical layer annotation provided by the LMBRA atlas. The detected subdomains from SPARGCN include the layers 2, 3 colored in red, layer 4, 5 colored in purple, layer 6 colored in orange, and the hippocampa region CA1 colored in green and the cebiculum region colored in blue here. And more importantly, we see that Sparta CN. We see that SPAR-GCN outperformed the Lovain and SQ-Learn. If you look at the results from these two analyses, you will see that they tend to mix the neocortex layers together. Compared to LOBEN and SQLERN, Bayes space performed slightly better, but it's still worse than SPAR-GCN, because in this region, they mixed layer 6 and the Hippocampo CA1 region together as one single cluster. Once once has one single cluster. So, this analysis indicates that SBARTCN has higher sensitivity in detecting more refined tissue structures such as the cortex in the brain. After the spatial domains are detected, we can further perform domain-guided differential gene expression analysis to identify what genes are enriched in each domain. So, here I show you a few example genes that we detected in this data set. For example, for domain one, For example, for domain 1, we detected this gene PVALB, whose expression pattern marked perfectly of this domain 1. And for domain 5, we detected the gene MRGN. And for domain 8, we detected this gene TRIM62. For domain 0, which is colored in red here, you can see that the shape of this domain is very complicated. We are not able to identify one single gene to One single gene to mark the expression pattern of this region, which is not very surprising. But we are able to identify a set of genes such that the combination of them can give us a metagene that uniquely marks the expression pattern of this region. So here is a set of three genes that we identified. You can see that by combining the expression of KLK6, MBP, and subtracting the expression. And subtracting the expression of ATP1B1, we can derive a metagene. And this metagene expression uniquely marked the expression pattern of this complicated spatial domain. So spatial clustering analysis is very useful. It can give us a very rough idea about how the tissues are organized spatially. But in order to have a deeper understanding of the tissue macroenvironment, it's also necessary to. Environment, it's also necessary to know where each cell type is located in the tissue. So, in the next part of my talk, I'm going to briefly discuss how to perform gene expression and histology integrated high-resolution tissue annotation. In order to achieve a high-resolution tissue annotation, we need to have a high-resolution gene expression data. But unfortunately, the current mesium data do not have a single-cell resolution. This picture here shows. This picture here shows how the spots are laid out in the VCM platform. We can see that the diameter size of each spot is 55 micrometers and the center-to-center distance between two adjacent spots is 100. So this spot layout suggests that in addition to the lack of single-cell resolution, there is also a lot of tissue gaps that are not measured for gene expression. But on the other hand, we have the companion high-resolution histology image. High resolution histology image obtained from the same tissue section. So, this means that we can potentially leverage information in the high-resolution histology image to enhance the gene expression resolution and impute gene expression in the tissue gaps. To do so, we first use a contour detection algorithm to detect the tissue area in the histology image first. And after that, we generate a very fine grade for the tissue. A very fine grade for the tissue and impute the gene expression for each superpixel within the tissue. To impute the gene expression for the superpixel, we identify its nearest neighboring spots based on the edge weight that is defined in the SPARGCM framework. And we can then use their weighted average to impute the missing gene expression for the superpixel. So, in order to evaluate how this simple imputation How this simple imputation approach works, we analyzed a data set generated from mouse brain. And here are some example genes that I want to share with you. The first row here is the original gene expression that are observed at the spot level. And the second row is the enhanced gene expression generated from our super resolution gene expression procedure. If you compare these gene expression plots, you can see that. Expression plots, you can see that using our algorithm, we can substantially increase the gene expression resolution. And after doing this, there are no tissue gaps in the gene expression data anymore. And our super resolution gene expression made it possible to treat the gene expression data as images. And this opens the door for many interesting imaging-based data analysis. So, next, I'm going to show you how gene expression and the histology images can be integrated. Histology images can be integrated together for high-resolution tissue annotation. And for this application, I'm going to use the cancer data as an example. To annotate a tissue, we will first generate a super-resolution gene expression images for a set of microgenes. The user can specify a list of the microgenes that they are interested in. And these microgenes can be either tumor-specific macrogenes or marker genes for other cell types, such as immune cells. The cell types such as immune cells. And because different cell types may have different numbers of macro genes, to make our method robust to the number of macro genes, we use a metagene to summarize the gene expression pattern for a given cell type. This way, we will only have one gene expression image for each cell type. And for the histology, we know that there are three color channels, RGB. To make it comparable to gene expression images, we need to convert it into We need to convert it into a gray-scaled histology image so that we only have one color channel for the histology. So, after doing this conversion, we now have one gene image for each cell type and one image for the histology. So, we can simply stack the two images together and generate a new image that has two color channels, with one channel representing the gene expression information for the cell type of interest, and the other channel representing the And the other channel representing the histology image information. So, by using this new image as the input, we can perform convolutional neural network-based segmentation. And this will allow us to identify where each cell type is located in the tissue. And this annotation will achieve a pixel-level resolution because the histology image has a pixel-level resolution. So, for example, when you combine the gene expression provided You combine the gene expression provided by tumor-specific marker genes, we can tell you where the tumor cells are located in the tissue. And by combining gene expression provided by a cell type-specific markers, we can tell you where the specific cell type is located in the tissue. But in addition to the high-resolution cell type annotation that I just discussed, we can also automatically separate the tumor region into leading edge and the tumor. Leading edge and the tumor core. So, here is a data set that we analyzed in our study. This picture here shows the pathologist manually annotated the leading edge in the skin cancer sample. And this is the leading edge detected by our method. You can see that the leading edge that we detected agree in general with the leading edge provided by the manual annotation. Manual annotation. And this is the tumor edge and the tumor core that we automatically detected in this data set. After the core and the edge are separated, we can then perform a differential gene expression analysis between the core and the edge and detect what genes are enriched in the edge and what genes are enriched in the core. So, for example, in the tumor core, we detected genes such as AGF BB2 and CCMB1. And the CCMD1, you can see that both genes have a relatively high expression in the core, but low expression in the other regions of the tumor. And by performing gene setting enrichment analysis for the top enriched genes in the core, we found out that these genes tend to be enriched for path space that are related to the metabolism of lipids and the hypoxic condition. Condition. We know that there is a high proliferation rate in the cancer cells in the tumor core, and such high proliferation of the cells can lead to hypoxia and the nutrition deprivation. And this will affect the lipid metabolism of the cells. So what we found from the enrichment analysis agrees with our known knowledge about the tumor core. For the tumor edge, we detected about 100 edge in terms of the tumor. About 100 enriched genes. And here are two example genes that we detected. You can see that for CXCL12 and IR2RB, you will see that they have relatively high expression only on the tumor edge, but their expression levels are generally very low in the core. By performing teen-staty enrichment analysis for the edge-enriched genes that we identified, we found out that these genes tend to be enriched. These genes tend to be enriched for pathways that are related to the immune system and the inflammatory response, which also agrees with our known knowledge about the tumor edge. But next, I'm going to show you that using our high-resolution startup annotation procedure, we can also computationally detect tertiary lymphoid structure or TLS in cancer. A TLS is an abnormal lymphoid tissue that resembles a secondary lymphoid. That resembles a secondary lymphoid organs, and they are believed to be the site of immune response activation against the tumor, and they play a very important role in immunotherapy. In the TRS, we are expecting to see an aggregation of B cells surrounded by CD4 T cells and a small number of dangitic cells. In addition, the TLS needs to have protein CXCl13, which reputs the B cells into the site. B cells into the site. So, in order to computationally detect the TLS, we need to determine where the B cells, C D4 T cells, dendritic cells, and the CXCL13 are located in the tissue. We can perform co-localization analysis to identify a potential TLS. So, by using this procedure, we inferred two potential TLS in this data set. And these are the zoomed-in plots for Zoomed-in plots for the TRS that we detected in the histology image. We looked at the images very carefully together with our pathology collaborator, and our collaborator confirmed that the two potential TRS that we detected are real TRS. So, in the latter part of my talk, I will briefly discuss how spatial transstomics data can help recover the spatial location information for Spatial location information for cells in single-cell RNA sequencing. We know that single-cell RNA sequencing is very useful and powerful in characterizing cell types and states, but a big limitation of single-cell RNA seq is the lack of location information within the original tissue. And since the spatial transatomics can simultaneously measure gene expression and their location information, we can potentially leverage such data to help recover the missing location. To help recover the missing location information for cells in single-cell RNA-seq. This slide here shows an overview of our recently developed location recovery model. The key idea is to use the spatial transforms data as the training set to learn the relationship between gene expression and the spatial location. And then use this learned relationship to recover the 2D location or tissue layer or spatial domain for cells in the Spatial domain for cells in the single-cell RNA6 study. And because the training data set might be very small in sample size, in order to avoid overfitting and obtain additional information that is needed for model training, we also have an optional data augmentation step where we use a variational auto-encoder to generate artificial spatial gene expression data. In this way, we can enlarge the training data set sample size. Sample size. To learn the relationship between gene expression and the spatial location, we use a supervised feed-forward neural network to do it. And our method can recover the 2D location of a cell as well as the tissue layer or spatial domain of a cell. To evaluate the performance of our location recovery model, we analyzed a single-cell RNA-seq dataset generated by Hawja et al. By Haoja at Haw. In their study, they manually dissected the six cortical layers from post-mortem human brains, and they generated a single-cell RNA data from each of the six layers. Because the true layer information of each cell is known, this data set allows us to evaluate our layer recovery accuracy. So we train our model using the spatial transforms data generated by Main. Data generated by Maynard et al. in their nature neuroscience paper, where the authors manually annotated the six cortical layers. Using the information learned from this training spatial transcomics data set, we recovered the layer information for cells in this hot et al. data. And these bar plots here show the results obtained from our method, salary, and a competing method called Tangra. The red bar. Gram. The red bar in each plot indicates the percentage of the salts that are correctly recovered, and the pink bar indicates the percentage of the salts that are recovered to adjacent tissue layer. Using our approach, we achieved a top two accuracy of 74%, but for timegram, the top two accuracy is only about 35%. And in fact, the timegram performance. The time ground performance is not much better than random prediction. So, here I wanted to show you how the results can be visualized in the brain. So, here for each column, we focused on the cells that have known cortical layer information. So, for example, the first column here is for the cells that are known to come from layer one in the brain. And the colors And the colors in each plot represent the frequencies of the cells in this layer being mapped to different regions in the brain, with the yellow color indicating higher frequency of being mapped and the gray color indicating lower frequency of being mapped. So, if you look at this first column, you will see that salary outperformed 10 gram. So, for example, The time ground. So, for example, layer one, if you look at the annotation, layer one is expected to be located here. You see the yellow color produced from the salary recovery, but you see that the colors do not agree that well with the annotation for time gram result. And in addition, we see that for this data application, the augmentation step in salary really helped for some of the layers that are available. For some of the layers that are very, very thin. So, for example, if you look at layer 4, you will see that layer 4 is very thin in the cortex. If you only use the original data in the training set, you do not have enough sample size to learn the relationship between gene expression and the spatial location. But after doing data augmentation, we can see that we increased our ability to recover the locations for cells that are coming from this. For cells that are coming from this very thin layer. If you look at the tangram results across cells from different layers, you will not see any difference. So this data analysis clearly indicates that tangram is not very sensitive in distinguishing cells coming from different locations in the brain. So, in this talk, I have shown that the expression and histology images can provide complementary information about Provide complementary information about the teacher organization. It's very important to consider histology images in spatial transcriptomics data analysis. I also show that a proper use of the spatial transcriptomics data can help recover the location information for cells in single-cell RNA-seq. And what I presented today is just a start on how to integrate spatial transcomics, histology images, and single-cells in data analysis. In data analysis. But I think there are many exciting, interesting, open questions that need to be solved. I think for those people who are interested in developing methods, this is definitely an area that has a lot of potential. Finally, I would like to thank people of my lab, especially Jian and Qi Huang. Thanks very much for listening.