quite a community, but um it's wonderful to be here and to meet new people as well. Okay, so as usual I get this so where I come from, I mean my main interest for many years is really study boundary value problem for PDEs, mostly integrable PDEs. I mean I started off studying NLS, KDD, all this class in one one space dimension. One space dimension. But in fact, linear PDEs are also integrable in a sort of trivial way. And while the main motivation was studying the non-linear problem, actually in the course of developing something I will speak about, we found a lot of surprisingly new results, or at least results that were not in any book we could find, and nobody could give me any reference for in the case. Any reference for in the case of very simple linear evolution PDEs in one space dimension. And in particular, after we look at it as from the PDE theory coming to give some information on the spectral structure of some linear differential operators that unbounded domain with boundary value pro with boundary conditions imposed, so not self adjoint because of the boundary conditions. Because of the band limitations. And in this talk, I will speak of this really just a very simple example. This will be my paradigm for the kind of results that we found, and not just me, this is work with many others. Dave is here and will speak of something similar later today as well. And Thanasis Focus, who was my person I had my first postdoc with and I collaborated a long time with. Collaborated a long time with, and also others on the fringes. I also would like to talk, but I don't know if I have time to get there, of other curiosities about third-order problems that arose that are not directly a derivation or within this particular framework, but are sort of curious. So, if I get there, I'll say something about them as well. So, the talk mainly is about how PDE theory can contribute. Theory can contribute to understanding or to say something new about the spectral structure of these operators. And as I said, my main example throughout will be just something that's sometimes called the Stokes equation, the AD equations, depending which community is the linear part of KDB. So it's not an example concocted for mathematical abstraction only. It really comes from publicated well, from for me it came from being the linear part of. For me it came from being the linear part of the Cortex-Drizzi equation, which is one of the main integrable models in mathematical physics. Okay, so just one question that you may mull over while I rattle on. If I give you this problem, and I give you three homogeneous boundary conditions, and the first question you have to ask yourself is, where do I put the boundary condition to ensure that the problem is? Condition to ensure that the problem is well posed. And this again is not as immediate, the answer is not as immediate as one might think, but it was known in the Russian community from the 60s. Not really in books as such, but it's not too difficult once you start thinking about it, what the answer should be. But let's say you have these three homogeneous boundary conditions that make the problem work most. Although, so in many cases, it's not central. So in many cases, it's not self-adjoint, but you can show that it has infinitely many real eigenvalues and eigenfunctions. Does it hold then that you can construct the solution by separating variables and using these eigenfunctions to, as a respaces, to write it like this? That was what I expected all as in in a case when there were this you know real eigenvalues, infinitely many different eigenfunctions that one could compute. So Functions that one could compute. So I'll answer that later on. One crucial thing to keep in mind, really, is that this is not a new idea, it's pretty classical, the idea of eigenvalues as singularity of complex valued functions. And in the context of linear elliptic PDEs, for example, already in the 19th century, this was called the Watson's transformation, essentially the forming a The forming a series representations of the here is just a series solution of a two-point boundary value problem to a contour integral via residue calculation. Just put a function which has poles at the eigenvalues and then use any contour around any um region where the f function has no singularities, then it's bounded at infinity. If it reaches its is unbounded. Reaches is unbounded. So that was, it's quite a natural idea. I mean, it's an old idea. What I will talk about is sort of putting this on its head. It's the converse of this. So this approach that I've been working initially with Anastasis Focus for a long time, we call it now the unified transform approach. I don't know how much that name has stuck. It's really just a generalization of Fourier transforms. But it goes the other way. So in general, The other way. So, in general, even for a two-point boundary problem, so something that you have in mind as having a series solution, we can derive in general a complex integral representation. So, on a complex contour, a bit like that. And when there is an eigenfunction series, then this representation can be defaulted to that series. But it's more general. So, that's just for context to keep in mind to follow what is being. What is behind all these ideas? So, what does this integral representation look like? I have no time really to go into how we derive it. If I have time at the end, I will give the main idea. But this is just to say what the answer at the end of all the churn of applying this generalized Fourier transform looks like. And really, the name of the game is thinking of Fourier transform, classical Fourier transform. Classical Fourier transformers on R, but move away from the real line, allow the spectral parameter to be complex. And that opens up, of course, all the toolbox of complex analysis. The functions that are involved are generally exponential functions, so they're analytic where they are wherever they're bounded. And and that and that is really it's very simple, but that's really what's in in at the bottom of all this. In at the bottom of all this. So, for an evolution problem of this form, P is just a polynomial. Think here the third-order derivative of U if you want. Initial condition, boundary conditions at the two end of the interval. That's your data. And this will not be all the boundary conditions, of course. So it will just be three of them. And so there are some unknown boundary values that are. Some unknown boundary values that are part of the problem. In fact, if you try to solve this by Fourier transform, taking just Fourier transform and integrating by parts, that's where you get stuck. Because when you integrate by parts, all the boundary values come in and you don't have all of them. So you have it somehow to represent them or to eliminate them from your solution. If you are in second order, the heat equation thing, you can use sine. The heat equation thing, you can use sine or plus sign. Huh? There's this plus sign missing between the intervals. Sorry, where? Oh yeah, sorry, there is a plus sign missing. Sine and cosine transform do that for you for the heat equation. If you have Dirichlet conditions, you use sine transform that kills the derivative at the end points. If you have Neumann, you use cosine transform. If you have three or more boundary conditions, that doesn't work. If you use just Fourier transform. So you if you use just Fourier transform on a third order problem, you end up with three boundary values at each end and you only have three boundary conditions. So you have three unknowns and only really, there's no real transform that will eliminate them. And that's, if you don't believe me, try. So it's really crucial that you pass to the complex plane in order to be able to do something about it. If you do that, and then manipulate, use analyticity, the symmetry of the regions where all the functions are. The regions where all the functions are bounded and so on, you can define a sort of Direct transforms or some functions just of lambda, lambda being the complex spectral parameter, in such a way that then you can represent the solution of this problem in a form like this, which is very nice for because x and t are explicitly the dependence in x and t, which are the physical parameters of the problem, is explicit and everything else is sort of. And everything else is sort of hidden here, but this is the form you get. And these contours are really the Stokes lines, if you want. It's the contours where the t exponential is purely oscillatory. So this is really a sort of generalization of what you would expect for a solution that you obtain if you use Fourier transform. So as I say, just take it on faith for now. There is a lot behind this that I'm not thinking, but this is the That I'm not doing, but this is the form you obtain. So, what are all these bits that appear here? And this is, as I said, Dave Smith was my PhD student at the time that I was studying these third-order problems. What did I do? I don't know. Full view, sorry. And so he did a lot of work with me on this. But so these functions are really just this zeta plus, zeta minus functions. Zeta plus, Zeta minus functions here are transformed of just the given data. So somehow you are able to eliminate the unknown boundary values and have functions that depend only on the given data. So if your boundary conditions are homogeneous, the Z plus and Z minus will only depend on the initial condition. This function delta here is a determinant that you obtain in solving a system that you set up to eliminate the unknown boundary values. And the zeros of this determinant are crucial. Of course, if these zeros are on the contours, these integrations has to be understood as a residue calculation. There will be residues at these poles that you have to confuse. But most of the time, the zeros are not on these contours. And that's important. And they they are really the the eigenvalues not the of the different properties. Not quite. I mean, but not that if the two roots or the square root, not the eigenvalues usually Not the eigenvalues, usually we take the square of the cube, but it will become clean. And if the associated eigenfunctions, so guess if the eigenfunctions of the operator, the differential operator, are known to form a basis, for example if the problem itself adjoint, then this representation is equivalent to the series one. Even in that case, though, it has some advantages because it's, for example, is uniformly convergent at all the boundary when t equals to zero. When t goes to 0, x goes to 0, unlike the series one. It's fast, it's exponentially fast to converging, so for it's very good for numerical schemes, which we have also implemented. I won't speak about that. But the evaluation of this, the solution using this formula is very fast and very efficient. So, just to give you an idea of what this would look like if Look like if I had the heat equation, for example, what would these contours be? So you would need to have, if this is R and this is the lambda plane, these contours are where you would have e to the i lambda x minus lambda squared t. That's what we would have for the heat equation. So the contours are where lambda squared t has zero imaginary points. So that's pi over four. And this is, well, these are the contours. This is well these are the contours where lambda one squared is equal to lambda two square. These these lines are this gamma plus and gamma minus. That's gamma plus on the upper in the upper half space and this is gamma minus. And the zeros of delta, as I tell you, are the discrete eigenvalues. They are the square roots, so they are just the number of the integers. And if you want to write this as, well, I have this in the slides, but just to give you a In a slides, but just to give you an idea of what this is, yeah, that's my next slide. So, why if you, for example, have the Dichlet problem for the heat equation, that's what the picture looks like. Explicitly, what these functions that you can compute look like, and of course, they only include the initial condition μ0, the fully transform of it. These are always transformed. But if you then, as I said, If you then, as I said, if you evaluate where all these exponentials that appear are bounded at infinity, so where the functions are analytic, you can deform these contours down or up to the real axis and evaluate all the cancellation that occur, and what you come out is just the regular sign slip. So there is not giving you something that you did not know, but it's a different representation of this solution. Representation of this solution. And as I say, for the second-order problem, well, we knew it already, but for the third-order problem, it's quite crucial. Why is it crucial? Because if you have the third-order problems, three boundary conditions, actually, where the zeros are with respect to this contour really crucially depends on the boundary conditions. It always does. But in this case, where they are. Where they are changes what you can do with your control deformation. So, for example, if I take this set of boundary conditions, which is sort of mixed, because I have a parameter beta here that could be zero, and if beta is zero, this is in some sense the simplest uh extension of a Dirichlet type boundary condition you can imagine, though it's not Dirichlet, it has to involve one derivative because I need three conditions. Because I need three conditions. So if I have beta zero, then this determinant doesn't have this second part. You can study asymptotically, where you cannot solve exactly for the zeros of this, but you can study that there's theorems from the 30s that tell you where asymptotically the zeros of such exponential polynomials have to lie, and you can show then that they lie asymptotic, and that's all that matters for any And that's all that matters for analytic functions. All you want to know is the behavior at infinity, as far away from the contour of integration as is possible, right in the middle of these regions, and it's impossible to deform them all the way to these zeros. Whereas if beta is non-zero, then if beta is exactly one, the zeros are right on the integration contour, so you just do a residue computation without having to deform anything. And if it's between zero and one, And if it's between zero and one, then asymptotically they are on the integration contour. So again, you can deform the contour to pick up all the residues and write this as a series. So when this was actually what the problem is. You have to deform it to avoid them. Yes. You can do that. I mean, I'm not being very technical here. I'm just giving you the impression of it. So really, the contour would look something like this. But when I did this, I asked myself the question, so can you always find this serious solution? And it seemed to indicate that it wasn't always possible, which we now know. It in fact it isn't. What are the eigenfunctions? Uh, can you express them in some for some values of beta analytically? No. I can express the eigenfunctions modulo the exact values of the eigenvalues that you cannot compute all this. You cannot solve the same. Can you show how the norm grow? Yes, you can compute all that. So they grow exponentially faster. Yes, and you can in fact prove that the series, the L2 norm of the series grows. Yeah, that's exactly how you can do it classically. After you discover that that must be the case, you know what to do. It's just finding M. So this is what the zeros look like. So, this is what the zeros look like, just for corroboration, numerical corroboration. So, when the solid lines are these Stokes lines, where the exponential is oscillatory, and when beta is zero, the zeros are right as far away as possible from them. And as I said, unboundedness prevents you from using Cauchy theorem and deforming. And deforming. Whereas when beta is non-zero, asymptotically, you see, this is a very small beta, they start to be here, but then asymptotically they will go onto the contour. So once you go far, if you take the asymptotic value of these, you are able to show deformation. Because if you close within any finite ball, you can deform any way you like. These are only analytic functions. It's at infinity that the problem is. And you see when beta And you see, when beta is one, actually, they will be exactly on the constant. So that was good, that was quite exciting because, as I said, it's not in any book, but this is quite old, this realization that there is a dichotomy for third-order problems. Certain family of eigenfunctions do not provide you with a risk basis. And so, this integral representation where now I have remembered to put the plus in the middle. The plus in the middle, um, is all you can do. And these are two representative problems, uh, specific problems. In one case, when beta is a half, you can actually show that this integral representation is equivalent to a series, whereas in this other case, this sort of Dirichlet case, where there is no coupling of any of the boundary conditions here, for option one, the norms do not Open one, the norms do not grow up exponentially. No. There are, you see, that this, well, yeah, they're quite different. And you can prove it directly from the representation that this unit is finite, is comparative in alternative ways. And yeah, so these are, as before, the zeros in these two cases. The zeros in these two cases, where these solid lines separate the regions where the t-exponential decays or grows as lambda goes to infinity. And this is, you can actually write this as a transform pair, a sort of generalized fully transformed pair, tailored to this specific boundary value problem. So, for the first problem, I have not written the whole expression, but this is the explicit expression of the transform, if you want. So, rather than just have So rather than just have, if we were Fourier transform, it would be just e to the minus i lambda x f of x, and that's it. Here we have a whole host of other things. There will be also e to the i alpha lambda x and so on. Alpha, I haven't written it here. Alpha is the rotation by 2 pi over 3. Sorry, I thought I put it down, but I have. Which is the rotation that moves these regions from one to the other. One to the other. So, this is just to say explicitly: this is what the discriminant looks like, and you can prove that you can actually deform these contours onto a series, and you just evaluate the zeros of this delta, and you get a series representation. That's actually how we compute, even in the series case, the numerical uh just by computing numerically the residues. Whereas in the other case, there's something similar, but it's a difficult discriminant and so on, and in this case, there is none. On, and in this case, there is nothing. But you can actually write it down expressly as a Fourier transform here. And what Dave will talk about it more, but what he pushed was passing from this transform pair to actually a proper statement in spectral theory. So think back to what you have when you look at. What you have when you look at sine transform on a half-line, which does not have non-zero eigenfunctions. Okay, so if S is the second-order differential operator, and you are looking at the half-line with a Dirichlet condition at zero, there are no non-zero eigenfunctions. But you can define, Gelf and Die Lenkin did it, define a functional instead that satisfies this. Satisfies this, and in fact, the function is just the sign transform. And he called this a generalized eigenfunctions or an eigenfunction. So we have these transforms and that gave sort of inspiration to do something similar, but there is an interesting twist to what you have to do. So, this is Dave's terminology defining these augmented eigenfunctions, and they are simple. And they are similarly some eigenfunctional that do not quite though, do, oops, sorry, I'm pushing the wrong thing, do not exactly give the usual relation of an eigenfunction or of an eigenfunction. There is a remainder term. And this remainder term is just analytic in the right place. This remainder eigen. This remainder functional satisfies that once you integrate, it disappears, either by itself, and that would be really just a generalization of Gelfand's eigenfunctional, or after you divide by lambda to the end, if you're looking at the end or the differential equivalent. Okay, so if you define this augmented eigen functionals in this way, so modulo some analytic functional. Analytic functional, then you can diagonalize the operator, your third-order operator with boundary condition, which is non-self-adjoint, in the sense that it's diagonalized in the world of analytic functions. So there is modulo, some analytic reminder that this appears when you integrate along the suitable contour. So they do provide this transformation. This transforms to provide an effective diagonalization if you put yourself in this complex analytic world. And completeness, then, you don't need to do anything. It follows from PDE theory because these are simple PDE. We know existence and uniqueness of solution. So the completeness and the fact that you can also the completeness of your eigenfunction basis, if you have one, classical eigenfunction, just follows from KB theory. You don't need to do any extra work. Do any extra work. So I think they will speak more this afternoon about this. Almost out of time. I've said the most important things. I did want to talk a bit about other applications, things like time-periodic boundary conditions. I will not because I have no time. I just want to make a pitch for another talk that will happen later. And this has nothing to do with analyticity or this, but it's just something that I got invoked in just because I know so much about third-order linear stuff. And it's about Linear stuff, and it's about revivals of the Talbot effect. And it's a cute thing. I don't know if you've ever heard of this Talbot effect, but let me show you what it is. Look at it, although it was found for second-order problems, also also for these third-order ones. You start with a discontinuous initial conditional step function, compute the solution, and look at it, and it looks like this. This version regularizes. The dispersion regularizes the jump, but it gives you a pretty nasty looking continuous but nowhere differentiable function. Pi, you can compute the fractal dimension of this. But if you look at it at rational multiple of pi, suddenly the initial jump is revived. It's there again. Okay, some copies of it translated and moved around, but the initial to start with that initial step function, that's what it looks like at any rational multiple of one. At any rational multiple of one. So I thought, oh, very nice. What happens if you take any other boundary condition? Is this really, I mean, it's a number theoretic property that has to do with periodicity, but I was curious. So we looked at quasi-periodic and other things, and for second-order, this is actually a very robust phenomenon. It it still holds for non if you add nonlinearity to some extent if you take for second order quasi-periodic, for example. Quasiperiodic, for example, second order quasi-periodic still has this revival, third-order quasi-periodic doesn't. So, third-order is different if you look at quasi-periodic conditions of this form, depending on a parameter theta here, so that this is still a self-adjoint problem, actually. Um for Stokes it only holds if theta is rational. For if you look at for linear Schrodinger, theta can be anything and you still have it, but no. And you still have it, but not for those. So, these are other fun things that you can do with third-order operators that are diff it displays a really different behavior from the even order. So, that's all I wanted to show. There are all the details that I have completely brushed under the carpet in these papers. You got any questions or comments? I'm actually curious how you were doing the computations. Were you actually doing the numerics? What were you using? Oh, we were using some... So we actually compute for when you have nothing, when you want to just evaluate this complex contour, you deform just enough to have fast decay to infinity and you just. Fast decay to infinity, and you just you cut the these are there are algorithms of say profaton that do this very well. Okay, and then some quadrature is applied or something. Yes, I mean it's non-trivial, but I have I think I still have some pictures here maybe. Oh, I cut them off, but I can show you if you want. Yeah, I play follow-up. It's in the third reference. Yeah, it's this reference here. Okay. Okay. Any other questions or comments? What about the online audience? Is there any question or comment from the online? Very quiet. We have one minute. In one minute, can you say something about the applications? You didn't have time to. Because I know. I don't know if I can say. If I have one minute, you know what I will show is a very cute derivation of the Fourier transform inversion that doesn't use distributions or anything. And that really is at the basis of how we derive this complex transform. So instead of doing anything else, think of trying to solve this ODE, which really encodes the Fourier transform, for line. Holds the Fourier transform for lambda complex, and you want a solution in a solution μ that is bounded in lambda. And so, well, you can solve it. These are two solutions that are bounded, one in the upper half plane and one in the lower half plane for lambda. And then you compute the difference, and the difference, if you do it, is just this. So, this u hat is just the usual Fourier transform of u. So, u is any sufficiently Schwartz function. Say it's Schwarz function, and u hat is its Fourier transform. So by just finding this solution mu bounded in lambda, we somehow encode the direct transform. And then if you're given u hat, so something, so you're given a function on the real line and you want to find new analytic everywhere such that analytic in the upper half plane, analytic in the lower half plane. In the upper half plane, analytic in the lower half plane, such that the jump on the real axis, it cannot be analytic everywhere, is given by your given you have. And that's just a classical problem, and it has a classical formula for the solution, this Lehmann formula, so a Riemann-Hilda problem, that's it. And then once you have this function mu, analytic in C plus and C minus, and with the given jump, you can recover u from it. And that's just the inversion. And doing this for For more generally, it is exactly how we get those representations. Plus, you have to work on eliminating the unknown boundary conditions. But this is a very cute derivation of the Fourier transport. So, I hope it was worth your minute. Coffee break. 