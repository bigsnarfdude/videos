Consider situations which we constrain the set of couplings that are of interest to us when we're doing optimal transport. So, for example, if one is trying to, oops, sorry, if one is trying to couple stationary processes, one might wish to use stationary couplings. So, those are called joinings. I'll say a few more words about those. And so, we'll talk about estimating optimal joinings. I have some results in consistency and rates of convergence. I'll then say a few. I'll then say a few words about optimal transport for Markov chains, where we end up looking at transition couplings and some applications to graph alignment. So, the first part of the talk is going to be mostly me trying to motivate why we're doing this. My goal here is not to throw as many results at you as possible. So, I hope this will be informative, mildly entertaining. We'll see how that goes. And if you have questions, just let me know. All right. Just let me know. All right, so just a review of optimal transport. I'm pretty sure that everyone in the audience is well familiar with this, but I'm framing the optimal transport problem through random objects. So rather than speaking about measures, because that's going to suit us later in the talk. So we have a random object X taking values in some set X and a random object Y taking values in the set Y. And we are interested in considering couplings of those. In considering couplings of those. So you'll recall that I'll just use C of X Y to denote the set of all couplings of X and Y. And you'll recall that a coupling is just a pair X twiddle, Y twiddle, where X Twiddle has the same distribution as X and Y twiddle has the same distribution as Y. I'll be using twiddles throughout the talk to denote joinings and couplings. In addition, finally, we have a cost function between our Between our outcome spaces, I'll assume that's non-negative here. And the optimal transport problem here down at the bottom of the screen, there's two problems. One is to find the minimum expected cost, to find a coupling, excuse me, having minimum expected cost. And then there's also to identify the set of couplings that achieve that minimum. Okay, any questions about that? I'm assuming this. Any questions about that? I'm assuming this is all standard stuff. So, okay, my next slide is basically what the talk is all about. And if you want to go to sleep after this, you can. The idea is this. In general, if the stochastic objects X and Y and an optimal transport problem possess structure which is critical or interesting in some way, then one may wish to restrict attention to couplings with the same or similar structure. So one can think of that. Structure. So, one can think of that as constrained optimal transport, if you like. So, the cases of interest here are one when X and Y are stationary processes, and two, when X and Y are ergodic Markov chains. Okay, so let me say a few words about this. So first, we'll talk about stationary processes. So, all right, optimal transport for stationary processes is just a special case of what I talked about before. Here, the stochastic objects X and Y are. Stochastic objects X and Y are processes. I'll let them be one-sided, infinite, discrete-time processes. So X represents a process, X1, X2, X, and so on. X is stationary, and the XI's take values in the finite set, X0. So we're going to be considering finite alphabets throughout. Some of our results extend to the case of Polish alphabets, some of them don't. But for simplicity, we'll assume that. But for simplicity, we'll assume that all processes here are finite-valued. So y is a stationary process, which is the other object. It takes values in the finite set. Y zero, and we have a single letter cost. So let me clarify this. So when I'm evaluating the cost between two infinite sequences, I'm essentially just looking at the first component of those sequences and assessing the cost based on that first component. Now, that's clearly a restriction. Now, that's clearly a restriction. One could, in principle, look at costs that involve either finitely many letters or the entire sequence. Everything I say here extends immediately to the case of finite letter costs with a little bit of complication. And if one is looking at, say, a continuous infinite letter cost, then it's going to be uniformly approximable by a finite letter cost. So there's not a lot of loss of generality. Perhaps the single most important. Perhaps the single most important motivation for looking at a single letter cost is that in practice, right, we typically don't have access to infinite sequences, we have access to finitely many observations, and we want to make some conclusions, statistical or otherwise. Okay, so what does optimal transport for stationary processes look like? All right, well, a coupling of two stationary processes is just a joint process, which I'll call X twiddle, Y twiddle. So we can think of it. call X twiddle, Y twiddle. So we can think of it as a sequence of pairs. We pair X1 Twiddle, Y1 Twiddle, X2 Twiddle, and Y2 Twiddle, right? To be a coupling, the X Twiddle process has to have the same distribution as X, and the Y twiddle process should have the same distribution as Y. And so for single letter cost at the sort of interest here, the standard optimal transport problem considers this quantity, right? So we're interested in minimizing the expected cost here. The expected cost here of x1 twiddle, y1 twiddle. This is the first, right? So we're taking the expected cost over the first entry of the coupling, but we are minimizing that over all possible couplings of x and y. Okay, so this looks fine, kind of, but we'll see that there's a couple of issues, and these don't require a great deal of thought. First thing is, you know, when bad couplings happen to good processes, right? So in principle, So, in principle, as we'll see, this set of couplings without further constraints is really, I think, too large to be reasonable. And so, I don't think this minimization problem is that meaningful. So I'll give one example in a second. The other problem is by nature of how it's set up, the optimal transport problem here is really only focused on a single term or on single term rather than a long term rather than long-term average behavior. Average behavior. I'll clarify that in a moment. So, first, the example. Okay, so suppose we've got the simplest case possible, I've got two Bernoulli processes, just coin flips. So, X is a sequence of coin flips and Y is the same. And we can easily form a coupling X, twiddle, Y twiddle as follows. First, I start off with a sequence of coin flips, and then I'm going to let the X process be all the even coin flips, and I'll let the Y process be all the odd coin flips, unless. odd coin flips unless the index i is a power of two. And in that case, right, so if yi, if I'm considering yi, if i is a power of two, then I'm going to let y i be equal to z to the two k's, which means this corresponds to xk. Okay, so we see, so what I've done is I've got, and if we look, so the y process is still IID Bernoulli half, the x process is Bernoulli half, the X process is IID Bernoulli half, but because I've matched up some of these, some of the Y's with some of the X's that appear much earlier, right? The resulting coupling is A, non-stationary, and B, it's long-range dependent. Okay, so, and this is not obviously a very exotic example. So we've got two really simple processes, and we have a coupling that just doesn't make sense in my view, right? This isn't, if we're trying to carry out optimal transport, this is not a coupling that we would consider to be reasonable. That we would consider to be reasonable. Okay, so that's problem number one. Problem number two is, I guess, you call myopia. And if you think about what happens when you're doing an optimal coupling, is there questions? Yeah, of course. Go ahead. So it is a bad coupling, but there is a good coupling that achieve optimal transport in this example. So it's not like optimal transport is only achieved by a bad coupling in this example. True. True. True. True. True. Now we'll find cases, for example, in the Markov case, even if we restrict ourselves to stationary couplings, we'll see that we can, I don't show it, but it has been shown that the joinings or the stationary couplings that achieve optimal, in some cases, will achieve optimal transport in the Markov case are not Markov of any order. So weird stuff happens. So here I'm going to propose that we just consider stationary coffee in a moment. Just consider stationary couplings in a moment, but yes, I agree. This is just to show you that the set of couplings is rather large and we can couple two nice processes in a way that would seem counterintuitive. So the second issue here with standard optimal transport is myopia, and that is that when I'm carrying out the optimal transport problem with a, again, this relies on the fact that I'm looking at a single letter cost, right? Essentially, I'm only really trying. Essentially, I'm only really trying to minimize the expected cost at one time, right? So essentially, I'm linking up. I have these two processes. They start at time one, and then they continue to time two and three. What the optimal coupling problem is doing is just trying to link them up, which is to say minimize the cost at time one. After that, it doesn't care. So X and Y can essentially evolve independently after time one. And at those subsequent times, That at those subsequent times, the expected cost can be large. Okay, and again, maybe this is fine, but if one thinks about what one is trying to do in the stationary setting, it's natural to regard a coupling as an alignment of the two processes, right? I have two stationary processes, I want to align them, and in aligning them, what does that mean? I guess I would like once the alignment to be such that the alignment is aligned. Be such that the aligned processes at a given time are close to each other, at least on average. Okay, so standard optimal transport doesn't achieve this. Nothing wrong with standard optimal transport. It's doing what we want it to do. It's just that we're asking it, maybe it's the wrong question. Okay, so the natural thing to do, or a natural thing to do, is to say, why don't we just restrict our attention to stationary couplings? So those are called joinings. So, those are called joinings. I'll say a bit more about those in a moment. But a joining of two stationary processes is a joint process, same as before, nothing new here, right? The X twiddles must be the same as the X's in distribution. The Y twiddles must be the same as the Y's. But then there's this other easily stated but super important condition, and that is that the joint process itself must be stationary, which makes sense, right? I'm coupling two stationary processes, it makes sense that I would. Processes, it makes sense that I would consider stationary couplings. So let me let J of XY be the set of all joinings of XY. Note that J of XY is the subset of C of XY, which is the set of all couplings. Okay. So a few things. First of all, this set, you might say, is there a joining? Well, the product coupling is always a joining, right? Product coupling is always a coupling. So we know there's always a coupling. That same coupling is also a joining. So we know that there's at least one. So, we know that there's at least one joining, and that's the product, the product coupling. So, this set is non-empty. If we think about, we think of J of XY as a set of measures, right, moving back to the sort of measure point of view, this set is convex and compact in the weak topology. And moreover, the extreme points of this set, these are process distributions, right? The extreme points of this set coincide with ergodic joinings, which exist, which will always exist if X and Y themselves are ergodic. themselves are ergodic. Okay, so these joinings have nice properties. Joinings go back. Joinings were first studied by Furstenberg back in the 60s, and he did some really beautiful work. And the first thing that he looked at was essentially this. If I have two processes and the only joining, if J of Y expo I contains the product coupling and only the product coupling, those two processes are said to be disjoint, right? And it turns out that disjoint processes do not have common factors. So then in a kind of high So, then in the kind of high-level sense, they are relatively prime. Right? And so, this was Furstenberg was sort of trying to understand in the kind of number theoretic sense, the processes, right? And which processes could not be expressed as factors of others. Okay, in any case, beautiful stuff, but we're not going to do anything that deep here. So, okay, so, all right, then if we have, once we've restricted ourselves to joinings, now we have the optimal joining problem. We have the optimal joining problem. It's essentially the same problem we had before, but we've just changed the set of things that we're minimizing over. So you can call these, you know, the admissible couplings. So let Tc of XY be the minimum of the expected cost under joinings. And let JC opt XY be the set of joinings that achieve the minimum. And because of the compactness and all that, everything here, there's no need for. And all that, everything here, there's no need for infima, everything here is a minimum, minima are achieved and all that. So, there's no problem with that. All right, so it turns out that there's a simple and in retrospect, perhaps not super surprising connection between optimal joinings on the one hand and optimal transport on the other. And we can see that simply through a couple of definitions. So, let CK here just be if we have two sequences, X and Y, let's see. have two sequences x and y let c k of x y just be the average cost of the first k elements and let c bar of x y be the limb soup of c k okay so it turns out that essentially the optimal joining problem with cost with this single letter cost c is equivalent to the optimal transport problem with this infinite letter cost limiting average cost c bar okay which is convenient Okay, which is convenient. Can I ask a question? Of course. So, does it mean that for the zero one loss, you just get the Ornstein d-bar distance? Yep. Okay. Yep. That's the same theory, right? Essentially, that's so what Ornstein and Weiss, sorry, Ornstein, what well, I guess what Ornstein showed and what was later shown by Shields, New Hoff, and Gray was this connection here, right? That you could express this problem, the optimal joining problem, as a limit. Optimal joining problem as a limit of optimal transport problems. And so that's due to them, or this is a sort of an extension of what they did. And it requires a little bit more work. You have to use the cyclic monotonicity of optimal transport, but you can also establish this, essentially bringing the limit inside, right? So there's an exchange of limits and that involves cyclic monotonicity. So yeah, again, I'm not claiming like this is a greatness here, just sort of a simple connection, which is useful. This sort of a simple connection, which is useful to us as we try to do some estimation. So, okay, so let's think if I'm a statistician, I guess I might want to estimate something. So, suppose I've got two ergodic processes, stationary and ergodic, right? And I have X that has distribution mu and Y has distribution nu. And let's assume for simplicity, they're defined in the same probability space, and that I have access to n observations from X. n observations from x and n observations from y and i want to estimate the value of the optimal joining problem and also identify at least in the limits some type of optimal transport plan so then the question is how do i do that from data right so i see galvorn nodding i don't i think everyone in the audience would probably do the same thing that we're going to do now right and that is well all right you know we'll have to somehow uh consider the k-dimensional estimate the k-dimensional K-dimensional estimate, the k-dimensional marginals of the x and the y process, and do something with some optimal transport stuff on them and glue it all together and see what happens. So that's what we're doing. All right. So with this notation here, again, mu k is just the k the k-dimensional marginal distribution of the x-process and mu k, same thing for the y process. Here's our estimation scheme. So again, these are our observations which are given. Our observations, which are given. Well, then we'll choose a block length k here that is substantially smaller than n. I'll say in a moment what sort of block lengths might work. And then we do the following. First thing we do is we estimate mu k, the k distribution, the k dimensional distribution of the x process, and we form an estimate mu k hat of that just using the relative frequency of k tuples. Okay, pretty simple. Again, everything here is finite, so this makes sense. So, this makes sense. We do the same thing for the Y process. We form an estimate nuk of nuk. And then we use this connection between optimal transport and optimal joinings in the following way. We let Tk hat here be the solution of the optimal transport problem with cost CK and with marginal measures mu k hat and nu k hat. Okay, and then we let pi k. And then we let pi k hat be a k-dimensional measure that achieves the minimum there. Okay, so we have a number tk hat over here. We've got a k-dimensional measure pi k hat over here. This number t k hat will be our estimate of the true optimal transport cost. Okay, and then we're going to use pi k to try to estimate an optimal transport or optimal joining plan. So, what we do, and again, this is a standard construction, you can find this in Paul Shields' book. Paul Shield's book, where he goes through Ornstein's proof of the isomorphism theorem. So, we're going to form a stationary distribution on the product space of x0 and y0 using this k-dimensional measure as follows. We first concatenate independent copies of pi k, and then we simply randomize the start position to make sure that it's stationary. Okay, again, all fairly standard stuff. So, what do we get in the end? What do we get in the end? Well, we get consistency. So we begin with a kind of existential statement. You know, there exists a sequence K such that with probability one, these estimates, these numbers here converge to the optimal joining cost. And moreover, these process measures here converge weakly to the set of optimal joinings. Okay, so I'm not sure if there's any questions about this. I'm not sure if there's any questions about this. This, of course, leaves open the question of: well, how the existence of a sequence is perhaps somewhat unsatisfying, right? So, here it leaves. Sorry, the sequence may depend on the distributions. Absolutely. Absolutely. Absolutely. So here's a sufficient condition, right? Obviously, we want these block lengths K to grow, or at least not to decrease. We like they need to tend to infinity. So that's pretty obvious. If, and here we have, if we think about just the zero, one cost here, right, essentially Hamming cost, right, it's enough that K, that the K-dimensional estimate, the estimate mu k here of the K-dimensional measure of the X process converge essentially in an optimal transport sense, right, using the average, this is just the essentially an L1 cost, right, to the true. To the true marginal μk, and the same thing happened here, right? So, if k grows slowly enough to enable both these things to happen, then we're good. Okay, so, all right, we can tweak this. I don't want to say too much about the next result, but you know, if we make assumptions, then we can prove stronger things. No surprise there. So, if the processes X and Y that we are observing are phi mixing with mixing coefficients phi of L, and those mixing coefficients have Phi of L, and those mixing coefficients satisfy an appropriate growth condition, then we can get, you know, essentially sort of finite sample error bounds on the expected difference between our estimate of the true optimal joining cost and the optimal joining cost. Okay, and these estimates depend on m and k, which we can choose, right? M is a sort of a gap that we use in the proof. Again, sort of a more or less standard, not fully standard. Standard, not fully standard, but a relatively standard argument with big blocks and little blocks, right? K here is our block length. So some special cases where it can be a little more concrete. If the two processes X and Y are IID, then the right-hand side with appropriate choices of M and K is big O of n to the minus a half, which is what we would expect in the IID case. And this is kind of concordant with what's already in the literature. In the literature. So, not claiming anything new, but just wanted to double-check and make sure that we're in the right ballpark, which we are. On the other hand, and I don't know if these are, I cannot say if this is the best possible, right? This is an upper bound. We don't have matching lower bounds, at least not yet. If x and y are Markov intergodic, then the right-hand side is substantially smaller. We get a bound log log n over log n. So, which seems weird. However, it's worth, it's. It's worth it, it's maybe less weird when we remember that if X and Y are Markov, and I'm considering again joinings of them, there are many joinings of Markov processes, right, that are not Markov of any order. So the family of joinings that we're considering is substantially larger than the family of Markov processes of the same order as X and Y. Okay, so that I think is what gives us this rather disappointing rate of convergence. And again, I don't know if it's improvable. And again, I don't know if it's improvable or not. I conjecture it probably can't be. You probably can't do much better without making more constraints. Okay, so, you know, if you can do it, you should do it with a penalty, right? So the results inconsistency in rates also hold in the case where we look at the penalized problem. So here we're minimizing the expected cost, but we add in a But we add in a penalty term that depends on the entropy of the stationary process. Now, again, this entropy makes sense, right? We're looking at finite alphabet stationary process, a joint finite alphabet, stationary process. So the entropy rate is well defined. And we can get similar results. Okay. I think I've got a few minutes. So let me just say a couple things about Markov chains. All right, this is essentially the same setting we had before, and the same principle applies, right? Suppose now that we know that the stochastic objects of interest now are not simply stationary processes, but more specifically, they're ergodic Markov chains. So let X be an ergodic Markov chain with transition matrix P. Suppose Y is an ergodic Markov chain with transition matrix Q. We'll assume that Xi and Y, as before, X i and Y as before are taking values in finite alphabets, and we have a single letter cost. So here we encounter a difficulty that I mentioned before. A joining of X and Y need not be Markov of any order, right? So, and considering Markov, we could restrict ourselves to joinings that are Markov, but it turns out that that is still problematic. And what people have done in the literature is to go one step further and look at something called transition couplings. Called transition couplings, or what we call transition couplings. You can find mention of this. There's an unpublished note of Diaconison and Aldus, where they are thinking about this problem, and this is essentially what they choose to do. So fast definition. So a Markov chain, essentially a coupling for us, X twiddle, Y twiddle, is a transition coupling of X and Y. If it has a joint transition matrix R, and there's this condition that says. And there's this condition that says this: if I look at the probability starting at state xy of going somewhere else, right, that probability is itself a coupling, right, of p of x, p dot given x and p and q dot given y. So I require that this condition holds for every x and y. So we're looking at a special set of Markov chains here. And then there's this fact which actually is. Which actually is some work. So, if P and Q are given, or you have appropriate estimates of them, then one can find approximate transition couplings in time, which looks like the size of the alphabets to the power of four. Which is somewhat disappointing, but if you think about the dimensions of R, right, this is kind of about the best you can do. So, but to get So, but to get an operative way of estimating these transition couplings, we combine some ideas from Markov decision processes, basically reinforcement learning, and ideas from computational optimal transport, in particular entropy regularization. And what we find is these complexity bounds here are comparable to existing complexity bounds. I believe Riglet and Weed and others have bounds of the same sort. Have bounds of the same sort. They're considering standard entropic OT. Here, we're focusing on the Markov case. So, I just want to say one thing, like, what's this good for? So, one thing that we think it's good for is graph alignment. So, if I have two weighted graphs, they have non-negative edge weights, then I can, and a cost function on the vertex sets, I can essentially turn this into, can turn each of the graphs into a random walk. Turn each of the graphs into a random walk, right? So if I let x be the simple random walk on graph G1 based on its edge weights, I can let Y be the simple random walk on G2 based on its edge weights. Those are both Markov chains. Under appropriate conditions, they'll be aperiodic and irreducible, so ergodic. And the important thing from a practical point of view is that these Markov chains capture the structure of the graphs, right? So the local structure of the graph. So, the local structure of the graphs is reflected by the transition probabilities, how the chains are moving from a vertex to another vertex. The global structure of the graphs is reflected by the stationary distribution overall. And well, okay, I have two Markov chains. I can apply optimal, I can look at an optimal transition coupling of these and to synchronize them. So, here's an example. For this, I credit my student. For this, I credit my student Kevin O'Connor for the nice example. So, here we have two graphs with the same vertex set, and so our cost function here is just it's going to be zero if the two walks are with the same vertex and one otherwise. And we see that while the graphs are the same, the edge weights, which are represented by thicknesses, are different. And so, what we see upstairs here is independent, we have independent random. Independent, we have independent random walks on G1 and G2. Those will be colored green whenever the two walks happen to be in the same state. And what we see downstairs is the optimal transition coupling that we get. So we see about a 12% match upstairs and by contrast, a 52% match downstairs. All right, I don't have too much time. So let me just summarize. Just the basic idea. Again, the theme of the talk is if the stochastic object. Theme of the talk is: if the stochastic objects X, Y, and X, and Y in an optimal transport problem possess critical structure, one may wish to restrict the OT problem to couplings with the same or similar structure. I'm sure there's many other examples of this. The two we talked about here were stationary couplings of stationary processes and transition couplings of Markov processes. And with that, I will stop. Thank you very much. Thanks. Thanks. So Andrew, questions? So if the alphabets are not discrete, then of course these rates are very different, right? And it depends on the post-function, I guess. Yeah. Yeah. If the alphabets are not discrete, we can definitely say something in the Polish case. And there, of course, you. And there, of course, you have to worry when you're trying to do estimation of the marginals. You're going to have to quantize things, right? So, and then the cost will come in. So, yeah, it's definitely more complicated. And for the entropy regularized case, there's a problem. There's a semi-continuity condition for entropy that does not hold in the Polish case, but it does hold trivially in the finite case. So, yeah, so there's work to be done. I mean, we were primarily interested in just. Done. I mean, we were primarily interested in just seeing what one could say in that simple case. And that is the setting. I'm not sure if this is justification or not, but it is the setting that was considered by Ornstein and Shields and Gray and Neuhoff, right? They were primarily concerned with the finite alphabet setting. But yeah, there's certainly more that could be done. So, Andrew, another quick question. So, I forgot, do we know the bar distance between Bernoulli P and Bernoulli Q? Between Bernoulli P and Bernoulli Q, like, is there some simple formula? Or I think, yeah, there's, yeah, there is. Um, so then you're because I thought that if not, then your method could, you know, be used to guess those distances, right? I think if you have a Bernoulli P. Yes, the uh you could I have to, yes, I there is, I think there is, there is a simple like closed form thing for Bernoulli P and Bernoulli. thing for Bernoulli P and Bernoulli Q. So I don't think this would be helpful in that case. I think there should be a closed form for that. That's pretty simple. I assume that's in some of the Ornstein Weiss work. They did some things on estimating processes, so I would think it's there. Yeah, so I'm not claiming that we could, that you could certainly estimate it in this way, but I think there's probably a closed form. Okay, thank you again. My pleasure. Thank you.