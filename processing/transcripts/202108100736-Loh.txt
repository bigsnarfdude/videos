And Ruen Zhou, who's also at Cambridge now. And this is actually work from a few years ago, back when Min was a postdoc at the Wharton Department of Statistics at University of Pennsylvania. And I was also previously there. So I'll get right to the problem. I know that the audience here is somewhat diverse. So just a very quick review of the question of community recovery or estimation. I know several people here have worked on. I know several people here have worked on this problem, although it might not have been talked about yet in this workshop. So the problem is that what you observe is a graph, which previously was mostly just in the form of an adjacency matrix. So the edges are either present or absent. And the goal is to try to recover some sort of underlying structure that is in the form of the community, the communities of The communities of the graphs. So, the idea is that every node is in exactly one community, and you want to recover this underlying community membership. Now, oftentimes there might be higher rates of connections for nodes within the same community, or maybe lower rates of connections. And for that reason, you can imagine that even if you just observe one adjacency matrix for edges that were formed according to some stochastic process, then you should be able to. Process, then you should be able to recover the underlying network structure. Okay, so the mathematical model that we'll look at is the stochastic block model. And for the stochastic block model, the idea is that you have all of the nodes being partitioned into one of K communities. And then the presence of an edge, it's sort of like an Erd≈ës-Meini graph for each one of the communities within community edges. So with probability p sub. So with probability P sub sigma i sigma j, there's some probability that each edge is present for nodes within communities i and j. Sorry, communities sigma i and sigma j. And this gives rise to a sort of block structure of the adjacency matrix. So w here is the adjacency matrix that you observe. And w will generally be close to the expectation of w and you would imagine that the expectation And you would imagine that the expectation of W would then have these particular blocks. So, for instance, if the probability of connections for nodes within the same community is P, then the expected value of W would be a matrix with a whole block of P's and then a whole block of P's in the lower right corner as well. And then maybe there's some other probability of intercommunity connection. So there would be a block of Q's in the upper right and lower left corners. Okay, so the questions that are asked in So, the questions that are asked in general are: what are good algorithms for estimating these communities given one instantiation of an adjacency matrix? And also the question of computational tractability. So we want to know what are the limits of estimation and what sorts of algorithms can be used to recover these communities. So, what we'll be looking at for our measure of accuracy is the misclassification error. So, this is the fraction of nodes in the graph, where the graph is on n nodes, which are Graph is on n-nodes, which are classified incorrectly. So, what you see here is you see a Hamming distance between two vectors. And the first one is the assignments of the algorithm, sigma hat, to the nodes in the graph. And so W is the observed adjacency matrix. And then you have some algorithm that turns that into a mapping on the nodes, which partitions the nodes into, assigns each one to one of K communities. And then you see that there's also a minimum. And then you see that there's also a minimum overall permutations of the k assignments. And that's just well, it's because maybe you label the underlying nodes according to some system of one through k. And then your algorithm might output something with, let's say, the ones are switched to twos, et cetera. So you just find the optimal matching, and then you see how far off you are. And what we're interested in is we're interested in the risk, which is the expected value of this loss. Expected value of this loss, because of course, there's randomness in the way that the adjacency matrix is generated, and there's also potentially randomness in the algorithm. And what we're interested in is this minimax risk quantity. So this is something that as statisticians, we study a lot. So we're interested in the best possible algorithm. So we want to take an infimum over all possible algorithms, theta hat, which are going, or sigma hat, which are going to be performed on the observed gene C matrix W. Observed genes C matrix W. But it's also a minimum max risk. So it will be a max over, well, the worst case that we could have, where the worst case is of the underlying structure. So the original community assignments, it could be, well, it's a worst case, sigma naught, which is the underlying actual community assignments. And then in general, we'll also have the parameters of the model lying in some sort of parameter space. So in this case, I'm putting this, I'm denoting p by the underlying. I'm denoting p by the underlying parameter space. And what does that mean in this block model? Well, I said that what we have is these w's are generated according to a Bernoulli matrix that tells you the probability of connection. So we'll be interested in understanding what this quantity looks like for different thetas of interest. Okay, so there's a lot going on in this slide. So previously, what had been studied was the question of community estimates. The question of community estimation, where you just have an unweighted graph. So you just have an adjacency matrix where the edges are either present or absent according to the stochastic block model. And it was proven by a paper by Zhang and Zhou a few years back that the rates of estimation are governed by this quantity I n. And I n is written out here. It turns out to be a sort of divergence between the Bernoulli distributions with probability A Distributions with probability a over n of success versus probability b over n of success. There's a lot going on here. So, this is the rate that sort of governs whether or not this minimax risk is going to go to zero. So that's what we mean by this quantity sort of, well, I mean, if you want to see whether or not this quantity is going to zero, then what's controlling it is clearly this I sub n. And furthermore, to be a little bit more precise, so we're looking over a particular parameter space. Particular parameter space. And the parameter space that one can look at are where the within community edges are connected with probability at least a over n, and the between community edges are connected with probability less than or equal to b over n. If you want to simplify things, you could just say, let it be the parameter space such that within community, edges happen with probability a over n, and between community, edges happen with probab. So again, the quantity I sub n is written out here. Is written out here. And you can specifically write down what the expression would be for this Reni divergence in terms of A and B. Okay, so as I said, if we want to see when this minimax risk is going to go to zero, then it depends on what I sub n looks like. And in particular, it depends on whether n times I sub n is going off to infinity. Okay, so the rough intuition of this is that, well, in some sense, Is that, well, in some sense, the distance between the distributions, Bernoulli A over N and Bernoulli B over N, tells us how hard it is to distinguish whether nodes are part of one community or the other. It sort of, well, it kind of tells us how difficult it is to distinguish whether or not the edges are coming from between community connections or within community connections. So, the question that we wanted to ask was: well, can this be generalized? So, usually in the past, the rates of estimation were given just in terms of the first equation. So, for a particular A over N and B over N, then the rates can be written as an expression in terms of A and B and N. But if you think about it as a sort of divergence measure between Bernoulli A over n and Bernoulli B over n distributions, which are generating those edges, which are either within or between communities, then you could ask, well, what if I replace this by some arbitrary distribution? By some arbitrary distribution. And so, this is, if you think about a weighted graph, then it kind of makes sense. Maybe there's a Pn that generates the weights for within community interactions, and there's a Qn which generates the weights for between community interactions. And so you could ask the same question, well, should it be this Reni divergence which is showing up in the rate? Or maybe it's something else, which in the case of Bernoulli distributions happens to just simplify to being a Reni divergence of order one-half. Okay, so. So, this is a slide that maybe we don't have to pay too much attention to. So, there are applications. Of course, we're driven by the theory rather than the application, but there are cases in which you might observe a weighted graph and want to do some sort of community partitioning. So this is an example with the C. elegans worm, and you might want to try to look at the connectome and separate these into different communities. To different communities. And what you might see is maybe, so I've said everything in terms of weights, but maybe the edges are in terms of a certain type. So they could be classified as category one, category two, or category three. Okay, so this gives rise to what we're going to talk about next, which is the problem when we're not exactly looking at weights, but we're looking at labels. And this will connect to the more general question that we'll talk about later in the talk, which is general continuous weight. Continuous weights. Okay, so for now, imagine that the way that we're generalizing this problem is that we don't have weights on the edges, but rather than just having the edges being present or absent, we have possible categories. So the edges could be red, blue, or green in this picture. And again, we want to try to use the information in this observed graph to recover the underlying communities. So let PN and QN, which are here going to be And Qn, which are here going to be discrete distributions, be the distributions of edges within the communities and between the communities. And the point is that, well, you could just drop all of the information about the edges. So imagine that you color everything black and you do community recovery. Well, you probably don't want to do that because there might be some useful information that's there in the colored edges. Okay, so here this is just showing a picture. Showing a picture. I think I mentioned this with the CL again, but you could have edges of different types, and you want to do something better than just combining everything and doing a recovery algorithm for usual unweighted cases. Okay. So what have people done in the usual binary case? Well, this is very much inspired the algorithmic development that we did here. So you could try to figure out the maximum. You could try to figure out the maximum likelihood estimate for the community membership of the underlying nodes. But as you might imagine, this becomes computationally infeasible. So there's a algorithmically, what had been studied a lot is a method based on spectral clustering. So the idea is that W can be viewed as the adjacency matrix, the 0-1 adjacency matrix, can be viewed as some approximation to the underlying. Approximation to the underlying matrix that gives you the probabilities of connection. And in the binary case, as I said, if you have probability P of edges being present in the same community and Q of being absent, then you have a block structure where you have a bunch of blocks that are all P's and then you have a bunch of blocks that are all Q's. So now if you view the, if you just look at the adjacency matrix and think about each one of the columns as being a separate data point, then you'll see that actually there's a Then you'll see that actually there's a well-separated cluster structure amongst all the n vectors. In fact, all the vectors for nodes in the same community are going to be identical. And so what you can do is you can, thinking about w as a perturbation of the expected value of w, you can try to do spectral clustering and then look at the output of this and classify nodes according to how they're clustered with spectral clustering. Okay, so a seminal paper by Leigh and Ronaldo in 2015 showed that. In 2015, showed that, in fact, if you do this, then you do get some good guarantees for community recovery based on spectral clustering. And well, what weak recovery means is it's the type of separation that you would need in order for this minimax risk to be going to zero. Okay, but that turns out to be a good start. But if you want to actually get optimal rates on the actual minimax risk, then there's an additional step. Risk, then there's an additional step which Chao Gao and co-authors introduced, which I might touch upon a bit later because we also use this idea, which takes that initial estimate of what the underlying communities are and refines it a little bit to get optimal rates of recovery for the final output of the algorithm. Okay, so one thing to note is that you might imagine maybe instead of just dropping the labels. Just dropping the labels of the edges, well, I could try to do spectral clustering on the labeled edges too. So maybe I have red, blue, and green, and I can write those edges as one, two, and three and look at the adjacency matrix of ones, twos, and threes and do spectral clustering. Well, that maybe is not such a good idea because I could relabel my edges as 1.5, 6, and 7, right? And I mean, since the labels are somewhat arbitrary, I probably want to do something a bit smarter than just applying special clustering on the weighted adjacency matrix. Weighted adjacency matrix. So the idea that we had was to look at some unweighted version of the graph and then perform the spectral clustering and refinement algorithm on the unweighted version of the graph. Of course, as I mentioned just a moment ago, we don't want to just drop the colors of all of the edges because then we might lose some information. So the idea is actually that we want to look at a subgraph that just corresponds to one color of edges. One color of edges. So here's an example. These are two possible mappings. The first one just drops the colors of the edges, and the second one simply creates a subgraph of blue. So the claim is that actually that there's going to be one good color that we want to look at. And we look at that good color, then that gives us an initial partitioning of the communities that's good enough that we can then refine it and achieve optimal rates. So let me maybe talk about. Yeah, I'm going to kind of sub-sample these slides. So I'll just say that there's an analysis in the Zhang and Zhou paper for binary weights on the edges. And in addition to analyzing the performance of the algorithm that they gave, they, of course, had to give a lower bound to show that it was minimax optimal. And modifying that analysis also gives us a rate. So in particular, this Renny divergence. So, in particular, this Rennie divergence of order one-half shows up as well. In this case, though, we have L different labels, L different colors that we can have of the edges. So, it's a different expression. Okay, so in terms of picking out a color that we'll use to cluster on, the idea is to look at what this Reni divergence is when we have L colors to the edges. And you can write down what the Reni divergence is. And it turns out that you can write as a first order. You can write as a first-order approximation, you can write this as the Hellinger distance between the two discrete distributions pn and qn plus lower-order terms. Okay, so now the point is that, well, we're going to look at a setting where weak recovery is possible, where n times i n is going to infinity. This is an expression you saw earlier in the talk as well. If that's the case, then it turns out that, well, if you look at what n times in is, then there must actually be some term which is going to be divergent. term which is going to be divergent. Okay, so it's a sum of finite L plus one quantities. So if the whole thing is going to infinity, then at least one of them has to go off to infinity. And it turns out that that's actually sufficient in order to get weak recovery. So if you want to make this risk quantity go off to zero, then you can just cluster on L star alone. Of course, that doesn't mean that we'll get optimal rates in terms of the minimax risk, but that's at least an initial. Max risk, but that's at least an initial first approximation of what the community structure should look like. Okay, so then the next question is: how would you identify this privileged color based on just observing the graph? Because you know that now there's one that there's at least one color that will give you some sense of what the communities are, but you need to, you don't know this a priori. Okay. And also, how do you refine this initial clustering in order to obtain? This initial clustering in order to obtain better rates than what you would get by just clustering on one color. So, I think I'm going to go quickly through this part so that I can get to the continuous weights parts. But the idea is that you sort of try, first of all, you try every single color separately. So this is a very natural idea. You know that at least one leads to good clustering. So for every L, which is the possible label that you look at, you perform the unweighted community recovery. Community recovery algorithm just for that particular color. And then you look at these within community and between community edge probabilities. So, what you have, of course, is you have the observed graph. And then you can, once you cluster into communities, you can estimate, well, if the probability of connection for edges within communities is the same and for edges between communities is the same, you can just take an empirical average of what's observed to get an estimate of what these connections are, p hat and q hat. These connections are p hat and q hat. And then you sort of look at which of these colors amongst all of the L estimates that you've gotten achieves the maximal separation between P L and Q L. And the maximal separation sort of for technical reasons is written as this ratio here. Okay, and that's the color that you'll use as L star. So that's the color that you'll use to give you sort of the best possible community assignment just based on one color. Based on one color. Next, though, we have to actually go from this rough community assignment to an optimal one. And here's where I guess I'll just say that, well, details are, of course, in the paper, but they're also based on the refinement methods from Chao Gao et al. for binary matrices. So, kind of very, very roughly, what you want to do is for every node U, well, you have a clustering that Well, you have a clustering that came from clustering according to this privileged color L star, but you want to refine it based on local assignments. So what you do is you cluster everything else using the spectral method that I described, and then you choose the color of U based on some sort of weighted vote. It's kind of like a vote amongst all of its neighbors, but it's sort of weighted by the likelihoods of connection. And then after that, you combine all of these, you combine all of these. Combine all of these assignments. So this is probably too fast for you to follow, but yeah, so the rough idea is sort of a local refinement for every single node based on its neighboring assignments. And this is based on similar ideas in the paper by Gao et al. Okay, so basically what we end up with is optimal rates. So that's what this theorem is saying. It says that we have these bounds with high probability on the probability on the on the on the loss between sigma hat and sigma naught and you can change this also into a bound on the expected value of the loss function which is the same type of which is exactly the risk that we're interested in and and the rate is again going to be characterized well what we saw earlier was exactly n times in over k okay so now in the remaining couple minutes um i want to say a little bit about the continuous setting so this is actually technically So, this is actually technically quite a bit more difficult. So, what we want to do now is instead of just looking at a discrete distribution on L possible values, we want to look at a case when maybe we have distributions that are Gaussian distributions or exponential distributions or Laplace distributions or something else. So, there was very little work in this area before we worked on this problem. And what we were able to do was to figure out what the optimal error rates were and come up with. The optimal error rates were and come up with a computational befeasible method to also recover the rates. Okay, so just a little bit more detail. So, what we assume is that the weight distribution is going to be the mixture of a point mass at zero and some continuous distribution. So, that means that with some probability, there's no edge present. And then if there's an edge present, then we draw the weight of that edge from a distribution with density Pn or Qn. Okay, so we have a probability P0 and a probability Q0 of. And a probability q0 of no edge being present for connections within and between communities. And then otherwise, we have continuous distributions Pn and Q1. So the idea is that we want to go back on what we proved in the first part. So we want to go from continuous to discrete. And the game here, just very much like density estimation in non-parametric statistics, is that we want to come up with the right discretization of these continuous distributions so that we can apply the method that we talked about earlier. The method that we talked about earlier. And the idea is that the arguments for what I talked about with the labeled case, where we had L different labels, well, that theory also works as long if L is slowly growing with N. And we just need to make sure that L is growing slowly enough because, of course, well, the method that I described earlier, I said that it depended on being able to pick out some privileged color L star. out some privileged color L star because we had a summation that was going to infinity so at least one of the terms needs to go to infinity. Now that only works if you have a finite number of weights L but it turns out that if this number is growing but slowly, then the theory for recovery should also still hold. Okay, so now we need to figure out what the right sort of discretization would be. And well we can look at several different settings. And well, we can look at several different settings, which we did in our paper. So, if Pn and Qn have bounded support, then we can look at these equal length intervals and we can tell you exactly what n needs to be in order for this to work. Otherwise, you can imagine maybe you have a Gaussian distribution, right? And you want to partition it into intervals. And so, what you do is you first apply an appropriate transformation and then partition into equal length intervals. Okay, so. Okay, so it turns out that this method will do well. So, I mean, just to reiterate: so, after you partition into these intervals, then you just color the, if the edge weight is within the certain interval, then you color it as red versus green versus blue, et cetera, et cetera. So, we could prove that, in fact, if you do this method, then it gives optimal rates. So, here I've kind of hidden all of the technical details. All of the technical details. What you need is for PN and QN, these continuous distributions, to satisfy appropriate regularity conditions. It will also be related to the method of discretization. So I've written with respect to phi, which is the transformation that you might apply to your distribution first. So, right, so the take-home message is that these rates end up being optimal. And maybe just one last thing is. And maybe just one last thing is that you might ask: well, I said a whole bunch of regularity conditions. So when will this actually hold? Well, you can, for the case when you have distributions which are finitely supported on some interval, then it's sufficient for Pn and Qn to be uniformly bounded away from zero and infinity, and also the same for the derivatives. In the case when the distribution is supported on all of R, then Is supported on all of R, then we have some examples showing what sorts of transformations and discretizations you should do so that this method works for, let's say, Gaussian families and Laplacian families and more general families. But of course, the details are going to be more technical. Okay, so just to wrap up the talk, what we were able to do was to look at the error rate for misclassification for more general stochastic block models, both labeled and with general continuous weight. And with general continuous weights. And what we saw was that the rates are indeed characterized by the Randy divergence. And just as a quick recap of the algorithm for optimal recovery, in the continuous case, we reduce to the discrete case. And what we do is we look for this particular color L star, which gives us a good initial estimate. And then we go through a method. And then we go through a method of sort of locally refining the assignments of each node in order to come up with an optimal rate. So that's it. Thanks a lot for your attention, and I'm happy to take questions. Thank you.