Yes, thanks for invitation and organizing great workshop. So you see the title of my talk, I'm interested in data integration, which is a great segue from Katya's talk, I think. So I do want to acknowledge the work, this is joint work with lots of Work, this is joint work with lots of great people. John Fincy and Ming Wei Tang are my former PhD students who finished their PhD when I was still at the University of Washington. Now John is at NIH and Ming Wei is at Facebook and also collaborator John Wakefield, Trevor Bedford and Guidas Duras on the couple of papers about this work. So I will start with the motivating example that I'm sure. That I'm sure most of you are familiar with. It's Ebola in West Africa. And I want to just illustrate what ideally we would like to do with this data, I think, is to look at the surveillance data, kind of the traditional surveillance data, confirmed for all cases. Ideally, as we learned during this pandemic, we also would like to know not just the positive tests, but negative tests that have been performed. But that's Been performed, but that's at least for me, it's a more recent discovery how important this is to take into this into account. But normally, you can download cases and you can pretend that it's not important to know how many people have been tested and how that varies across time. And then you have also have genetic data, and from that, we can reconstruct a genealogy. And this is just a genealogy, a big kind of genealogy that Gittis Dudas with co-authors reconstructed. Gitas Dudas with co-authors reconstructed in this very nice nature paper that they published about kind of phylogeography and phylodynamics about Ebola outbreak in West Africa. So ideally, I think we would like to analyze this data jointly and maybe add more data, hospitalization data, critical care, usage, mortality, just throw in all of that. And I would argue that's probably very important because we know that these models are hardly easy to estimate that we're trying to fit. Hardly easy to estimate that we're trying to fit to this data right now. So, again, this is just a background slide to set notation. You by this time certainly familiar with all of this. There are multiple ways to view these SIR type models. I prefer the graph on the bottom, which is a proper transition graph of a Markov chain, rather than a strange S goes to I goes to R diagram that I never understand actually. This one I can understand. There's a state space. This one I can understand. There's a state space that we consist of triplets of counts s, i, and r. And you can increase i by one by simultaneously decreasing s by one, that's an infection, or you can have a removal event which increases r by one and decreases i by one. So that that's a kind of a formal mathematical description of a model. You can view it at individual level, and it's if you want to do sort of like, you know, some something that people would probably call. Something that people would probably call agent-based modeling, and you can actually think about individual people being infected. But more often, we collapse these individual-based views into counts, and it's kind of population-based view, a collapsed mark of chain that actually is depicted on top. And so, on the bottom, I'm showing you two representations, right? One is the differential equation representation on the left. On the right is the three realization of a stochastic model with the same parameters. Model with the same parameters, the same population size. And you can see that even, you know, in the population of 10,000, the dynamics is not purely deterministic. It actually is not purely deterministic, even if you increase it to 100,000. And it's actually quite kind of a question for statistical inference when it actually becomes deterministic to the point where you can ignore completely all this psychosticity. Aaron King, who I believe is in the audience right now, at least he was a couple of minutes ago, has a paper about Ebola outbreak called Ebola. About Ebola outbreak called avoidable mistakes, something. And one of the avoidable mistakes is fitting ODE models to cumulative counts, which is he demonstrated quite nicely. It's a no-no. So I'm interested in stochastic models. Deterministic models are fine as well. In fact, when I was asked to help the local healthcare agency with the COVID modeling, I also used deterministic models just because I panicked that I wouldn't be able to. That I wouldn't be able to do everything in time to fit a stochastic model. But I would like ideally to be able to easily fit stochastic models to noisy surveillance data, which includes genetics data. And so when you start doing that, you will quickly find yourself in the space of hidden markup models where X is your latent state, which is something you never observe, is your number of susceptibles, infectious, removed individuals, exposed, hospitalized if you have a company. Hospitalized if you have complicated compartmental models and so on. And then if you're dealing just with case counts, for example, with incidence, you can formulate X so that actually tracks the cumulative incidence rather than prevalence. So you don't mistake prevalence for incidence. That tricks for that. It's possible. And then what you actually observe is a noisy realization of a latent incidence, right? So every week you have some actual number of cases that have appeared in the population. What we observe is typically Population, what we observe is typically just a fraction of people who maybe were symptomatic and decided to get tested, or and so on. And so, what we're interested is in this posterior distribution of all these parameters that govern these models, are not removal rate and so on, giving this, for example, this vector of counts in the simplest possible case. And if you look at this, the likelihood is intractable because it's a likelihood of hidden. Intractable because it's a likelihood of hidden markup model where the number of states is so large that you cannot do your usual matrix algebra and you know Blown-Walsh algorithm and all of that. And so you have to do something else. So and something else usually means something like we've seen particle filters. That's probably the most versatile approach at the moment if you want to fit these models to data. And it's certainly being used in Data and it's certainly being used in the case in the MLE framework or in the Bayesian framework. What we're proposing, and actually, it's not really, we are extending the work of Paul Fernhat, who with colleagues proposed to use a linear noise approximation, which is a really very old technique from the 70s about how to approximate these population processes with locally Gaussian processes. So, what this, I'm not gonna dive into. What this, I'm not going to dive into this old beautiful theory, but it basically replaces this intractable transition densities that would cannot or probabilities if you work with discrete space or densities if you want to work with continuous representation, kind of stochastic differential representation of the SIR type models. It replaces them with locally with the Gaussian, multivariate Gaussian density. And so, which is really nice because now it becomes suddenly. Now it becomes suddenly really a Gaussian latent state space model. It's complicated, it's not really a non-linear state space model. So the Kalmel filter and things like that do not really work the same way you expect them to work here. But it's certainly possible to adopt modern MCMC. You can unleash Hamiltonian Monte Carlo or elliptical slide sampling that we did use here or a lot of other techniques because suddenly now you are in attractable Now you are in a tractable situation where you complete the likelihood, if you augment your likelihood with its unobserved latent trajectories, now decomposes into this product of tractable Gaussian transition densities and some maybe non-Gaussian likelihood. So this will be, maybe if you work with case counts, maybe this will be your negative binomial product of negative binomial probability mass functions in coalescent, structured coalescent. This will be some sort of structured coalescent likelihood. Of structured coalescent likelihood, and as we will see in a couple of minutes, and so on. But that's nice because that's you know, these Gaussian, latent Gaussian models with non-Gaussian likelihoods, it's a bread and butter of computational statistics. It's lots of techniques are available to tackle that problem. So, how I spoke only about the incidence data so far or case counts. Case counts more generally. Adding genealogical data, again, there are lots of choices, and Katia talked about some of them, and one of them is structured coalescent. You can use Ber-Dev processes. Here, the approach we're taking really is kind of agnostic to what route you take, but here we're going to use the Eric Boltz's structured colestin approximation approach. There are other structured coleson approximations that are also available, or bird that process is available. Available or bird death process available. And under this framework, we can write down the density of a genealogy. And we assume the genealogy is sampled, you know, is sampled under the sort of heterochronous assumption that serial sampling of sequences. This is kind of the more very general density of the coalescent, structured coalescent, where lambda is your coalescent intensity. And that coalescent intensity under simple Kingsman coalescent. Density under simple Kingsland Kaalessen, this is just one, you know, proportional to, or inversely proportional to effective population size. And under Eric Wolf's structured colourless approximation, it's proportional to something else, which depends on SI in infection rate data. So again, those are latent properties. If S is not changing much, as you can see, it basically kind of collapses to the Kinsman coalescence because S is a constant. And so in this case, your prevalence is basically. Your prevalence is basically proportional to effective population size, which is probably for Ebola. We're not going to take this assumption, but you will see that for Ebola, it's probably not a bad assumption. So you could fit Kingdom's Mancarles and the interpretation would be more difficult, but it's effectively Kingdom's Mancarles and for Ebola application. So, what people have done before, and most of the authors of these papers are in the audience, so at least some of the core. Papers and the audience, or at least some of the co-authors of the space of the papers. So you can take the OD base structured Carlos approach, and that's what Katya did in the previous talk. And it's a very nice series of papers starting from 2009. The most recent kind of methodological one is Volta and Severoni, which is the BS2 module description of how to do for these very general, with a very general language of how to specify these models. The Bayesian inference we have particle MCM. The Bayesian inference we have particle MCMC actually started with the paper by David Just Mussen, Kachakeli, and Ollie Radman back in 2011. And actually, that's actually what we're trying to do in a sense. We're trying to sort of do this paper, but without particle MCMC, you can think about this way. Like, is it possible to do this without particle MCMC? And that's what we are trying to do. And these maximum likelihood iterated filtering, the approach that Aaron King at Ionidis and co-authors have championed as well. Co-authors have championed as well. So, what we're doing, we are doing, we're turning the Bayesian crank the same way you can turn the MLE crank. The Bayesian crank here is we're trying to estimate posterior density of all model parameters and unobserved latent trajectories, like number of susceptible individuals at each time point, number of infectious individuals at each time point, or whatever your compartmental model specifies, how it specifies compartments. How it specifies compartments. And it's proportional to its product of two likelihoods. There will be one incidence likelihood, for example, if you're working with incidence data. And this probably will be negative binomial, you know, product of negative binomials, because that's kind of standard to account for over dispersion in case counts using negative binomial distribution. This will be structured coalescent the way I showed before. It's tractable under the approximation, a quality approximation or some other approximations. This is epidemic model, and it's here. Is epidemic model, and it's here it's important that's what allows us to use to avoid using particle filters. This is epidemic model trajectory density, which is just a simple product of multivariate Gaussian densities that are derived from this LNA approximation. So that actually what makes the whole thing work, because ordinarily this is even this is intractable. Even if you don't want to sum over x, you still don't know what those transition densities or probabilities are for large populations for sure and complicated compartmental models. Sure, and complicate compartmental models. But here we do, and some priors. And so we turned Bayesian crank, and we're trying to infer all these parameters and latent trajectories jointly from sequence data, from genealogy data, even though genealogy, of course, not data. And we really want to integrate over genealogies as well, but we're not there yet at all. Even this is not kind of working the way I want it to work, to be honest. So we are usually trying these experiments with assuming incorrectly the genealogy is fixed and known. The genealogy is fixed and known, and then trying to relax it if it works. So, let me just illustrate the example that I started with. I'm going to use only one country. Why I'm going to use one country? Because we cannot do three countries. I would really like to do three countries jointly and account for importation, exportation events, and jointly analyze the cases and the sequences from all three countries. I cannot do it right now. It's too hard. So, we're going to. Too hard. So, we're going to do one country, it has about 1,000 sequences from Sierra Leone. Um, we're going to take one beast tree, estimate it somehow from beast, you know, I mean, we know how, but supposedly it's not a better presentation of a genealogy, again, incorrectly, assuming now this is data. We're going to take the case counts as well, the incidence data from WHO. We're going to fit an inhomogeneous SIR where we will try to infer Where we will try to infer it's very simple SIR, except we are trying to estimate non-parametrically basic reproductive changes in basic reproductive number. So we can assume some smoothing, Gaussian marker random field smoothing or autoregressive smoothing and estimate the changes in R0 on a grid without really specifying the functional form of how R0 changes or change points or anything like that. And we're going to try to do it from both genealogies and the observed sequence data. Observed sequence data. Any questions so far? I'm kind of going a little quicker than I usually do, but Alexander, do you am I doing on time okay? I didn't know I didn't notice when we started. Yeah, you still have a couple minutes. Okay, yeah. Okay, so let's look what happens in terms of results. So that's my last slide. So this is our estimation of this time-varying R0. And so it's a busy kind of slide there, three colors. And so it's a busy kind of slide there, three colors, because I want to really show you what happens if you use just genealogy, so just structured coalescence, just incidence. So just genealogy is in green, just incidence is in blue, and if you use joint modeling, which is red. So one obvious thing that you notice, and that was noticed by, of course, David Res-Mussen and Katya Kohli, and Ollie Ratman in the 2011 paper. You have sharper inference. 11 paper, you have sharper inference. Of course, if you had more data, you should have tighter credible intervals. And that's what these credible bands illustrate. Those are pointwise, really credible intervals, but we're connecting them so they appear as bands. And so you have a tighter estimation of the changes in the basic reproductive number, which starts at above 1.5 and drops below 1 eventually. The uncertainty is much larger if you just use structured coalescence, and it's still larger still if you just use the incidence. Incidents. As you can see, I would ignore the bumps in the density. We have all sorts of problems with MCMC convergence, its preliminary results. We should run it longer to avoid all these kinks. But we do believe that the signal is real is in effect in sense that if you just use genealogy, you're basically recovering your prior in terms of the recovery rates or mean infectious period. So it appears, at least anecdotally, that just from genealogy, Anecdotally, that just from genealogy and structured coalescence, it's not possible really to identify the infection rate. Sorry, not infection rate, the recovery rate. Or at least it's very sensitive to the prior, that's for sure. The incidence data does not agree with the prior and pushes it away from the prior, and the red is somewhere in between, which is the joint inference. I also want to point out the reporting rate. The opposite is actually true for the Rate. The opposite is actually true for the reporting rate in some set. Not the opposite, but reporting probability, reporting of probability of seeing a case. That parameter is present only in the incidence model. That's in the emission distribution of the incidence model. And typically, you actually have a hard time recovering it because you can either you can play with the parameters and so reporting increases, you can increase latent prevalence and decrease latent and decrease the reporting rate. Only kind of the product of latent prevalence. Of latent prevalence and reporting probability are really identifiable. So, typically, you do have a hard time, even though here it doesn't recover the prior, the posterior of the just incidence data, but it's quite influenced by the prior. And so the joint inference actually makes the picture a little bit clearer, and it does show that the reporting probability is not as high as incidence data would suggest, or at least that's what genealogy is pointing to. Pointing to. And this is just kind of a posterior predictive sort of diagnostics where the joint and incidence-only models kind of agree in terms of producing posterior predictive distributions of the data that we observe. So I think this is promising to me. What's the most exciting about this is not the computational aspects of it, probably, although that's what made it possible, but actually the thinking about how these combining the Is combining the multiple data streams, how it really resolves all these identifiability problems that we typically have with these models. When we publish papers and we give talks, I at least we're not always very upfront about how really ill-posed these problems are. They are horrible. They are not easy to fit. If you want to fit like a 20-compartment model to incidence data, I really want to see the prior versus. I really want to see the prior versus posterior plots everywhere for every single parameter because I don't believe all of them will have prior and posterior kind of disagreeing with each other, showing that you have gained some information from the data. And so these data integration approaches, I think, are really promising. I think we should be doing more of that, but it's kind of hard. I will finish with references, acknowledging the very nice work on which we're building, you know, starting from Eric Woltz and co-author's paper in 2009. Eric Woltz and co-authors paper in 2009, and then this paper, PLOS Combined 2011 paper that I mentioned multiple times, Paul Ferghan's LNA, introduction of LNA into inference for infectious disease models. And the work that I talked about actually is not published, but if you put together this last two papers, so the first paper by John Fincy is the first author is about LNA applied to noisy incidence data. And Mingwei worked on genealogies, and then Mingwei actually extended it to the joint model. To the joint model, but we haven't published that work yet. But if you put the last two papers together, that's kind of the talk that I gave today. Thank you. So we started one minute late, so I'll sneak in a quick question from David. I really like the idea of LNA, but I'm wondering what's the worst that can happen if the density of the latent variables is strongly non-Gaussian and possibly multimodal. For example, epidemic without R not close to one, that doesn't take Flows to one that doesn't take off most of the time, but sometimes explodes. Yeah, it's a great question. So, LNA is not without faults, of course. It is an approximation. Moreover, it's an asymptotic approximation when population size is large enough. So, actually, we are responding to reviewers right now with this John Fincy paper where we try to break LNA because reviewers ask us to break it. Like, when it breaks, when the time interval becomes too large or population. Becomes too large, or populations too small, and to be honest, we have a hard time breaking it so far. It does, I mean, I'm sure it breaks eventually, but we applied it to three different regimes, kind of Ebola, flu, and SARS-CoV-2 regimes, hoping that one of them will break it. That didn't quite happen. But it does have a hard time, for example, when we have one of the compartments becomes too low. So, presumably, that will be important. So, if you Presumably, that will be important. So, if you have very few, very low prevalence, for example, eventually we'll break it, I believe. And we can see it, but only if you include the tail of the epidemic, for example, the very tail, if you start including really low prevalence part of the epidemic, we can break it. But it's surprisingly robust. I was actually thinking it would break faster, but it's not breaking as fast as I even would like it because for demonstration purposes, I kind of wanted to break it faster. Thanks for the talk. We should move to the next speaker and