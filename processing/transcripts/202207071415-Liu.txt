Hi, everyone. Thanks so much for the kind of wonderful workshop. So, yeah, I'll be talking about sort of mixing times, so new techniques for establishing mixing times for flower diets. And I guess this will be notes they'll not have like editing strains or anything like that, but they'll have sort of the non-commercialization distribution. And I guess as a computer scientist, everything I will do is at discrete time scale. Do is it the screen time score? Hopefully, something like this goes to someone here. And this is based off of joint work. So, this is kind of a survey plan in some sense. This is based on joint work with several people, based on jointers. I'm not going to identify which one is which. Okay, so for simplicity, we're going to focus on a site distribution, you know, not necessarily, not necessarily a product distribution, it's going to be over the hypercube. And maybe to fix, to keep things concrete, let's fix one. To keep things concrete, let's fix one particular distribution that I'll sort of use as a running example for the talk. I mean, you can think of Eisenhower model or anything like that if you'd like, but I'm going to focus on sort of this model called the hardcore gas model. So here, the input is a graph G, and I'm not going to restrict myself to lattices like C D or anything, but you could keep C D align if you'd like. So I have some finite graph G. I'll have some parameter lambda that you can kind of think of as like a parameter standard. That you can kind of think of as like a temperature parameter, and I'll have the following of a corresponding heat distribution. Let me just call it mu g of lambda. And it's going to mu g of lambda for a configuration of that sigma. It's going to be proportional to lambda. Of lambda to let's just say the number of vertices which are occupied. So I can think of: so I'm going to assume one means that vertices occupied by prior to all zero stuff. And this is, and this is for configurations satisfying the following constraints. So satisfying for all pairs of neighbors. All pairs of neighboring vertices. I want it to be the case that if one of them is occupied, this will force the other one to not be occupied. So another way to think about this, I guess, in maybe more computer science-y language, I would say, you know, this is a subset of vertices such that no pair of vertices in that subset are connected by an edge. That's another way to think about it. So, connotoriums are independent sets of the vertical. So, this is the gauge distribution. And sort of, we'll be interested in sampling of this distribution. We're going to apply dynamics to sample. So, this distribution is interesting in that it exhibits phase transition behavior. So, for instance, so first I'll be considering sort of the collection of all delta found in the regress. So, there was this work from Kelly Python, which sort of appeared this critical point just by looking at sort of the internet delta regulatory. And this is where I This is roughly sort of the phase transition for this distribution on a collection of all delta value terms. And then one interesting aspect of this sort of work. So this is a critical value of this parameter lambda, sort of where the phase transition produced. And I'll explain a little bit more what I mean by phase transition. But sort of my interest as on the computer sciences side of things is that sort of this Is that sort of this critical threshold corresponds to also phase transition and complexity? Okay, so we've shown sort of a sequence of works. So first by Weiss, I drew Weiss in 2006 that if your parameter lambda is below the square threshold, then there exists a sort of efficient say approximation algorithms for the partition functions. So it's each of lambda, which is the sum for all these configurations of lambda to the of that vertics. And this game can then turn into sort of sampling algorithms. But notably, this algorithm is not based on more conditions. Make sure that at least when there's blood script or threshold, things are capable. And then sort of on the other side of things, slides as a head, some folders by slide summon. Well, or Says that basically, if the lambda is above this critical threshold, then no such option exists unless sort of some standard complexity theoretic hypothesis like diagnosis hypergoing? You mean something that works for all graphs or all boundary graphs? Okay, all the graphs. Bounded graphs. Okay. All double-bounding graphs. Okay, so yeah. I mean, you can. Of course, you ask the question for, like, say, ZD or some special classes of the graphs, and there, I mean, you might have slightly, you might have a different might have a different critical point. If you want something universal, then you need lambda which is smaller. So you want if you want an algorithm which is universal, then you need lambda to be smaller than double C. Yes. As I sort of mentioned earlier, is that sort of this efficient approximation algorithm is not based on Markov transients, a rather complicated deterministic algorithm that runs as a rather poor running time. So you scale this sort of roughly n to the log of the maximum degree over little delta, sort of how far you are below this critical threshold. So that's a pretty, you know, it's very efficient in a very Efficient in a very loose sense, this insensitive polymer. Okay, so in this talk, I'll be interested in sort of getting analyzing mixing time of the labor dynamics for this distribution, and we're interested in getting sort of tight mixing time. So, not quite as tight as sort of getting cutoff phenomena like we saw in the previous talk, but sort of mixing time is sort of correct up to the constant factor. So, yeah, so any questions? So, any questions? Yeah, feel free to stop me at any point to scale it up. So, we're interested in blobber dynamics. So, just call blob dynamics. It's each step, you pick a random vertex. So, I'm working discrete time on each step, pick a random vertex, and then I resample the assignment of that vertex and then everybody's after So this is due to building on very, very, very suboptimal which says that sort of Which says that sort of let's say potential with some capital delta and let's say we discovered night quality mixes and order of n lens. So n is the number of vertices. So, n is the number of vertices, and the constant depends on this little delta and the maximum. Okay, so a few things about this. So, first, at the gamma again, what we do is we sort of establish the lower bound, the blocks of that constant. And this sort of then followed by name before. Yeah, you say n log n this is discrete time and yesy. So instead of I just pick one random word to pass there's a follow-up set to what's the conditional delta please? Oh, it's upper bounded by a constant independent of the size of the graph, independent of n. Okay. So just some constant like five percent. Sorry, and small delta lambda is more than one minus delta. One minus little delta. Yeah, this critical threshold dependent. A little delta is just some, this is like for all just a little gap, just how far you're out. Okay, so it wasn't enough to just model? I think, I mean, always put something there, but I mean, But I mean it does depend on something. And also we don't right now we don't know what happens if lambda is equal to this. So I was going to say that so there will follow up works by many people. So they remove this boundary assumption. Remove this boundary assumption. So it's using sort of more sophisticated versions of our techniques, you can get this. So there have been improvements, right? So for any delta, for any delta, for any maximum, for any maximum delta, I mean as long as you're the land is below the corresponding critical threshold, but yes, for any maximum degree. I don't understand because lambda critical depends on so basically so here I have to so so here I have to assume lambda is bounded or sorry big delta is bounded but here they say for every force of the maximum degree upper bound for every L delta greater than zero L delta greater than zero, and for every lambda that is less than one sign. Could that help to quantify this one? So for every choice of the maximum degree, for every choice of the gap, and for every choice of lambda, which is below the corresponding. So that so the degree could depend on n. Yes, this could indicate. Whereas here we didn't have louder. I mean, this implicitly depends because there's the okay. So there is a delta back. Yeah, there's no there's no delta. There's just little delta, which is the gap how far. Okay, so and maybe which okay so this is my um and then this. And then maybe this will be also interest to some of the participants here. We also, I'll just mention one more result, which is for random cluster models, which may be interesting to the participants here. And another follow-up work by basically the same authors here, plus so if I consider graph twice for all graphs g for all parameters. For all parameters one, and then for all choices of just k if I look at say the random cluster measure this defined as I have a subset of measures that is proportional to cube the number of kinetic components in the The number of connected components into a subgraph. I just take one. Let me look at that as F. This is successful edges of say okay, number of edges. Then the single, there's this dynamic, single swap marketplace mixes and So, in particular, this gives you a way to, for instance, compute a partition function for the random cluster model whenever q is between zero and one. So, in general, it can be anything, any positive real number, okay? But sort of between zero and one, I think it was previously seemed rather intractable, but turns out using these techniques, we can get, for instance, samplers and can compute partition functions for this one, at least between. The small, at least between zero. So, most of the results for Renault Buster are for large and large. So, these are just sample results. Using these techniques, we can also study, for instance, Ising models, and so on, but I'm not quite yet where it doesn't happen. So, just for Q smaller than one was where the KG break breaks. Okay, so now so this is just a sample results. Now let me tell you about like what is this technique of spectral dependence and how it applies some of these results. So some of the first defines actual events, and these are Spectral events, and these are these will be sort of quantities that are not too unfamiliar to you. So, the planets are kind of a matrix whose entries are indexed by the vertices graph. So, it's given by the following function. So, you should, I guess I'm going to call this an influence. This way, this has to be like a thing about it. So, formally, it's given by, so if I say, The formally is given by. So, if I sample a configuration for my distribution mean, look at the conditional probability that the vertex V is occupied, given that I know that vertex U is occupied. And then minus same thing, except I now condition UB. And call this an influence because you're sort of seeing how much this, you know, fixing the assignment for u affect the assignment, the probability for p. n by n matrix in general has positive or negative entries, in general asymmetric. But it'll turn out SBLIG values. So we'll say mu is U is actual independent if the largest eigenvalue by one plus it the one is different distracted reasons just because on a diagonal it's all lost why this name so This name, so why eta spectrally independent? So, spectral just because you're looking at eigenvalues, and the independent because I guess you're these are like sort of correlations, right? And so, if you want this, you want this thing to be close to zero, then you're saying that sort of the vertices, their assignments are roughly independent of each other, right? The correlation is controlled. With dependent screens, we would have eta equals zero, right? If you're fully independent, it says zero. Because I'm you're kind of quantifying how close you are to the back because I'm there and I don't have to use so now let me let me say how this thing implies, you know. You know, what the exact connection is to this thing not very good at managing word space. So we're going to do results, one for sort of generic distributions, one for sort of distributions like Parker model. So the first result sort of gives So the first result sort of gives a very loose spectral gap of the Global Database, assuming spectral gaps. That assume our distribution new, as well as all of its conditional distributions, and all All condition. What I mean by conditional distributions means I take my distribution and condition on the event that my configuration is consistent with some assignment to some of the vertices. For some subset of vertices, I can append them to some occupied or unoccupied and add resulting conditional distribution. So for all conditional distributions, any cell. So assume they are all spectrally. Then the gap, the simple gap of the biodynamics. So, in particular, if I have a spectral independence primary data, which is upper bound by a constant independent of the side of the graph, independent. Independent of the side of the graph, independent of the number of vertices, and this inverse plot on the spectrum. So, when you say all conditionals, you mean all the conditional distributions? Exactly. Yeah, sorry. So, this will give you polynomial spectral inverse polynomial spectral gap, but it's like not, I mean, it doesn't give you tight makes the hydrogen. So, we have a secondary term for the spectral case of sort of. For the special case of sort of about the degree, I guess I'll call it graphical models. You can just think hardware model rise and kind of just kind of gives the distribution of some consistent hardware. So this is the same thing. Assume the same thing from here. Mu and all of its conditionals are special independent. We further assume that relative alphabet and graphical models, undirected graphical model or market feels better term, you can just think gives And the locks over the constant or cloud dynamics at least constantly depends on spectral dependence and the maximum graph. So, if I get a spectral dependence where it is bounded by a constant, and I have a bounded degree graph that. Have a bounded debug graph, then this is sort of optimal luxury that you can get. So, maybe what I'll try to do, so my plan for the talk is sort of myself is sort of, I'll try to sketch the proof of this theorem and I'll sort of weight my hands even more to kind of sketch in a more vague manner how you would. More figure matter, how you would improve the proof technique here to get this theorem. Then I'll sort of spend the last maybe five minutes to say a few words of how you might some techniques to establish this property. So It means that the constant in front depends on eta, but if you don't have a an exponent which depends on equal and sort of depends on what the exact dependence is.  Sorry, yeah, so I didn't define it formally. Just keep in mind, say, the Gibbs distribution of, say, Eisen model or Harkour model or some instructions. Roughly speaking, there's some underlying graph and sort of the random assignments of the PC to satisfy. Assignments of the vertices to satisfy some nice conditional independence relation, right? Namely, that if I kick, say if I have two vertices to a v and condition on the values of the random variables in a separator, then these assignments of these two vertices are one of these independent code or conditionalize the separator. That's all very good. And I'm sorry, I did the backbone. So, first I can then sort of it suffices to prove the fault. So, I guess that so maybe let's say, I'm gonna write the key for the transition matrix our dynamics. Lower dynamics and I write p sub v, s outside components flower a conditional distribution. Distribution specifically by condition mu on the event that my configuration maps B to the sample x. So for all versus b in the graph, well In the graph of choice of the occupied graph, so here's the block opposition. So at the following point, so that's the gap right then at least slide normalization here as one minus the spectral independence per hour by distribution. Times the min of the conditional distribution. So I take the min for all vertices assignments of the gap of library dynamics for the conditional distribution. If I can prove this, then I can prove at least the first theorem, right? Because I can sort of then apply the theorem to each of these conditional global dynamics and. And we're just doing an dominant argument. Is that okay? So, really, it suffices. This is probably the heart of the argument that you want. And so, there are many ways to prove these things, but this is the one I'm presenting sort of at least the way I personally find the most. And it's a little bit more than your algebraic rather than probabilistic. Okay, so the first thing is. Okay, so the first thing is: I claim that you can write down the power dynamics, you can decompose it as kind of average. Some of our vertices, some of our spin signs. So, what does this mean? I mean, first off, a target is doesn't make any sense because these matrices live in different dimensions, but you can only. Different dimensions, but you can always trivially have these conditional buffer dimensions with zeros for configurations that are just not within the domain of the conditional distribution. So this makes sense. And the one over answer is sort of for normalization reasons. Basically, the key point is that sort of if I have one particular transition, then after I've chosen that I want to update the coordinate by, the update probability or the acceptance probability is the same. Probability or the acceptance probability is the same between all these products, and then so one over them just sort of to normalize things correctly. If I got this, then I can sort of do the same thing where I put sort of the associated Laplace. So now write sort of the identity matrix again, sort of restricted appropriately. Now I want to use sort of lower bounds and spectral gap for this conditional lower device. In an appropriate sense, at least. And now I have basically the gap. Of increase of logodynamics times where instead I subtract off now the stationary distribution, basically the top I guess. You can think of this as just being this is the all ones vector for the condition of blobber dynamics as a top eigenvector. And the corresponding left eigenvector is. And the corresponding left eigenvector is the stationary distribution. And all this is saying that all of the non-zero eigenvalues of this Laplacian are at least the central gap. That's all. There's nothing wrong. And once you have this, I can take the name of all these spectrographs. So yeah, at least. So let's take the name. Let me know if you can't see the back. Now I can just push the sum in here. The sum over these guys can be the identity minus basically an average over these rank one matrices. So this would be minus sine. I can write it. Now, let me sort of show you what this thing is, right? So, this is an average of Of basically of transition matrices of some kind of trivial markup chain where you sample one step from the conditional distribution. So, let me just show you sort of diagrammatically what this thing is. This is sort of how I think about these things. So, what it is, is so imagine I have, so up here I'll have this case of all configurations. The space of all configurations here are space of all partial configurations. What I mean by that is I have assigned data assignments to n minus one big vertices. I have left one vertex unassigned. Part of configuration on n minus one vertices. And at the bottom, you'll have sort of all. You'll have sort of all vertexes out of those partial configurations on a single vertex. Down here I'll have say press. Up here I'll have a configuration sigma. Now what is this doing? What I think about this is: so if I sort of isolate So, if I sort of isolate one of these transition matrices, I start from, say, a vertex assignment pair, and I'm just sampling in one step a configuration which is from the condition distribution. And so it's such that sigma Okay, repeat please tell you, so let's first focus on just one of these matrices. So what this is saying is that this is the marketing which sort of in one step samples directly from the conditional distribution. So I want me to visualize that because I'm sort of starting with some specific vertex assignment error and in one step I'm just taking step two. Now we're going Now, when you think of what this average is doing, what I'm doing is the following. I start from a full configuration up here. I will go to a unified variant where it is consistent with this configuration, and I'm resampling another bigger and sigma satisfying sigma. So, in other words, this So in other words, this average average of Markov operators is exactly sort of the what maybe some people call the n minus one uniform block dynamics, where in each step, I pick a uniformly random subset of vertices, n minus one main, and then I resample all n minus one main data. So this markup chain really is not implementable, right? You can't implement any step of this markup chain, and you expect it to mix very, very fast. But so the upshot here is that sort of But so the gap shot here says sort of what this says is that the gaps at least the min gap and the gap this dynamics where I resample n minus one right here is the same stuff. All right, let's take a look. You can kind of think of this almost as like maybe even a comparison result between cloud dynamics and something that makes much more global moves. And now all that remains is to see that relates this gap to the spectral independence of the distribution. So basically, the claim is that sort of this thing exactly n minus one over n times this thing. And let me explain briefly how that while using this. So basically, the rough criteria is the following. So we're looking at spectral gaps, right? So okay, I can think of. Spectral gaps, right? So, okay, I can think of this as being on this sort of biofarter space where I'm starting from a point up here, coming up here, and then going up. But I do the same, I do the following, sort of flip this out to set. I can start from a vertex and the pair here, go right here, and then come back here. And these things, they'll have the same spectral gap, right? I mean, just from the fact that sort of the just from the fact that sort of sort of the standard any algebraic fact that matrix the matrices a b and b a have the same eigenvalues let me now define this this will just be the markup chain on just the vertex assignment pairs which which comes exactly why is that which i start from the vertex assignment pairs I start from the breakfast assignment pair. I'll go up to a full configuration and I'll come back. This is just sort of the reverse operation of quite a bit. I reverse the order of operations. So it is Q. So now let's sort of compute the entries of this Q. So I claim That the transition probability must say some vertex assignment to say some other vertex signal pair is exactly one over n times the desired conditional of I sample a random configuration. I condition on the event that that configuration maps V to S. And I look at the event at the probability that that configuration also maps vertex U to S prime. And the reason for that is just, you know, what is the probability that I jump from a vertex, a vertex element pair BX to another vertex element pair U as prime? I have to first sample from the conditional distribution of condition on BS a configuration that also is consistent with US prime. So that's exactly from the conditional distribution. And I have the one over just in the fact that I pick a uniformly random vertex of summer parameters consistent with added configuration. Does that make sense? So now you can kind of see that. So, you know, vaguely, you see that the entries of this random block here, which is a very local random walk, it's only on vertex, it's not in Paris, has sort of, no, at least entries are very reminiscent of the entries of the influence matrix. So what you can prove is that sort of the secret, right? So you can prove. See that right so you can prove it sort of applies to appropriate production operators, which I won't get into. I sort of suspect the gap is right. I'm not going to go into details, but again, it's sort of fine. Go into the details, but we can sort of find some projection operators that will project this sort of transition matrix and recover the latest matrix. This is really the quality, not uh yeah, so it's I mean if you want you like it's really yeah, it's really the largest microphone. Actually, that's but it is a little bit more any questions about this. So, this is sort of the sum of this spectral gap. I just want to sketch briefly, wave my hands even more, just sketch how you would get sort of the lock solo path. Or at least a better, how to refine this method. And what's the sort of how this you actually take advantage of. Is you actually take advantage of the fact that your distribution is this graphical model on a boundary graph. The first thing is what I can actually generalize this fact, right? I don't have to just condition on a single vertex assignment pair. Single vertex assignment pair by condition on say k thing. So for every k you can also say that I choose a uniform random subset of k vertices okay so configuration adjust that subset of vertices of the bottom. Of the blob dynamics for the distribution where I have a condition on the configuration to be consistent with this entire part of the configuration on k mega purposes. Just in this and the same way, you can also say using the same exact same idea, it's going to say that the gap is at least sort of the min gap. times so instead of the block dynamics where I update n minus one many vertices I'll have block dynamics and minus k where do you Yeah, we're in each set, I think, a random subset of n minus n because it's out there all. So now here's sort of what we use, what you can use, you can observe, so more at least the critical observation. The first is that if I take k to be linear enough, so a large number of vertices, okay, and this will still be expected to be very fast fixing. Still, we expect to be very fascinating. And what we can hope, and what will try to be the case, is that at least for most choices of this subset of vertices S, this gap is actually very good. So a priori, right? So this gap is very good. So I'll explain why. But sort of a priori, I mean nothing else. This is just some global dynamics on some graph of, say, n minus two, n over two many vertices. And a priori, we have very little control on that. But sort of one nice thing, at least about boundary graphs, is that once you have picked a random linear size subset of vertices, you will shatter the graph with a very high property. And that's sort of the key property of boundary graphs that we use. What do you mean by shadow? Yeah, so then let me explain what I mean in this second. So this is sort of the key ingredients and then also for K, which is so Which is so you know, it's on the order of n okay, I'm not going to specify constants here. Um, for uniformly random, let's say K versus I look at so the induced sub graph on the remains. On the remains, right? So, thinking of it like I've conditioned, I've pinned the vertices in S to be occupied or unoccupied. And so, I can think of us, I just delete those vertices from the graph. And in the remaining graph, which is the induced subgraph on B minus S, this graph, let's see this way. So, all magic of the Method of maximal connective components. So I'll go maximum points of this and do subrefer with log at the first time. So a query, right? After I have a condition on, say, this random subset of vertices, I have a lot of dynamics on some distribution over vertices of a graph that has m over too many vertices. What this says is that that graph actually has a bunch of small components. All the components are small. All the components are small, right? And because of conditional independence, the resulting distribution is a product of the distributions, independent product of the distributions, over to the connected components. And so you can hope that this gap here is actually good. Because on a graph of log n vertices, the second gap is always at least exponential, and that would just follow the n. So there are. So, there are lots of issues with this at the moment, right? I mean, first off, the main issues that we want to omit here are the worst case overall choices of the preface. And this is sort of where I'll just raise my hands and not really go into details. But so there are ways to prove analogous things where what you do instead of, so the problem with this is kind of like the gap is already in itself, like the definition of the gap is you look at the worst case function, right? But what you can do is something a little bit more Something a little bit more average case, that you first fix some function on an entire space. And then you have similar kind of decompositions of this function and the entropy of this function or the variance of this function. And you push through some more analysis and there you'll find that instead of a worst case thing, you can do a little bit more averaging. So, I don't want to say more about that, but that's roughly how the kind of tools that we're meant to very briefly say some of the techniques that. Some of the techniques that are now known to establish balance on this matrix on the spectral norm of this matrix and for all this conditional distribution as well. About the four known cluster techniques. So that is for now going to maybe a sort of trickle-down course. This is sort of migrated by OpenPIM building off works of Garland. We also have some more sophisticated version of this. Basically, the rough idea here is using. Basically, the rough idea here is using, you know, this is kind of a magical effect, which truthfully I could produce a proof for you, but I'm not sure you would be very enlightening. But it roughly says that bounds on influence matrix for conditional distributions on bounds for the influence matrix of u itself. I mean, of course, there has to be some this. I mean, you have to quantify everything. I mean, you have to quantify everything, and you know, things can the balance in general degrades, but it is one way you can do this, and it's been useful in certain settings. So I'm just giving you a very high level that there are techniques here that people don't have. Let's look at this correlation here. I guess probably. I guess all the audience here would know. Basically, it just says that these influences become exponentially fast in the graph distance between the rows of the story. So you can imagine intuitively by a sort of, this says that all the rows and columns of this matrix are k x metric, so you should get a balance metric. I mean, of course, like there's some subtlety here, right? Like, there's some subtlety here, right? It could be the case that the rate of exponential k is much slower than the growth of the balls in the graph, but at least for something like that, and then I guess maybe the technical terms like spatial mixing. Yeah. This is not always that this exponential decay is only when only It's only one in the only hydrator. Yeah, yeah, yeah, yeah. In the hardcore. The third is zero prize. Before you get one about this one, you should also perhaps have that also for the conditional. So strong special mixing says that for all conditions. Okay, before that's going to be the third is sort of this. The third is sort of a small brick. It's by zero freeness of the multivariate partition function, which is why you write it as this. So I guess you're not a very correct. If your multivariate partition function status has zero free in some large region which contains sort of say the all ones point because you're looking at the distribution itself without external field um then you can also prove bounds and insertion as well and i guess the final thing which is more of just like uh intellectual curiosity but not that useful you can say that if you have a coupling yeah like say what's that attractive coupling um Or maybe like a purpose condition, like we saw in the previous talks, so you can also establish spectral references. I guess I'm missing tons of references, but yes, but this, I guess, I should probably wrap up. So this concludes open problems of it. Yes, there are a few problems that I guess one is sort of, I mean, there are lots of distributions which sort of conjecturally believe that they satisfy special, right? So that's the proper All right. I'm going to say delta boundary. With say Q at least, we believe this distribution is spectral independent and hence other dynamics of mixed random cluster. Um cluster or q between one and two other distribution I care about. Another question is sort of like these are sort of techniques that have been around for a while in sort of the approximate counting and sampling community, but it'd be great to develop new techniques that will be obvious because for a lot of these problems, these techniques are sort of now running up against barriers. I guess maybe inspired by the previous talk today. So it seems like we need new tools to analyze this bar entropy. That's basically as you want to say proof cutoff for like cloud dynamics beyond sort of say the regime in which you have one strip of contractive couple mix, but in which you can still hold for weapon mix. So maybe And this talk more, I have more of the problems, but excellent. 