To us on state-governed random turn games. Thanks so much. It's an honor to speak here and to join everyone this week in this celebration of TMO's work and to see all the vibrant connections that are going on in research at the moment around topics he's been fascinated by throughout his career. I'll be talking on a topic with a slightly different flavour, although there will be some interesting connections into things like KPZ during the course of the talk. The topic is state-governed man in terms of gains. It's joint work with Governor. Term Gains. It's joint work with Garwal Peter at the Rennie Institute. It's been on the archive since late last year. Let me explain the ideas. Well, I've got a route to explain the ideas, so let me jump in. Firstly, let me explain that we were inspired by, as many mathematicians in the topic were, by work of Perry, Schram, Sheffield, and Wilson, and sort of developed the notion of a game called Togo of War, after we explained from the mathematics literature. And then, after conducting the research, only in the last few months. The research only in the last few months, I realized that there's actually a whole parallel universe of economics contributions around a game that's actually remarkably close to the game that Garbra and I developed, inspired by the mathematical literature. So you have many citations to these economics and mathematics works, but none, those are disjoint bubbles. There was no article apparently, as far as I can see, that the site. As far as I can see, that cites both. So, retrospectively, the work I'm talking about is actually forming something of a synthesis between two vibrant but disjoint strands of research. What these strands are will become clearer during the talk, although I won't express things directly in terms of them too much, but the ideas will be there. I'll say brief things. Okay, so it's convenient to start with a very simple idea as a lead in a harmonic function on a graph. In a harmonic function on a graph. This is the kind of setting I'll work with. A finite graph will have a boundary in red B on which will be defined a function, a real valued function, and each of the red dots gets a real number. Then if we speak of the harmonic function, it minimizes a certain gradient L2 problem, and of course, the harmonic function has the property that its value at any given interior vertex is equal to the Interior vertex is equal to the average of the values of the neighboring vertices. We can speak, if you want, of a two-harmonic function, this is its power of two in the minimization problem. We have a, oh yeah, so simple random walk is probabilistically associated, of course, to harmonic functions by the mechanism that if I start at my green dot V here and run a simple random walk in which each neighbor is chosen uniformly at random until that process stops on the boundary, and then I evaluate. Boundary, and then I evaluate the boundary function, the mean value of that over the randomness of the walk as a function of the starting location is equal to a harmonic function, which is the unique harmonic extension of the boundary data to the graph. And for emphasis, I could call that the two walk in association to two harmonic functions. So I can then have a notion of p-harmonic function. I think p Laplacians were mentioned during the conference. And then it's interesting to ask what a p-walk is. Ask what a p walk is. My purpose is served by focusing straight away on the case where p is infinity. What is an infinity harmonic function? Time presses a little bit, so let me express things just purely by the analogy with the average. We take a different average in defining an infinity harmonic function. Rather than a very uniform choice of neighbor, we take, in some sense, the opposite extreme, and we average the maximum and minimum values of the function on the neighboring sites. function on the neighbouring sites. So that's a well-defined, it's not very hard to see that in this context that is a unique object extending boundary data. There's a unique one extending the boundary data. That goes into the question, what then is the infinity war whose role is analogous to simple random warp's relationship to two harmonic functions, ordinary harmonic functions? This is where game theory comes in. So both on the slide and over there, we have the kind of context, simple discrete context. Context, a simple discrete context that was subsumed and treated by Perry Sham, Sheffield, and Wilson. A game will be played starting at the green dot on which a counter will initially be placed. The rules are simple. At each turn, a fair coin is flipped, and my players, Mina and Maxine, will move the counter heads if it's heads. Maxine will move, and if it's tails, Mina will. And the game will end when the counter reaches the boundary, the red boundary, at which moment. Boundary, the red boundary, at which moment MENA will pay Maxine the evaluation of the boundary data at the terminal counter location. So naturally, MENA is attempting to guide the counter towards locations in which the label red vertices carry low or negative values, maxi into locations at which those values are highly positive. Okay. Yeah. So some measure measure theoretic, game theoretic generalities. A generalities strategy for a player is a complete specification of how they're going to play in any given situation in the game. Strategy spaces for the two players, you know, where am I going to play at the counters here? It's fairly simple to specify the spaces in this case. And then if I have a given strategy pair, I can consider the mean payoff function as a function of the strategy pair. And the implicit in this notation is the counter starting location. That's the mean value of the terminal payment that's made. The mean value of the terminal payment is made if the two players adopt those strategies and you watch the game play evolve. That's a random process. Where does it finish? Take the mean, that's this positive. Okay? Yeah. So game value, another game theoretic generality. Imagine, okay, to specify this, imagine that each player is respected at respective time is put in the perhaps disadvantageous position of being forced to declare their entire strategy what they do in every conceivable situation. They do in every conceivable situation to their opponent. If Mina is forced to do that, V minus, and she behaves correctly in that situation and does the best she can in terms of mean payoff. It takes a moment to decode this time presses slightly, so I won't explain it. In Freeman, C. Freeman, in fact, you can already see it anyway. This is the correct formula for what best that MENA can do in the situation. If the roles are reversed, then Maxine. And maxine is forced to declare strategy, I have E plus. It's very intuitive that it's a disadvantage to be forced to declare. So we have this, correspondingly, we have this trivial inequality. And if the two quantities are equal, then the gain value exists, this is the definition, and the gain value is equal to the common value. So this is somehow often how game theory gets going. It's how it did historically and in a sense also for us. In a sense, also for us, very generally. Okay, so what is the relationship then? What is the infinity walk in the relationship? Well, this is a theorem that's rarely implied by the work of Perry, Schram, Sheffield and Wilson. So on a finite graph context, we're working in the gain value exists, and it is the infinity harmonic extension of value data. Maxine will play to an H-maximizing neighbor, according to this definition, at any According to this definition, at any given turn, meaner to an h, minimizing y. I mean, it's very, I'm not giving an exact proof here, but it's very intuitive in a sense, given that formula, the average of the minimum and the maximum, the tension between the players, the game-theoretic interpretation sort of almost wants to jump out at you. Yeah, so I should say as an aside, that is not the principal theorem of Perry-Trans-Seffer and Wilson. They were much more interested in an interesting problem. In an interesting problem of taking a kind of continuum limit of this type of game, playing the game in a Euclidean context, permitting the players at each turn only to make tiny steps, and considering a limit of vanishing step size in terms of gain value, they show that that gain value converges in such a limit to a PDE continuum analog of the infinity harmonic functions I've defined it, which is rather formally specified by this equation that's written here. So that was a In screen, yeah, but so that was a direction that caused a certain amount of interest in PDE and probability. Our context is discrete. Now, it serves my purpose for what I want to say, to ask a certain question, conceptual question really, about the game. Take a very simple game playing this on a line graph, for example. Maxine will be rewarded with a payment of $1 from Mina if the counter arrives at the right-hand endpoint, no payment being made at the end. And no payment being made at the opposite. So clearly they will push in these directions. The game is, in a sense, a trivial one. But let's ask the question: what is the strategic importance of a given move? I can ask that in terms of this question. Suppose we offer Maxine the right to buy the first move, and then the later game, every later turn of the game, will run as usual. How much should she be prepared to pay? So here's an answer: if she buys the first move, her mean pale. Let's move. Her mean payoff, we have, I suppose, this function here. When lambda is, well, I haven't got to lambda yet, but lambda is just one in this picture. It's a fair coin. So the value function is just linear as a function of this location. So if she buys the first move, she'll get the counter naturally enough, one step to the right, and then her expected payoff will become this function. It will exceed the payoff in the symmetric case if she doesn't always apply the first move. It's a symmetric situation. So the X of Situation. So the excess is 1 over 2n. So that seems a pretty clear answer then. She should be prepared to pay of order 1 over n, or more specifically, 1 over 2n. But, oh yes, so everything. And then that's true for every starting location. So the overall conclusion seems to be all the positions in this particular graph, at least, are equally important, and they're each, quote, worth of order 1 over n, where n is the length of the graph. But there is another view, and this view says, you see, Is another view, and this view says, You see, under optimal play, there's not really much play choice to make, but it is optimal, I suppose. The players will simply have 50-50 chances, it will be a simple random evolution for the counter, and so naturally it will take time n squared to actually reach the bound. So, this argument says, well, all the steps are equally important. You know, if you're playing for a dollar, you're kind of thinking about the future, allocating resources. So, each term must be worth 1 over n squared. Okay, so an inconsistent. Okay, so an inconsistent prediction. So, in summary, then, there are two different predictions: what could roughly be called a space prediction, the first one we gave, that describes an importance of 1 over n to each move, and an inconsistent time prediction, the latter prediction, of 1 over n squared. So those are kind of, and this plays a certain crux in the talk as kind of something I'll come back to, these predictions. I think at this point, I'm yet jumping out to show you. Jumping out to show you some simulations. It's interesting to play these games on certain graphs and in directed settings, I suppose there's undirected settings I've seen so far. A hex was something that the authors I mentioned were very interested in. The game is described on this board. You may be familiar with it. Players, in this case, will make 50-50 win probabilities and they will put red and blue hexagons down and they will attempt to forge connections across this. Forge connections across this region, north-south for blue, east-west for red. And it's not hard to see that the optimal strategy in any given situation, if I froze this, in an intermediate situation, the correct rule for filling in the rest of the board, or rather, sorry, the correct rule for playing it is to figure out the location which is most likely to make a difference if you were to fill in the rest of the board at random 50-50 chunks. At random, 50-50 chances. So go to an intermediate location, like this one, as I say, fill in the rest of the board at random, and then pick the maximum pivotal probability, or rather, its location, and play there. That's true for both players. So you have an interesting kind of pair of competing growth processes that are somehow linked to critical percolation on the honeycomb lattice and therefore to that kind of universality, SLE, sickness. SLE6 type objects going on, although in a rather novel way. And then I think we did a bigger one. Yud, Zador, my wife very much helped me do these simulations. Yeah, so here you have a thirty-one by thirty-one and there's something called a Gale algorithm that's being used to sort of compete. You see these sort of competing rubbing shoulders Piet Pine called it, um it's the guy who invented. He's the guy who invented X in 1942 in the Netherlands. So, yes, okay. I won't explain everything in the script. Can you repeat what's happening in this simulation? Because there were some points appearing in different positions. What exactly is this simulation? It is a simulation of, in any given situation, it's a simulation in which one repeats these samples the randomness, 50-50 chances in the rest percolation, and then you compute the unplayed hexagon that has the maximum. Unplayed hexagon that has the maximum probability of being pivotal for the outcome, so making a difference if you switch it, and then you colour that red or blue and then iterate. Now, if I play that, and then you see what gave the maximum. Each given hexagon, imagine each given hexagon that's unplanned. You play it red, you actually fill it in. You play it red, you actually fill it in, and then hundreds of times you sample the rest of the board, and empirically you determine the probability that you're going to win. And then instead, you colour it blue and do the same thing, obviously, you can use the same simulations, and then you pick the location for which you get the number that's biggest. And that's being plotted dynamically here. The strength of the critical point is units terminology for we would say the maximum Hitler probability. And then the win probability is being dynamically. And then the win probability is being dynamically plotted in the middle sketch. So, yeah, that caused that in a growth process that is an interesting one study and there's work to do there. We can also play a KPZ game. So, this is here. I'm going to play a game now in this corner of the square matters. Oh, I started wronging here. Let me just show you a simulation and kind of explain what it is. Well, okay, so in each given situation, okay, so the payment will be made at the end of the game. Mina will have the pay maximum. Red is one and blue is zero. So at the end of the game, a geodesic path in LPP has been determined, perhaps this one. And so MENA is going to have to pay maximum of this quantity. So optimal strategies again turn out to be, in essence, the play in a location that's. In essence, to play in the location that's most likely to be on the geodesic path from this point to this line. So it's a game that forges an analogous connection to LPP in the KPG universality class as hex does to things like physical calculation. It's an object that's, I think, of some interest. In a certain sense, these are the asides for my talk, actually. So let me get back to telling you about what Galton and I did as a control. Isn't it just to whoever gets the corner? You pay, it's not really in this game a question of winning or losing. Mean or what pay maximum the number of ones on the path. So yes, the person that will enter the statistic, the thing in the corner, but the actual overall payment will end up being the geodesic value in Benubi LPP, because players will be uncovering zeros and ones. Will be uncovering zeros and ones in a rather complex random fashion, but the actual outcome will be a sample of the new London Pacheco percolation. So yes, the geodesic path will certainly run through that location, but of course its value won't, well, its value won't be the LPP value. Sorry, did the players choose that this vertex will be one versus two, or did they just uncover it and it's stranded? When the coin is flipped, and then if a maximum wins, it's And then if Maxine wins, it's it's trivial that she will choose to put a one down in a location of her choosing as her choice. Mina, it's trivial that she will put a zero down. They will in fact, under optimal play, they will actually agree on a location, and that location will be the one that's most likely to be on the geodesic. In essence, that's true. Okay. I need to. So I might be error under there, isn't it? Slide, I suppose. So state government tug of war, this is the game that Barbara and I introduced, inspired by the work I've mentioned, although it turns out to be very close to Harrison Vickers' original contribution in economics or other economics literature. So it carries an additional So it carries an additional parameter lambda, but otherwise the context is sort of the same setup. So Mina and Maxine at the beginning of the game will now be given certain fortunes. For normalization reasons, Mina can have one and Maxine will have Lambda. Now, at the start of a given turn, yes, a coin is going to be flipped, but at the beginning of the turn, the two players have some fraction of their original fortunes left, and each will be invited to And each will be invited to stake some part of their remaining fortune, her remaining fortune. So let's see what happens if a maxine at a given term stakes A and mean at B in terms of specifying the game. So these sums are obviously taken away from the players and they're never going to be returned to them, neither during the game nor at its end. On the basis of the states that have been made, A and B, a coin is float, but it A coin is flipped, but it has a bias, and its bias is that: well, the probability that Maxine wins is A over A plus B. In other words, your probability of winning is the proportion U state of the total states by the two players. Naturally, if Maxine doesn't win, Mina does. Apart from that, the rules are the same as the random tug of war is specified. You move where you please to a neighbour at least. At the end of the game, Mina plays back scene F, the terminal counterlocation. Terminal, counterlocation. If you've got a little bit of fortune at the end, you lose it. So the role of the fortune is to mediate, regulate strategic decisions during the game. It's not to reward players. That stuff happens in the currency outside the casino as it did in the original game. Is that clear? Okay, so two needed concepts as I attempt to describe how we can go about solving these games. One is Solving these games, one is a fairly straightforward generalization of the infinity harmonic to accommodate the idea of bias. The original definition is lambda is one, and the new definition corresponds in a sense to a biased coin whose probability of landing heads is lambda over one plus lambda. So, this is the definition. It's a weighted average of the maximum and minimums. It sums that up to one. It's an average. Average and to give you a sense of the linear function when lambda is one when you're already seeing. And then on two simple graphs, if lambda is bigger than one, you have these sort of geometrically tilted type outcomes. Okay? Yeah. And then decompositions come in. These functions can be rapidly computed. A paper of Paris and Sunich from about three years ago did so. If you take the simple t graph, it's three boundary vertices have been labeled by the f function. It has two internal. Function has two internal vertices called the north and south. There are sort of two basic ways you might consider a computing as function. You could give this edge priority, that's a copy of a line graph with two edges, and then you can fill in the appropriate function and then complete this way with another suitably scalar copy of this. Or you can simply treat this as the high wave, the whole thing, and then that's a line graph of length three. So this kind of function. Length 3, so this kind of function can be put in. And then which one actually works in terms of delivering this definition depends on whether lambda is less than or exceeds the golden ratio. If they're equal, the two outcomes give the same. If lambda is equal to the golden ratio, the two proposed definitions coincide. Otherwise, there's this sort of change in decomposition. And then the Nash equilibrium concept in the games theory, which probably is familiar, but briefly. So spaces of strand. Briefly, so spaces of strategies for the two players. If you have a strategy pair, it is a net equilibrium if neither player, given that the other one is doing their stuff at the other end, is going to unilaterally deviate if they're measuring things by this sort of mean of payoff. So there's a form of stability about the object. Now, if you're at a Nash equilibrium, it's easy to see that the payoff, the mean payoff, is equal to the value of the gain as I defined it earlier. Of the game, as I defined it earlier. So, using these concepts, I want to describe something about how to solve these games, how I'm going to try. So, here's a very idealized proposition that begins to hint at the form of a solution, introduce this quantity, it's a bit like this basic. Okay, this quantity is, in a sense, how much difference, in a certain sense, the next term makes. Well, anyway, it's the difference of the maximum and minimum values of the Launderbias function. Okay, so a small hypothesis is that this function The small hypothesis is that this function is positive, not very important, just means that there's something to play for at each part of the board. And more importantly, is to make the hypothesis that there's an ash equilibrium that is pure. So what that means is that it's not a random strategy. Players could choose to randomly stay 75 cents, 25 cents at a given turn, or whatever. We're hypothesizing that there exists a mesh equilibrium in which the players make. Equilibrium in which the players make non-random states. That's a significant hypothesis. So, under that hypothesis, the gain value exists in state-government tug of war and it equals the lambda bias infinity homicide extension of the given variant bit. Okay, so under a contentious hypothesis, there is some structure emerging in terms of the solution of a game. So, the proof of that is a form of minimum. A form of mimicry. Okay, here's the given equilibrium, the pure equilibrium that is hypothesized by the proposition. Now I'm going to do something a bit different potentially for Mina, who is going to react to Maxine's end of the National Equilibrium. And she's going to mimic. Maxine is staking deterministic amounts at each term. It's a deterministic fraction of her fortune. Mina, I propose under this strategy, will stake the same fraction of her own fortune. The same fraction of her own for the shell in each turn. And then, if she wins the right to move, she'll move to an H minimizing neighbour, where this is a bit of type actually, where it says H here, and on the next slide, it means H lambda dot. The lambda biased infinity harmonic function, my lambda, is the original ratio of fortunes between maxine and mean in this game. Okay, so if mean. Okay, so if Mina decides to go ahead and play a game against Maxine's S plus zero, what happens is if you analyze H lambda along the gameplay, that's what Xi is, you get certainly a bounded function, but it's a super margin gap. That means its mean value is dropping, it's not going up. That reflects the fact that we don't really know what Maxine is doing, but Mina's managing to ensure with the constant fraction that she's winning. The constant fraction that she's winning each term with probability one over one plus lambda, and when she does so, she drives this function down as much as possible. Now, the game finishes almost surely, that's trivial to see, and then that therefore means the limiting value of this object is the terminal pair, because h lambda dot is an extension of the variability. So, on that basis, we learn that this mean payoff under this particular mean payoff under this particular combination, this particular strategy pair, is this is an h, and the mean value of h at the terminal location, and by the supermartingale property, it's less than or equal to h at the beginning, which is h of v. But you see, we were hypothesizing that this was a pure Nashi query, the Nashi query specifically, and therefore Mina doesn't Mina's already achieved something by doing that, but she can do no worse if she actually plays her under the Nashik query by definition. Definitions. So that means that this quantity is less than this one, and the argument is symmetric and can be played out for maximes and tell us the opposing inequality. And therefore, this quantity of the mean pair of fact the Nash equilibrium is equal to, as it says, generation, h lambda v, which is the content of the proposition. Yeah, so that's all very well, but it poses two significant questions. One, does a pure Nash equilibrium in One, does a pure Nash equilibrium exist? And two, even if it did, actually, there'd still be some interesting structure to explore here. Because if for definitely, you know, Maxine's fortune is Lambda times mean at a given counter location, what we learn from the proposition, if the answer to discretion is yes, is that the players will stake a shared proportion of their present fortunes. We can call that S lambda V, call it the state function, and then that poses kind of an interesting question. And then that poses kind of an interesting question about the structure of the problem. What is that function? So, those are the two questions that I need to go about answering to tell you about our theorem and about, in some sense, a positive solution to the game under certain conditions. And yes, I want to deal with actually, in a sense, the second question first. Let's assume that the pure Nash equilibrium exists and use a perturbative argument to try to identify the state function. So suppose under those arguments. Function. So suppose under that hypothesis that mean a has fortune one, maxim fortune lambda. I can record as capital S this state function, the common portion they would state at the impending term. So under optimal play, therefore, it's maxine new state lambda s, mean at s. I want to do a perturbative argument to get some information. Suppose that Maxine very, very slightly chooses to increase her stake. What are the effects of doing that? There's a gain effect and a lot of There's a gain effect and a loss effect. The gain effect is coming from the first turn. She's putting more resources in, she's more likely to win it. She has a heightened probability of winning the first turn. It delivers something for her if she does. I don't really expect you to kind of totally accept the form of these expressions, but rather the rough flow of this and the rough reason for them. We have this gain effect, you know, kind of differential effect, the small eta. There's a loss as well. The loss comes at There's a loss as well. A loss comes at the second term or later terms. She's been a little bit exuberant at the first turn. She's a mild hangover the next morning, and that is persistent. She has a very slightly decreased chance of winning every subsequent turn. So when we average over all subsequent gameplays, she might be losing out, and those costs accumulate. We have this formula. It's a formula a little bit like resource formulas for population in the certain sense. This is the delta difference it makes if you. This is the delta difference it makes if you lose later. This is related to the probability of Jacobian factor related to the probability of losing related time. So under optimal play, the gain is equal to the loss. Otherwise, Maxine would simply alter eta to be zero plus or zero minus to exploit the inequality. The fact that g equals l, if you go back to the formulas derived, delivers information about s. This is that information. It's a ratio whose Whose numerator is the delta function, that's essentially the space prediction from earlier. The denominator is the mean evaluation of those delta differences that accumulate along the rest of the gameplay. And it's not the same as it's run in a different spot, but it's related to the time prediction we saw earlier. So you have a a different notion of the strategic importance of the first move there. It combines the two recipes from earlier as a ratio, in fact. Well As a ratio, in fact. Well, probably speaking. So, okay, that's all very well. It's still heuristic. Oh, yes, there's another formula here. The denominator can be viewed in terms of giving up some money. There's a formula here in terms of the lambda derivative of h. Yes, so we have a view of a space versus money in these formulas, or short-term versus long-term. Spending now trying to win the turn, short-term, holding some back. Holding some back, long-term consideration, or this is more winning some territorial advantage now or paying for it with cash, which is needed for later. So that's a plausible picture, perhaps, but there are two problems and they continue to be related to the earlier two-posed questions. One, even for very simple graphs, there's no pure measure for it. So the whole theory isn't working, it seems. Two, H lambda V, which is near. Two H lambda V, which is needed to be differentiable, it would seem for these kind of heuristics to be delivered, isn't differentiable in lambda for very simple graphs as well. Let's look at examples that explain those points. Let's consider a simple graph where Maxine is one step away from winning, but she's in a weaker financial position. What she's tempted to do in that situation is to go for broke, basically, and bid everything at once. And bid everything at once. A strategy globally is just try to finish the game at the next turn. Well, okay, that's the conclusion, but we can, if a pure Nash equilibrium exists, we know what the gain value is, so we can draw a kind of conditional gain value plot under the assumption that at maximal state A and mean or state B. You get this kind of plot. There's a certain amount to take in here, but these are best response curves for the mean and maxine. The mean and maxine. This cross is the kind of saddle point prediction coming from the perturbative formula I gave a couple of minutes ago. And you want these curves to cross nicely at this point if the saddle point is going to be global. And in this example, it isn't. And the reason is that as we come down in terms of increasing financial disadvantage for Maxine, just before we reach here, Maxine goes for broke and doesn't play the strategy. And doesn't play this strategy. She prefers to try to bid everything in and close out at the turn. So a local saddle point is predicted by a perturbative infinitesimal argument, but it's not necessarily global, and in practice it isn't. The other problem, non-differentiability, is simply illustrated by the Perisanish decomposition going back to the T graph, going through the critical point when you switch, which is the golden ratio. You have two continuous torsions, and they have a point of non-differentiability mass. And they have a point of non-differentiability, when I take the minimum, or whatever it is, they're going to get to that point. So that ejection is simply an illustrated. Okay, so to summarise then, in the proposed state formula, you have this denominator, this fails to be differentiable, or if you want the game theoretic, do you mean on the T graph can play, finish out the game from the north vertex, or she can play downwards to continue a longer game, and that actually gets the whole notion of. And that actually gets the whole notion of the gameplay is a badly defined concept because there are different things Media can do. It's a bit of a mess, it seems. Okay, so two problems. Oh well, this kind of summarizes what I just said. Two problems in Jaw. Now two modifications to solve them. I'm going to deliver you a theorem in the next few minutes, and I'm going to do it by doing certain things to overcome this problem. Doing certain things to overcome these problems. I'm going to change the game, make it legendary. That's going to stop the problem that a local saddle point isn't necessarily global. And I'm going to restrict the class of graphs to be a certain class of trees in order to overcome the T-graph type ambiguities in Mina's play. So the leisurely game could have been called the lazy game, that would have been more classical mission, but my casino players are taking their time with it. New players are taking their time with those. Okay, so I have a new parameter, epsilon, between 0 and 1. It joins the other parameters in the state government over 4. The rule change is pretty simple. Stakes are demanded and offered, and then at the moment, the casino comes along and flips a coin whose heads probability is the new parameter of sign. If the outcome is heads, the term takes place according to exactly the same rules as before. Same rules as before. If the outcome is tailed, no turn takes place, the states are swept away, never return to the flow. Okay? So low choices of epsilon. We introduced this because the notion was that low choices of epsilon would disable the efficacy of the go-for-broke strategy. If you're one step from there, even in a weakened financial position, tempted by go-for-broke, would you go for the broken? From attempted by go for it, broke, would you go for it if you know that a low probability of signing coin is going to be flipped, unless it's head, your entire pill simply be swept away and the counter won't even move. Probably not. Okay? So if you do those kind of conditional value plots, there's a kind of a Poisson game that is a formal zero plus link with this game, and these are the kind of plots you get. The local side point is beautifully hit by the two best responses. It's beautifully hit by the two best response curves at all, very compact and smoother than before. So that seems positive for the theory. Yes, so let me in a couple of minutes get to the crucial messages. So the second modification is to work with so-called root-borne trees. These are trees in which there's a distinguished leaf, the boundary set of the leaves, there's a distinguished leaf that is the root, and F, the reward for. root and f, the reward function, is the indicator on the root. So the key fact for us is that the Peristinish path composition of such a tree is independent of lambda. It's determined by sub-tree diameters in a certain way. So here's the root, and Maxine is definitely aiming for there, and then the decomposition of Perisin Sandwich doesn't depend on the land of these trees. Decomposition is Decomposition is expressed purely in terms of geometry of the tree via certain sub-tree lines. Okay, so that gets me to the theorem with Garbo. Okay, so if I take a rootable tree, the basic message is that I can choose epsilon small enough so that the theory works, is roughly the message. The gain value exists from any vertex and is equal to the lambda biased infinity on function, where lambda recall is the initial. Where lambda recall is the initial relative fortune of maximum mean. That relative fortune is an almost sure constant through gameplay. Each national equilibrium is such that the player who wins the right to move moves to an extreme or vertex according to H lambda dot, maximizing if it's maxine, minimizing if it's mean. And I want to say that the state formulas are correct from earlier. There's one modification that's natural. If excited is small, and it is in a simple sense. Is small, and it is in a simple sense, then you see you're going to be sitting there for one over epsilon steps before a move even takes place. So it's completely natural and intuitive that the order of the stakes that are being made or order of epsilon. And so the predicted formulas are correct, but they now carry an additional factor of epsilon, the coefficient of the epsilon, is the one or other, both of the two formulas we saw earlier in this. There is one caveat. There is one caveat in the theorem, and in a way it shows various substances of the problem, including this one. It is necessary for this to work, or at least the proof we have, for that to work. It's necessary that the payment at the end of the game, well, if the game doesn't end at all in increment time, it's necessary that Mina be forced to pay Maxime the full penalty of $1 in that situation. There's a counterexample where the former There's a counterexample where the formers predict that if Maxine starts doing well, she rarely takes her foot off the gas. And so Amina can then rather bizarrely force herself into a financial death spiral and then offer half of her fortune at this vertex all the time and then get the game not to finish. You have to be discouraged from doing that in order to get this thing. Yeah, that's what that size is. Yeah, so then self-funded type of war was a paper. Self-funded Tub of War was a paper I wrote on my own last year. Basically, we have this precious resource in this game that's regulated by the finite provision of resources at the beginning of a game. There's another way to make something precious to someone. Spend as much as you want, we now say to our two players, but you have to pay for it yourself. So under this view, self-funded card of war, what happens is you receive the terminal payment according to game rules at the end of the game, but subtracted from that is all of the stakes you actually offered. The stakes you actually offer, you have to pay for them yourself. So that game is remarkably close actually to the Harrison-Vickers game from the mid-80s, although we only actually realised they've done this in the last couple of months. So it was an interesting sort of retroactive experience comparing the economics and mathematics literatures and topic normally. Yeah, so that one, the South Funding Tube of War, I think it's quite a fun project, has some fascinating behaviour that goes on in it. Fascinating behaviour that goes on in it. Can't describe it now. On the Trail of Lost Pennies is on the archive. 90 unique Nash equilibria on graphs like L6 or larger. Counts pretty many Nash equilibria when this game is played on a copy of Z. It's very subtle behaviour, very interesting behaviour if there's a slight discrepancy in senses between players, causing profound differences. Some sketches there I don't have time to explain, but they perhaps relate to some rather intriguing behaviour that goes on. Some rather intriguing behaviour that goes on in the cell phone version of the line. The trailblazer space, as I call it. Yes, so I end with an image of a proshmont between the two players after all that antagonism of the zero-sum games. And I wish happy birthday both to Eleanor and to we have time for a question. We have time for a question. So in the main result we covered epsilon k is necessarily strict less than one. Do you have to be? It definitely has to be in yes, it definitely has to be less than one in a graph as simple as L3. It doesn't in L2. The theory in L2, which is a one-move game, is sort of Move gain is somewhat trivial and positive, this theory. There's also questions about whether it has to be independent of you could ask if I take the class of line graphs and consider the best epsilon that works, does it decay to zero with n? There are a lot of interesting questions in the topic, actually. Yeah, so but there are these. But there are these several types of counter examples that are illustrated, and you do obviously have to box them off with positive results. Due to our time constraints, I think we should withhold further questions unless somebody has a quick question. Let's thank Alan Monten. We take a very quick one-minute break.