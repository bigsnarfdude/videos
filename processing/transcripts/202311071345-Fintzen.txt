I'm telling you today about representations of p-adic groups. So, what is p? P throughout the talk is, oh, I need to make sure not to write too large. I think you have the big screen. P is the prime number. And what else do we need is notation. So, f is a local non-Archimedean local field. So, that means f is a pilot extension. So that means F is a finite extension of QP, or F is the Laurent series over a finite field. F contains a ring of integer O, contains a uniformizer pi, and the residue field, so O mod pi O, is denoted by FQ, final field with Q elements. And then the main object for this talk is denoted by. The main object for this talk is denoted by G. G is a conducted reductive group over our field F. So feel free to think of GLN, SLN, or syntactic orthogonal unitary groups if that's what you usually do. But if you like exceptional groups, feel free to imagine the most complicated group you like. Except I make one small assumption. I want to assume that the group splits over a tame extension. Over a time extension of our local field F. So, for example, well, for example, all the inner forms of GLN, they become split over time extension, so they are totally fine. Most examples you can think of are fine. And so, what is the big goal? The goal for this talk, but also for mathematicians for a long time, 50 years or so. For a long time, 50 years or so, is that we want to understand, want to understand explicitly, as explicit as possible, the category of all smooth representations. I'll leave some space. Smooth representations of our periodic group G of F. So I call it PRIT group even if it's in. Group, even if it's in characteristic zero. If it's too small or you can't hear or see, please let me know. Try to write smaller because there's the big screen. So we want to understand all the representation, all the smooth representations, so all the representations so that for every vector in your vector space, it's fixed by some open subgroup. So this is a nice category. And I should put an adjective, not adjective, like a specification here that I'm looking at complex. Specification here that I'm looking at complex representations. So, representations over for complex vector spaces, the maps into the space of automorphisms of an infinite dimensional usually complex vector space. You could also work with an FL bar vector space where L is different from P for most of the talk. For the construction of supercascular representation, it works the same, but not for the type theory. But the HEC algebra still works the same, just not the type theory. So, that is the goal. So, that is the goal. And well, how would we approach this? So, there is this nice fact that if we take an irreducible smooth representation, so pi is an irrep of g of f, I will drop the word smooth. All my representations are smooth. Then there exists some Levy subgroup, so some parabolic subgroup which we can write as a Levy times unipotent verticle. As a Lavi times unipotent radical inside our group G and a supercaspital representation sigma supercaspital representation of the Levy M such that the representation pi embeds into the parabolic induction from our parabolic subgroup to the Our parabolic subgroup to the whole group of this sigma, sigma on the levy trivially extended. And well, that's also the definition of supercuspital representation of supercaspital if it doesn't embed into a proper parabolic induction. So for supercaspitals, you have to take p equals g here. And in general, an arbitrary irreducible representation can be embedded into the parabolic induction. So that means if we now have as our goal to understand the category of all smooth representations, Understand the category of all smooth representations. There are two sub-questions. The first one is: how do we construct and understand these supercastable representations? And the second question is: then, once we understand the supercastles, how do we understand the rest built out of the supercastable representations? So, let me start with the first problem. Problem one is construct all the supercospital. the super cospital supercospital representations of our group G of F. And the vague answer, and I'll make this more precise throughout the talk, the vague answer is that we can do this under minor assumptions. So, almost all the time we can do that. Well, depending on what you mean by almost all. So, under minor assumptions, we can actually construct the supercast representation. So, let me just ask a few questions to gauge the audience. Maybe the first question, raise your hand if you're awake. I think most people are sleeping, so that's not working. Okay, let's try it again. Raise your hand if you're awake. Okay, raise your hand. Okay, raise your hand if you're very comfortable with brunching decomposition. Raise your hand if you're very comfortable with or can define depth zero supercaspal representations. That's getting less. Okay. So once we can do problem one, we'll use the and let me just do it quickly. We use the We use the Bernstein decomposition to try to understand everything. So the Bernstein decomposition, so most of you said your family resid, means that we can decompose the category of smooth representation into a product of smaller blocks, which are the Bernstein blocks, indexed by Levy subgroup and the Levy subgroup and the supercaspular representation sigma of that. And so the product runs over all the Levy subgroups M and supercaspular representation sigma of M up to an equivalence relation. And since a lot of you are familiar, isn't it? Let me just say this in words for those who are less familiar, is it? So an equivalence relation is obtained by, well, if you have one levy and a supercasp, what we can do is we can just conjugate everything by elements in G, conjugate M. Elements in G, conjugate m and sigma to get another pair. And these two should be obviously equivalent because, well, they are just obtained by conjugation. The other thing you're allowed to do is to twist sigma by some unramified character, so by some character that's trivial in all the compact elements. And then you obtain the Bernstein blocks here. And what are the Bernstein blocks? So I just recalled the fact that every irreducible representation embeds into a parabolic induction. Embeds into a parabolic induction. And so these Bernstein blocks are built out of you take your Levy M with the representation sigma, parabolically induce this up. And then that might not be irreducible. So you just collect everything that's inside there. And that forms your Bernstein block and all the possible extensions of those. So if you do this for all the elements in the recurrence class, parabolically induced, collect everything that you find, that is your Bernstein block. And so, well, that was. And so, well, that was done by Bernstein. He decomposed the category into these blocks. And now I said that problem one was to construct these supercasp representations here. And then problem two will be that we want to understand these Bernstein blocks. These are the smallest blocks. So problem two is understand these Bernstein blocks. And if we understand them, we understand the whole category because the whole category of resource presentations is just the product. All right. So let me maybe still carry around an example. It might also help those that are not working with these all the time. So let me take the example that GSSL2. G is SA2. What are the possible levies? So, one option would be that m is equal to g, and in that case, sigma is just a supercasp representation of g and the Bernstein block, the elements in the Bernstein block are just the direct sums of sigma. Sigma, sigma direct sum with itself, and so on. Infinite, arbitrary direct sums are allowed, nothing else. Nothing else. Supercaustpitals don't have any interesting extension. And the second case is the case that M is, I call it T, just the diagonal matrices, the maximal split torus. And then if I look, for example, at the block for T with the trivial representation, this block, well, it is called the principal block. And I like to use this as an example later to see non-trivial examples. Later to see non-trivial examples. What is this? Well, I said we just parabolically induce up our trivial character from the Burrel that's parabolic containing our tourists to the whole group. And what do we get? Well, this contains the trivial representation. And as the quotient we have is Steinberg, so that means this principal block contains the trivial representation, I call it trip. I call it triv. And it also contains the Steinberg, but also non-trivial extensions. So that second block has a lot of more complicated structure to it rather than just being a representation and a sum of it as often as you like. And so how do we now understand these blocks? Well, the idea is that we want to understand them by Hacker algebras, but for this we need to understand the hacker algebras. And so here's the vague answer that I'm going to make more precise. Answer that I'm going to make more precise, the vague answer to a problem two is that is joint work and progress with Jeffrey Adler, Kazum Manish Mishra, and Kazuma O'Hara, which is we want to understand all Burns D blocks, so we want to take an arbitrary Burns Dean. So, we want to take an arbitrary Bernstein block and want to understand it. And I'll show throughout the talk how to understand it in terms of modules over Hacker algebra. And it turns out then, well, you need to understand the Hacker algebra. And it turns out that this whole thing is isomorphic to representations of some smaller group G upper zero with the Levis subgroup M zero and a representation sigma zero. And a representation sigma zero, and so far that doesn't have any content. But I'll say two things: I'll say that this sigma zero is depth zero. And there were less hands up in the air for depth zero, so I will say a few words about depth zero in a moment. But for now, all you really need to know is that this block here is rather well understood. Thanks to work of. Thanks to work of Morris in the 90s. So the Depth 0 blocks, we already understand how to describe, and now the result is that if we take an arbitrary block, it's actually equivalent to a Depth 0 block. And more precisely, the underlying Hacker algebras of the titles are actually isomorphic. That's the second big result. Big result and combining these two, we understand the whole category of the smooth representations of our group G. Oh, I should be at this board so that the camera is showing that one. All right, so what are possible applications of that? Well, there are many. Well, first of all, understanding supracastrals. Well, you're all a lot of you have worked with these things. A lot of you have worked with these things. If you want to work with them, you want to write them down explicitly. So you're the wrong audience to advertise it. You know that already. The advantage of relating everything back to Deb Zero or having this equivalence of categories means that you can relate a lot of questions about the representations, about general representations back to representations of debt zero, where they're much easier to much more practical. Some things are already known, others are just much easier to prove them. Easier to prove there. Another application, for example, people are currently trying to, it's very fashionable to do everything categorical, to write down a categorical local angle correspondence to upgrade everything. And it's current work in progress to write down a categorical correspondence for the depth zero representations. And now, if we know that arbitrary blocks are equivalent to depth zero blocks as a category, like these blocks are equivalent, then you can compose this with the depth zero. This is with the depth zero local Langlands correspondence, a categorical one, and thereby get a whole categorical local Langnance correspondence because also on the local Langnans on the Langnans parameter side, one has a correspondence between the depth zero parameters on the sheaf, on the space, and arbitrary ones. So that, for example, allows us to reduce the search for a categorical local Ungnanz correspondence to depth zero. G0 is depends on yes, in this, I guess I said it as vague. So the precise setting will be that if we let's say we assume p doesn't divide the order of the vile group, then g0 is still tamely ramified. If you are working with small primes, then Are working with small primes, then not all types are yet known. There are settings where this might not be tamely ramified. But if you assume p doesn't write the order of the while group, if you assume that you're working with a chim-u type, which is the one we are working with, then it's tamely ramified. No, except that I so the question was so the first question was if G0 is fairly ramified, which it is if you assume P doesn't divide the order of the revolut group. The second question was we don't really need it. Well, so the precise result uses Kim U types where you need it. The precise result, the even more precise result, gives you an axiomatic setup, which also applies to some G0 that are not tamely ramified. That are not tamely ramified, but you first need types, and you don't always have types. So I'll come to that maybe now. So I'm erasing problem one is to construct all specust presentations, and problem two is to understand the Bernstein block. Problem two is to understand the Bernstein box. So, since I know that not everyone here knows the full story, I want to give you a quick overview of what is known about the construction of supercospital representations and at the same time types. I haven't told you yet what types are, who is comfortable with types. Comfortable with types. Okay, that's much less. Okay, so I'm going to tell you what types are in a moment, but the picture that I'm drawing for you works equally well for both percussals and types. And so that's why I just want to draw the picture once. So I put the word here already. All right, so in which cases do we know things? Well, for GLN, we know everything, almost everything. We know half the construction. Almost everything. We don't have the construction of supercascals and types. I guess I shouldn't. So we know everything, and some people here are very angry at me. That started with Howe and Moore. It was finished by Buscher and Kutzko 30 years ago. Classical groups, if P is not equal to 2, we also know thanks to Sean Stevens around 15, 20 years ago. I don't know, time flies so fast. We also know other forms of GLN. Forms of GLN of GLN, thanks to Sicher Stevens. All these constructions are very similar to GLN. You try to relate everything back to GLN. So I'm really interested in doing it for general reductive groups. And so in that setting, I usually like to draw a picture. So let me draw this picture since not everyone has seen that. So I want to draw in this direction the depth. So the depth could be either zero or it could be a positive. Are a zero, or it could be a positive real number. Who knows what the depth of a representation is? Okay, either everyone falls asleep, or let's people know. All right, so what is the depth? So I said smooth representation, we seem to be familiar with means that if we take a vector in our vector space, it's fixed by some compact subgroup. But we don't know how large or small this compact subgroup is. And so, what Moy and Persert did, Moy. did my my handwriting is getting ugly and moy and pressette in 94 96 they introduced the filtration of the group by a compact open subgroup that's getting smaller and smaller and smaller and so the depth is now the question how far do we have to go down in this filtration until we get fixed vectors and once we have fixed vectors that's the depth the depth is how far we have to go down into this filtration In filtration. And so, if I want to draw the depths in this direction, horizontally I want to draw the prime number P. P is our prime number. Of course, some results depend on what the prime number P is. And so what Moyer and Presser did is they studied these depth zero representations. So these are the representations here at the surface. And I should also add Morris, who did the same. Morris, who did the same around this roughly the same time, 193-1999, using a different approach. So, what are maybe I use this moment to actually tell you what are the zero representations? So, fact, let's see, do we need? Let me actually put this up here so that I have more space. Oh, sorry for the camera. So fact: all Depzer supercospital representations are of the following form. So let me do three things in parallel. Let's say the first is G is the general general group. And the second is the special case where G is semi-simple and simply connected, so that I don't have to define everything for general groups. And then the third one is the example for SL2. So for the general groups, So for the general group, you take G of X. What is this? Well, X is a vertex in the group splitting, and G is the stabilizer of the vertex. And if you don't know what this is, it just means, and I want to do this only in the simply connected case to simplify thing, it just means it's a maximal compact subgroup. So, for example, for SA2, we can take S2. For SA2, we can take SA2 over the ring of integers, or we could take matrices of the shape OOPOP, not P uniform either. OOYPIOY inverse form. These are up to conjugation, the two options we have. What do we do with this group? So this is just the Komba group. I want to construct a representation of a Komba group. I want to construct the representation of a compact group first and then compactly induce. So I quotient out by what I call gx0 plus. And what is this mysterious zero plus? It's the zero plus, the next filtration subgroup in the Moi Passet filtration. But if you don't know what it is, it's rather easy to just define by just saying it's a prop unipotent radical of Gx. And what is this quotient here? So, this quotient is just, these are just the FQ points of a reductive group. So, now we have to deal with representations of a finite group. Well, then that I assume we know. Well, this is actually super difficult, but let's black box this. And let's just take rho, a caspital representation. So, to some finite dimensional vector space, Caspel representation of this finite group of Lie type. So, means there's no fixed vector under any unipotent radical of any parabolic subgroup. So, that's how we get a representation of a compact group in the general case. And well, here in this case of SL2, portioning out just means we mod out the proprietary unit radicals. So, these are matrices of the form 1 plus 1 pi. matrices of the form one plus var pi o on the diagonal and var pi o off diagonal. So all the matrices congruent to one mod var pi, that's congruent to SH equivalent isomorphic to SL2 over the residue field. And then we just compose this with the Caspital representation row. That gives us a representation of the finite of the compact group and then the final depth zero representation is just noted pi sub rho is just the compact induction. Rho is just the compact induction from this gx to the whole group of this representation rho. So let me just write down the definition because it's helpful to see for later. So as always, just all functions on the group to the vector space V Rho that transform via the rho on the left. So f of k g is rho of k f of g. Of G for small k in GX and N E G and F is compactly supported. So that's a Depth Z representation, explicit example, most general case. And all Depth Z representations, all Depth zero superconductospitals are of this form. So here they are. And Depth Zero representation, as you now see, just corresponds. Zero representation, as you now see, just corresponds to representations of finite groups of Li type. So when I said we reduce everything back to depth zero, it means we reduce everything back to representations of finite groups of type. Stand here for three seconds so that the camera stays with me and smile. Hurry up, I want to move. All right, I hope you caught it. All right, so these are the Deb Zero representations that we. These are the depth zero representations that Moyen Persett constructed. So, this is actually, I should have given credit, this is Moyen Persat or also Morris. And now I want to go deeper down in the ocean. I want to construct the representations of positive depth. And then I lost my eraser. Camera can stay there. All right. And so the first general concept. The first general construction for general groups, I think, was provided in 1998 and then generalized in 2001, was provided in the first one in the thesis of Jeff Adler and then generalized by JKU. So he constructed a lot of representations which I just draw as circles here, some ugly things. So these are the representations that he constructed that are some of The constructor that some of you are working with. So that's the most general construction that those who like to work with them work with usually. I guess if I talk about types, I should also mention Julie Kim. And I guess in general, I should put a small initial there for fixing a few gaps. And then the question is: what do we get out of that? Do we get everything? And in 2007, Julie Kim Kim actually proved that under the assumption, two assumptions, if p is very, very, very large and if the characteristic of the field is equal to zero, then use construction gives us everything. Use construction is exhaustive. Not in the sense of you're exhausted after reading it, but in the sense of you get everything. You get everything, the prime is very large. So that's peak large, very, very large. And everything here, if you assume that the characteristic of the field is zero, comes from use construction. And then there, well, several questions remain. Question is, is this optimal? No, it's not optimal. I've sh in 2000, can you read green? 21. 21. I showed that if you assume p doesn't divide the order of the while group, then use construction is exhaustive. So that means we just assume we are in this region where p doesn't divide the order of the while group of the group, and suddenly we get everything from use construction without the assumption on the characteristic. So that means we can construct all supercastity representation just under this minor assumption. And that's what. And that's what, when I say we reduce the cardiac, for example, that the arbitrary Bernstein blocks are equivalent to depth zero Bernstein blocks, we actually use this construction here. So that leaves one more question, which is what happens here if the prime is very, very small? And well, there's definitely more out there. And so that's something I was actually hoping to talk about today. Was actually hoping to talk about today, but let me put optimistically at 2024 here. So that's joint work with David Schreen, who is in the audience. We are trying to construct some new representations down here. This will not be everything yet. There's more work to be done, but it will be in some sense, hopefully, everything that's tame. But that's sadly not yet ready to be presented today. To be presented today. So that's the overview of what we know about the construction of supercast representations. And let me keep this for a few seconds and then I rotate it. So this is the picture, meaning if P doesn't write the order of the while group, we know everything. And I'm going to rotate this if I figure out how so that I can bring it back. So that I can bring it back if you like. So, what I am hiding now is just that we know everything, almost. So, that was the construction of supercaspular representations. And the second half, I want to talk about types so that we can understand all Bernstein blocks. So, who is awake? Raise your hand if you're awake. Okay. Your hand if you're awake, okay. Excellent. So, then my question of who knows type I think was rather accurate, but most people don't. So, let me define what types are. A pair K Rho, where K is a compact subgroup and Rho is an irreducible representation of K. Of k. This is called a type. And a type for a Bernstein block, so a type for a pair consisting of a Larry subgroup and a supercaspation of the equivalent is an M sigma type. If the following happens, if for every irreducible Irreducible representation pi of our big group, G of F of our PRD group. The following two things are equivalent. So the following are equivalent. Equivalent. The first thing is that this representation pi is contained in the Bernstein block corresponding to m and sigma. And sigma. So it means pi is obtained by parabolically inducing sigma from the parabolic containing m or something in the equivalence class and then taking a subquotient of that. That's the first. And this should be equivalent to the second condition, which is that pi, the representation pi restricted to this convex open subgroup K contains our representation rho. Or in other words, I could say that the space of homomorphism. The space of homomorphism, of k homomorphisms from rho from rho into pi is non-trivial. So that means what does a type do for you? A type tells you if something lies in the Bernstein blog or not by just restricting your representation to the compact subgroup K and then checking if it contains rho or not. And so restricted to the compact subgroup, it's something that decomposes, it's semi-simple, so it's something that's. Simple, so it's something that seems reasonable to do. That's nice. It's not just nice, but there's a nice fact. But let me maybe before I give you the fact, give you the examples. I think I have time for the examples. Let's see. Yep, quick example. So the example of SH2. Well, in the first case, where we had that M was equal to G. M was equal to g and say sigma is compactly induced. Say sigma is this pi of rho over there. So it's compactly induced, compact induction from k to g of the representation rho. In that case, the type for this is just k and rho, not surprisingly. So k and rho carries all the information. The second case was Case was the case where m was equal to t, the diagonal torus, and sigma was trivial. In that case, we can just take the Eva-Hori subgroup and the trivial representation of it, and that is a type for this pair T trivial. So these are two examples. So, these are two examples of how types look like. So, IW is the Iberhori subgroup. So, O O P O O intersected with S and Q. Oh, not P, but all right, that's an example. Why is this useful? Nice theorem of Bushnell and Kutzko. Kushnell and Kutzko from 1998 says the following. Suppose that you have a type. So I didn't actually, well, I told you they exist, but now I just define them. So suppose you have a type. Suppose K Rho is an M sigma type. And so, when do these types exist? The types exist exactly in that situation. So, for classical groups, if p is not equal to 2, for GLN, we know that their tides exist. And for general groups, if p doesn't evaluate the order of the viral group, they exist types. And so, in all these cases, the following theorem applies. In all these cases, suppose that K and Rho is a type. Suppose that K and rho is a type, then the Bernstein block corresponding to this m and sigma, the one that we try to understand, is equivalent to modules over some Heckel algebra, which I write as H of K rho and define in a second. And well, to be precise, write unital modules. So once we understand these Hacker algebras and the modules over this, we understand. And the models over this, we understand the Bernstein block. What is this HK rho? HKRho is something very explicit. These are just functions from the whole group. Over there we define compact induction going to the vector space. Now we go to the space of endomorphisms, just as the complex vector space of vivo. And we have the transformation properties that f of k1 g k2 is equal to g k2 is equal to rho of k1 f of g rho of k2. So now rho pops out on both sides, it makes sense. So these are the image are basically matrices we can conjugate and we can multiply by matrices on both sides. Or more precisely the space of endomorphism so we pre-compose and post-compose by another endomorphism and F is compactly supported. Compactly supported. Another way to define that, and if you are bored and don't know this yet, that's a good exercise. It's equivalent to say this is the space of endomorphisms as G representations of the compact deduction from K to the whole group of our representation Rho. Row. I said these are algebras, so the algebra structure here comes from convolution, something one can write down explicitly, and here it just comes from composition. So that's how you can describe the Bernstein block in terms of explicit modules. Am I in the let's see? What am I time-wise? Okay, time and Okay, time enough for an example. So let's see what this looks like in the example. In the example of G being SL2, well, you don't really need SL2 for case one. In the case of A, what is the Hacker algebra? In the case of A, the Hacker algebra is just isomorphic to the complex numbers because I said it's the endomorphisms of the compact induction. Now the compact induction is the representation sigma, which is irreducible. Is the representation sigma which is irreducible? And well, the irreducible of an irreducible representation are just c and so we see in particular that previously I said that these Bernstein block was just sigma and sigma plus sigma, sigma sigma plus sigma, and so on, which is exactly the same as modules over the complex numbers. The second example, I think I have to stand here for a second and then I move over. All right. In the second case, that's the more interesting case. So, in the second case, we have the Eberhurry subgroup together with the trivial representation. I'd like to use Presentation. I like to use the first version, which means it's just functions. This is just functions that are compactly supported on the double coset of our group G, which is FL2 in this case, modeled by Iberhori on both sides, because on both sides you asked it to be equivariant for the action, but the action is via the trivial representation. And you can write down very explicit description for that. Description for that, and that's actually what we will generalize. So, I'm spelling this out. So, you can think of it as the vector space. It has a basis indexed by, well, it's given by T sub W. What is W? Small W is an element in what I call capital WF, some affine well group, not the one of G necessarily, which is equal to just in this case generated by. Just in this case, generated by S0 and S1, and no interesting relations, so just the relations that the squares is trivial. So that's one, the other is a line. And so that's just as the vector space. And what are the relations? The relations are of the form that Tsi times Tw is equal to one of the following two things. In the length additive case, we just length additive case we just multiply the the index so that's in the case that the length is additive and otherwise there is some index so there's q minus one times s w plus q times s sorry t sub s i w otherwise Well, and you have a similar formula for the other way around. I'm not spelling that out. So that's how the relations look like. And I might want to write this also as the whole thing here. I write this as HF of this WF and Q. So I want to write here, that's the affine Hacker algebra attached to this is the affine. Algebra attached to this is the affine group that it's attached to the affine while group and by q or should I in I write down what this coefficient here is that pops up all right so that was an example how does it work in full generality so in general um what do we know in general so Morris showed in 1993 1993, how to compute the depth zero Hacker algebra. So, Morris in 1993 described the Hacker algebra explicitly in the case where this rep G M Z M sigma, so this burn. Sigma, so this Bernstein block is, well, or that's not a good way, in the case explicitly in the case of a union of Deppsview Bernstein blocks. What do I mean with the union of Bernstein blocks? I'm staying at this blackboard, just getting covered. Blocks. I'm staying at this blackboard just getting color. Ignore what I mean by the union of. So that can be removed. So I'm not even worrying about defining that for you right now. So he describes this Heck algebra in the case of a depth zero Bernstein blocks, i.e., we have these Bernstein blocks and sigma, where the sigma is now depth zero. And we have seen earlier what that map. That map. And so he describes this very explicitly as follows. Well, this is actually a bit our reformulation, but it's essentially up to erasing Union and Morris. It's a semi-direct product of the first thing is some group algebra, but it's twisted by some co-cycle that's a bit mysterious. Semi-direct product with some algebra. Product with some affine Hacker algebra of some affine value group, not the affine value group of the group necessarily, some other one, together with some parameters. So that's how Hecker algebras look like. In general, one can explicitly describe what these groups are, one can explicitly describe what the parameters. There are explicit formulas for that. So that's the Affine Hacker algebra that looks like this just with the Affine. Just with the alpha while group being more complicated. So that's the Deb0 case. Yes. Does he know what is? This is index zero. So if you like, I can put a G0, M0, sigma, zero to make things clear. My zero to make things clear. So, this is just in the depth zero case, and then maybe put a zero here everywhere. So, Morris only works in the depth zero case. And that's why the hope was now that we can reduce everything back to depth zero so that we understand everything. That's the content of the last 15 minutes. All right. More questions at this? Okay. All right. So here is something that Jeff Adler used to call a dream. So I like to call it a dream because dream sounds good. Which is the following. Which is the following. We know the Debzer case, so the dream is that everything relates back to that. So the dream was, and now I'm providing the precise detailed setup. So let K rho be a type as constructed by Kim and you. By this, I mean I've shown you on the Shown you on the previous board that all these representations come in the supercaspel case from use construction. And when we work with types, it was actually Julie Kim together with JK Yu that observed that the same construction works also for types if you relax some of the input conditions. So, to be precise, if P doesn't divide the order of the Weygroup, we have types for all Bernstein blocks, and they were constructed by Julie Kim and JKU by using JKU's construction, but relapsing a few of the conditions so that it doesn't provide us with supercospitals anymore, but with types for all Bernstein blocks. The construction of Kim and U includes Construction of Kim and U includes the type of the supercasps that JKU constructed. Was that the question? Correct. In general, a K and a row, or maybe I'm... A K and a row might not give you anything. And if you're lucky, it gives you a type. And if you're super lucky, and let's ignore the center, it gives you even a super casual representation when you induce app from. Representation when you induce up from k to the group your representation rho. If you get a supercaspital representation by induction, then this k and rho is a type, yes, exactly. And so the special case are the types of supercaspitals by JKU, and then Kim and you generalized that to get also non-supercapital trends. So let's say we have such a type, and I should specify M sigma, an M sigma type. Then the dream is that then there exists some subgroup G zero inside G, which actually in most cases In those cases, it is a twisted Levy subgroup. So it becomes the Levy subgroup over some field extension. And there exists an Levy subgroup M0 inside G0, a Levy, and there exists a sigma zero is super caspital representation of depth zero. It's a depth zero supercaspal representation. And there exists a K0, I think I put it up there, rho zero, a type for this M0 sigma zero and M0 sigma zero type such that the Hacker algebra, let me write down the group as well for the group G of this type K and rho that we started with our arbitrary. And row that we started with, our arbitrary type is actually isomorphic to the depth zero type that we have just provided. So more slowly, we start with K and rho and a Kim U type, and the hope is that then there exists a K0, rho zero that is a type for a dead zero block of a smaller. A dead zero block of a smaller group G0, such that the Hacker algebras are isomorphic on the nose. And remember, we understand the Secret algebra thanks to Morris. That was on the other blackboard, so therefore we also understand the Secret algebra here. So in particular, the corollary of that would be that the Bernstein block corresponding to m sigma is equivalent to the Bernstein block. So the Bernstein block for G0 of this M 0 sigma 0, which is now of depth 0. So this is the depth 0 part. Yes, please. Yeah, so the question So the yeah, so the question was somehow there's Kim and U and there's K and Rho for G, but supercaspilis of M. So how are things related? There is in some sense this K and Rho lies above some type K M rho m, which is a super caspetal type for M and sigma. Haspital type for M and sigma. This gives you M and sigma. And above that, you can make this precise, you can construct K and Rho, which is in the type for your representation that I didn't give a name. That's somehow a lifting up from the levee to the hook. Yeah, that's how Kim and Yu constructed that. Yeah, I'm using the theory of covers of Bushnell Kutzko, Kim and Hu. Covers of Bushnell Kutzko, Kim, and you just constructed these things. Yeah, excellent. I am happy to provide more details in seven minutes as well. But if there are more questions, does the sigma zero end up just being the depth zero part of the sigma? Excellent question. So what is the sigma? No, I think you mean Rose here. No, I think you mean rho zero. What is the so does the end up the depth zero part of the yeah is the sigma zero like um just the depth zero part of the JKU construction? I guess you could also say is this just the row zero here, the one that goes into the input of the construction of types. Adela knows, I think, what that means. And the answer is yes and no. I haven't really told you any details, but the quick Details, but the quick answer: where is the camera there? So, the quick answer is: no, and yes. What you need to do is you need to take the row zero that goes into the Kim U datum and then twist it by a quadratic character, which is the quadratic character that I introduced together with Lawrence, Tasha Khalita, and Lawrence Spice. And that's actually what does the work. So, this row zero here is the one coming from Kim use. The input into the construction of types for Kim and U. The construction of types for Kim and U twisted by the quadratic character constructed by Lawrence Tasha Kalitha and Lawrence Weiss, that is alphabetic order. And that's very important. But I haven't actually stated the result yet. So let me maybe state first the result and then I'm happy to make this more precise as well. So what is known or what cases is the dream true? The dream Dream true, the dream is true. Well, it's true for GLN. That's how everything started. I mean, Bush and Kutzka did all this theory, and if you look at it, it should be true for GLN. For classical groups, there was Heyerman who did a lot of work on this based on work of Moglin. So we understand a lot there. I haven't seen this statement, this is nowhere in the literature as far as I can tell. What is true for germology? True for general G. There are some small cases where people looked at specific groups. So let me maybe just focus on general G for now. So for General G, in 1998, Morris, not Morris, Roach did the case that M is equal to T, where T is a split maximal torus. So this means in particular that the group So, this means in particular that the group G is a split torus, and that was done by Roche. And then, not much happened for general groups until recently in, I think, 21, where Kazuma Ohara proved the case of M being equal to G. So, that was Kazuma Ohara. So, that's the case where we have a supercaspital type. So, now we have the two extremes. You have the one extreme where it's a torus, the other extreme where The other extreme, where sometimes you could say it's more boring, but there is also something to it. But in the case of SL2, that was the boring case. And now the question is, what happens for arbitrary M? And well, that's the theorem, expected theorem, work in progress, and joint work with Jeff Adler, Mishua, and Kezuma O'Hara. OHARA, and that's very much in progress. So, fingers crossed, it all works out, is the result that the dream is always true. So, we always have this isomorphism of Hec algebra, so we always understand arbitrary. Understand arbitrary hacker algebras attached to types, to chemu types, so we understand arbitrary Bernstein blocks. And this is the supercaspital case, so that was the case for acid. So in the case of acid, you can, I mean, in most cases, this is abelium, but not always. So this is the case in the example where we had. Where we had SL2 and then SL2 and sigma being some supercastle, this was just sigma, sigma plus sigma. So this was rather boring, and the Hecker algebra was just C. But in general, it can be a bit more complicated. But that's the one extreme, and the tourist is then the other extreme corresponding. All the different facilities come from the All the different communities come from the center. If the center is trivial or connected, no, not trivial. Yeah. If it's simply connected, then nothing interesting happens. If there is a center and infinite center, then things get more complicated. Am I going to explain the twisted levy? So I think I have two minutes. If you ask me after, I'm happy to explain it to you. Let me just see. Let me just make two remarks and then answer that question. So let me just make some remarks. Maybe the first remark is support preserving vector space isomorphism is. Is easy. So that's a really bad formulation. But what I mean is it's really not difficult to write down and support preserving vector space isomorphism. The difficulty is really to get an algebra isomorphism. So algebra isomorphism is difficult. And what can I say about the proof? What can I say about the proof in a minute? I want to make one crucial remark that the question came already up. We need to twist things by the cadretic character that Tasha, Kalita, Lawrence Spice, and I introduced. So one crucial ingredient in the proof is that the proof requires us requires us to extend the quadratic character, the quadratic. The quadratic character of myself versus Tasha Khalifa and Lauren Spice to a larger group. So a larger group, which is essentially the support of the hacker algebra, so that we can actually make everything can work because it's really. Well, that everything can work because it's really crucial that we use the twist of this character by this quadratic character for the proof. I mean, for this isomorphism. If you don't put the twist in, it just doesn't work. There are counterexamples. And there was one more thing I wanted to say. Oh, and the last thing I wanted to say. Let me just say this in, or let me put it in the corner here. Remark three is that is that our result result works for much more general K and rho. So we have a general axiomatic setup so that future people working on this can just check the axioms if they are satisfied everything else works. So we hope, for example, to apply this to future construction. Like I said, David Sweene and I are constructing time. I said, David Sweeney and I are constructing types for small primes, so we hope that this will apply to that setting as well. It might apply for classical groups as well if it's not yet known there. And let me stop here and then take more questions.