A coil 20 data set. So it's essentially a little ducky that is being photographed from different directions. Okay. So there is 72 little duckies and each image is 128 pixels by 128 pixels. So here's the result of doing PCA on this data set. And I'm being naive. Uh, and I'm being naive, I'm using the Euclidean distance in the data space. Uh, so if you do PCA into R3, this is the picture that emerges here on the right. Okay, so this is sort of a satisfying dimensionality reduction because you can say, oh, the data accumulates around a circle, and the circular feature is parametrizing the direction of the face of the little dot. From another point of view, the three principal components that you're computing. About components that you're computing are not super useful features, I would argue, because if you just look at the features themselves, they are not the ones that are telling you that the dock is turning around. So if you just look at the first PCA naively, the semantic value of it is not sort of the circularity of the data. And the reason is essentially that the data has non-trivial topology. Okay, so even though it is intrinsically one-dimensional, Intrinsically one-dimensional, you cannot realize it in R without distortion. So the goal for the talk is: can we use the topology underlying the data to inform dimensionality reduction procedures? Okay, so that's the hope. So this is Giacomo Alpanisi. So Albanisi was an Italian mathematician, and he's famous for a lot of work in the. And he's famous for a lot of working on the right geometry, but he's particularly known for the Albanese map. What the Albanese map is, is a smooth map from a compact, smooth Riemannian manifold, sometimes orientable for some theorems, into a flat torus of dimension L, where L is the first Betty number of the manifold. The map is using the topology of the manifold explicitly. And before I tell you how to construct Before I tell you how to construct the Albanese map, let me tell you some properties of it. Okay, so the first property is that it is a topology preserver, meaning that if you look at the induced map in cohomology, at least the first one, you get an isomorphism. Okay, so the Albadesi map preserves low-dimensional topology as measured by the first cohomology group. The second property is that the Albanese map is her. Is that the Albanese map is harmonic, meaning that it is a critical point of the energy functional. And here's where you need M to be oriented to have a sort of a global volume port. And then the third property, which I think it's maybe the coolest, is that it is universal in the sense that if you have a map H, which is harmonic and smooth from your manifold M to a flat or a T, then the map has to. Or st, then the map has to factor through the Albanese map. And the A here, H, is a unique affine map. Okay, so it is topology-preserving, harmonic, and universal. Now, let me show you how the Albanese map is defined. So, to define it, the first thing you do is you look at the harmonic, smooth harmonic one forms on M and And by the Fodch theorem, we know that this is a vector space of dimension, the first bedding number of the manifold. So here's what you can do. Look at the maps. So take a loop in the fundamental group of the manifold. And what you do is for every one form on M, you're going to take the line integral around the line. The line integral around the loop. Okay, there are sort of two theorems here for this to make sense. The first theorem is that if I give you a homotopy class of loops, you can find a smooth representative. That's the Whitney approximation theorem. And then the second theorem is that if I have two loops that are homotopic, the integral along the loop is the same for the form W. And that's of calculus here. So what you're getting. Here. So, what you're getting is a linear subgroup of the dual space of one forms, which is again R to the L. And because of the homotopy invariance, the thing on the left is essentially a discrete abelian group, right? So what you're getting is an L-dimensional lattice inside of these L-dimensional vector space. Okay? And then the final. And then finally, this is the final ingredient. Suppose that you have a smooth map where it starts at some fixed P0 and it ends at some other point P, then consider the following construction. For every one form W, go ahead and integrate the form along the path gamma. This does not give you a well-defined map. Give you a well-defined map from P to the dual because it depends on the choice of the path. That said, if you choose another path to P to the endpoint, the difference is going to be valued in this lattice. So this is the Albanese map. It takes a point P and it gives you something in this quotient. And you should think of it as for every point P. As for every point P in my manifold, I start at P0, take a path, and then integrate the form along the path. And this is exactly what the Albanese force. Okay? And this is the thing that has the three properties I mentioned. So it is topology-preserving, it is universal, and it is harmonic. Okay? So, any questions so far? Comments or complaints? Comments or complaints. So we get to these sort of yeah, so the way you're going to fit it is using generators for the lattice, which are going to be representatives of integer one forms. So, okay, so I'll say more about this in a moment, but the way to think about this. Moment, but the way to think about this thing is again as giving you a sort of a basis for the lattice. Yep. Yes. In this case, I'm only thinking of compact manifolds, orientable sometimes, smooth Riemannian. Yeah. Yeah. Okay. Okay. So we. Okay, so we get to the sort of typical dichotomy that people that do computational mathematics have to deal with. So, in the ideal side, you have sort of your manifold, and you know who the first body number is, and you're able to compute sort of the one forms, and you have sort of these integer lattice inside that. And then there is sort of the data side where you typically have just a finite metric space with a crappy distance matrix. But it is what it was. But it is what it is. So, the question is: how do we move these types of constructions to this side of the world? And depending on your sort of sensibilities, you can say, oh, you know, maybe my data set is sort of sampled around the manifold, and I can do something like, I don't know, discrete exterior, something. That's one avenue you can take. We're going to take a different one because this is my talk. So the first thing. So the first thing I'll say is that even in the ideal world, the one harmonic forms, the harmonic one forms can be identified with the first cohomology of the manifold with real coefficients. And here I'm using a couple of theorems. I'm using Hodge theorem and then Durant cohomology, and then Durand theorem to link Durant co-homology to singular parts, inflation cohomology. And if you do that, then the forms on this side really just look like one cohomology classes with real coefficients. And then the integer lattice looks essentially like integer cohomology classes. Okay. And once you're here, then there is an equivalent thing you can do on the right-hand side, which is this business of persistent cohomology, which is by now sort of an established part of computational mathematics. Part of computational mathematics. I'll tell you a little bit more about this and then how we can get maps to Tori for real data sets. That's what's coming up. Okay, so if you have an abstract data set, which we're thinking of as a metric space, then you can construct what is called the Rips complex at scale alpha. So it's an actual sequence complex. The vertices are the points in X. Are the points in X? You draw an edge if the distance between those two points is less than alpha. You draw a triangle if the three points have been connected by edges, and so on. And what you're seeing here is alpha being varied from zero to the diameter of the data set. The idea is that as alpha is changing, the simplicial complex reconstructing is approximating the underlying topology of the data set. Of the data set. At some point, maybe you get it correctly. And at some point, you're going to cover sort of the hole that you see in the middle because you're allowing longer and longer edges to be added to the components. The game is instead of trying to optimize alpha, you're better off just looking at all of them. So, what you're doing is you take your input data set, you construct Data set, you construct this family of simplicial complexes, one containing the other, and you get what is called a simplicial filtration. Now, if you apply cohomology to each one of these simplicial complexes, you get what is called the persistent cohomology of this filtration. If you maybe have your sort of cohomology toolbox a little bit rusty, what the A little bit rusty. What this is, is if f is, for example, a field, this is a vector space over f and the dimension of this vector space tells you something about the number of n-dimensional holes in that sequential complex. So what you're getting here is a family of vector spaces, and the inclusions of these inclusional complexes are giving you linear maps between them. Are giving you linear maps between them. That's it. So, what's the game one can play? If you have taught linear algebra. We were having this conversation this morning. But if you have taught linear algebra and you have taught Gaussian elimination, then there's something. that there is something that one thing you can do is do something akin to gaussian elimination but uh simultaneously over all these vector spaces and all these linear maps right it has the same feel and what it does is it um it chooses bases for each one of these vector spaces in such a way that uh the basis elements are either sent to distinct basis elements To distinct basis elements, or distinct basis elements, if they go to the same thing, that thing has to be zero. Okay, so that's the that's what the basis in Gaussian elimination is doing for you, right? Or the pair of bases you compute in Gaussian elimination. So if you look at sort of this evolution of cohomology features, this is called the barcode, not only because it looks like a bunch of horizontal bars, but because it uniquely determines Because it uniquely determines the isomorphism type of this family of vector spaces and linear maps. Just like the dimension of a vector space determines the isomorphism type of the vector space. Okay, so just to give you a very concrete example of how this works, here on the upper right, you have a torus and you have the cohomology of the torus. Again, in dimension zero, Again, in dimension zero, you get one copy of the field, dimension one, because the torus has one connected component. In dimension one, you get two copies of the field, two one-dimensional loops. One is the one that goes vertically, the other one is the one that goes horizontally. In dimension two, you get one copy of the field because the torus is a two-dimensional object that bounces a three-dimensional empty ball. On the lower left, I'm showing you a I'm showing you a sort of a uniform sample from the Taurus, and I'm computing the barcodes as I described earlier. Think of these lines as compatible bases being chosen as you expand the RIPS parameter on this point lab. And what I'm hoping you see is that the number of long bars coincides with the dimensions of the pohomonic. Dimensions of the hypohomology. And there are theorems one can prove to the extent of: you know, if my sample is near my manifold and near has to be quantified using the geometry of the manifold, like the curvature, then the cohomology can be inferred from the long bars. Okay, so there are theorems like that. You know, they make you feel sort of warm and fuzzy, but you cannot use them in practice. In practice, unless you know something about the manifold. At any rate, another way to visualize the barcode is as what is called the persistence diagram, where each bar is replaced by a dot in such a way that the beginning of the bar is called the birth, that's going to be the x-coordinate, and the end of the bar is going to be the y-coordinate, the death of the topological feature. And then length of the bar is equivalent to vertical distance from the diagonal. Okay, so before we were looking at long bars to tell us sort of the true topological features underlying the data, now we're going to look at dots away from the diagonal. Okay, here are the little duckies again. Duckies again. So, you remember how the little duckies went around a circle? Then, hopefully, that is captured by the persistent diagrams, where in dimension zero, it is telling you you have one strong feature connected. And in dimension one, it's telling you you have one strong feature. Again, that's the circular feature that you're looking at. Okay, so back to the Albanese map, but for data sets. But for data sets. So, this is part of a paper we published last year. And here's the story. The idea is to take a persistent cohomology computation where X is your data set, and you want to turn this persistent cohomology computation into a map to a torus. And you want to do that in a similar way as the Albanese map does. map does. On the left, again, R is the rich filtration, and then Z map P is the integers modulo P, and then P is either a small random prime or a large prime. This is so that these classes you're finding here behave like integer classes. Okay, so secretly, these are going to be sort of the, are going to be used to construct the basis for my lattice. Okay, let me tell you. Okay, let me tell you something a little bit about this map. Okay, so if you look at each coordinate, each one of these L coordinates is given by a function f eta i, where again, these etas come from the cohomology classes I computed in the persistent cohomology calculation. So, if you give me a cohomology class, now integer cohomology class. Integer cohomology class, then the procedure goes as follows. First, you find the harmonic cocycle representative of that cohomology class. And this is actually something pretty easy to do. What you're finding is the tau such that this has the smallest L2 norm. Okay, so it's the same thing you do in like Hutch theory where the harmonic representative is the The harmonic representative is the one that has the smallest norm. And then the way the map is defined is as follows. For a little B in one of these open balls, you plug it into this formula. And what one can check is that this is equivalent to integrating omega along a path. So again, it has the same feel of the Albanese map, and it has. Of the Albanese map, and it has the same property, meaning that if you look at all the etas and you pull in and you look at the isomorphism at the homomorphism-induced incohomology, it preserves the topology. It's not harmonic anymore because we don't have Riemannian structures or anything like that, but still topology preserving. Okay. So this is this little document. This little dot with a lot of shaking is just the images shown in the order in which they appear in the data. Okay, so random. But again, remember that we have one cohomology class that we can turn into a map to a one torus. So what I'm going to show you now is the video, but now the image is come ordered. Some order as given by the angular coordinate. So, my point here is that this function is now capturing the circular coordinate that is supposed to parametrize the data. This is another data set that we saw this morning. So, again, the little Yoda is rotating. It is doing 310 rotations. 10 rotations. And at the same time, the dog is also rotating, but it does 450 rotations. And this is the persistence diagram one can compute from this input data set. And hopefully what you're seeing is one class in dimension zero connected, two classes in dimension one, so two holes, and then one class in dimension two. So consistent with the torus. And if you think these two And if you take these two classes and do the sort of toroidal coordinates thing, you get this picture. So hopefully what you're seeing is that the horizontal direction is Yoden, so Yoda rotating, and the vertical direction is the dog rotating. Okay, so I have five minutes. We're going to go over. We're going to go off-road. Okay. And whatever, whatever happens, happens. So if only I had those powers. Okay, so hopefully you're seeing some code now. Okay, great. Okay, so I'm going to run through. Okay, so I'm gonna run through sort of a maybe a toy example, but I think it's a cool example. So this is a sampling of something called a moduli space of unilateral equilateral pentagons. So this is the space that we're sampling. So it is pentagons on the complex plane, such that the side lengths are one, and you think of them as two rotations. And you think of them up to rotations and translations. Okay, so that's the data set. So, just to show you some of those pentagons in real life, so this is how they look like. The color is supposed to encode the energy of the pentagon, meaning the rounder it is, the lower the energy, the darker it is. And the more jagged it is, then the more. Is then the more light it is, higher energy. Okay? So that's the thing. I'm picking one pentagon pair equivalence class. Again, up to rotation and translation. Okay, so this is what happens when you do a principal component analysis into R3 for these data sets. Again, each dot is a pentagon and the color. A pentagon, and the color is the energy of the pentagon. I'm gonna go ahead and rotate it. It's a pretty picture that it is. Does anyone have a guess what space this is? What manifold? Something, something. What I want you to notice is that there is splashes, there is splashes in. There is splashes in color, like there is a red here, and then it goes into purple. So, what I'm just telling you is that you had a space at the top, the real manifold, and then when you projected with PCA, it got jumbled. So, a bad projection. Maybe PCA is just not the right rule for the job. So, I'm going to go ahead and run ISO map using a sort of a self-sample so that it runs in reasonable talk time. And here is the picture from IsoMob. That's even less helpful, I would argue. I'm going to run LME, because this is an analysis conference. I should have run like harmonic locally linear embeddings. And you get this thing. Again, pretty, but not. Again, pretty, but not super useful. I'm going to go ahead and run the persistent cohomology computation on this data set. So there's like 14,000 pentagons. Everything is happening in real time. So what happens in the end is that there are eight one-dimensional features and one two-dimensional feature. Okay? Eight one-dimensional features, one two-dimensional feature. Features, one two-dimensional feature. So the guess would be that this is a surface of genus four. So four tori sort of glued together. That's the hypothesis, at least. So I'm going to go ahead and run the toroidal coordinates. That's going to take me to the torus in eight dimensions. That's the first many number. But I'm going to project that down carefully. But I'm going to project that down carefully into a three-dimensional forest. And this is the picture that emerges. So hopefully, so here's what is happening. So this is R3, so the box, but remember that this is periodic, meaning that the left and right-hand side are identified, the top and bottom are identified, and the front and back are identified. identified. That means that these glues here, that these glues here, that's two handles, these glues there, three handles, and then in the back you get the fourth handle. So that's again a four dimension, a sort of four tori glued together. With a little bit more code, you can actually look at what the pentagon is. At what the Pentagons are doing. So, for example, here, maybe you can get in there and really look at the pentagram. Would they show it up? Yes. Just to try to understand how the pentagrams are, how the surface of genus 4 is parametrizing these pentagons. My scale. What am I showing here? Yeah, I want to go back to the presentation. I really meant off-road. Okay, so just to finalize. So this idea of mapping into spaces whose topology is useful, sort of to disentangle the data. The data again can be can be helpful. And it's part of a larger story. And this is what we call dry maps. So when you go beyond dimension one, the target spaces are no longer tori, but the target spaces are these things called Eilenberg-Macleany spaces. And what you get is sort of similar theorem saying that these maps right here are going to preserve the topology. Preserve the topology of the underlying data set. It is also a library, and we have a paper in the Journal of Open Source Software. And the demos that you saw are essentially running this thing. That's it. Thank you. Low. One, zero, one, two, maybe three. Yeah. So the, you know, the question is, do you want more? I mean, maybe you do. Maybe you do have a reason to believe that your data is a manifold of dimension 10. If that is the truth, then you need a lot of data to release. You need a lot of data to really span out that space. Maybe you do have it. But at least I haven't seen those examples yet. And there is sort of work to be done to try to approximate the higher dimensional points as well. But I think it would be useful to have examples to motivate that theory. In this example, you're fixing all the lengths that you want. Correct. I have to imagine the version of the noise that manifests. Yeah. In the length, yeah. More generally from the point. Yeah, so here's the theorem you can prove. If you have two data sets and they are close in Gromov-Hausdorff distance as metric spaces. As metric spaces, then the barcodes are close in terms of the number of long bars you're seeing and the endpoints of those long bars. And once we're there, the sort of the pipeline follows. But you have to start with that kind of nuts, right? But if you have now two point clouds, and then you take one and you add an outlier way off, that messes this up. On the manapool side, you've got onto clients, harmonic one part, right? Yeah. Yeah, so what you do is So, what you do is, so what is a cocycle? Let's say a one cocycle in the simplicial world. So, it's a real number on each edge, such that on triangles, one edge plus another edge gives you the third one. That's what a one-co-cycle is. So, you take that function or that vector, essentially, where you have one value per edge, and what you're trying to find is the thing that is cohomologous to it that has the smallest L to normal. The smallest L2 norm. Again, just think of it as just as a vector and that has the smallest Euclidean to norm. And that's a least squares problem. A linear least squares problem, because delta zero is a linear problem. And I think that what you're thinking is that on the PDE side, the theorem you want to equate is that harmonic maps are the ones that minimize sort of the L2 norm within their class. Without within their class. So you replace that optimization problem. Now, you would do it per cluster, like all of this happens per connected component. But the cohomology tells you that, because H0 is telling you what are the connected components. 