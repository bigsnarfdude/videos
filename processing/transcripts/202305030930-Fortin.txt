Sometimes it's over my head, and that's okay, but this has really been useful. It's awesome. My goal here is a little bit of an agenda. I want to convince you that you should be studying, you should be using rat data a lot more than human data. And the argument is multiplefold. The first one, rats are cuter. Sometimes they're smarter, but they get much, much, much better data. Okay, and I'll show you some of this. Okay, and I'll show you some of this kind of thing. Okay, I understand human conditions, we're all humans, we want to study humans, but the rodent data is a lot better, okay? And you should open up a little bit, okay? What I'm focusing on today is LFP, so local field potential activity. So in there, I'll talk about some of the work we've done using kind of decoding ensemble activity, like spikes. There's a lot of work on spikes. And there's a lot less work on the local field potential activity. And I think that's where there's a lot of activity. Field potential activity. I think that's where there's a lot of model development that could be done. Talk about that. I prefer to use my own laptop because some of the videos don't show up sometimes and keynote doesn't act up a little bit. So I apologize for that. Does that have to stay there? Yeah. Yeah, yes. Perfect. Thank you. All right. We're back in business. All right. So. Back in business, all right. So, just to put that work into context a little bit, so I would say that the main question my lab is trying to answer is: how does the brain support our ability to temporarily? How do we remember things in time? And I'll give you examples of that. So, this is something that neuroscientists know, but it's probably not something you're super familiar with, but you may know this as a kind of a factual thing that our memories are spatially and temporally organized. So, when you remember a specific experience in your life, it usually comes with information about where it happened, but also when it happens. What we call the spatial and temporal context. What we call the spatial and temporal context. A lot of people are studying the spatial context, where locations kind of thing. We tend to focus on the temporal aspect, the time aspect. And specifically, we tend to focus on what we call the memory for sequences of events. How do you remember the specific order of things, right? And that capacity is critical to daily life function, to our daily life, right? But it's also impaired in many cognitive disorders. So basically, this is one of the first things that's impaired in Alzheimer's or in aging, right? It's a challenging problem for your brain to solve, and it's one of the first. Challenging problem for your brain to solve, and it's one of the first things that fall apart. So it's a really important thing to study. We know very little about how it works. The example I usually give people is like, how do I remember last week, like on Wednesday, that's the series of meetings I've had? So these people in my lab, and this is a collaborator you may be familiar with, but essentially, how do I remember that on that day? That was a series of meetings I got. Okay. And essentially, it's really the question is like, how do we remember that things happen in that order? How we remember that things happen in that order? And more specifically, what are the neuronal representations and computations supporting this capacity? What we call neural mechanisms. Essentially, if you think the analogy of a computer, like how are neurons representing that information and computing that information, right? So these are the kinds of things we're interested in. It's a very, very detailed level of analysis. So we primarily study rodents, but we do work in humans too. So I should say that we're approaching this from a cross-precise. Say that we're approaching this from a cross-species perspective. So, rats and humans, and where the advantage that gives you, you can leverage techniques across species, right? So, we have this kind of a task, and I'll tell you a little bit about that in a minute. So, we have rats do it. We can have people do it. In people, you can do fMRI. In rodents, you can do a lot of techniques that I'll talk about in a second. Okay, so there's an advantage to having this cross-species approach. But today, my goal is again the bias is to really focus on rodents a little bit. And there are massive advantages to rodents. Massive advantages to rodents. So you get really high precision imaging activity data. You have the ability to manipulate circuits. So I can record from this structure and then turn off the structure that projects to it at the millisecond level. You can't do anything like that in humans. You have access to many disease models. There's a decent amount of funding. You may be ignoring. And I would say that the last bit is probably what would be more appealing to most people. Like there's a great, I would say there's a greater need and potential. There's a great, I would say there's a greater need and potential impact for advanced statistical models in that area. Because you're all doing human stuff, there's not a whole lot in the road in sight of things. But obviously there are disadvantages. Nothing's perfect, right? And here, like something that you may not be super familiar with, like it's really difficult to get that data in rodents. Yeah, you get better data, but ultimately you don't have 25, 50, 100 subjects. You're always gonna have fewer subjects. Well, that's it. You're gonna have data from a lot more cells. From a lot more cells, a lot more trials, a lot more everything. But your end is your end of subjects is always going to be smaller. So, if you're a reviewer in a statistical grant and they're rodents and you have five to eight subjects, that's not weird. That's normal. But you're not going to have massive amount of subjects, kind of what you're used to. So, again, it's a different way to think about it. A lot more data from fewer subjects. It's more difficult to study complex behavior. Obviously, if I give you a task, I can tell you what to do in the scanner. I can tell you what to do in the scanner, right? Rats are like, what do you want me to do? You have to train them through steps. So there's a lot more work to do. But as again, that's a commitment, but it works. The disadvantage, it's always the disease models in rodents, they're models, right? So if you study Alzheimer's in people, you know the person has the rheopathology, right? Well, later. But in rodents, it's always a model. So that's their disadvantages there for sure. Okay. So yes, nothing's rosy, but I think in terms of getting to neural mechanisms. But I think, in terms of getting to neural mechanisms, you really need these kinds of techniques. And I would make it a case that's really something you can only do in rodents. Does that make sense? I'm not going to go through all of that, but the idea is like much of it, I would say that much of the technical development in neuroscience is in rodents. There's only so much you can develop scanners right now in fMRI. You can't double the Tesla. You can't just, there's only so many things you can do. The omics stuff, like, yeah, that's actually, it's both in rodents and in humans. It's both in rodents and in humans, but you can see a lot of the different techniques, like EPHIS, which is what I'll talk about, all the calcium imaging and like all brain clearing. A lot of this stuff is done in rodents, and there's a lot of NIH money going there. So a lot of technical development imaging is in rodents. But these technical advances are worthless without advances in data science because neuroscientists don't know what they're doing. Okay, so I honestly don't. So I'm not going to go through all this again, but I'm trying to emphasize is the Go through all this again, but what I'm trying to emphasize is the top. It's basically like, we're like, oh, we have this new technique that doubles the number of neurons you can record from. Yeah, let's do that. Let's put all this money in that. Well, what are you going to do with it? Like, if you're going to do the same thing you would do with individual cells, you're not getting emergent properties out of this, right? So neuroscientists have been slow to develop that because we're not trained the way you guys are trained, right? So ultimately, there's a failure on our part there. And we're dealing with highly multi-dimensional and highly dynamic data. And I'll show you examples of this. Dynamic data. And I'll show you examples of the dynamic data. If you're looking at this time window that's too small, you're not going to see the effect. If you look, you have to average out over too big a period, you're not going to see the effect. So you have to find a sweet spot, right? So it's highly dynamic data, and that poses a lot of challenges. All right. So that's just kind of setting the stage a little bit. So what I want to do is kind of focus on three parts. I'll tell you about our experimental approach to study the memory for sequences of events. Studied the memory for sequences at events, the example I gave you. Then I'll focus on our main scientific questions and analytical challenges. I'll give you an example from a recent paper where we focus on ensemble cells, ensemble-like spiking activity. And the idea is just to give you a sense of the questions. Okay, don't worry about the models there. And then we'll try to focus on LFP a little bit because again, I think this is understudied. And I'll try to put up a few questions, kind of maybe for the discussion, can come back to you. Okay. All right, let's talk about the approach. Let's talk about the approach. How do you study something like this in rodents? Right? Well, rodents are really good at odors, that they just can remember all these different odors. So these are the kinds of stimuli that we give. So in people, I would give you words or images or whatever. Rats, you give them odors. They're really good. So essentially, the way we train them, and it's complex, we don't need all the details here. We kind of train them by stages. And eventually, when the day where we're trying to record the data, what they know, they know that, okay, this is. What they know, they know that, okay, there's a sequence of orders A, B, C, D, E. That's the sequence. And they get it many times during that session. And just to know that they know the sequence, sometimes we mess with them. Sometimes we mess the order, the sequence, right? A, B, D, right? If they know the sequence, they should know it should be C, not D, right? And they have to tell us, I think this one is out of order, right? So you'll see a video in a second, but essentially, the idea is like we present the same sequence many, many times, A, B, C D, E. Sometimes we mess with the sequence to make sure they know it and they pay. Mess with the sequence to make sure they know it and they pay attention, right? But for each item, they have to tell us whether they think it's in the correct order or not. So if they think it's in the correct order, they stick to their nose, they smell, and they stay there until they hear a beep. All right, you'll see that in the video. Hopefully that plays. There you go. I'm going to mute it. All right. He's going to get odor A, and there's a beep. He gets a reward. Oda B, beep, reward. Oda C, beep, reward. Or D, beep, reward. Go to the back. Comes back. The third one is out of sequence. So that one he's got to sniff and pull out quickly. A, beep, reward. This one. Reward this one, got it right, and this one beeps put it back. Third one, again, it's out of sequence. He should pull out on that one. He's not going to. It was A, B, there's a little flicker there. Don't worry about it. This one. And he does a buzzer like, ah, you got it wrong. And then. It's wrong, and then they are cut people. I told you that, right? So he got it wrong, and then the camera is not usually there. He's like, he blames the setup. Like, he's basically looking at us like, I didn't screw up, you did, right? So they're really cute. They're kind of big too. So they're bigger than mice. You can actually put more stuff on their heads, essentially. All right. Does that make sense in terms of what they have to do? All right. So you can train them to do the task, and eventually you can implant electronic. And eventually, you can implant electrodes in them. So, these electrodes-that's the old-school way of doing it. This is kind of a micro-scope version of it. So, we used to do these things by hand. So, like, you grab each of these electrodes, you twist them, groups of four. Like, each electrode is 10 times smaller than human hair. Like, they're tiny. They're normally to work with. But essentially, through surgery, you put them in there, you put them in the brain, and gradually they don't feel anything. So, under surgery, but later on, you can bring the electrodes down, and the brain has no pain receptors, so you don't feel anything as you drive down. So, a train amount of tasks, you implement. So you train them on a task, you implant, and then over a few weeks, you lower the electrodes. And once the electrodes are where you want, you collect data. Let's do it. And then you run them in a task. So you record neuroactivity data as they're doing the task that I just showed you. Okay. Over the years, I'd say like the technology has kind of gotten better and better. And right now, you can actually record a lot of cells from the task that I just told you. So it used to be that you have all these different things coming out of the thing on there. Different things coming out of the thing on their head. But now we can actually put a little gizmo that's about this size and it records to an SD card. The thing that you have in a camera, it records to this. There are no wires anymore, right? So you can record to this. It doesn't affect their behavior and everything. So that helps the behavior. We get better behavioral data this way. And this is the kind of data we're getting. Okay, so these are spikes, right? And this is just one LFP trace. So essentially, we have a lot of LFP trace. I'm just showing one in this case. But in this, you were. This case, but you and this you record from anywhere between, and that data set, anywhere between 50 to 100 cells as the animal is doing the task. Okay, every millisecond you have information about whether or not the cells fire. That's only one presentation, one sequence, they would get dozens of those in one session. Okay, so it's a massive amount of data. What we're trying to do now, and again, this technology advancements a lot, is like the next technology is really these silicon probes. And what we're doing now is like on the order of 256 channels. On the order of 256 channels, what I'm assuming is only 64, right? So you stick that essentially instead of making the electrodes yourself, you put them in the brain region. So in this case, it's through the hippocampus. And from each of these contacts, you can see the little blue things. So these are all like 64 channels. You record, you know, the millisecond level precision, the LFP, and on top of it, little spikes. So if you zoom in, you can see the spikes on top of the LFP, right? It's what we call wideband. You're not filtering for spikes and putting. You're not filtering for spikes and LFP separately, you just report whiteband everything. So you get LFP and spikes on top of it. So that's the newest thing, and that's what everybody's moving to, actually. All right. Does that make sense? Technical side of things? So you don't insert it vertically, but we also insert it something. You can put them where you want. Okay, so the deeper in the brain, the harder it is, right? The brain, the harder it is, right? But I, and you can do angles, and then the thing is, you want things to be sticking up usually, so you don't want something to stick out on the side of the animal because they'll bump into it basically. So, there are limitations about the angle you can go in, yeah. All right, let's talk about some of the scientific questions, but yeah, learn the tasks before you study for you usually yeah once they know the rules we can learn the next day so once i collected data on this sequence i can give them a new sequence right and then that they learn a day so what's hard with a rat is basically like all right there's going to be a port there stick your nose there and then it's going to be multiple oh there's not learning the rules of the task that's what takes a long time to teach them but basically the next day after like let's say i showed you ab C D there next day I showed you ABCD there. Next day you can present EFGA orders you've never seen, right? And within a session, they'll learn that that's the sequential relationship between those new orders. So you can look at new learning just the next session. So we collect data multiple days. Once you do all this work, you want to collect as much data as you can. So yes, we can look at new learning. Yeah, I'll talk about that in a second. So there are different probe types that are probably more suited for. Different probe types that are probably more suited for different regions. In our case, we tend to focus on the hippocampus and prefrontal cortex. These are structures that Michael, at least the hippocampus, he mentioned. Those are memory structures, right? But you can go anywhere, really. Go back to the third part. Are you getting multiple measurements by each time for a custom time? Yeah, so I think those things usually sign up for like 30. things usually sample like 30,000 hertz basically so like just you get a super very fine precise kind of thing right so we usually downsample it just to deal with one kind of one data point per millisecond but we can go a super five super high rate and the issue is like just people like oh we can acquire that data but then you have gigabytes of data for every session it becomes a storage issue yes yeah remember or not but i mean yeah i know Yeah, how much space on the button people are here, yeah, yeah. So it's basically it's a lot of high-res data, too. Like this is beautiful, right? So, and that's yeah, that's only a few hundred milliseconds. So, on the story side of things, that's you know, we have to worry about servers and everything. So, but I think you can downsample things and you can deal with a more manageable thing. So, you can say, I'm only going to look at spikes, or I'm going to downsample this a little bit. So, you can deal with files that are reasonable. The raw data is what's massive, right? And that's that's. Right, and that that's that's the storage issue. But for you guys to deal with a file, it would be reasonable. Sort of smaller than FMI. These are great questions. All right, let's go over just some of the questions. So we focus mostly on your hippocampus, the curved structure and prefrontal cortex. Yes, you have these brain regions. Okay, rats, the brain is very similar to yours. So hippocampus, very, very, very similar. This is remarkable. Prefrontal cortex, it's a little bit. Remarkable. Prefrontal cortex, it's a little bit different, but a lot of similarities. So, yes, you have those. And then it's not super important, the details and everything, but basically, our working models that these different brain structures that we know the epicampus and prefrontal cortex are important for that task I just told you about. So, if you inactivate them, to turn them off, the animal is impaired, right? So, we've known this for a while. But, like, how are these structures doing this? Like, what are the representations, computations they're doing? So, we think they're just kind of making different types of computations. So, in the epochanical side of lanes, and you'll see some. and the epochanical side of the zanes and you'll see some of the examples I'll tell you I'll tell you about is that okay if you have a sequence A happened B happened and then C and then D. So they're widely separated in time. We think hippocampus is good at grabbing that information, condensing it, that A leads to B, leads to C leads to D. So this is what I'm showing you up here. And prefrontal cortex does something else, and I'm not going to go in details of that. What I should point out is like these two structures, they're in the middle of the brain, so they don't do anything on their own. Order brains, so they don't do anything on their own, right? So, we work in this clear functional relationship with a lot of brain structures. So, that's the hippocampus, even within there, there are a lot of anatomical structures talking to each other. And outside of that, that's a prefrontal cortex, but there's also connection with a lot of brain structures. So, we're really talking about a system working together. And you can stick electrodes in all these different brain regions to see how they're talking to each other, okay, and get that millisecond level resolution. Okay, so. Okay, so broad level, these are the kinds of questions. So I'll tell you kind of a recent example in terms of where we're decoding. I guess we called us decoding. You guys called it predicting, I think, in which we're decoding the represent neural representation from ensemble spiking activity. So that's published like a few months ago. But yeah, I think don't worry too much about the models. I know it's hard for you to think because you think about the models much more than we do. But just kind of, let's focus on the questions a little bit to give you a sense of what we're trying to do. Okay. So it kind of To do okay, so the kind of questions we were interested in that paper is basically: you know, we just know the hippocampus, the brain structure I told you about, we know it's important for that kind of task. But how does the hippocampus do this, right? How does the hippocampus represent time within events? So, how does it differentiate distinct types of task critical information, right? How does it represent these sequential relationships like A, B, C, D, can they compress? And how is it compressing all that information within individual theta cycles? Individual theta cycles. It might make a little bit more sense when you see it. But these are the types of questions we're interested in. Some of the challenges is that hippocampus activity tends to be sparse. So brain structures tend to do things a little bit differently. The hippocampus is that the cells don't tend to fire a whole lot. And then you fire a little burst or then you fire a little few spikes. But overall, they don't fire a whole lot. Different brain structures will do vastly different things. And now it's. And the non-spatial coding, the type of coding I'll show you about, it tends to be kind of subtle when distributed in individual neurons. Once you have the right dimension to show it, it's clear and it's there, obviously. But to the naked eye, it's not super clear. Okay, so there's definitely work to capture just the right dimension. And as you'll see in a minute, it's highly dynamic data. So you have to find the right time scale to look at these things. You're not, you're not going to see anything. So it could be useful as you are many neurons, you're referring to many neurons. And when you talk about spice, I think we can fire this per neuron or across neurons all you need and fight and the rest of the resting or that's a good question. Both. But the one I was referring to is the few spikes per neuron. Few spikes per neuron, basically. But there are a lot of neurons that you know are there because you can see them sometimes, but in your tasks, you're like, eh, not interested. There's not going to do anything. But I would say that's less than we thought originally. And I'll show you, like, we've been able to pull them out. Like, they just fire a little bit of moment, but they're consistent. So it looks like, ah, you're not interested, but you are. You fire at a specific moment. But overall, what I was referring to is like, in general, they tend to have a lower firing rate. All right. So. All right, so first thing we looked at is kind of how does DPCAPIS code for time within events, right? And for this, we use a kind of a Bayesian time decoding model, and I can tell you later about this, but it's a standard type thing, and I'll show you in a second. Basically, what we found is that the ensemble activities organize these sequences of firing fields. So this is a hippocampal activity, and probably don't need all this. I'm going to try to block some of it, right? So what I'm showing you right here, this is just the raw data. It's just to visualize it. So in the columns and rows here, these are all. In the column and rows here, these are all the cells we record from at the same time. This could be like hundreds, right? And then this is basically during presentation of order A. This is time during order A, right? So all order A presentation averaging on top of each other, and I'm showing you the activity from all these different neurons. And we're normalizing it. Red is the peak of that neuron, and blue means like low firing, no firing. And I hope you can see this. And here what we're trying to do, we're sorting them by their peak firing during A. them by the peak firing during A. So this cell right here fired really early during A, this during the middle of A, and this toward the end of A. Okay. So if you don't visualize it just right, you're not going to see this. You think, oh, each cell doesn't fire a whole lot, but they're consistent. They'll fire like this cell consistently fire at that moment during A. So it's very, very, very specific. Correct. Yeah. So there, but yeah, it can't be perfect every time, but on the average. Perfect every time, but on the average, yeah, you can see it pretty consistently. Yeah, so on a trial-by-trial basis, if we showed a raw data, yeah, it's pretty consistent. You can see it, but it's not a perfect prediction, no. It's not every single trial and every single time. There's a lot of probabilistic stuff in there, yeah. And do you see that as a problem? We see that just totally normal for us. All right. And then what I'm showing you here is essentially all the same cells, but I'm resorting them by their time in B. And all the same. In B, and all the same cells, we sorting them by the time in C and in D. And it's just to visualize the data so you can see the sequence of things in red. And what I'm showing you on the right, that's just the Debayesian time decoding model. That's really the real result. So is it open that a lot of those are the only series out of every place? Is that humans? Yes, so you can see it. Oh, I see that. The model doesn't care about any of that. But if I'm not sorting them like this. But if I'm not sorting them like this, it just looks like a jumble mess to you. No, I mean, the actual portable model when you have the model is actually on the various people. It's only when you do a comparison, like the other, if I show you the other plots, maybe it's of interest. But there's no like neuron one, there's no, I can't put in the brain that's this neuron. I can't do that. Yep. Yeah, I'm resorting them, yes. And against. And again, it's just sort of weakened visualizes. And on the right, that's just a Bayesian time-decoding model. He's basically training him on the activity doing A, doing B, doing C, doing D, right? And you say, here's a pattern of activity. What time do you think it came from? And it's just really good. So you can feed any pattern of activity over any specific time and it decodes very, very well that specific time. There's a lot of time information that's ignored. I mean, you can do this by eyeing, right? Say, if I tell you this cell right here is active, you can pretty much. Here is active, you can pretty much take a good guess that it's probably at that time during C, right? And that's what it models does. All right, so far, people have shown this before, so that's not super new. What is new, and that gets to your question a little bit, is now that time cell coding, what we talking about, time cell varies with the position. So hopefully what you can see at the top over there is like that's all cells sorted by defiring during A, during A. If I'm grabbing. During A. If I'm grabbing those cells and not sorting, and I'm looking how does this look like during B, during C, during D, you can see the vastly different. Okay, so there's a big change from here, and then more of a change, more of a change, right? So in that sense, they're not time sales in general, but they're time sales during A or time sales during B or C or D. And I'm not going to go through the lag effect, but what it means is basically the further you are from A, the more different it becomes. And you can use that as kind of And you can use that as kind of a sequential lag information. But can we say that B and C are similar and that's interesting too? Yep, yep. And what's really cool is this one. So we're extending the model now instead of looking during each order, we're essentially just kind of sorting all the cells. And now I'm kind of averaging out all the presentation of the full sequence, like A, B, C, D, the whole thing. It's like 30 seconds long. It's crazy, right? And we're doing that. And again, Right, and we're doing that and again averaging across all those sequences, you can still see kind of a little, you can visualize like a little sequence there, right? And essentially, you can actually decode time in the whole sequence very, very well, right? In the sense that if I give you a pattern of activity, the model said, oh, that's between B and C, like 0.2 seconds between the two. So there's a lot of time information in there. And the reason why that's super important is basically these time cell signals, people are thinking, okay, well, that's what allows you to keep track of where you are in a series of events. Where you are in a series of events, right? So, like, oh, I did this, I did this, I did this. I'm here in the sequence of things I'm trying to do, right? That keeping track of time kind of thing, this is what this signal seems to be telling me. So people have never seen this like in between, like in a whole sequence of events. And that's what we thought was very important. That's just one aspect of the things we're looking into for time stuff. Does that make sense? Yeah. You're saying that there is specific solvent CSC. There is this sort of sequence of neurons here, right? There is a sequence that either has the entire trial going on. I can't tell if you think that's interesting or like uninteresting, but yeah, no, that's uh so all these. So, all of these neurons are from the same brain region, they're not necessarily next to each other, but as a group, that ensemble, they'll just have different moments in time they tend to fire. But as a whole, you bridge the whole time period. They're not necessarily next to each other. They're not, I'm firing, I'm going to make you fire. It's not just like that. Yeah. Yeah. So basically, the files are. Can you define some kind of causality between groups of norms or granules? Can you define that based on this scale data? I would like to. And that's going to come back in the questions at the end. Basically, can you have a predictive power? And we've done some stuff in the paper saying, if that coding is weaker, the animal is more likely to make a mistake kind of thing. But it was not really a model of causality as a whole. And we would like to do that. That would be something that you would just say. And then we can analyze that data and see what it means biologically. See what it means by graduate. Okay, yeah. So this data, here the analysis, should you do it with one rat only, or do you do it with like uh so this is we do it individually per rat, then each rat shows you this, and then we also collapse the data so you can see all as a group. And there are times you can collapse data across subjects if they were presented with the same thing. But if you're in this technology, how can I say that with a food that goes by the same? Put that grow in my nerve, right? It's on the link. Yeah, so if there are some animals, like a leech or whatever, a cockroach, you can record from those. And that neuron is going to be the same between all these species. Like that neuron, they all have the same neuron. In your brain, in a rat brain, apicable neurons, like I can't say that this is neuron 17 and this is the same neuron over there. It's completely different. So you have to kind of let go of that mindset. So these brain structures, it's like neurons, they're all hippocampal neurons, but there's no. They're all hippocampal neurons, but there's no perfect mapping from here to the next subject. Anatomically, there's a range like from this side of the hippocampus to the other. They all have similar properties, but you can't say this neuron, you can't track this neuron across subjects. It's not a thing. Yeah. I'm not sure what this has to do with the order with this. What do you have to have? You represent the filing, whatever you don't record it, you can always do this sorting, right? You see, it's sort of the cells already done in terms of when they fired. Yeah, so all right, this is average. So I don't want to spend too much time on this because I think it's important, but I have a lot more to show. So I'm going to move on, right? But we're not going to talk too much about that after this. So, of course, like with any period of time, you can sort cells by their peafiring rate, and you can always get a sequence. What this shows is basically. Sequence. What this shows is basically on average, it's pretty much the same sequence, pretty much the same neurons. So basically, over like hundreds of OTA presentations, you get a similar sequence. That's not happening by chance. So if I'm looking at one exposure, of course, you can sort things, right? But if many repetitions of the same thing and you average it out, you see a clear pattern like that, this is not by chance. There's something consistent about that stimulus, triggers that series of cells to fire in some way in that pattern. We in that pattern. But no, these are great questions. All right. Don't bother us because it will be looking at some of the discussions on the finished response. Oh, that's fine. No rush. You should not have said that. No, I was trying to keep good track to time. So I'm sorry. I was not trying to rush. I was trying not to be bad guy. This one, I mean, I'm not going to spend too much time on this. You can also just kind of feed the activity. You can also just kind of feed the activity, essentially the neural activity, to another encoder. It's like, hey, find patterns in there, right? Kind of an unsupervised way. And in this case, it's just basically the same data you just saw, doing order A, B, C, and then you just feed it to a network and it just finds clusters, right? Like, hey, at that moment in time, I see separation, right? It doesn't, the model doesn't know what they are, but then we can color code it and be like, oh, yeah, that turns out to be order A over here, or the D over there, that got separation. So we've applied some of these kind of unsupervised way to kind of cluster data. Of unsupervised way to kind of cluster data and see what's going on at different moments in time. So, I'm talking about this now. This is really complicated. I'm not going to go through it, but this latent state thing can come. We'll come back a little bit later. What I do want to tell you about is this one. So, in this case, we use a CNN-like convolutional neural network to decode the content of the activity. So, I'm not looking at time now. I'm looking at a specific moment in time, what odor is being represented right now. Is being represented right now. Okay, so that's what we're trying to do. And how we're doing this is basically like this is the model training period. So I'm grabbing that moment in time, like during A, that kind of time window, like you train the model, this is A, this is B, that same time window. This is C, and this is D, right? So you train the model, this is what the activity looks like. And now you again, same principle, here's a pattern of activity. What does it look like? Does it look like B, C, D, whatever, right? What I'm showing you up here is kind of just the latent representation. I'm showing you up here, it's kind of just the latent representation of one subject, right? So, this is way before an ODC presentation, right? Like around that time window, right here. And each dot shows you what did the activity look like on that trial in that time window, right? Each dot, you can see kind of a blob in the middle. There's not like if it looked like B, it would be up here. It looks like C would be up there, A over there, right? You can kind of see a blob in the middle, right? So that tells you it is not really looking like any of the odors at that time point. Like any of the odors at that time point, which we expect that that's kind of a sanity check. The next one is where it starts to get interesting. Okay, so before the animal was about to get over C, what does it look like? So this is data from one radical start. The centroid is in B over here. I forgot to say this is data collapse across subjects. We just get the peak decoding. So each dot is a trial and it's color coded for which older was it, like the strongest decoding probability. Probability, but at that moment in time, it doesn't look like C, it looks like B actually. So it's playing, decoding the previous odor, the thing that happened five seconds ago, right? And then if you kind of keep playing it at that window right here, at that window right there, what does it look like? Well, now it looks like C, right? Of course, during C, we'd think of it looks like C. But if I keep playing it later in C, what does it look like in C D, right? Okay, so let me kind of rephrase that. So, during an ODC presentation, you saw the rat, right? They stick their nose in there and he's blasted with ODC. What his brain is telling him is like, oh yeah, previous was B, current one is C, the next one is D. It's playing that during an ODC presentation as he's getting the order. This is what I'm talking about, the dynamics, right? Not really going to decode that if you don't have the right time scale. We talk about theta associated in this case, because at that time, Associated in this case because at that time the LFP showed a very strong theta, right? Does that make sense? Yep. So I'm excited by just before the receiver, it's doing it like right in order C. But what you're describing is kind of, that was my original prediction. Like right before C, he's predicting C, right? Like, I'm about to get this. But they're trained on this. They really know the sequential relationship. On this, they really noticed the quantial relationship. This leads to this, leads to this, and people had shown this for spatial locations. So, for rats, like if I'm right here at this location, you can kind of see a sweep like this. Right before I'm here, it plays a location that's before, right? And it plays a location that's next. Something that's happening a little bit later in the future, it's playing it closer in time. Like, I'm predicting I'm about to get there, but I'm also maintaining something about where I was. So, this didn't come out of nowhere. So, it came from a very clear prediction from just the spatial literature, the place field stuff. spatial literature the place field stuff so the rad is and it's really replaying it like b c d so if i'm showing the activity for on on d trials it's doing the same thing it's playing the sequence and if it doesn't do that so here if the coding is not good basically like it's more likely to make an error but the causality thing like we would like to get a little bit better than that is it possible that there's still the bees molecules still uh red bounded Still, uh, red and uh, um, but why would D show up? Like, yeah, so one, but there are these odors are super like rats are super good at odors, they don't need remnants on their skin to be like to be like they can remember that, but in the same session, they get like hundreds of those orders, right? So, whatever thing, like it's not, if it was just once, they'd be like, Yeah, there's kind of a trace and you're using that, but uh, no, there's just there's whatever trace would be all on top of each other. Whatever trace would be all on top of each other, and we know they're not doing that. Yeah, and is the pattern of A slightly different for the others? Because there's nothing previous to A. Yeah, thank you. A is weird because you saw the animal ran and stuck his nose right there, right? So for A, like we actually can't use it because because they're running and getting there, the activity right before is weird, it's different, right? Is the theta stronger? And I just, we just can't quite decode it the same way. So, like, if you feed like any sort of decoding model, it'll tell you, oh, A. Decoding model will tell you, oh, eight, and then everything else, and then, like, that's not helping us. We're trying to look at those. So, A is different, and a lot of times it just doesn't care, like, yeah, it's not A. So, we learned that lesson. All right, so I'm going to show you essentially that this similar results, but like more compressed in time now. So, what I'm going to try to show you is essentially that the non-spatial event representation like this is sequentially organized within a theta cycle, within 200 millisecond. I know it's a Within 200 millisecond. I know it's a lot of information. I'm just gonna try to simplify it a little bit. So think back at what I just showed you. So B, C, D replayed. Okay. So now what I'm trying to show is basically doing one theta cycle right here during a given order, like we call it present order, because we have to average across trial type. In that theta cycle, right here, if we train the models during that, the trough of that cycle, the bottom of that cycle, so if we grab order A trials, B, C, D, like that time window, and we change, like these are A. That time window, and we change like these are A trials, these are B, C, and D, and whatever. And now we're decoding basically: here's a pattern of activity. What do you think it looks like? A, B, C, or D. So we use the same approach, but now it's much condensed in time. Okay, this is actually logistic regression models because the decoding is not nearly as good as the CNN, but for us, we can actually go at a much quicker time scale. Hopefully, you can see this here, but it's the idea that within a theta cycle, the present stimulus, the thing that they are presented right now, that tends to be a very important thing. Present evident right now that tends to be decoded more in the bottom of the theta cycle. That's the theory that came from. There's a lot of evidence for that. And then the past stimulus, the thing that happens five seconds before, whatever, that tends to be represented right here in the descending phase. So essentially that data cycle, if you split in three parts, descending, trough, and ascending, here's when you would predict to have the past stimulus being represented. And here's you predict to have the future stimulus, the thing that happens seconds later. That happens seconds later, right? And we kind of just visualize like evidence of that. Essentially, we're essentially in the trough here, just according to prediction, we're seeing exactly that. If it's a B trial, you see a lot of B decoding during the trough. And then in the descending phase, you see a lot more A. And in the ascending phase, you see more C, right? So ABCs compress within a theta cycle, within 200 milliseconds. And on C trials, you see the same thing, the same pattern. C in the trough, B in the Trough, B in the descending phase, and D in the ascending phase. So that information that's normally separated by several seconds, you can decode it within a theta cycle. And essentially, you're capturing a little bit of past, present, and future in these theta cycles, and it kind of moves up with you. How is the same amount of time of course, right? Do they have an idea what happens if it's not? There's a little bit of jitter for the order length, and we try to do that intentionally, but it doesn't really affect this. So, this is more an abstraction. Oh, I know that was A, whatever. Like, I don't think the length would matter too much. Yep. Is there anything inside for like from the LIP? And then yeah, so if I'm grabbing the LED, I cut an algorithm too much, but because basically I'm aligning the decoding from the spikes to The decoding from the spikes to the face of the LFP at the top, essentially. And then, like, a pinning behind that. I'm sorry? There's some feeling behind it. There's a what? There's a pin behind it. Yeah, yeah, yeah. No, this came from again the place cell literature. And the idea is that at every location that, you know, if your rat runs on a track, basically, like it codes for the current location in that data cycle, you have a little bit about the upcoming location right here, and a little bit about the past location right there. So you're compressing that information in these data cycle. Information in these data cycles. So, people have shown that for locations, yes. Are you averaging across the cycle there then? This one is averaging across like order, like that. I'm trying to try to find that same cycle, whatever first cycle after 100 milliseconds, that same cycle across these order presentations. And we're trying to average out across those, yes. Does that make sense? Yeah, okay. And we also see that that information processes through the theta cycles. This may not, in neuroscience, it's a thing. I don't know if you guys care about any of that, but if I'm looking at the code, the decoding of this stimulus, the print stimulus across these different theta cycles, the one before, early, middle, and late during that order, the stimulus will be represented late in the theta here, in the trough here, and more early to get that information. Get that information process across cycles. Go from late to more middle to more early. So that's because that's what I meant by the past, breadth, and future eyes with you essentially as you go. And people had shown for spatial. So again, I spent too much time with all of us just to give you negative questions. We're trying to go for where are you with this timeline kind of thing? How do you create information? kind of thing how do you create information like this in kind of the sequential order and that kind of thing now i want to move a little bit to uh extracting more information from the as we said like here it was all coding on some some use the lfp activity just to align spikes a little bit that made a difference that helped but the lf itself has been practically ignored in the field like we just don't do a whole lot with it and i think a lot more with it i think that's what i'm talking about What I'm talking about. Why? Well, it's easier to kill spikes. Spikes that I just have to do a lot of pre-processing. Okay, these are different cells. And then, you know, it takes a long time. Once you have it, you can analyze them, but it takes a long time to say, these are different cells. Okay. Else, just this in the brain, and you collect that. And the processing pipeline, the automatic sorting stuff is better now. You can actually have these spikes relatively easy. Of these spikes relatively easy, demand labor is cut down quite substantially. So, that on the legal side of things is easier. It's a more practical approach functional relationship among brain regions. The problem is, as I said before, you can put these different parts of different regions, different brain regions, right? But you spread your channel like this, you're not gonna have a ton of spikes from these different areas, right? So, this may not be the best, spikes may not be the best thing to do. Best may not be the best thing to look at function, right? So, usually, you kind of we kind of commit. If you're looking at cells, you stick all your channels in one structure. If you look at relationship, you'll spread them around. Okay. Then you may not collect a ton of cells, these different regions, we'll collect great fees. So, that's why I think we're looking at relationship LFPs. I was saying before, too. I was saying before, too, like this kind of an unmet science need and potential high-impact discovery on the LFP side, I think, right? This is an example from us basically. Like, the way we do it is very like a bunch of these LF faces. I'm going to focus on one of them and do a spectrum of it. And then, oh yeah, you see this little bit of theta, like, oh, yeah, beta here. You try to relate this with behavior. And it's just not a good way to work. Some, it's not a very sensitive way to look at this, right? And this is. And this is work we did with Pieribaldi and Babakshaba, actually. But if we just use the same data and just look at LFP, we can actually separate odor B and C. We can actually do a little bit of that. And this is in Europs, but essentially just using LFP, you can actually decode or predict these separate odors. And so there's enough information in there to do that. So is this like a factor model or something in the reference stuff? It's just there's something. I don't remember all the steps, but basically, it's just kind of you kind of grab the make like a lake representation of it, and you should look at trajectory within that lake representation. Yeah, yeah. Yeah, I don't remember what dimension reduction that is. Yeah. But most people said that would be impossible, but it's there. It's something there. All right, some of the challenges. LFP is complex, it's a noisy thing. It's complex, right? Look at this. Noisy saying it's complex, right? Look at this. It's just it's not super easy to find the patterns in there, but it's clearly a lot of information, right? Um, it's a little bit noisy, like, just because you don't know what's super informative, what's not, right? I would say probably the biggest problem is like it's not as intuitive as spikes, right? If you're a neuroscientist, like even people not doing this for a living, you can show them this, they understand this. That cell fires or doesn't fire, they get that. If I'm showing them this, they're like, what the hell is that? Right? Is that right? That's harder for people to interpret, and that's going to be a battle. That's a problem. So, by working on elephant, you have to do a better job visualizing it. These people, it's you know, there's not it's not super intuitive for them to look at this. So, visualization really is key there. And I should say it's true. The LFP is still the ugly duckling. Like, there's sells as if there's this dogma in the field that spikes is the real information, LFP is just this byproduct. And that's just not true. Okay, so there's a lot of information in the LFP, and it's been understudy. LFP and it's been understudied, but people realize, like, yeah, there's actually information in there. So I don't, it is the ugly ducking, but I think people are changing. There's more and more in the LFP papers out there. Okay, that's fine. That's why we're doing this. Do you model the relationship between the LFP and the spiking pattern in terms of like entity and the threat on that for you? People have tried to kind of let me answer that. I don't know if that's that question, but where does the LFP come from? That question, but where does the LFP come from? Is it coming? Is it related to the spikes here or spikes in different regions? It seems to be kind of more an averaging of incoming inputs. So it seems to be like the dendrite in the region you're reporting from, it's getting all these inputs from all these places, and it's the postynaptic potential on these neurons right here. So it's the dendrite essentially doing this. So it's kind of an averaging of all the inputs it's getting. There could be separation for segments here. Be using the version elsewhere, not here. Yeah, there's a little bit about this region, but it's kind of an index of what information is coming into this area of the electrode. That's what's actually interesting, right? It's basically like it's kind of averaging of all the stuff coming in, but it also is intimidating, right? All right. So let's focus on just some questions for discussion. I can lay them out there and we can explain. I can lay them out there and we can expand and we can talk. I'll let you run this, but oh, yes, before I just kind of tell you about the types of data set that we have. So, the current one that's available, like it's just, we're recording from the hippocampus. That's the example I just gave you about, right? So, we stick electrodes in the hippocampus and like throughout the hippocampus, you can record from different parts of one region in the hippocampus. We also have pre-frontal recordings. We can stick all the electrodes in this other area and kind of spread it out in that area. Out in that area. We also have some multi-regional recordings in which we stick electrodes at a bunch of places, like 10 regions, to see how they functionally relate to each other. And that's all in the same task that I just talked about. And what will be available soon is basically like we recorded like using these silicon probes, like we're starting to use them now so that within a year we'll should have something. And that's just that's the whiteband data. And what we're trying to go toward is just these closed loop approaches. And you've heard about that. That's kind of the next thing, right? About that, that's kind of the next thing, right? And it's the idea that in rodents, you can actually let's say you decode something, and oh, this is happening. This, I'm decoding blah, blah, blah, or this is happening in the activity. You can at the millisecond level interpret this and manipulate the circuit. So, for example, if I'm detecting that this decoding happens in this region, I can inactivate the incoming input from this other region at the millisecond level precision. It's the optogenetics, right? So, we haven't done a whole So we haven't done a whole lot of that, but the field is going toward these things, right? Can you manipulate the circle, which means you really understand what's happening? I'm predicting this would happen. If I'm turning that off, I should lose this bit, right? That's the idea. You don't have to do this in a closed loop approach, but this is kind of where it feels going. Like, just do this live. I'm seeing this happen. I'm going to turn it off. All right. So, in terms of the question, can we clarify? Can we quantify the flow of information among electrodes, but within trials? Okay, so a lot of work tends to be done: like, hey, within this session, this is the flow of information I'm seeing, but within the session, a lot of stuff is happening, right? There's stuff, there are moments when it's uninteresting what's happening, and there are moments like just cumuloseconds that that's we're predicting that flow of information very, very precisely, right? So can we do that in a way that basically like, I'll get to that bit in a second. I'll get to that bit in a second. So, yes, we can kind of quantify the flow information within trials. That's a bigger question. The issue is always, and Kelly talked about partial directed, yeah, partial coherence, the idea that you need to account for shared information across a large number of channels, right? Some of that LFB is basically like you can see some of that information is shared across channels. Well, how do you smartly account for that without losing all your signal, right? Using all your signals, right? So you have to approach this in a clever way. And I don't think there are scientists who are equipped to do that. These coherence or lead-lagging relationship can be quantified at high enough temporal resolution. Because again, at the whole session, sure, you may see something, but that's not important. That's not useful. That's not the speed of thought that we're trying to get at, right? So we really have to look at something below 200 milliseconds. Is that possible? Or is that just heresy? I don't know. Can the key feature of the signals be identified in an unsupervised way? So we've been taking a dumb approach, like, yeah, that's kind of theta and that's kind of beta. We define these bands, but I don't always know which one is going to, that specific frequency. I don't know the bands necessarily. Are they going to be exactly the same in this other task, right? Is there a way to do this agnostically, right? And I think that would be a good plus. But things like, yeah, the power specific pattern, the frequency band, the cross-frequency, frequency coupling, is there a way to do this agnostically? coupling is there a way to do this agnostically and then relate to the closed look like eventually that's not the first step but eventually can you do this online right i'm seeing this pattern and i'm turning it off right or something like that right that would be the goal but in my view the question one is really about functional relationships right the second question it relates to that what i was showing you with the ensemble activity can we do that we're probably not gonna be able to do all that but can we get to To be able to do all that, but can we get to decoding or prediction right from just LFP alone, right, or at least using some of that? And the question really is the challenge: does LFP contain enough information to do that, right? So, I showed you before that there's a little bit of separation. Clearly, there's something there, and that was not optimized, maybe, right? So, maybe there's enough information to really decode these things with LFP alone, but I don't know. And you can bind it in spiky information, get a lot more. And again, that same issue of resolution is for us, it's always going to be an issue that can this be done at a fast enough temporal resolution? Because if not, it's not something that we can use, right? And again, can it be done online eventually? This can't be the first step, right? So eventually it would be like that. And kind of question three, can we model the specific series of neural states in some way? It's kind of a dimension reduction kind of thing. Let's say order A, B. Say order A, B, C, D, like different rats have their own different representations of this, but do you see a trajectory that you go from A, B, C, D, and this subject, and this subject, it goes through its own A, B, C, D kind of thing. Can you model something like that with LFP? And that would be great. That would be super useful. So again, does LFP contain enough information to model all this? I don't know. Can this framework support? Can this framework support causal inference? There were some questions about that. Like, if I don't go through this series of states, I'm going to screw up. Like, can we get to something like this as well? So, this causal inference, NIH, that's the key. If you don't have causal stuff or at least try to address these things, they're not nearly as interested, right? So, there's definitely a big push for causal inference. And the thing I would say, like, can we maintain, oh, can we do all this without losing like explainability? Right? If you show a crazy If you show a crazy waveforms and models and this or that, and nobody's going to understand this, like you really have to find a way to visualize it just right so everybody can understand it. Statistician and neuroscientist. That's really at a premium for this. And the last thing is that can we take some of these ideas and, because again, we have fMRI using the same task, and there are similar tasks that we can do. Can we use some of the things we've developed for the LFP back into other types of time series data? Can that be applied to both? Right, can that be applied to bold fMRI for example or EGN? That sort of thing. Good question. Yeah, so on that talk, if there are any possibilities, then you do fMRI and then you see it in this way. fMRI and what? And then you need the question. Nope. So it's they're metallic. And in monkeys, they've done some of it. In monkeys, they've done some of it. It was like non-ferrous electrodes, and it's just really expensive. And what we're trying to do is not even like we're trying to use ultrasound, actually. We can kind of mimic ultra, like fMRI using ultrasound. It's actually better than fMRI in buildings. So we're trying that soon. That you can have electrodes in there. So hopefully that would work. Yeah. F nears? Not familiar. Oh, yeah, yeah, yeah. Um, that might work, yeah. But I don't know enough about it to really get confidence. All right, let me just acknowledge the people who worked on this and I can come back. So, this is my lab. So, postdoc, Gabe, Moncy, and Keelan, they've done a lot of works I mentioned. You'll recognize some of these people, right? So, these like data science collaborators, because again, neuroscientists, we can collect this crazy data, and then we're like, we don't know enough to. Like, we don't know enough to analyze it ourselves. So, we really need help from other people. And without this collaboration, without the data science and neuroscience working together, that was hopeless, right? So, I want to make that case that this is something we should be doing more. All right, so I'm going to end here. So, I don't want to take too much of your time, but does that make sense? We'll go over some of these buttons again during the second. So, I think we can. So I think we can end for now, right? Yeah, so what's coming up next? So we're going to present it for a couple of questions. We'll be doing breakout sessions split to three different groups led by Amy Fernando and Mara Carina. And just pretty much just have at it on a pick of question, let's say, and we'll discuss some groups and then we'll present some discussion over. So, one thing that the three of us have worked on, this in collaboration with David Madison, who is editor-in-chief of data science and science, we do have a special issue coming up basically related to this conference. So, if any of the ideas that we discussed during this week or things that you're already working on, can we? Can read the papers, then we have a special issue that's important to be available to that. So, something to strongly consider, and especially as a way to perhaps form new collaborations since there are people here that you probably know very well. So, I'm throwing that out there, and I will be sharing more details as it's up to time. So um but there's any um so the groups that we will be uh dividing for was in the program. Um it's different from the tracks. The tracks is the one you listed for. So the groups are organized, the programs, you can just check. And for now we are going to have a break for the coffee plate and after coffee plate we do the discussions and we feed back to the group. The discussion would last To the discussion with the last 40 minutes, and then we pick back to the general group to them.