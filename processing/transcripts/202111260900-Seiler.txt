Contain the words moving frames. The title says already what the topic is. I've been working together with my postdoc, Matthias Sais, and also with Markus Lange-Higgerman and Daniel Roberts for a fairly long time on this, because it turned out that there are many subtilities involved, and to really get all the details in the right places has taken several years in the end. So, So the word singularity is a bit overloaded in mathematics, and particularly in the context of differential equations. There are many different kinds of what people call singular behavior. What I will speak about is perhaps closest among the familiar stuff to stationary points or equilibria of vector fields. In the sense that what is happening at such a stationary point, the rank of The rank of a distribution, namely the one generated by this vector field, jumps. And that exactly will be our idea of singularities: that we have geometric structures on a geometric model of a differential equation, and there certain ranks may jump. The whole thing is part of a larger project, which has essentially two goals, and I will just speak about the first one. The first one. We follow the traditional differential topological approach to define singularities, but we also want to make things effective. Officially, I'm a professor for computer algebra, so I should bother about effective computation. And so we want to use tools from commutative and differential algebra to detect singularities and not just as in the Singularities and not just as in the traditional theory for scalar ordinary differential equations, but for arbitrary systems of ordinary or partial differential equations. What I will not speak about in this talk is, of course, the next step. Once you have found your singularities, you would like to know what's actually going on there. But I will not speak about this today here. So, I think with the audience here, not many people will need. Here, not many people will need an introduction to how one geometrically defines differential equations. You use chat bundles. So a chat is an equivalence class of functions which have the same Taylor polynomials of a certain degree at a certain expansion point. And I will dance a bit around the issue whether I will treat real or complex differential equations. I will say a few things along. Equations. I will say a few things along the way why we dance around there. But so sometimes k will be R, sometimes K will be C. And we only look at the situation that we take jet bundles over a trivial bundle, so we can say that this jet bundle is diffeomorphic to some affine space of the right dimension. And to fix notation, so I will always denote the independent variable. Always denote the independent variables by x, the dependent variables by u, and for the derivatives, I will use a standard multi-index notation. And there are obviously natural vibrations in the chat bundles by simply going from a higher order to a lower order and so forgetting the higher order Taylor coefficient. And quite frequently, we will also use the projection down to the base space where you forget everything except. Space where you forget everything except the expansion point. And what also probably everybody knows here to encode the different role of these variables, we have the contact structure, the jet bundle. And we will use it in form of a vector field distribution, not the co-distribution, annulating it, but the vector field distribution. And you have on one hand a set of And you have on one hand the set of transversal vector fields that essentially encode the chain rule. And you have these vertical vector fields corresponding to the highest order derivatives, which come from the cutoff at some finite order. So what is now actually a differential equation? Well, the classical answer that you find in most textbooks on the geometric theorem. On the geometric theory of differential equations, is that you have a fibered submanifold, and that this, if you restrict this canonical projection to the base space to your fibered submanifold, then it is a surjective submersion. First, perhaps a small remark on terminology. We do not distinguish here between scalar equations and systems. So, typically, when I speak of a differential equation written out in local equations, there's really Written out in local equations, there's really a system behind it. The problem is that the definition, as it stands here, is way too strict for many applications. For example, in many applications, you get here a very trivial example, such quasi-linear equations. And here, if you look at the corresponding fiber submanifold, you don't get a surjective submersion. A surjective submersion. At x equals zero, you have a problem. And so, if you insist on using this definition, we can stop this talk right now because this definition excludes all kinds of singularity that I'm interested in. So we have to do something a bit more relaxed. And simultaneously, I want a definition that can be more used in the context of effective computation. So we become a bit more algebraic. We become a bit more algebraic. I first define the notion of an algebraic chat set as a locally Tsariski closed subset of a chat bundle. In other words, it's a difference of two varieties. So instead of manifolds, I'm now looking at varieties and I may take out a bit of these varieties. So this automatically implies I'm studying only differential equations with polynomials. Differential equations with polynomial nonlinearities. And if I look at the description of such a jet set, it involves equations and inequations. And you will see later on why such inequations are important. And now not every algebraic jet set is what we call an algebraic differential equation. We replace this definition of that the projection is a surjective submersion by the following Uyghur condition. By the following weaker condition, we look at the image of our jet z under the projection and require that its Euclidean closure, so with respect to the standard Euclidean topology, not with respect to the Tsariski topology, is a whole base space. And so equations like what I had before are not a problem. Here, the closure misses one point of R or C, namely x equals zero. C namely x equals zero, but of course, if you go to the Euclidean closure, this point is added and you have no problem. But the main task of these conditions that you should want to exclude such things, which of course cannot reasonably be considered as differential equation, this is still the case. Okay, now I still have to meet an engineer who comes to my office and tells me I have this beautiful. My office and tells me I have this beautiful locally tsariski closed subset of a chat bundle. That's not the way an engineer thinks. So, an engineer he really wants his traditional systems. So, let's first look at the algebraic case, meaning that we look at polynomial equations. And as I said already before, for us, a system always comprises equations and inequations. In equations. So, when we speak about an algebraic system, we have two sets of polynomials: one defining the equations, one defining the inequations. And a solution set of such a system is then exactly what one calls a locally Tsariski closed subset. A trick that's important for us is we always assume that we have an ordering on our variables. Ordering on our variables. Please do not confuse this with a term order as it appears in the theory of Kripner basis. It's just an ordering. So we give some meaning to the numbering here of the variables. xn is greater than x n minus 1 and so on. And we call leader the largest variable that actually appears in a polynomial. And the idea is now always to consider a polynomial as a univariate polynomial in its leader. In its leader, and the coefficients are polynomials in the smaller variables. And then one has a classical notion of the initial, which is just the leading coefficient of this univariate polynomial, and the separant, that's what you get when you differentiate your polynomial with respect to its leader. What do we need these notions for? We want particularly nice systems. Particularly nice systems, and they are called simple algebraic systems. Such a simple system is first of all triangular. So every equation and every inequation has a different leading variable. Then furthermore, we assume a kind of constant decree condition. On the solution set of our system, System, the initials are not allowed to vanish, neither from the equations nor from the inequations. And similarly for the separants. Why this? If you think of one of these equations as determining its leader, and assume, for example, you want to solve this equation numerically. Then suddenly you are close to a point where the initial starts to vanish. Starts to vanish, you will have numerical problems. So, for a good and nice representation of at least parts of the solution set, you want such simple systems. The square freeness, so that also the separants are not allowed to vanish, by the way, implies that all roots are simple, cannot have multiple roots, which is also of advantage, say, for numerical computation. And the somewhat surprising result. And a somewhat surprising result by Thomas says now: given an arbitrary algebraic system, you can define a disjoint decomposition of its solution spaces into the solution spaces of simple systems. So, even if you have an arbitrary system that is not simple, you can actually get all its solutions by looking just at simple systems. So, certain parts of the solutions. So, certain parts of the solution space you cover with nice simple systems. And the point is, such decompositions always exist over algebraically closed fields. So if we really want to use this, that's one of the reasons why we should have to go in principle to comp the complex numbers. From the differential equation points of view, we would actually prefer reals. But here, if we really want to use such algorithms, we should use algebraically closed fields. Use algebraically closed fields and so the complex numbers. These Thomas decompositions do not only exist, they actually can be determined algorithmically. That is quite expensive, but it's possible. And even implementations exist for that. For example, in Maple, by a group of people around Vladimir Gett and Aachen people. To give you an idea what Thomas decomposition. What Thomas decomposition is about, let's look at a very simple case where, okay, I've had, of course, only a real picture, an algebraic curve. The Thomas decomposition looks at the projections along the coordinate axis. Now, in this case, along the y-axis is why I've chosen this as the largest variable. And the Thomas decomposition decomposes in such a way that each In such a way that each fiber should have the same cardinality. And so here you see this dashed lines, certain X values where the cardinality changes, where you suddenly have only two points on the curve, whereas generically you have three points. And that is exactly what in this case, the Thomas decomposition would do. It would give you the discriminant of the system. Or just to give a rough idea what this actually is about. What this actually is about from another point of view. Now, one can extend all of this to differential equations. I don't want to go much into details here. You introduce now differential polynomials. Here you take a space field, okay, I've chosen for simplicity a field of rational functions insume variables still x1 to xn and we have And we have then the usual derivations with respect to these variables. In addition, we throw in a finite number of differential unknowns, unknown functions. And of course, we can look at the derivatives of these and to make contact with our geometric theory, I will call this chat variables. And the ring of differential polynomials is now simply the polynomial ring in all these chat variables. The polynomial ring in all these check variables, which are, of course, infinitely many variables, which make some problems in theory and in algorithms. In many instances, of course, we would like to restrict to just the finite part of it by going only up to a certain order of the chat variables. And now one can again relate a bit with the geometric approach. If you look at this polynomial ring here, At this polynomial ring here with our independent variables, the dependent variables, and the chat variables up to order L, this would be just the coordinate ring for this affine space here, which we can identify with our chat bundle. Now one can do similar games as before. One needs a ranking, total ordering on the chat variables with certain conditions. Variables with certain conditions. And once you have this, you can again introduce notions like leader, initials, and separants. We can introduce a notion of a differential system in the obvious manner. And again, it's a mixture of equations and inequations, which perhaps is a little bit unusual. Slightly more difficult here is now the question of solution spaces or solution set because it requires. Because it requires a choice of a function space. I will here for simplicity assume, as it plays no real role in what I will be doing, that we look at formal power series solution. Different function spaces are possible, but in fact, here's algebraic point of view and what analysts really like to do often diverge a little bit. And again, you have now a notion of simple differential systems. Now, first of all, a simple differential system should be a simple algebraic system. If you consider, if you forget for a moment that you're actually dealing with differential polynomials, you just consider each of your equations or inequations as a polynomial infinitely many jet variables that appear in it. Then you can look at it as an algebraic system, and it should be simple in this sense. In addition, In addition, it should be involutive for the Gianni division. Okay, I will not explain precisely what this means. Let me just say the following. This means the system is formally integrable. There are no hidden integrability conditions. So, in a simple system, also such problems, which are very typical for differential equations, that you might have to be concerned about the possible appearance of integrability. About the possible appearance of integrability conditions, they are solved here for a simple system. And again, one can show an arbitrary differential system can be decomposed, disjointly decomposed into finitely many simple systems. And again, everything can be done algorithmically by combining the algebraic Thomas decomposition with Jani-Rickey's theorem. That's where this involutive. That's where this evolutive Vosgani division stuff comes from. And again, even an implementation exists in Maple. Okay, so now we have two points of view. On one hand, I abstractly defined differential equations in chat bundles. And on the other hand, I just spoke now a bit about differential systems. And we would like to bring together these two points of views. Together, these two points of use. As I said before, in applications, the starting point, of course, would usually be a differential system and not some locally Zoriski closed subset. And there exists a rather obvious way how you can get an algebraic chat set out of a differential system. Okay, you first have to prescribe yourself the order in which you want to look at this differential system. At this differential system. Then you take the differential ideal generated by your equations in the system. You truncate it at this order L. You take all inequations up to order L into account by multiplying them and looking at the ideal generated by this product. And then you look at this difference of varieties. Of varieties. You take the solution set of the equations and subtract the solution set of the inequations. I have no time here to explain you why this very natural construction leads to a lot of problems. These are more algebraic points, and I don't think this is the right place here to discuss this a lot. Perhaps just one problem. Perhaps just one problem. It's not so easy to actually get this ideal. That's just one point, but there are also a number of other more subtle points. So how can we do better? Well, the first thing is let's assume we have a simple differential system. Because of the existence of the Thomas decomposition, we may assume this. If our system is not simple, we simply decompose it. We simply decompose it into a certain simple system. And then it looks a bit like magic. We do a certain saturation. That's essentially the only difference we do. We take our differential ideal and saturate it. And then we just follow the same recipe we do before. The difference now is now you get something really nice. You get automatically radical ideas. You are immediately You are immediately able to compute these ideals, that's not very difficult. And for those more familiar with the geometric theory, you have automatically something like formal integrability. When you do this construction at different orders, you have this natural projection property. So you get something nice. Or another thing for Another thing, for many purposes, one prefers locally integrable system. For example, the book of Peter Alvar on symmetry theory, that's a standing assumption for most theorems, that you have a local integrable system. Again, in our setting, we slightly relax the definition of locally integrable. We only require that a Tsariski open and dense subset consists of points through which solutions go. Solutions go. And then we can show if we start from a simple differential system, its Tsariski closure, which essentially just means let's ignore the inequations, will actually be a locally integrable algebraic differential equations. So it's really time that I now get to singularities. This was officially the title of my talk. I need one more definition before. I need one more definition before, namely these geometric structures that we study on differential equations. So assume we have a point on an algebraic jet set. Then we can look at this point at those part of the contact distribution which is tangential to the jet set. Questions a bit what does tangential mean when we are speaking of varieties? Speaking of varieties. At the smooth point of the variety, there's no problem. That's a classical tension space. At non-smooth points, we actually take the tension cone, but that will not be very relevant in my talk today. So I call then this intersection or this tangential part of the contact distribution, the vessel cone at this point. And the symbol cone, that part of the cone. And the symbol cone is that part of the vestio cone that is vertical along the last projection here from order L to order L minus one. As I said, at smooth points, these are actually linear spaces and you can compute them as solution spaces of linear systems. In a more traditional language, the vessel cone, or smooth points, the vessel space, is a set of all potential Is a set of all potential integral elements. Using a terminology by Carton, other people speak of infinitesimal solutions. The problem now is that if you deal with channel algebraic jet set, the vestiocones at different points might have different dimensions. That's what I spoke about: that certain dimensions or ranks are jumping. But also, as we'll see in a minute, But also, as we'll see in a minute, the way how it lies in the tension space of the jet bundle may vary. So now we really get the definition of the various types of singularity that we are studying. The first point, of course, we are now on varieties. So we must expect that there are non-smooth points and we call them algebraic singularities. them algebraic singularities. In this talk, I will mainly ignore them. When you want to define singularities, the first thing you should very carefully think about is what is actually a regular point. So the longest part of this definition is indeed the definition of a regular point. So of course it should be a smooth point and And we want to have a Euclidean neighborhood of it on this chat set, such that the vessel spaces, here we are now at smooth points, I can really speak about vessel spaces, define a regular distribution on this neighborhood. And this distribution should be decomposable, the direct sum of the symbols. Of the symbols on this neighborhood and a complement. And this complement should be an n-dimensional, transversal, involutive, and smooth distribution. In previous works, we called such complements Vesio connections, because if you want to prove existence theorems on solutions, that's actually the way you do it in the Vesio approach. Consider such distributions and integral manifold. And integral manifolds to it represent graphs of prolonged solutions. We'll later see that the key problem here is this condition of involutive, but I will say more about this in a minute. Otherwise, if we look at the definition, we see it has two parts. We have this regularity condition, and we have this condition that we have a complement with sufficiently nice properties. Nice properties. In particular, the complement should be n-dimensional. Now we can distinguish two ways how one can deviate from being regular. At regular singular points, we still have in a neighborhood a regular vestier distribution. But if we look how the symbol lies within the vestier distribution, we no longer have. We no longer have space for a sufficiently large complement, which automatically means there cannot be any solution going through such a point. At irregular singularities, it's not even possible to find a neighborhood where you have a regular vestier distribution. So, here definitely dimensions or ranks are jumping. Some remarks to these. Some remarks to these definitions. Something that I really miss in the classical literature is to emphasize at the very beginning, perhaps I should go back, I say I've chosen an algebraic differential equation. In practice, it's not so clear how you should choose this. There are often several possibilities, and what you get as results will depend on this. Concerning detection, which is my main topic in this talk today, the algebraic singularities are no problem. That's the standard thing in commutative algebra, algebraic geometry. You kind of detect them with a Jacobian criterion. Here, these two points, three and four, in the language of differential topology, the conditions there simply say that these points are critical. Now these points are critical points for this restricted projection map. I don't know whether any of you is perhaps familiar with the textbook of Vladimir Arnold on ordinary differential equations, where he studies for scalar first-order equation singularities and also defines regular singular and irregular singular points. If you compare my definitions with what you find in this book, my definitions are much more complex. My definitions are much more complicated. And the key point is: I am speaking here always about neighborhoods. This never appears in the classical literature. Indeed, as long as you study only equations of finite type, so in particular either not underdetermined ODEs or maximally overdetermined or holonomic PDEs. Then you don't need these neighborhoods. You know a priori. You know a priori what are the right dimensions of the vestier distribution, of the symbol, and so on. And you cannot simply, at a point, compare what you get compared with the right value. And so, in particular, involutivity is no issue. As soon as you go to infinite-type systems, we have to bother about this involuntary, and that makes This involutivity, and that makes really a lot of problems. If it was not for the involutivity, actually, to distinguish whether a point belongs to 2, 3 or 4 is quite simple. I have no time for the details, but what you do is essentially you just look at the linear system defining the Wester distribution and you look a bit how its row echelon form looks like, and then you can immediately see in which case you are. This gives you everything. This gives you everything except for this involutive. And so, in fact, for PDEs, we are not yet sure whether this definition is really complete. We cannot prove that every point on a PDE belongs to one of the four classes we have just shown there. There are still certain gaps or loopholes where a point might disappear, and this is really. Disappear and this is really a problem. I don't want to go into details of this example. Here I've taken an over-determined system of two PDEs, just to show a bit how this might look in practice. The key point is before the leading derivatives, here in each equation we have just one second order derivative, the leading coefficients are not constant. They depend on the independent variable. They depend on the independent variables. And so now, here in this case, you actually get seven distinct cases. The regular case is simply that these two initials do not vanish. Then you will find the three-dimensional vesiocone and you have everywhere a two-dimensional complement. Now you can go through a lot of different cases. Here now the X vanishes, but the other one does not vanish. Does not vanish and certain other inequations. So, here you see now why suddenly inequations show up, even when the original problem has not contained any inequations. So, under certain conditions, you find here regular singular points where the vessel distribution still has the same value, but the complement has no longer sufficiently high dimension. Under other conditions, here both initials are vanishing and some other conditions. Now, suddenly the rank of the vessel distribution jumps to a higher value, but you still have a too small complement. But sometimes it's also possible that the complement has the right dimension. That is not possible in the classical theory of ODEs, but for PDEs, this may appear. But for PDEs, this may appear, and we call this purely a regular singular point. And so on. So one can go here through many cases. And you can imagine the larger your system is, the more cases arise. So it's really computationally non-trivial to get all this. I said I will not speak about the analysis of singularities, but to give you at least a small idea. At least a small idea what can happen there and why it's of interest to look at them. I want to show you a bit what might happen there. I will restrict here to real ODEs because that's where we have the best understanding. It may, for example, happen that you have only one-sided solutions that either end or start at the singularity. You may have that the traditional unique That the traditional uniqueness theorems break down, that you have several solutions. Might be finitely many, but it also may be infinitely many. Then, as that, my equations always have polynomial nonlinearity, so they are automatically analytic, and you would expect analytic solutions. But it's possible that you have solutions of finite regularity. I have here a small example, again, a quasi-linear second-order equation. Linear second-order equation, and the key thing about this equation, in fact, is this constant term, some parameter k in it. And for example, if this k is a natural number, then strange things appear. So you have infinitely many solutions then, but they are all only of regularity ck, not any higher regularity. For other values of k, non-integer, you will have a Non-integer, you will have a smooth solution, like you might expect. Actually, it should be an analytic solution, even. But you have again infinitely many solutions of a certain regularity. And this was just for this point, x equals zero, of course, so that we get to the singularity. And the key thing is this value for the derivative, the first order derivative. And if you take instead of plus k half, minus k half, half minus k half all these singular phenomena disappear you just have like you might expect a unique smooth solution there's still a few singular things going on but i will drop this so you see this can be very subtle what happens at such points if you prefer to see it in form of pictures you can also go look at my background that's another example than what i have here on the slide here i look for example at the unit sphere in the chat bundle as a differential Sphere in the chat bundle as a differential equation, and on the right-hand side, I've plotted the solution. You can see the equator is the locus of all the singularity. And if you are at a point here on the equator, and if you're on the left half of it, you have two solutions starting here. If you're on the right side, you have two solutions ending here. And then we have in addition, And then we have, in addition, what we always call the West Pole and its antipole, the East Pole. These points are irregular singularities, and infinitely many solutions approach them without actually ever reaching them. Here's an example of an algebraic singularity. I've just taken now the cone in the chat bundle. And here with some ad hoc method, we do not really have a complete understanding or Complete understanding or big theory for that, you can show that through the tip or the vertex of the cone, four solutions are going. Two of them are analytic, and they either stay on the lower semicone or on the upper semicone. But as you can see on the right-hand side, on the graphs, they both have a horizontal tangent at the origin. At the origin, and so what you can do is you can make such games. You can start, say, on the lower semicone and then just jump to the upper semicone. And that way, you can construct two additional C1 solutions. This seems to be quite typical for algebraic singularities. I said already before, the real point when you speak here about singularities is to understand. is to understand what regular actually means. And for finite type, we have a complete understanding what regular means. If you look at a sufficiently small neighborhood of a regular point on a finite type equation, then this neighborhood is uniquely foliated by prolonged solution. This is a bit stronger than the classical statement that you have a unique solution through every possible. Unique solution through every point, but that also these solutions define the foliation. For my example before, even the case when it looked like the standard case with a unique solution, you would not have such foliation. When you move on to equations of infinite type, then you have the problem that you have too many such foliations. Namely, you have infinitely many such foliations. And so, in principle, And so, in principle, a proper understanding of regularity should probably include some further statements. Yeah, really, how many, how do these perhaps lie relative to each other and so on? But we do not really know how to express this. Okay, I should also perhaps present a few results instead of just definitions. What is not an issue in the classical theory is to prove that. Theory is to prove that singularities are actually something exceptional. I mean, if you call something singular, it should be something exceptional. And so we can also prove in the PDE case that indeed the regular points contain a Tsariski open and dense subset. So generic points are regular. The proof here on the slide looks fairly short, but I do not have the time to discuss. I do not have the time to discuss it, but I just want to say that these points here make up a major part of my habilitation thesis. And this point was a complete PhD thesis of a student of mine. So there are deeper results lying in this proof. It's for finite type, it's very easy, but for infinite type, it becomes problematic. Again, the classical theory, people just look at individual points. As long as you look at scalar equations of first order, something like this, yes, singularities are typically isolated points. There's no need to look at anything more closely. For larger systems, one definitely has to discuss a bit how should one organize these things. So we introduced the notion. So, we introduce the notion of a regularity decomposition. We call an algebraic jet set regular. If in some sense, it's first of all a smooth manifold, and the geometric structure that we discuss always, the vestier space and a simple space, behave uniformly on it. And say for irreducible algebraic jet sets, we now define the notion of a regularity decomposition. Of a regularity decomposition as a disjoint decomposition into regular algebraic jet sets and the algebraic singularities. And our second main result, I will go with it very fast, look at the time, just says these things actually exist and we can compute them algorithmically. That's again where we need both the differential Thomas decomposition and the algebraic Thomas decomposition. Differential one. Composition. Differential one gives us a simple system, and then we use algebraic Thomas decomposition to really detect the singularities. I think most here in this workshop in the classical geometric theory. And there everywhere you have this assumption that you have a regular differential equation. But it's very rarely really rigorously defined what a regular differential equation is. Equation is. If you look, say, at what Kartov wrote, off some hypersurface, everything works fine. He never bothered to describe this hypersurface, to find it, to look what is going on there, and so on. The problem with the notion of a regular system is also that nobody really has effective criteria to verify that a given system is regular. And we can provide at least a partial answer to that. The point is, now we have to go the opposite direction from what we did before. We have to go from a differential equation, our sense, to a differential system. And again, there's a fairly obvious way of doing it, but the system you get that way is not necessarily very meaningful. So you need some conditions on the system. Some of Some of them are rather straightforward. We want to get nice ideals. Of course, when from the system we go back to an equation, we want just to recover our equation. But here that's the problem of all definitions of regularity. You have to look also simultaneously at all prolongations. So, in principle, you have to look at infinitely many equations or manifolds. What we can show is when our algorithm produces When our algorithm produces a regularity decomposition, then we have on each irreducible component a unique algebraic system that defines a regular differential equation in our regress sense that is Tsariski dense and entirely composed of regular points. So we can really extract such regular systems. Okay, to conclude, perhaps just. Okay, to conclude, perhaps just a little outlook. What are open points? You have seen for the algorithms we need complex numbers. Thomas decomposition is over algebraically closed fields. But of course, for many applications, we would prefer real equations. Furthermore, in real equations, you would like to have inequations of inequalities, positivity constraints, that kind of stuff. So we have done And that kind of stuff. So, we have done some work on this. I've mentioned already the question of taxonomy for infinite types. I've mentioned the question of algebraic singularity, there's not much known, and this problem of local solution behavior. But I see my time is running out. So I just want to give you a quick look on the references. Most of what I told you is from this paper, our first attempts to look at the real case. You can find Real case, you can find stuff here, and a bit on this analysis stuff for the ODEs, where, for example, this example I showed you are coming from that this paper. Okay, thank you for your patience. So I saw that Evelyn had a question over chat. Edelin, do you want to voice your question or me to read it? Yeah, no, this was at the very beginning. No, this was at the very beginning, so it's a bit of a. So at the very beginning, you said that you had a Thomas decomposition only when the field was algebraically closed. And I was really wondering, because, I mean, as you know, I've worked on boost decomposition algebraic different steps. And we don't need that there. There are many different kinds of decompositions around. And okay, I'm not absolutely a specialist. I'm more a user of Thomas decomposition and not. Composition and not so much, somebody really works on them. Our specialists, in particular Daniel Roberts and Markus Lange-Heckermann, they always tell us for the Thomas decomposition, for some of the steps you do there when you compute them, you really need the algebraically closed sets. Mourino Maza, for example, claims he can do the same without such assumptions, also over the reals. But on the other hand, some people claim that Hand, some people claim that his programs would not give us disjoint decomposition. So it's really a bit about details. For example, here in our work, when we looked a bit at the real case, we simply moved on to quantifier elimination. I mean, anyway, you work then with semi-algebraic sets. But there are still also some other tricky issues when you go to the reals. For example, at some places, we look at radicals. At radicals. The question is: Do we actually need a real radical, or does it suffice to have a radical in the classical sense? So there are many tiny issues here, but I fully agree with you. You can do a lot over the reels. Thank you. Any other questions? Yeah, Rob, please. So, do does the idea So does the idea of resolution of singularities apply to any of this, to any of these ideas? I mean, so obviously we're dealing with differential equations and varieties, but things like maybe doing blow-ups or at finite points or compactification at infinite points, that is moving from affine space to projected space and then changing and then having those singularities. Changing and then having those singularities go away. Are these ideas at all interesting or useful in this context? Yes, that's a good question. They are definitely relations, and sometimes it's a little bit tricky. But yeah, let me show you some relations. For example, how do we actually analyze what is happening at such a point? Well, Point. Well, what will happen in such situations is you have a vector field. That's actually these curves here, the integral curves of a vector field on the sphere. And this point will be a stationary point of it. And essentially, we just look at what happens at the stationary point. Here, for the sphere, this is elementary ODE stuff. But in most interesting cases, you will get stationary points with many zeros. Stationary points with many zero eigenvalues. And then, for example, we have to go to blow-ups to analyze what's happening there. So, this is one thing where we really straightforwardly apply some of these techniques. Then, in the definition of the vessel cone, I used the tension cone. Here again, that's something. Here again, that's something we still have to investigate. The tension cone has relation to what goes on in a blow-up. Now a blow-up in the classical sense of algebraic geometry of a singularity of a variety. So it has been on our to-do list for years, but we have not managed to really look at it. I think there are many, many connections between such things. In particular, blow-ups, definitely. things in particular blow-ups definitely appear so you were talking about R or C interchangeably but when you discuss singularity of solutions you mostly were talking about R and in particular say something like left solutions right solutions situation in C of course is much more complicated because you may have solutions Uh, because you may have solutions stemming from singularities that exist only in some core. Yes, you are right. So, do you have any idea how you can actually give criteria for existence? Let me put it that way. We are looking at it in a slightly different way. Indeed, when we look at solutions, we typically just do the real case and we do real OD. That's where we have most. Do real OD. That's where we have most experience. We have some ideas about how to extend this to finite-type PDEs. But okay, let's stick to ODEs. And here my feeling is I've looked a bit at what people have done over the complex numbers. But my feeling is people there are studying quite different problems. We are interested at solutions that really exist. Solutions that really exist at our singularity that goes through them in many cases. In the complex case, the typical thing is that solutions do not exist at the singularities. And what people study, say, if you look at the Fuchs-Frobenio theory, people study what kind of singularities the solution developed. So it seems that people look at very different questions than that we are looking at. And they use, of course, in very different techniques. And it's about monitoring. Techniques and it's about monotromy, Stokes matrices, whatever. But these really seem to be different questions. But I must admit, I do not know much about complex solution theory. This is Peter. So that was what you just said was maybe hard to answer what I was going to ask about the technical series solution, say, the regular. Take a series solution, say at regular singular points, so you have this whole big theory for ODEs. Sorry to interrupt you. In the Fuchs-Frobenius theory, you have also a notion of regular, singular, and irregular singular. But this is different from our notion of regular, singular, and irregular singular. These are really very different things. As I said, Fuchs Robinhoods, they study something different. They study something different. They look at solutions that do not exist at certain points, and the question is exactly how do they become singular there? That's then what you have. This is Frobenius series and so on. That's quite different to our solutions that happily exist at these points. They might have perhaps only a finite regularity, or what is more typical is that simply several solutions exist, where you expect normally a unique solution. Normally, a unique solution, but so these are really different issues. We probably need to start the next lecture, so let us thank Werner again and then. I know, I'm not told how to see how just one second, Roman. I need to press the right buttons to record your talk. Just one second. Maybe just a simple act of disconnecting, reconnecting. Okay, so