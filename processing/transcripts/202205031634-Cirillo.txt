Yes, artificial intelligence. First of all, thank you for the organizers to create this space and for giving me the opportunity to moderate this session. I am Davide, head of the Machine Learning for Biomedical Research at the Life Sciences Department of the Parcels Recruiting Center. And our panelists for this session are Thomas Bunch here in person, professor in Professor in the department of biosystems, science and engineering at ETH in Zurich. Not yet. Okay. Okay, Sarah Luker was online. Sarah, are you with us? I am. I am indeed. I'm here. Okay. Okay. We saw a talk from Sara yesterday, Sarah. She's the co-founder of Beth Canada. She's the co-founder of Data Analytics and Trustworthy ML Initiative. And then Joshua Williams, are you there, Josh? Hi, everyone. Okay, hello. So PhD student in computer science at the Carnegie Mellon University. Okay, so let's start. I will give just a very brief presentation about to introduce the topic and then we will start an open discussion. This is really meant to be an open discussion. So the audience. An open discussion. So, the audience is invited to participate if you want to raise your hand like physically or virtually if you are following online. So, bias in AI. So, let's start with an example that, I mean, to me, it really like summarizes very well the magnitude of the problem. So, the data that is currently available in Currently, available about depression in men and women in the population is telling us basically two very different stories. In one hand, we see that the incidence of depression is way higher in women compared to men. And it's, of course, very important because it's telling us that we have to focus on this, to give special attention to this. But on the other hand, we also have data that is telling us that. Also, we have data that is telling us that the incidence of suicides in men is way higher than in women. And this is telling us that, well, we are doing really wrong in diagnosing this condition in men. So where is the problem and how to reconcile those data? Why the data that we see from population is selling this, is like that. So essentially, the reason is that. That. So essentially, the reason is that the sex differences in the manifestation of depression between women and men has only been studied very recently. And so the typical signs of depression that we are all thinking when we think about depression are actually associated to women, while in the case of men, those are completely different. You can see outbursts of anger and aggression, substance abuse and risk taking and things like this. Risk taking and things like this. This is, of course, not present in the data because it was not known before. But you have to think that if you are taking this information, if you want to develop, for instance, an algorithm, a system, an artificial intelligence system for this particular medical condition, you only have a partial vision of the story. And this is like, of course, one of the main sources of Main sources of bias, which in this particular case is a problem of representation of a specific group of people in the data that we might want to use. So the taxonomy of biases is huge. And actually in the literature, there are many attempts to survey all the possible biases that might occur. And here in this slide, we have just a few examples that are related to technical. Related to technical bias about the data and about the model, but we also have to consider cognitive biases. So, biases in the way we judge reality, our perception and stereotypes and ideas. So, as for the biases related to, let's say, the cycle of development of an artificial intelligence system, we can see here some examples, like for instance, historical bias that can be present in the data that has been. Present in the data that has been accumulated over the decade in the history of humanity. Representation bias, in which you have an underrepresented and you can have underrepresented groups in your data. Measurement bias when you're using basically the same metric and you're not adapting this to measure whatever you want to measure. And then all the biases related to the model itself, aggregation bias if you are pulling heterogeneous sources of information and using them to. Your information and using them to train a model. Evaluation bias, if you which can occur, for instance, depending on which benchmark are you using to evaluate your model. And finally, deployment bias, which is essentially about where and when you are going to deploy your model and who is going to use it and for which purposes. But again, to suggest six biases, there are many, many, many, many more. In particular, in the area of medicine, the representation Medicine, the representation bias and in general, you know, the underrepresentation of specific categories is one of the main sources of bias that can be found in all the medical activities. For instance, here we have a representation of the path that lead to the development of a drug and all the steps in this pathway. We can see that the generation of biased data can occur in all those steps when we are using only male mice. And we are using only male mice during our preclinical studies, as it has been done preferentially throughout history for a series of reasons. We have an underrepresentation of women in clinical studies, and of course, when you are putting out into the market a drug that is being developed in this way, it's not surprising then that you have side effects in women. Now, the problem for artificial intelligence is that all those steps produce data, and this data is available. Steps produce data, and this data is available, is stored in repositories that you can access in order to train your systems for whatever purpose. And so it's very dangerous to have this quality data, but partial in a way. And so all the examples that I'm going to give are specifically focused on sex and gender bias, metric intelligence, because it's where I work with, but you can easily translate this to other types of biases, like rational biases. Biases like rational biases and other attributes that you can add. So, okay, there are many examples for why we have to consider, for instance, both sexes, even in the preclinical studies. Here it's different cell lines that convey the transmission of pain in male mice and female mice. And if we are only using male mice, of course, we are just looking at one side of the physiology. Side of the physiology of the biology of pain in this particular case, what should be done is something like this. So you have to run like two experiments in parallel with the main mice and female mice, but nobody does this because of course it's the double resources, it's double money that you have to spend. And so, you know, what you generally do is to just perform your experiments in one sex. It's even Sex. It's even more shocking if you look at the clinical trials. So, those are clinical trials that ran between 2015 and 2016 that led to the identification of new molecular entities. And as you can see, basically in some medical areas, for instance, here we have rheumatology, the percentage of women that were present is extremely low compared to men. So, again, whatever drug is going to be. Whatever drug is going to be developed from those new molecular entities that have been identified, it's not surprising that we create problems later on in the population. And indeed, in the population, in the pharmacovigilance phase, for instance, we can see something like this, like women has a higher probability of being administered drugs that are eventually toxic for them with a higher probability than in many. High probability than in men, which is this curve down here. And indeed, a lot of drugs have been withdrawn from the market because of greater health risk in women. So it's a real problem. And as I said before, all this biased data is produced and is stored in human data repositories. For instance, two of the largest ones are in the US, TBGAP. Are in the US, TBGAT, and in Europe, EGA. Those are two repositories of human data. And in a recent study, what we did was to essentially analyze the metadata associated to those studies that are contained in those databases. And as you can see, like FMU means female, main, and unknown. And as you can see, especially in the DDGAP, like a lot of studies actually contain a mix of samples. Some of them, you know. Samples, some of them you know that are from females, some of them you know that are from male, but many of them also are from unknown sex. So, this is, of course, like a bit complicated. In Europe, it's a bit better. Okay, of course, it's also important, as I said before, where you are deploying your model. You can have, for instance, a digital divide, so the access to the technologies in different countries is different. Countries is different. And so, if you want, for instance, I don't know, to study digital biomarkers using smartphones, you have to consider that if you are in South Asia, you have, you know, like women are less likely to access this technology, to own a mobile phone. So it's something that if you want to do this kind of studies, you have to take into account. I'm skipping some slides because for the sake of time, but basically, like in this Basically, like in this paper that we published a couple of years ago, we surveyed the status of sex and gender bias in artificial intelligence for a series of technologies, important especially for precision medicine. There are a lot of examples. One of the most notable is about word embeddings. So, you know that word embeddings are retained biases that are contained in the documents that are used to train those vectorial representations. To train those vectorial representations of words. And in a really nice work in 2017 by Calis Khan, it has been shown that essentially there is a relationship between the semantic context of a word and related to women and the actual job that women more often occupy in our society, which essentially means that up here you will find words like secretary and nurse, and down here you will find words like manager and doctor. And doctor. So it's a real problem that can be transferred to the artificial intelligence system. And we expand upon this in a book that is going to be out later this month, I guess, with a collaboration between many different institutions and also companies. Okay, a lot of activities. What we can do about this. So the take-home message of this workers workshop is that essentially. Workshop is that essentially interpretability is definitely the way to go. And there are many tools, toolkits, open source toolkits for at least detecting bias and explore some mitigation strategies. This is one, it's called Ecuitas, it's from the University of Chicago. This is another one, we talked about this this morning with PUSH, it's AI Fairness 360 from IBM. So as you can see, essentially, those are tools that you can like with. Can like with which you can analyze your data and identify if there are biases of different kinds and also act upon this with specific mitigation approaches. The topic is very hot topic. This is a work that has been published like some days ago and it was like it has been like you know a bit of discussion on Twitter about this. Discussion on Twitter about this work. Very briefly, what they did was basically to train a model based on the subjective, let's say, impression of people looking at faces. And so essentially, what they found is that the model was performing very well, but also they find very strong correlation between things that are generally, you know, the, you know. reflects the stereotypes and the ideas of the people that have been involved in this labeling of pictures. So it's very small, but you see that there are like a high correlation between labels like familiar and smart with, I don't know, like white and European or things like that. So it has been a lot criticized and it has been accused of perpetual Accused of perpetuating digital phrenology, basically. And so, yes, the topic of biases is definitely something that we are witnessing and it's very important right now. So, after this not so short introduction, I would like to then ask to the panelists, like at what point in your career did you face for the first time the problem of bias in artificial intelligence? Bias in artificial intelligence. So, I would like to start with Thomas, which is here. And so, what is your story, Thomas? So, I want to give an example, which was not the first one where I encountered it, but one which I found the most difficult to find. So, there is a difficulty in the data, in clinical data, to find out Data to find out if there's a question: Is there a bias or is there no bias? And so, for example, you have, let's imagine you have two different settings. So, one is clinician recognizes that there is something wrong with the patient. And the first thing the clinician do is changing the fluid administration a little bit, changing the ventilation a little bit. Now, this is not part of the event definition of a deteriorating patient. Definition of a deteriorating patient that means it's before the end point. And if the artificial intelligence picks up the signal in the data, it will lead to a lower prognosis, even though the doctor is in fact helping the patient. Now, the complete opposite happens. For example, when treating a patient with acute kidney injury, it is not advisable to give diuretics. Advisable to give diuretics. Nevertheless, so acute kidney injury sometimes manifests that the injury, the kidneys doesn't work and the body blows up because of so much fluid. And then the doctors, like Michelin and the doctors, they want to help. That's why they give diuretics. But it worsens the process. So here, in both cases, in both cases, the algorithm picks up the signal and gives you, well, worst prognosis. Well, worst prognosis, but it's a completely different thing. One is a bias in the data where treatment is given, although it should not be given, and the other one is treatment is given and the patient is recovering. So I know this is a little bit different compared to what you've shown, but I think this illustrates nicely that we should be really, really careful talking about biases in general, because we should always. General, because we should always look at the exact thing what is happening because sometimes it might look like a bias where it isn't, and sometimes it is a bias, and you assume it's just another artifact, one that you've already seen a couple of times. So, you should be a domain expert in the field where you apply your AI. I think that's Your AI, I think that's incredibly important. Okay, thank you, thank you so much, Thomas. We are going to touch upon this later for sure. So going on, maybe Sarah, what is your story? Like your first contact with bias in artificial intelligence? Hi, so thank you so much for putting together this panel. It's lovely to stop in again. And again, sadly, I can't be there in person. Again, sadly, I can't be there in person because I think these are exactly the type of conversations that are fun to have in person. But when I think about bias and technology, I often think about the scaling of biases. I think that's what a lot of us are concerned about, is that we often have undesirable properties, which then, when you apply to large data sets and deploy it. To large data sets and deploy at scale, can compound historical patterns of injustice or can, in fact, lead to unequal treatments or undesirable treatments. My first encounter in terms of reflecting on the impact that algorithms can have was actually, I was working with, I was teaching machine learning just before I joined Google. So this is over five years ago, and we were teaching in Kenya. We were teaching in Kenya, so we wanted to use a local data set. So we use Kiva loans. So Kiva is a non-profit and it actually makes all this data very transparent. All this data is available online. What's fascinating is that a lot of the loans reveal really interesting biases. For example, taxi drivers do very poorly at securing loans at Kiva. In fact, it's very hard in terms of the time to secure a loan. The time to secure a loan, you don't want to be a taxi driver. There's something about that that makes it harder to raise money through the loan loan, kind of you set the amount. We also realized that women were asking for far lower loan amounts than men. And so even in terms of what capital was requested, we kept on noticing the average loan amount was far less. There are many things that contribute to this, and you start to understand that it's various aspects of the Various aspects of how we use technology. For example, the algorithm of what loans are surfaced can compound patterns like this, because perhaps since taxi drivers are never clicked on, they're never surfaced at the top and they never make the home page. There's also perhaps the reinforcing quality that, for example, if Kiva were to ever suggest a loan amount, it may end up suggesting far less for women asking for a loan than for men. Asking for a loan than for men. So I often think about this a lot: that it's in the design of our technology, it's in the training of our technology, it's in how we, as was pointed out, like it's even in how our data reinforces it. And so a lot of my work recently has focused in how our optimization techniques themselves can impact and amplify or reduce harm. And I think that's something we often don't think about because we often kind of abstract. We often kind of abstract it away. It's just a data problem or just a deployment problem. So I've been thinking about that more recently. Nice, thank you so much. So Josh, what is your experience? Yeah, so I am still a grad student, so my career is starting out. But when I started my PhD program, I really thought that looking at questions of bias in AI was just a worthwhile problem. Of bias in AI was just a worthwhile problem to start thinking about for the next few years. But it really hit home for me once I found out that the city of Pittsburgh, like where I am, was running a predictive policing program. And for those who might not be super familiar with the term, basically what was happening was that Carnegie Mellon had partnered with the Pittsburgh Police Department, and they were using all of our historical arrest data, incident data, all kinds of things. Data, all kinds of things, in order to predict which blocks of the city were likely to have some criminal event happen over a five-day period or something. And they would give these maps to the police, and the police would deploy additional patrols, or they would recommend additional patrols to those areas for the police. And so being a Black American and with the history of policing in the US, it was a bit concerning knowing that I'm a part of the FAIR ML field, I guess, and all the And all the examples of things that happen with bias and data. And it was alarming this was happening. So I partnered with the group that was started looking at what was going on with the police, trying to talk to a few people, running educational seminars for different community groups in Pittsburgh so they understood what was happening. And ultimately, we were able to work with the city council in order to get legislation passed. Get legislation passed that limited what could happen with these kinds of programs in the future. And I know I'm kind of light on sort of the implications of this, but if anyone is interested, I can talk for hours about that. But this whole incident really colored my impression of how I should be thinking about these kinds of problems. Because while the police were running this program, there was so little transparency that not even the police oversight boards were aware that this was happening for a year and a half. That this was happening for a year and a half. And this just really cemented that centering at all of my work should be around what does the community want? What do they think about these things? What solutions do they agree with? And trying to align what I do with what they do. Because de-biasing an algorithm might, oh, I feel like de-biasing an algorithm might not necessarily solve all the social problems that we're situating these algorithms with. Problems that were situating these algorithms within. Yeah, that is actually somehow related to what Thomas was saying before, because it's essentially like try to involve the end user in a way, like the person that are more affected by the problem or that are going to be used, to use in the real scenario, the system. This is definitely important. Definitely important, which leads me to ask you: like, what is then the, you know, the status of awareness about those problems in your specific fields? Like, I'm talking really about, you know, like among colleagues and among stakeholders, like what is your feeling about this? Like, it's awareness is definitely important, but what is the status right now in your area of application? Is that for me or anyone? Yes. Maybe Sara and if you want to start first. So I guess I'll interpret this question more broadly to be, well, what's the precision of language that we have about bias? I think that in some ways there's a lot of work to be done. So I think that many of our tools to speak about bias require like a perfectly curated data set. Like a perfectly curated data set with labels for all the sub-attributes that we care about. There's also a challenge of explaining to policymakers. And I think as scientists, we need to do a much better job at translating our technical knowledge of different domain problems to policymakers who need to make decisions about what technology is more risky and what's implications of deploying certain technologies. So I would almost say that the two main So, I would almost say that the two main challenges that I see is one is an algorithmic challenge. So, we've talked about this kind of vaguely in different ways, but there's almost, I think, the tension that sometimes to solve bias, we need better representations. We just need to learn the data better. This is often what happens when we have an underrepresented attribute. We're not doing a good job on that attribute or subpopulation because it's low frequency. There's also the other conundrum. Sometimes our notion of bias. Conundrum. Sometimes our notion of bias is that we've learned an undesirable property in the data, and we almost want to impede model generalization. So this is a tension. And so in some ways, it's from a scientific perspective very interesting because in order to solve this tension, we need to have adaptive models where we can treat parts of the distribution differently. And that doesn't quite fit with what our current models are good at, where we delegate an entire representation to the model and we just say, High representation to the model, and we just say, see later than training. So I think it's from a technical perspective, it's very fun, it's a challenge. Many of these are very core to machine learning progress, but this is something we must tackle. We must also realize how our model itself impacts the representation that's learned. The second core challenge is a communication challenge. So I really loved Josh. I think it's really impressive. It's really impressive that you are already translating a lot of these algorithmic insights into policy wins. And I think that's what we as a scientific community need to be much better at. And frankly, we're not. I think that we still are, we don't connect our work enough to its implications. And that has to be different types of forums. So we need to be talking to policymakers more. Talking to policymakers more, but it also needs to be a concerted effort to empower more everyday users of technology to be able to understand the implications of what it means in terms of their privacy, their security, and how we empower that conversation. Right. Yes, this is a Yes, this is so thank you so much, Sarah. I totally agree, like those two challenges, the algorithmic ones and the communication. I also find particularly interesting those level of risks that you mentioned at the beginning that needs to be taken into consideration. And I would like to ask Thomas, that is more into, let's say, clinical activation area, like what is, how do you see this, like having, you know, Having, like, you know, considering like different levels of risk of accepting or not accepting certain biases or certain decisions. Difficult question. Yes. I don't want to answer because I don't have a good answer. Yes. What comes to my mind when thinking about biases in AI? About biases in AI and analyzing bias in general, I think this is one of the main challenges in science of our time because it's a problem where currently we don't have a universal solution. So, for example, so when you look back, for example, when vaccines came up, you needed a scientific method to prove that they're working, and that's how the They're working, and that's how the clinical trials emerged. The next, there was the smoking debate in the 60s, and it took forever to prove that smoking is causing cancer. And what was required was the scientific definition of causality and intermediate variable. And I think we need something similar for analyzing biases because we have them at so many different levels. So many different levels, and disentangling them is such a difficult thing that this is a challenge that needs a new solution, essentially. And I think, so one way where I turn for this solution is a philosopher called Latour. I don't know if you know him. Latour proposed a parliament of things, and this parliament. And this parliament of things already exists virtually where people pretend to be the North Sea and they have the voice of the North Sea. So you bring things into Parliament. I think Latour's example was the ozone layer that you imagine how does it feel for this particular entity? How would this entity voice its opinion, its views? Its opinion, its view on the world. And that's why, for example, I really think the individual stories matter, because that enables this emerging of parliament of things. I know this is very abstract, but and it's not answering your question. I just feel provokeously. I was asking this because. This because this is something that so basically, I was involved in a conversation with representatives of the European Parliaments about bias and those levels of risk is something like the level of bias that they can accept, you know, it's something that is really on the table of the conversation. So when it's been mentioned, like it rang a bell somehow. But as you said, it's very difficult. But as you said, it's very difficult because I mean, who is taking the responsibility of setting a threshold for whatever decision it is? This is very, very, very complicated. So, Joshua, I would like to ask you about this point of disentangling the bias, because it's true that, I mean, before I was showing like, okay, the measurement bias, the aggregation bias, like they are separated, but actually, they are all interpreted. Separated, but actually, they are all intertwined. And it's very difficult to actually find the relationships between them. Sometimes they are very interindependent and it's very difficult to disentangle them. So what is your view about this? Which is also related to those compounds effect that Sarah was mentioning before? Yeah, so I guess this might be more of a hot. I guess this might be more of a hot take, but I've never thought too much that we should really be, that the goal is to disentangle all the biases because everyone alive has their own upbringing, their own values, their own thoughts about the way our system should work. And all of that is reflected in the data that we have and trying to disentangle all of our biases from the data. All of our biases from the data in all sorts of different ways. I feel like that takes away some of the agency that people have around the models that are used and the way that they're implemented on their lives. And so my thought is always just really centering on the understanding of the end user and really centering on: is this something, is this approach, is this data something? Is this data something that you agree with? And even if they have their own biases about, like, oh, I think that I don't want to give as many loans to taxi drivers or something. I think that everyone's view should be represented in some way. And if we're fully able to disentangle all the biases in our models. All the biases in our models, then we're sort of just taking away some of the representation on the way that the world is. And if we do that, then it gets much harder to actually address the historical context and the biases that exist. Because we're sort of, it's almost like I've always viewed it. It's almost like we're sort of plugging our ears and saying, like, everything is okay. Let's just get rid of the bias and go from there. Because we really need to be aware of the contexts that these things are used in, that our data came from. In that our data came from, and working around that, not just trying to make sure the data is acceptable for whatever use we have. Related to this, Sara, do you think that, like, for instance, co-creation is something that can help in order to engage more the, let's say, the end users of a system in In order to also for the machine learning engineers to be aware, to realize that there might be problems that maybe are not apparent for them, but talking with the communities that the system is supposed to serve can actually help. So, I don't know how do you see this? Yeah, I mean, almost certainly. Certainly. I mean, I think inputs are always very valuable because especially, for example, I think we often export technology. And when I say that, I think that the location for a long time of engineering hubs has been very different from where problems are located, and that's changing, but not soon enough. So, I do remember, for example, I was in Ghana here in the Two and a half years ago, and when you call an Uber in Ghana, people don't like you to use the card, they like cash, but they won't tell you, they just won't show up. So you'll just wait for a long time. But instead, there should just be an option on your, you know, Uber Ghana where you say, you know, it just defaults to money. And in fact, the fact that that hadn't been incorporated just shows it wasn't a feedback loop at all, which results in this kind of odd thing where you just. Which results in this kind of odd thing where you just see the car very far away, just never coming closer. There's things like this. I mean, we always want more feedback and we always want more channels where technology is, we have the feedback loops to improve data, but also correct for how we engage with our technology. Because it's very much one of the tricky things about what we've alluded to, but haven't said explicitly, the key challenge. Haven said explicitly: the key challenge for algorithmic approaches to bias is that our notions of bias change. We are society. Think about comedy. Comedy is always at the decision boundary of what's acceptable and unacceptable. And comedy has changed so much, even if you look at this last decade. And that just our notion of what we see as a desirable behavior versus undesirable is a facet of our society. Is undesirable is a facet of our society. So we need our algorithms to be adaptive. And this gets to the core challenge: is that our current way of designing, retraining, updating algorithms is particularly poor at this. So ways in which we can engage and get feedback and understand what parts of the distribution is on mount or painful at are important. It comes back to the question of how do we allocate limited human resources and limited human. And limited human annotation time. And that for me is similar to the question of risk: if we have limited resources, where do we place them? And I think it's an important conversation to have because it's what I work on a lot right now. I work on, well, how do we rank parts of the distribution by what is challenging so that we surface more challenging parts of the distribution we suspect are more problematic to human auditors? Those things like that that I think are very promising. Like that, that I think are very promising research directions where we leverage algorithmic signal to also focus human annotation time. So, that coupled with let's just have more feedback loops on the interface itself, because we often forget technology is not just the algorithm, it's how it's delivered. So, for those Uber drivers, there should have been a channel for them to say, This is absurd, take out this particular payment mechanism. This somehow reminds me of the importance of the human in the loop. There's a lot of talking about this right now, which essentially means taking into account also the feedback of the people that are using. And this can also be done in an adaptive way. And it's really d definitely like a really interesting line of research which is definitely close to Winook. To research into bias and how bias can emerge and how they can be corrected. Okay, so I wanted to touch upon something related to so basically in sometimes like not all the biases are undesirable. Sometimes you really need a bias to learn. I mean it's you know having a bias is actually That having a bias is actually fundamental for any learning algorithm. So, and sometimes in specific areas, for instance, in precision medicine, as we saw before in the slides, you really need to make a distinction between women and men and between different categories. So, I would say that not all biases are undesirable, and there are some desirable biases that needs to be introduced somehow in specific contexts. somehow in specific contexts in order to be specific and to be, you know, to have a personalized, to have a more precise prediction. Now, the question is about privacy, like how to reconciliate this personization and this being very personalized and having all the information about women and men in order to deliver better care and whatever, with privacy concerning With privacy-concerning issues. So, Thomas, how do you see this conundrum between the need of accessing those sensible information, but still the problem of privacy? It's really difficult because, on the one hand, you want to measure as much as possible to get. As much as possible to get as accurate possible picture of the individual that you're treating. But already at the stage of data storage, we have the problem that we cannot store genomic information in Switzerland in theory. And I think this is already, if you think about privacy, we already are in our Are in our research beyond what some ethic committees around the world are allowing. And I think I don't have an opinion on that. So I do not feel entitled to have an opinion on that. But all I'm saying is, it's difficult to grasp that topic. But from a technical point of view, for instance, you have For instance, because some, you know, there are some papers, for instance, that are talking about always having those sensible information and then removing from, you know, having this information there in order for you to recognize, to detect a bias, but then, you know, like masking or removing them. So how this is basically like one. This is basically like one possibility, or the other is just no, no, we are not going to collect any of this data and we find you know workarounds. So yeah, I understand that there are many different ways. Yeah, so again, genomics. You cannot do research on GPM data if you're not allowed to store that data. So that's kind of there is this conflict. And I think when I think when, for example, looking at the conflict of stem cell research, which has been resolved around the world, it was always the ethic committees of different countries who agreed on doing stuff. So it's a policy question. And I think if we as scientists, we can, of course, give our opinion. For example, research on genomic data is not possible if you don't have genomic data. Genomic data. But in the end, everybody, also the policy makers, should be involved in that. Josh, how do you see this problem? Also apply to other areas, like social justice. Because by medicine, we have this type, we always have to fight with this. fight with this but um i i was wondering like also of course this apply also to other to other areas so um what is your opinion about this uh you know privacy uh and sensible attributes uh how to deal with with this yeah i guess my thought is just being very clear on all of the assumptions and make justifying every step of sort of the data creation process the research process and everything Process, the research process, and everything. Just creating enough openness of what you've done and how the system should be working to allow someone else to look at it and be like, oh, I might not have access to the model that you've trained, but you wrote that in planning for this experiment, you decided that it was okay to have 90% men and 10% women for this study. And I disagree with that. Disagree with that. And so having this creating a space where you can very clearly write down or very clearly provide: here is the data I'm using. Here might be some issues or reasons people might disagree with it, but I think it's still necessary because of XYZ reason. And then having, and then just writing down your hypothesis about what you expect to happen. Like if you're saying that there is some mismatch in Mismatch in the sensitive attributes, or there's some issues in sensitive attributes, or there's some historical context to take in that you can write down or provide that expectation of, even though that's there, here is what I believe should happen. And then sort of investigating your validations at your validation tests and seeing, did that happen? Did that not happen? Why? Why not? Just really going into every step. Really going into every step of every decision that you've made so that someone else can look at it and say, I agree or disagree with what you've done. Yeah, that's really interesting. It's also like very, you know, kind of a proposal of a regulatory framework that somehow is accounting for this. And it's very interesting. Sarah, do you think that this is applicable? Like people would do that, like, you know, noting down all the possible, it's difficult. It's difficult now, especially like the level of presentation of. Yeah, I've thought about this a bit because I think there's been some great contributions to scholarship in terms of how we should document and what artifacts we should track. There's two main pain points I see that we need to think about. Firstly, data is rarely a static entity. So often it's refreshed or often it's added to. So the overhead, once you do it once, the question is who maintains it. So we have to. The question is who maintains it? So we have to think a little bit about that. I think most practitioners you meet want to do the best thing for their data and their model. It's equipping them with the tools where it's feasible within their job roles. So I think about this a lot because I think about, for example, when we have engineers who deploy models, I think often, well, how do we empower engineers to see this as a type of like code quality where this is as easy for them to do as checking in code? For them to do is checking and code. I think that hurdle hasn't quite been met yet. So, often when you were doing this artifact documentation, it's a pretty overwhelming experience for someone who's a practitioner because they have to go through it all. Often, the way someone interprets a question is not the way someone else does. I am encouraged by more and more conversation around this. Like, how do we arrive at a checklist that feels more manageable? But that has to be coupled with tooling. But that has to be coupled with tooling. So I often think that one of the hurdles for interpretability as well is how do we empower practitioners to actually have interpretability tools that they use throughout the entire workflow rather than just as a panic moment before they deploy. We need to quickly figure out if we're doing something wrong. And right now, I suspect the main hurdle we have is that this is often seen as a pain point before you get to deploy rather than something that's actually tooling you want to use. Something that's actually tooling you want to use throughout the process. So that's also on us as researchers. We need to think about: well, what are more efficient ways and how do we, what do, what do practitioners actually need to audit as they go through? Because we focused a lot on things like local explanation methods, but most people who are deploying may have huge quantities of data to try and order and try and understand. So, how do we tailor that? With respect to the Tailor that. With respect to the privacy question, I think it's very tricky for healthcare because you've got a combo dynamic of extremely stringent requirements for safety and for auditing, as well as the need for privacy. So if you employ usual privacy techniques, you sacrifice interpretability to a large degree. And but it also, to your point, To your point, it's not a bias. A bias, we must be precise about this language. A bias is when a protected attribute is leveraged for an undesirable outcome. So it lowers the outcome. When we're leveraging a protected attribute to improve the outcome of that group, this is meaningful. This is improving the healthcare outcomes for a certain group. So we had to be more precise. Group. So we have to be more precise about how we talk about this, but also, I think, more broadly, if we relax the requirement for stringent oversight, which is not medical care, but I am very encouraged by decentralized algorithms. So how can we individualize our algorithms based on data that's still kept private, but we never centralize that data? It's too far away from healthcare because it's, you know, the degree of precision doesn't allow it, but it is very. Of precision doesn't allow it, but it is very interesting from a researcher perspective. Federated algorithms give us a lot of interesting opportunities to explore tailoring how we deliver models and also still preserving people's data privacy because we never have to use the data or centralize it. There's a lot of others. I mean, there are a lot of obligations of federated learning. This morning we touched a bit upon. we touch a bit upon upon that and that's uh that's that's an interesting framework let's say precisely yeah it's an interesting framework where we're to study also like to apply the interpretability you know approaches because i think it's completely uh unexplored in a in in a way so and the application to the healthcare is is definitely important i'm not sure about the you know the performances uh but uh but yes i mean of course this comes with challenges Yes, I mean, of course, this comes with challenges and difficulties, so it's a kind of a balance. But yeah, no, a question that I have, you were mentioning, like, we have to be precise on the definition of bias. And so, like, for instance, like a positive action, would you say that is a desirable bias in order to, for instance, increase the information or like? The information or like our knowledge about one particular group in order to deploy, to deliver a better system. I'm quite interested in this, you know, like semantic discussion about the word bias because we generally use it. No, I mean, I think it's very. So, we've had other examples of this, correct? So, even facial recognition API is doing far worse. API is doing far worse on dark-skinned women in particular. That's an example of the model has not learned those features and not that we want to deploy facial recognition at scale, but there are scenarios in which we do want lower error rates on subgroups and we want to almost reduce the disparate error. Reduce the disparate error. So, in that case, we would like better representations. Where it becomes tricky is that a feature which is helpful in one place can be harmful in another. So we have to be careful about data sharing and making sure that we are leveraging and protecting the use of protected features in a way that only can improve outcomes, but cannot be centralized in a way that could then be used. In a way that could then be used to be a harmful outcome. But in general, I think we have to have a much more nuanced understanding of what we mean by the use of protected features, because bias is when we leverage a protected feature for an adverse outcome. And so I think that often we conflate the notion of a protected feature with a discussion of bias, but in fact, the bias part is related to the harmful outcome or amplifying the disparate error rate. Amplifying the disparate error rate. And I think that's a good, it's a good to have a more nuanced conversation around this because it also empowers us to improve outcomes in certain regards, like for healthcare. For example, it's also good for in many ways, like women and minorities have been marginalized from healthcare, mainly because we haven't been present in the data for a very long time. So, this is an important conversation to have. We shouldn't shy away from talking about gender when it can improve the outcome. Gender when it can improve the outcomes and health outcomes for patients. So, in those few minutes that are left, I would like to open the discussion also to the audience. Maybe if you have questions. So, Mark. Yeah, I have a question, I guess, from all of you, because I've been thinking this about this for a while. And what shocked me was what you also talked about, Amelia, of the bias in clinical trials where women were. In clinical trials, where women were not present for a series of reasons, that options were convenient at the time to organize the trial. I mean, the cost of the training and clinical trial and all the experiments with mice, you cannot just point our fingers at those people and be like, oh, you didn't think of including you, and it's your fault. And now our systems are all stupid. What I was wondering was if we can use our algorithms and if we can try to make up for that at the algorithmic levels, that we don't have to. Algorithmic levels that we don't have to redo everything from scratch, right? Because obviously, the solution, the easiest solution to address is to collect more data, that it wouldn't be possible to replicate all of these clinical trials. And the problem is that at the patient level, the doctors and the physicians make decisions based on those results of the clinical trials. So they have a huge digest at the end of the year where they read what's new and they get a What's new, and they get updated, and they change their decision making based on that. But those data that we're providing them are biased, so we are influencing them with the bias. So, my question about the interoperability for healthcare is also like these tools will just give them more understanding about what's happening. But if we don't correct for what was done before, for how we structure the studies, we are not really empowering them much more. Really empowering them much more because they will still be biased in the decision. So, my question to wrap it up, I think, for 15 minutes, but to wrap it up is how can we correct for that bias in terms of algorithms using the data that we already have? Can we generate synthetic data? I'm sure that it's good enough to make it up for what we don't have and we cannot collect anymore by now. Thomas, if you want to comment about that, what we do with Uh, comment about that. What we do with all this accumulated bias there. I don't think it's really possible to generate synthetic data because, um, well, all the data that you generize, that you generate from needs to be mirroring some data that you already have. And that means you just the loop that you're describing, you're just making it larger, you're not breaking it. Um, so I think, I think it's really um I think it's really regarding your point. My opinion is it's a data collection issue. At the moment, we collect without purpose, and that's a problem. We work with large data sets that are just scrapped from a database, and we pose our questions. And I think, regarding those questions, for example, breaking the cycle of perpetuating biases, I think you really need to collect. You really need to collect data, new data, with the research question that you have in mind. And this is really a lot of work, and I think more people should do it. To tip it from there, Willen, that the very prone to selection bias is how you gather data. So, how do you control some data? Well, it depends on the question that you're asking. So, for example, like I'll for example like uh uh i just no please no because then uh like the thing is that like one thing that often happens even like when we compute the values just that like often maybe even unconsciously people just cover data that prove their point right they don't have to do it maliciously but like sometimes that pays unconscious okay so so regarding key values i think that's a different issue yeah um but regarding But regarding the data collection, of course, if you want to have a new study with new data that proves the point of, well, there's a different effect for the drug on women than on men. Then you collect the data with this bias in mind, and then you can answer only this question. You cannot answer. You cannot answer another question. That's what I'm saying. And regarding p-values, I think p-values are a measure for correlation. So I think we should treat it as such and not like a causal thing. And I know everybody heard it, like it's not correlation, it's not causation, but I think this is really deep because if you If you compute a p-value, you measure correlation. And as the example that I showed before, I explained before, the reasons can be, there can be really a very variety of reasons. So then you have to think, well, okay, I observe something here. What could be a possible bias? And then you design an experiment and you collect data and you evaluate that data. Evaluate that data. So that's, I hope I answered the question. I have a question related to Lara's question. Because you're asking that, could we make up for the representation of there's also a reason for the under-representation window. That is because I think in a large number of cases, it's just because people's data are as secret as female cycles. And so the data is more noisy, so it's easier. So it's easier to use for equations. Could any algorithms help take away or make this difference smaller so that it would be easier to use? Absolutely. And I totally agree. It's not a justification. I mean, it has been, for instance, the female mice, that was the reason. You know, the cycles of hormones and the behavior. Hormones and the behavior in the cage is so like a series of reasons for which then you choose the other way. But it's not a justification, like you have to study how to control for all this. Like, you know, it's not just taking the easy path because it's more convenient. And the same for women in clinical trials. Most of the problem is on the recruitment, which is lunchtime, the children, you know, The children, you know, pregnancy. So, there are a series of factors that are not accounted and that are real, and you just go for the easiest solution. Okay, just recruit or man. So, most time it's really like very, very practical, the problem. And yeah, so I don't know. I wanted to like maybe, I don't know if we have time, like five minutes. 25 minutes left, just to your question, Mara, because it's about all those techniques for bias detection and bias mitigation that we have. It's really how useful they are. It's just a kind of trying to do something with the data that we have, try to ask some signals. So I wanted to ask Josh, for instance, how do you see this, like the use of data that has a Of data that has already been collected that we want to use, and we have all those algorithms for bias mitigation and correction. Like this really makes sense. Or as Thomas was saying, like the best option would be to actually, you know, use these tools to identify that there is a problem so that we can produce new data. Yeah, so I think the way to approach it is sort of through. To approach it is sort of through the really through co-creation of different stakeholders. Like, as an example, talking about my own research, I've done some work in bail decisions where like I want to see whether or not judges are being harmful in their decisions of who, like what kind of bail to set, what to do, and all that. And so we have a whole lot of data that is publicly available on who, like, what. On who, like, what does this person's criminal history look like? What charge were they given? What was their bailout come? What judge thought they saw, all kinds of things. And we were aware that there is a whole bunch of historical biases around that. And we weren't sure really how to wade into that problem and try to do something that really felt that we weren't going to make the problem worse. Going to make the problem worse with the data that we have. And how we went about that was just talking to different legal groups, like the American Civil Liberties Union in Pennsylvania. We had a bunch of conversations with them about potential pitfalls and potential concerns that they would have. What kind of legal issues have happened in the past? What things should we consider? What things should we consider? And they really helped us craft a question using the data that was still available that we felt was worthwhile on asking a more nuanced question of what's happening in this system. And so I feel like there are, like I think Thomas mentioned earlier, a lot of it is domain X, like getting input from domain experts on these things because they can help craft. Because they can help craft problems and help you look out for issues that can arise in the data and give recommendations on how you handle all the historical biases and the issues that exist in what we have. Because it's not like we can just go and recreate all of our bail and criminal history data in the US or something. So, really, just working with the domain experts on what to do and what they think about. Okay, okay. So, I don't know if the Okay, so I don't know if there are more questions from the audience. Okay, one. Yeah, actually, following on that thought, I was thinking that actually the problem of bias in database generated by humans might be a problem that we can never fix. Because it's actually a difference between the actual ethical principles that we have now and the ethical principles that we had with Tata was committed. So maybe it's a matter of just it's a constant battle in which It's a constant battle in which we keep as a society advancing and progressing in our ethics, and we keep looking down on our past. So maybe we never want to stop evolving, getting better. And so this might be a battle that we want to look quite further. Absolutely, absolutely. I mean, it's what Sarah mentioned before, that that those bias, like the the the definition and the concept of bias is changing over time and so we always have to uh ignore it. Keep fighting as to use your words. And yes, and the definition of fairness and all those concepts are definitely like a dynamic concept. So, okay, thank you so much. Thank you so much for this conversation. I really hope that this conversation in general keeps It will keep going, like also in other circles, and together with all the different stakeholders and actors that we identify in all these interventions. And thank you so much also for the messages that you were sending out about the importance of this topic in our fields and in general for our society. So, thank you so much and thank you. And thank you all for the questions and your engagement. Thank you. Thank you for having us. Thanks, everyone. Thank you. Thank you. Before anybody mix, we're going for a hike today. Uh a truck height is great. I know what we're doing.      