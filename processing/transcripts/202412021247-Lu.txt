I would like to thank the advisors for the warm communication. It's really nice back to being in depth, especially seeing the nice mountain muke. Okay, so back to business. So today I'll talk about interior C2 estimate for Hessian Cauchy equation. So basically the outlay is that the first part, majority part, I will talk about the history and then the later part I will talk about recent progress. So, as we see, first part I was talking about is second-order linear equation. It's all we're actually learning basic PDEs, a little bit advanced than undergraduate PDE. Then I talk about hashing equations, then talk about a related one in geometry, which is called special Lagrangian equation. Then I'll talk about hashing quotient equation. Actually, all of them are closely related to each other. So, the first thing we talked about is So the first thing we talk about is this one we actually learned even in undergraduate. So I was teaching the second year calculus. So we actually proved the mean diagram property for harmonic function just using divergence theorem. So if you take the gradient of your function, then this is still a harmonic function. So you can still use mean direct property for this harmonic function, which is diu. So then you're actually using a divergence theory. Actually, using divergence theorem again, so you can actually double your gradient using the L infinity ball of your function. Okay, so this is very elementary. Alright, so this is very special for harmonic function because in general you don't have minimal property, but a similar thing actually holds for second-order linear equations. So if you haven't really seen this costly solution, just neglect it, just consider us. Neglected, just consider a smooth solution. Okay, so here this is, I just want to say that this one works for weak solutions as well. So basically, you have your second-order degree equation, which is uniformly elliptic. That means your Aij cosi cosj is only above and below. Or you want to say your matrix Aij is positively at. Then we can find the L infinity norm on the half-pole by the L1 norm of. By the L1 norm of log function. So here we don't really need L1 norm. Actually, it can be any LP norm for P greater than zero. So this is in literature this is called the local maximum principle. The upshot here is this A actually only need to be L infinity. We don't need any continuity or anything further. So if we look at that history, then we know if this function, this equation know if this function is divergent structure then this is the classic mesh uh de george mash mosa iteration uh if for non-divergent structure like this this is the classic uh krylov satarov estimate okay uh note that the important part here this one works for this causing solution which we will actually use beta all right so this is analogous of the uh mean value property for the second order equation so uh the The linear equation. So the second one is gradient density. So this is actually easier. Basically, use the maxim principle. You can show that the gradient on a half pool is bounded by some issues. So here, we assume this Aij is a Lipschitz and F is Lipschitz. You can bound this gradient using the lower order term, which is L E given the norm of this U. So this is really analogous to the gradient. So, this is really analogous to the gradient estimate, but here we cannot use the mean item property. So, you do a maximum principle, say, let's just take the maximum of gradient u squared and times a test function, then just do computation. It's, well, it's not difficult, but some work. Alright, so that is the second order equation. So, what I want to say here is: if we double-check this gradient estimate, you will see that. Asthmate, you will see that there's no information on boundary. So, only thing we know is: okay, so this is a C3 solution, for instance, and the right-hand side of your constant C depends on, say, the coefficient Aij, your function F, and lower order estimate of distributed. So here, lower order is just real distributed. So it's not like a derivative problem. So automatically, you will have the boundary condition, your derivative, and your domain, omega. Your domain, omega, for instance, convex, or so on, so forth. Here we don't have any information. So, this is in literature is called interior estimate or purely interior. So, sometimes it can be a little bit confusing, but let's just think it's material. Okay, so now if we, so this is a linear equation, so we can take derivative, okay? So, now you take any derivative you want, apply it's a ingredient estimate, okay. Not exactly the same, but a similar one, so you can actually. But a similar one. So you can actually get interior estimates of all orders, say CK estimates of this. Alright, so that means we can get interior estimates of all orders for this linear equation, so end of story of linear equation. So question is, what about non-linear equations? Can we actually get those things? So this question is too big. So for non-linear equations, there's one big side. One side, one big difference between linear and nonlinear is that for linear equations, basically after you do gradient estimate, more or less you're done. For nonlinear equations, you have to do C2 estimate and go forward. I'm talking about second order non-linear equations. And it isn't known that given any random second order Fouling non-linear equation, you're probably not able to do C2S. able to do C2S. So most of the time people are actually worried about the C2 estimate. So basically you have say L infinity estimate, gradient, C2. So sometimes L infinity estimate is most difficult, sometimes C2. So I will focus on this C2 estimate. Alright, so what kind of fully non-equation I'm interested in, right? So I can write, say, millions of equations. Write, say, millions of equations, but most of them probably do not make sense. Okay, so the first type of equation I'm interested is called Hassi equations. If we recall back to the linear algebra, then we know we have those elementary symmetrical functions. So this is just times the eigenvalues of this matrix or this lambda. Lambda and then do the summation. So, in particular, so if you say k is 1, then this just the trace of this lambda, which actually gives you Laplace equation. So, if you take this k equal to n, then this is the elaborated Benjamin Pai equations. So, now if we go back to geometry, so suppose this lambda is the eigenvalues or principal curvature of the hypersurface. A curvature of a hypersurface, then this k equal to 1 is mean curvature, and then k equal to 2 is scalar curvature. This one you actually need to use the Gauss equation. And then K equal to N is Gauss curvature. So this type of equation actually just has geometric meanings and makes sense, okay? So we'll see. Rather, we do have, say, interior C2 estimate for this type of equations. So that's our first thing. So that's our first thing. So the good news is for n equal to 2, it is a case. That's done in the 1950s by Haines. So suppose you are C to alpha and the convex solution. So in this talk, I will not be very rigorous about the cones for the eigenvalues of this lambda. I don't want to mess it up. So in this case, the In this case, the natural cone is indeed convex, so just say convex. So this is in n equal to two situations. So then you do have interior C2 estimate. This one depends on C alpha norm of this F and the info norm of F and U as Liches norm, C01 norm of this U. So this is indeed a pure interior C2S minute. Some remarks. Okay. The first one is: if you try to hands and choose the proof, they actually need to use complex analysis. So that's the reason why they can only do for input two. Because complex analysis, at least in this situation, only works for two dimensions. So there is one new proof by Chen Han Long in 2016. So they actually use Maximum principle, so with complicated test functions. A complicated test function, then you do the computation. Okay, so there's another one by Jacqueline Rio using partial Regenda transpose. So this one is much easier. So basically, you can transpose this Montjampai equation into a linear equation. Then you can use standard linear equation estimate to do this one. The third one is, another one is in the original result of HINS, he actually required F to be C3. C3. So the requirement by the shoes paper is C2 alpha. This one can be weakened to say C2DB by our previous work. So everything will work perfectly for input 2. So now the famous program of example tells us this thing is not going to happen for high dimension. So he actually gave a very explicit example. So this you can write it off. Example. So, this you can write down immediately. This ux is just like this one, then you can compute by hand. Okay, this f is explicitly written. I don't want to write down, but it's not difficult to check. So, this sigma depends on n here. So, this sigma depends only on n. We can compute by head. Okay, so in a very small ball, then you do have this discussion solution u, okay? So, this d square u equal to f, and f is actually analytic. F and F is actually analytic, not only analytic. But as you can see by the expression of this U. So U is indeed C1 up to 1 minus 2 over N, but it's not belong to any first product. So this is the, so in particular, it's definitely not C2. The main reason, as you can check, that is for this U, there's a C2. For this U, there's a singular set. A singular set is exactly one line, okay? So passing through the origin. So if you require, for instance, if this one is, say, strictly convex, or you require, say, you have a domain which has smooth boundary data, then you can show that this one is, you can show this U is smooth. But here, the main reason is we don't have any. is we don't have any information on the boundary, so this one can happen. Actually, so there's one more thing. Actually, one can show by Erber's result, if you assume, presumably, U is C1 beta for some beta greater than this exponent, then this one is smooth. So this one is a critical exponent. Alright, so this is the Men-Sham-Pai equation. So n equal to 2, okay? I'm greater than x swing. To 2, okay? And greater than 3, no? Okay, so now what about the Hessian equations? So Erbas has a very easy observation. We can just extend the Moon Zhang Pai equation, say k equal to 3, then extend it to trivial V on other direction, then you have sigma 3, and then similar thing for other. So that extends trivial, say, sigma k equation, you do not have interior of C to S. Not even dimension. Not your dimension increase. Yeah, your dimension n is greater equal than k, right? Yeah, so what can ask if the dimension just goes to 3? Is this counting? Is that still counting example? So k has to be less equal than n. So if n is 3, so then you have sigma 3, which counter example, right by combo for rap, sigma 2. Sigma 2, then it's next. Sigma 2, then it's next slide. Thanks for the motivate us. So it's the extremely harder case. So right now this is the, so say sigma 2 equals 3. That's the easiest situation, right? So the first breakthrough is by water Louis, so they actually put By order in the UN, so they actually prove the sigma 2 equal to 1 equal to 3. So, this one we'll talk about later. This is actually related to special Lagrange equation, okay, for special Lagrange geometry. So, now the second one is sigma 2 equal to f equal to 3. So, this is actually not trivial. So, everyone will think, okay, so if it's true for constant, then it's true for a function. But this is not subtle because for n equal to, for sigma 2 equal to 1, so this is coming from. Could do one, so this is coming from special Lagrangian geometry. So you have to use those knowledge. But now this one is not related to special Lagrangian geometry, at least for several years ago. So until like probably I think last year, people realized you can do twist special Lagrangian equation, and then this one can be deduced easier. But at that time, when Chu proved this theorem, there's no such thing exists, no one knows. Okay, so this is. Those okay, so this is a very nice result, and this uh Shen Kai and Yuan for n equal to two, sigma equal to equal to one. So this is proved, I think, in June or May last year. And I think I just checked recently, so this one is going to appear in AnosoMass. And the last one, so this one is new breakthrough. The reason is simple. So there's no special Lagrangian geometry. Any special Lagrangian geometry in here, so we have to do new methods. Okay, so they actually have a new method, a completely new method using blow-up analysis and other things. All right, so those three without assume any convexity situations. So if you assume convexity situations, so that's the actual condition, okay? So actually, people can prove, say, sigma 2 equal to f and n greater equal to n 3. We have different We have different kinds of proof. McGumel, Seoul, and Yuan actually using blow-up analysis. And Wa and the Chose actually use a maximum principle analysis. And Shanghai and Yuan actually use the agenda transport. And Mooney to consider the discussion solution. Basically, say you want to prove that the support plane has dimension good enough so that you do not have. Good enough so that you do not have things messed up. Okay, so this is the history of the sigma 2 equation, which is a difficult one. So I would like to remark that up to now, we know sigma 2 equation is okay for 2, 3, 4 dimension, and dimension 5 and above is true under actual conditions, but we don't know if it's true or not without those actual conditions. So this is a very difficult question. Okay, all right, so that's the word for sigma 2. Okay, so everything we talk about is about d square u. Okay, so if we change our view a little bit, we, I think in the first talk, JT talked about shorthand tensor. So this one, this thing inside is d square minus du du minus gradient u squared over 2i. So this one is the shorthand tensor if you evaluate. Tensor, if we evaluate just the background metric is ingredient up to some normalization, because normalization everyone is doing not exactly the same, but this is if we do the normalization like this, so we have this equation. So for this one, people actually know that we do have interior zero estimate by 1 and 0 here. So here, I want to mention that this is a key thing that This is the key thing that makes things work is this minus gradient square over 2i. So if we are in a negative cone, so basically that's just for this whole tensor, you put a negative sign, then it's actually a long-standing question. Whether we have interior C2S for the negative cone. So that's not no. So this thing is actually the most important part. Okay, so this is it for sigma IK equation. And so Fiji actually can prove interior C2. Interior C2 SMA works if we restrict it on this tensor for very large fully nonlinear operators. So I'm going to restrict myself still to T square u, not to this tensor. Okay, so I said before that the sigma 2 equal to 1 is very special. So this one is related to the so-called special. Is related to the so-called special Lagrangian geometry. Okay, I will do a little bit very, very brief of mention of special Lagrangian geometry because I'm actually not an expert. So we are in the easiest setting. Suppose you have x, okay, which is a point or vector in Rn. Now suppose we have a submanifold, which is represent by gradient u. So u is a function raw r into r. So now this is going to be a separate manifold. So this is very special. Several manifold. So, this is very special. In general, you cannot write your sub-manifold like a gradient function. So, if in this situation, this is automatically going to be Lagrangian, which I will not discuss what is the meaning of Lagrangian. So, here, you can actually compute easily, okay, by Harvey and the Lawson, that the mean curvature vector, because we're manifold, is the 90-degree rotation of this gradient theta, and this G is. And this G is a metric because we are submanifold, you have a metric. Okay, so now this theta is called Lagrangian phase. Okay, so this theta satisfies this arctangent lambda i. If you do the summation, that's eight theta. So in a certain sense, okay, so this one should be regarded as a potential equation of mean curvature vector. But if you check literature, people want to write this equation as so-called random mean curvature equation. Run to the mean curvature equation. Okay, so if we check a bit, so it's not very consistent but understandable. Okay, so this is the equation we're talking about. All right. So if you can see, right? So your C type is zero, then the gradient will be zero. So your mean curvature vector will be zero. So it will be a submanifold, okay? Minimal submanifold. And Harvey and Austin showed that for those Lagrangian submanifolds, if it is Manifold, if it is voluminizing sub-manifold, if and only if this is constant. Okay, so this is by the Hardy and Marson 82 paper. Okay, so basically, when we talk about submanifold, because a high co-dimension thing makes everything terrible, okay, so the theory of submanifold, minimum sub-manifold, is not as well understand as the theory of manifold. Well, understand as a theory of minimal surface because the co-dimension, even in co-dimension two, it's just terrible. So, this special Lagrangian gives us some hope because this is a very special structure. So, this sometimes we say it's just a local version of this Lagrangian special Lagrangian geometry. So, if we can understand more, then we can understand special magnetic better. But if there's the analysis. The analysis of this special case is the local case, is not as good as the general case, say special Lagrange, mean coverage, Lagrange mean curve flow and singularity and so on and so forth, is about 5 billion each. Okay. Alright, so here, as we can see, the gradient estimate, okay, so one thing, so this C2 estimate is actually. Is actually the gradient estimates of your position vector because x, which is your position vector, is du. Okay, so in a certain sense, if you can see a step manifold, then this is the gradient estimate of your submanifold. So here, C2 estimate of U is gradient estimate of your submanifold. Okay, so the first theorem I want to talk about is by Warren and Guern. So they did it in dimension three, okay, and by 3 okay, and by 1 and you add so high upper high dimensions. So suppose you have this equation, and suppose you have restrictions on your face, which is greater or equal than n minus 2 pi over 2, which I will talk about later. Why this angle? So basically, in special branches, so you have n branch, okay? So this one restricts to the two outmost branches, okay? So we actually have very good regulatory algorithms. We actually have very good regulatory on those things. But all other branch things are not as good. We'll see later. So then you do have this purely interior C2S fit. Okay, depends on N, depends on the C01 norm of this fit. So this is perfect. So the requirement is fixed is essential. So we have other results. For instance, by term, water, and uen, assuming the convexity of this U, then you also have Of this U, then you also have this interior situation. Alright, so in our previous work, we actually studied this one in this theta X situations, okay? So this potential equation for the Lagrange-Minkurge equation. Okay, so we have very similar situations. Suppose the theta in the same range, the minus 2π over 2. Minus 2 pi over 2, then you still have the interior C2 estimate. So it depends now extra because of this C11 normal, C2. Okay, so because of in nature there's an integration bypass nature, so actually one can show that by recent work at Joe and Dean that you don't really need the C110 or C theta, you only need C011. The C011, C011, okay? Because after integration, you can get one regularity of, one degree off. So now the supercritical case, which is not greater than equal to spread, was obtained earlier by the child. All right, so this is this one. So as I said, we do have singular solution. So all those examples are actually done by for n equal to 3. So for n equal to 3, so you have Okay, so for A equal to 3, so you have 3 branches, right? So the branch less or equal than minus pi over 2, and from minus pi over 2 to pi over 2, and greater than pi over 2. Okay, so Nadera Shubili and Vladu and Wang and Yuan and Moon Yen Salen, so they depending have different singular solutions. So the one I should mention is the Wang Yuan singular solution. So this one is very special. So basically, smooth. So basically, it's smooth outside the origin. So, the only singular point is your origin. So, now it's smooth everywhere. So, that means even if you impose boundary condition, it won't help. Because your boundary, say, on the ball of radius one, it's smooth everywhere. So, that won't help. It's not like the Pogrellov example, you have a line, so then if you assume smooth on the boundary, so then this one will not happen. But here, But here you can't hope that. So now the Mooney and Samine example is more dramatic that it is actually C01 but not C1. So it's Lipschitz but not C1. So this is actually disproved conjecture by Nadera-William but as you can see from Wang Yang's paper basically this UM is C1, right? C a little bit more than C1, but not, right? More than C1, but not right. But this Munier-Savin example shows that there is not even C1. All right, so that tell us for the special Lagrangian equation, it's not as good as we want them to be. Okay, so we alright, so as I said, the C ma 2 equal to 1 is very special. So that is related to the special Lagrangian equation. To the special Lagrangian equation, how the relation is. So we can write the special Lagrangian equation in equivalent form. So this summation of the sigma k equations. Okay, so this is not good. If you want to do analysis of this equation, it's going to be a nightmare. So let's just examine, let's say, to make our life easier, say n equal to 3, see what will happen. So if we restrict our n equal to 3, So if we restrict it down n equal to 3, so then this k will be two situations. Either it's 0 or it's 1. So now this sigma 2k plus 1 thing, this will be sigma 1 minus sigma 3. So this sigma 2k thing will be just sigma 0, which is 1, and sigma 2. So your equation is much easier. Of course, we're just doing simplifications. So now we make our life even better. So suppose theta is pi over. So suppose theta is pi over 2. So then this cosine is going to vanish. So you will get this sine pi over 2, which is 1. So then you actually get sigma 2 equal to 1. So this is why we can actually get that equation from this very, very beginning. So now if you make your life another way, say sita is pi. So then the sine theta is going to vanish. So you get sigma 3 equal to sigma 1. So this is. So this thing actually. So this thing actually already observed in the 1982 paper by Hagnier Lawson. So in a very special case, you can have those two beautiful equations in A equals 3. Alright, so sigma 3 over Sigma 3 equals sigma 1, which is, if you just divide by sigma 1, so this is a quotient equation. Okay, so now we know except Hessian equation. We know, except Hessian equations, Hessian Cauchy equations also a bunch of things that make analysis very attainable. So, a naive question is, well, we can do sigma 3 over sigma 1. Can we do other things? Okay, so it's going to be happen. So, we know that sigma 2, you can do a little bit of things, right? How about Cauchy? Gamma one code. So here, this is convex cone, right? You're always working with convex solutions. Because you sigma. So, okay, so for this one, for this one, you only, if you are in your special Lagrange geometry, you only need this theta to be greater or equal than in the very beginning of here. You don't need any assumption, right? As long as the theta is safe. Theta is say this is you don't need any assumptions, right? Because that is already in the good one. So this is only need to be greater even than pi over 2. But if we're talking about the hash and quotient in general, so we're gonna assume good code. So in my case, I'm gonna assume convex. Alright, so I will talk about two things. Talk about two things. The first one is the estimate. So here I will in n dimension, okay? So sigma n over sigma k. So this k has to be n minus 1 and n minus 2. We'll see later why. So the top degree has to be n. So because the column degree has to be n, so the natural core is convex core. So that's why I don't want to talk about other things, okay, to confuse you. So now, if you are seeing for convex solution, If you are C4's comma expression, then we do have interior situations. So this one just depends on usual things, say n, c11 normal f, internal f, and c 01 normal. Okay, so this is the nice distribution, okay? Okay, so this requirement, okay, so of k is actually essential, okay? Which I'll see immediately to the next page. Okay, so basically, this is the advanced theorem, basically saying, okay, so we do have interior C2 estimate for this situation. To estimate for this situation. So, next page, we don't. So, if you have this n minus k greater equal than 3, so in the above theorem, you only have n minus k is 1 or 2. So, not greater than 3. You can just do similar things as program, for example. So, just change your exponent. So, in progress, the exponent is just 1 minus 1 over n. So, you just change it to be 1 minus 1 minus. 1 over n minus k. So then this one sets by perfect equation by the sigma n over sigma k equal to f. Now you can compute f explicitly, so it's going to be an analytical function, which depends on n and k. So the sigma depends on n and k. So this one will be, as you can see just by our exam, expression, it will not be c1 beta for any beta could be to say this one minus two over the minus p. Minus 2 over the minus p. So as you can see, you have to be this m minus p greater if it has 3. If it's 2, it's not going to work. Alright, so that is this setting. So very just analogous to the progress of the example. So in this thing, we have the full understanding, right? So if k is the minus 1 and the minus 2, we have estimate. If k is the minus k greater than 3, say no. Okay. No. Okay. Alright. So here is some idea which I'm going to express what things can work and what things not work. So the first thing is normally when we do estimate, we want to do a time cutoff function and then we do maximum principle. So that's a standard way to do it. So why is this thing we cannot do? Okay. So there's a very hidden reason. Very hidden reason. See, suppose we have a test function Joe. So suppose if you are familiar, so suppose I have lambda one is the largest value. So I want to do maximum things from here. So normally, it's the same thing as take a lot. So you will have this, if you do the if you do maximum principle, so you will actually have this. And then something about the low. So, those things, if you check, because we talk about cutoff function, so low will be zero when on the boundary. So, low will be very small, okay? So, this thing will blow up. Similarly, this thing will blow up. So, we cannot, okay? So, basically, the thing coming from this law we cannot under control. So, we are not able to control those things. The reason for the why I do. For the Grand Wands paper, we will talk about the conformal thing because you have this minus gradient square over u, so this is really big enough so we can control anything here. But here we don't have that term. And for quotient equations, there's another thing that is different from the hashing equation. It's the following thing. So if we consider f is sigma k, then f i i, you will get something like. You will get something like sigma log, which is the largest n. But if f is, let's say, just sigma n or sigma n minus 1, so this fi, you'll just get some constant sigma. Okay? So basically, or if you do other things, so if you do the summation of those things, for hashing equation, it will help you, but quotient equation, because you have divided those things, so it won't really help. Okay? So that's. Won't really help. So that's why this is interior, a pure interior estimate. We know from history that we can do so-called Pogrelov type interior estimate for hash equations, but up to now, there's no result for hash equations. So even though we have an interior estimate for this sigma n over sigma m minus 1, m minus 2, but we don't actually, in literature, Actually, in literature, there's no such result for gravitatized as it. Same thing because of this. Alright, so this is a difficulty. Alright, so how are we gonna do the estimate? So we'll not do the, just do the maximum principle. So we're gonna use three steps. So this is a new method inspired by Shenka and Yuan when they prove the sigma 2 equation on the semiconductor. By two equation on the semi-convexity condition. So, in this case, you actually do three steps. So, the first step is you do a Jacobian encoding for this law of lambda one. Okay, so this part basically take care of all the third-order term. So, normally for this part, the third-order term, as we know, for doing normally PDE4C to estimate, so normally this third-order term is most troublesome. In general, this Jacobi. In general, this Jacobian inequality we don't even know for hashing equations. For general hashing equations, we don't know. For this one, the very special structure of the quotient equation actually we can do, which is quite surprising, okay? Because imagine Hessian quotient equation is more complicated than hash equations, right? So if we cannot do Jacobian equity for hashing equations, we do not imagine we can do that, but turn out that's true. That's true. Alright, so the second one is we do a Legendre transform to rewrite this Jacobian query, okay, to make it to be uniformly elliptic. So that's the exception part. And the third part is we're going to use the local maxim principle I talked about in the very, very beginning of this talk. So you can bound your maximum using integration. So then you can use integration by bounds. So I will do a little bit more detail. Do a little bit more detail. So, the Jacobian equally looking like this. So, f is our operator, say sigma a over sigma k. So, fii is just derivative of ui. Here, I'm lazy. I'm saying everything is diagonalized. Okay? So, technically, you should write fijpij. So, I'm saying, okay, everything is diagonalized, so I can just think this. Okay, so now this b is just log lambda one. So, basically, you guys say, okay, not only now this b. You guys say, okay, now this Bi is set up. So you have, say, Bi is just H11i over H11. So this CN is positive. So not only you can take care of all the third-order term, you can have extra about the CN. So when we do this Jacobian encoding, when we take it out of the third-order term, so the key part is the concavity encoding. Key part is the concavity inequality. Basically, you want to say the second order of this F, okay, give you some good terms. So this is what we call Jacobian equity. This is actually proved by unpublished notes by Guan and Sroka. Okay, so as I said, this Jacobian equity not only takes care of the third order terms, but you have extra. Okay, so this extra is just in this term Fi Bi squared. Fi Bi square. Okay, so now let's just re-examine what this means. Okay, so this means if you just don't care about the right-hand side, because this is the CN thing you can grade equal than zero. So you are actually a sub-solution. This U, this B is a sub-solution of the equation, right? So say Fi is obviously positive definite. So this B is a sub-solution of this equation. So now the fact is we don't have So now the fact is we don't have estimates on this d square u. So the only thing we know is Fi is elliptic. So this equation is elliptic, but we don't know if it's uniformly elliptical or not. You don't have upper bound and lower bound of this FRI as this matrix. So that's basically the difficult part for Linani PDE because you don't have C2SM, so you can. Because you don't have C2SM, so you can see nothing about this FRF. So, in particular, it's not uniformly thick, so we cannot use the very beginning the slides what we talked about. So, now here comes the second step, which is the Legendre transfer. Okay, so in coordinate X, we are not good. So, we go to coordinate Y. So, what is coordinate Y? So, we just do a Legendre transfer. Okay, so the reason, so here you So, here you can actually do Logan transform for u. You don't have to do the Logan transform for u plus x square over 2. But that one rely, so if you do the Logan transform for u, everything works except that for k equal to n minus 1, it works. For k equal to n minus 2, it's not working, so that's what you have to do a little bit adjustment. Okay, so you do the general transform, so you have y. y is just du plus. y. y is just du plus x. So that's new coordinate. So you have d squared w, okay, which is inverse of d square u plus r. Alright, so you can define a new operator which is g. It's operator of d square w. Okay, so it's very standard. You just write as this equation and then this is equal to minus f. You don't have to be minus f, you can say you can define new operator equal to all f. That doesn't really matter too much. Okay, so now. Okay, so now your V star is your V, but in just depends on Y. So in Y coordinate, okay, so now your Gii is just taking derivative of G over Wii. So just transform everything in Y coordinate. You will have this thing Gi I B I I star. So everything is in Y coordinate. So this is just Jacobian inequality. Instead of writing in X coordinates, now it's writing Y coordinate. But the most important part. But the most important part here is in y coordinate, this GII is uniformly elliptic. So that is the key to make everything work. You can do, let's just say, Mohian-Pai equation, you have Jacobi inequality, but if you do Logan transform, it's not going to be uniformly weak. So this is a very special property. In general, you have a fun you have polynomial PDE. If you just do this Legendre transform in general, you are not hoping to get General, you are not hoping to get the uniformity. The hidden reason why, say, n minus k greater than 3 is not working is if you do this thing, this gi is not going to be uniform. So the other thing actually can work, but this thing is not working. So this is a hidden thing. So as I say, if you do not use u plus x squared over 2, then for the sigma n over sigma n minus 2, it's non-confidential. Sigma n minus 2, it's not going to be uniform unit. So that's why we have to plus something so that gives this. So this is very, very special. So now your B star is a sub solution for uniformly elliptic equation. So that's in the very, very beginning. We know you have a local gradient, isn't it? So we can bound this B star using the L1 norm of this B star. Okay? Which is exactly what we have here. Which is exactly what we have here. Okay, so let's just say that's an origin to make things easier. Alright, so now this B star lives in Y coordinate. So you want to transfer back into X coordinate. So you have to put it Jacobian, which is just dy over the x's, just determine d square u plus i. So now you use your equations, okay? And then you want to do integration by class, okay? So your ball will be niggard. So, your ball will be larger and larger because of your test cutoff function. But now you can work with again, you're gonna use the Jacobian economy each time you do integration by parts. So you're gonna just taking care of this gradient term using the Jacobian Ukraine. So that's one thing I probably should be mentioning is that our F here is not divergence free, okay? So this is not like hashing, okay? So, this is not like hashing. Hashing is a divergence free. So, we can't really just use this f immediately. So, you have to do a little bit. But it's manageable. So, basically, the idea is, okay, I want to do integration by path. So, to get this. So, in there, you can actually do that. So, that's basically it. Thank you. Post repair.