Of theta, and then the output is y. And then you want to do an inverse problem, which is suppose you have observations of the outcome, how can you infer about the model parameter? And we will be using a probabilistic approach, meaning that we will assume that x follows some probability distribution. And in particular, the idea of Bayesian inverse problem is that you want to, well, you have some prior knowledge about the distribution. Have some prior knowledge about the distribution of x, and then you want to use data to actually correct the prior distribution to actually get posterior distribution that is basically data-informed distribution. So, the setup is that you have a prior density, and then you can get posterior density by basically the Bayes' rule. So, this is presumably something that everyone knows. So, you basically need You basically need a likelihood function which describes sort of like given that if your model parameter is small x, what is the probability of observing the observations? And then you multiply that by the prior distribution, and then you divide everything by a normalization, which integrates the numerator over all possible x values. So this is why this whole thing is proportional. This whole thing is proportional to basically likelihood times the prior. And you can call this distribution, well, this expression to be rho. And oftentimes, you actually have closed form expressions for rho. For example, if your model is some deterministic function of the model parameter plus Gaussian noise, then you simply will have basically Gaussian as your likelihood, and then you multiply that to a prior. That to a prior, although it doesn't have to be so. So you will have some expression for this density, however, it's un-normalized. So you only know rho, you don't really know the normalization constant in the front. And the goal, for example, could be you want to do statistical estimation, you want to do statistical inference. So in general, you pick an arbitrary test function. So that's an observable. You want to know, for example, the You want to know, for example, the meaning of this observable condition on knowing the data, knowing the observation. And then, of course, oftentimes you resort to statistics. So you want to collect a large number of random variables, xk, and then just use an average. So you want this xk to be actually samples that follow the posterior density. So how to do this? How to do sampling from To do this, how to do something from unnormalized density is actually a classical problem. For example, so there's something called Langevin Monte Carlo, which is pretty popular. So it's a very simple algorithm. So you just let the potential F be negative log density, and then you use simple iteration like this. And if you iterate for long enough, your iterate will be actually a random variable that is following a distribution that is very close to the target distribution. Close to the target distribution. So everything is nice, except you have to be very careful knowing that x here has to be actually in an unconstrained d-dimensional Euclidean space. So that was easy, but what if your parameter, your model parameter, is actually from a constrained subspace, a constraint set? So it could be actually a subset of the D-dimensional Euclidean space, but Dimensional Euclidean space, but it's actually some constraint set. And you want to sample a distribution that is actually supported on this constraint set. And of course, this is relevant to English problems because, for example, maybe by some prior physical knowledge, you know that your model parameter is, well, for instance, positive. And then your density may actually say nothing about this constraint, but you really have this additional constraint. Constraint, but you really have this additional constraint, and you want to generate samples that are actually in this constraint set. So there are actually a lot of approaches for this problem. And as far as I know, existing approaches that have theoretical guarantees are actually mostly for convex constraint sets. For example, you can have something called projected Langevin algorithm, which is basically Algorithm, which is basically Langevin Monte Carlo. So this is Langevin Monte Carlo, but then at each iterate, you actually have to project your result to the constraint set. Okay, so you can sort of see why you need convexity because otherwise this projection operator is not even well defined. And this is a very natural idea. It actually worked. I mean, there are theoretical guarantees, but you can also see that this is computationally costly and maybe slowing down computation. And maybe slowing down convergence as well. You have a family of Markov-Chain Monte Carlo methods, very powerful, but they usually work for limited distributions on limited constraint sets. And then you have something called mirror-linger-run algorithm, which is the thing that I'll focus on today. So here is actually the iteration of mirror-linger. Of Mirror-Langevin algorithm, it does this constraint sampling problem, but of course, this expression is very complicated. There are a lot of things that I need to explain. For example, what is phi? What is this thing? How do I construct a mirror language one? Why, and how it works. So the main tools that I'll be using include the following. So first of all, I will be actually exploiting interactions between dynamics in continuous and discrete time. Continuous and discrete time. Number two, I'll be actually using an interplay between optimization and sampling. And number three, I'll actually be leveraging geometric ideas. So in order to explain, you know, even just the terminology, let me start with some warm-up examples. So these, I'll start actually with unconstrained problem examples for unconstrained problems. So like I said, Langevin Monte Carlo does unconstrained sampling. Does unconstrained sampling. It uses this simple iteration. And it's actually, of course, an algorithm in discrete time. And you can actually view it as the time discretization of some dynamics in continuous time. So it's actually the Euler-Maruyama discretization of over-debt Langevin dynamics, which is given by this simple SDE. And this whole thing is really a sampling generalization of optimization because in optimization Of optimization because, in optimization, of course, you know, everyone knows gradient descent. You simply do iterations like this. And also, you probably know that gradient descent is a time digitalization of a continuous dynamics called gradient flow. If you compare these two guys with the things above, you see, okay, so you really just add noise to gradient flow, and consequently, you also add noise to the discretization. So, a lot of people say, oh, that's A lot of people say, oh, that's how the sampling and optimization are connected. The connection is actually much more profound here. So, in fact, one beautiful thing about these two concepts is that yes, over-depth Langevin dynamics actually does sampling in the sense that as time goes to infinity, the distribution of the solution will actually be given by the target distribution. The target distribution. So, this dynamics is doing something for sure, but it's also doing optimization. It's actually doing optimization in a space of infinite dimensionality. So how does this work? So, if you look at over-dempted Langevin dynamics, OD as an abbreviation, it's a stochastic process. Okay, so at each time, X actually follows some density. X actually follows some density. And you can try to understand how the density evolves in time. And in fact, you can write down a PDE that describes the density evolution. It's going to be an instance of a focal-Planck equation. Okay? And then the density has no longer any stochasticity. It's just some object living in an infinite-dimensional function space. And if you look at this density evolution PDE, it's actually a gradient flow. PDE, it's actually a gradient flow. Okay, so you have basically an objective function, which is actually the KL divergence, that basically describes how far away your current density is from the target distribution. And then you basically do a gradient flow with respect to this objective function. But you have to be careful. For example, so you have an infinite dimensional setup. So you really have to define what do you mean by a gradient. Fine, what do you mean by a gradient? And here, if you pick the gradient to be with respect to washerston distance, washer stand metric, you can actually prove that this gradient flow is exactly going to be the Fokker-Planck equation that gives you the density evolution. Okay, so this is why sampling is closely related to optimization, and in fact, they are almost the same thing. So that's unconceptual. So that's unconstrained sampling. Now, let me also prepare a little bit about constraint optimization. So, in fact, let me start with some geometric preparation. So, our constraint set is just the curly D. It's given to you in most cases. It doesn't come with any geometric structure given to you. APRA, you can actually assign a geometric structure to it. More precisely, you can consider something called tangent bundle of this thing. Bundle of this thing, you can assign a metric to the tangent bundle. That sounds scary, but in fact, you know, it's very concrete. You can represent the metric by a positive definite matrix G. And then if you give me two vectors in the tangent space, then I can define an inner product. The inner product is just g sandwiched by these two vectors. You can, of course, choose to use Euclidean space, so you can. So you can just use identity as the metric tensor, but you don't have to. For example, you can actually choose the metric to be the Haitian of some function, some convex function phi, and then you will have something called a Haitian metric. And in general, if you have a metric tensor, you can do actually optimization by still doing gradient flow, but you actually have to do gradient flow in a Riemannian. Have to do gradient flow in a Riemannian space. So it's pretty much the same thing, except you have to use some metric tensor inverse in front of the gradient. I mean, if you care about y, very roughly speaking, really, you have to have a metric in order to have a gradient. If you just do nabla f, it's something called differential, but if you really do this appropriately by doing this correction via g inverse, you get something that is agreed. You get something that is aggregate. But I will not go too much into the geometric aspect. But instead, I want to say if you choose your metric to be the Haitian metric, then the remaining gradient flow, of course, becomes something like this. This is something called natural gradient flow. And in fact, the name is probably because it's Euler disputization, which is like this, is something famous. Like this is something famous in the literature, it's called natural gradient descent. And why is this famous? Partly, it's because you can actually choose good geometry to accelerate the convergence of your optimization. For instance, okay, so if I choose phi to be actually the objective function, and I nominally choose my h to be 1, then you actually recover Newton's method. Okay, and of course, everyone knows that Newton's method converges fast. That Newton's method converges fast under reasonable conditions. But that's not the usage that I'll be following. So there's another usage. You can actually design this phi to avoid going out of the constraint set. In fact, so this is a natural gradient flow. And very roughly speaking, you can choose phi such that, okay, so as you approach the boundary of your constraint set, this whole vector field becomes. This whole vector field becomes zero. And then you can never go across the boundary. So if you start inside the constraint set, you will stay there. Yes? I don't have a very explicit characterization of the key of the constraint set. Let's say, you know, I know that some function of f is positive. Can you easily see that? I cannot. Yeah, I agree. Yeah, I agree. I guess this is required D to be convex, right? Yeah, this requires D to be convex, but for his situation, so you can still get a convex set if you have some nonlinear constraint that is unilateral. It really depends on the function, but yeah, so yes, so for convex functions, theoretically speaking, you can do this. In practice, of course, it depends on how explicit your constraint set is. So, yeah, sure. Thank you. Yeah, sure. Thank you. But you know, the universal idea still applies. So you want to choose phi such that you don't really cross a boundary in the continuous dynamics. Of course, that's not enough, but you can actually make everything precise. You can choose phi to be something called a mirror map. It's an important notion from convex optimization. And let me give you one example. So if you have a positivity constraint, Have positivity constraint, you can choose the mirror map to be something like this. And then you can actually prove that the continuous dynamics really does the constraint optimization. But this doesn't mean much because you have to discretize everything in order to get an algorithm. And if you discretize, for instance, by forward Euler, then you get something like this. And the discretization can still break the constraint. For example, so if you Okay, for example, so if you are very close, if you are very close to the boundary but not there yet, this thing might be small, but it's not zero. And because your time step is not infinitesimal, it could be large and then you can actually make a large jump and really go across the boundary. And then you break the constraint. So that's a problem. But there is a fix. There's a beautiful fix. The fix is to say, okay, so you shouldn't do this. Say, okay, so you shouldn't do this discretization. You have to discretize in something called the mirror space. Okay, so what is mirror space? So, one cool fact about mirror map is that the gradient of phi is actually a bijection from the constraint set to Rd. And also, the inverse is easy to obtain. So, the inverse is the same as the gradient of another function, phi star, and this phi star is really. Phi star, and this phi star is really the dual of phi defined by the Lajrona transform, if you prefer that terminology. And then you have a nice duality between primal space, which is a constraint space, and the dual space, which is sometimes called the mirror space. How does this work? So you give me anything in the constraint space, let's call it x, it's constrained. I can actually get a y by doing this grad phi apply to x. This y will be unconstrained. Be unconstrained. And then, you know, you can take the natural gradient flow dynamics and rewrite it equivalently in mirror space. You will get something like this. And then you discretize. You can just do forward Euler. So you discretize this and then you get an algorithm. And you discretize the Y dynamics and you get an algorithm. Starting at this point, you'd no longer have equivalence. Okay, so the Y iteries can't actually start, well, most likely it'll be. Start well, most likely it'll be different from the X iterates. And in fact, X, like I said, can go out of the constraint set. Y can never do so because Y is in unconstrained R D, so Y can do anything. But then if you actually pull everything back to the primal space by using this inverse of the grad phi, you of course get something that is inside the constraint space. So this is actually how you can do constraint optimization. You can do constraint optimization. So, the method has a, it's very famous, it's called mirror descent. So, you can cost everything purely in mirror space, and then this is your dynamics, this is your algorithm, which is a discretization. If you don't like that, you can actually write everything, you mix the variable. The continuous dynamics looks like this, much more intuitive. So, it's like gradient descent, except you have to really pay close attention. So, here you have y dot. It's given by a gradient that is based on x. Given by a gradient when it's based on x. Okay? And then the discretization, you can't really directly discretize this, you have to restore it to the discretization above. But the idea is also very clean. So you want to move in the primal space constraint. Well, you first go to the mirror space, you do one gradient step, and then you go back to the primal space. Okay? So if you want, you can also rewrite everything in a primal space. It'll look like this. Look like this. And okay, finally, we can start talking about constraint sampling. Okay, so this is our target density again, you know, supported only on the constraint set. And then, of course, the very first step, it's very tempting to say, okay, I take, I find my gradient and I add Gaussian noise like before to it, and then it won't work. So I really had to resort to continuous dynamics for help. So, what is For help. So, what is continuous dynamics? Recall that mirror descent in primal space is really just natural gradient flow. And what I need is I need to add noise to this dynamics in order to turn this into a sampler in the sense that as time goes to the infinity, I don't converge to a minimizer of f inside the constraint set, but instead this constraint distribution. And it turns out that you cannot just add a noise. You have to add a noise as well. We have to add a noise as well as an additional drift term. So the noise here is actually a multi-pollutative noise. It's not even a Gaussian noise. So it's pretty non-trivial. And then this term is sort of a term that corrects for the fact that your metric tensor is not necessarily a constant. Okay, so you can easily prove that this dynamics has stationary distribution, that is, the target density. I mean, you just plug in the stationary distribution. Stationary distribution into the focal plug equation of this dynamics, and then you verify by algebra that you get stationarity. So that's easy. But what is not easy is how did you get this term and that term? So what is the power agent? I mean, of course, if you are really smart, you probably can just stare at the Fokker-Planck equation associated with this and then figure out what is missing and then keep on adding t terms to the Fokker-Planck equation. Terms to the Foucault-Planck equation. I don't like that because I'm not that smart, but there is actually something deeper. So you again can resort to the infinite-dimensional optimization picture. So you can sort of look for SDE whose density evolution is the gradient flow. Again, you're trying to minimize some distance between your current state and the target distribution. You can keep on using relative entropy, which is the same as KL divergence. Which is the same as KL divergence as the objective function, but you have to modify the metric from which you do the gradient. So if you modify the Wascherstad metric to something that is a Riemannian generalization and do the gradient flow, you can get Foucault-Planck equation, which yields exactly this SDE. Okay, a lot of details hidden in the rub. This is really beautiful and not completely shallow. There's another thing. There's another thing. So, if you're a computational person and you look at this, okay, so g is a second-order derivative, and then it seems you need third-order derivative. That sounds scary. But in fact, this is another magic that mirror variable can do. So, in fact, if you go to the mirror variable, you can rewrite the dynamics in mirror variable, and you get this. So, you see that the noise that you add is really strange. It's somehow accounting for the curve. somehow accounting for the curved geometry. And then if you discretize the time of y dynamics, then you get mirror-launched one, the ugly expression that I showed you earlier. So you have to add a very specific type of non-Gaussian noise. Okay, so that's basically the construction of Mira-Lange 1. And finally, we want to know how well it performs, how fast it converges, et cetera. So, because I work on machine learning, I learned one thing through sort of hard lessons. So, every paper in machine learning has a title that says, X is all you need. And if you don't do that, they don't read your paper. And now I mean the dilemma that mathematicians don't read my paper, and machine learning people don't read my paper either. So there we go. So now asymptotic error is all you need. So we want to analyze the So, we want to analyze the behavior of mirror-language one algorithm. We want to quantify the convergence. So, this actually has been looked at. So, the previous state of the art result is an asymptotic result, which says, okay, so if j goes to infinity, and after this thing converges, you can quantify the something error. So, you can look at basically like the distribution of x infinity and also the target distribution. Also, the target distribution. And then you can look at the distance. And then you see, they show that the sampling error is square root of d times h plus square root of d times alpha. D is the dimension of the problem. I don't have enough time to talk about this, but of course a lot of you already said that oftentimes you have high dimensional problems. So the dimension dependence is pretty important. What I do want to emphasize is, okay, so, by the way, H is the step size. The way h is the step size. What is alpha? Alpha is actually some third-order regularity coefficient of phi. So if you have unconstrained problem, for example, then you can pick phi to be just the quadratic function and then alpha is going to be zero. But if you have constraint problems, you know, this term is non-zero. And this is null-managing, meaning that even if you let h go to zero, you still have some error. And this is a statistical bias, which is not very favorable. Bias, which is not very favorable, but they actually conjectured that this vanishing, non-vanishing bias is unfortunately unavoidable. But it is. So we actually managed to show that this is actually, the error is actually vanishing. And we actually get a stronger result, which is a non-asyntonic sampling error bound, which says, okay, so if you look at the distribution of my iterate at the J-C iteration, and I compare it to the target density, then It to the target density, then under Wash-Stend distance, you have an upper bound. So the first thing, the first term is basically saying that the initial discrepancy is exponentially decaying. And the second term is saying, okay, so you have some constant error. So of course you can recover asymptotic error. Okay, as j goes to infinity, this thing goes to zero, and then you get the order square root of d times dh. And this is vanished. You don't have alpha anymore. But the non-isyntartic error bound is nice in many ways. For example, you can say other things. For example, if you want to reach order epsilon error, how many steps do you need? You can show by using this that you need at most d over epsilon square steps. Okay, so there are many cool things that you can do with the synthetic algorithm. How did I prove this? So this is, I'm gonna be down very soon. So, this is, I'm going to be done very soon. So, step one, you have to cast everything in a mirror space, in a dual variable. Because in the dual variable, the algorithm actually discretizes the continuous dynamics, discretizes some continuous dynamics. And step one, next step, you can actually quantify the discretization error, so sort of like local truncation error of this discretization, but because we are in an SDE setup, it's a little more complicated. It's a little more complicated. You have to quantify something called local strong order, strong error, and also local weak error. But that's usually easily doable. And finally, we have a framework called mean square analysis for sampling, which is basically a framework that allows you to transfer local integration error, which is easy to obtain, to global sampling error, which is usually hard to obtain. Okay, so and then that's basically it. Then you get everything. Basically, it then get everything. Let me skip mean square error analysis for something and just conclude. So basically, everything is described in this paper. Mirror-Andrewan algorithm converges with vanishing bias. You can scan the QR code to get the paper. Andri Vibisono was actually the lead author of this paper, so a lot of credits should really go to him. The proof is really based on the The proof is really based on the technique, a pretty general framework of analysis in a different paper. Those are machine learning papers, so they are short, so please take a look. But that's it. Thank you very much for attention. Questions? Okay, I wrote this one down because I knew I was going to forget. So, with like mirror descent, is it like Like mirror descent, is it like taking gradient steps via dual space? Because I know that for like gradient descent, normally, or extending cleaning space, it kind of doesn't differentiate between like based on primal space and dual space. And so it's mirror, so like basically I'm actually asking about phi, is it representing like sort of bijection between the primal and dual spaces? That's right. Yeah, yeah. So technically speaking, the gradient is still the gradient. Is still the gradient is still in the primal space. But the gradient actually acts on something that is, oh, where is it? It acts on something that is in the mirror space. Somehow I lost it. Yeah, so sorry, so, but you see, so the gradient is still actually. Ingredient is still actually a function of x, which is in the primal space, but it actually changes y, which is in the mirror space. I was just wondering what the volume generalization of the cost is time. Oh, it's easy. So you just change the cost function to be something that is a geodesic squared. Geodesic squared. Yeah, it's very easy. The question is, what if the constraint is like a non-linear equality constraint? Oh, then that's completely different. So things from convex optimization doesn't work anymore. Something from a constraint is not manifold, but by equality constraint is actually well studied as well. You just have to use a completely different set of techniques. I mean, okay, not really, but the Okay, not really, but there are Riemannian ways to do it as well, and there are non-Riemannian ways to do it as well. I think it has the same problem: that continuous voicing can always stay on the manifold. Yeah, sure. But desecratization is another thing. Yeah, it's always the case that description can break a lot of the structures. And for people like us, we want to preserve as many structures as possible. And that's why you really have to use app. You cannot really just use a regularizer to regularize everything. Use a regularizer to regularize everything. We are doing this through a non-parametric way. We will just update and rather than the state. We will just update parameter because then the linear constraint usually is the parameter and the state equality. That's usually how you do it. Like you update the parameter, then the parameter gives you the state. So you know the relationship is always under mind. Yeah, yeah, yeah, yeah, yeah. Happy to learn more about them. I'm sure there are a lot of smart ways to stay on the constraint manifold if you have an equality situation. Thank you. Thanks, Monaje. Our next speaker is Aisa Paris. Can you follow up with? And remember that after this point we have a break and keep asking questions.