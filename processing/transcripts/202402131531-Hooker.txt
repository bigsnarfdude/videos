This goes all the way to me complaining that I started doing interpretable machine learning in the early two thousands when statistics didn't bottom up when computer science really didn't want to know to and everybody's doing this badly. But sometimes you do see things where people are getting results that I think are potentially misleading to them, and this is sort of And this is sort of one example of that. This work owes a lot to various grad students whose last name is Joe. Jungje buys the districts of Berkeley. He's still working with me. Ichinung Jezu both graduated from Cornell a few years ago. As well as Pei Ruxu and another Berkeley student, Jeremy Daldwasser. Let me see if. Let's see if I can get this to advance. There we go. Okay, so in general, I think that the world of interpretable machine learning is a case of buyer beware. And I have talks where I complain about various aspects of that, but in this particular case, here's one particular form, and I can show you that this is. And I can show you that this covers a fair number of cases of what people tend to do. So, machine learning, classic introduction to interpretable machine learning talk. We have high performance at the cost of intelligibility. So, your random forest, your deep learner, your support vector machine, if you're as old as I am, are going to nice predictive performance. Nice predictor's performance, but you find it very hard to tell people here, just decide what they're doing. Now, there are multiple approaches to trying to then take that and give you some sense about what's going on inside your black box or some representation of how your black box is functioning. A lot of them can be represented, you know, involve a representation in terms of a Involve a representation in terms of a simple model, either globally or locally, but I'm going to provide you with some indication about the way this model works via a surrogate that you can interpret. Now, I know Cynthia Rudin will have told you, don't do this yesterday, and she has good reasons to think that, and might take some of my talk as yet further reasons to think that. There are some other There are some other interesting results recently about the ways in which this could miss stuff. But I've also increasingly seen many people try to do this sort of thing. So this is often obtained by what you would call, what I'm going to call a distillation framework. And the terminology is to say you've trained a black box which you're going to call a teacher, and then you have a student model, which is going to be Going to be is going to be from some intelligible model class. And what you're going to do is you're going to generate a large corpus of example data. And because you've got a teacher, you can get a prediction for each one of these sets of covariant examples. So I can get as many, as large a data set as I like, and then I'm going to build this intelligible student model on this. Intelligible student model on this pseudocorpus that's being used. Now, why do I want to go through this instead of just building an intelligible student model yourself? Well, because I often, again, Cynthia might would argue with me about this, but often you cannot train a student model to perform as well because you're doing things like selecting structure. Structure. And that adds some instability. And having lots more data means that you can, and in fact, I can now say, look, I can mimic the behavior of my student to any arbitrary precision. Sorry, mimic the behavior of my teacher to any arbitrary precision I like. You can also localize or examine what happens under distribution shift, and so I can represent my data set, or I can use a student to represent. Or I can use a student to represent what my teacher does on something that's not actually my training day. Now, you can use this for a few things. If you're in computer science, you may just want to do this for model compression. So some dice paper from 2015, although the idea goes back to 95, of saying, well, here's a great, big, large neural network, and in fact, I can probably compress this into a smaller neural network. Into a smaller neural network, if I know where I need to go and I know how I want to represent it. I'm not going to worry about distillation in this form, but I am interested in interpretability. And so here's an actual case. This is sort of something that came out of Junja's most recent internship at Uber. Uber's one of Uber's One of Uber's platforms is a safety platform that says, you know, whoever has dangerous behavior, we are going to deactivate them. Deactivate them. Sorry, you are not safe to drive for us. You're too high risk. You can no longer be part of our system. Those are based on a machine learning model. They're pagey about this, or at least Wunju was not prepared to tell me more than that about this. Me more than that about this. But they do need to create an explanation to say, here, this is why we are kicking you off our platform. And, you know, the classic, hey, because our AI said so is not sufficient remains. Here's an example from my own work running quite a while ago. This is work with Robert Gibbons, who's at the University of Chicago, and Matt Finkelman. Chicago and Matt Finkelman, where we had a problem of trying to shorten medical questionnaires. In particular, the questionnaire, the main questionnaire used, or the main instrument used to diagnose severe depression is a questionnaire that runs to 88 questions. First indication you might be depressed is you couldn't be bothered finishing up. 88 is actually a lot of questions to go through. And a lot of them are redundant because that's the way psych questionnaires tend to be. Way psych questionnaires tend to be generated. And so the question was: could we reduce this for streaming purposes? And the idea was we'd like to use a decision tree because it gives us a nice sequence of questions. It allows us to be a little bit adaptive. So hopefully this would be better than just having, you know, we could have a shorter sequence by using a decision tree. But trees are actually fairly bad predictors, and we didn't. Bad predictors, and we didn't do super well if you just train a single tree. What we did was, I built around the forest to predict depression based on 800 observations. We had about 800 observations in the data set. I then generated 12,000 new data points and built a tree to predict that random forest. We went out to about depth five. That gave us, you know, accuracy about effectively equivalent to random forest. About effectively equivalent to random forest sensitivity and specificity were both above 0.8. Here we go. This was the tree we got. I do not remember what question 52 was, but it was something like, you know, have you felt like you've had energy today? And, you know, here's our ROC curves for both of these. So we're doing pretty close to equivalently at mimicking the random harvest. I didn't put up the ROC curve for a tree just built on the original data. Retrieve just built on the original data. It's, I mean, not hugely dramatically, but noticeably lower than what we can do on test data with a random forest. Okay, and so we published this. And then I went and talked to I Chen. Joe squared is because that's both Ichen and Jungzo who ended up working on this. And said, well, I chose 12,000 just out of 12,000 just out of thin air. There was actually another set of couple of papers by a guy called Johansson, who was also interestingly using 12,000 to do a similar thing on a different task. Was that enough? Well, let's do this twice. This is the same random forest, different set of 12,000 Shiodo examples. These are basically created by perturbing examples in my data set. I don't know if you can. I don't know if you can see this. It's the same top node. Bottom node is different here. Third layer, everything is different. None of what I'm getting out of these three is the same. Oh. Okay. Well, now we'll talk about whether or not that really matters. About whether or not that really matters in this particular case. But example, I got this idea originally because I had heard her talk about Fair Isaac way back in the early 2000s where they were doing this when denying somebody a loan. And why do you not get a loan? Well, it's the bottom note on this decision tree. Yeah. Now what about even by double the size? Now I've got 24,000. Okay, even more. Well, I know that that's enough. Well, here. I asked each of that. I asked each of that. And this is what we did. We said, well, okay, look, what I can do is these trees are built greedily. They're built split by split. So I can ask, would this still be the split, the best split with a different data set? So I'm at the top node. I've selected question 52. And I've said, okay, if I imagine regenerating the data, would I still select this place? Would I still select this split? And what I'm doing is I'm really making a comparison between split one and the next best split, split two. And I'm asking, you know, is that real? And then I'm going to ask, if I looked at this difference in the score for those two splits and realized this on a new corpus, now these scores are just averages, I can basically sort of say, well, how different would those be? How different would those be? Well, I can measure effectively their averages. So I've got a central, approximately a central limit theorem, and I've got a variance. I can measure the differences between these guys. And that gives me a probability that a replicate pseudo-data set would give me a different sign of these two splits that I can measure here. If I increase that size, will that difference shrink to zero? True to zero? Yes, there is implicitly an n in my pseudo-sample in that sigma. Does it mean larger the better? Larger the better. Okay. Yeah, so this works the way you would expect it to. And in fact, what I can do here is I can do a little power calculation and say, well, I want the replication probability to be alpha. There's a slight little difference from a usual sort of standard confidence interval type of argument that I've got a 2 to here. Of argument that I've got a 2 here because this is what would happen if I repeated this, as opposed to where's the truth. So I could find a new sample size, and this is just a little power calculation to say, well, how large does this need to be based on my current estimate, my current estimate of the variance, to make this large enough? And in fact, you know, okay, cool. So that's a new sample size. Actually, I'm making comparison splits against all. You know, comparison splits against all the other features that I might make. So we'll add a sort of Benjamini-Hofberg type procedure to sum up the total error. And I'll just do a little search up until I get to a large enough sample size that I'm confident that we've controlled this non-replication probability at an acceptable level. Yeah. Does either one of these trees guarantee the same output? Guarantee the same outfit performance? No. I don't have a tree. I'm just asking, would I get the same tree? Would I choose the same thing? That does presumably mean that indirectly you would expect that Split 1 is in fact giving me a better representation of what's going on in my random forest and will translate through to a better ROC, but that's. But that's, I don't have that direct calculation. Okay, so if I do this, here's a little, I guess that's where I was at. Here's a little sort of demonstration, four classic data sets. This is my depression data set. Base here is, I've got base and approximation tree to bases. What happens if I use, I think, a base of 10,000 data points to do distillation? wants to do distillation. Approximation tree is what happens if I keep on adaptively adding data at each split. So I say, no, this split is now stable. Each solid bar here represents a single tree structure in 100 replications. So effectively, most of these base things are giving me individual, like unique structures for every replication. I'm largely at least reducing that diversity. At least reducing that diversity, we had to go up to have a maximum of, oh, well, we had a maximum of 50,000 data points generated per split. In order to achieve this, if I wanted to make this unique, I'd need to go up above a million. That's starting from 800 data points. And largely, this idea. And largely, the statement is: people don't do enough Monte Carlo. I mean, this is just a talk about Monte Carlo. I mean, I am all we're talking about reducing Monte Carlo variants. Standard practice does not do nearly enough, put nearly enough Monte Carlo work into making this happen. There's our revised tree. We also added some, because this was from a random forest, we used some results that we had developed for a central limit theorem for the predictions of random forests to test whether. Of Brandon Forest to test whether the actual teacher was constant relative to chance in the leaves, and by and large they still were. You don't need to exactly replicate this in order to achieve good classification performance, though. Okay, I want to step back for a minute and ask: so what? I mean, should I retract my 2013 paper? Now, Now, I am not inclined to in this case because I wasn't really trying to interpret what that tree was doing. I was just using it to shorten the questionnaire. And okay, there are multiple ways that I could get to that accuracy. It's fine. So, no, I don't need, and you know, you could say, does it matter? Well, no, not if I'm going to say my tree replaces my teacher when in use and will then remain fixed, right? And will then remain fixed, right? And the only thing I need to provide as a justification for my decision, and this is true of Cynthia Rudin's work as well, is here is our formula. This is how our decision process works. As long as that level of transparency is sufficient, then so what? Sure, you got this by random. Who cares? It performs well, there you go. But if my tree is a surrogate for the actual model, Actual model, if it's intended to provide some sort of causally based reason for a decision. And if you look at European litigation under GDPR, what an explanation means is getting towards saying there has to be some sort of causal justification. Or if my model might be re-distilled for a new prediction, then I could be in trouble. Then this starts feeling pretty uncomfortable. The obvious question: why not just use the training data? Always question: Why not just use the training data? Well, in fact, it didn't work as well for our trees. I might not have it. Training data, test data, those usual algorithmic things as well. Let me give you an example of distillation where this really is necessarily the case. Lime. What does Lime do? So, locally, interpretable model explanations. We already had a quick introduction to this. Introduction to this this morning. What does LIME do? Well, basically, it fits a sparse local model. It generates data around, you're interested in explaining what's important for a particular prediction. You generate data around that prediction. You fit a linear regression with a lasso penalty, and you tune your lasso so that you get eight features or five features, depends, but you know, a fixed number of features. Here is an Here is an example of this. There's the usual sorts of things you might do. Here's a data point. You get a prediction, and it will tell you these are the features that I thought mattered. And then this is sort of the linear regression coefficient on those. This is how much they change. How much they change it. I did that again under the defaults. And you know, I get some of the same things here, but not all the same features get predicted. And if I was to do this and sort of tell you about why you're not getting alone, and you come back tomorrow, and I give you a different explanation, you might not be super happy. So here we go. Here's exactly that data point. Here's exactly that data point. I repeated Lime 100 times. This is the frequency with which each of these features appeared in the five features that we set. So these guys, yeah, they're really strong, but it starts getting a little bit hairy afterwards. We can stabilize this a very similar way. We ran the Lars algorithm, which does another sort of competition between features when features enter the model. Enter the model. And we just asked, sure, that competition is measured by a covariance, have a nice central limit theorem as well. So is the covariance between residuals and this feature larger than the covariance residuals between the covariance residuals and that feature? That's another test that we can do. We can do a power calculation, workout if we've got enough data. So, you know, if I look at, this is the Jacquard index, so for each level I'm looking at, what's the stability? Looking at what's the stability over 20 replications of the set that comes down to the top three features, the top four features, the top five features. Standard versions of Lime, but with defaults, not great. We're getting most of those. Again, usually a much larger corpus than you expect needed. We were up to, we maxed out at 10,000 and in fact we're Out at 10,000, and in fact, we're told we needed more here. That's for an individual prediction. So here's a couple of test points that we're showing you the same thing. This is to predict, this is using the EPIC data, so to predict sepsis from the past 24 hours of that should be vital signs using a recurrent neural network. We've actually got really poor replicability in what lines. Or replicability in what line is giving, say, a doctor, unless we stabilize it again with much, much more work. I'm going to speed through this a little bit. And here you can see I sped through creating this because this was copied from the algorithm in the paper. But we asked about: can I do something more generic? And so the way we decided to do this is we said, well, let's think about having equivalence classes of model structures in our student models, generating a bunch of initial student models and asking which, can we work out that what appears to be the best equivalence class really is? And do I have enough data to say that? And if so, we'll work with. So, we'll work with that, and if not, we'll generate more and retrain models in these equivalence classes. A few examples. Here is Cynthia's following rule lists. I hope she talked to you about them. They're a very cool idea. But effectively, it's a sequence of tests where, and she would train this with a Bayesian. We used a Bayesian process. We used a Bayesian posterior. Actually, we ran it several times because it doesn't actually mix very well. So we re-ran a folding rule list several times to get a whole collection of initial models. Symbolic regression gets used a lot these days for people doing physics, especially if you want to say, hey, I'm trying to fit an ODE, I want to try and do something non-parametric. Now, I think that's highly problematic in terms of trying to create non-linear differential equations, but that's. Trying to create non-unit differential equations, but that's a talk for a different time. And now I wanted to create something understandable out of that. So you create a model in which you express an algebraic formula in terms of a tree. And you can then do some, effectively, run a genetic algorithm to try and search for these formulae. And we again run through a bunch of these guys and ask, do I have enough data to be able to tell you? Have enough data to be able to tell you that the thing that looks most, the equivalence class that looks like the best representation of these. So once again, we can do that. I was actually a little bit surprised that this was not harder than it was. Here's a sort of looking at the complexity of the model classes that we were looking at in terms of depths of tree, et cetera, and how much we were stabilizing. And how much we were stabilizing things. And as we get more complex model classes, it gets worse to work with, unsurprisingly. But we could make these sorts of things work. Here we go. This is depth of model class, and then the maximum sample size we were prepared to go up to, and how stable we were able to make these things. As is entropy, so larger is worse, and over here. Worse, and over here, you know, we're sort of a bit beyond our computational capability. That's not for super large students either. These are still fairly small models, and they are really pushing what we can do without very large computational resources. A little bit of sort of theory about what matters here. Well, you know, how many student models do you start with, the complexity of the model, so effectively, how Of the model, so effectively, how many potential equivalence classes are there, sample size that you might go up to, and we can give you sort of some bounds based on the complexity and how many models you start with on how far you all have to go and how far you might exceed sort of what you really need to have to do. What you really need to have to distinguish those. But I want to keep us to time. So, some thoughts about this. First of all, if you want to control the stability of interpretation, if your interpretation is kind of random, I think it's not clear to me what you're learning. You can certainly argue about whether I want the optimal decision tree or something that's optimal at every level and these sorts of things. But if you want to control the stability of the interpretation, The stability of interpretation, you really need very large sample sizes. And large means at least an order of magnitude larger than current practice. Reproducibility here actually is kind of interesting. It's a different standard than inference. And it's one that I've been interested in exploring in the context of machine learning in a number of other contexts in terms of saying, if I got a new data set, how different would my answer be? How different would my answer be? And that takes away the question of what's the truth I'm trying to cover. It just gives you the notion of stability. There's a bunch of interesting questions about when does what uncertainty matter? You know, when we've talked about fairness, we've talked about the fairness of this algorithm, not the problem that the algorithm was trying to solve. And often that's right. It's not necessarily always. Necessarily always. And none of this has accounts for something for the uncertainty of the teacher, right? Given that my teacher is going to itself have variance that's associated with the data I've got, that might reduce how much I need to worry about student interpretation, but it gets really difficult to quantify outside bagging ensembles. And it makes it hard to really represent uncertainty in the students. And particularly, one of the things I want to know is all these. And particularly, one of the things I want to learn is all of these students learn structure. They learn hard decisions, hard threshold decisions, this or that. That really is what makes stabilization here challenging, but it makes stabilization important because it makes representing that uncertainty even harder. And if I'm going to start worrying about, well, how does uncertainty as a teacher feed through to the students, then I can't avoid this, but it is going to lead to a challenge. It is going to lead to a challenge. Okay, I'm going to stop there. Just one part. So, I don't have any quick questions for the so in learning theory, there's this notion of algorithmic stability. And there, nobody ever tries to get exactly the same model out. What they try to do there is get a model that performs similarly, or it performs similarly on like any possible model, that kind of thing. And I guess I'm not totally sold that it's. Sold that it's important to get literally the same tree every time. Even for interpretation, even for reproducibility, just set a random seed and then it'll get the same tree with the same inputs. And if I lose the random seed or move it, or like I... If you lose your data, everything changes too. Well, but that was the point. And so, yes, but that's fine for generating a tree for global input if I'm interested in something. If I'm interested in something like LINE, then I'm necessarily generating new data. Right, and I could say your random C12. And if you get the same input, you get the same explanation. And that's the only thing you're guaranteeing any. It is the only thing I'm guaranteeing, apart from saying some sort of fidelity. I think there's some utility in saying this is a random process. If I'm worried about the C, that's sure. That's sure. I can also say here this exact data set, as long as I still have hold of that. But if I change that, then I can't get it back. There's an issue of the... There are two issues. One is the performance of the different solutions. The other one is more challenging about what are the causal issues. Yes, there is also the, if I think this is telling me something about the underlying Telling me something about the underlying process in the model that is more challenging. Many, many years ago, I tried the tree algorithm along data. I could get 1,000 different trees or even every single tree is different. This example yesterday was 100 million trees, right? Yeah, I see one question up there. I see one question up there, but in fact. Yeah, so it's a follow-up actually. Do you have some thoughts on how this stability corresponds to the Russian set of models? Because my intuition would be that this sample size has a relation to epsilon that you use to kind of say that, okay, all the models are like zero point nine AUC minus epsilon. So if you increase the sample size, maybe the epsilon decreases, so then you actually converge to the same solution. So, then you actually converge to the same solution? Yes, I think there's certainly, if you think of a rational set of student models, then something like this, you know, something like that holds. And that's associated with saying I've got this kind of structure. That said, you know, I started looking at LIME going, oh, well, it's a linear regression with LS. So that's got, like, surely that's pretty easy, right? We don't have to do so much. And I was. And I was floored by how, you know, that these data, I mean, we know what happens if you have a nice orthogonal design. These data generated are effectively from, I mean, independent covariates because they're independently generated around the point of interest. And nonetheless, I can't keep that statement. So it's not quite the Rachemont set because some of that's about things like covariates that aren't. Things like covariates that aren't the particular covariates that you generate that aren't really encapsulated in terms of performance. Our next one is our next speaker.