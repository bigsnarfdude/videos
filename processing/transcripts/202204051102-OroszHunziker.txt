Say a little bit about what a vertex algebra is, and here's the plan for the talk. Oops, one second. Something is wrong with voice list. Give me one second. Sorry. I don't know why they won't go. Okay. There. Here's the plan for the talk. I'll try to give a little bit of motivation. Why should we study such complicated algebraic structures? Then we'll go into the definition, which is a little technical. And hopefully, we'll focus on some examples, three examples that we'll see today: Heisenberg, USC. That we'll see today: Heisenberg, Virasoro, and affine vertex algebras. And in the end, I'll give you some very biased selection of results that I think are interesting that relate vertex algebras to other fields. So let's start with some motivation. Why should we care about vertex algebras? Well, one reason is some interesting phenomena in string theory is described by vertex operators. If you think of conformal field theory and you look at its symmetries in general, You look at its symmetries in general, that has some nice algebraic structure. It actually is a vertex algebra, and the most common case is that you'll get the Virasor vertex algebra, which is one of the vertex algebra we'll talk about today. And if you're not so interested in physics, but you like finite groups, well, if you think of the monster, the largest finite simple group, it is the automorphism group of a module called the Munchen module. And I think the next talk actually talks about the Munchen in a more general setting. In a more general setting, so these are three humble reasons why we could study vertex algebras, and now we'll move on to the definition. So, um, maybe I should say that I can't see everyone's faces while I'm presenting this, unfortunately. I'll try. Let's see if this still works. But feel free to interrupt and ask questions at any time. If something's not clear, if my accent is funny and you would like me to repeat anything, please feel free to unmute yourself and ask questions. Free to unmute yourself and ask questions. Okay, so before we move on to the definition, we'll do a little bit of formal calculus. So we're going to grab delta, it's a power series, the sum of all the powers of z, where n is moving over all integers. And if we grab another power series that we're just going to describe formally as a sub k c to the k, where a is in some vector space b I claim that multiplying a in z by delta in z divided by w is the same as multiplying Is the same as multiplying A in W times delta C over W. So, in some sense, this delta is working as an evaluation distribution. And we'll do this algebraic nonsense together just to get used to what's happening with this power series. So, I just grab A of Z and multiply it by delta using this definition, and I end up with A sub k z to the n plus k. I just group together the powers of z and w to the minus n, just because I have one over. Because I have one over W to the M. Now I'm going to call M plus K something else, L, and I can change variables because I'm adding overall integers. I can do this and rewrite this in this way. And now I'm going to call L minus N something else, S. And I can rewrite this in this way. And now it's clear that I started with AZ delta in Z divided by W, but I ended up with A in W times delta of Z. W times delta of z divided by w. Okay, in particular, if I grab this A of Z to be Z minus W, I get that Z minus W delta in C divided by W is the same as W minus W times delta C over W, because I can evaluate this power series in W, and this gives me zero. And more generally, one can see that a high enough power of Z minus W times a high enough derivative of delta in Z divided by. Derivative of delta in C divided by W gives you zero. So one thing to notice is in the space of power series in positive and negative powers of Z, positive and negative powers of W, there are zero divisors. Well, we grabbed two non-zero things, and their product gave us zero without any of them being zero. Great. Great. Why do we care about the delta power series? Well, hopefully, this will explain a little bit of why we care about this. So, we're going to work with this modified version of this delta power series. As I said, we're just doing formal calculus and like complex analysis in some way, or just algebraically manipulating this power series. They will be useful because we need to formalize things that, in general, in physics, were being done without being too careful about convergence. We inverted. About convergence, we in Vertex Address, we actually care about carefully defining things so that convergence is guaranteed and we make sure that when we're doing these computations, things make sense. Great, so this is what W minus one delta C over W looks like. And I can divide it into the positive powers of Z divided by W, or non-negative powers, and the negative powers. Sorry, is there a question? Oh no. Sorry about that. So I can divide it in the non-negative powers of C divided by W and the strictly negative powers of C divided by W, which I wrote as positive powers of W over Z. Great. I claim that this chunk over here is the expansion of this rational function in this domain. And this chunk over here is the expansion of the same rational function, except the opposite with the opposite sign, in this other domain. And let me show you at least that this purple chunk is true. So let's grab this expression. It's the sum of w over z to the n for any n strictly greater than zero to infinity. And I can rewrite it in this way by just using that if I were summing from n equals zero to infinity, this would be like the usual complex analysis exercise in which. Complex analysis exercise in which we are writing this power series expansion for this function, of course, as long as w divided by z has absolute value less than one, right? But if that holds, which is exactly this domain, this is equal to this. And now I can take common denominator z here, and I get as z minus w. I can write this one as z minus w over z minus w, which is what I did over here. Simplify the numerator, and this is exactly what we get. So, hopefully, I've convinced you that this weird. Convinced you that this weird delta nonsense actually has some complex analysis meaning, and really, this modified delta function is the difference of two expansions of the same rational function in two domains. And this is all we'll do with deltas. This is the end of formal calculus, but I do want us to think about every time we see a delta, there's some weird complex analysis on the background. We are taking differences between expansions of the same. Differences between expansions of the same meromorphic function in different domains. And this will be happening on the background in vertex algebra theory all the time. Great, so now we're in good shape to go on to our definition. So what is a vertex algebra? A vertex algebra is a vector space, V, a distinguished vector that we call the vacuum vector, and a linear map, Y, that eats someone in V and speeds out a power series that has coefficients in endomorphisms of V. So it eats in A. So it eats an A, and then YAZ is the sum of A sub n Z to the minus n minus 1, where A sub n is an endomorphism of B. And you may be curious about why we have this weird expansion. Instead of putting a sub n times z to the n, we have this weird shift minus n minus 1. I'll say a little bit about this in a second. And this power series has coefficients and endomorphisms, and it's very infinite. It's infinite, you know, there are infinitely many negative powers of z. There are infinitely many negative powers of z, infinitely many positive powers of z, but we will want some sort of finite net condition. So after you evaluate this endomorphisms on a b in the vector space, you will only allow this power series to have finitely many negative powers of c. This is what this condition says here. A sub n b is zero for n sufficiently large from the way we expanded this. That means there's only finitely many negative powers of c once you evaluate these endomorphisms on a specific. Evaluate these endomorphisms on a specific vector in V. Okay, this weird expansion is done so that when you, if you want to fish a sub n, you can do it by taking residue of z to the n, y a z, or contour integral around a closed path integral, if you're thinking of complex analysis. And about this condition, an acceptable y AZB would be a sub 100 b z. sub 100 b z to the minus 101 plus and this is the way coefficients are paired a sub 1 b 0 to the minus 2 a sub 0 b z to the minus 1 a sub minus 1 b and you can have infinitely many positive powers of c here but after you evaluate in a vector b this should be truncated below and of course there are certain axioms vector space vacuum which is our distinguished vector and a linear map y and we won't focus too much on the axiom Y. And we won't focus too much on the axioms. We have some vacuum axioms, so axioms that tell us something about this distinguished vector that we call the vacuum. It says y has to send the vacuum to just the identity in the morphism. So as a power series, it has almost every coefficient is zero, except the coefficient that goes right next to z to the zero, which is the identity in V. And if you send any element A with the map Y to its power series, once you evaluate in the vacuum, you end up with no negative power series. Vacuum, you end up with no negative powers of z, and you can evaluate actually z equals zero and recover a. This is known as the state-filled correspondence, and it's telling us in particular that y is an injective map. And then we have Jacobi identity. So very similar, if you have worked with Lie algebras, it's very similar to the Lie algebra Jacobi identity, in which we have the commutator of two such maps in terms of an iterated product here, except we're going to correct this Jacobi identity with some delta functions. So again, And delta functions. So, again, this was the purpose of the whole discussion of formal calculus. You can think of this Jacobi identity as mixing up Lie algebra axioms and Cauchy's residue formula. We're kind of taking the difference of the same Meromorphic function in different expansions. Sorry, different expansions of the same Meromorphic function in different parts of the complex plane. And this is how really vertex algebras were born. Physicists were doing this. Born physicists were doing this, we're taking differences of things that were expanded in different regions of the plane because they corresponded to different states that were interacting. But to make it rigorous, we need this delta functions to make sure that we're keeping track of how things change once we move domains. Great, and equivalently, and we won't focus too much about this, there are many ways one can define a vertex algebra. One can instead of think of the Jacobitan tree, have a linear map T that goes from V to V that is compatible with the vacuum. That is compatible with the vacuum. It sends the vacuum to zero. It's compatible with y as taking derivatives. And instead of the Jacobian entity, we ask that y Az and Y B W, well, they don't commute, but they commute after you take, you multiply them by a large enough power of Z minus W. And these two purple things here are equivalent in the definition. So, for the purpose of today to study examples, we will use the second version, even though Leipovsky, Merman, and Frenkel introduced. Lepovsky, Merman, and Frenkel introduced vertex algebra using the Jacobian view in a parallel way to with Lie algebras. Great, so what is a VOA or vertex operator algebras? Well, it's a vertex algebra that has a nice grading. We won't take too much time on this. It's Z graded, and each grading component is finite-dimensional. There's some lower boundedness condition. There's another distinguished vector that we call the conformal vector that gives you our representation of the Virus-Or algebra. I'll explain more what this is. Algebra. I'll explain more what this is. And once you decompose your conformal vector with a map y in this way, L0 is the thing whose eigenvalues give you this z grading. And L minus one is this map T that we had in the definition. So what is a VOA? And we can kind of ignore this slide because there's like probably too much information in there. It's a vertex algebra with a nice C grading and another element, another special element in degree two that we call the conformal vector. That we call the conformal vector. And we want this because, in general, the Viras-Sorr algebra will be the algebra of symmetries of conformal field theory. So, you want to have an actual Virus-Rohr algebra in your vertex algebra. Great. Here we go to some examples. So we'll focus on this first example quite a bit, the Heisenberg VOA or the Fock representation, and then we'll talk a little bit about the Virasora VOAs and the affine VOAs or VOAs associated to infinite dimensional V-algebras. I hope to explain one carefully and kind of mention things about two and three because things work quite analogously in that side. So let's go with the Fock representation. So what we're going to do, we're going to graph the Heisenberg Lie algebra. So a nice object, not a vertex algebra, a nice Lie reasonable Lie algebra. It has infinitely many generators Xn and a central element Z. So Z commutes with everyone. And Xn and Xn almost always commute, except Almost always commute, except when m is the opposite of n, and in that case, you get nz. Great. And now we're going to grab some sort of highest weight representation, induced representation. So I'm going to grab a one-dimensional vector space that I'm calling C times one. So one is just the vector that spans a one-dimensional vector space C1. And I'm going to make it an H plus module. So first I'm going to graph a positive part of H, just the positive X of N's or the span of the positive. The positive x of n's or the span of the positive x or non-negative x of the n's and the central element. And I'm going to make this one-dimensional vector space an h plus module by declaring that xn kills this vector for every non-negative n and that c acts by one. And now I induce. So I'm just going to, in some sense, let all the x sub minus n act freely, compatible with this action. And then all the x sub n with n positive kill one, and z just acts by one. And z just adds by one. So linearly, pi is polynomials in the negative x of i's. And I claim that I've written here down, well, the beginning of a basis for the space. So we have this vector at degree zero. At degree one, we have x minus one one. x one one died because x one one is zero once we're in this representation. At degree two, we have x minus one squared, x minus two. At degree three, we have x minus one cubed. And you may be wondering, and this And you may be wondering, and this is a Poincar√© Brick of B type of argument, but you may be wondering where are the elements of this form? x1, x minus 2, 1. Well, x1 and x minus 2 from this rule here commute. And now that I have x1 and x21, I get 0. So in some sense, the x i's with i positive are annihilating operators. They kill things. If I have x2, x minus 2, well, x2 and x minus 2 don't commute, but they almost do. They actually give you. They actually give you x minus 2x21, which dies, plus 2z1. So this just gives you 2, 1. And things like this can be reordered because they commute. So it is true that this is a basis. And moreover, one can see from just doing a couple coming. Does this make sense? Yes. Yes. Great. And let me know if I'm going too fast or too slow, please. But from just like trying some commentators and seeing how. Commentators and seeing how things work once we're in this space, in this representation, one can see that the x minus i's are creation operators. They create things from this vector. And in some sense, like they act freely, one could say, to create things, while the x i's kill things and actually behave like taking the ribbon's with respect to the x minus i's. I mean, one can see it right here. One can see it right here. Great, so we're going to call those annihilator or annihilation operators. Great, and then just to kind of insist with this isomorphism, a typical element in pi is really a polynomial in the negative x sub i s. Okay. Okay, what's the VOA structure in pi? We have the vacuum. It's just this vector that we grabbed. T is defined as some sort of derivation. T is defined as some sort of derivation. The conformal vector has to be someone at degree two, and we choose this guy. We could choose another one, but for now, that will do. And then we have to define y of the vacuum is, we know the identity, so we don't need to worry about that because it comes with one of the axioms. And then we need to define y of this element. And what we do is we just jam all the generators of the Heisenberg algebra in a power series and call it x of z. And that will be a generating field for a BOA. Now we need to. Now we need to define y of this element. And wouldn't it be nice to just take the product of xz and xc? That would be the tree. That would be the reasonable choice if you want y to be compatible with this structure over here. And this is a good idea, except it presents some problems. There will be some divergence. So here's this map. Here's this idea of defining y of this as the product of two x's. Of two x's and the coefficient of z to the minus 12 has no problems since it looks like this, and then this is never the opposite of the other one, so I can commute them and reorder it. And things are fine. Once I use that xl with L positive is annihilating operator, I can see that when I evaluate this thing in something for large enough L, this will die. Hopefully, you agree with that, right? We have polynomials on the negative axis, and then this is derivatives on the negative axis, so this will be truncated. So, this will be truncated and it will make sense. However, if I look at the coefficient of z to the minus two, then things are opposite to each other, and now they almost always will commute, except there will be some problems here. And if I look at this, this is the sum that I get. I just separated into strictly negative, strictly positive. The second chunk here, I claim, has a problem. So if I grab this, this I claim will bring problems, will be divergent. Will bring problems will be divergent. So I have this element. I, for instance, apply it to one. I could apply it to anyone here, right? Or any sum of this, but I apply it to one. And I get, well, I'm taking derivatives. That's cool, but I'm also multiplying by the variable. So I'm going to end up summing infinitely many copies of the vacuum. And that's divergent. That is the problem in vertex algebra. This is what mathematicians fix to. Fix to make sure that computations, well, physicists as well, fixed, to make sure that computations make sense. So, wouldn't it be nice when you have a situation like this in which the derivative is here to try to just force it to go to the right? And that's exactly what normal ordering will do. So we fixed this problem by defining the ordered product of xk xl to be xlxk if l happened to be the opposite of k, and just xk xl otherwise. And now we are all set. We can actually define the product. We can actually define the product of x with xc, except we do it the normal order product of xz with xz, as defined in this way. And now we have our VOA structure. So this is what y looks like, and we can define y more generally. Okay, so Chelsea, what's how much time do I have left? Because I actually didn't check when I started. We only have a couple minutes. Okay, gotcha. Okay, cool, cool. So let me just show you important ones. Show you two. Can you say it again? Yeah, no, you should finish this once. Good. Okay, so the Virso algebra similarly is an infinite dimensional algebra. It has a central element, and we can do the exact same thing. We grab an induced representation. So I'm going to go a little fast because I want to tell you a little bit about the results at the end. But we do the exact same thing. There's a one-dimensional vector space, an induced representation. An induced representation, and we have a similar structure. So we have a vacuum. There we go. We have a vacuum, we have t's a derivation, we have a conformal vector, and we define y in the same way. And we're always using this idea of taking normal ordering products so that annihilating operators go to the right and creation operators go to the left so that we don't get divergent things. And the last VOA that I want to show you is an affine VOA. So if you have a tax-moody algebra, which is an affinization of a Which is an affinization of a simple Lie algebra in this case. It has a Lie bracket that's well defined, and you do the same thing. You grab an induced representation. For the purposes of this talk, we'll talk about SL2. It has a basis EHF, and then its loop algebra has a basis ENHNFN, where you're tensoring with polynomials. And again, we have a vertex algebra structure that's pretty standard here. So the vacuum is just that vector. T can be defined like so. The conformal vector is defined in the same. The conformal vector is defined in this way, and you can see that we need some restrictions on the level k here so that things make sense. And then the map is defined in the same way. We have three generating fields, and we're going to take order product. Okay, cool. So here comes the end. For sub-center charges, the Vira-Sero-vertex algebra is not simple, which is good news because once you go to the quotient, infinitely many modules will die and you will end up with a modular tensor category. So let me maybe just say that again. So let me maybe just say that again because it may be interesting for people in the audience. This vertex algebra is dependent on a parameter. When the parameter is of a special form, the vertex algebra is not simple, which is good news because you can take the quotient vertex algebra and end up with finally many modules and a finite category that actually is an example of a modular extensive category. And something similar happens with the SO2 story. When k is a positive integer, this is not simple, but its quotient has finally. But its quotient has finally many irreducibles and gives an example of a modular tensor category. Okay, thank you so much. I'm sorry that was rushed, but I'm happy to answer any questions or chat a little more if you're interested in the details. Thank you. Questions? Yeah, maybe one small one. Flora, when you say modular tensor category, it's simple for you. It's semi-simple for you? Yes.