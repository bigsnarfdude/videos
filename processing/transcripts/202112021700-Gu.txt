Screen, right? Can you see my screen? Yep, yeah, we can. Okay, sure. So I'm going to talk a little bit on the marginalization of latent variables. So here's the outline. So I will use a few slides just to review some progress in the Gaussian process for emulating slow computer model. Simulating slow computer model. And then I will talk about the representation of this certain type of Gaussian process by dynamic linear model through the direct connection by this stochastic differential equation. And I will highlight the marginalization there in common filter for this type of development. And now I will introduce a few recent developments. Few recent developments along with it called Journalized Principle Component Analysis and Gaussian orthogonal latent factor processes. And then, if we still have time, I will talk about some more exciting work about more general setting, more general design. Okay, so just briefly review what Gaussian process is. I think a previous talk by Catherine Heller. By Catherine Heather, give a pretty good review. So, suppose for a simple setting, let's suppose we want to model some real value function. For a given input, let's say this is a physical input, a p-dimensional input, we model this by a Gaussian process or a Gaussian process prior, meaning that each marginal distribution is a multivariate normal distribution with certain covariance, and this covariance is. And this covariance can be captured by a kernel function. And then, while you use kernel, I will talk about it later. And the mean function, let's say we have certain mean of it. So this type of setting is very widely used to emulating slow computer model, which are like a numerical solution or partial differential equation. And for some of the mechanical model or some of the molecular dynamic simulation. And then in this setting, this kernel was typically, well, for those mechanical model, when this theta is not a huge dimension, but it could be much higher than three or two. Widely used kernel is this product kernel. So this can be, so this kernel function, each time. Be so this kernel function, each dimension can be written as a product of this kernel for each coordinate of this input. And then each of this kernel can be written as like power exponential or maternal or Gaussian kernel. I mean, like this. So the advantage is that for each kernel, we have a different parameter. For example, we have a range parameter. So in spatial setting, In spatial setting, for example, theta is a spatial input. This can be usually isotropic kernel, such that this is a Euclidean distance, is also used. In some of the physics simulation, like molecular dynamic simulation, actually this kernel also used when this input is a density, is an electron density of certain space. And that's actually quite useful. This density could be, you know, that could be a function. So that could be a huge dimension. Huge dimension. For some mechanical model, this kind of kernel is widely used. The reason is for certain dimensions, it could be pretty weakly. For certain dimensions, we don't have lots of change. So you see, this is more flexible compared to some isotropic kernel. So this is only a sound review. So why this method will work? Okay, so in general, for example, I'm demonstrating like 2D true function for certain function. We don't know. We have some observation. We have some observation, okay, and then you use the value of this observation. Could be the it could contain certain noise, okay, depending on the numerical solution of certain PDE. It could contain small noise, and then you have certain domain of this physics. You try to fill this space, for example, this n to the uh this number of observations is 12 or 2d. This is very, very small number of observation. And then you can see it's not that accurate. When you increase, then it becomes accurate. Then it becomes accurate. So, in general, we so all the convergence later, I mean, it requires this kind of so-called space-filling design. So, in general, you need to figure space. This is sometimes forget, actually, by some physicists. They claim, okay, this Gaussian process, and sometimes it was called the predicted mean was a kernel-rich regression. It shows this work really well. But if you have huge-dimensional design, If you don't feel it, in general, when the point is far away, the prediction will be inaccurate. Okay. So this is sort of already why known and then you can compute so-called predicted distribution. Basically, given certain end point, you can compute for new input theta start. Suppose we give some objective prior, okay, for the mean parameter and variance parameter. For the mean parameter and variance parameter, and then you can show this is a t distribution. Um, and then and then and then if you have frequencies, you can you can probably use some MLE. But the maximum likelihood estimator for this parameter was found to be unstable. I'll just quickly show you. And I didn't give a specific form, but all this form, like predicted mean, was, I mean, it has a closed form expression, and all the uncertainty is well known. Okay, and then this. And then we have some. So, this is some work when I was a PhD student at Duke. Basically, we find out this MLE for sunsetting. Basically, when the sample size are not super large, you can see the maximum like your estimator of the parameter in the kernel, those gamma in the kernel, could be could cause certain, could cause the mass not very stable. For example, here I'm showing: okay, this correlation matrix. Okay, this correlation matrix could be estimated to be a very like independent matrix. This typically happens when your observation is jumping up and down, right? So, so this in the MIE goes to independent, like identity matrix, then the predicted mean, sorry, in this case, I mean, degenerate and with certain spike. This is not very good. And in another case, for another function, when this is estimated to be very, very correlated, then you may have certain, you know, I mean, inverting this may. I mean, inverting this matrix could lead to a very large arrow, and then we have this certain parameterization such that we can show that these two cases won't happen by studying the tail. In a sense, these two cases we define as a tail. The estimate parameter of this marginal posterior mode won't go to any of these two cases. So, there are some paper to avoid this case. And also for large sample. um and also for large sample now i mean for for image this also can happen even you have lots of data i mean image like uh microscopic uh image or something like that so this i think uh after i graduate i realized this is actually more useful than i thought anyway so um what when this is a scalar value uh we just talk about a simple gaussian process and and for many settings for example uh for physics simulation uh the output is in The output is in either temporal, spatial, spatial, temporal domain. In that setting, for a given physics parameter, right, the solution is on a certain domain. We also have certain work on this so-called parallel Pasha-Gaussian process that's suitable for emulating a computer model. Sorry, each of the theta you give output, a huge output, either like an image or like a Either like an image or like a very long vector. For example, in one setting, we have 23,000 for this Titan 2D simulator that simulates power classic flow on lots of spatial coordinates. And then computational order. Okay, we use objective prior, we compute a predicted distribution, and all this predicted mean has a closed form, and then give a new physical parameter. We can use predicted mean to predict the setting. We don't need to run this computer model. And then predicted mean only take n. And then predicted me only take n squared to k times k and the n to a qubit operation here. n is the number of the training output, which is kind of small. And small, I mean 50 or 100 for like two-dimensional setting. I will show you like four-dimensional setting 50. I mean, that works reasonably well for this Titan 2D. And this K is really large. K can be like 23,000, can be when you have tempo components. Can be when you have tempo component is even large, like 10 to the ninth. This is linear to the k, so it's pretty fast. Okay, so we have this package. We, I mean, spent a lot of time on this package. And then, and then, and after a few months, we polish it and adding some more function. So, to use this package, I mean, it's very easy to use when you have a scalar output, you have some dimension, a p-dimensional. Dimension, p-dimensional input, you can use this training. You have this give certain training and then you predict it. And then, and then you have all the predicted mean, predicted uncertainty as a predictive variance 95 interval. And when you have a vector, this vector could be really long. Let me just highlight. It could be very long. You can use this function. This is in a robust guess package. And then a user can specify, you can add this argument. For example, you add a nugget, add a noise term, use other kernel, use. Uh, use other kernel, use isotropic kernel. We have isotropic kernel. You can use posterior mode or preverse year. I added MLE, MLE. If you're really like that high frequencies, you want to use MLE, that's fine. And then this prior join robust prior, when you have a large dimension, but you have a few, some dimensions that do not affect a lot. These are so-called inner input. You can, this prior, the posterior mode will have certain shrinkage effect. Will have certain trinket effects, it will force this to reduce it. Basically, those input will go away if they are only in the covariance matrix. And then that can identify this inner input. Okay, so this is a package. Maybe you want to try it. So far, your other group and our group keep using this package for different applications. For example, this is a Titan 2D. I just talked about each simulation at that time, take like Each simulation at that time take like one to two hours for this supercomputer in the University of Buffalo to run. And then what we are interested, for example, we define a hazard event. So here, for each setting, we compute the maximum power curst flow height when volcano erupts at this Montasha Island. So then one can define, for example, hazard events when you have this power capacity flow high larger than. This power capacity flow height larger than one. And then we compute this one meter of control. So inside this one meter, that's power capacity flow height is larger than one, outside smaller than one. These are all auto sample prediction. And then this, I think, this only use 50, only 50 with four dimension. And then this is the number of grid. It's actually quite accurate. You can see why it's true, one is prediction, right? I mean, many features of this space. I mean, many features of this space, like the hue, all these are incorporated in this computer model. So you see that it could have holes right in its controls because because here there's a hue, so the flow is unlikely to flow in this region. Anyway, so we also work on other setting mechanical models. For example, we link some observation like the GPS and some of other observations like a satellite radar interfereogram to what's going on, this travel. What's going on this chamber of this for the magma? Okay, so then we have physical mechanical model to model the geometry of this chamber, and then we can use the PDE to solve it. And then there are, so here this is an emulator for this. So these are, I think, the number here are the depths and the location. And then they also have analytical model. And then this is solved by final element. Solved by final element, and then you can see ours are always very accurate to approximate this final element, but analytical model is not accurate. And for this application, basically we'll have those ground deformation. So this magma, I mean, inflowing, that will affect how this ground change. So then we use this ground deformation to infer what's going on underneath and then to elicit the hazard. So we have a paper we publish. We have a paper published a few years ago in Science using this to study the volcanic eruption in Hawaii. So, I just want to highlight this very fast. So, using this, they actually run full MCMC. So, you can imagine this paper is full Bayesian inversion. It runs full Bayesian MCMC to invert the physical parameter of this physics model, okay, using satellite interferograph, GPS, etc. Okay, so our model is. Etc. Okay, so our model is very fast. So each second using emulator, when you have, for example, 2000 coordinates and then spatial coordinates, it's pretty fast. It still can generate like more than 200, 300 simulation per second. This is even faster than analytical model. Final element is too slow. You need many, many seconds to generate even one simulation. So it's not shown here. So it's very fast. And then this emulator allowed. Then, then, this emulator allows them to use this final element model in this calibration setting because they need to generate lots of MCMC samples to calibrate this physical model. And more recently, like worked on this area for a year or two, this molecular dynamic and then this density functional theory are so widely used. So, just here this talk of this machine learning of this density functional theory. This basically simulates. Theory. This basically simulates the molecular dynamics and then use density functional theory to describe the material behavior. For example, given atoms configuration, you can compute the force, you can compute the energy, and then also the talk is estimated like there's one sixth of the world's supercomputer that they are computing some kind of density functional theory thing. I don't know whether that's true or not, but. I don't know whether that's true or not, but I mean, this is so, I mean, it's very widely used in physics, in chemistry, and then there are lots of machine learning development. So there are lots of physicists and Pliemass and CIS people using this, emulating this. For example, the input for this setting, I mean, for this one, it's atoms, 3D atoms position. And then we actually need a pairwise distance to guarantee some so-called translation invariance, meaning that moving. Translation invariance meaning that moving this atom from somewhere to somewhere that I mean this this force still keeps like this. So the tip the p is super large and for this setting actually isotropic kernel was used okay because they are in a real space and then actually they work reasonably well and this actually is a function so they they use this electron function to describe what's going on and then uh the the the the setting there basically is that it set this this Basically, is that this function satisfies certain constraints, and then we need to find this function, and then we need to find the functional, right? This energy is a functional of this density. Okay, anyway, these are some just introduction. So the real thing here, okay? So the preferred thing I did, so Gaussian process still widely used, and so as the deep neural network. It just, when the tests become more and more specific, it seems to create a high barrier for non-experts. Create a high barrier for non-experts to use. And some of the very widely used signal processing algorithms like common future fast Fourier are so widely used in daily life and in many research projects. And then there are some challenge I didn't solve and then still remain challenge. And then there are lots of wonderful work. One thing is computer issue, right? So there's still this end to the third. So when your input are really, when you have more input, right? Right. More, I mean more than a thousand. That means to say, more than a thousand is already pretty slow. For spatial setting, there are lots of wonderful work for approximating low dimensional input, I mean two spatial or spatial time pro three. And it's a choppy and less smooth kernel like exponential kernel by like the nearest neighbor, like SPDE. And then in machine learning, this induced point also widely used. And then we actually follow one of And then we actually follow one of the work, that exact computation that's available. And okay, and then you see here that due to inflation, each question was more. Now this question was, well, originally it's median, but due to inflation, it become billion, now it's trillion. Okay. So do we have exact reason without any approximation that we can compute multi-dimensional input for general design, like letting up a cube? So that's Lighting up a cube. So that's the question I want to put here. Of course, there are other issues. Originally, there are this robustness issue. I sort of did this at the beginning, I talked about it. Of course, I think there are two fundamental issues that prevent Gaussian processing. One is this computation, the other is multi-dimensional. When you have really high dimensional, how do you constrain it? How do you actually feel that space, that functional? I mean, if it's a function, if you emulate a functional, how do you emulate? How do you feel that space? Anyway, so the quick simple result for So, the quick simple result for certain settings, we have exact method. This blue is exact. So, there are some QSGP and SPGP and machine learning methods induced point. And then LNGP is not really good here because, as I talked about before, when you have this smooth function, right? Very smooth function, this isotropic assumption is not very valid. I mean, they are very good for spatial settings for exponential kernel, okay? But for material, for higher order, I would. But for mature, for higher order, I will show you why they are not that good. And for some setting, exact GP actually does not take a lot of time, actually, I can tell you. And then, why do we want higher order? Because in non-parametric setting, you know, when you have higher order mu, it converges faster. And we have finite example, when you have a very smooth function, it really converts really fast for those higher order settings. But for spatial, I mean, that's already pretty good sort of method. Okay, so. Method. Okay, so here, this is, I think many of you probably heard or know or where. So for Gaussian process, here I'm just reviewing one dimension with a matern, right? Maternity is why you use, for example, nu equal to 2.5. This is a default setting in my package and also in some for other Gaussian process package. You have this form of correlation. Let me just add a noise here. Suppose, well, either you are time serial setting or you have certain error in PDE solution. In PDE solution. People have found that you have for this setting 1D, by originally, I think this connection by Wito and then Saka has a very nice paper 2010 connecting this to SDE. So this classing process can be exactly represents by this classic differential equation. Okay, and then this number is a transformation of this roughness parameter and range parameter. And then this one, this can be solved exactly. This can be solved exactly, and then we have a dynamic linear model, right? This is a continuous state space, so it can be unequally spaced. So, suppose I think Catherine mentioned this equally space, and when you have unequality space, you have something else. This is exactly unequally space. Okay, so this T could be very unequally spaced. We still guarantee this is this is exact computation. We can compute exactly exact distribution simply because this can be written as a continuous time dynamic linear model. Continuous dynamic linear model. So, this you can review, for example, some of the dynamic linear book by Mike West, by Sonia, by, well, there are many, many books. I mean, this is the continuous time. I think this is directly a representation of Gaussian pro this Gaussian process with matern 2.5. And all the half integer maternity can be represented as this one. The reason, okay, the reason is, okay, you see this, this, this basically, it Basically, it depends on what. So, the latent space actually depends on the latent value and first and second derivative for this one for this higher order, like this one has a twice, it's twice differentiable. So, it actually depends on the first and second derivative. And then the reason, okay, why this can be computed exactly is that this theta, this joint distribution, this position matrix is a block triangle diagonal. Okay, block triangle. Trine diagonal, okay? Block triangle. Each of these is block, and then all these are zero. And then, because of this, we can run common future. And actually, I will tell, I will talk about why the NGP work really good for exponential because they only look at this latent variable. If you look at this setting, you actually need to condition on the derivative, okay? Now the first and second derivative of the process. Anyway, so simply because of this fact, now Because of this fact, now, okay, all these values, I mean, just say, okay, this G and W, they are closed form, okay? And then in this paper, we compute this one, return 2.5. Actually, at that time, I don't find any, so we computed this look tedious, okay, look annoying. Some of this, I admit, I use some because you have this matrix exponential, it's really hard to do this integral. Some of this I use Mosmatica, I believe, at that time, in 2016. At that time, in 2016, and all this course, you look at this paper in the supplement, we give all this, and then they are all coded in this fast gas package. So, they are actually exact. So, the exact using common future, okay? So, common future basically one step look at predictive distribution. You can compute, you know, one-step predicted distribution, and then the filtering distribution, and then at the end, so you filter forward and smooth back. Okay. And then you or the like, so here I'm just You all the like so. Here, I'm just saying all the likelihood you want this likelihood function, right? You want this likelihood to do maximum likelihood estimator or to do some um full MCMC or do some maximum posterior mode. All this you can use this likelihood. This likelihood after this common filter, now you see you can find this F and Q. They are basically here. Okay, that become this normal with this only O O N. This is linear computation. So this likelihood can be computed exactly normal. Likelihood can be computed exactly, no approximation and per detailed distribution, also exact. Okay, at the observed value, first of all, it's exact condition on all observation is exact in O n. You can compute this smoothing distribution and all the prediction in the middle outside, okay, it's all one for any of this, okay. It's exact predict distribution. Computing this is exactly the same as computing Gaussian process. For example, with this paper, with Yan Xing Shui from Johns Hopkins, we compute, for example, this. We compute, for example, this material 1D. So, this is a setting metallation. So, they find these are weakly kind of correlated, right? Along with the CPG size. And you have lots of CPG size, like 10 to the 6th, 10 to 5th. So, using Gaussian process directly inverted, up to like 3,000 is each computing, each likelihood, you need 40 seconds, okay, for a very good computer. For this, you barely see any time change, okay? Time change, okay, using using this, and they are exactly. Let me let me say this predicted mean likelihood they are 10 to the negative 15, 30. Okay, no approximation at all. This is a machine position, okay? They are exact the same thing, just two ways of computing this. And then we actually using that, we actually can emulate this miscellaneous level of that. So, this actually has something more, but the key idea is this. So, common. is this so carman did command tackle this problem okay the right he he basically have this this um this diam right dynamic linear model and for for this model let me just say okay there could be two possible way of doing this problem okay um one is sampling okay i mean keep sampling mcmc are so useful right one thing one may tempting to sample this okay yesterday we just have a very Okay, yesterday we just have a very good talk by you, you know, these sampling of this physics phase transition IC model using GIB sampling and global dynamics. Actually, this kind of is still very, very useful. I mean, MCMC is wonderful, okay? It's just in this setting, the theta, you have a huge dimension space, let me just say. Sampling theta, okay, it's not very efficient, right? Not very efficient, right, in this setting. If you directly sample from a posterior, okay. And then if your frequency, you might attempt to optimize it. Okay, you might want to give certain constraints and optimize this theta, okay? But then that ignore the prior of theta. So many, many years ago, what common did basically, in the common future, I just, I mean, I just talk a review, he integrate out this data iteratively, okay? Theta iteratively, okay? Combining the prior of theta. So he basically integrate all this, right? Iteratively. He didn't sample it. He didn't optimize it. And luckily, because of that structure, that he can sample it and only take O n. That's much, much faster than n qubit. And everything is exact. And solution, but it remains optimal as of L2 loss. So marginalizing out the latent variable is exactly what Bayesian will. Variable is exactly what Bayesian will do. So, right here, maybe comma is a Bayesian, or at least he thinks as a Bayesian, or maybe I will say you think as an objective Bayesian, because this is a Bayesian objective Bayesian conference. Okay, anyway, what's the generalization? So, if you look at this, for simple 2D, you may want to attempt to vectorize it. For example, you define a certain grade with certain missing value. Define a certain grade with a certain missing value, you might want to vectorize this y, but I just want to remind you: if you vectorize it here, this q could have a huge dimension. Okay, well, it's not as large as that, but it still has a large dimension and you need the inverse. When this y is only a scalar, this inverse is so quick, right? There's no inverse, one over a number, right? So, so when you directly vectorize it, you still have this, this could be slow. So, what was the innovation, some of the recent development? Recent development. One development is called this generalized principle component analysis. How many times do I have 15 minutes or so, right? Is it? Yep, 15 minutes. Okay, okay. So, okay, so quick, just review this. So, this model, okay, this you can think of a random factor model when the factor are correlated. This model in spatial setting. This model in spatial settings is called linear model of coordination. And in machine learning, this is a subcase of semi-parametric latent factor model. So this Z each row is a Gaussian process. And then the prior is independent. Well, they have a correlation across this X. X could be time, could be space, could be physical input. Okay. And then this, this, so Y can be think of, you can think of this multiple time series. Okay, this X. series, okay, this x then becomes time, and then they may have correlation between this time series, then this a can be actually captured the correlation of that, okay? So this model, LMC, was so widely used and also in computer model emulation, I'm just reviewing some work that had been done. So for this model and computer model, this Dave Hictor have a very nice paper in JASA 2008 and Ray Paulo, very nice work. In that work, this A thing, this matrix factor loading matrix. thing this this matrix vector loading matrix was estimated by PCA okay a very useful model and then PCA and and then many years ago in 1919 by Tipping and Bishop Michael Tipping and Bishop a very nice paper saying okay when Z is completely independent no correlation either across this this row or column right here the model is is this this Gaussian process as a multivariate normal okay so if z is independent then p CA and PPCA this latent space PCA and PPCA, this latent space, linear latent subspace of this A, okay. The PPCA, okay, what they do is, okay, we assume this prior is the normal integral of Z exactly, okay, by hand. And then we can show that what's the maximum marginal likelihood estimate of this A, and then they find out that one, the subspace, that maximum marginal likelihood estimate is exactly the same as PCA. Okay, then if you look at, so the good thing is they have a sampling model, but the sampling model is not the sample. Model is not the sampling model of this one. This one induce correlation across x. X could be time, could be space. Okay, this model assumes independent factor. So that means this PCA actually is maximum. I mean, the subspace, okay, they are equivalent when Z is completely independent. Is it completely independent? So this is not the exact maximum module like your estimator. If you have correlation here, okay, it's correlation here. So there are also frequencies. So there are also frequenties. Frequenties, then they give constraint and then they do optimization, right? For this, this GSAM bi and this factor model. And actually, in the beginning, I should acknowledge they don't assume Z is random. They treat as fixed unknown constant. And they do optimization. And then this actually, they can show this A also the same as PCA. And then there are lots of later, you know, this biometrica. Metrica and also statistics, they try to incorporate this correlation, okay? And then, of course, many, many wonderful sparse factor models by many other people like Mike Wise and Downson, and machine learning, they cannot marginalize all. So they find lower bounds, okay? They find this variational auto-encoder that's pretty, pretty useful, okay, using many settings. Okay, what we do is, okay. Okay, what we do is okay, we assume still we use that model. You have temporal correlation, you induce temporal correlation, you have this prior. Then let's just follow what Karman do, what Bayesian will do. We integrate them out, right? You have a prior, you integrate out, right? Okay, so here we assume, okay, so this assumption, because this only the linear subspace can be identified and all the variance part can be modeled by this, so we use this. So we use this. This kind of constraint is also used in previous paper and also in some of the factor model settings. And given that, we can integrate in our Z. And integrating R, we can show that the marginal likelihood is maximized exactly here. The only change, okay. The only change is now the correlation comes. You see, the covariance now comes as this one. Okay, so this. As this one, okay, so this is still the principal eigenvalue, but it's another covariance matrix, okay. And then we don't, when the covariance matrix are not exactly the same, it's optimized a certain quantity, okay. This quantity, and then we have this algorithm, but very nice algorithm here. This is not by us, but there's algorithm that preserve this constraint is this C4 manifold. And then we actually implement it. So they are code. There's a GitHub for this. This is pretty quick, let me say. This is pretty quick. Quick, let me say this pretty quick. Why is quick for 1D input the code actually that use that common feature thing? So, for anything that relates to time, 1D is very, very fast. So, then let me just show you some simulation. So, first, when you have this setting, when you have correlation, this is a similar setting. We put it there in the paper. If you sample the model, okay, and then you, so we evaluate by the, for example, the angle. For example, the angle, okay, angle of the linear subspace. For example, from two model, and then we compare with PCA, compare with this biometric reason biometric paper, etc. And then we can show these are much, the angle are all much better. And we also show that the covariance are not different. They are also much better, if you trust me. And then we also show if this is misspecified, we use, for example, it's a material for Gaussian kernel, is an exponential kernel, whatever, as long as you have. Whatever, as long as you have certain covariance, okay, you will do better than PCA, okay? Because PCA is, you know, the latent space is just like the review here is independent, it's completely independent. When you have some correlation, you model it, you will do better. Okay, in terms of prediction, in terms of prediction, remember there are two types of correlation. One is across time, one is across this time series. The PCA only can capture one part when you have both. Capture one part. When you have both parts, you see, this is the truth. When you have really noisy data, right, the PCA capture one part, and then if it does not capture the part across time, then you could be very, very wiggly. And the GP PCA here, the method are kind of close to that. Okay, all the code are online, okay? All this simulation and the real setting. Okay, so real setting, how to use it. The good thing is in real setting, you can model, for example, spatial temporal this later factor on time. This latent factor on time. And then the very large spatial covariance, you don't need to specify the covariance matrix. Okay, you estimate it directly after marginalize out the latent factor. For example, this is the spatial temporal model, kind of large setting, you know, 100 months and lots of grids. And then, for example, these are some simple comparisons between GVPCA, PCA in terms of auto-sample prediction and also spatial. Auto-sample prediction and also spatial temporal model, right? We can implement. For example, here, if you look at predicted interpolation, so this interpolation, okay. So for some, so the different is a choose holdout value and then these hours and this by the spatial temporal model. Spatial temporal model does not work really well and also pretty slow, much small, kind of slower than ours. The reason is it's a globe. We didn't implement that spherical curve. We didn't implement that spherical kernel because it is a globe, and in our model, we don't need to specify that. We don't know what kernel it is. It directly estimated that by captured by that A matrix. Okay, so you'll see the RMSE is better. And we also show some computer model simulation setting for that case. Anyway, so the last example, just quickly mention, when you have irregular missing, so let's say our model cannot be written as a matrix, we have missing value of that. Matrix, we have missing value of that matrix. How do we do that? This is not, we find out some nice property, but it's still not ideal. The reason is we still need to sample the Z. So, but anyway, still, this almost the same model. But now we, let's say we still like spatial temple, we can have certain kernel put it there. The setting here is still the same, I would say, the prior independence, orthogonality. And then now the data could be irregularly. The data could be irregularly missing. And then we derive this, is we derive two posterior, I mean, some good property. So, from this prior independence and this, we can derive the posterior independence. Okay, this is quite a nice feature. It's not very hard to derive given that. And then we can also derive this so-called orthogonal projection. This is why it's a huge dimensional matrix, a huge matrix. This can be decomposed by this product of this multivariate normal. Product of this multivariate normal. So you don't need to deal with a huge like n1 at n2 times n1 n2 covariance, but with this sub covariance, this matrix. And then we can show this, okay, purposely just show you this GP separable model. They are subcase of this, okay? They are subcase. They can all be written by this model. And this is more general in a sense that you don't have to assume the latent factor share with the kernel. So this is in general not separable at all. So, this is in general not separable at all, not separable in general. This is LMC. And here, this paper will also further explore there are some trends. When you have a huge data, like this spatial setting, this spatial data, very large data, you actually can have certain trends. You can identify certain trend. Then the mean, the trend does not have to shear across the coordinate. So, here we write down certain model of the mean, but then when we sample by giving. When we sample by Gibbs, right, the marginal model, then they have this identifiability issue because this Z and this mean, okay, they are both flexible. So way to deal with it, still follow the same spirit of Bayesian of comma, basically integrate out. And then before you do the sampling, so if you want to sample B, integrate out Z. Okay, so that's the lemma we do there. Integrate out Z, if you want to sample B. If you want to sample B, then the identifiability issue sort of goes away. Okay, anyway, but then if you have, well, they are column-wise, row-wise, it's kind of tedious. But I write down in this form simply because when X is one dimensional, I can use common filter. Okay, the overall algorithm here is here. I mean, still, we need the MCMC if we have irregular missing case, we still need to see. Uh, case we still need to sample z, uh, but the thing I want to remind you here is that anything related to time is linear, okay, because of the common future and the package. For example, this is a simple simulation. This is small because we can run full. And then this is because this is an exam model, let me say, this is exact. No, we don't reduce the rank. It's pretty quick. We don't need to reduce the rank. And then the And then, if you look at the difference between exam hours, these are only MCMC arrow. Okay, exponential. And the computation order is this n times dmax. Okay, it's pretty quick. And then for height, this is a larger, so I can't run exact model. But if you see, if you look at here, even we have a large space of missing, the prediction is still relatively well. And then I also computed Computed, for example, compared with this aggregate, this data set is one data set in this paper. And in this paper, they have like around 10, 8 to 10 methods, verbal wide use, spatial method, or to do certain spatial interpolation. How the prediction in this paper, recently accepted, we also do that. Okay. So, but the question there is still, There is still this, okay. Carmen didn't have any, he does not need to sample Z, okay, but he only can do time very, very fast. Okay, 2D, of course, there are other things going on. For this case, irregular missing, you see, this is irregular missing. We can represent it. So, what, so still, maybe this is go a little too fast, is that we can parameter. us is that we can parameter one one coordinate okay uh using that that that that latent factor z of t the other part is that a part that capture that such that that can make it very fast even you don't reduce the rank okay that's the kind of take-home message so so we we did quite we did a little bit on on on on on this kind of case image image was missing all this this this thing so what's going on This thing, so what's going on? If we have journal design, that you cannot write as an image, right? Like, like this, like this, what's going on? So, this is really recent work. So, we can actually I can show that we can write down this process as element-wise. So, suppose, so this is still say I'm writing n, but let me just finish, maybe finish this. So, we still can write down s as this, but with an element-wise representation, okay? This element. Representation. Okay. This element-wise is by, I mean, for each of this observation, we can still write down this latent space representation. This, I believe, this is very new show. And this is actually, I do it a long time ago. But the question, okay, so let me just maybe just say there's a result and done the paper. So this representation, how do you This representation, how do we view it? So, when you have this lattician half a cube other thing, you can view this as a lattice, but with each column or each row, that only one observation, right, for this latticium cube. And then if you want to sample this lighting hypercube, you can see directly sample it, you view it as this lattice and then reduce the other point that will be n squared. That's already much faster than n cubic, right? No result has been known for this. No result has been known for this general. I mean, sample this will be n squared, but for sample dis directly, actually we can do n log n. This is because of the sound of this binary cut thing. But then the question, okay, still, whether this is helpful in the general setting, remove from that spatial, I mean, that missing lattice, that's a very important question. Very important question, okay. We are working on anyway. So, let me just uh stop here. I think time is up. So, uh, this is our group and a few graduate students, and I will acknowledge the support from this NSF CDSE program, and also the internal booking ground of this material innovation platform. Okay, so this is my email. If you are interested, I look forward to hearing from you. Okay, thank you. 