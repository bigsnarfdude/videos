And I'm excited to present our paper today, Risk Fors, Label Bias in Everything But the Kitchen Sink. Maybe not to plug in the USB receiver. Also, it's this guy here, so that's okay. This is co-authored work with Julian Yarco and Charger, and if you're interested in And if you're interested in reading the full paper, it just was published at Science Advances. So, I want to start with some motivation. In 2009, Philadelphia's adult probation and control department started using statistical tools to prioritize supervision of old individuals. Those deemed high-risk were doing poor supervision, and those deemed low-risk were doing less supervision. What the creators of this algorithm did was they used a random forest to predict future homicide charges. They signed a variety of features. And the intellectual inspiration for this project is the following quote from the creator of the algorithm, who said, if shoe size or sunspots predict that someone's going to size or sunspots predict that someone's going to commit a homicide, I want to use that information even if I have no idea why it works, right? So sort of a kitchen sink approach to just throwing everything we have in a particular. And oftentimes when we think about these types of questions, the debate over which features to use is framed in terms of accuracy and fairness, with more features presumed to improve accuracy. So we might think that it might improve accuracy a little bit to include shoe size, but there are a lot of really good normative reasons I think we all agree with to not include that in a free department like this. But I'm going to show today that in cases of label But I'm going to show today that in cases of label bias or target construct mismatch, when the true target of prediction, let's say criminal activity, differs from the available proxy label, i.e. charges of criminal activity, then the story is more complicated and there are actually some statistical reasons we might not want to use kitchen sink models. So this is a sort of stylized example for my primary domain of knowledge, most of my work outside of this deals with criminal risk assessment. We have sort of a really simple model of the world. We have arrested time zero, which we observe, arrested time one, neighborhood, which is just a binary proxy for whether it's a high or low concentric neighborhood. Proxy for it to high or low policy neighborhood and unobserved behavior. And this is a pretty normal setup for criminal risk assessment, right? We have some features, and we're going to try to train a model to predict our proxy label. We're actually interested in our true label, B1. We really don't care if you were arrested for homicide. We really care about the thing, which is that were you actually committing the homicide for a bunch of reasons? These labels might. So it turns out our intuition is correct about these models. Including neighborhood yields a better estimate of arrests, A1, when Rests A1 when including in the model. So a more kitchen sink model including neighborhoods going to do a better job. But excluding neighborhood yields a better estimate of behavior. So a model only using A0 to predict A1 will perform better on B1 than a model using Z in this stylized example. Now why is this? Just sort of intuitively, neighborhood is going to sort of steer us wrong in the case of behavior. That's because conditional on past arrest, neighborhood is negatively correlated with future behavior. If you have two people, one both with one. If you have two people, one both with one arrest, but one lives in a high policing neighborhood, you're probably going to think that their underlying criminal propensity is lower because there's high policing in that neighborhood. On the other hand, neighborhood is positively correlated, as you can see here, with feature less. And the sort of theorem to prove in this paper is that for linear models, performance improves by dropping a covariate whose conditional correlations with the true and proximate labels have opposite sides. This is a super stylized example, but I'm going to apply it to a paper that will probably be mentioned 20 times over the course of this conference, which is Obermeyer et al.'s 2019 science. Which is Obermeyer et al.'s 2019 science paper, where we talk about healthcare systems using algorithms to identify patients for high care risk management. And they're trained to predict healthcare costs, but these are not the same as your other healthcare needs. So unequal access to care means less money is spent caring for black patients who are just as significant patients. What we do is we use the data from Overmeyer et al. to train a complex model using all available information and a simple model where we exclude past costs and demographics to predict future costs. And the idea here is that intuitively, past costs, initial and current health, might On current health might have a positive correlation with future costs, but might not be correlated or negatively correlated with future health. And it turns out this is the case, right? We're actually able using our simple model to enroll more high-needs patients in the program than our complex model, and also to enroll more black patients. Now, of course, in this case, you might want to train on the true label, right? Someone brought up earlier that's not actually the true label, but you might want to actually train a model there. But there are tons of situations where we might not have access to anything resembling a true model, and that would be the case in criminal resistance. True model, and that would be the case in criminal risk assessment, where we really can't get a good estimate of criminal behavior. So, oftentimes, people think about prediction, and we don't need to understand why our features have predictive power, but we hope to show here in this work that actually it can be really helpful to think about the data generating process in cases of label bias, and we need to be more careful about patients-same approach to predictions. Just like the causal ML literature has shown how prediction can aid our understanding of causal inference, here we show how data generation processes can be used to aid our understanding of information. And thank you so much. And if you want to hear more about whether And thank you so much. And if you want to hear more about when this might or might not apply to practice, hopefully I'll come over and chat to the post test. Could you say what you mean by neighborhood is negatively or positively correlated with some sort of outcome? Do you mean like some indicator of the level of policing in that neighborhood? Right, so this is a sort of a very stylized example. What you'd really want for that value is like some estimate of Like some estimate of police presence in the neighborhood. Oftentimes, police