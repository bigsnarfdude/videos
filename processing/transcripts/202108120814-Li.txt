Thank you, organizers, for this amazing workshop. Today, the topic will be on the symmetric perceptron. This is based on joint work with my advisors, Emmanuel Abe and Alan Slack. So I'll start with the model. We consider a matrix G of size m by n, where each entry is equals plus or minus one. equals plus or minus one with probability a half. This is a picture illustrating the random matrix. And then we look at vector x of size n, where each entry also equals plus or minus one, and ask the following question. For kappa real, when is this following set non-empty? We look at the matrix G times the vector x after normalization. This is a vector of lens m. We require this to be entrevisely large. require this to be entrevisely larger than or equal to kappa. An easy observation is that when this matrix is taller or when m is larger, the inequalities are harder to hold true because the resulting vector is longer. And when this matrix is shorter, when the matrix is shorter, the inequality is easier to hold true. So when we are raising this question, when is the volume set not empty? this volume set not empty we're extra we're extra actually asking what is the largest m such that this set holds um and if this set is not empty we ask what does this set look like what is the solution space structure and a further question is is there an efficient algorithm that can help us locate one of the solutions in the space so three questions in mind what is the largest m such that the set Is the largest m such that the set is not empty? If it's not empty, what does the set look like? And whether there is an efficient algorithm in finding them. So, before I present the literature, let me mention a few popular variants of this model. In our case, Gij are having plus minus one spins. This is called the Bernoulli disorder case, versus if Gij are taken to be standard Gaussian, this is called the Gaussian disorder case. The Gaussian disorder case. And here we're taking, sorry. Yeah, so when X is having easing spins, this is called the easing perceptron. And when X lies in the sphere, this is called the spherical perceptron. So Mark actually talked on Tuesday, and his work is on the spherical perceptron. So just both are popular models to consider. Models to consider. Today, I will just focus on the easy perceptron with Berlin disorder. Let me review a few popular, a few literatures. So we define this ratio alpha to be the ratio of m over n. We call this capacity. The reason that we have this name is because originally Perceptron is a toy model of a single layer network, and this ratio alpha represents the capacity of. Alpha represents the capacity of this network. So, this is where the name comes from. In the physics world, Cruz and Mazard in 89 can compute explicitly the volume term, one over n log of the size of the solution space, converged to an explicit function. And under their series of work, they expect to have this capacity threshold, where this is the time when this set as of Is the time when the set S becomes an empty set or this explicit function hits zero. So basically, above this threshold, you shouldn't be seeing any solutions and below it, there should be solutions. And this is physics work. In the mathematics side, Kim and Rush in 98 and Talgrad in 99, they showed that when kappa equals to zero, the critical threshold or the competitive Threshold where the competitive threshold lies between epsilon and 1 minus epsilon. In particular, for this side of the inequality, Kim Rush proved it constructively. They construct a multi-scale majority algorithm to show this side of the inequality. When saying alpha C is between epsilon and 1 minus epsilon, I'm not implying that there exists a critical threshold. Threshold. This just says that when alpha is above one minus epsilon, there's no solution with high probability. And when alpha is below, there's a solution with high probability. And we don't know whether there's a sharp threshold or not yet. It's just says what happens in the extreme cases. And there has been 20 years without much progress until Dingan Sun in 2019 showed a capacity. Showed a capacity lower bound that matches the prediction of the physicist. This is a very nice piece of work. And in addition, Xu in 2019 showed the existence of a sharp threshold. So this is all on the current progress for capacity threshold. And there are some other results for the solution space. In the same paper by Cruz and Mazard, they actually predict They actually predict that typical solutions belong to the cluster of vanishing entropy density. And Huangan co-authors extend this conjecture to say that actually typical solutions are indeed isolated. So here is a picture illustrating a rough idea of the behavior of the typical solutions. Each point indicates a solution, and you can see that points are typically of a linear distance away from each other. So same picture. So, seeing pictures like this typically means that there will be no effective algorithms, efficient algorithms in finding solutions in it because solutions are scattered. But this is not the case. Find by Bruce Day and Akina in 2006, where they proposed very nice message passing algorithms that are shown to empirically succeed in locating solutions. So, how to explain this simulating context? To explain this seemingly contradictory phenomenon that, on the one hand, typical solutions are isolated, but on the other, there are very nice solutions, finding solutions, very nice algorithms, finding solutions in it. And so to explain this, Portassian co-authors, they propose that there might be because the existence of this rare clusters of positive entropy density, so sort of besides those typically isolated. Those typically isolated solutions, there are some other hidden structures in there that are well connected, and those are actually the solutions that are find by these successful algorithms. So this is sort of the physics prediction picture for what should be happening in the solution space. Okay, so the current literature for those three questions. So I've been talking a lot about using perceptron, but today's topic. Using perceptron, but today's topic is actually on symmetric perceptron. So let me go to symmetric perceptron. So the symmetric perceptron is just a symmetrized version of the original perceptron problem. The matrix and the vector are the same, but we just look at the different symmetrized set of inequalities. We look at absolute value of t times x normalized by 1 over root n. We want it to be less than or equal to kappa. Or equal to kappa. Originally, we don't have the absolute value and are having larger than we got to kappa. Here, the difference makes the problem symmetrized. This is the corresponding picture. This is a model proposed by Opin and Cosmos in 2019. The reason that we consider the symmetric perceptual problem is because it's expected to share similar properties to the To share similar properties to the original easing perceptual model, that the existence that typical solutions are isolated and the existence of this rare cluster of positive entropy density. But compared to the original asymmetric perceptron, this is a more tractable model to work with. So that's why we consider it. Okay, so let me give the first part of our results. Part of our results, we will have a few serums, and this is like the logical relation between the theorems. We will first have a normal limit result for the number of solutions, and this will imply a continuity result connecting the planted model and the original model. And this together will imply a sharp threshold result and a freezing result, the freezing property for typical solutions. I would like to mention that while we mention that while we are sorry while we are finalizing our papers it comes to our attention this paper by Perkins and Xu that are uploaded around the same time they're also they work on a similar symmetric perceptual model but analyzed the Gaussian disorder case they had a similar sharp threshold and freezing property but they solved the problem in a completely different way that they don't have the log normal limit and the contiguity I just want to mention their work I just want to mention their work and let's start with our theorems. So, the first is a log normal limit. If we define Zg to be the number of solutions, then for any alpha less than the critical threshold, the number of solutions normalized by the expectation converging distribution to a log normal limit. This beta is an explicit parameter that depends on kappa. We know it's just written in this way. This way. So, yeah, it's sort of just characterize how many solutions there are. With this result, we're able to show the contiguity result. So we first define a planted model. Let x be uniformly chosen and random from plus minus one to the power n. And then we use this p star to denote our Plantin model, you know, the P star to denote. The p star to denote the 2t law of g condition on this x is one of the solutions. This defines the Planting model, and we are able to say that for any alpha below the critical threshold, P and P star are mutually contiguous, meaning that for any sequence of events a n, p of a n goes to zero if and only if p star of a n goes to zero. Contiguity is a very nice theorem to have because as long as To have because as soon as we have it, a lot of properties are easier to prove in the planted side, as in the freezing property that we will show. With this continuity, we're able to translate whatever we have in the planted side back to the original model. So, yeah, it's quite a powerful tool. So, this is continuity and and next it's a sharp threshold. Alpha larger than alpha z, there's solution, there's no solution with high probability, alpha less than alpha z, there's solution with high probability. Okay, and the last is the freezing of typical solutions. What do we mean by freezing? So recall that our solution space consists of vectors of length n. For each entry in a vector, if we flip its sign, In a vector, if we flip its sign and it no longer becomes a solution, then we call this variable or this entry frozen. And if a vector has a linear number of frozen variables, then we call this vector freezing. So this theorem basically says that typical solutions are freezing. So sort of, when in other words, you start with the typical solution X, and you look for other solutions of distance less than Dn from it, and you realize that the only one. And you realize that the only one that satisfies the condition is yourself. Or, in other words, all the other solutions. It is a linear distance away from all the other solutions. This is the typical case. Let me maybe go through the proof briefly. So, a direct first moment and second moment computation already gives you. Moment computation already gives you the form that this critical threshold should look like. So, this critical threshold alpha c should equal to minus log 2 over log of p of kappa. This is the reason sort of I say that the symmetric perceptron is more tractable. In the original asymmetric case, if you compute the first and second, if you compute this ratio, it diverges. It does not go to a constant. But here we But here we have a constant C. But in order to show a sharp threshold result, we need to account for where does actual variance come from, how to reduce this C to one. And this is where we used a dense analog of the small subgraph conditioning technique. So we construct a random variable y that are linear combinations of some generalized cycle counts. Some generalized cycle counts and have a result like this. It basically says that the number of solutions normalized by expectation is roughly just equals to exponential of y. And because we can show eventually that y is normally distributed, so exponential of y is log-normal distributed, and that explains why we have the log-normal limit result for our number of solutions. Okay, and the intuition. And the intuition behind how we actually construct this Y comes from the SK model that I will explain in a second. So if we use G1 to denote the first row of G, so we can first look at what happens for the first row. We divide S1 of G to be the set of all X that satisfy the inequality just for the first two. The inequality just for the first two rows. Then X belongs to the S1 of G is equivalent to say that this inequality holds true. And so say, because this is a very symmetrized problem, say if G11, our matrix, the top corner of our matrix, and G12 equals to 1, the condition on X belongs to S1 of G. The marginal distribution of The marginal distribution of x1 and x2 doesn't change because this is a symmetric problem. It always equals plus or minus 1 with probability a half. But what does change is x1 and x2 become slightly negatively correlated because you sort of impose this, you want the absolute value of their sum to be small. So they're roughly slightly negatively correlated. And we can actually compute exactly like how correlated. Like how correlated they are. And roughly for one row, for the Joe, their correlation condition on the Joe is roughly of this moment. And if you assume the effect of rows were multiplicative, they're sort of independent, if you assume this is true, then you product all of these terms out, rearrange, and you get something like this, where JPG is a luminous. This is where JPG is a normalized inner product of a column P and column Q of your matrix. So, and this JPG is JPQ is asymptotically normal. So, this really looks similar to the interaction term of the SK model, where you have some constant up there, you have a roughly normally distributed term, and you have your easing speeds. So, in the SK model, So, in the SK model, we can expand the partition function to sums of products of cycles. And here we can do a similar term that we expand the partition function. And so the resulting sums of generalized cycle doesn't work directly, but our y is constructed to be something similar. There are some terms that are too large that we can't control, so we change the form. Control. So we change the form a bit, but the general idea actually comes from the expansion. So this is a first set of results. Let me briefly go over a second set of results. Recall that we have three questions raised at the beginning. What is the capacity? What does the solution space look like? And what are there nice algorithms that can locate solutions in them? So the previous research. In them. So the previous result sort of answers the capacity question and part of the solution space structure: that what does a typical solution look like? So this is a upcoming work with Emmanuel and Alan, and we're going to answer the rest of the parts. So we construct a multi-scale argument very similar to the Kim Rash argument, the Kim Rush multi-scale algorithm that we show that when alpha is much lower. show that when alpha is much lower than the critical threshold there this multi-scale argument algorithm works to locate solutions and the way it works is as follows so if this green box is our matrix then we first review this amount of columns and run a induction basis step and at this time we will look at the inner product for each row some of the rows are satisfying Some of the rows are satisfying, like those ones, and some of the rows are not satisfying. Not satisfying, meaning that they're having larger absolute values, much larger than kappa. So we look at them, and then we review this amount of columns, and for each column reviewed, we do a weighting majority volume. So for each row, there is a sign that it prefers. If the inner products here is very positive, then it prefers a negative. Then it prefers a negative sign to pull it back. So each one has an opinion with a majority vote on each column, and this moves the sum of the points far away back to the interval that you want. So say at this time, there are some further unsatisfacts. Say these are the unsatisfying rules. Maybe they also come from here, but this is just an illustration. So we can repeat this step, review. Repeat this step, review these columns, and do rating moderatority votes for these rows that are not satisfying. And eventually we can iterate this argument and show that after all the iteration, any row will lies in the interval that we like with high probability. So this will show that this algorithm works in finding solutions. I would like to say the first step and the last step are a bit different from Type are a bit different from what the middle is doing, but the idea is roughly the same. So that's on the algorithm. And then a second result is on the existence of linear size clusters around kappa prime solutions. So our original inequality is less than or equal to kappa. If we replace kappa with kappa prime, satisfy the inequality unrevisedly, then we call the resulting vector. Then we call the resulting vector is called the kappa prime solution. Because kappa prime is less than kappa, the resulting kappa prime solution is a nicer solution, but it's also rare to see because the inequalities are stricter. And we show that for this nice but rare solutions, there are linear size clusters. It is in a linear size cluster. This is very different to the typical typical. The typical typical behaviors because typical solutions are completely isolated. But here, because it's very nice, it lies in linear size clusters. And a third result says the existence of a connected component of solutions of diameter n when alpha is very much smaller than the critical threshold. So the reason, so diameter n is the largest diameter you can get. Diameter you can get. It's sort of the set x is connected to all the way to minus x. Okay, and we're able to show that actually this multi-scale argument can find solutions in this connected component. So it sort of explains why there's algorithms. So there's like a hidden connected structure that helps the algorithms locate solutions. And this set of works also works for both symmetric and Works for both symmetric and the original asymmetric perceptual. For the asymmetric perceptron, some of the wordings need to be changed, but the general philosophy is the same. And I will like to show an illustration. Note that this is really just an illustration. This is not a simulation. Some of the balls I draw by hand. When alpha is above critical threshold, there is no solution and you gradually decrease your And you gradually decrease your alpha. And when alpha gets below critical threshold, but it's close, then you see that the majority of the solutions are isolated. But there are those rare, but nice ones that starts to have linear size clusters around them. And then as you further decrease your alpha, the linear size clusters around those solutions get bigger and bigger until they percolate. Then this is the time when you start to have this very big. Time when you start to have this very big connecting component. Okay, so I want to say that actually, in a high-dimensional setting, all of the spots should be very sparse and shouldn't be a solid ball. But because we're drawing this in 2D, it should actually have a lot of holes and valleys in it. Yeah, typical ones inside the ball shouldn't be, shouldn't be solutions at all. Yeah, so just Yeah, so just sort of the straight phase. So this three results answers in like gives the proofs for what the physicists expect to be true. And yeah, maybe a last remark that I want to say. So this, the techniques we used here won't allow us to show a sharp threshold for when will there be For when will there be efficient algorithms? So, this left open, you know, what is the correct threshold that you can have below this, there will be efficient algorithms that can find solutions. This threshold empirically shouldn't be the same as the, shouldn't be the same as the critical threshold for having a solution. So, there is an area that there shouldn't be any efficient algorithms and yeah, and below it. Yeah, and below it, there should be effusion algorithms. And whether, say, it's the same as these stars to percolate, we don't really know. Yeah, so these are some further directions to consider. So that's it. Here are the references that I mentioned. Thank you very much.