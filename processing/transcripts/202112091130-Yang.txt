So we are at our last talk for the day, and Yinan Yang is going to talk to us about computational algorithms that implicitly regularize inverse problems. That sounds very interesting. So, Yiman. Yeah, thank you very much for all the organizers. And it's my pleasure to be part of the workshop. I really like the format. Oh, awesome. The format: Oh, I'm women, so I have a lot of topics I learned not only in mathematics but also how to balance work and life, and also maybe future to how to become a mom. So, today I'm going to talk about some recent works I'm interested in that also some extension of my PhD. This semester I'm at Berkeley, but I will be in Europe for one and a half years. I see some of the participants are from Europe, so I'm very much looking forward. From Europe, so I'm very much looking forward to meeting some of you in person, hopefully. So I consider the inverse problem more or less in this format, that we have a lot of information, maybe X and Y pairs. X in wavy inversion may be just my source and the Y will be data I get from my receiver. And one can think this is like directive to NOM and map that type of data. And in the middle is something that we really want to know. Is something that we really want to know. That's the oracle or the black box that we want to investigate. Sometimes it's a model already given to us: conductive, the Dussey flow or the wave equation or Hampol's equation. Sometimes we don't know that much, so we need to learn it or model it by neural network or some other ways of modeling like model reduction, etc. But no matter which format, which model we treat it as in inverse problems, there's always As in inverse problems, there's always like parameters in the middle of the oracle that we want to learn. And what information are given to us are exactly those x, y pairs. So that's where I kind of can relate to inverse problem to neural network because there is actually a training aspect of it. So in terms of the model, I feel that's most of our problems. If f is given such that I'm dealing with wave equation or Hampstead's equation, I already have. Hampel's equation, I already have a model. I just need to find that particular coefficient. It's nice in the sense that hundreds of years people think this is the way to model wave, so let's just use it. But that's also the source of our difficulties is because of the map from the parameter to the data, non-linear or variable posed, and etc. So that's basically all we are dealing with. So another type of problem will be like Another type of problem will be like, I don't know F and I can freely to choose how and I can choose model F. I think this actually gives us a freedom to have a better forward operator. So for example, what Carola talked about yesterday, we can try to see the map from neural networks rather than being constrained to the partial differential equations. And in Sue's talk, she was using model extension. That's also one way that I modify my forward operator and make it nice. My forward operator and make it nice in the sense I can easily match the data. And I remember in Liana's talk, she was using model reduction, but she also mentioned in some circumstances, she actually better convexity. I think that's also an effect that we modified F and such that our inverse problems difficulty changed accordingly. Well, this is very nice, except that sometimes it's not very easy to find such a modification. Very easy to find such a modification. So, when building neural networks, a lot of people will have to do trend error, either convolution neural network or generated red versus a pool of them. And also for reduction, things we can do for elliptic may not be the same we can do for hyperbolic. So this is our problems. So one of the way for most of us who are not solving using direct formula, we will formulate it as an optimization problem. Formulated as an optimization problem. So we feel it's more like an inverse data matching. I have my observed data and I have my simulated data, and I want to find the difference between these two. So the entire inverse problem can be like a PD constraint optimization. And then there is actually something we can choose. One is where I think my M belongs to. So in that case, I need to have a metric space for my M. And the My m and the curly m is just the set of functions I'm searching, and the dm is the metric I'm given to that particular metric space. They will come back later. And another thing that we need to choose is how we measure the data discrepancy. I have observed data and it's a bunch of them. I have my simulator is a bunch of them. But in the end, my objective function is one scalar. So I need to select a metric and then that's corresponding to data domain metric space. So D. So, D is the type of functions we are searching, and D sub G is the metric you give that particular set of functions. So, these two matrices basis will be the main part of my talk, because different ways of choosing the model space metric space and the data space matrix space can actually make a difference in terms of the difficulty of finding the solution, especially in this optimization framework. So, this is if there's any takeaway message of the talk, I just want to remember this message that how to choose these two metric spaces can make a difference. Okay, so based on my very limited knowledge and experience in inverse problem, I have tried and see several aspects from different effects by choosing these two metric spaces. In data space, you can see resolution change, robustness to noise, and optimization lens. Bothness to noise and optimization landscape. In the model space, you also see the first two, and you may also chant the convergence trajectory. A very simple example is that use Newton's method versus use gradient descent, you are going to have different trajectories. So I'm going to explain all my observations in these four different aspects: resolution analysis, robust need to noise work stability, improved optimization landscape. Optimization landscape, especially when we deal with nonlinear inverse problems. And finally, how they actually may change the trajectory. Okay, so for the first part for resolution analysis, this is something we looked at in the paper with Bronque and equation in inverse problem. So it's very easy to say things explicitly in a linear problem. So I'm just going to use linear problem as an example. I want to solve a linear problem in the form of an equal to g. form of a n equal to g and m is the unknown. And I don't have really clean g, I have a noisy data g delta. So now I'm going to have the following assumption on this for a very important part. M, I assume that is in the Hilbert space H beta for some beta integer beta. And A is diagonal inferredoman. I think this is very naive, but in this case, we can make all the calculation exact. The calculation is exact. So, I think A is a diagonal inferred domain, and similar to all the inverse problems we deal with, the forward problem are smoothing. Therefore, I assume that alpha is bigger than zero, such that this is like a smoothing operator. Okay. And alpha, the bigger the alpha, usually the worst the problem. And now I also consider the data space where G and G delta are in Hs. So I choose S such that Hs is bigger. That Hs is bigger than the output of the linear forward operator. So the output is supposed to be in H alpha plus beta based on this assumption. But I now assume that H, they're actually in a bigger space. So that actually gives me a lot of freedom that I can choose different S. So since it's a linear inverse problem, for HS, I defined through the Fier domain. So one can define the HS norm, but first take a Firo transform. First, take a fur transform of the input and scale the different fair components differently. So that's one equivalent way to define HS norm. And this S really make a difference because if I choose S being zero, then I treat every frequency exactly the same. But if I choose S positive, that means I care a lot on the higher frequency. And vice versa, if S negative, I care a lot in the lower frequency. So in the end, based on all the End based on all the assumptions we have, we will have an optimal resolution based on where we decide to cut off the inverse operator and realize that epsilon resolution actually depends on alpha plus beta minus s. So the final resolution is a combination of the two spaces. Most of the time we cannot choose alpha very well because that's really given to us as a universe problem, but we may have choice to over beta and s and z. To over beta and s, and that will directly affect the resolution. Okay, so that's one aspect. Second aspect is how can I take advantage of that to improve my stability of the inverse problem. So continuing the earlier linear problem I just mentioned, if I write it down the analytical solution of this m, then m will be more or less in the sense like a little bit like linear, oh, sorry, rated least square formulation. So p here will be. So, P here will be the operator corresponding to which S I use. If I use a positive S, P becomes a differential operator. If I choose a negative S, it becomes an integral operator. And that actually have a lot to do with my reconstruction. Because if my G delta, which is noisy data, has a lot of higher frequency noise, I don't want to have anything that amplifies that. So even what we would know, least square formulation will overfeed the noise. So even if... will overfeed the noise. So even if I use s equal to zero, I already see a lot of noise, not to mention choose s positive. So in these circumstances, it's a lot nice to use negative noise, negative s. So here we have a simple demonstration. I have a denoising example and there's some noise in the data. So when I use L2 norm, I overfeed the noise. My reconstruction is more noisy than my input. This is because no regularization directly fitting will be. Directly fitting will be like that. But if I'm using a negative S, actually, the solution can be regularized. So we don't see the noise effect that much. And one can see even the noise become much more intense. It's more preferable to use an even weaker norm, which means even smaller S. And that reconstruction will be, we will not see the noise in the data that much. So this is what I regard as an implicit regularization. Implicit regularization because those reconstruction didn't even have a regularization term, and the only objective function is the data fitting term. So, you can see directly different ways we choose the objective function really can regularize our solution. And this is purely a cross-section of this image. And then we can see when s equal to zero, we overfeed the noise. While the black dotted line is noise, when s equal to zero, our reconstruction is even more noise. Our reconstruction is even more noisy. Okay, but if we use some weaker norm, we actually can recognize it. But at the same time, the loss is we lose resolution because we decide to ignore the higher frequency, which also means we decide to ignore the higher frequency component of the edges, et cetera. So the edges is gone. Okay, so data was about higher frequency noise, but in some of the applications, we may. But in some of the applications, we may encounter the case that the noise in the data is actually lower frequency. I see noise was a main part during the panel yesterday. So it's possible we have a low frequency noise, but it's relatively rare. So I find one example in reverse time migration in wave inversion. If we just use the image condition, we will get the image like the upper left. And a lot of noise in this image are actually higher lower frequency. And in this scenario, when we do the denoising step, When we do the denoising step, it's more preferable to use stronger norm because stronger norm will focus on the higher frequency and ignore the lower frequency. Therefore, we can kind of remove the noise pretty well and the PSNR actually tell us really how well it does and the bigger the better. So, this is a case that what S2 choose has a lot to do with what noise we have in the data and it needs some. The data and it needs some a priori assumption. So, the Varsha-sen distance was mentioned several times during the workshop from other speakers, too. So, I just want to bring in this understanding that actually the quadratic Rashi-Sen matrix is a weak metric in the sense if we linearize it, it's asymptotically H minus one semi-norm. So, which means in terms of a Hilbert space is in the weaker side of L2. L2. So recently, both the asymptotic connection and the non-asymptotic can illustrate that W2 is kind of equivalent to H minus 1 in terms of fitting. That's why we can see in some of the experiment I had before, even if I have such a strong noise, like the noisy data, the blue one, as you see, that's my created observed data. And I do inversion, full-friendly inversion. Fourier inversion. My simulated data or synthetic data is the black one. Although the black one and the blue one look so far away from each other, we can still reconstruct the Mammoth model pretty well. That's because we are using a metric, data space metric that does not see the difference between the blue and the black, because the metric itself is blind to those high-frequency noise. Therefore, our reconstruction. Therefore, our reconstruction will not be able to see that kind of oscillation. And that's why the reconstruction didn't really get too much affected by such a strong wise. So that's also the effect of choosing different metric space. We can implicitly regularize the inversion. Again, there's no regularization term here, and the regularization effect, robustness to noise, purely come from the objective function we choose. The objective function we choose. Okay, the third aspect is improving the optimization landscape. I think that's something quite straightforward to understand because we choose a different metric space to measure the synthetic data and observed data. Correspondingly, we are going to get a different objective function, and that objective function is directly the one that affects my objective optimization landscape. And a very simple example. And a very simple example is what I think Sue already brings up. You can compare two recur wavelet or two wavelet. Most of the time, they are translation of each other. And measuring translation is not something L2 norm can do. It's very non-local type of change, global transformation. L2 norm is really local. It's pointwise comparison, thinking about its definition. So the L2 norm will give you terrible, terrible optimization. Will give you terrible, terrible optimization landscape, and that's the source of difficulty inversion for quite a long time. But if you simply choose another way of measuring these two, for example, the quadratic rushes and distance, you immediately get a globally convex optimization landscape. So that's just a direct effect that we can actually change the type of optimization problem we are dealing with by changing the objective function. So this is simple. So, this is a simple format in Yung Wandi, but this directly changed actually is reflected in large-scale optimization. So, again, we choose to invert the Mamusi model all the time. On the left is the L2 inversion. The first row are velocities, and the second row are residual. So, residual here is purely the difference between the synthetic data and the observed data. While the right-hand side is While the right-hand side is the W2 inversion. So, for both of the two inversions, we start with the same initial guess. So, it is the same point in the model space. But because we use different objective functions, they actually have different convergence paths. And one converts to Mamusi, one gets stuck at a local minimum. One feeds the data, so the data residue is almost gone, while the other has a lot of the residual that in the last 100 iteration just refused to deny. Just refuse to diminish. That's the sign that we get stuck. Similar things you can also see by choosing different HS norms. So, really, the change of optimization landscape is probably not limited to optimal transport, but the nice thing about optimal transport is the fact that it's global change. And the definition of optimal transport problem matches a lot to the hyperbolic type of inverse problem we are. inverse problem we are well we are solving. So here I have one intuitive explanation. So when we get stuck at the local minimum, usually the gradient is zero. Okay and what is the gradient in our inverse in our inverse problem based optimization? Is the featured derivative with respect to data projected on the feature derivative with the Jacobian with respect to M? So it's really these two. So it's really this two. Most of the time, djdf is never zero because we always use an objective function that is pretty convex, like L2 norm. It's convex in f. It's convex in the data. So dj df usually is never zero. The other thing is df dm. So that's purely from the yield post-mission inverse problem. So most of the time we also put ourselves in a situation that this one shouldn't be zero so that we have uniqueness. If now we If neither of them is zero, when we get stuck, that means the projection of over the Jacobian DFDM is zero. It's purely because the data want to go this way, but my model want to go the other way. And the projection tell me, oh, there's no feasible way of doing that. So, one intuitive way to change that is, why not I change how the JDS goes. I want to modify. Goes. I want to modify DJDF more aligned with my inverse problem. My inverse problem is own natural continuous dependence. And the truth, the choice of using optimal transport in this situation is exactly because the velocity and the wave field dependence are very similar to transporter phenomenon. So if I use optimal transport based metrics, I will also get the transport phenomenon. And this too allows. Phenomenon and these two aligned, and I will not get a zero gradient. So, this is one intuitive understanding why Rashi's dynamic actually plays a role here. And so in this paper, we investigate maybe it's not just limited to weight-based inversion. Any situation that DFDM, which comes from my inverse problem, has this transport feature. I can choose something as the transport feature to align with that. feature to align with that and get convexity and get like continuous convergence until the really the meaning to minimize it. So we list a few examples that is beyond hyperbolic PDE, such as reconstruction from localized sources or even deconvolution from diffusive environment. So this is a direct effect when we choose different data metric space, we will have a difference. We will have a difference, see the difference in the inversion. So, finally, I want to bring up a little bit this change of convergence rate and the trajectory. And it is something recently I would like to combine with the proximal operator from the optimization community, like how they try to understand the gradient flow, etc. Again, this is the universe data matching problem we deal with, translated from the original universe problem. Then there is Then there is the gradient we are calculating from the L iteration to the L plus one iteration is really locally a minimizer of this proximal operator. So, and in this proximal operator, we have two choices. One is our objective function, and another is we want the objective function, but we also want the parameter to go as fast. To go as fast as possible. So we have to have a notion what do we mean in the parameter space, it needs to go as fast as possible. That means you need to have a metric to measure in the model space, what do we mean that by as fast as possible. So therefore, this two metric space actually play a role in terms of my gradient and actually different choice of DM. You will have different gradient flow. That's why you may hear about like H minus. That's why you may hear about like H minus fun gradient flow, Kanheiler, Alan-Khan equation, etc. They actually corresponding to different choice of DM. And here, since our objective function usually is just a data matching, so our objective function also has a lot to do with which metric space we use for the data domain. So both data domain metric space and the model domain metric space contribute to Contribute to my next iteration. So you can consider that if you change one of them, you may highly likely have a very different next iteration. And for nonlinear problem, after 100 iterations, we may have very different convergence result. So recently, there's a notion of natural gradient descent in the sense that you still want to minimize your objective function, but instead of minimizing in the model parameter space to make it model parameter space to make it go as fast as possible. I'm measuring what do I mean by as fast as possible in the data space. So a simple change of this second term is the corresponding gradient descent is called the natural gradient descent. And they will also make a difference in our convergence. So another thing that I think some of us probably know, I just want to emphasize that gradient doesn't Emphasize that gradient doesn't is not equal to the derivative. Gradient is only equal to the derivative when we are in the L2 space because there is a representation back and forth, they are the same. But as long as Dm is not L2, then the gradient will highly depend on which metric space of Dm we are choosing. And as a result, the gradient will be different and we will have a different convergence result. So here I just use very simple gradient flow to Use very simple gradient flow to show as an example. If I choose dm as L2, Dg as L2, then I have this very standard gradient flow for a linear inverse problem. But if I fix DG and the trend dm, I have a 1 minus R passing inverse in front of it. And if I fix Dm to be L2, but the change DG to be H negative 1, I also have a M minus Na Parson in front of it. But the M minus Na Parson show in different places. And this is another way to demonstrate different choice of DMDG. You are changing your gradient flow. And if A and M and Star Passion, they are not commutable, these two are also different. And maybe possibly have different properties. And overall, they will change your convergence speed, change your convergence path. And for non-linear problems, that will highly likely to change where you converge to. To change where you converge to because different dynamics have different baseline of attraction, you may converge to different stationary points. And I actually have an example for that. This is a linear inverse probability least square reverse time migration. I choose a different DM. After the same number of iterations, one result is clearly more close to the truth than the other because now I change my convergence speed. Because now I change my convergence speed. And this is a nonlinear problem. It's a very simple 2D Gaussian mixture model. This is because it's nonlinear, I'm trying to find one of the mean in the Gaussian mixture. And I choose different DG here to compare in the natural gradient framework. The same objective function, the same model parameter DM, but I'm choosing different DG in this component. And L2 natural gradient actually converge to Gradient actually converge to the local minima, while this W2 natural gradient gradually have this convergence path and converge to the local minima. So you can see for linear problem, we may change the convergence speed. And for nonlinear problem, we may highly have a different stationary distribution, sorry, stationary point minimized. So finally, it's time for summary. So for my So, my perspective is to treat an inverse problem as an inverse data matching problem in this optimization framework. And I want to emphasize that these two metric spaces play a huge role in our optimization, in our like really computational inversion result, even without any regularization term. So, the impact I will summarize for this four-part resolution analysis, noise robonis, optimization landscape, and the convergence speed. Optimization landscape and the conversion speed. So, I hope this is something interesting. And I want to thank all the workshop organizers and all my collaborators. And if you have any questions, I will be more than happy to discuss. Thank you very much for this excellent presentation, Vinan. Yvonne has a question. Yeah, you know, it's a very good talk. Thank you so much. I really like this brilliant talk. I really like it. It's a brilliant talk. So, I have a question about page 21. Yeah, so you had in front of Washington, you have a one over two tau. So, what is tau there? Tau is stack size. So, ingredient descent, we have the stack size. Yeah. I see. So, so there's no, yeah, this is really cool. So, you never have the, so instead of choosing the regularization parameter, you choose the S, right? Oh, yeah, you can choose DG. Yeah, you, if you. Choose DG, yeah. You, if you restrict yourself to hurtable space, then you can choose different S. Yeah, so I, yeah, I have one more question. So, is there any way to kind of the S will be relying on the noise in the data, right? If you were motivated in the sense, you want to regularize the noise, then if there's a noise, definitely there's a good benefit of choosing different S. But let's say you just want a faster convergence, you can also choose different S too, because the convergence really is. Because the convergence really is the Leipzig constant of this, and you choose different as you may modify your operator and have a Liebschetz constant close to one. Yeah, then you will be better. Can I ask one more question very quick? Last time when I listened to Bjorn's talk, at that time, with respect to this wave inversion, my impression that I asked him was about, you know, like it will be very, very messy if you want. It will be very, very messy if you want to deal with energy dissipation. If it's not a pure elastic equation, right? If there's energy dissipation, I remember he mentioned in this Washerstein, when you try to solve this Lajumper equation, there is some difficulty. So is there any progress in that direction? You mean computing the Wascherstein metric? Yeah, like suppose I want to do wave inversion, but I have dissipation in the system. Energy is not conserved. I think that's fine because I never have. I think that's fine because I never actually used the fact that energy should be conserved. What we were doing is you have the waveform data. I just compare trace by trace. One time event for a fixed receiver compared to another time event, fixed receiver. You have to normalize that to one? Yes, that's where the energy comes in. Yes. So there is artifact in that case, or there's some difficulty. I think there's some device. Some difficulty. I think there's some development in terms of how they do it. They can map it to a graph space. I see. I see. If you have the reference, send it to me. Yeah, yeah. Great. I will send it. Thank you so much. Okay, is there another question for Yunan?