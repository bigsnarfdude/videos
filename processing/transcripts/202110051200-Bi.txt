What are you working on now, Juan? I'm basically, yeah, I can do a quick introduction of myself, actually. So my research was mainly focused on a recommender system, which was introduced by Patrick and my Debushi early this morning. And now recently, I developed a new research area, which was joint work with Professor Xiao Tong Shen. I guess Professor Shen is also here. I guess Professor Jen is also here. We are doing privacy-related research. So, currently, like both of them are my research interest. And the work that I'm going to introduce today is a work that is kind of an application of recommender system, but to sales forecasting context, which has some context-specific features that we try to incorporate and accommodate in our method. That sounds really nice. And thank you for the short. Really nice. And thank you for the short introduction. And I have two o'clock, so I guess you might as well go ahead and begin. Oh, sure. Thank you, David. Thank you for the invitation. I have both my chat box and my Zoom, the grade view open. So any questions, you can raise your hand to send in the chat or directly interrupt me. I'm clearly fine with either of them. So the work that I'm going to present today is called Improving Sales Forecasting Accuracy. Called Improving Cells Forecasting Accuracy, a Tensor Factorization Approach with Demand Awareness. This is a joint work with my colleague and department chair, Geddas, and William and Annie. William is in Shanghai Jiao Tong University and Annie is in UC Irvine. And yes, many of you may know that sales forecasting has been a long-lasting topic. Even I think before a personal computer was invented, people had been doing sales forecasting, maybe on paper. Doing sales forecasting, maybe on paper by using some linear method, using a ruler, and see whether the trend is going up or down. But why we are still talking about sales forecasting in the current age, in the big data era, because it is so important. A lot of times it is related to financial planning, related to supply chain, related to inventory control. And a lot of things, once you have a good forecast number ahead of time, it can guarantee a good planning of the business. A good planning of the business way ahead of time, way ahead of your competitors. So, this is one of the reasons why sales forecasting has been a long-lasting project. And to be honest, it is very related to people's humans' behavior. So, actually, there's very little way that we can forecast the things very accurately. So, that's why the methods are keeping evolving while a researcher keeps on going. And one thing that is particularly recent, urgent, and imminent is with the Urgent and imminent is with the collection of big data, sales forecasting is also changed a little bit. So, for in the current business mode, a lot of things have been changed. Switching from the old-fashioned type of person-to-person transactions nowadays, a lot of things can be both offline and online. Your competitors may be invisible to you. That's completely possible. And a lot of sales is not directly done by in-person, but it can be across multiple stores and platforms. Stores and platforms. And as a store manager, as a distributor, and as a manufacturer, what to sell is actually the question. And as a customer, consumer, what to buy is your question. So how to make the matching more efficient? How to make the matching more effective is essentially the question that we are trying to ask. Not fully answered, but to some extent. And also not to mention the B2B sales forecasting. And challenges brought by say globalization lead to some industries have to see retail saturation. Received retail saturation, and now during the pandemic, some retail actually see great shortage. That's on both sides, it's actually happening simultaneously. And limitations on production, supply chain inventory, et cetera, are also posing great challenges to this sales forecasting context. Could you give an example of retail saturation? That's a good question. I know in some countries, for example, the computer screen is actually all Screen is actually of saturation because nowadays, due to the advance of the technology, actually producing computer screen is way cheaper than they were. So even though you are making high-end screen, but there are a lot of low-end competitors making the industry kind of to some extent difficult. That's what I heard, not necessarily true after the pandemic, but that's what I heard when I heard a story. Thank you. Sure. And motivation. And the motivation of this work is that advanced systems nowadays allow the sales of many products being tracked and reported. If you ask some people in Walmart, in Best Buy, in Target, they know the sales of every single product every day or even every hour, if not minute-wise. So this allow us to do some, say, very granular type of sales forecasting, even on a very short time period, and is the motivation of keep doing sales forecasting research. Keep doing sales forecasting research. And sales forecasting, to a lot of extent, especially for our work, is not necessarily targeting at end consumers. So, the work that we are proposing is basically for store managers and manufacturers who are doing bulk sales or bulkers. And the situation that we are considering is across a great number of stores and products. For example, if I'm a store manager, I may have a not, but if I have a store manager, I may have tons of stores. Manager, I may have tons of stores, and for each store, they may have thousands or millions of products on my shelves. And what are the ones I'm going to sell next week is actually the thing that I'm considering. And actually, it borrows the concept of online recommender systems, but with unique challenges associated with offline store product matching. So yeah, the goal is quite simple. Research goal as a research paper is to improve forecasting accuracy. So here's a few things that we are leveraging. Few things that we are leveraging. First, by borrowing information from recommender system, borrowing methods from recommender system, we are trying to leverage information from other stores. Similar to recommender system, now we consider each store as a unique user, each product as a unique item as in recommender system. And one contribution is that we try to explicitly formulate sales competition and we try to predict sales of product which has never been sold, which is analogous to the cold star product. Analogous to the co-star problem, but it's not the main focus of this research. And we try to recommend products to stores and stores to product manufacturers: what are going to be the best optimal outlet for your product, and which are the best product for your store managers to sell. And the impact of work is that even 1% of improvement brings huge economic impact, for example, on production, financial planning, as I mentioned earlier, staff allocating, marketing, et cetera. allocating marketing, et cetera. And I believe early in Patrick's talk, he also mentioned a similar thing in terms of how important accuracy actually is. And we actually had last year, we had before the pandemic, I would say, we had a student product that project, sorry, work with a Fortune 500 company, agricultural company, where we try to forecast the region-level milk sales. And when the project is done, we ask our client what's the impact of Our client, what's the impact of having the accuracy improvement of milk region-wise milk sales? And the client says that the result we have is about 1.55% increase in accuracy. And what they say is that that can lead to potentially, I'm not saying this is accurate, it's potentially what they say is lead to three minutes impact, but this is before the pandemic. I don't know whether the forecast we made for them actually becomes useful because of the pandemic. Things have changed a lot. But these just as an example, lot but this just as an example trying to show that uh improving the accuracy sometimes even uh even even even very small number can lead to green and huge impact may i ask a little bit more about the milk example sure i would have thought that milk is a very stably purchased quantity and there wouldn't be very much variation from week to week or month to month in how much milk is being sold at a grocery store. Grocery store. Yeah, go ahead. And so I would think that they would probably be able to measure this very, very well. But is that not the case? You are making a very good point. So before we take over, took over this project, the client themselves have some elementary, well, I won't say the elementary, but some simple statistical models trying to make region-wise, you're precise, this is region-wise muke forecast. This is region-wise MUC forecast. Their accuracy, I probably can't see the number, but as you said, it's quite accurate. But they just try to be more accurate in terms of, you know, taking advantage of it or win their competitors for whatever reasons. It's not 100% accurate, but they want to be more accurate. And on top of their already very large number, we improve it by 1.55%. And thus they say this is going to make this amount of impact. But you see, it's correct. But you see, it's correct. And they are, they were interested in follow-up with a more granular level of research, for example, down to farm level. The client themselves has kind of hesitant about whether this way can lead to a better statistical model because farm level things become more, I would say, stochastic because cows, they are not machines. They do not exactly follow your order. If you say, hey, I need two gallons now. I need two gallons now. They don't necessarily follow what you say. And that's one of the reasons. So, before the pandemic, it was only at the region level. It was planning, they were planning to have it at the farm level, but it was kind of paused at this moment. But that's a good question. Your sense is precise. At the region level, it's quite stable. Thank you. And yeah, so there are some technical challenges. In early today's talk, you already heard about it. So I'm going to be quick in this slide. So, I'm going to be quick in this slide. So, data can be extremely huge. You can imagine we have a very large recommender system and can be extremely sparse at the same time. It's not saying that we have tons of data and the data has to be dense. The user item matrix is always very sparse, regardless of whether you are recommending movies or you are forecasting products. We're always very sparse. For example, there may be millions of milk products nationwide, but down to each grocery store, there may be just tens. Each grocery store, there may be just tens or if not hundreds of products there. So, in our numerical study, what we saw is we have about 1500 stores, over 15,000 products, and weekly data collected from 2008 all the way to 2011. And we have 165 million observed transactions out of 5 billion possible transactions. So, it's hypothetically, every store can theoretically sell every product. So, that's how we got this number. Product, so that's how we got this number. So, a total of 3.3 observation rate. And the thing before ahead of everything is that the algorithm has to be scalable. So there's a unique challenge that is not usually seen in movie recommender system because before this project, we did a lot of movie recommender system projects. We know it. So sales competition actually exists. For example, if I have time, I can go online, say, go to movielens.org. Say, go to movilands.org and rate millions of movies if I do have time. I can read millions of movies, get all of them five stars. That's completely legitimate. But for sales, it's a little bit different. As an end consumer or even as a store manager, your budget, your source, resources are actually very limited. As an end customer, if I buy a car today here at Toyota, I probably don't have a car to buy a Honda tomorrow. So resource is limited. If I choose to go for something, I do. If I choose to go for something, I do not have a resource to go for the other thing, which is the unique challenge and one of the motivations of us doing this work. So, sales of competitive stores have a negative effect, and which is on top of the collaborative nature as in recommender system. In recommender system, as Patrick mentioned, we have collaborative filtering. Similar users may have similar tastes, which is still true in sales forecast. If your store and my store are very similar in essence, and you have very popular. And you have very popular sales of one new product. And I do have confidence in believing that if I introduce that product, I can also perceive a very good sales. But this is on top of the sales of competition, which also exists. So there's pros and cons that have this type of interplay. We cannot tell. And also, not to mention the limitations on product supply chain and inventory, et cetera, which may have some, create some surprise or serendipities on your. Surprise or serendipities on your forecast values, which this part is the thing that we observed, but in our method, we do not deliberately resolve it. So, also, there's new product introduction. There are market carnivalizations. And another thing that is very unique is that we are forecast of future sales. Early in the morning, we discuss the temporal effects. If we measure people's taste or preference as a temporal thing, if you like, like Patrick said. If you like, like Patrick said, if we like an action movie when we were a childhood, but become, we start to like drama when we grow up, this type of change may be very difficult to formulate, but which is essential in sales forecasting because sales, we're not interested in predict what has been sold last year, or if something were introduced, have to be introduced last year, what's going to be the potential sold, the potential sales last year. We are interested only in future sales. So essentially, from Future sales. So essentially, from the temporal aspect, everything is co-start because every day is a new day, every week is a new week. So latent factors for new temporal, new temporal index, I would say, are not available because of this reason. So these are basically the two unique challenges that motivate this work. Any questions so far? Oh, what exactly is market cannibalization? Well, that's a good question. So if I'm, say, if I have a So, if I'm say if Apple has now recently Apple has their iPhone 13 released, then you would expect the Apple iPhone 12 sales will go down because of the release of the iPhone. Perfect example. Thank you. Yeah, it's basically the replacement or the competition because of the new products introduced by your own company. So, it's still part of the sales competition. And there are tons of related literatures which has. Tons of related literatures, which has been introduced early in the morning, so I won't replicate them all. So, latent factor modeling is definitely one of them, a very important one. Tensor decomposition is also one line of great research in this area. And also, very recently, deep learning is also kicking and playing a very important role. So, I definitely believe there are many deep learning experts sitting here. So, I won't talk much about it. And the proposal method actually. Method actually is a tensor-based method, and we try to slightly borrow things from deep learning, not methodologically, but as application-wise, to see if it can improve the accuracy. And like I said, the contribution is to incorporate sales competition. We basically at the beginning, we only enforce negative correlation, saying that local stores and products have competition. So it's natural to believe the correlation between the sales in the competitive stores are or products. Competitive stores are all products are negative. But later on, based on the reviewers' comments, which is great to us, they say that there could be some type of strategic alliance or type of other things leading to the correlation could be positive. But methodologically, they are similar. So you will see later. But negative or positive correlation among legend factors of local stores and products is basically the key contribution of this work. And one example, as I mentioned earlier, is that if a consumer If it's that if a consumer interests in borrowing, sorry, in purchasing a bottle of Pepsi, perhaps he or she will not purchase another bottle of Coke because of the budget constraint. Another additional advantage is that we forecast via time series model neural networks to extrapolate current latent factors into future time points, which is the key challenge of doing sales forecasting under this matrix or tensor factorization framework and the possibility for new product introduction. Possibility for new product infrastructure. So, tensor actually is a high-dimensional array. Sorry, Sean, I won't be an example of something you said, like an alliance that leads to positive correlation. Well, that's one of the reviewers' comments. I think what they say is that, say, if I am a manager of a chain of stores, then if I have some promotion strategies which will Strategies which will be applied to all of my stores simultaneously rather than just on one store. I could imagine that there might be a positive association between products that are purchased. If I buy a pen, I'm also likely to buy a pad of paper. Yes, if among products, then yes. If among stores, then the example I provided to Patrick is the one that I have in mind. I'm not sure how practically this can be, but how practically this can be implemented. But how practically this can be implemented, but ideally, conceptually, this is the case that I can come up with at this moment. Okay. Okay, thanks. Yeah, because my mind went to like a manager at a Walmart and a manager at a Target colluding to somehow increase sales of both stores. And I was struggling. That was possible, I would say. That is possible. If that's true, then yes, this can also be framed into this framework. You will see this positive number, negative number methodologically. Number negative member methodologically, no difference, but just conceptually, how to accept it. I think I could provide another example. Like you can think of a shopping mall, and one store lowers the price, then more people come to the shopping mall and just buy other stuff in other stores. So then you will be like positively correlated, although they might be selling furniture and clothing. So that's a very common setting. That's a nice example, Ron. Thank you, Ron. That's a good example. Yeah, so a tensor basically is a multi-dimensional array. Is a multi-dimensional array. So, if we were in yesterday in Anrus talk, basically, he already mentioned what this tensor is. So, in our case, specifically, we have store product and time as three modes of the tensor. This is the way we define this. It's also based on the data we have. So, maybe if we have distributor-level data or have manufacturer-level data, then the definition of the tensor may be slightly different. But based on the data type we have, basically, we are doing. Type we have basically, we are doing store product and time. And as you can see, by generalizing matrix factorization, we are trying to impute what is the sales value of product in certain store in some future time point is the goal of the research. But we are considering between store competition and between product competition. And there are two things that basically we can consider: one is the unit sales, another is the sales amount. So, this actually we struggled. So this actually we struggled a lot on this topic. So at the very beginning, we are doing a uni-cell forecasting because we believe this might be more interesting to people and which is also robust against inflation for some sort of reason. But later on, we figure out there's the data cleaning actually encounter a lot of problems because unit can be very, you know, homo, not homogeneous, heterogeneous. For example, a six-pack is also count when you. Six pack is also count one unit, but one bottle of beer is also count one unit, but they are having different item number. But essentially, you should consider a six box as six units of consider one bottle of beer because the price is six times. So essentially, we have to switch to a dollar side, to dollar sales, which is okay, but within the three years of period, it didn't show any inflation or things that can be quite affected, but but we're Affected, but we're aware of this issue. So, and the framework that we are borrowing is CP decomposition, which is one of the two commonly seen decomposition methods in tensor. Basically, we are decomposing a tensor into a summation of rank one tensors. And we have discussed about it early in the morning, we discussed about the rank, how it is defined, and how this can be low rank, but we are not discussing this issue. But we are not discussing this issue here. So just assume that the K is pre-specified. If you're not satisfied with the K specified, you can try another K and see if the forecasting accuracy can be improved on your validation set. So CPD competition is imposed. And here is a formulation of sales composition in our method. Suppose in a given region, we have MG competing stores, and our goal is to design. And our goal is to design a penalty function such that it impose negative correlations among latent factors of these stores. So the latent factors are hypothetically provided by the tensor factorization. And that is we have a target correlation matrix in mind. And this is one thing that we're going to improve in the next paper. But so far, Fistori put up with me that for each values of this target correlation matrix, we do have a number in mind. For example, we can specify that. Can specify that between Target and Walmart within this region, this is an active 0.5 or something like that. We have a number in mind that this is a severity of competition within the store. And usually in our real data analysis, a region usually only have two to three stores. So the region we define is not a state or half a state that size. It's a very specific region, the first three digits of the zip code, which is a very small region, within which usually there's not. Region, which in which usually there's not too many stores. I remember the largest number is only six. So the matrix is always very small. But there have some natural constraints because this is a correlation matrix by design. So it has to be a positive semi-definite. So the number we specify cannot be too large, cannot have a very super high magnitude. And then the next goal is we want to design a penalization function such that the estimated That the estimated correlation provided by the latent factors, we want those estimated latent factors or estimated correlation vectors to be as close to the true one we designed in the previous page as possible. That is our goal. So we have a design correlation matrix in mind first, and then we want to want the tensor decomposition's latent factor to follow that correlation matrix as much as possible by using a penalization function. A penalization function, and this is the estimated correlation function based on the latent factors. So, based on eventually, we are doing this. This is the minimized loss function is actually the tensor CP decomposition, but subject to the correlation matrix of each region being close to the designed correlation matrix as close as possible. And then we figure out there's some question, there's some issues there. This is because, oh, sorry, I should stay in this slide. Should stay in this slide for one more second. So, we figure out there's an issue because the off-diagonal matrix of the design, the correlation matrix, are non-zero. So, we're essentially shrinking something to a non-zero value, non-zero constant, which is not necessarily the Lascelle format or any of the other format. And it's essentially very challenging. So, what we propose to do is not directly solving this problem, but equivalently considering using the single. Using the singular value decomposition of the design matrix, multiply that to the estimated correlation matrix, such that equivalently we can solve this problem. Instead of directly calculating the difference between the estimated one and the design one, we are now calculating the inverse of, sorry, the inverse design correlation matrix multiply the estimated one and the identity matrix such that the identity matrix has zeros in all of its off-diagonal values. In all of its off-diagonal values, such that the estimation actually is shrinking towards zero rather than towards some negative, non-sorry, non-positive constant number. And this is the correlation, how it is defined. This is the negative half of the original design matrix. And we are shrinking, essentially shrinking every single element of the diagonal matrix into, sorry, the of diagonal elements into zero. And a path. And a penalty function is actually a summation of all of these penalties for every single pair within every region. So essentially, what we are doing is the penalty function can be, sorry, is to define the penalty function in this way. As long as J is not equal to K, this is the two stores within, or two products within the same region, we are shrinking the difference towards zero. And we're summing a summation over all the pairs of stores within that region. And further, we have. Within that region, and further, we have to summation over all the regions. And this actually has a very beautiful quadratic term, such that we can solve it by using alternating least square or whatever methods that can be fit. So essentially, yeah, this is more specifically what each of these terms looks like. And these are the A's and B's column of the design matrix. And this is the sign of this whole thing. So I'm going to be quick on this technical detail. Be quick on this technical details, but essentially, it's sorry, it is still the same among products. Products, we do not have regions, but we have a category of the product. For example, these are milks, these are coffees. So, we believe that within each category, the products has essentially such correlation or positive correlation or competition. And then, yes, the overall criterion function can be written in this way: this is the tensor decomposition loss function, these are the regular. These are the regular L2 penalty, and these two are the design penalty function we just mentioned. One is for the stores, the other is for products, and the summation is over all the regions. And within regions, remember that we sum over each pair of stores or pairs of products. And yeah, any questions so far? This is the formula of our proposed work. It sounds like It sounds like you're going to do very large sums because you've got you're summing over all the different products. Yes, that's true. So, but essentially, we are because of the definition of the region, essentially, if we do not have this penalty in a regular tensor factorization, we can do the estimation of the latent factor store by store or product by products by because the estimation of one product's latent factor is not associated with the estimation of the other one. So, we can do it parallel. So we can do it in parallel or the distributed computing. But here, the individual level is no longer each product with each store. It becomes each region. That's another reason why we choose to use a very small region. One is to avoid very large correlation matrix. Another is for parallel computing to be easily implemented. So, yes, but you are right. Thank you. Sure. And then. And then we can use block-wise coordinate descent algorithm to estimate each of these latent factors. And yeah, then the next question, which is also mentioned earlier, is also a challenging question is how do we forecast for future values? So by completing this tensor, we do have the hypothesis sales of each product in each sale, but in the past years or weeks, but we're interested in knowing what are the future values. Some existing literature is showing that, okay, once the Literature showing that okay, once the tensor is completed, one thing you can do is you can, for those pairs that you are interested, for example, if you're interested in ski board in this Walmart, you can just do a time series forecasting of this particular pair of stored products, product A, store B, and trying because all the past values are imputed, you can do a linear, say, ARIMA or LSTM to forecast what's the future sales. But as a recommender system, if we're interested in System, if we're interested in knowing every pair of future sales, then this type of way will no longer work. So, at least in our example, if we have thousands of stores, we have millions, sorry, tens of thousands of products, then a simple multiplication will end up with, say, how many of them? Maybe nearly a billion of store product pairs. And if you want to know every single of them's future value, this could potentially be a problem. And this is one of the benchmarks that we compute. And this is one of the benchmarks that we compared with because of the to see the computational efficiency of the work. So instead of directly extrapolating the sales of the imputed values, we try to sorry, we try to extrapolate the latent factors. Because the tensor decomposition for each of the past time index, we now have a latent factor associated with that index. We can actually do a forecasting method, say ARIMA or RSTM to extrapolate the future. To extrapolate the future of the latent factors, per se. And then once the forecasting is done, so if for each dimension, if the latent factor is Wt T from small, sorry, small t from one to capital T, then we can extrapolate W capital T plus one, capital T plus two, etc., and to capital T plus H, if H is the desired horizon of forecast. Then the forecast value can be provided by using the store-specific latent factor, product-specific latent factor. Product-specific latent factor, but now we are using the extended extrapolated latent factor provided for the future time points. And these end up with the forecast value. So the user-specific latent factor and product-specific latent factors are time-invariant. So those values are fixed. But the time-dependent, sorry, the temporal latent factors, we do not use those directly provided by the proposed method in the previous slide. We provide the extrapolated, we use the extrapolated value. We use the extrapolated values, and then these provide the forecast value for each pair of store products future sale. And we can write this whole thing into a one-step procedure. And this is basically the overarching criterion function we have, the loss function, the competition penalty, which we proposed. We proposed, and the regular L2 penalty that is common in tensor deformation. And finally, it's the time series forecasting. Can you imagine this can be a RIMA forecasting? Then these are the AR and MA coefficients. But if this is the LSTM, then these are the weights and the bias vectors or matrix associated with the RSTM forecasting. And you can throw all this in Python in Keras and they can give you an estimation of the final result. Estimation of the final result. And yeah, any questions so far? This is the, I think this is the last slide of my model formulation. Have you tried using LSTM on this? Yeah, you will see the results shortly. Okay, thank you. Yeah, so a quick summary of the method, just in case I didn't make anything clear. So first we do tensor decomposition to get store specific latent factor, product specific latent factor. Factor, product-specific fiddle factor, and time-specific evidence factor. But instead of directly calculating the inner product for all three, we first extrapolate the time-specific VIDEN factor to future time points through using a RIMA or LSTM. And then we do inner product using this to replace the original one, such that we can get and observe the product sales forecast. And numerically, we are doing IRI marketing data analysis. So they generously provide the data. They generously provide the data for research purposes for us without charge, and basically, we have 165 million observations of weekly store sales in dollar. That for the this is the reason we mentioned earlier why we are doing it in dollar rather than in unit. And we have 1500 grocery stores from almost all states in the United States. And the product category across displays, coffee, deodorant, diaper, frozen pizza, etc. Basically, the first seven. Basically, the first seven are still very popular. Sorry, all the eight except photography are basically very popular. Photography in the grocery store is seeing actually a decreasing trend. So the sample size for this category is particularly small, even back in 2008 and 2011. So we are doing at a weekly level. So basically, we are having 208 weeks in total. And some examples of sales competition. One question. Comp sales competition. One question that raised by some early audience in other conferences that hey, you see muke sales or other sales are quite stable. How do you say observe competition exists? Well, we all know that competition exists, but how do we observe that? This is actually a very challenging question for us at that moment. So we find some special examples where both regions are in the New England area that only have two grocery stores. The reasons we propose, sorry. The reasons we propose, sorry, we present this slide is just to show you how what is actually sales competition, how to illustrate it. Because if you challenge me, why do you show a region with more than two stores or products? And my answer is that is possible, but that will obfuscate the actual sales competition. So, this is the reason I only illustrate regions with two possible stores. And you will see for this, for the left panel, the milk cells in one store. The milk cells in one store actually go up once the other store actually was closed. And this is one of the possible evidence of accommodation. For the other example, we see the milk sales actually decrease once the opening of another new competitors. So there are some other similar patterns in other regions and for other products. And when a region or when a product category has more than two sources or products, basically the Or products basically the such evidence is becomes less evident. So, and not to mention, we do have hidden competitors, for example, those stores not collected by RRI and online retailers, these are not considered. And we understand this can be extra-exogenous, but yeah, it's not considered in this example. So, the method is compared with many existing methods at that moment when we develop the method. First, we compare with individual. First, we compare with individual ARIMA model for every store's product pairs. One way is to see: okay, if there's no sales, we code the sale as zero, or some people say, hey, you shouldn't code it as zero, you should code it as not available, either way. And we consider vector L2 regression. We consider LSTM for each store along. And we also consider three latent factor models. So BPTF is a very good latent factor model. Also, probability tensor factorization. probability tensor factorization, generalization of probability matrix factorization early introduced by Patrick. And we consider factorization machine, we consider temporal regress the matrix factorization for each store. And most importantly, we consider the vanilla version of the tensor CP decomposition. By seeing that, I mean, this is exactly the same tensor decomposition as we use in our method, but without sales competition. So we want to use this method as the most This method as the most relevant benchmark to see how the proposed sales competition, the regularized sales competition, can have influence on the result. So we can keep an eye close around this one. So this is actually the result. So the first six columns are the existing methods. Some of them are not well, are not deliberately designed for this large scale store product sales. Store product sales problem. So we will see some methods actually have the forecast values very bad. This is not because we deliberately make them bad. It's because this design, the setup is not favorable for them. So we try to put up with them. And this is the CP decompositions result. And this is the proposed method. We call it ATLAS. The result A means using ARIMA and L means using LSTM. So we use the tensor decomposition part is the Use the tensor decomposition part is the same, but forecasting-wise, we choose to try two different time series models. And you will see that, regardless of which one being the best, they consistently performs better than CP decomposition as well as those existing methods. And computationally, the proposed method and some other tensor or matrix space, the decomposition method actually have the linear computational complexity in terms of number of source products and number of weeks. A number of weeks, but some method actually was not able to take advantage of this information. For example, vanilla ARIMA actually has to do sales computation, sorry, sorry, time series forecasting for each sort product pairs, which lead to a higher computational complexity. For our case, it runs dates, but for these methods, usually within one to two hours. We do have other robustness check results. For example, reviewers ask us to check what if you have. To check what if you have a shorter training size, what if you have a longer testing size, or what if you combine multiple categories into the same category, et cetera. We tried all those combinations and the result is consistently good. So I don't have those in our slide, but you'll be interested, I can show you our paper. But trust me, the result is okay. Computationally, our method is not the fastest. I acknowledge that. At least it's slower than CPD for many cases because of the imposed. Cases because of the imposed sales computation formulation that actually requires additional time calculation. But compared with existing method that had higher theoretical complexity, our method is actually significantly faster, especially similar speed as CBD or TRMF, but significantly faster than those existing methods, which do not take care of the tensor matrix structure. All right, that's All right, that's everything we have. So we propose a large-scale product sales forecasting. Again, it's not for a single store product pair, it's for large scale. So we cannot make sure that every single store product pair has a forecast accuracy very high. But overall, it's average on average, it is a very good result. Sorry, contribution-wise, we propose a novel temporalism factor model, which particularly incorporates sales competition. Is sales competition. And we show competitive numerical performance on 165 million product sales data. And the application is that these provide insight and store level future product sales and design support for new product introduction strategy. So yeah, that's all the reference. And thank you very much for the opportunity to present at Burst Workshop. Thank you so much. That was a wonderful talk. And you certainly managed to cover a lot of ground. We certainly managed to cover a lot of ground. Thank you, David. So far, we've been looking at fairly simple tensors with three or maybe four dimensions. Do you think this type of thing might scale up to even larger tensors? Oh, that's a good question. So the order of the tensor is not like the dimension of the tensor. Dimension tensor. Dimension of the tensor. Dimension tensor of means if you have more and more stores, then basically computational complexity grows. I would say linearly, if not, well, maybe n times log n at most. But as you say, this is a very good question. If your order of the tensor grows, actually that impose very huge challenge to the problem. For example, we are, my collaborator and I have another Journal of Machine Learning research paper actually considering fourth-order tensor, the fourth order. Fourth order tensor, the fourth order is actually the promotion strategy, providing that for the same store and product in the same time point, what if we impose different promotion strategy? At that research, we actually have 30 promotion strategies, the combination of price reduction and display, et cetera, a total of 30 promotion strategies. But that was already competitionally challenging. If you can consider a fifth or sixth even higher order. Or six, even higher order tensor, I believe it could be a problem because you have to consider every single combination and how.