For the introduction, I'm Jabel. I'm a PhD student from Oxford going the final year, and this is joint work with Harold Dover, Hauser, and Zoe, and SAPO. So for the introduction, I just want to really quickly sketch the motivation and what this talk is not going to be about. So just to mention that in supervised learning, we are concerned with finding good representations of the data or which linear functions. Data, or which linear functions can approximate input-output maps. So, I'm not going to talk about, I'm going to ask everybody else that why the signature is a good feature map for learning from sequential data and modeling functions of sequences. And I'm also not going to talk too much about the scalability issues. So, I'm going to assume that people in the applications are mostly aware that there are two kinds of There are two kinds of formulations of signature-based algorithms. One is based on computing signature features that are tensors and therefore have this exponential growing number of feature dimensions with respect to the truncation level of the signature. And the other viewpoint is the kernel-based viewpoint, where kernels with by a kernel trick By a kernel trick, it is possible to avoid tensor computations, but we can get a different complexity, which is comparing all observations between two sequences parallels. The sole X is actually not qualified complexity. So we are not going to try to design scalable signature algorithms or algorithms that approximate a signature, which do not suffer. Signature, which does not suffer from either of these issues, or less so, while retaining the age of the properties. So, just to start with a recap, I'm mostly going to work it in this period over sequences of bounded variation of arbitrary length. So, that's the bounding variation norm, and I'm usually going to assume that we have a I'm usually going to assume that we have a uniform bound B. But also, theoretically, we often think about beta in continuous time, so we can obviously define bounded variation paths, which are continuous on some finite interval and have bounded variation. So for a sequence, I'm just going to call the one variation norm the summing norms of the increments. Norms of the increments and for a path, just the supremum of overall discretizations. So, this is the data space that we are working with in machine learning now. And signature features are, as we all know, just iterated integrals of this form. So, since I'm working with one integration part, I'm just going to assume this is a Riemann integral. If we collect all such tensors events at the right integrals, then we get features that live in the sort of tensor algebra. And in practice, we often truncate the signature at a given level, and then we get features up to given tensor degree m. So the default is to truncate tensor algebra. So that's the features thing that we are working over. So just to quickly say a few things. So just to quickly say a few words about signature kernels, now if you have two level n signatures or sequence or paths x and y, then the signature curve just can be evaluated by integrating the inner products of the increments of the path. So this gets around the tensor issue, but from this we see that here we have every increment. Here we have every increment compared against every other increment since these integration domains are independent. So this means this will scale quadratically in the sequence lines or the path lines. So for the truncated signature kernel, we can also take X and Y, not just in RT, or we could take some We could take some reproducing kernel Hilbert space. So, what we do in practice is we take a pound, we lift it into a reproducing kernel Hilbert space. So, here I'm assuming that I think there were some talks about kernels before, so I'm also not going to talk too much about that. And I'm just going to assume that you know why kernels are cool. Yeah, so here we can have X and Y B and R. Actions might be an error-producing kernel in Burt's space. And another point that I'd like to mention is there's a different kind of algorithm which computes the signature kernel not by a dynamic programming approach, but by a PDA discretization approach by our friends. So now just maybe a quick handle at the end of the day. Just maybe a quick add mention that the signature features I'm working with is maybe not the classical signature features, but there is actually some other choices to discretize these integrals. So Kira and Oberhaus were introduced the order D discretization, in which essentially this sub summation over sub-sequences will be sum over all sub-sequence of a sequence I1 to Im. 1 to Im and this I denote by this hash mark function essentially a bin count which says I have indices I1 to Im I count we have the unique elements and I count the frequencies of each of the unique elements and then this is just normalized by the factorials of the of these counts and the order pre-discrimination says that if Says that if some index appears more than p times, then we just ignore that. So for p colesm, this is just a classical signature of a p-stargin or path. But for example, we could take different p, so for p equals one, then we essentially reduce to summing over six subsequences of a sequence. See subsequence of a sequence with non-requistic elements. So, what is actually also makes sense to do because even if we do this, there is a concatenation property that we know from the signature. And for example, for P plus one, there is a quasi-shut half-shackle product. So, this is how us to get universality even over signature features, defined this way. So, I'm just going to say we focus on it because it P equals one case for simplicity of computations in what follows. Right, so we have signature kernels and now another ingredient is random Fourier features that people have used to approximate stationary continuous and bounded kernels. kernels. So the given idea is that if we are given a static kernel, cycle a static kernel, which is a kernel over the state space of a path. So in this case today is to be Rd. And we have the reproducing kernel space associated to it. Then it is well known there exist many feature maps in the some field word space that it's um isometrically isomorphic in reflecting from a fieldwork space or um Or if we cover the kernel function, for example, we can just take the phi to be the reproducing kernel map, or we could take the eigenfunctions of the associated integral operator, which we know from exists, but this is not what we are going to do. We actually want to find a dimensional map which will be random. Which will be random in some sense. So, this will be a random approximation to see which way in the vein of so yeah, this will be working with random approximations. Yeah, so for us, the question is: how to find such an approximate feature map? So, previous work has introduced Introduced random Fourier features for stationary kernels using Boefner's theorem. So, this is a classic paper in kernel learning and which a few years ago on the test of time already. So, it's a pretty popular approach. So, which says, Bochmer's theorem says in this context that if Kappa is a continuous bounded stationary kernel, then it can be referenced as the Fourier transform of a non-mega finite measure. Measure. So now we have lots of general idea. We are just going to assume that COPO is rescaled such that lambda is a probability measure, just for simplicity. And then essentially we can write Koppo as an inner product in an L2 space of random variables. So it's an expectation of e to i times omega transpose x e to i. transpose x e to the i times transpose y g8. Yeah, so this is the representation of kernel that we are going to use. So how do we use this point of view to define a finite dimensional approximation which is on average a good approximation to the kernel. So what we can do is we can draw random samples from this From this lambda measure, since we assumed that it's a probability measure. So if we do this, then we essentially end up with so basically we assume that the kernel is real and then the imaginary part just integrates out zero. So we are just going to end up with cosine cosine omega transpose x y where omega is a sample from lambda and then just using standard identities we can rewrite this as an intercollegiate features sine w transpose x times cosine w transpose x. So just to give a little bit of context, if we consider the linear case where we linear case where we people often use as an expected isometry for the Euclidean kernel Johnson linear Shrell's map. So for example if you take W to be a normal random matrix then W transposed X is an expected approximate deserving geometric and R D. But if we push this through a sine and a cosine, then it actually becomes the Approximation of the Gaussian girl. So I think that's quite cool. Yeah. So now, but we don't have superficial synchron at this point since often it chooses the kernel even with the task. Another interesting point is that just by DCP, this approach can also be used to approximate the derivatives of RFFs, which could be These are RFFs, which will be important for us later on. So that's my ingredient, and the theoretical results that were given for this kernel is essentially that if you take the supreme of the error, so Kappa Fielder is now a random kernel defined in a previous way, and Kappa is the original kernel, which is infinite-dimensional. Yeah, so we have soup here and we have its larger equal Silence, so that's missing. Of course, now we are talking, we want to pump the error, the probability error is bigger than some value. So we have this largely equal epsilon. And that's more than equal than this quantity over here. So we have epsilon appear here, here, and here. And we have And we have d the other, which is the dimension of the random kernel. So, essentially, what we see here is that with respect to dimension, we have exponentially fast decay of the error. So, it works very well, and you need from the overdata. So, yeah, this one shows that if we increase the If we increase the random dimension, then the error converges to zero at a rate in probability of square root log b filter over d filter. And this case was further tightened in a series of follow-up works to one of shared convergence with respect to one over square root of B filter, and this was also extended. Bit builder, and this was also extended to the drivers of RFS. So it's pretty well investigated theoretically, is what I'm trying to say. So we have now both ingredient signature kernels and an ongoing features. So now we are just actually do a pretty simple trick to put these two together. So now I'm going to define a random kernel for a signature that approximates For a signature that approximates the inner collective of signature features. So we do not care about approximating the signature features themselves. We just want the kernel to be approximately an isometry. So the naive approach would be to just lift the sequence, where we have a sequence x, and just lift it observation-wise using this random feature map. This random feature map. But unfortunately, this would not give an unbiased approximation of the signature kernel because of this of the sum over products of such as this, because the expectation would not factorize over these programs. So the workaround that we used for this is quite simple actually, but it's quite powerful as we see. So for each of these terms in the product, in the summation, we use a different term. In the summation, we use a different kernel for a feature kernel. So the point is that we choose this to be independent. So this would be replaced by Kappa 1 fielder and this would be replaced by Kappa N fielder. So it's quite simple hack really, but it if you do this we can prove essentially a lot of things. So um okay. So just some theoretical guarantees because this is a best approximation of a signature kernel and under some assumptions and control such that it doesn't distort the distances too much, then we have the following uniform control over bounded variation sequences. So if this is the suping of the error of this bounded variation sequence space, then we have the following quantity that controls the error. Following quantity that controls the error. So, just to quickly go over it, B is the maximum variation. C is this distortion rate. So, the Leach is function of the curl. The W's are the norms of the random matrices. And now we see that actually the rate at which the error of the signature curl is actually not just controlled by the error of the random free. Error of the random 3A features, but by the secondary value of the random 3A feature kernel. So this partial one, which just denotes the essentially the hash and of the signal of the hash of the error mapped with the supreme error of the random 48 features. So this is why it was quite useful for us that we had previous work studying and following the convergence of only the promotions of the RFFs but also the very links of RFFs. So now we can assume the following three conditions. We assume that topo is three times differentiable. We assume that it has a finite third moment of this form. And we also assume that it obeys a burn-time formulation of this form. So we have the expectation of w i is just the of W i is just the i coordinate of the random based matrix sample from lambda that is j coordinate and then the expectation of the product of B is a must of weight is inequality and and then essentially we have the following approximation bound over the Reynolds initiative kernel so so this there should be that two So there are two cases to consider. This is the first case where the error is small. When the error is small, then we essentially have the same rate of convergence as for the dragon was left silent, but if the error is big, then it has much heavier tails. So this is actually not surprising because if we have for lack of many random variables, then On variables, then the product will converge much slower as the individual ones. So we have this fstrance, so two over one factor here. So I'm pretty sure that this can be tightened because I was messaging these plans for quite a long time, but I managed to fit it into one line, so I'm quite proud of myself. So I guess some interesting observations is that as the As the disconstant beta here, and this is epsilon, which depends on the variation of the sequences, at the least disconstant of the static kernel and the dimension of the input space or the trace of the spectral measure. So, yeah, I guess as you would expect, if the maximal possible norm of the sequence is get long. possible norm of the sequences get longer, then the error can also get bigger. Or if the kernel distorts the distances a lot, then the error can also be big. So is that point intuitive? But I guess an important point is that it doesn't depend on the number of steps in the sequence. So we could have a it it means that essentially we can re-sample sequences as we see fit, we can resample them and we would still have the same convergent state over a kernel. Same converted state over a kernel. So this scalar V replaces the role of the sequence, the number of steps essentially. So this is the question. Just within, we have about four minutes including questions. Okay, FSS perfect actually. So just this is just some very basic experiments that we've run I think a long time ago because it's usually like Because it's usually like the iteration is like have an idea, then see if it works well on some initial experiments. And then if it works, then prove something. And now that we could prove something, we go back and do even more experiments, I guess. So these are just some very initial results. And so these are time-series classification data sets, and we compared these results from previous work. So this was just more like a sanity check, that then we can confuse all kinds of signatures. Compute all kinds of signature kernels, signature features, signature PD kernels, time-line programming signature kernels, then to see how the performance compares. And we could see that it's well the drop-in performance is not too bad in some cases. Even in some cases, it improves. So this could be, for example, due to a regularization effect or something like that, which is very promising. Which is very promising. So now we are working on potentially making bigger experiments where we couldn't use any of the previous variations. Obviously, this will do with more interesting ones. Yeah, I guess that's it. So just to conclude, so we introduce a ski-level approximation to the trunk initi signature kernel, pre-composed with a continuous bounded stationary kernel. continuous bounded stationary kernel kappa and this could provide a non-biased approximation that converts the improbability to the true kernel. So this was the summary and then the next steps we plan to do more evaluation on and pre-validation on large-scale data sets. Other possible evaluations could be to tighten the bounds using a tighter bounce from follow-up works on the optimal RFF rates. Also, to consider banding or estimating the performance of downstream learning tasks. So, meanwhile, the kernel is close in probably to the true kernel, but it doesn't mean that the downstream learning tasks such as kernel regression or Gaussian process regression are also close to their solution. So, that's an interesting direction that could be using techniques of Uh techniques of um of some um of works on this topic, and then next advance the order of these features. Um thanks for the attention, I think that was all I wanted to say.