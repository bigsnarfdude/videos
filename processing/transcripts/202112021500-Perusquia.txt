For that kind introduction. And of course, I want to thank the organizers for inviting me to present this talk that is actually part of my PhD. And of course, this is a joint work with my PhD supervisors, Jim Griffin and Christiana Villier. And as you can see from the title, it's going to be kind of like a review. kind of like a review. So as you might guess, it's a little bit like the most non-technical part of my PhD, which fortunately enough, we were able to produce a nice paper review on Bayesian anomaly detection applied to computer systems. In particular, we are going to be talking mostly about a specific kind of anomalies obviously applied to computer systems. Computer systems. So, obviously, we're going to be talking about computer systems. And when we talk about computer systems, we need to think a little bit more about what they actually mean. Of course, we could think that it's only our our personal computers, the smartphones, the smartwatches that make our life easier by by tracking, for example, our health. For example, our health, or when we exercise or by doing our research, when we think about computer systems, we need to think a little bit more also on the direction of this new era of the Internet of Things, for example, where we have all these new smart devices that we can control with our cell phone. We have, for example, voice controllers, we have the smart locks, we have the smart logs, everything that can connect to the internet. Everything that can connect to the internet and we can control in our home is part of this kind of internet of things approach to these smart homes and the smart houses. What is also a little bit more, when we think about computer systems, we also need to think a little bit about cloud computing and servers, for example, the AWS, which basically allows people to control and To control and manage their business from home, everything that we do online, everything that every application, or at least most of them, are nowadays like based on these cloud computing technologies. And obviously, because we're talking about a little bit of this, we also need to think about the applications, daily live applications like for entertainment, like Netflix, or if we want to do like an online shopping, we go to Amazon or, for example, this wonderful event. For example, this wonderful event is hosted in Zoom. All of these are involved in all of these computer systems kind of framework. And because everything is connected, we also need to think a little bit on how they connect, right? And how they are communicating with each other. And in this direction, we have like these computer networks. And of course, one of them, like the most important of them, is like the internet because it allows The internet because it allows us to basically do everything nowadays. So I think it's safe to say that when we use these computer systems in a good way, life has become easier in certain aspects. As I was saying, we can track our health, we can connect with people that has been like amazing in these COVID times. COVID times, we can also have or attend online events like this one. Everything is easier and everything is a little bit better when these devices are used appropriately. However, and here comes like always here comes about these systems because they are created by humans, of course they are not perfect. And these systems are of course vulnerable to many things. things for example they are prone to humane errors in like in 2017 that a misty command made the aws services basically down for almost like five hours and in that case it cost the largest companies in the u.s around one hundred million dollars and and it's just only taking into account like large companies into account like large companies because obviously small people like a small businessman or like people doing like owning like small small businesses that run obviously everything on the AWS services of course lost lots of money in recent like in a more recent example we also have the in October we have like this Facebook this that Facebook was down like and all the Facebook was down, like, and all the services of Facebook were down for almost seven hours. And these were due to an engineer, like doing that routine maintenance, right? So obviously, this kind of anomalies are something that unfortunately we cannot do anything about. And no one can know that some of these might happen or will happen because obviously it's like a human error. However, when we talk about vulnerabilities and computer systems, About vulnerabilities and computer systems, we also need to think a little bit of something else and a little bit more scarier. And it has to do with cyber attacks. For example, one of the last ones, like really, really famous one, was in 2017 with this one accry virus that amongst all the companies and business, it collapsed the entire national health system of the UK. Of the UK. I mean, there's like in the Imperial, they made like a report on the cost of this, and it was about six million pounds that it cost. And of course, it's not only about like the money, right? For example, in this, when the national health system was collapsed, many people had to cancel their appointments, including major surgeries and cancer patients. And of course, it's Of course, it's not like just like something like it's not only about money, these kinds of issues. And this is basically what we're going to be centering our attention. So we're going to be talking mostly about cyber security. And from this collage, you can see that cybersecurity actually kind of englos like lots of things. It has to do not only with like virus. Like virus and cyber threats, it has also to do with people and it has to do with companies and it has to do with governments as well because of the breaches and because of the, for example, the weak leaks. And I mean, it's like a really, really big thing right now. So in a kind of like a broad sense, cybersecurity can be defined as the set of tasks and procedures to defend computer systems from malicious threats. Systems from malicious threats. And of course, because we know that computer systems are like really complex objects, this requires like the joint effort of many professionals. It has to do with IT experts, it has to do with software engineers, it has to do with network architects, it has to do, and of course, because we're in a Bayesian conference, it has to do, of course, with mathematicians. To do, of course, with mathematicians and statisticians in particular. So nowadays, cyber attacks are a little bit more dangerous, are more sophisticated, and they are happening in more frequent basis. This has made that they are obviously more difficult to detect, and they are more difficult to detect, especially to the traditional systems. Originally, Originally, these systems, what they were meant to do, were to collect large database of these attacks, of known attacks, so that they could scan the system every once in a while. And if they detected something known, then they could raise a flag and they could check if an anomaly was happening. Of course, because nowadays these cyber attacks have evolved, these systems are not longer effective. These systems are not longer effective for these new kinds of threats. So that's why, that's how statistics is going to be important in this whole bit of cybersecurity. And like in a broad sense, the objective or the goal of statistics in this kind of area, in any anomaly detection, actually application, is to build a model of normal behavior that can be used as a threshold, as a benchmark. Threshold as a benchmark so that we can detect anomalies. And from a vision point of view, this has, I mean, it has been like in recent years, there's been an increase on the interest in these kind of problems. To be honest, for example, I never knew anything about this until I started my PhD. And I have recently found that, I mean, there's like really interesting challenges and problems. Interesting challenges and problems that have become quite popular nowadays in some groups of patient statisticians in the UK. However, because it's kind of new still, I have found, we have found that it's not as widely explored as other approaches, for example, the machine learning and the classical statistics, of course. And in the literature, and this is what my paper review is mostly about, is that we have found that these kinds of threats of the Bayesian statistics applied to cybersecurity have been centered on three different classes of threats, which are the volume traffic anomalies, network anomalies, and of course, malicious software, or also known as malware. So, volume traffic anomalies, this is precisely an example. This is precisely an example of what an anomaly in this kind of setting looks like. We're going to be looking at this kind of time series. We're going to be interested mainly in this measure called the packet rate, which actually is kind of like a measure telling us how much information is entering the system, like in per unit of time. So, the idea of these kind of attacks, of these kind of cyber threats. Of these kinds of cyber threats is to saturate the victim's network. And by doing so, I mean, the main goal is for it to crash so that the horrible 500 free HTTP or at this and the service becomes unavailable. Of course, these kind of attacks are, I mean, when this occurs, it's not always because there's a severe attack occurring. This can be due to normal kind of... Can be due to normal kind of connections occurring. For example, this is quite common. I think in Mexico, like a couple of days ago, the whole CMML systems crashed because everyone wanted to buy Spider-Man tickets. So obviously, it's not always because of a cyber attack, but these systems, so these anomaly detection systems also need to kind of deal with these kinds of things. Examples of this when we are like into the cyber security. Into the cyber security framework include the denial of service and the distributed denial of service attacks. In recent years, the second one is the most found because it allows the cyber criminal to hide and it's really, really difficult to detect who is in charge of this kind of acts. So the main objective of So the main objective of statistics applied to this kind of problems is that we want to detect the attack as fast as possible. And of course we want to do it by seeking a low false positive alarm. Because this, when we rise a flag, that the system might be in trouble, this system stops, and of course it costs money and it costs time. So we even want So, we don't want that, obviously. So, the idea is pretty clear: we want to detect the attack as fast as possible and seeking like this trade-off between accuracy and the false positive. From a statistical point of view, this is mainly done through a sequential change point model, because this is, if we go back to this kind of to this, to this example, this is going to be something. example, this is going to be something that we need to check like in like on real time. Obviously, if we had like the whole time series, we could perform like offline sequential offline change point models. But because we're interested in doing it as it occurs, then we need to think about the sequential change point. In this setting, the idea is that at each step, we need to test this hypothesis where we say that the attack has occurred against it hasn't occurred. Again, it hasn't occurred. From a vision perspective, most of the work done in this direction has to do with the Shir JF robots procedure. These are some of the papers you can find about this. Basically, we need to consider this stopping time of this statistic that is built based on the likelihood rate. Based on the likelihood ratio. And of course, A is a threshold that needs to be, of course, to be obtained from the problem. So in particular, I mean, although it's kind of, there are like several challenges in this direction. Of course, when we say that there's like a sequential change point setting, it's because we're considering that there is this pre- There is this pre-change density F and there is a post-change density G, but the issue with this kind of real-time application is that they are not known in advance. And in particular for cybersecurity, it is said that these two densities are poorly understand. So there's still not enough information about the behavior of these densities. Densities. This has made this, for example, to address these kinds of issues. Some common assumptions is to consider that the packet rate follows a Poisson process, or in another papers they use a normality assumption. But it has been shown that the traffic network actually behaves like a self-similar process. So obviously, there's like lots of work that could be considered in this direction to address this kind of. To address this kind of limitations in the current modeling. Obviously, there is also this opportunity or this kind of challenge that in most of the papers, they consider like an IED example, which is kind of unrealistic for traffic network. And maybe some people are trying to create like more general. Are trying to create like more general frameworks to consider the non-IDD case. In particular, the same authors, Tita Tartakovsky, is working actually on hidden Markov models, for example, trying to create a little bit more of a dependent model in the data. But still, I mean, you can see that there are like many things that could be done. So obviously, because So, obviously, because the class of cyber attacks is like big, really big, it's not only going to be enough to trying to detect them through the volume traffic characteristics of the network. There are other kind of cyber attacks that are more related to the network per se. In this case, the objective of the cyber criminal. Of the cyber criminal is to access the network, and in order to gain access to the network, it might be because it's a rogue user, because they stole credentials. And of course, this is kind of critical because when if they gain access to the network, they can stall sensitive information. All the data reaches, it's because something like this happened. To model this kind of anomalies, so trying to detect this kind of anomalies, we need to consider something a mathematical, a different mathematical framework. For this, it is quite common to consider or to view the compute, to view a computer network as a lipid graph where we have the set of users and we have the set of computers on one side. So we have this direct graph, and the idea is that. Throughout. And the idea is that certain users are going to be connecting to certain computers. And we want to actually from this kind of data, we want to know if a user is going like rogue and is trying to access a computer that he shouldn't be trying to access. Or maybe it's because a computer, for example, this one might be like a really important computer within a university or with an organization of element that shouldn't. Organization of element that shouldn't be accessed by some individuals in this set. So, in this kind of problems, there are like two main goals. Of course, because we're trying to build a model of normal behavior, in this case, we want to analyze and model the normal connection pattern of each user in order to predict whether, like to know and predict. Like to know and predict, like, if a new connection is going to be kind of sketchy or anomalous. And of course, because we can learn from each other, it is also important trying to cluster them together. And in that way, we can actually learn their expected behavior through the pace. For example, this is, I mean, I like to think about this like in a university where we have like different Where we have like different groups, and of course, different groups are going to be connecting to different servers to different services or different computers. So that when there's like a new like a new people within the department, if we know which department belongs to, then we know like which computers he might be trying to access. And this is important like to so that's why it's important like to try to cluster them together. To try to cluster them together. From a Bayesian point of view, there are, I think this is actually the area that has like more, that more people have been like trying to do things. These are just some examples of it. For example, people have been using the Poisson factorization model, where the objective is to model. Where the objective is to model the number of times a user connects to a computer. And this, of course, has been done by this Poisson assumption, where we have this kind of modeling. So the model can be interpreted as having K-latent features characterizing the users even by theta. And of course, we have like these latent. we have like these latent for example this could be like the the job title of the department and we also have the beta i which are the latent characteristics of the each computer like the number of processes per day or the type of computer the type of the the memory anything we we might think about so at the end the idea is that a certain feature might have like a high score A certain feature might have like a high score for all the machines within one department and low scores all the way. So if a new people enters the department, we might know through the piece, which connection should he or she be trying to connect or should be trying to do. In this other kind of modeling in this direction is available non-parametric actually. Habaya non-parametric, actually, and they use the Dirichlet process to identify compromised computers. This is done through the predictive distribution. And the basic idea of the authors is that we can quantify kind of like the surprise of each new connection in each computer through these kind of predictive p-values. So at the end, we kind of So at the end, we kind of know or we kind of measure how like that connection shouldn't be appearing. So for each unique connection in each computer, we can obtain these predicted p-values. And the idea is that we can combine them together. And by combining them together, we can create like a scoring system of the computers within the computer network. Computer network, and then we can rank them. If there's like a lower rank, it means that the computers are being compromised. Of course, because we have a Dirichlet process here, it was a little bit in the line, the PID manager process as well. It's actually the same author, it only changed the first one. So the objective. The first one. So the objective is the same with the Pittman-Georg process. We all know that, just as with the Dirichlet process, this has like a nice predictive closed distribution. So again, the idea is that we can obtain this kind of predictive p-values depending on the predictive distribution. The only thing that changes in this framework is that they wanted to create a little bit more, a more general approach to the connection. Approach to the connections, and in this case, they are conditioning code. They are trying to model the joint connection between X and Y. Both of them have this hierarchical Piedmont jurisdict structure. So, in the case of computer networks and in the case of network anomaly, we need to always remember that computer networks are like really Are like really complex objects. And because of this whole thing of Internet of Things and these whole new devices able to connect to the Internet, I mean, every computer network has like really, really experienced like an important growth. And of course, because everything, most of the things are online now, the internet has been also impacted by that. So there's like lots of new connections like every day. Like every day. So, literally, you can think of computer networks as large dynamic objects. And the kind of modeling that they require, or the kind of when we model, when we're trying to detect anomalies in this kind of setting, we need to keep in mind that our modeling needs to be robust against noisy observations, because in this case, when we have that amount, that When we have that amount of information, it's kind of difficult to always have noise-free data. And for supervised learning and for training these kind of models, it's like really, really important to have that. And of course, they need to be scalable from a Bayesian perspective and a little bit from the experience I a little bit of the experience I gathered in the last couple of months working at the Last couple of months working at the Imperial College at the Department of Computing is that I mean the people like in charge of this kind of research, I mean they find like Bayesian statistics interesting, but sometimes I mean the scalability is an issue for them because they want like fast things and that's why they actually don't use that much the Bayesian statistics approach and they rely more on Approach and they rely more on machine learning kind of stuff, trees, decision trees, neural networks, everything that can be like scalable that provides like scalability to this kind of problems. And the third area of anomalies that we can find in this kind of That we can find in this kind of setting is the malicious software, also known as malware. So, a malware is any software designed to disrupt, damage, or gain access to computer systems. Nowadays, it's, I mean, we can have, like, for example, the spyware, we can have hardwares, trojans, ransomware, we have worms, we have lots of virus. And because there are many types and because they can mutate literally. Literally, if you change a little bit of the code, then they can be considered like a really, they can be considered like a new kind of virus. And also it's the detection of this kind of malware is difficult because nowadays this software is actually capable of learning. So that's why like traditional antivirus systems are not longer like an option. Like an option because they rely again on gathering like signatures of known malware so that they can be compared. So that we every time we download a software, we download like a text, we scan the system. The anti-bureas use this blacklisting approach where they try to compare against known things. But because they can mutate, I mean, like a slight change of the code, they cannot longer be detected. Can no longer be detected. So, in this kind of objectives of malware, it's not only about detecting malware in the wild, it's also important to classify them into known families. And where these families are given by, for example, these ones. Because if we know what kind of malware they are, we can also know like the damage they have made, and we can also know how to reverse. And we can also know how to reverse the effect. And we can also stop. We can also know how to stop the spreading across the computer network. Because once a virus is inside a computer network, it's highly likely that it will travel around it and it will keep infecting all the systems and other computers. So that's why it's also important to know how to classify them. To classify them to this kind of tasks of detecting and classifying malware, there are basically two kinds of, there are basically two approaches. There's a static approach where we can analyze the, literally we can analyze the binary code of the malware. This is basically the hexadecimal representation of the zero and ones thing that is comprised of the malware. In this direction, I mean, and up to my knowledge, there's only mentioned machine learning models used. This actually is kind of like a nice setting, maybe, for statistics to consider. In this setting, the idea is to create like code to extract features from this hexadecimal representation of the binary code. Representation of the binary code that can actually characterize the malware. And if we can know how to characterize it, then we can actually know how to detect them. Of course, this is a little bit complicated because of the high dimensional. It's a high dimensional problem. Even for, even like, even, for example, a malware can contain like millions of these. Symbols, and when we consider like a small collection of malware, we can deal with billions of different features. So, this is like an area that hasn't been explored from aviation point of view. It's only mostly about machine learning models. In fact, if you go, if you visit, for example, like this website, like Kool. website like Khool that hosts this kind of activities where the idea is to detect and have like the most like the best accuracy or the best performance of the model. Most of the people use decision trees and obviously the boosted versions of these ones like the gradient trees and extreme gradient trees which provide like a high accuracy with I mean it's only just like a machine learning model and it's still not And it's still not like completely considered from a Bayesian point of view. And there's also a different kind of approach to detecting malware in the wild. And it has to do by analyzing the set of instructions that are called the dynamic traces of the malware. Of course, this also is like a challenge because we need to execute the malware somewhere. Execute the malware somewhere. And we have to do it carefully because if we do it wrongly, we can compromise the computers. Usually this is done in a sandbox environment where we can run the malware and kind of like see what's happening, what it's doing, like what kind of commands it's using to infect the system. However, this is However, this is like this has become a little bit of a challenge because it has been observed that now these kinds of softwares are intelligent enough to know if they are being run in a sandbox environment. So once they actually know that, they stop and they look like if they were completely benign. So in order to model this kind So in order to model this kind of traces, the most commonly used approach in a Bayesian framework is to consider them like a Markov chain. So the end is like a sequence of instructions. And these instructions can be grouped together into like common ones. And then we can consider like a Markov chain. And once we have the Markov chain, then the interest relies on modeling the probability transition matrix. For example, For example, the same authors that proposed this idea for the first time used a symmetric prior Dirichlet distribution. And then when they were able to estimate the entries of this matrix, they use a logistic regression to know if a new malware of a new software was actually benign or not. But it's not the only approach. But it's not the only approach available in this direction. Both of Bayesian non-parametrics methods have been used. For example, Cow et al. used a mixture of these led processes so that each, when we say PI is the transition matrix of each algorithm of the malware I, and then it's centered on this, this has, so that this matrix has this matrix Dirichlet distribution centered on. Distribution centered on this q on this matrix qi and uh yeah i mean there are like uh this is like two kind of different approaches but the idea is to use the entries of this matrix to to estimate the probability or to know if a malware or a new malware has been detected in the wild In the wild. So, I mean, when we talk about a little bit Bayesian methods applied to cybersecurity, I think they possess like really interesting and challenging problems. Not only because they are like high-dimensional problems, also because of the requirements of the data. Sometimes the data, I mean, there's like I mean, just like because of the privacy and confidentiality, it's difficult to find data to use in this kind of problems. But it has been observed like an increasing interest in cybersecurity, especially from aviation point of view. Of course, this kind of, I mean, we only, I only presented three different kinds of them. Obviously, when we think about cybersecurity, there's like lots of different. Security, there's like lots of different kinds of different threats that are not being considered so far. And of course, there are like different kind of modelings that we could think about. And I mean, I only presented a little bit of them. But to be honest, there's not like, I mean, there are a lot of other things to consider, but it's still like not widely explored area of research. Explored area of research from a Bayesian point of view. And I think it's actually kind of a I mean, it's like it's like a really interesting problem because it also has kind of like common things with other kind of applications. For example, we have like sequential change point problems in the volume traffic anomalies, kind of resembles when we're trying to detect financial changes in financial time series. Changes in financial time series. And when we talk, for example, about malicious software and how they infect computers and how they spread across a computer network, it kind of resembles again, like all these, for example, the COVID, right? So I think there are lots of common points between lots of the applications that we have been using for in basic. Using for innovation statistics that could be like, that could be directly used in cybersecurity problems. And yeah, I think I went a little bit too fast. Thank you. Thank you, Jose Antonio. Are there any questions? From the audience here, seems not from the participants. Let me check. There's a question. Hold on. About the data, like, I don't know, like, if you've studied some data or it's like a public, publicly available data sets regarding this that would be, I don't like to. I don't like could become like the famous data set in this for this application. Yeah, yeah, yeah. In actually in the paper review, we actually go a little bit more into detail in this kind of on the kind of data sets you can find. Of course, they are kind of limited, but there are like specific ones for these kind of problems. For example, for the volume traffic for the malicious software, Software. Microsoft hosted an event in Cable where they released a database of almost 11,000 different malware of nine different families. And for the user-computer connections, there are also some databases that people have been using that are free to access, actually. The others that are a little bit more kind of maybe could be a little bit more complicated to. Could be a little bit more complicated to access these ones. The volume traffic anomalies, what there is like a huge repository where you only need to ask, I think it's from Los Alamos National Laboratory. And you need to send like a request on which databases do you want to use, the purposes of the research, for example. But yeah, I mean, at the end, it's a little bit too limited still, because obviously companies, I mean, Companies, I mean, they don't want to give away this kind of data because of privacy constraints. Thank you. Thank you, Jos Antonio. I don't see any more questions from the audience here, not from the no, so thank you very much. Thanks again, José Antonio. And let's move to the next speaker. 