All right, great. All right, so Olis is here, which is good. I'm actually excited about this talk. Olis, just give us a minute so people can come in. You're on mute right now. You can start sharing your results. If you want to share your screen just so we can make sure everything works, that'd be great. Uh-huh. Yeah. So, oh, perfect. You see it? I see it, and it looks great. Ah, you changed the title of the result. Algorithms to sampler. Ah, okay. I play with the title a little bit. That's fine, that's fine. Actually, I think your talk is a great lead into the panel discussion. I have so many tabs open. Okay, so people are accumulating slowly, but surely it's two o'clock. Give me maybe let's give it like some 30 seconds on this or something a minute and then. And then we'll start your your talk. Okay, so we are happy to have Polish Tenko of now IBM tell us about his most recent result. The title is slightly different from the title of the paper, but it's as good. GIF sampler with noiseless and noisy random quantum circuits. All right, Oles, thanks and please take it away. But can I just say one other thing, Oles? So if you guys have any questions, One other thing, Olis. So, if you guys have any questions, feel free to put it on the chat. And you know, we can make it interactive if it's okay with Olis. People can that's completely fine. I actually like very much interactive talks where people ask questions, even was it at first slides? Like, um, so don't hesitate, please ask. I think we have plenty of time. Um, Of time. And thank you very much for inviting me to this talk. So, as you can guess from the topic, it will be discussing Gibbs samplers, which are realized by random quantum circuits without and with presence of noise. This work is done in collaboration with Ermes. You can find all the material on archive. So, if you're interested. So, if you're interested during or after the talk, you can look at the archive, or you can also chat, send me email or chat. So, and I will start from motivation. And motivation is that Gibbs states or Gibbs distribution is one of the most common states in nature. When we talk about Gibbs distribution, we usually mean this density metric. We usually mean these density matrix, which depends on system Hamiltonian, inverse temperature, and partition function. And because this is a common state to describe thermal equilibrium, ability to prepare it on a quantum computer can be very beneficial for many applications. For example, one may think about preparing Gibbs state in chemistry and molecular physics to figure out the temperature effect on structure of molecules. Effect on structure of molecules. Also, preparing the Gibbs state can be very beneficial in condenser matter physics, where using the Dirk probe of the Gibbs state will reveal some insights about states of matter, quantum phases, for example, high temperature superconductivity or quantum critical phases. KIP states have using quantum machine learning and optimization because you can encode some. Some answering the problem in the low temperature state or temperate, some fine temperature state of the system. And also, there is a potential use of deep states in quantum error correction. In particular, we know that the renotion of thermal quantum memories in high dimensions, where preparing thermal state and thermalization is actually able to correct errors. So, and device. Device which will produce our Gibbs states, our sample from Gibbs states, we call Gibbs sampler, has this particular schematics. So usually we consider it a black box, which has some unit transformations and measurements inside, all parametrized by a set of numbers, which I will collectively denote by theta. And this black box gets input in terms of qubits. Gets input in terms of qubits, which I historically divide on system and ancilla. And the difference between these two groups is that we discard ancilla of the end, while we keep a result of the computation at rho s. And our box will be a good Gibbs sampler if the expectation value over Î¸ of the parameters will converge to the Gibbs distribution, even if. Distribution, even if rho s individually do not. In this case, we can always run this box multiple times with multiple values for theta. And if at the end expectation is equal to thermal state, any observable will also converge to thermal observables, which in most cases what we need. And if you want to rely such box, historical number of algorithms, which I'm going to review right now. It's very hard to make a boot systematization, so I will try to provide it, and then I will provide you with two algorithms which we developed, which can be conceived as a kind of algorithm that hybridize different ideas and has benefits of many existing algorithms with the same time lacking of disadvantage of such algorithms. Of such an algorithm. I will show in particular that circuits can be using random parameters to be adjusted to noise and can be used for intermediate noisy quantum devices. So this is the outline of the talk. And if we want to review existing methods for Gibbs state preparation, there are three major avenues of doing so. So, so first avenue, I will call the eigenbasis samplers, this samplers which are based on quantum walk, and they can be summarized as follows. So in this case, we prepare state in this configuration, where E L is an eigenstate of E Hamiltonian, L is some buffer state which used to record information and C L probabilities. And once we prepare this. And once we prepare the state, the targets of the algorithm to make a conditional transformation of the state to make Cl squared to approach the Gibbs distribution. There are a lot of methods working in this way. A prominent example is quantum metropolis algorithm. And if you look at this algorithm from the perspective of the density metrics of a system, it is carding the ancillary. Discarding the ancillas, discarding this register, it would look like a diagonal state where transformation is just kind of a reshuffling of probabilities on the diagonal. And because it's very simple picture, these methods are very easy to analyze theoretically. They have a great deal of connection to classical algorithms. And what's important, many of them have empirical polynomial performance. Polynomial performance, for example, metro-polis algorithm, which makes these algorithms in many cases sufficient. Unfortunately, if you want to rely on this algorithm in practice, you have a big deal of implementing a quantum phase estimation because you want to condition on eigenstate. And this is very, very expensive in the near-term devices. It also will require non-local circuits. And this also type of algorithms require fault tolerance. Algorithms require fault tolerance because even a single bit flip in these algorithms will cause very unpleasant results. So it will change in one bit in energy will lead to completely senseless random walk. Therefore, these algorithms require full fault tolerance. An alternative to this algorithmic approach is a more kind of Is a more kind of physical motivated approach, where physical motivated approach, when we divide our qubits on system and ancilla and prepare large ancilla into thermal state and then put them in contact, which means that we evolve the combination of our system and sciller under unitary where any generated by Hamiltonian, which is some. Anytime generated by Hamiltonian, which is some of system Hamiltonian, Ancilla Hamiltonian, and coupling. And if we have generic enough coupling, it's about coupling small enough and bath is large enough, and we can prepare the thermal state to bath, then this algorithm with natural succeeds in preparing thermal state. And this algorithm has a big advantages to algorithmic approach because. Approach because the circuits or basical dynamics is local. You do not need all-to-all qubit coupling to that. It just should repeat natural Hamiltonian couplings. We know this algorithm empirically performs nature, and it has some sort of self-correction, which means that if we have a single bit flip somewhere in between, the entropy getting from this flip is will probably absorbed by bath. So you will get maybe slightly different state from. Be slightly different state from Robita, but its mode will be an orthogonal state to Robita. Unfortunately, all these algorithms have no polynomial time bounds. Before, we show that how it can be improved. They require a huge ancilla, which will be much, much larger than a system, sometimes in many models exponentially large. And its system is still noise sensitive in the sense that any constant rate of noise will create a great error. Finally, is the most Finally, the most modern, the most recent type of algorithms which can be used are what would call the feedback samplers, classical feedback samplers, which include variational algorithms, where we introduce some initial state, initial, sorry, circuit, which parametrizes by a set of angles, and then get a measurement of the output. And then we use We using some cost function, we optimize the circuit to get the state which is which substitute of qubits in the terminal state. And this type of circuits are great because they require minimal resources. The architecture of the circuit can be very, very minimalistic, which is perfect for intermediate scale quantum computing. The circuits can be chosen local, and the major benefits here. The major benefits here is that if your circuit is noisy, when you do optimization, you actually optimize and basically the circuit for noise as well. So you somehow mitigate the noise. The cons of this approach is that this algorithm has no probable convergence, which in many cases, if you just pick up some architecture, there is no way to prove that this architecture can reliably. This architecture can reliably reproduce GIP state, even if you have a great deal of optimization. It depends on ANZATs of the initial states, usually. And in most of this case, even if QF forms, there is no good example of clear quantum advantage for this approach. So algorithm in some sense are kind of a hybrid of these approaches. They provide algorithms which have provable convergence on one side. On one side, similar to algorithmic, uh, uh, to algorithmic uh approach, they require minimal resources comparable to variational algorithms. And compared as a variational algorithm, they have some noise adjustment which can be used to optimize and make circuit resilient to noise, which all together makes the algorithm friendly for noise intermediate quantum devices. Quantum devices. If anyone has a question, yeah, it's a good time if anybody has a question. It's been pretty clear, so maybe keep going. Okay, so let's go to algorithm one. So algorithm one is, I would like to put it in the I would like to put it in the context of random circuit because, first of all, this algorithm one has a big connection to this field and also it suggests some fundamental result in this direction, but also it proposes some kind of application of random circuits. So, what, first of all, what we call random circuits. So, when we talk about random circuits, we usually mean the following problem. We specify a blueprint of a circuit, which A blueprint of the circuit, which means we specify the qubits and the gates, but we don't specify unitaries. And then for each gate, we generate unitary randomly from certain distribution. It can be Harley distribution, it can be Clifford distribution, Clifford group gates, or some other distribution. As a result, we have a family of circuits, not one circuit, but entire family. And once we measure the output of the circuit, as well as use some intermediate measurement result, we're interested in some property of this family of circuits, which unify all the circuits together. And this problem has a great importance, for example, for quantum advantage, where, and Ramis is known as experts in this, where we can. Experts in this, where we can show that if we have a high random circuit, the probability of outputs for most of high random circuits is impossible to estimate using, impossible to reproduce using classical algorithms, which has a big implication, or at least we have some arguments why it's hard to do on the classical arguments, which gives a good argument while quantum computing. Argument why quantum computing probably cannot be simulated by a classical device. This type of circuits also have been modeled for generic quantum dynamics, where the output can have different scale of entanglement with system size, area law, volume law is critical. It also can be served as error correcting codes. And today we'll show that there is a family of such. Of such circuits which serve as the producer of GIP states or GIP samplers, and obviously, maybe the gates of the circuits cannot be high-random gates, so we have to fix a particular distribution of unitaries, which I will describe right now. And to do so, I will focus first on a very simplistic circuit where I have only gates, which applies to all qubits. Only gates, which applies to all qubits altogether. So the architecture circuit looks like this. And by this box, I assume this following unit rate. So each box has an input, each gate has input a single ancillary qubit and some number of qubits, system qubits. And it acts as follows. It acts as an X on ancillar qubit and this operator on system qubits. This operator on system qubits. So, this operator is the square root of beta a beta inverse temperature h Hamiltonian in D, some integer. And I will, without losses in generality, I will assume that H is a positive. So, I can do this square root. So, entire unitary is parameterized by angle theta. And to make ensemble, I will assume my theta is distributed using as a Gaussian, a random Gaussian. So, the random circuit will look like. So, the random circuit will look like here, where each theta is generated for a random ensemble. And this circuit can be actually used as a Gibbs sampler. So for that, we need to prove that this raw, the output, the circuit, which is expectation value over circuit angles as well as inputs, converges to Gibbs state. However, it's very important that it's very important that to do so we need to fix that output of every intermediate measurement returns our zero so um the the entire circuit would uh work like this we take a random circuit randomize the angles run the circuit if every measurement returns zero we accept the circuit randomize input and randomize the circuit otherwise Otherwise, we reject the circuit and we run it again. So, in this case, the density matrix will look like this, where epsi is the initial state, PA is a projector on zero state, and rho of theta is output. And it's a very nice feature of this operator pi and this is that we send which you see. We sandwiching of this unitary over two projector and zero give us cosine, and it's very easy to see that the output of the circuit will converge to Gibbs distribution. For that, we just need to use the very famous averaging formula, which converts our imaginary exponential into real exponential. So, if we have just depth one circuit. Depth one circuit that would result in just cosine square, which after averaging produce a superposition of infinite temperature state, identity matrix. And I mean, not infinite, it's kind of superposition of I in e to the minus beta h, which looks like already some something going towards the right direction. If we take depth two, it will be two cosine squares. Two cosine squares, and after taking expectation value, we'll get something like this. While if we take D-depth circuit, after taking expectation, it will give us this expression. And we can decompose this expression using binomial formula to getting a mixture of e to the minus beta h states and minus minus beta h states. States, where C and K here are binomial coefficients. And if we familiar with binomial coefficients, you may notice that if we have increase in D, sorry, so here we see D of K, this binomial distribution, as we increase D, it slowly and slowly converges to a delta distribution. So as a result, the expectation value of the circuit. The expectation value of the circuit when we take d to infinity will give us to the mixture which will be delta distributed around temperature beta. And I will comment how this convergence work precisely mathematically in terms of error. But before I would like to say a few words how this circuit can be improved in the case when we have Can be improved in the case when we have no ability to apply the gate which applies to all qubits. And that can be done if we have a Hamiltonian which is sum of local terms. For that case, instead of applying the gate to the entire circuit, we can apply it the gate that applies only. The gate that applies only to a group of qubits. So if Hm here is the Hamiltonian sum of Hm and Hm acts on a group of qubits, we operate with the gates which apply only to a subgroup of qubits. So the circuit, as a result, will look like this. Let me explain how entire circuit works in general. We assign some qubits to the system as a previous. Qubits to be system as a previous, and some qubits to be ancilla. Just before you um continue, can I uh ask you a question? I mean, actually, to you a question that was just asked. So, you know, you're familiar with this question. Since the success probability is exponentially small in D, what do you so Nicole Younger Halper is asking, since the success probability is exponentially small in D, what do you prefer to use as your value for D? So, algorithm one does have this exponential. So, algorithm one does have this exponentially small acceptance probability. I think it has to do with that. Yeah, so this algorithm will require exponential time to run. So, this is the answer to the question. So, we have to, I will address it just in the next slide. Okay, thanks. So, there is a prose econom of this algorithm. So, there is a prosecon of this algorithm, and the process of this algorithm that it applies for arbitrary Hamiltonian. So, you can use HM, whatever Hamiltonian you like, but the output will be exponentially hard to obtain in terms of time, otherwise we'll be bound, we can violate certain complexity restrictions. And yeah. And yeah. If we are already interrupting, can I ask kind of a more principled question? You're presenting here a random circuit, and the result which you show to us is that the average over kind of an input state yields the Gibbs state. Yes. Yes. So yes, which is the kind of the probabilistic average of all your kind of random theta gates. And of course, the question, which I would immediately have as a probabilist, what else can you say about this distribution? Because if you run this experiment many, many times, you will. You will always get some other pure state. And okay, so the average sample is the Gibbs state, but what else do you know about this distribution of states? Yeah, it's a very good question. So I have two points about this. The first one, and that's what I said. And that's what I said a bit earlier in the very beginning: that every individual row S, which will depend on theta, can be different from Gibbs distribution. But if we want to, for example, assume we want to figure out what the expectation value of certain local. Sorry, am I correct? Sorry, am I correct, Emmy, that the state which you get out of this meat grinder is always a pure state? In the algorithm one, yes. It will be a pure state because... Rho is, yes. So only the average over the rho theta is actually a gibbed state, as a mixed state, no? Yes. So, you see, so I assume, so we need to think about this as any kind of practical question. So, practical question is, for example, you want to estimate some local observables. That's something you need for physics, for example. You want to say, find the energy of the certain thermal energy. Then you will just do, you measure observables. You measure observables, and to measure certain observables, you need to run the samplers many times because every time you do a measurement, you get plus or minus one, say, if it's a it's it's it's a qubit. Yes, but but seeing somehow, if you, for example, just measure one observable, and you always you run this many, many times, then you will get a distribution of your measurement. Only of your measurement results which depend on theta. And you have a statement about the average with respect to theta. That doesn't say anything. How broad this distribution in theta is. Yeah, actually, I will have results about this. And actually, I have for Algeria Weim, I have a proof that it actually converges for it gives a uh it gives a result for any any any any any uh any theta and so i i will i will give you i will give you answer about the width of this distribution okay good concentration results to use the yeah but it will be it will be it will be a result but i i just want to make sure that it's a separate issue and i will say that even if we didn't have this uh anti-concentration or sorry i didn't have this Sorry, I didn't have this concentration. It's still a viable algorithm because we will just, if we want to get thermal observables or any other thermal observables, we can just randomize the circuit at every time. And the output will be impossible to distinguish from Gibbs state. It will be just the expectation, thermal expectation values. So there is no problem with that for every theta, the result will be pure state and will be no thermal state because what's important at the end, that expectation value will be thermal states. And I think by the time you're finished with the algorithm, once some of these questions will be answered, yeah, just give me. Just give me brief, just one minute to finish explaining this part and then I will give you a present you full result. So we have this random, we have circuits and every circuit where every gate, sorry, we have a circuit, circuit contained as a previous in D cycles and each cycle contain the number of gates equal to the number of local terms and each gate is followed by a measurement. Gate is followed by measurement post-selection. And this is how Zips works. And we've commented that in general, it's not necessary that these gates are ordered in a certain way. You can actually think about this HM as a discrete random variable. So you can generate these gates in your circuit also randomly by choosing random HM for each gate. That will also. HM for each gate, that will also work. So, but let me show the result. So, and the result is the following. You have three different ways to run your circuit. You may, for example, randomize your circuit previously, we called mode two. We randomize circuit, then run it until it succeeds. Then run it until it succeeds, and then measure, and then we randomize circuit again. In this case, our density matrix is just expectation over theta, because we summarize. We also can actually randomize circuit after each run, independent whether we succeed or not, whether intermediate measures return zero or not. In this case, we have a like weighted average with probabilities P, which is probability of. Probabilities P, which is probability of success. Or another experiment, we can randomize the circuit only once and run the circuit all the time with the same value of thetas. And I think the question was about how this will perform. And the theorem which we show is as follows. If there is a small parameter xi, which is equal to beta squared times number of Hamiltonian terms m divided by d. M divided by D. So if this parameter is small, the relative entropy between Gibbs state and the output, either one or two, is always upper bounded by something time, some constant time psi. So decreasing the depth of the circuit, or increasing the depth of the circuit, we can always make the difference between output and or beta in average small, which means all the expectation value, once you compute it, they will. Value once we compute it, they will converge to a thermal distribution. While in setting three, when we fix the circuit and write it multiple times, we can make a probabilistic statement, something like this, that the probabilities that output will be different, much different from the Gibbs value is always can be made arbitrarily small. So, by making psi small. So, for example, if we choose epsilon to be square root of psi, then the probability of s being b times square root of psi is less than equal to the square root of psi. And increasing psi, we can make sure that the distribution is very, very peaked. So, and final final note is about time and this algorithm. about time and this algorithms algorithm is indeed uh requires exponential time to run uh it's universal uh it applies for any gates and the time is overbounded by by this quantity where alpha is related to partition function so um which means that this algorithm can work only for limited system size but let me let me show why this algorithm still is exciting Algorithm still is exciting. Apparently, this algorithm, and for the best of my knowledge, is the most efficient algorithm from all algorithms, which at least I know for small system sizes, for limited devices. And one of the nice features, which makes it very, very efficient is that, for example, if you want to Find a thermal state for a Pauli Hamiltonian, which is a sum of Pauli terms. It appears to be that realizing this unitaries, which we described before, using discrete circuits is pretty easy. For example, single public terms require only one qubit gate to realize single public terms require only one C naught to run, for example, in H, which likes this form, where I added I to This form where I added i to make it positive would require only one c naught and single qubit gates for h in Hadermart and theta extrotation. And the same similar thing will be for y and z gate where instead of h you will use either trivial gate or s gate. And if you want to do three qubit term, that will require only three qubit unitary which require two qubit term, it will require only two C nodes. So So, this simplistic reduction actually makes the circuits very, very small compared to any circuits which will require quantiface estimation. And that gives us a pretty good deal of benefit for small systems. Can I ask you a question? Yeah. Can you please comment on why one does not expect anything faster than exponential time for this algorithm? Well, so this algorithm. Well, so this algorithm can be set up to, for example, be some Hamiltonian which are known to have QMA hard ground states. So because this algorithm converges in polynomial time and temperature, I mean polynomial with resources which require polynomials With resources which require polynomial, with resources which are polynomial in temperature, we can try to use this algorithm to solve some QMA hard problems. And unless we some the whole field is useless, you wouldn't expect that. Yeah, yeah. So this means that we cannot trick it using this algorithm. Therefore, the exponential time is a payoff. Exponential time is a payoff for universality of this algorithm. Thank you. So, let me show just quickly the numerical and experimental result for this particular algorithm. So, here I use a very small system, three qubits, to unseal a similar circuit which you saw before. I use d equal five depth of the circuits, which basically Circuits, which basically is the entire depth. If you think about, or maybe you should multiply it by two for because you need two C naughts. So the real depths of circuits are around 10 in this scenario. So I will look how Look how preparation works for my for the spin Hamiltonian the previous slide for this one and looking at how the output the probabilities of different different eigenstates compare to theoretical prediction so here dashed line is theoretical prediction blue one is the output of noiseless socket and you can see even for d equal five the correspondence is very very very tight so Very very tight. So, uh, um, while we also may think look about the circuit by adding some noise, and here I simulate noise by adding 1% decoherence for each two-qubit gate, for each C naught. In this case, of course, the distribution will be a little bit different. It will deviate from perfect distribution. And at the end of the talk, I will show you how we can deal with the situation. We can deal with the situation using the fact that we have many random parameters and we can which we can control. So, similar thing will be for if we look at thermal energy compared to theory for different temperatures, and you can see that the noiseless circuit output is pretty close to theory. Finally, this plot shows the result for IBM Casablanca device and IBM Casablanca. Casablanca device and IBM Casablanca is a good device because it allows resetting qubits. And instead of looking at the overlap with eigenstates of the Hamiltonian, we look on overlap with states like 0, 0, 0, 1, 1, 0, and 1, 1. And we show that for two qubits, we have a pretty good result, pretty good correspondence. So let me go to algorithm two. So as what was noted that algorithm one Noted that algorithm one requires exponential time, so that's why maybe you will be interested in algorithm two, which gives us some promise for scalable Gibbs state preparation if we make some assumption about Hamiltonian. So algorithm two is a little bit different from algorithm one. So instead of the random idea of random circuit, it will use this idea of random Hamiltonian evolution, and it's a modification of digital path. Of digital path approach, which I described in the beginning. So, as algorithm one, it consists of a number of cycles, and each cycle does a repetition of the following procedure. You take your system output from the previous cycle, you add, reset ancillary into thermal state, which I will comment on very briefly. Then you apply the unitary transformation. Apply the unitary transformation under generated by Hamiltonian. And this Hamiltonian is a sum of system Hamiltonian, Ancilla Hamiltonian, and coupling. So the time for each cycle is generated randomly, and I will show how we do it in the next slide. We also use a path to be independent sum of the Hamiltonians, so decoupled Hamiltonian with frequency. With frequencies, which I will again come on to the next slide. And we also use a coupling as an X acting on ancilla and some V operator, which is a local operator which acts on a system qubit, which I denote by Q of M. And Q of M just means it's qubit in the system which corresponds to M S and Silla. So the single qubit operator which corresponds to. Which corresponds to m anthilla. So each anthilla has a single qubit in correspondence in the system. And that's how this term is constructed. So the circuit for that will look like this. It's very similar to previous one. We have some ancillas, we have a number of qubits, we set up qubits in fine temperature state, then we prepare the states in a thermal state of an of the A thermal state of an of this independent Z Hamiltonians. And that's for that, we need only just reset Ancilla, make X rotation and measure it. And that's how we can prepare statistically prepared thermal Gibstate for the Hamiltonian. And then we just use device to implement a unitary evolution of this system together with Ancilla. So to make an analytical result. Analytical result for this algorithm to show that it can be efficient, we need to randomize it. And we use the following randomization. First of all, we generate the time evolution for each cycle is generated from Poisson distribution. And this is a very interesting theoretical trick because expectation value over gamma is the Laplace transformation. That's why it's very convenient to use Poisson distribution as. Use Poisson distribution as times. But also, as a frequency of the qubits, we generate randomly from minus omega to omega, where omega includes all possible frequency transition in a Hamiltonian. So it should be larger than normal Hamiltonian. Though I will comment that this is not the only possibility to prove our results. So you can choose a little bit different omegas. And finally, we just And finally, we just choose local Hamiltonians to be randomly generated as random terms. And the statement is that once we do this randomization, if Hamiltonian satisfy eigenstate thermalization hypothesis, algorithm two is equivalent to quantum metropolis algorithm. And that's what I'm going to show you right now. But to understand the statement, first, I will first maybe need. First, I will first maybe I need to cover what the eigenstate normalization hypothesis is and how we use it. So, the state normalization hypothesis is a statement about eigenstates of Hamiltonian, and it can be formulated in form of strong random matrix and z. So, assume V is a local term, and when I say local, I mean the number of qubits involved in V. So, for example, single qubit or two qubit operator. Or two-qubit operator. And if we have a local operator, the strong random matrix is that say that the matrix elements of this operator follow this structure, where VBU are some diagonal terms, which usually a smooth function of energy. And the of diagonal terms are exponentially small with and proportional to Proportional to random metrics. So if we have a random V is equivalent to generating operators random R. And if we do this, if we assume this NZATs, we will call this Hamiltonian ergodic. And what's important about these Hamiltonians is that, for example, the long-time expectation. The long-time expectation value for this Hamiltonian is a usual thermal state, which is not relevant, but it's an important fact why this eigenstromalization hypothesis is widely studied in literature. So now let me comment on quantum metropolis algorithm and brushing out any particular details which how to realize it, the quantum metropolis algorithm is basically. quantum metropolis algorithm is basically a classical metropolis algorithm formulated in a quantum way. So we consider mu and E mu to be eigenstate of Hamiltonians. And then this algorithm runs as follows. You start with eigenstate of the Hamiltonian. You apply a random transformation, VJ, and for purpose our algorithm will assume this random local transformation is the same local term. Local term which we have in the Hamiltonian. Then we measure Ï in energy basis, which means we collapse to a different eigenstate of the Hamiltonian. And then we accept this move with a probability which involves the energy differences between energy levels. And if we accept it, we use now our new as a state to repeat also. Stay to repeat all the procedure, or we just reject it and return to our mu. So the entire process is probabilistic. So repeating this many times, there is a strong empirical evidence that for many relevant Hamiltonians, such procedures succeed in polynomial time. For example, in the original paper of Christian Tam in 2011, they studied it for spin Hamiltonian and they discovered scaling of this algorithm pretty close to linear incessant. To linear in system size. So, which is kind of agree with what we know from classical Hamiltonians, where we have an actual strict n-log n bound for certain classical Hamiltonians for Metropolis algorithm. And could I ask a quick question about that? Yeah, sure. Because as a quantum algorithm, I'm wondering about what happens when you reject. What happens when you reject the move? Because you can't go back to mu, right? So, having made the measurement, you can't, and there's a no-clone, no-cloning theorem means you can't make a copy of mu before you start. So, in the paper of Kristen, they have a kind of smart and complicated way how to do it. So, you indeed cannot copy state, but you But you can, because you can kind of store, you can use Ancilla and you can store the result of previous step in the Ancilla register. So basically, the idea of this paper is that you can have like a memory where you have a previous state. Where you have a previous state, you have an active state, and then you can, if you decide to reject, you have a way to swap it and proceed. But I will note that it's a very good question. It's very interesting to expose this, but it's not very relevant for this particular talk because we are not going to realize the quantum interpolous algorithm. What we're going to do is to show that this algorithm, which is widely. But this algorithm, which is widely studied in literature, is indeed something we can compare algorithm to. Sorry, did I ask the question? I hope. Yeah. So it was good. So then basically, if we describe this process mathematically, it will describe by this stochastic, stochastic process where pK. Where PK is a probability of having state Î¼ and TÎ¼ is transition amplitudes which depend on the matrix element of V and energy differences here. And there are some coefficients B mi nu which are symmetric, so they are independent. So B mi nu is equal to B nu mu. And that's how mathematical this process is described. Process described. And after these steps, the state which is generated by this algorithm, if we have it somehow realized, it will be this one with PD as a probabilities of different eigenstates. So now, when we have this mathematical formulation, just say the following, our result, that there is this algorithm setting, our algorithm to setting, and such matrices B mu, such. Such matrices B menu, such that output of our algorithm is approximately equal to output of rho for certain B menu. So in particular, the theorem says that there exists D-step quantum metropolis algorithm such that distance from for average circuit of algorithm two and Gibbs state is upper bounded by distance of quantum metropolis algorithm. Of quantum metropolis algorithm and state beta plus some small epsilon. And this can be done if we have a particular scaling of the parameter of the algorithm 2. In particular, we have to scale the number of ancillas to be equal beta times d times omega, where d is a number of steps in a metropolis algorithm, and omega is basically the spectral. Basically, the spectral norm of the Hamiltonian. It usually scales with system size, with number of qubits, linearly. Lambda is coupling between system n and sclla. And gamma is basical a parameter in Poisson distribution, which relates, which is equivalent to inverse time for each cycle. And this is an asymptotic statement. It's valid, as you see, for large D and small epsilon. In small epsilon. But as soon as we have large d in small epsilon, this theorem provides you with the estimate what the parameters Hamiltonian can do. And you can read them in two directions. You can read that. If you have a quantum metropolis algorithm, you can design algorithm two. Or if you have algorithm two, you can always find the quantum metropolis for which. Police for which this process relates to. Which distance are you using? Sorry? Which distance are you using? Here's a trace distance. But in general, the distance is not very important here because it's an asymptotic statement. So, I mean, almost any distance because we can always, yeah, so the correction. Yeah, so the correction we can always tell any distance which behaves normally due to when we expand over small parameter will be suitable here, which has no some divergences. Okay, and using these parameters, you can estimate the total time of the algorithm. Total time of the algorithm. So, if we believe that there is this metropolis algorithm that for this particular Hamiltonian succeeds with probability in these steps, with any error which satisfies you then, time to reproduce it scales like this: beta times d cubed divided by epsilon, where epsilon is this additional error we have. So, this makes because So, this makes, because if we believe that D is polynomial, this means that this algorithm promises to converge in polynomial time for this type of Hamiltonians. So, on this slide, I just show you very, very briefly because I want to highlight another point about noise mitigation. Can I just interrupt you and ask you? Can you please comment why you? Can you please comment why it would be advantageous to use this algorithm over quantum metropolis? Well, yeah, it's a very good question. Thanks. So, quantum metropolis algorithm requires quantum phase estimation, which first of all is very sensitive to noise. So, it's very hard to do it in current generation of devices. And also, it will require certain high connectivity of the circuit. While this algorithm. while this algorithm doesn't doesn't is not is not as as as as as noise as noise sensitive in in general it uses some natural connectivity so it uses the same connectivity as your Hamiltonian has and therefore is it's much more promising than quantum metropolis Than quantum metropolis for near-term quantum devices. So this plot shows the performance of this algorithm for small devices. And you see, again, the output for average exact and exact are pretty good. And if we had noise, we have some. Good, and if we had noise, we have some deviation which have to handle. And now, let me let me now go in the direction and try to explain why this algorithm additional explain additional benefits of these algorithms. So, unlike deterministic algorithms, these algorithms have many random parameters for which they converge. Uh, they converge, which means that we have a kind of degree of freedom. Uh, so we have a lot of free parameters which for which noiseless circuit converges. And we can leverage this in the case we have a noisy circuit. So, when we have a noisy circuit, now some random configuration appears to be better, some noise configuration appears to be worse for. To be worse for noise. And it's the idea that we can use this random parameters to optimize and to compensate the noise. So, and it works as follows. So we start with a random configuration of angles, and for which we know that we, in noiseless circuit, we succeed, basically. For noisy circuit, we not probably precisely succeed, but we are close probably. Then we estimate some loss function. Then we estimate some loss function, and then we use classical feedback to readjust these angles to compensate the effect of noise. So, in this work, we do some pretty, I think, naive thing. We use free energy as a loss function to just illustrate how actually adjusting angles would compensate the noise. And of course, there is a big question how to. Of course, there is a big question how to estimate this free energy, and there are different ways. For example, for all grid one, actually, the partition function which enters the free energy is, in fact, can be done using output of the circuit, probability success of the circuit, but in general, it's a big work in progress, how to construct a good loss function. But assuming we can estimate the Assuming we can estimate the free energy of the system, simulations show that we can greatly reduce the effect of noise. So, for example, if we use optimization over our CT angles of the circuit, so this picture show that we can basically entirely compensate 1% of depolarizing noise. So, basically, making the noisy circuit give an output which is pretty close to exact. Exact and the same thing about the algorithm too. So, if we take the leverage on changing times, frequencies, and coupling operators, we can also reduce the effect of noise pretty effectively, which suggests that moving that direction is a pretty promising thing. And I think additional to Additional to a smaller amount of resources required to implement these algorithms, we can actually also operate them in a presence of noise. So just a quick summary. I presented two algorithms. One, algorithm one works for arbitrary Hamiltonian, algorithm two works for ATH Hamiltonians. Algorithm one usually exponential, while algorithm two take polynomial time. Both of them Polynomial time. Both of them use polynomial number fancillus, where algorithm one is pretty tight and they converge for random path angles and allow some noise adjustment. So this project has many different directions to go. For example, one can show that algorithm one, so I one here stands for points for algorithm one. Stands for points value root one, have an advantage, for example, in preparing ground states. So the time of the preparing ground states using this algorithm will be inverse proportional to overlap of your initial state and the ground state and your answers, which you input into the algorithm. Also, it can be shown that it can be used for efficient preparation, one-dimensional and frustration. one dimensional and frustration one dimensional frustration free Hamiltonians uh sorry uh class should be sorry so should be classical 1d and 1d frustration free Hamiltonians and it also can be highly optimized using high order product formulas while algorithm two can be done uh using weaker ETH approximations we use strong metrics and Z which is quite quite quite quite strong assumptions Quite strong assumptions and also can be analyzed for average key performance. And I would like to note this work, which is published pretty much the same time as we do, where we use also different, a little bit different DTH approximations and also have some results on average case performance. But as a cause, they have a much worse scaling for the algorithm for the The algorithm for the system size. And finally, one big deal is a full-scale simulation, which will show that algorithms will provide a good tool to prepare a GIPS state. Thank you very much for your attention. I hope this was informative and interesting for you. Thank you, Ola. Very nice and clear talk. Talk. We have time for a couple of quick questions before a group photo and then the panel discussion. Anyone has any questions? All right, Seth Lloyd. Seth, you want to go ahead, unmute? Unmute. I'll start my video. There you are. Yes. Great to see you guys. I'm really sorry. I can't be there in person in Banff, though. It's a nice day here in Cambridge. So, Oles, what happens with Willis, what happens with systems that have an internal impediment to thermalization, such as those, for example, that exhibit many-body localization? Is a problem there that they don't obey the eigenvalue thermalization hypothesis? I would imagine that might mess things up here. Yeah, so they clearly violate ETH, so I cannot give any guarantees about those type of systems. You can run an algorithm one. Algorithm one, right? Algorithm one looks good for that. Just wait around for a while. Yeah, so there are some works which suggest, so if you have state which have a low correlation between different parts, there are algorithms how to deal with the states using preparing some like Using preparing some like parts of the system and then trying to fuse them together. So I know the progress in this direction. So you may think about algorithm one in this matter. So if you want, you can use a fusion of different parts of the system. But I'm not sure I can give a pretty good path from the top of my head. Maybe. Thank you. Thank you, Vols. Thanks. Thanks for the questions. The next question is from Seven Bachman. Please. Yes. Hello. Thank you very much. Thanks for the talk. You insisted very much that algorithm one is universal, but has a bad scaling in time. So could you imagine that either this algorithm or a refinement of it would be better in time if you know that the In time, if you know that the Hamiltonian and you use that the Hamiltonian is local. I mean, right? Yeah, I worked quite a lot on trying to make it work for prove it efficiency for certain types of Hamiltonians, but But unfortunately, it's very, very, very, very hard to formulate even if you're in the context of algorithm one, for which Hamiltonians, it should be easy to prepare thermal states in terms of complexity. So ETH seems to be in the same thermalization. ETH seems to be in the incentive hypothesis is very strong assumption, Hamiltonian, which guarantees its efficient preparation. So there are some results showing that if you have low correlation between parts of the system, that type of Hamiltonian thermal states will probably easy to prepare. But at the moment, I don't know any good way to. Know any good way to include this assumption about low correlation between parts to improve algorithm one. And that will be probably answered to Seth's question as well. So if we have a system like a localized, many body localized system, can we actually do thermal state preparation? But in the current form, algrid one doesn't give it. Give it a direct direct way to do so. Thanks. Yeah, also, Steph, don't feel too bad. We're all at home now. The meeting is entirely online now. For no reasons, that would be obvious to you. I noticed that, but still. Okay. It would be great if we could all be there. That would be great. That would be great. Any other questions? Well, you know, if not, we can transition to the group photo. And I don't exactly know how this is going to work, but apparently it's going to be a group photo. Yes, hello. I can do it now if you all turn on your cameras and just take a sort of like a screenshot and do. Like a screenshot, and depending how many you are, I might need to take several of those. Just give a couple seconds. Yeah, there's gallery. Yeah, I have to erase blackboard quickly. Make a photo of the gallery review, just screenshot of gallery viewer. No, that's actually also from a call exam. Three, four people still, if you'd like to turn your cameras on, otherwise you won't be in the picture. On, otherwise, it won't be in the picture. Yeah, turn these cameras on and don't move too much. I will give you a countdown, so don't worry. All right, so give me a big smile. Second, let's say cheese. And we'll do another one just in case. You can say cheese. Awesome. I will edit these pictures and post it on the workshop's website. You should get an email notifying you of it. Thank you. Wonderful. Thank you. Thank you so much. Great. So are there any other questions? If not, we can take a short break and resume at 3:15, seven, eight minutes. Seven, eight minutes. Addina, we'll leave it open in case people want to interact. You can just unmute yourself and talk to one another or ask Onas a question. Otherwise, see you all in seven, eight minutes.