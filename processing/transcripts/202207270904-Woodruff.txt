Yes, experts problem. This is a problem of sequential prediction. So we have a set of experts or just people with opinions and we have a set of days. And in this problem, what happens is on day one, each of these experts will make a prediction. Okay, so let's assume that by Okay, so let's assume a binary prediction, and we'll assume that for most of this talk. So, for example, on the first day, the first expert might say that it's going to be sunny, the second expert might say that it's going to be rainy, third expert, sunny, fourth expert, sunny. And then you look at these predictions and you make your own guess whether it will be sunny or rainy on that day. And then you see the actual outcome for that day, if it was sunny or rainy. And then you move on to day two. You move on to day two. Each of the experts makes a prediction again, and you know, you might decide how to pay attention to these experts, you know, maybe based on previous outcomes and whether or not the experts were correct. And based on their predictions for the second day, then you make your prediction, you see the actual outcome, and so on. Okay, so this repeats for some number of days. Okay, and your goal is. And your goal is to generally be as right as often as possible. And how do we measure the performance formally? So we make no distributional assumptions on the input, and we judge our algorithm based on the standard notion of regret. So the definition of regret here is the number of mistakes that our algorithm makes more than that of the best expert divided by the number of days. Okay, so if the best expert among all these experts has some mistake rate, All these experts has some mistake rate, you can think of this as the difference of our mistake rate and the best experts' mistake rate. Okay, so you know here is this example again. And if you look at what happened in this example, by the way, there's a little bit of noise. If anyone has any questions, just please feel free to ask, and I'll also monitor the chat box. So this is what happened on this example. On this example, we see that the second expert was wrong on the first day and predicted rainy, but the actual outcome was sunny. The third expert was wrong on the second day, and so on. So if we look at each of the experts, these red boxes indicate mistakes. We see that there's one expert, namely the first expert, that was only wrong on one day. So the error rate of the best expert is. Of the best expert is one divided by four. There are four days here. And if you look at what you did, you made these predictions. You were correct on the first two days, but on days three and four, you made a mistake. So your algorithm made two mistakes. The best expert made one mistake. And so the difference is one divided by four, the number of days. The regret is one fourth. Okay. So, yeah. So, yeah, so just to set up some notation, we have n experts who decide on a binary outcome, 0 or 1 on each of capital T days. The algorithm sees the expert predictions and then predicts either 0 or 1 on the day. And then the algorithm sees the actual outcome and can use this information on future dates. Days. Okay, and again, the cost of the algorithm is the number of incorrect predictions, and the regret is the number of mistakes we make minus capital M, which is the number of mistakes of the best expert divided by T. Okay, so just notation, we have N experts and T days. Okay, so yeah, there are many applications of the experts problem. So, to, for example, ensemble learning, there's the Adaboost algorithm, which if you're familiar, Algorithm, which, if you're familiar with it, it combines a bunch of weak learners into a strong learner. There's forecasts and portfolio optimization, and multiplicative weights and algorithms for the expert problem are used much more broadly in online convex optimization. So, what are algorithms for this problem? So, a classical algorithm due to Littlestone and Warmuth is the weighted majority algorithm. So, how does this work? Does this work? You have each of your n experts, and they start off with a weight, okay? And all their weights are equal to one in the beginning. And what happens is that when an expert makes a mistake, you penalize that expert by multiplying its weight by something less than one. So here we're going to multiply its weight by one minus epsilon for some parameter epsilon. Okay, so you see on this first day that experts one. Experts one, three, and four were correct because the actual outcome was sunny and they predicted sunny. The second expert was wrong. So we changed its weight from one to one minus epsilon. Okay. And so this is how we penalize experts. So we just, if they're rolling on a day, we scale their current weight by a one minus epsilon factor. How does our algorithm actually do predictions? Well, we just take the deterministic weighted majority. Uh, the deterministic weighted majority. So, what does that mean? You know, we add up the weights corresponding to sunny, and if that's larger than the sum of the weights corresponding to rainy, then we predict sunny. Okay, otherwise we predict rainy. So, you know, here the sum of sunny is three, it's larger than one. So, we're going to predict sunny and so on. Okay, so this is a classic algorithm for this problem. And what can you show? And what can you show about this algorithm? So, this deterministic weighted majority algorithm has a very nice guarantee. It says that the number little m of mistakes made by this deterministic graded majority algorithm is at most a multiplicative 2 plus epsilon times capital M, which is the number of mistakes of the best expert, plus 2 divided by epsilon times natural. Divided by epsilon times natural log of n. Okay, remember, n is the total number of experts. And here, epsilon is just any parameter that you want. Okay, so you can set it however you would like. You know, one sort of deficiency of this algorithm is that at best you're going to get a multiplicative two approximation, however you set epsilon. The proof of this theorem is a very simple potential argument. You just look at the sum of the weight. You just look at the sum of the weights of all the experts. On the one hand, it's at least the weight of the best expert, which is at least one minus epsilon to the capital M, because the best expert made capital M mistakes. Its weight got sort of scaled by one minus epsilon at most that many times. And it's at most n times one minus epsilon over two to the little m, which is the number of mistakes. Little m, which is the number of mistakes you make. And why is that? Well, each time you make a mistake, at least half of the weight was wrong, which means that at least half of the weight of all the experts is going to get scaled by a one minus epsilon factor, which means that the total weight of all the experts gets scaled by at most a one minus epsilon over two factor. And this happens little m times. Little m times. And so if you just solve, you have these two inequalities. If you just solve for the number little m of mistakes, you'll get this bound here for the deterministic weighted majority algorithm. Okay. Now there's also the randomized weighted majority algorithm, sometimes called multiplicative weights. And so this achieves a better guarantee. This achieves a better guarantee in expectation. So, the only difference here is that what you do is instead of predicting the, instead of deterministically choosing the majority weight, you just sample a random expert proportional to its weight and then output the prediction of that expert. And what you can show for this is that the expected number of mistakes of this algorithm is at most one plus epsilon times the number of mistakes of the best expert, plus again, order natural log n over epsilon. Natural log n over epsilon. And here you can set epsilon to be something very tiny and get a very low additive error bound, additive bound on the total number of errors. And yeah, so just to summarize all of this previous work, in the first bullet here, the weighted majority algorithm downweights each expert that is incorrect on each day and selects the weighted majority as the output. It makes 2 plus epsilon m. 2 plus epsilon m plus order natural log of n over epsilon total mistakes, the randomized weighted majority randomly follows an expert and predicts according to that expert. And if you set epsilon appropriately, then you can show that the regret of this randomized weighted majority algorithm is roughly order one over root t. Okay, so I'm going to mostly ignore log factors. Okay, and that's. Okay, and that's information theoretically best possible. So one over root t is the error rate. The total number of mistakes is roughly root t. Okay, so that's all well known and standard. And the question that we ask is how much memory do you need to solve the experts problem? And specifically in the data stream setting. Okay, so if you look at all these algorithms, these At all these algorithms, these algorithms require omega n memory, and why is that? Because they're maintaining a weight for each of these experts. And you know, what if the number n of experts is very large and you would like sublinear memory in terms of this parameter? Okay, so there are lots of applications actually where the number of experts can be very large. I mean, each expert can correspond to maybe a potential algorithm, and maybe doing some optimization is. Maybe you're doing some optimization, some search and some optimization problem, and each expert might correspond to a potential search path. Maybe it's the number of constraints in a linear program and so on. So n can be very large. And can you get algorithms that use sublinear and n memory? Now, of course, you can use no memory and just do something silly, like randomly guess each day. This might still be good if the best expert makes a lot of mistakes, but. Expert makes a lot of mistakes, but in general, this is bad if the best expert makes very few mistakes. And so, what are the space accuracy trade-offs for the experts? Problem? Okay, so I hope the setup of the problem is clear. All right, so yeah, and just the streaming model. I mean, you've already seen this earlier in the workshop, but just because I want to show you this cute slide. So, here is how we can view this. View this. You can view this each day on a day as waking up with no memory except maybe a note from your past self, which is at most some number S of bits. Okay, so you wake up, you have this note of S bits, S is your space. And what happens is you see the predictions for today. And you can imagine streaming through these predictions. That's what our algorithm would do. But from a lower bound, Algorithm would do. But from a lower bound standpoint, the lower bounds would hold even if you got to store all the predictions on a day, look at them, and then compress your information to S-bits moving on to the next day. But yeah, just imagine streaming through these predictions. You make a prediction, you see the outcome, and you write a note to your future self at most S bits, and you fall asleep and forget everything, and the process starts over. Okay, so this is the streaming model. The streaming model. And there are various notions, you know, sort of, you know, on the order of a streaming model. These are totally standard notions. So yeah, again, what we have is a prediction and an outcome. We have a set of predictions and an outcome on each of capital T days. And the standard streaming model is the arbitrary order model where an adversary might choose a worse case. An adversary might choose a worst case ordering of the days and outcomes in the stream beforehand, and this is fed to the algorithm. Okay. But a more benign model would be, say, the random order model, where an adversary might choose worst case set of predictions and outcomes, but then the order of the days is randomly shuffled. Okay, and why might this be natural? I mean, in many of these, you know, in many applications where the expert's problem might be relevant. Where the expert's problem might be relevant, it might be that the days are independent, right? They might be coming from a distribution. And then these naturally, you know, if the days are IID from some distribution, then this naturally gives rise to a random order. Okay, so these are the two models that we're going to focus on in the talk. So, just to build some intuition for this problem, what are some natural ideas you could try here? Well, one thing to try is to. Well, one thing to try is to just try to find the best expert, right? If I could memory efficiently find the best expert, then I could just follow this best expert forever. If I could find the best expert quickly enough with a small number of days and then just follow them forever. But this, you can show, requires omega n bits of memory. And it's a very simple reduction from two-player set disjointness. So, you know, in the set disjointness, So in the set-destruction communication problem, Alice has this set X, this n-bit string, and Bob has a set Y, again, represented with a characteristic vector, this n-bit string. And the promise is that either that these two sets, X and Y, they're disjoint, their intersection is size zero, or they uniquely intersect. Okay, they intersect in a set of size one. And the randomized multi-round communication complexity of this problem, as you As you know, is omega n bits. And the simple reduction to identifying the best expert for the experts problem, it holds already for two days. So you can think of Alice being the first day. So Alice has the end predictions from all the experts on the first day, and Bob has the end predictions from all the experts on the second day. Experts on the second day. If you want a general number t of days, you can just copy Alice's input t over two days, and you could copy Bob's input t over two days. So they can each represent t over two days. And the usual reduction to streaming is as follows. So Alice creates a stream S so that each element of X is an expert that is correct on day one. And she's going to run a streaming algorithm on that information and pass the state to Bob. And Bob's going to create a stream S prime so that each element of Y is an expert that is correct on the second day. So the sets are indicating who is correct on day one or day two. So let's just think of a few days. And yeah, so as I said, we have a usual reduction of Alice running the streaming algorithm and passing the state of And passing the state of the algorithm to Bob. And at the end, the algorithm is going to output some expert I, and Alice and Bob can just check if the intersection of those sets is equal to I. Okay, and so this solves set disjointness. I mean, technically, set disjointness is a decision problem, but this problem of identifying this potential position where they can intersect solves the decision problem. Where they can intersect solves the decision problem just by checking if there's an intersection there with negligible additional memory, additional space. So you solve set dystronius with omega n, you solve set dystronius, so you must use omega n bits of memory. Okay, so you know, you might look at this and think this is hopeless. You're not going to identify the best expert with low memory, but this is really not the end of the story. Okay, this is sort of the beginning of the story. You know, a low regret. You know, a low-regret algorithm need not find the best expert. Okay, and you know, particularly for this instance, what you could imagine, you know, think now of having T days where Alice has the first T over two days that are just copies of her input to set this jointness, and Bob having the next T over two days, which are his copies of his input to set this jointness. You know, what would you do here as a low regret algorithm? Well, you know, you might take a few days until you find. You might take a few days until you find an expert that is, you know, outputting one. And you would just follow that expert for roughly t over two days. And then you would start noticing that expert is doing bad around the t over two plus first day, t over two plus second day, and so on. And you would switch to some other expert that is outputting ones on the next T over two days. And so maybe you make a few mistakes in the beginning, and maybe you make a few mistakes around the T over two day, but most of the time you're always correct. And what you did is you just followed two experts. And what you did is you just followed two experts that were each correct on roughly T over two separate days. And you never found any best expert. And the best expert might be one expert that's correct on all T days. You never figured out any information about this best expert. And you use very little memory and you had low regret. Okay, so this is a question. I mean, can we? I mean, can we do significantly better in the streaming setting? Can we get a low regret algorithm without identifying the best expert? Okay, so these are our results. I'll start with a lower bound, which gives a trade-off between memory and regret. And what it says here is it says that any algorithm that achieves regret delta less than a half with Than a half with constant probability, must use at least n over delta squared t memory. Okay, so let me put this into context here. So if you remember what I said, the information theoretically optimal regret is roughly, you know, up to log factors, delta is one over root t. Okay, so if I plug in this regret into this memory bound, I see that I need omega n memory to achieve. I need omega and memory to achieve the regret that these classical algorithms, this information theoretically optimal regret, do achieve, which basically says I can't do it in the streaming model. So I need to store omega n bits of memory, which is basically storing a weight for all the experts. But this bound, it doesn't rule out, it's a lower bound, it doesn't rule out the possibility of a trade-off, right? So if I want a regret, So if I want a regret, which is maybe a small constant, then the lower bound only says I need n over t memory. Okay, and if I could match that with an upper bound, that would be a significant improvement over the, if the number t of days is large, a significant improvement over the end memory required of maintaining a weight for each expert. Okay, so this is the lower bound, and I want to stress that it holds not only for That it holds not only for the arbitrary order streaming model, but also the random order model, and even more generally, this IID streaming model that I sort of mentioned in passing earlier, which is just that the days are sort of the distribution on predictions on each day are independent and identically distributed. So yeah, so this is a very general lower bound. General lower bound. What about upper bounds? So we show that actually there is a matching upper bound up to log factors of n over delta squared t memory in the random order model. Okay, so the days are in a random order. And this goes all the way down to the information theoretic limit. So for any delta larger than Larger than root log n over t, this bound holds. And it, yeah, as I said, it matches the lower bound, which also holds in the random order model. The algorithm is oblivious to things that it shouldn't know, so it doesn't need to know the number m of mistakes made by the best expert. As I said, this third bullet is just saying that it goes down to the information theoretically optimal limit. Optimal limit. It does extend to general costs. So instead of just having binary outcomes and whether or not you're right or wrong, you can imagine having to make a prediction and getting a cost between zero and row, and you get regret which scales linearly in row. So I won't say more about that. But yeah, so this matches the lower bound for random order. And finally, we also get bounds for the Finally, we also get bounds for the arbitrary order data stream model, but they're not tight. Okay, so here's what we get. We have to make an assumption that the number m of mistakes of the best expert is at most delta squared t up to log factors. But again, it holds for any delta larger than one over root t. And if the number m of mistake satisfies this upper bound, then there is an algorithm. Then there is an algorithm that achieves n over delta t space and regret delta with high probability. So, just to understand this, if delta is a small constant, then this restriction on the number of mistakes of the best expert is not too severe. It's just saying the number of mistakes of the best expert is at most the number of days t up to log factors. So maybe it's not a severe restriction. Not a severe restriction. And in that case, we get n over t memory, which, again, is better than n memory. On the other hand, if delta is close to the information theoretic limit, one over root t, then it's saying, you know, the number of mistakes of the best expert, we can only tolerate it being something like, you know, a constant. But even then, when the number of mistakes is a constant, we still get. Is a constant, we still get a memory which would in that case be n over root t, okay, when delta is one over root t, which is still an improvement over n. And, you know, if you look at this bound, you might think something is a little bit off in the sense that I just told you that there's a lower bound of n over delta squared t. And delta is a parameter which is much less than one. Less than one, you know. So, what's going on here? This, this, this memory here is this upper bound is beating the lower bound. But again, we have this restriction that the number of mistakes of the best expert has to be at most delta squared t up to log factors. And so what this really is saying is that although this beats the lower bound, what this is saying is that the hardness from the lower bound is actually coming from instances where the best expert makes a lot of mistakes. Expert makes a lot of mistakes, okay, makes many more than this assumption that we're imposing to get this upper bound. Yeah, so again, you know, the algorithm is oblivious to hyperparameters like the number of mistakes of the best expert, and it goes down to the information theoretic limit, as I said. So, those are the results. I guess I should stop here and see if there are any questions. And see if there are any questions so far. Sorry. Does this question become trivial if you insist that your algorithm will be deterministic? Yeah, so I'll mention at the end, so for deterministic algorithms, you can actually show, and this is a follow-up work, and I don't know how to prove this in a trivial way, but you can actually show an omega n. But you can actually show an omega n lower bound for any number of days. Okay, even if the number of days t is much larger than n, it could be much larger than n, or as in the setting of this talk, it could be smaller than n. You can show an omega n lower bound and the for constant regret, okay, for delta constant. And I actually don't know how to prove this other than some using some recent work that we had actually. Work that we had actually on multiplayer set disjointness that works for that gives a lower bound for very tiny failure probabilities, in particular for deterministic algorithms. So, yeah, maybe I shouldn't get into the details. I can say more at the end of the talk, but yeah, I don't know how to prove this in a trivial way. It uses some recent results and gives omega n lower bound for deterministic algorithms for constant regret for any number of days. Regret for any number of days. Yeah. Yeah. And David, another question. The first bullet: can't you just get when the number of mistakes is high, then a random predictor is getting regret about delta squared in this case? Okay, so the number of the regret is a parameter. The number of mistakes, you know, could be something like half the days. But, you know, but the best expert might make t over two. You know, might make t over two minus delta t errors. And so delta is a parameter, right? You want to, your delta might be very small, even though the number of mistakes is very large. And so you would have to distinguish these two instances. You know, so random, yeah, so yeah, yeah. So it's still hard depending on delta. Yeah. David, can I ask you the definition of IID model? What was it exactly? Towards it exactly, uh, yeah. So, uh, I guess uh, when I present this lower bound, it will uh uh be clear what it is. Um, so yeah, I mean, so then I can wait if you want. Okay, yeah, yeah, yeah, yeah. Um, yeah, okay. Um uh any other questions? Do you assume that there are only two outcomes or it doesn't m really matter? Or it doesn't really matter? It doesn't really matter. For this talk, I'm just going to assume binary outcomes. Okay. Okay. Let me continue then. So yeah, I'm going to start with the lower bound. Okay. So yeah, as I said, the result is that there's an n over delta squared t memory lower bound to achieve regret. To achieve regret delta, and it holds for many different streaming models: arbitrary order, random order, and so on. Okay, so we're going to use communication complexity, and we're going to use what we're going to call a distributed detection problem. And here's the problem: I'm going to call it epsilon diff dis. diff dis problem. So there's a parameter epsilon to this problem. We're going to have T players. Okay, now each player is going to hold n bits. And so what you should think of is each player corresponds to a day in the experts problem. Okay, so the players are not experts. The players are days. And the player holds the n predictions of the experts on that day. Experts on that big. And we need to distinguish two cases. Okay, so there's the no case here. In the no case, every bit of every player is drawn IID from a fair coin. Okay, so everything is completely independent and random. And these are just uniform coin tosses. Okay, this is just a communication problem at this point. Okay, so I'll describe the connection to experts. I'll describe the connection to experts in a moment. That's the no case. In the yes case, what we have is there's an index L between 1 and N, which is selected arbitrarily. And the L bit of each of these T players. So you can think of, if you want to jump ahead to the experts problem, this is the Lt. And each player has the L prediction of that. The ELF prediction of that player, sorry, has the prediction of that ELF expert on each, you know, one player has it for each day. So this ELF bit is chosen IID from a Bernoulli distribution with parameter one half plus epsilon. Okay, so it's a slightly biased coin. And so this player imports their IID across the days. So on each day we flip a coin, it's heads with probably one half plus epsilon. Heads with probably one half plus epsilon, independent across the days. And all other bits are chosen IAD from a fair coin. Okay, so all the remaining n minus one experts have independent uniform fair coin tosses, just like in the no instance. Okay, so this is the problem. We have T players in a communication game, and they'd like to distinguish if they're in a no or a yes case. Okay, and this is actually the hard instance. Hard instance, I think, that Madhu was asking about. I mean, so this is the case when the best expert is actually making close to mistakes on half the dates, but slightly better as a function of this parameter epsilon. Okay, so what is the communication of this distributed detection problem? Yeah, so you know, just let me just say it again in case it wasn't clear. Say it again in case it wasn't clear. So, you have a player for each of the T days. You know, here we have four days, and on each day, you have the outcomes of the experts. Okay, so here there are five experts. This player has the five outcomes of the experts on the first day, this player on the second day. In the no case, all these coin tosses are independent and uniformly random. In the yes case, there is some planted expert. Okay, so this would be L equals two in the notation on the previous slide. In the notation on the previous slide. And on this planted expert, it's a little bit biased towards being one. So a little bit biased towards being heads here. And they want to distinguish, the players are going to talk to each other. They want to distinguish no versus yes with low communication. Okay, so that's the problem that we just defined. There's a natural, simple upper bound protocol. Protocol. So you can randomly choose one over epsilon squared players up to a log n factor and just exchange all the bits of those players and see whether some bit has bias at least epsilon over two, say. Okay, so you know, this is just the standard thing that if I want to distinguish these coin tosses, I need one over epsilon squared samples to distinguish bias. Bias one half plus epsilon versus bias one half. And if I can just union bound over all the n experts by putting a log n factor here. Okay, so that's yeah, yeah. What was the communication model? Who speaks in what order? Yeah, okay, good. So thanks. Yeah, so this is going to be a the lower bound is actually going to hold for multi-round communication. Multi-round communication, but we're not going to need that for our application. So, what we're going to only need is that the first player is going to speak first. It's going to send its message. Okay, so actually the lower bound holds even if the message is posted on a blackboard to all the remaining players. But in our reduction, the message is just going to be sent to the second player and then its message to the third player and so on. Uh, so yeah, so it's it's it's the Blackboard communication model where we're gonna prove this lower bound. Yeah, thanks, yeah. Um okay, so yeah, so this is uh an upper bound and you know this is just the picture, you know, we sample a few players and uh they exchange all their bits. Um, uh, yeah, so here in the Blackboard model, they would just post uh their bits. Um, okay, uh. Okay. Now, that, you know, so this is just the problem again. This is the protocol I said. And the communication of that protocol is, you know, not great. I mean, so it's n over epsilon squared. We have one over epsilon squared players sending n bits or posting n bits. And what you can show is actually there's a matching lower bound up to log factors of n over epsilon squared. Okay, so even if you just don't, you know, even if you try to do something more clever. You know, even if you try to do something more clever, maybe do some sketching algorithm or somehow compress what you've seen, you can't beat this simple sampling protocol. n over epsilon squared is optimal. And yeah, so why should we think this is the case? Some intuition. As I said earlier, you do need one of our epsilon squared samples to distinguish between a fair coin and one with bias epsilon. And one with bias epsilon. And so the intuition that you should have in mind is: well, the players roughly need to solve this single coin problem on each of the n coins. They need to figure out on each of the n coins, you can think of it as the column of this matrix that I had in this picture earlier. Is it a fair coin or does it have bias epsilon? Now, technically, they don't need to solve the problem on each of these. On each of these end coins, like they don't need to identify which coin had the bias, if there is a coin with a bias. Really, all they need to do is just solve the OR of n instances of a single coordinate problem, where the single coordinate problem is just saying, you know, am I a fair coin or do I have bias epsilon? Okay, yeah, so formally, all the coin. Formally, all the coins are independent in the no distribution. Okay, all inputs are just independent coin tosses. And so, what you can use, what we use to prove a lower bound, is a direct sum theorem for an OR of n instances of a smaller problem. So, this is old work of Baryosev et al. using the information complexity framework, which is introduced earlier by Chakrabodi et al. Introduced earlier by Chakrabody et al. So this reduces to showing high information cost under the no distribution on a single coin. Okay, so what is this going to mean? So information here is going to be additive. So what we'd like to do is show that the information that the protocol reveals about one of these copies has to be. Copies has to be, you know, one of these n instances of the coin problem has to be large. The information has to be at least one over epsilon squared. And what does information mean? It's the information that the transcript of the protocol reveals about the specific coin tosses on that instance of the coin problem. Okay, so just the mutual information between the transcript. So the mutual information between the overall protocol transcript and And you know, so what we're looking at overall is we're looking at all of the coin tosses. Okay, so we'll have n coin tosses on the first day, you know, and then we'll have n coin tosses on the second day, and so on out to T days. And we're looking at this sort of all of this information. The mutual information lower bounds the entropy of the transcript, which lower bounds the transcript size. So, this would give us a lower bound. And using this direct sum framework, we reduce it to showing a lower bound on the single coin problem. So, we're just going to look at one of these days. And what we'd like to argue is that for one of these days, this mutual information is large. Mutual information is large. One over epsilon squared. Sorry about this. And I have, sorry, not one of these days. Sorry, one of these experts, one of these columns, one of these coin toss problems. We'd like to show that the information is one over epsilon squared. And then using this direct sum framework, we would get an n over epsilon squared overall lower bound. And so what this really boils down to is showing this one over epsilon squared information for the C. Information for the single coin problem. And yeah, so again, what is this problem? Here we have T players, and each player is getting an independent sample, either from a fair coin or from a coin with bias epsilon. And these players need to talk to each other. And what we care about is how much information they're going to reveal about their inputs. And we need to measure information under the no distribution. Information under the no distribution. Okay, what does that mean? Well, in this reduction to the single coin problem, what we're going to get is a protocol that is correct distributionally, meaning that its correctness guarantee is such that when, you know, if with probably a half, I have bias epsilon and probably a half, I have a fair point, then it's going to have, you know, 90%, say, 90% probability in being correct under. Percent probability in being correct under that input distribution. But what we're measuring information with respect to in this mutual information is under the no distribution. Okay, so these coin tosses are always IID coin tosses. But you know, correctness is with respect to the this mixture of a yes and a no distribution. So we use that, you know, you can use that in the proof, even though the information is only measured with respect to a no distribution. This is all sort of the standard trick of Bariosev et al. Of Bar Yosef at all. And using so-called strong data processing inequality and min information cost, which allows us to lower bound the information under the no distribution. You can use existing work to show that there's a one over epsilon squared information lower bound for this coin problem. So this goes back to work of Ducci et al., who showed this first for one round protocols. One-round protocols. This is actually all we need here. But there was a nice follow-up of Garg et al., which extended this, the multi-round case and sort of cast it more in the familiar direct sum framework in theoretical computer science. And there was a follow-up work by Braverman et al. in the sparse case. But just using these results, you can show one of epsilon squared is a lower bound to the single coin problem. Squared is a lower bound to the single coin problem, and overall, you get this n over epsilon squared lower bound for this problem. I'm a bit short on time here, so I'm not going to say more about that, but let me just say how this is used here. So, yeah, so I'm just summarizing what we have for this distributed detection problem. We show that n over epsilon squared communication, in fact, information is necessary to solve. Information is necessary to solve this problem, but it doesn't immediately apply to the experts problem. So, you know, what is the issue with the experts problem? So, yeah, again, yeah, so in the reduction to experts, each player in this communication problem corresponds to a different day. Each bit in the epsilon communication problem corresponds to a different day. This communication problem corresponds to a different expert. And we like a reduction, which says that if we have a low-regret streaming algorithm, which is run on the first day, it makes a prediction, and then it saves some state, that state is passed on to the second day, and so on. We'd like a low-regret streaming algorithm to solve this communication problem. Now, we can't just have the ones in the ones in the inputs correspond to the to you know to the to the predictions of the experts you know why is that well i mean the uh it would be very easy in this case okay so let me just you know give you this uh this picture here so if if we have some coin which is biased towards uh outputting uh uh you know towards being one well your algorithm it could just always output one Your algorithm, it could just always output one. Okay, it doesn't even look at the input. So, you know, it's not solving anything. You know, if it's if you're in a no instance, you know, there's no good expert. So, you know, it's fine. If you're in a yes instance, there is a good expert, but I didn't find anything out about it. I just always output one and I did pretty well. So we can't just directly reduce from the communication. Directly reduce from the communication problem. Instead, what we're going to do here in this reduction, yeah, so as I'm saying here, an algorithm with, you know, that doesn't learn anything about the input can still have good cost if we just have the bits and the inputs to the players correspond to the predictions of the experts. So, what we can use here is a masking argument. The idea is. The idea is: well, each player in the communication problem is going to mask the prediction of the experts by a random bit, which is unknown to the streaming algorithm. Okay, so let me, and the, and the, so the predictions are going to be flipped by that bit. So it's one bit per day. A random bit is chosen. All of the predictions on that day are XOR'd with that bit. That day are XOR'd with that bit, and then the outcome is also XOR'd. Okay, so it's not going to be that one is always the right prediction to make. And so let me show this. So it's a randomized reduction here. Okay, so this is what we started with. We said that, oh, here's this bias coin. So say sunny is corresponds to one. Then, you know, all I have to do is I just output sunny on every day and I'm And I'm, you know, pretty, I'm correct, you know, pretty, pretty often without learning anything about the input. But what we're going to do instead is on the on the first day here, the communication player is going to choose a random mask. It's a bit which is 0, 1. If the mask is 1, it's going to flip all of these predictions. Okay, and the streaming algorithm, it doesn't know what this mask was. So it just sees all these predictions. So it just sees all these predictions and it has to make its prediction. We also flip the, you know, so we flip each of these predictions, which are the inputs to the distributed detection problem, and we also flip the actual outcome accordingly. Okay, so that means, you know, if the expert is right one half plus epsilon of the time, then it's still right after we do this masking. It just makes it harder for the streaming algorithm to know what, you know, What to predict on that day. So, sort of the communication player is choosing this and hiding it from the streaming algorithm that it's running the inputs on. On the second day, the mask was zero, so we left everything alone and so on. Okay, so why does this masking argument work? Well, here is the reduction. So, in the no case, all the coins are unbiased. So, we all we have. Are unbiased. So we have fair coin tosses. And this random mask, also, prediction is random and independent of all the coin tosses. And so no expert, there's just no algorithm that can do better than being correct on one half plus delta over three days. There's just no algorithm at all because all the bits are independent. Because all the bits are independently random. And by turnoff balance, there's no algorithm here that's correct. On the other hand, if you're in a yes case, then there is this bias cove. And this masking preserves the correctness of this best expert. It's still correct on at least one half plus delta days. And so, you know, the streaming algorithm is going to be predicting the right outcome on a non-trivial fraction of days. Non-trivial fraction of days, you know, better than what random would be predicting for any algorithm. And so, what we can do is we can look at the output of the streaming algorithm and see if it is correctly predicting the output of the day. What is the output of the day? It's one XOR with that mask. And we can just see, you know, was the streaming algorithm correct on that day? And we can keep track of the fraction of days that the streaming algorithm was correct on. And if it's correct on a good fraction of days, then it means that, hey, there actually was a very good expert there. Very good expert there. And so, therefore, we can distinguish these two cases. Okay, so just to summarize this reduction summary, we had this communication problem. We proved an n over delta squared total communication lower bound wherein the epsilon parameter of that communication problem was order delta, the regret. And setting epsilon to be order delta, we had a reduction to the streaming setting. The streaming setting. If you have this total communication, then it means on average for one of the days, the memory was at least n over delta squared t, and that's how we get our streaming lower bound, our memory lower bound. Okay, so that's the whole lower bound argument. And going back to Sefer's question, so I'd like to say that this lower bound holds in the IID model. And you can see what this model here is, like you have a distribution that's defined for each of the experts. That's defined for each of the experts. So, you know, for all but one of the experts, it was a fair coin. For one of the experts, it was a biased coin. And then you're getting random samples of that joint distribution of N experts on each of the T days. I hope that answers that IID question. Okay, any other questions about this lower bound? Let me ask you one other thing. Why did you need to mask it every day separately? Every day separately, like if you just do it once, what would go wrong? Um, so I guess, like, you know, another thing you could think of is, so really all we need here is to hide this from the streaming algorithm, right? So you could imagine having a fair coin versus bias one half minus delta or one half plus delta. In the no case, we would have completely random coins. We would have completely random coins. And in the yes case, again, as long as you have this mask, so the sort of the streaming algorithm doesn't, you know, in the yes case, you would have to still be correct or not on a good fraction of days. And it would, as long as it sort of deviates away from what turnoff would say. So in that case, it would say that any algorithm would be both both uh uh correct and incorrect on very close to a t over two fraction of days and and you know so this as long as you get bias away that that would also that could also go through yeah all right thanks um uh any other questions here okay let me just briefly talk about upper bounds uh i think i started uh maybe five minutes after so i don't know uh feel free to stop me at any time um so um Um, so, um, uh, yeah, so uh, uh, here is the um, let me start with some intuition for the no-mistake regime. Um, so this was the result that I'm claiming for the arbitrary order model. Uh, that again, if the number of mistakes is not too large, then there's an n over delta t upper bound. And so, the idea is, you know, we know there is a really accurate Is you know, we know there's a really accurate expert, uh, and um, you know, in the no-mistake regime, there's an expert that makes no mistakes. And so, what if we iteratively picked pools of experts and deleted the experts if they run poorly? Okay, so it's a natural idea. Maybe we just choose a subset of experts and follow them. And if they are not doing well, then we throw them away and we choose a new pool of experts. Choose a new pool of experts. Okay, so the natural idea here is: maybe I'm going to choose these two experts. And I noticed that, hey, these were the actual outcomes on these two days. And these experts, each of them was wrong at least once. I don't like them. I'm going to throw them away and I'm going to choose these two experts. And I'm going to follow them for a while. Okay, so yeah, so. Yeah, so if we iteratively pick a pool of the next K experts and output the majority vote of the pool while deleting any incorrect expert, then each pool will have at most log K errors, right? So suppose we choose K experts, maybe even deterministically. And every time there's a mistake, it means half the experts were wrong. And we're in the no mistake regime, which means that Regime, which means that those half-experts couldn't have been the best expert. We just throw them away. After we make log k mistakes, we've thrown away all of the experts. And maybe then we choose deterministically the next k experts, maybe in some lexicographic order. And so, what will happen? Well, at some point, we're going to choose the best expert. And in the no-mistake regime, we'll never throw away the best expert. And so, we'll continue to be creative. And so we'll continue to be correct forever once we've sort of whittled down that pool of K experts containing the best expert down to the single best expert. And so what is our total number of mistakes? Well, we want to achieve regret delta, which means the error, number of errors is delta t. And if we use, say, n over k pools, then each pool is giving Each pool is giving log k errors. And so we want to set the total number of errors that we get, n over k log k to be delta t. Why did I parameterize this as n over k pools? Because in each pool, you're choosing k experts. And so after n over k pools, you will have chosen the best expert if you walk through these experts in lexicographic order. So, you know, if you just solve this equation, you will see that the pool size. equation you will see that the pool size k should be at most n log n over delta t uh okay so you know what does that mean for us well for us we're you know keeping track of a pool of this number of experts this is our memory bound up to log factors so so yeah with n over delta t memory uh we can uh make at most delta t mistakes and we will find uh the best you know we will actually yeah we might not We will actually, yeah, we might not find the best expert. It might be that we just don't make many mistakes and we make at most this number of mistakes and we use this memory. Or at some point, we'll find the best expert and just follow them forever. Okay, so that's the no mistake regime, which is pretty easy. So yeah, all we do is iteratively pick these pools of N over Delta T experts, output the majority vote. Output the majority vote, and we delete any incorrect experts. And the intuition is if the number of rounds is small, the pools must have done well. So the overall regret is small. And on the other hand, the number of rounds cannot be large because at some point, the best expert would have been chosen and it would be retained because it never makes a mistake in this regime. Okay, so that's sort of a warm-up case. Now, this is the same algorithm as before, but now let's suppose the best expert makes capital M mistakes. Then we can simply tweak this algorithm to instead of using N over K pools, we use N M over K pools, right? Because we're going to walk through the experts. And if we walk through, if we have K experts in each pool, after this will walk. After this, we will walk through each expert m times. After we've walked through the best expert m times, we'll retain it forever, right? Because it only makes m mistakes. So now we use nm over k pools. We again want delta t errors. We solve the number of experts we need in each pool, and we get a memory of nm over delta t, which is not very exciting. The memory is scaling linearly in the number m of Scaling linearly in the number M of mistakes of the best expert. If the number M of mistakes is order T, this memory is super linear. So how can we do better? Here are some fix-its. So first of all, we're not going to walk through the experts in a deterministic order. We can randomly sample pools of experts. Of experts instead of iteratively picking pools. Maybe this doesn't help so much. If you have the hard instance in mind, you can imagine choosing this biased coin somewhere uniformly at random. This might not help too much. But there's a problem here that what if at some point you sample the best expert and it happens to be in this portion of the stream like this. The stream, like this area, this region of days where the best expert is making a lot of mistakes. Okay, you know, we waited, we took a lot of samples. Finally, we got the best expert, but it's at a really horrible time and it's making a lot of mistakes. We don't view it as a good expert. We throw it away, and then we have to wait even more time until we get the best expert again. There's also kind of an error which is with this algorithm, which is just deleting experts that have erred some fractions. Just deleting experts that have erred some fraction of the time, which is that there can be a buildup of errors. Let me say what I mean by that. So, you know, here's kind of a bad case study where I have these eight experts and here are our days. Sorry for transposing things on you multiple times, but you know what is happening. Know what is happening. So, on the first day, all of the experts are correct. On the second day, all the experts are correct. And then on the third and the fourth day, these four experts were wrong on both days. And suppose that I'm throwing away experts when they're wrong half the time. So, at this point, I would throw away these four experts. But suppose on the third and the fourth day, these four experts were still good, right? And so, these four. right and so these these these these experts down here they have some history of goodness right they're they're good on four days so far um and and so now it's going to take you know even more days for uh uh for these experts to be thrown away because they're thrown away when they're bad on on a half fraction of days since they were put into the pool. So these guys are already the bell. But it's going to take four more days. But you know, it's going to take four more days where these two experts are bad, and then we're going to throw away these two, and it's going to take eight more days here until we throw away these. Anyway, if you just look at this picture, we're only correct on two days, and we're bad on 14 days in this picture. You can fix this sort of accumulation of error very simply just by instead changing the algorithm a little bit. So you again repeatedly sample a random. Again, repeatedly sample a random pool of roughly n over delta t experts. You again output the majority vote of the pool, but now you delete any expert that's that has accuracy less than one minus delta over log n. Okay, so just changing this threshold fixes this accumulation of error problem. But we still have these main problems, which is we might sample an expert on a day for which it's bad. It's bad. And so, overall, the intuition that's sort of governing all these algorithms is what we like to show is if the number of rounds is small, then the pools must have done well. So the overall regret is small. And the number of rounds cannot be too large because at some point the best expert would have been sampled and retained. Okay, that's what we'd like to show for both our algorithm in the adversarial order and also in the random order. And I'm out of time, so let me just. I'm out of time, so let me just say roughly how it works just at a very high level. The idea is: well, you know, the best expert, it screws up on M days, right? It makes mistakes on M days. And remember, in our arbitrary order model, we assume M is not too large. Now, what's going to happen is if we're going to resample a pool of experts, well, when is it? Well, when is it a bad day to resample a pool of experts? Where a bad day means we actually sample the best expert, but then throw it away because we don't think it's the best expert. Well, if the best expert is making mistakes on M days, we have to actually sample the best expert in a region near those days where it makes a mistake. It has to sort of be in a sort of a bad density region. Density region of days. So, it's making a mistake here. And if we sample the best mistake, the best expert here, we might throw it away because we'll see after these two days, oh, hey, that expert made a mistake half the time. It's probably not the best expert. Let's throw it away. You know, these are days where the best expert didn't make a mistake, but if we sampled it, we would still throw it away. And that's kind of a bad thing. These boxes here, if we sample the best expert, we would keep it because sort of. We would keep it because sort of the fraction of days it's good on looking forward is high. And since the number of mistakes the best expert makes is small, not too large, and being in these high density regions, you can argue that the number of bad days is at most a one over delta fraction larger times the number of mistakes of the best expert. You kind of have to be in this high density region up to this factor of one over delta. Up to this factor of one over delta. And so then the idea is: well, you know, to make a lot of mistakes, I have to have a lot of rounds. You know, rounds meaning I'm going to resample a pool of experts. And if I have a lot of, you know, because we said if we, if we, on any given round, we don't make many mistakes because, you know, we throw away experts if they're not doing well. So in order to make a lot of mistakes, we have to have a lot of rounds, resamplings of experts. And if we have a lot of resamplings. And if we have a lot of resamplings of experts, then it's likely that we're going to resample an expert on one of these good days. And it's likely we're going to sample the best expert on one of these good days. Okay, so that's a calculation that needs to be done. And I should mention here, the distribution here is horrible. Like, you know, the days on which we resample, it depends on the entire history of what the algorithm has done on the experts it sampled in the past. On the experts, it sampled in the past, and so on. So, we don't have much control over the next day that we're going to sample a good expert on. Like, we don't understand this distribution, it depends on the entire history. So, all we're really using in this arbitrary order model is that there are enough days on which we're going to resample a poll of experts. And there are so many days that we're actually going to sample the best expert on one of those days. That's what we're using in the arbitrary order model. The random order model. The random order model, everything is much nicer because intuitively, what happens locally is the same as what happens globally. Okay, since things are in a random order, the best expert locally, it makes the same fraction of mistakes does globally. And so now we just need to sample the best expert on one day. We don't have to worry about sampling it on a bad day, one of these days where it is in a dense region of bad days. Of bad days. So I hope that intuition, you know, that gives you some intuition of how this works. And let me just wrap up here. So just move to the conclusions. Yeah, so just to summarize, oh, did it just put me back? Oh, did it? It just put me back to where I was. I'm sorry. Yeah. So to summarize, we showed this lower bound, this trade-off that n over delta squared memory is needed to achieve delta regret in arbitrary order, random order IID model streams. For random order model, there is a matching upper bound up to log factors. Matching upper bound up to log factors of n over delta squared t memory. For arbitrary order, if you assume the number of mistakes of the best expert is not too large, then you can get n over delta t memory with regret delta. These bounds do generalize to arbitrary costs. What are natural questions here? So, a natural question is: can you get tight bounds for arbitrary order streams? Okay, so we don't know. Okay, so we don't know what is happening here. This upper bound actually beats this lower bound, but it makes an assumption that the number of mistakes of the best expert is not too small. And yeah, sort of how general is this framework beyond the experts problem, right? Algorithms for the experts problem are used for things like linear programming and so on. Linear programming and so on, what else can be said here in the streaming model? And just some follow-up work. So as I mentioned in a response to a question, there is some follow-up work that says any deterministic algorithm must use omega n bits of memory to achieve constant regret for any number of days. And we, so this is just a So, this is just a manuscript at this point. We use a recent high-probability or deterministic lower bound for multiplayer set disjointness, which I don't know how to prove this without using that. So yeah, so deterministic algorithms can't really do much in this model. A very interesting. Oh, sorry. Another follow-up work here is. Follow up work here is if you want some sort of adversarial robustness. So, a lot of studies in these online learning settings allow the adversary to look at the predictions of the algorithm and design future inputs based on those predictions. So, our algorithms don't allow this. The randomized algorithms that I described, for the reason you can imagine. For the reason, you can imagine that the adversary might learn something about the pool of experts that you've sampled and sort of try to hide the best expert from the algorithm. But so we can't allow the adversary to see the state of the streaming algorithm, like see the pool of experts that are sampled in our particular algorithm, but we can allow the adversary to see the outcomes. To see the outcomes with a slightly, you know, with a worse memory. So we can get n over root t memory and actually handle adversaries that see the outcomes of the streaming algorithm on each day. And this uses a connection to differential privacy. And we don't know if this is optimal. So again, this is a nice open question. And in some very recent follow-up work on archive, they consider the setting where the number n of x. They considered the setting where the number n of experts is much smaller than the number t of days. And they were able to get an algorithm where the memory was t to the two over two plus delta. Oh, sorry, the number of errors is t to the two over two plus delta. And the memory is n to the delta for any delta between zero and one. Okay, so you know, if you have, you know, a root n memory, for example. Have a root end memory, for example, then you would get sublinear in T, T to the two of