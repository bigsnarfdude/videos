Collaborators from UCAM in Quebec and some people at SFU. So I was really hoping for the other talk from Share Show because it would be a perfect motivation for mine, but it didn't happen. So hopefully, it will still connect fairly well. So imagine you have maybe a bunch of hospitals co-training machine learning models, maybe with federated Learning models, maybe with federated learning, maybe even through a trusted third party, so they don't directly see any of each other's gradients or data or anything. And at the end of this procedure, where they co-train a machine learning model based on their joint data set, they each receive the final model to maybe make predictions, each on their own patients. And here we are going to focus on one of the participants. To focus on one of the participants of this co-training, so they have their own data, they have the model they received in the end, and what they're wondering is what could others learn from that model or even the predictions of that model. All right, so they, you know, they participated in that co-training, so their data is somehow included in that model, and they're wondering: well, you know, can we learn something? Well, can we learn something specific about my patient's data that I'm supposed to protect legally, morally, everything? Okay. And they're asking that in a context where we know that we can use trained machine learning model to do a lot of privacy attacks under training data. So, for instance, we can learn, in many cases, membership in the training set. So, this is one example. There's many. This is learning. There's many. This is learning membership for images in a computer vision model. There's good examples learning membership in medical studies from summary statistics released, I think, from gene expression data. There's like a whole literature on membership inference attacks. We also know that from some model, we can reconstruct a bunch of the training data. One of the most recent ones cited here. Here recovered like more than a terabyte of training data from GPT-4 or 3.5, but the actual one deployed in production. So we know that it happens. And so in this context, the hospital is wondering, well, for this model that we train together, for my data, what can we learn? What kind of privacy leakage am I susceptible to, given that I could train this model with all of Train this model with all these other people. I have a quick question. Is your train model released to attacker? Yes. Oh, okay. So the attacker could be another site receiving the model, or it could be essentially anyone who has query access to the model. Because we also know that just having access to the model, maybe through an API, you can just send data and get the predictions back is also a Can I do the protection of train model? I have the issue of can you protect your train model? Well, here they're co-training, and essentially in this case, the deal is like every hospital get the model in the end, right? They're co-training not for only one hospital to get the model, but for all of them to have a better model. So, the other hospitals do have the model, like, there's nothing we can do against that. And you're wondering, like, what. And you're wondering, like, what can they learn on my data, for instance? It's a longer debate, but protecting models is essentially security through obfuscation. It doesn't work. It's not done for a while in the security field, and it's not a very good way to maybe that's more my opinion, but let's leave that on the side for now. And, you know. You know, to quantify that kind of privacy loss or privacy risk that this hospital's data has through the model that was co-trained, we are going to use the formanism of differential privacy. So we've had a nice introduction for differential privacy this morning. I'm just going to focus on the stuff that's directly relevant for this work. We are in the setting of pure differential privacy. In the setting of pure differential privacy, so we have this epsilon parameter quantifying the privacy loss by bounding the likelihood ratio between any output of our differentially private mechanism if we change one member in the list. What does it mean in our case? The differentially private mechanism or potentially differentially private mechanism is the mechanism that trains the machine learning model. So we kind of know that if we train the machine learning model, That if we train the machine learning model with differential privacy, the other hospitals wouldn't be able to reconstruct exact data points from our training set. They couldn't run membership attacks. So it's kind of like the formalism we use to try to quantify privacy loss in our setting. It's a good definition for privacy, at least in some contexts, because it has really good properties and it's pretty much. And it's pretty much the lower bound on definitions of privacy that have all those good properties. I'm not going to expand on that too much. And when I mention privacy loss in this talk, it will be this epsilon, which is an upper bound on this log likelihood ratio. So essentially, as we've seen, a large privacy loss, large epsilon means a lot of privacy risk for my data, a small epsilon means. Data, a small epsilon means I'm very private, and so I wouldn't expect anyone with access to the model to be able to reconstruct or carry membership attacks on my data from that model. And a key property that we are going to leverage in this work is that there is a hypothesis testing interpretation of privacy based on membership inference attacks. Inference attacks, where you're going to say my null hypothesis is that data set D was the data set I used to contribute to the model, and the alternative hypothesis H1 is D prime, which is the same data set but with one other data point or one fewer data point. Okay, so those are the neighboring data sets of differential privacy. And essentially, Essentially, what differential privacy does is upper bound the power of any test at a given level that you carry based on the result of a differentially private meeting set. So if you're going to run this hypothesis test, carry this membership inference attack, and you're going to carry it at a given level, so you want to be wrong only 5% of the time, say, then if the result of Then, if the result of the computation is differentially private with parameter epsilon, then your power is bounded by e to the epsilon times the level of your cost. Okay, so those are known results. Essentially, what it says is if you want to carry a membership inference attack seen as a hypothesis test, using information from a differentially private mechanism, you cannot have a high-power test. Test. Right? Note that here, the randomness in this test is over the mechanism, so the training process of our model. So that's how differential privacy is defined. You add randomness in the computation, and then essentially this bound holds over the randomness that you add in the computation. Okay, so what does it mean? Okay, so what does it mean for our hospital if they want to audit the privacy leakage through that model? Well, what it means is that to audit this model for privacy, to try to measure how much privacy leakage they can expect of their data through that model. Essentially, they can use membership inference attacks to kind of like look. Inference attacks to kind of like lower bound the privacy loss. So they can say, based on my measurements, the model that I observe is incompatible with the model that would be epsilon differentially private for my model. So essentially, they can prove that the model is not privacy-preserving for their data by doing the following. They can train this model for each of their data. Model for each of their data points. They can retrain this model with or without the data point. So it's really in the training set or not. And then they can run a hypothesis test or a membership in France attack to try to guess whether this retrained model has X in the training set or not. And if they're able to do that with too much power, they essentially prove that. They essentially proved that this model is not compatible with a model that was trained with epsilon differentially private. And so, what this means is that they have at least that much privacy loss from the model to the other hospitals or anyone with access to the model. If they don't succeed in doing that, what they can say is: we tried and at least we can't show the list of projects. We can't show that it's not a project. So that's what an audit looks like. Yeah, keep trying until you get it. You and you can keep trying, but you can't ignore your failures, right? So these are like false positive rate and true positive rate over your trials. So if you if you fail for a while, then you will need to succeed for a while to violate the the burn, right? So The bar, right? So, yeah, you can keep train, but it's not going to be an issue here. The issues, though, will be that you need to retrain this model many times, and because it was trained with other participants, they need to agree, to cooperate, to not cheat and like really retrain with all their data many times per data point in your data set. points in your data set and so an audit is like really this way is really like not doable in practice and you're also like somewhat auditing the training algorithm more than the final model because you retrain a new model every time to carry your membership inference attack. There are ways around that so that's less of a problem here and there's another And there's another more practical issue, which is that typically to train a good membership inference attack or to be good at your hypothesis test, you will need to retrain the model even more just to train a model that can do the membership inference attack. Okay, so this in this setting is completely not doable. This middle point is because to make it more efficient, you can also poison the data set. So you can try to add data points. Add data points that are easy to detect so that you have a stronger hypothesis test. But now you're really changing the model. You need fewer retrainings, but you're not auditing the final model. You're auditing the algorithm that trains the model. So it's a slightly different semantic, and it's not the one we want. All right, so this doesn't work. Recently, there's been a very big progress in privacy audits. In privacy audits. So, this is a Neuribs paper that got some award for essentially managing to do privacy audits while retraining the model only once instead of many times like we've seen we would need with the normal bound on the power of tests with differential privacy. And the way it worked is to say that you can average over some randomness in the data. Some randomness in the data instead of the randomness in the training of your model. And essentially, you can define a privacy game where instead of having just one data point that differs and you need to guess that with high probability over retrainings of the model, you're going to take a bunch of data points, M, and then you include them all independently with a half probability. You're going to predict whether each of them is in the model you trained or not. So you train one model based on this data set where you have like inclusion with health probability. And essentially what they're saying, what they're showing is that if you're better at guessing than a random guess with probability e to the epsilon over 1 plus e to the epsilon, it's like the same thing as the randomized response we've seen. That's. That's kind of expected. We won't go into the details. Essentially, what this is saying is: if you're too good at guessing which data points were actually in the model out of your M data points, then the model cannot be epsilon differentially for your data points. All right, so now you're averaging over data, you retrain only once, so it's more practical. However, you need to play this game with a lot of data points to get a good audit in this case, and so you're changing the. And so you're changing the training set of your model. So the issue is that you would need to essentially not contribute half of your data in expectation to the federation to be able to later audit it. And so you can audit the model, but then you have a worse model because you're not putting your data into it. And if everyone is doing that, you don't have all the data in your model. So essentially, you're changing the model that you want to edit just to be able to edit it. Audit just to be able to edit it, and that's also what we want to avoid. Alright, so how do we kind of like get what we really want in this setting where we really want to edit the model we want to train and we want to train it with all our data and we want some kind of like privacy decay measurement. So, what we're going to do is to say, well, what we have is a model. Is to say, well, what we have is a model and a bunch of data that we know is in the model. So we have member data. The problem is that we want to run a membership inference attack and we don't have non-member data. So if we guess in all the time, we'll write all the time because we know we have only data that's in the model. So what we're going to do is train a generative model on that data that we know. Generate new data with that generative model. And essentially, we'll use the generated data, the synthetic data, as non-member data to run our membership inference setup. So, what's cool now is that we have a bunch of real data that's in the model and a bunch of generated data that's not in the model that we can use to train a membership attack model with a lot of data. And then we can test it. And this test, if we are too good at guessing what data is in the Guessing what data is in the model, then we are hoping that it's symptomatic of privacy decay. Okay, if I stop here, something is very wrong. Can anyone tell me what? So imagine my generating myself. So, imagine my generative model is really bad and it outputs like black pictures all the time, like the zero image all the time. Then, my membership inference attack will be very easy, right? Because I output the same picture all the time, so I'd be very good at guessing which one is a member and which one is not a member. But does that show that my model leaks any privacy? No, it doesn't, right? It's just because I'm testing between members. I'm testing between member images that are from a distribution of images and non-member images that are from a completely different distribution. They're very easy to distinguish. I've learned nothing about the leakage of member data through the model. So here, what we need is to add some kind of baseline that will try to distinguish synthetic data from real data. Synthetic data from real data, so non-member data from member data, without looking at the model at all. Alright, so we have one model that carries a kind of like membership attack, but without looking at the model we want to admit. And then we have another one that does the same thing, but by also looking at the model we want to edit. And if the membership attack is much better when it gets to look at the model than when it doesn't, At the model, then when it doesn't, then we are hoping that this difference is essentially measuring the privacy leakage through the model. All right, so that's the high-level process. Then we'll get a bit more technical to see what it means actually to do that, how we can formalize it. But if there is any question on the setup first, All right, high level, we have membered data, a model to audit. We take some of the member data, train a generative model, generate data to get this non-member data, and then we do the normal auditing pipeline, but with the base pipeline. Exactly. Thanks. That's like the next slide. Thanks, that's like the next slide. So, here to formalize what it means that we are measuring, we need a way to assess the goodness of our model. Okay, this is the privacy game. So, sorry, it was in two slides, not one slide. So, slightly more formally what I've shown before, we are going to sample with probability half whether we want a member or non-member, like in the previous work. In the previous work, except instead of taking an image and really putting it in the training set or not based on this, we're going to say if we want a member, we'll take a real member from our data set. If we want a non-member, we are going to take a generated data from the model. And then we're going to run our membership inference attack that tries to guess whether we have a member or non-member or a generated mod generated image or not generated or read image. Or read the image. Okay, the problem is that now instead of like having images from the same distribution and putting them as member or not, we have something from two different distributions and we put them as member or not. But all members are from the real distribution, all the members are from the fake distribution. So there's two ways for the membership inference attack to be good. Either it's good at detecting fake images, or it's good at Images, or it's good at doing membership on the model, which is what we want to measure, which is the right SS. So, essentially, to formalize all this, we need to start with a definition of what it means for the generative model to be good. Because if we can measure how good the generative model is, it's essentially our baseline, then we can measure the difference of how much even better you can be if you know the model and that's. You know the model, and that's your measure of viability. So, we have this definition of model goodness. Essentially, we say that the generative model is good if it puts a high probability to real images, to real data. So essentially, you're saying, well, your generative model can generate bad images, but it needs to generate good ones in the sense of ones that are likely under the. In the sense of ones that are likely under the true data distribution, often enough. And often enough is parametrized by this C e to the minus C parameter. So we call it C closeness. So the generator is C close to the true data distribution. It's like a one-sided definition of pure differential privacy. That's not surprising, is because that's what we were in the proof. That's what we were in the proof because the proof uses differential privacy to bound the power of a test. And so here we can plug that in when we have the right definition for what a good generative model is. It looks very strong. You're saying, well, if your data is very likely under the distribution, my generative model has to put, you know, can put a lower probability, but not too much lower. It's still very easy to do. It's still very easy to do a generative model with C strictly less than infinity. The uniform generative model will have C strictly less than infinity because it puts non-zero probability on everything. So as long as your read data density is not like point mass, you're good. The challenge will be, you know, do I have a good C or not? And so we'll want a good C. Or not. And so we'll want a good generative model, which is a generative model with a small value for C. How do we know if we have a generative model with a small value for C? Well, how we know is exactly with this baseline that I was showing you on this architecture picture. Essentially, based on this definition of C-closeness, if we can also have a bound on the power On the power of a model to distinguish between real and fake data just looking at this data. So essentially, we are first going to say: if our baseline is really good at guessing which images are generated images and which ones are not, then we have a very high C. If it's not very good, then maybe we have a small value for C on our generated model. Then we carry the membership inference attack that gets to look at. attack that gets to look at the model and essentially what this can show is that we are measuring C plus epsilon where epsilon is the epsilon from dp and c is the c from c closeness of the generative model. So essentially we first measure a lower bound on C just for the generative model with the baseline. Then we measure one on C plus epsilon using the membership inference attack. And then we return the difference. And then we return the difference. So, and that's what we use as our assessment for privacy loss. Okay, so obviously epsilon tilde that we return, we put a tilde on it because we are subtracting lower bounds, so we don't have a true lower bound on the true epsilon for the model. But essentially, unless the baseline is very good, then you're really measuring the true C for the model. So, essentially, what we are claiming is. Essentially, what we are claiming is based on the data we have, it's compatible with the model being C good. And if it's really C good, then our model cannot be better than epsilon TP. All right. Empirically, with current generative models, we can get like pretty good generative data. We've done a lot of work in the paper to show that the baselines do seem That the baselines do seem pretty strong. So, even though we don't have a real lower bound on epsilon, it looks like we're measuring something that's meaningful for the privacy loss. We've used the techniques on image data, text data, tabular data, and every time we observe the same thing, which is if you overfit your model more and more, you measure more and more privaciness. If you're measuring privacylus on models that are differentially private with lower and lower values of epsilon, you do observe lower and lower measurements of this epsilon tilde that we measure. And the results pretty much checked out with the results of other audit methods that need control of the training process. So empirically, it does check out. It does check out now, of course, the semantics are not perfect, but essentially, we have a first step towards an audit, a privacy audit of machine learning models without any control of the training pipeline. So just getting the final model and some membrane data that we contributed to it. Empirically, the results seem very close to those of Close to those of privacy audits with stronger guarantees but that require more control. And so essentially, the next step for us is can we get in this setting a true lower bound on epsilon? And so that's what we're working on now. It does seem like we have theory that checks out for getting a real lower bound on it. Getting a real lower bound on epsilon in this setting, the big question will be empirically: does it always measure zero when we have a real lower bound or not? So that's kind of like what we're trying to figure out now. And so I'm going to stop here and I'm happy to take any question. Do you, Jennifer, have some idea carefully how much uh price allows you to have? Large living hair because I see how much an art is that, how small it could be. Yeah, so that's why in our experiments we compare with this O of 1 approach, which does get a true lower bound on epsilon by actually putting members and non-members in the training data. And the results are fairly comparable. They're bigger here, so those other. Those other approaches with a lower bound, you know, they measure like slightly smaller but very comparable results. So it doesn't seem that we have too much leakage from the baseline to the measurement. So that's good. Now it's an open question to know, like, you know, those are lower bounds, are they good lower bounds? It's really hard to say. So what we know is that if you craft data with poisoning, for instance. Data with poisoning, for instance, and run those audits on DP algorithms, for instance, the lower bound they measure is pretty close to the upper bound given by differential private. So we know that those techniques, when you craft specific inputs, kind of like worst-case inputs, they are able to audit at least a differentially private algorithm pretty well. Now, here we're trying to do something a bit different. We're trying to audit To audit specific data. So we're trying to say, for this data, is this model consistent with an epsilon DP model? And there, it's much harder, like because we don't get to craft the inputs, they're real data, it's much harder, like I don't know of any technique that could give us an upper band. And so it's really hard for me to say, are those numbers far from an upper band or not? An upper man or not? But typically C is smaller than better, so then it's better to generate a model. Well, how do you call it like that? So C smaller means, okay, if it's the truth, smaller the better. Here we only get a lower bound for C and we want it to be as close to the truth as possible. So what we're trying to do is we work really hard for our baseline to be like as strong as possible so that we measure a C as high as possible because we know it's a lower bath. And then Lower graph. And then we can work really hard on generative models so that their true C is lower, you know, and then we'll get a better idea. Yes. The evaluation bit is a bit missing something. So all the baseline messaging have to hide some of the data. Where is yours? Is there a one hardware? Yeah, so here we are actually keeping the test. We are actually keeping the test set to measure accuracy to make sure we are auditing models that are not completely crazy. And that's the part we use as extra data for the other methods. There's just one data set on tabular data in the paper where we had to, we needed more non-member data for the OF1 approach to work, and so we had to reduce a bit the training data. But yeah, a key good thing about our approach is that we don't need thing about our approach is that we don't need that extra data but we do need it and so we we try to be apples to apples as much as as possible we have time for one more quick so maybe we can continue questions over the break but let's give Matthias another So that would be uh which is not changing.  And then you figure out what's going on. Do you know about the answer anything? I never was on my train at all times. Did you close that? Oh, it's a skin. Oh, you have been waiting for the initials. Here's the X. So your top may be here. Oh, this is just stage. You can download. I yeah, it shouldn't be here. It shouldn't be here. I send it. I send it. Maybe click here, Google Drive. Yeah, it's in the Google Drive because open it and then we'll share it. We're going to do maybe this download. So it's not thumbed up. It's sort of open inside the web browser. So some we can try to try and control that seem to work. Oh, it's here. Oh no, that's a common sense. I was hoping you had to go to the bottom of the corner. I'll look into the full screen view. Full screen. There's no issue. Oh, yeah, Windows, like Mac PDL. Oh, so it's not control as well as this one. Yeah, yeah, command is kind of the same. This one. Yeah, yeah, command is kind of a sense of control thing. Oh, but it cannot go back. Oh, yeah, it's perfect. Sorry. Oh, I do my. Oh, it's fine. You've done that your off. I'll just try this with our catch. What do you think? Yeah, so I think we can well, what you mean is nearly possible with the very narrows. Uh if uh did you know the model it has a generator does for a generator where we do we do observe some of that in a sense that there is a gap of it's what we are measuring. But we are measuring your I took this exactly. Um I don't know how very much, but I think it makes sense. What we're working on now is the rural direction uh like some diffusion models that they have like some lower value of layers. There were a value of layers in the number of numbers. So you need a generic model that you need to generate. So so there there's also another like first of that. That's how I look. That's how I look, but I'm pretty sure we have seen this with the actual using one is that we can do this. I said you could say my models are I see except I said more versus figure we could do. So that's like step for what you want. That works for it. We were asking about whether or not you can get the recognition of privacy using everything. I guess it depends the two settings, you know, the I and D setting devices are the same way. 