So, first, let me thank the organizers for inviting me to speak at this very nice conference. So, let me share just a little collection about my interaction with Bing Casseman. I think the first time I really talked to him seriously was like 10 years ago when he visited us in the University of Chicago. At that time, we were both intensely interested in what. Both are intensely interested in what we call now the basic functions, but we disagree profoundly what the basic function is. So, because for me, the basic function is a perversion, and for BIN, it is a computer program, so we don't agree. But at least there's one thing that BIN told me that profoundly struck me at that time was his observation, his basic function is the false. Is the formal series for coefficient, as we know, as a symmetric power of rho, the rho terms of dual groups? And so it's very complicated because we basically are dealing with this intractable problem from invariant theory, the same obscene, right? Like Betisms. So I think we don't under now we understand what are the perverse shifts is, how Beth functions correspond to perverse shifts. Purposes, we still don't understand why how to deal with this story of sim of sim. And I think it is really has some something to some bearing with the Levland's idea of Biondoscopy because when we want to plug the basic product of basic function into the choice formula, then we want to see some Some behavior that depends on row. And so it had to do with how the seam and a raw depend on row, which is very difficult problem from the environment area. They have no idea to sort of. That is one of the remarks of being that I keep thinking about when I do have it. But today I want to speak on another topic. So I had to apologize again because. So I had to apologize again because this is fairly new and sometimes unmajor to speak on this conference. But it's so nice, I cannot resist the temptation to speak, to present to you and to Bin this nice formula, although we have and this kind of very kind miraculous calculation, although we have not explored on the you know on the aspect of it. So, this is the formula for the roughway transform. So, this is that part. I think it's one of the essential parts in this cash down program. And I think thank you to Fedun, you have a very nice presentation of this circle of ideas. So, that allowed me to go very fast at the beginning. So, we start with. So we start with languages, Han Len defi automorphic L functions when you have a g-reductive group of the phi of some global field, let's say K, and row is a representation of the L group, a finite linear representation. And if you have an automotive representation of G, which is a tensor product pi is product of pi V at a place, then let me defy, I mean the further approximation, L function as a product of all undemified place. Probable on unvamified place, right? Where there's no trouble to define the local L factor, the unvamified place. And then the conjecture is, of course, that this, you know, this incomplete L function have neuromorphic continuations. And it should admit also the functional equation after you insert the correct local factors at the remaining place, at the activated place, as well as the ramified places that suppose some local. That supposes some local language conjectures. So this is the case where G is GN1 and rho is identical representation of GN1. So this is the data. This has been done in data thetis and that gives rise to the classical theory of Riemann zeta function, directly L function and so on. And it has been rather generalized by And it has been represented by encode MongoSA K, Tamagawa, to the K of G and to G Ln, and rho is identical representation of G Ln. So that's called the principal L functions. And as Langland pointed out that the general case would follow from the Goldman-Jacques theory of principal L functions if we know the Fontorand conjectures, because then you can just do the Fontorian leaf. Then you can just do the front-tolerant lift from G to G Ln and then apply Woodman-Zatke theory. And somehow the convert theorem tells us that this is basically equivalent somehow. If you know enough about your function, you could get also the functority that you choose. Although this has not been explored in full generality. Explore in full generalities or details. So this is, let me at the start review this Woodman's Again Tech theory. Maybe I should have added the name Iwaza, Watermagawa, and other people. So that is table GN1 and rho is identical representation and GLN and rho is identical. GLN and rules identical representations. So, in this case, the kind of basic analysis that had need to be set up is first we had to find a larger space. So, in this case, M, we look at the Mn, which is the space of matrices. We contain GN as open subset. And here, we don't think about the place of matrices as a Leonardberg GLN, but as a mono that contains GLN inside. The GLN is either. The GNL is an open subset of invertible matrices. And then there is Schwarz space. Here, MN is a vector space, so there's no, it's clear what the Schwartz space is for when the over some local things, FKV, complexity occurs someplace, then Schwartz and V is not immediate function with. Uh, function is smooth from the compact support of the matrices, and in this case, there's one very important function, it's just calculated from the integral matrices. And we have the Fourier transform, so because the matrix, the spatial matrix is a vector space, you have Fourier transforms, which preserve the Schwartz space and also this distinguishes basic function, the cartic function that Function that looks at the integral matrix if you normalize well the Fourier transforms. And moreover, we had a partial summation formula because again we do Fourier transform of vector space. So this package, a kind of Fourier theory package, if we do the Merriman transform on this, you see how this source function acts on the presentation of G and Len, then basically we get the complete theory of principal function. Theory of this point of function. So, of course, there's a lot of work in it, but I let me just try to abstract, make at least this vertical abstract arrow. So if you have enough, if you have this field, then you could have the field of L functions. So this is basically what Barraman and Crassban proposed 20 years ago. So when G of adaptive group of K and So when G is a reductive group of K and have a rho representation of L G, finite dimensional representation of L G, then it must have some kind of monoid, M rho, contain G, as an open subset of invertible elements. And that we know now we can construct the M rho into full generalities. It now even assuming G split and so on. That had been explained beautifully by Fidun yesterday. Yesterday. And then we need to define some spread of Schwartz function on so this function on GF, but it's better to think a smooth function on GF, but which must have relatively compact support and MF. So the support that is contained some compact subset of this M row F and the extra condition. So for example, this space should, of course, should be stable under Under convolution with smooth compact support measure on GF, and it must be also invariant under multiplication by function on MF. So there should be two conditions. So there's one is the prior shift on the do one side and one has to be also the shift on the on the geometric side. So the difficulty here is one of the difficulties. Here is one of the difficulties is the you know away from the basic principle and case with the Woodman's update case, this monoid is not smooth. And so the singularities of the mode need to be taken to account in defining this work space. The next items is this basic function which replaces the function which replaces the characteristic function of the Pythagorean matrix in the K-gon module case which whose satellite transforms is you know is the defining the undramified local factors so this for these basic functions so it's basically as I said at the beginning my talk is that the formal series of whose coefficients are C main up rho Abroad and that has been computed by many in many cases. But geometrically, this can be interpreted as an IC function, the international complex function on the R space of the top M row. This will be reason by Aboutier, Circularis, and myself. So this is say something about the Uh, you know, the singularity of the monolith plays a role in this, and and and you know, the you know, some kind of similar feature may happen to all shrust function, but it's not clear how to do it yet. But this is not a topic of my talk, my topic should actually identify the fully charts, the raw Fourier transforms. In Fourier transform matrices, it's clear Ts is the size. Is the psi of the trace of the matrix as a gun? And now we need to do what is the psi of the choice when you deal with some general row. And of course, the possible summation formula that is very much out of which for now. So let me give an about say something about the role for a transform, what we expect about it. From what we expect about it. So it has to be some kind of convolution form for the group. So for some function phi, it should convolve some k row, where k row should be some stable invalid distribution or stable different invalid functions. And it wants k row to be essential compact, essential relatively compact support in the M rho. That means when you convert M k rho with some with the cartesic measure, some compact subgroup. Some compact subgroup, then it becomes relatively compact in the monoid. So, whereas this convolution actually makes sense, right? Because a mono it you can actually do convolutions on the monoid, as long as composite for the monoid. I'm not, you know, I'm not that we're not there yet. We're not, you know, I don't care much about the conversion distill, but I think this conversion issue can be. This thing can be, you know, it shows up again and it's quite important later in my talk. But at least at this point, I think it's just roughly that, you know, conversely, C had to be related to the function, you can do convolution product or the monoid, as long as the function distribution has relative, essentially, relatively compact support and monoid. So on this point are more or less an obvious expectation. And the next one is kind of less obvious expectation. This is the one that was the point that was made by Barbman and Kasdan, that you really want that KROU has some kind of nice formula, nice algebraic formula, some kind of integration, some algebraic function, diffusion form or something like this. Where that it, you know, that is maybe just some function aesthetic. Know that is maybe just some aesthetic requirement. But the other thing is, if you want to able to prove the possible summation formula, we need some kind of nice formula for this kernel-fouled transform. So the next requirement, you know, there is some quite spectral. The spectral definition of this current is that because this function is invariant distribution, it acts by scalar on metric coefficient with reduce mode representations. And we want that KO act as the multiplication by the gamma factor, the gamma factor attached to row, right? And this is kind of circular. This is kind of circular. So you can say, okay, I just defy by this spectral expansion, you know, on the representation, and you just put the gamma factors. But there's two points into that approach. First, you don't actually know what the gamma factor is unless you know local evolution conjectures. The second is this formula is not going to be very pretty. It's just kind of unusable. You really need some geometric formula that can be a spectral expansion. Expansion of this kernel. So there's one, at least there's one requirement, the obvious requirement from this factor, you want the kernel act on magic coefficient reproducible representation as the gamma factors. Yeah, this must be compatible and parallel with descent. So that is one of the things that we want to keep in mind that you want to define a conon which compatible. A kernel which is compatible with the parallel set. And of course, we want this kernel to be involved at interior. And by also one to satisfy possible summation formula. So there's a lot of expectation. I mean, there's a lot of work to be done to get the theory on its fit. So, at least in the Taurus case, you kind of have understand it completely. So, with 13 torus, and here I insist that we should consider with the pay of non-speed torus for the reader to be clear later. So, when you have the representation of the L group of the torus, finite representation, then you have the multi-set of weight of T. Have the multi-set of weight of t-hat of the dual torus, right? That is multi-set, and the value output will act on the multi-set. But just from this combinator torrent data, you can view an induced torus. So, D rho is a torus that is induced torus that made from this multi-set with action the gala action. And it map to it have homomorphism to T by because it. To T by because it's made up of a multi-set of covet. So every element is covered. We have many. And as it's induced, orders is embedded to some kind of induced affine space, in obvious sense. And then you have the trace function by adding the coordinate, which is, of course, is Galois invariant. And in this setting, the monoid, each channel varieties is very easy to see. In this case, Easy to see. In this case, if D1 row is a conon of this map, the homogeneous mutual tori, then the M row is just the invariant seventh quotient of this of file space. So we have a file space divided by the kernel torus, and then we get the torrent priorities. Oh, by the way, I need to have some assumption in rows about the central option that has been mentioned. That has been mentioned by Fred Besiday, some positivity conditions, but I ignore it here. So, with this, then you can justify the full economy by integrating the psi of the trace on the fibers of this homomorphism row check. By this formula, we general rotate integrated. I rotate integrate integrate yeah so um so you know in the case like um you know the the in the case like you you take um t to be gm i think this is a very nice case to be studied t to be gm and rho is the the sum of of two copy of Of two copies of the standard representation. And then on the on the when you do a line, here let the map from Gm squared to zm, which is the product, right? And this is kind of customer integrals. And this, in general, this is kind of customer integrals. And there is, you know, it is kind of, you know, the positivity conditions with the sentence that make it this is very nice, this would be very nice. Nice, this would be very nice integration depending on some parameters that have been studied in other situations as well, like in for finite finish theory of a lot of cats on I think it's called hyperrummative sheets. I'm not I'm not sure, but this is um uh the theory is um this is nice and just pair. This is nice and dispelled. Also, in this case, by the way, I think it's not written down, but basically, you can run the whole program in this set and you have the basic function, you have a Schwarz function. Schwarz function means simply the taking smooth and composite part functional file space and do integration. And so, of course, it's preserved by this kind of row-foulet transforms. And you also have Parsons. And you also have Poisson summation formula because they have Poisson summation formula on the phi space. So, this is this is and this is very much the same formalism where you do the Hanken transforms in classical theory. You know, that when you look at S1 invariant function on the complex plane, and so this works for this type of how can a best-centered transform can be generalized. Transform can be zeroized into this toric situation. So, in this case, as I said, it is defined by some algebraic integrations, just by some algebraic, but decide to include this exponential function, exponent of the trace, and just by definition, basically, by the Basically, it are acted by the cover gamma factors. And we did, and it had as a compass support in the monoid because of this positivity assumption on the center. Also certified pass observation formula. So in this case, we have the payroll t is a zero t. So zero t is this function that is integrating each Costeman integrals. Integrals. So, how about the general groups? So, you know, in the general groups, what you can observe is it is, so here I assume speed to simplify, but you don't need to assume the speed group. You don't need to assume it. So, when you have a representation of the air on group and let it not be maximum speed tolerance, and then by this, but actually, by the same argument, because the as. By the same argument, you call the Galois action. The Vi group acts on the monthly setup weight. You should look at the monthly setup weight and the Vi group acting in monthly setup weight. So the Vi group actually acts on this induced torus anonymically. So you have this, you can divide the induced order by observable groups, and you have this kind of complete diagram. So there's a large part of which is Cartesian. So as indeed. So, as indeed with distributions, you can just think about the Ecuadorism diagram. It's not doing any harm. So, and then you see that in this case, the coefficient of T0 by W is exactly the same as the coefficient of G by G. So, this is from the standard base. That is what standard consider the invariant cost of G by G. That invalid quotient of G by itself, by G acting by conjugation. So by this construction, somehow we get a function or distribution on this standard base and it interpolates on tori at the same time. So you have the option on every maximum tori, and what happened is after they just moved together very well in the Actually they just glue together very well in a stable invariant function on G. So, of course, I mean, you can ask is it the right kernel? So, it works very well in the Toros case by construction. It works in the case of Botman Jap K case. And so, it diabetes it by construction. So, at some point, I hope that this is the right kernel, but it worked at two case. But unfortunately, in the next case, it stopped working. Next case, it stopped working. And when G equals to Z and 2, and rho is the SIM2, then all the wall. So when in this case, you can, you see, it's very curious. It's not correct, but the defect is rather. That is rather rather subtle. It doesn't look like a big deal. In the case of SIM2, for example, instead of integrating this function side of the trace on the fibers of this map project, you need to add some kind of multi, some characters on the one row into these integrals. So it's a kind of seen two scores and some two scores integrals, and then in the right column. And that is the right colon. But then it has been really difficult to understand in the kind of abstract way what we are doing here. So you want something that depends on G, but then you do some character in the kernel. You know, what the pattern is, doesn't have to do anything with G. So I didn't understand what interconnection should be. The corruption should be so. So, there is one very important piece of work of Laurent Lafour. Laurent Laforte wrote a lot of papers of these topics, many of them not unpublished. It's actually quite difficult what you want to find in these pie papers, but there's many interesting ideas in what he wrote. in the in what he wrote so he also he did some contribution k of g n2 and he also observed that this the this part of the same function so the the the function k row the code of the row flounce transform that we are shooting for and this function uh that defy by just by descent on the w oxygen on tolus are not the same so the basic reason is that the this function joy function on This function, joy function on T mod W is not compatible with parallel descent. So that is what he observed. And moreover, he actually showed how to correct it to make it compatible with parabolic descent. So that is what he does. He does something very interesting. So he looked at, if you look at G, now he look at some, looking for some function, invariant functionality. So it's going to depend on two variables, basically, the choice variable. Basically, the choice variable and determined variables. And what he so J1 can be seen as a function of the two variables. And what he does, he does the Fourier transform of the trace variables. And then, so this is the J row hat of alpha one, A2, where alpha one is a dual variable into A1. Alpha is not a, it's not a root here. Alpha is just a dual variable to the A. Variable to the A. So A are the coefficients of the characteristic polynomials. So we do the J hat draw is fully transformed J, so this kind of naive connot function on the obtained by W Arsenal T. And then he multiply by the absolute value of alpha 1 and he fully transforms back. Transform back, and that is his recipe for the right kernel. So, I said, okay, I need to thank Jacques. I think Jacques was the one, I think, if you remember correctly, who showed me this LaFour formula. So, it took me a lot of, oh, by the way to understand LaFour formula. So, actually, in LaFour formula, he also added another factor. So, A2 to dimension the V row. But when I did the calculator, I didn't see it. When I did the calculator, I didn't see it, so I think just probably just some kind of normalization of the measure. So, so I just decided to ignore these factors because actually they redo the, you know, La Faulet Riley proof, actually try to reprove his formula and it works beautifully. And I don't see these factors of dimension of Euro in the way I set up the calculation. The calculation. So now the question is to understand this formula of LaFor and how to generalize it and to understand it. So from now on, it is the Johanneslin. So this is very new. This project audio reactivates arrived in Chicago since September, October, so it is two or three months ago. Months ago. So you can take everything with some kind of sand, salt. Okay, so look at this formula. So the basic ideas that I wanted to do the correction, I want some type of operators that transform the kernel J row, the naive kernel, into the correct kernel. And the point that of course, this kernel needs to be independent at row. Going to not need to be independent at all, it just has some kind of operators. If you put in zero, you get the k row. So, in the k of g n two, if you ignore this factor of a2, that one should not be there. And then, what you do is just do some different operator, right? Because on the right-hand side, you multiply the Fourier transform by a polynomial and Fourier transform back, each of the multiplies a different operator. Apply the different operators. Of course, that doesn't make sense when you use a pdx because you do not have different operators, and so we're not multiplying with a poor normal, but you multiply by the absolute value of the poor normal, which is a real number. So, what you can think about is kind of different operators. So, the thing is, in general, you need some kind of pseudo-different operators. So, we set a polynomial on anthra. A polynomial on alpha, you need to some polynomial on two variables, both a and alpha. So I'm going to explain what this polynomial is. So the second thing is this. So it's some kind of we need to do the Fourier transform on the cartesic polynomials, on the on the cartelistic polynomials. And this operation was actually was in effect. Was in, I think, was appeared the first time in the work of Franken Langlands and myself a couple of years back. At that time, I really do understand. I basically, Lemmens asked me to do some technical work, prove some lemma here, some lemma there, but I just don't understand why we do this Fourier transform and calculate polynomials. And when I asked him, he just said that no, we have this mirror to a phi space, so we need to do participation there. So, we need to do participation there. So, but here I find it very miraculous that it is play a really big role here. So, what we are doing. So, we do the Fourier transform on the on coefficient of Cartesian polynomials. So, you could avoid determinants, but I think the formula looks better. You should put it in. That doesn't harm anyway. So, this is the formula. So, I should write out your formula. out your formula so the so we have this function j rho of a1 a2 a n right this is on the on the standard base of g and length so a1 l a1 is the trace of g a2 is the trace of red2 of g and so on and this form a file space just look at a file space and do the Fourier transform so that to get the function they have draw a variable alpha one alpha eight they actually do one variable a one The actual dual variable a1, which doesn't have any center right now, and then you multiply so you have to by some function, by some by absolute value, some polynomials, some some very cute but very mysterious polynomials, evaluating polynomials, and we integrate back. So, this and this you get the polynomial. So, the SSR gradient here is these polynomials. So, what are the polynomials? So, what are the polynomials? So, this is polynomial D and A and phi. Again, this is some kind of big polynomial, invariant polynomial two variables of A and phi. So it's better here that you think about it as an invariant polynomial D and G and phi, where G is a matrix. So A is a characteristic polynomial of the matrix, but let's think about G. And because it's invariant, you can just look at what the You can just look at what the restrictions do to torus, right? So we just look at the W and invariant polynomial torus. So that's why this polynomial torus. So then T alpha, when T is a diagonal matrix, T1, T2, Tn, is going to be the product over on subset I of 1, 2, 3n. So with N minus 2 element, that is a curious thing. That is a curious thing. You have n minus 2, and then on the properties again, these factors when you get into the Fourier transform of the on the and do the Fourier transform on the on the standard base. That means sum of ampha i of trace of lash i t i so that that's it. So you look you the same the same. The same, the same bilinear forms on this doing for the Fourier transform, but not over the set 1, 2, 3, n, but over all subset n minus 2 element into n. Why that? I have not a slight idea what it is. So it looks very much like we do the product over the set of roots, right? Because this is the, you know, the complement of this set is just i and j, and that is the root. It's just to do a Is the rule it's just to do a problem of some positive rules and you multiply something on the on some kind of levy gm minus two over gm2 on gm minus two in here I don't have the clear explanation of this thing but that is what come out in calculation okay so what we want to prove is this we have to prove that the k row when you confirm with phi and do the constant term Convert with phi and do the constant term. This might be the same as the J row on the torus converge with the constant term of phi. That is the same formula I want to prove. And we prove this on the assumption that phi is from the smooth composite for the big cell. Probably just true in general, but we just have not got the time to improve our results, but that is what we prove now. So that is very likely to be true. Be true. So you do this, you're going to do some kind of descent from GNN to GNN minus one to Gn. And you see, so the this will due to this kind of descent formula. You write the matrix, n by n matrix as a product of two matrix, like the big cell. So one is a lower unipotent matrix, only a row vector. And the right is also direct is an air matrix and this V. Matrix and this V is a column vector. And we do the integration over both variables V and V and V star. And what we get must be the delta distribution on 3 and multiply by the kernel for 4G and N minus 1. So let me tell you this. So why we So, let me tell you this. So, why we think that this kind of thing works in Genotes? It works because when you look at this formula, it actually quite looks surprising at the first view, but I think it's not surprising. Because if, you know, this integral is not clear, it's converged, right? You can actually do it with any kind of kernel, which is invariant function. It's just not, and it's nothing very special about this. Nothing really special about this KRO. The only thing that we use is KRO is to make sure that actually it converts somehow. So if it converts, then this thing gives rise to this integral, these double integrals, give rise to an invalid distribution on the variable v minus, right? So everything is invalid. And now we have a function on And now we have a function on the vector space and distribution that's invariant. So that must be a multiple of the delta distribution. There's only one distribution there. So the whole thing is how to compute this constant, right? And it happened that in the case of GMLAN, you can do it. You can do this calculation basing using many times this full return on the standard base. The on the standard base, like language like do it. Okay, let me explain the calculation. So, we're going to write the k row n, you know, by definition, is the integral. So, we can see it as some kind of integral of this function psi. So, instead of calculating with any with some kind of complicated function of variable a, we just compute with the linear, some kind of psi of linear function. function because it's some kind of kind of linear continuous linear combination of those functions so let's compute this this integral so we should do what to use this integral so the function psi of the of this alpha iai so that is the the or the thing that you use when you fully transform on the sample base and then you apply to this phi v minus so the relation between x and v minus in this formula so x is this is this complete So, x is this complicated formula that depends this match computer expulsion. So, to do this, you can just how to do some calculation on the characteristic polynomials. It looks scary at the beginning, but if a guy has some kind of brave and some kind of skin, then you actually can do it. This kind of is surprising. So, if you know. So, if you know X is this metric, it's called up to two metrics, so it's kind of big cell decomposition, and you want to compute the calculum X in function calculum of Xn minus one, the coefficient. And here what you find, you do the Ai, so the trace of which I of X is the sum of two terms. sum of two terms there's one the first term each of the the the the sometimes doesn't depend on v and v check that is the the that is the you know the the choice of vector i of this of this block matrix and there's another terms which it does depend on v and v check but then it's just a bilinear function on v and v check so that is the very special feature So that is the very special feature of GNLN, not true for any other groups. And this is what I use in earlier work with Shuyang Cheng, where you prove some related thing on the for the curb, but for finite groups, the better man current temperature for finite group. So you use this fact, but here you just go further, you should compute completely this on this thing. So what you found. So, what you saw really nicely, you can, when you write out this, just try to go ahead and compute this, you found this formula. So, this is the first qualitative you have these constant terms, and then another term depend on V and V check. And this actually can be beautifully explained in invariant survey. You know, in the how GLM minus one up to GLM by conjugation, the same kind of invariance that you actually see in In Jacqui Ralis fundamental name, for example. So, first one, for the first part, for these constant terms, constant terms, not the Harvey Strander constant terms, just constant terms, the polynomial. That doesn't depend on V and V check. And then you just compute this sum. Then you get this formula. So we start with Ai and you convert it to the Ai of the M minus one. the of the n minus one of the of the this m minus one time n minus one matrix right you had to this chain of variables and then the a antha i had to become this anthro i plus dn so the tn is now to be put into the into variable alpha instead of other variable a it's kind of strange but that's what it is and then the first a1 will be paired with tn and then the tn is going to the to the this Is going to this kind of leader transformation, actually. And the second thing is this. So you look at this, this is a billionaire function on V and V check, which depend on x n minus one. And it's a homogeneous polynomial on x minus one. So you can see as a matrix, n minus one times n minus one matrix with coefficients that are the evaluation. Coefficient that are the invalid function of x and n minus one. And then checking determinant. So the CI CI tinder in this matrix. And check the linear commission alpha i and check determinants. It looks like a fantastically complicated thing to calculate, but it just derived each other this coefficient of dn, big dn by dn minus one. Right? And you had to do the same chain of variable. Do the same train of variable here appears with the alpha for the n minus instead of alpha. So, so then you put in the formula, the you know, when you do these Fourier transforms, you put out the what have to do with this constant term, but just put it out because it doesn't depend on V and V check. And then whatever in is it just some linear transform on all the variables. form on all the variables and then determine under this uh this determinant comes up as some kind of factors greater than other factors and you multiply by them you know we simplify about the dn and dn minus one and so that'd be how you can do a descent all right so um to that everything i can say it you know it um we just there's just the there is it was the fact i think there's a theoretical the fact that it's something had to do with an invariant distribution Had to do with an invariant distribution, and it had to be a multiple of the data distribution. And what you had to compute is this constant. And the conclusion of this constant here is some kind of depend on the use of Fourier transform standard basis. And then we do it. It is not scared by the complicated formula, just do it, then you get some kind of mirror circulation, cancellation of polynomials, and you get to this kind of beautiful polynomial. And you get to this kind of beautiful polynomial dn dn. And the factor is usually dn divided by dn minus one after chain of variables. Okay, so what is next? Of course, we need to complete this descent theorem for every function, not just the function on the big cell, and it should prove involuntary invertibilities. So this guy, I expect that you can do it. It just it just it's going to be it's not it's very hard, some hard work, but you can probably do it. You can probably do it. Actually, LaFrop also says that he can do it for GN2, in the page GN2 for initiality, but it's not clear that he doesn't care. It's very formal, even the formal calculus does not seem to be completely straightforward to me. But anyway, that is the good sign that we can do it. Another good sign is now the kernel is met to the kind of simple. Kernel is mapped to the K of SIM of GN2 and SIM2. So, this is probably the good kernel. So, the K of SIM2, GN2, and SIM2 can be interpreted as some kind of double method. And then we do have a kernel that will compute by Devois, by Leisan, and Ziding. So it matches. So now we have another example that the colour is correct. We have to prove that in the case, we know the gamma factor acts as the right gamma factor. Factors act as the right of factors. Of course, you have to understand this kind of pseudo additional operator for other groups. The formula is very petty, but I cannot do a fully understand what it is. And of course, the big deal is it should put passive summation formula. Then it would be good. All right, so that's it. And so, happy birthday, 80th birthday being, and looking forward to your 90th birthday. And hope that you have better formula that time. Thank you. Formula at that time. Thank you.