Epigenetic radio associations. So, I guess everyone have heard of LASO or have used LASSO for linear regression, but LASO is a linear model that we have linear assumptions that what if Lasso used in a non-linear case. So, in this simulation, we generate 20 variable predictor x, which follow the normal distribution with a random error or the noise error. And the true value y was generated by. That is why it was generated by the first five true variables and give a non-relationship. Here are square positive exponential, a negative exponential, and a cosine functions. Here this curve is showing the shape of the underlying functions, how the nonlinearity looks like. If you run the lasso here, actually, the results are not very satisfactory. As you can see, As you can see, that here, even in this small course without any interactions, just a nonlinearity, the real numbers here, five, Lasso cannot successfully identify, but Lasso here, we see an inflection point here at 3. This actually is also the point by using the LASO-waste post-validation with one standard error rule. So, why LASO cannot identify? Why Lasso cannot identify, why there is a two-variable Lasso cannot identify. Those two, if you check the results of this point at three variables, we're going to see that, okay, those cosine functions Lasso cannot identify, which makes sense that cosine is a periodic function, which is hard to detect without some specific settings. But interesting is that we also could not identify. Could not identify those square functions as we always know that square function should be very strong. Why we cannot detect here in these situations, we think is because that here is the positive area, which is dominated by those positive exponential functions, and the negative area, which is dominated by those negative exponential functions. So, actually, those two parts mask those this square function. This square relationship. So, which in this simple case, without any interaction, it highlights the limitations of LASO. And the real design are even more complicated than this case. So, can we use Lascelle still to do the correct variable selection? Yes. That if you are smart enough, you can do some feature engineering that hard coding each variable like this. Each variable like this. You know that true format. Then you run in the last one that, well, you get the perfect results that you correctly select the five variables and with almost zero prediction error. By the way, this is run on 10 replications, so you have the confidence intervals here. But you know, this is like in a parallel world, in the real world, you always cannot know what is the true underlying functions. As a data scientist, we are always looking for. As a data scientist, we are always looking for some data-driving way to automatically doing such a job. So, can we? Yes, we can, that we can use neural network. Neural network is theoretically proved can approximate any function shapes. So, looks like it can, like any nonlinear relationship or or interactions can be clattered by the neural network as long as there are enough data or or enough reports for training. Reports for training. But now someone will ask: wait, you use neural network, you can do, you do can like approximate those complex functions, but where is the variable selection goes? How do variable selection? So in the 2029, the LassoNet solved this problem that by modifying the traditional neural net, we add a linear connection or skip layer or residual layer directly from the input. Directly from the input to the output. Those connections directly from input to the output are actually pure linear. There is no activation functions. So the variable selections are actually guided by those linear connections because pure linear, we can use maybe like lots of like method AOI regularization on that. So for example, here is the first node from the first input node to the output, there is no linear connection. The output there is no linear connection, then all the weights in the neural network, in the first layer of the neural network, will disappear because this is a feed-forward neural network. We do not care about the rest of the connections. So mathematically speaking, if you denote this is a neural network G weighted by the weights W and Lausonette did is just introduce a linear connection, X beta. Linear connection, x beta, pure linear. Then it solves this problem by minimizing the loss function. Here is the predicted value, and the real value is the loss. If it is classification cross-entropy or inspiration error, if it's a prediction problem, and apply an L1 regularization on the linear connection. Expect. This is like, yeah, just a simple L1 regularization with the parameters. R1 representation with the parameter lambda, which is the L1 representation parameter. And there is one more subjection: that the first hidden layer weights in the neural network are restricted by the absolute value of the linear connection. For example, here, if the beta 1 for the first node is zero, then those definitely those weights for the linear language in the first hidden in the first. Language in the first layer will always be zero. Here, our value m, m is a total here multiple parameter because it controls, like controls the linearity between the linear and the nonlinear. If the m is zero, assume m is zero, then actually all the w will be zero. In this case, the Lasso net will degrade to just a simple Lasso problem. If m goes to infinity, which is To infinity, which seems like there's no restriction on the neural network rates, then the neural network will dominate the function shape. The nonlinearity will dominate the function shape, but we will also lose the variable selection properties. Okay, so if we run loss net on this problem, you're gonna say, wow, looks not bad. It's definitely like a loss. It's definitely like a LASO classification engineering, there's a golden standard. But LASONet performs not so bad, it has much lower mean squared error than simple LASO. And even though it cannot correctly identify the five true real variable, but it is also at least identified for one more than the single does. So here, this is a cosine function, which is always hard to detect. But you can also see that. Detect. But you can also see that between the 4 and 5, there is a slight increase in the error. So actually, the loss of net by can still capture a little bit contribution from the cosine function, even though it cannot fall. So looks everything goes well, but then someone jumps out again, ask, say, how much can I trust this black box with a reduced set of variables? Okay. Variables. Okay, to answer his questions, I have to introduce another tool, which is the Bayesian regression. Specifically, the Gaussian process of one of the Bayesian regression. That I simply go through Gaussian process. Gaussian process given that predictor x and y, this is a linear model, how linear model model the y between y and the x, a linear assumption. But the Gaussian process is using a smooth function, a latent smooth function x. function x and those latent unknown smooth functions have a Gaussian process prior which is coded by mean mu and the kernel covariance k. So the built for all the strength of the Gaussian process is that given the test points x0 and y, you can get the distribution or the predicted distribution of the function values. Distribution or the function values can be computed in the curse form as this. So you can get the confidence, you can not only get the mean, but also the confidence interval for your prediction. And also, you can also get the marginal likelihood of your output y. So this is a log marginal likelihood for y. And yeah, this is basically showing that, saying that the Gaussian process actually are controlled by the Gaussian. Are controlled by the K, how you vary your K or vary your parameter in your K will affect the shape of your Gaussian process. That's basically your assumption of the data. Okay, now we have the tool for doing the feature, learning the feature representation. The ASONet, we also have the Gaussian process, can give you a reliable confidence interval for your prediction. Interval for your prediction. Our natural thinking is that we just combine them together. Yes, we can. We're just using the output of the neural network to the input of the Gaussian process. This rotation is from inspired by the working method. So actually, this idea is proposed in 2016 called Deep Current Learning. That is saying that given this is the original work. This is the original Gaussian process, but we are not using the original input feature X. We are using our neural network transform the feature, X beta plus this G. This is we call deep kernel because it's a kernel based on your deep neural network. And the whole model are optimized by minimizing this problem, just a simple lossing problem. But here's a loss function here. But here's a loss function here. The loss is not the cross-central word or the mean squared error. It's a negative log marginal likelihood of a Gaussian process. And by using this, you can also get a confidence interval for your prediction. This is a framework showing our proposed method, D-K-L. So it's deep kernel learning with feature sparsity. That this part is a neural. That this part is a neural network with a residual connection. This will guide you the feature selection in the input layer. And the output of this neural network will be used as an input to a Gaussian process. Because Gaussian processes with a longlinear kernel can sometimes be used as an infinite dimension of or a hidden layer with infinite number of neural rooms because the Gaussian curses correspond to an infinite dimension in the Deuce case. In the use case. So the beautiful of this method is that all the parameters, the weights of the neural network, the weights of the neural network, the coefficients for the linear connection, and the parameter for the kernel can be updated and run together just by simple back propagation using the chain rule. We also propose another value. Propose another variant which is called DKLASO. Because DK loss has a, as I said, has a problem that here, this is the final iser-Gaussian process layer, and you have a specific kernel, a kernel assumption on that. A kernel is your assumption on the data, like the smoothness or periodicity. If you get a bad assumption on your data, actually that will degrade, it will worse your model, making both your performance, prediction performance, and variable selection bad. And variable selection back. So we're thinking about that: can we make the whole variable selection in an independent pure linear case? That's we propose dk Lasso plus. So dk LaSo plus assume two latent functions, F1 and F2, where F1 is just a simple Gaussian process with a nonlinear kernel here. But F2 is also another Gaussian process, but with a linear. Also, another Gaussian process, but with a linear kernel, it's pure linear kernel. And the final y, the kernel for the final output of y will actually be the combination of the nonlinear kernel and the linear kernel. Here, alpha is a mixing parameter which controls the strengths of nonlinearity and linearity. If alpha is one, this becomes the pure nonlinear case, become the detail of so. And if that alpha equals to zero, then only in the Then only the linear kernel will hear. Now, this problem becomes the Bayesian linear regression. We will later show that the DKLASO, by using this pure linear setting, the DKLASO will, in some cases, outperform DK, sorry, DK-LASO plus will outperform DK-LASO. So, going back to this simulation setting that our other beginning, that if we run DK LASO and the DK LASO plus on this case, Plus, on this case, well, we see that the bottom here is actually the DKLSO plus. It has the lowest prediction mean squared error and also the narrow, the most narrow confidence intervals, which means that pretty much stable. It's not likely to overfitting that because the loss of a Gaussian, a Bayesian wave is inherently have some type of regularization. And here, here, this one. Here, this one, a little bit higher, is the DKLSO, which is a little bit worse than DTLSO, but still outperformed the Lausanne. And here, another competitor we call the FNDKL. FNDKL just apply the Gaussian process on a fixed already learned Lasernet neural network. And if you check the feature importance of the DK Lasso class, you can see, sorry, feature importance is. You can see, sorry, feature importance: this is actually the lambda value. Because the neural network, we do not have an effective size to directly show how the importance of your features or coefficients. So we can use an alternatively, we can use the lambda value that feature dropout from the model to show the relative importance of your features. If one feature will leave your model, One feature will leave your model at a very late, at a very high lambda, which means that, in some sense, meaning that it is an important variable. So, checking the importance midpoint over DKLA so plus, all the five, the first five true variables are correctly identified. This cosine one, even though are not very strong, but still better than, significant than others. We also have another more complex. Another more complex simulation that we have. We're doing a complex higher-order interactions. Basically, like we generate 20 or 100 input features. Here, the first 10 are going to be true features, and the rest 10 or 90 with 120 or 100 total features are going to be nuisance features. And for those 10 features, we will do 20 times that every time we randomly sample. Every time we randomly sample three or four out of it, and we're doing a complex interaction among those three or four, and finally we're doing this, we repair this 20 times, and add them together. So finally, there are going to be very complex higher-order interactions between those variables. And the final function will be the function of all the first 10 true variables. 10 true variables. So, in this case, that whatever in like 20 number total variable or 100 variables, we can always see that the DK LASO plot shows the best performance at the lowest mean squared error with the narrowest confidence interval. The DKLASO, even though performed worse than the DKLRSO, but still outperformed than the competitors. The interesting that is in this case, Interesting that is in this case, there are like 100 total number of variables. Both Lussel and FNDKL look like they lose the ability to do the variable selection that with the 100 variables is doing the best prediction means per error. But when the more irrelevant variables drop out from the model, the prediction error goes off. That is weird. But so this case we think it is the So, this case we think is difficult that the loss of net and FMDKO here are maybe trapped in a star optimal. And in such a case, by giving a stronger regularization, it will not help him escape from that suboptimal, but will even worse your prediction. Yeah. So, if you also check the feature importance map, heat map. importance map heat map the in this case 20 variable the first to 10 or true variable are exactly correctly identified by our method and also in this case 100 variable the first 10 or true features are also successfully identified by dklasu plus okay so this example we compare dklasu with dklasu plus in a very um uh extreme setting that what if we don't know the Stating that, what if we don't know the true kernel? Here we are assigning a very imperfect kernel, we are assigning a periodic kernel on this case, definitely wrong. And in this case, that actually DK LASSO performed even worse than simple LASO. But the DK LASO Plus still performs not bad, which in some sense, which in some sense saying that the linear kernel here actually works like a regular riser, another type of visualizer. Regularizer, another type of regularization that can prevent your model, like specifically given a wrong kernel, overfitting to your data. And also, actually, in the data, there are going to be both linearity and nonlinearity. The decay loss so by combining a linear kernel and a nonlinear kernel, actually expand the reflect function space to capture more information. Okay, so finally, I'll go over the quickly go over. So finally, I'll go over the quickly go over the application. We apply our methods on the COSA biobank. And we will do an association analysis between the metabolite and epigenetic age acceleration. The epigenetic age acceleration is by the DNA methylation age, for example, DNA methylation age minus the chronological age, which is an indicator of your risk for immunity disease like cardiovascular or mental disorders. Into these systems. So, this is showing how we're pre-processing the data, not so important, just showing that the epigenetic age are calculated by Horwa's method. It's a DNA methylation age. And in this case, the sample size are limited by the number of epigenetic data. So, only around 1,500 samples subjects have the evacuation data. And we have 1,257. 2257 common metabolites per sum across samples, considering a sample size is too many, so we're doing a preliminary linear regression to filtering for some some, sorry, very quick, filtering for some metabolites and for the univalid regression, then the force discovery Q value greater than 0.1 will be removed from this analysis. Yeah. Okay, so finally, apply. Okay, so finally, apply our methods. It's all done. Apply our methods, we're running 10 times to make sure the results are stable. And you can see the method will converge at the end. And the last one that our method, last DK Lasso Plus here has the best performance and will identify the most number of the variables with the last one identifying. Yes. Yes. We also have doing some practice. We also have some, this is the results that we can show you that first the top 10 variables identified by our method. The lasso here are also non-zero. But starting from the COP11 to 20, that we can identify some interesting metabolites, but Lasso cannot. And for example, this chemical was actually validated by the Evaluated by the yeah, okay, that's done. There's some future books. Thank you so much. How about your computational efficiency with the Gaussian process often suffer from that? Yes, true, but that's true. The computation of witness emphasizes is a problem. Exercise is a problem, but the next step we're gonna implement a stochastic evaluation version that now we can use mini-batch or and to or approximate the inducing points of proximity curve. We actually implement a little bit. Or this is the preliminary