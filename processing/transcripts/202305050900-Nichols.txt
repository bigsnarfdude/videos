Relative risk regression for longitudinal binary value neuroimaging data. Great. Thanks, Mandy. I am very sad that I couldn't be there. And there just were some family things that came up. I did eventually learn that you guys were not on BAMP, but yeah, I'm still sorry I can't be there. So, unfortunately, I haven't been able to see what else has been at the conference. So, I'll take a brief moment to. At the conference, so I'll take a brief moment to introduce the type of data I'm working with: lesion data from MRI images, or at the work that led up to this, sort of building up an evaluation framework, and then the work of the title, How to Deal with Longitudinal Binary Value Data. And apologies for anyone who's in at the CM statistics conferences. So there's a lot of overlap with that presentation there, but not everyone was there. So the work that's motivated. So, the work that's motivating me is data from white matter lesions. And there are two main sources of this lesion data. One has been discovered just as a part of natural aging. They call them white matter hyperintensities. They're simply, again, one of these great things with MRI. They just, the more people they scan, the more things they saw. These were described as sort of normal variants. But we now know that they, while they are a normal part of aging, it seems, Of aging, it seems that their prevalence is associated with cardiovascular risk factors like hypertension and elevated cholesterol, and also can basically be a predictor of stroke risk if you have lots of these. And of course, the other sort of important use for lesions or where they come up is in the process of the multiple sclerosis. So, multiple sclerosis is sort of characterized by these lesions in the white matter of the brain. These lesions in the white matter of the brain. I always point to MRI, I always point to multiple sclerosis as kind of the one neurological disease that actually depends on MRI on a routine basis. Because let's face it, very few people are being followed and treated with repeat scans when they have dementia. Getting an MRI scan if you have dementia is probably a research exercise, whereas people who have multiple sclerosis are getting scanned on a regular basis to help track and understand. Help track and help design treatment plans for these patients. So, the starting point of my work is assuming that someone has gone in and developed the methods or automatically or semi-automatically has defined where these lesions are. There are different methods out there. Of course, people in multiple sclerosis have been working on this for a long time. Because I'm in Oxford, I'll be using the Bianca method that was developed specifically for white matter and Developed specifically for white matter intensity data, and that's the data we'll be focusing on in UK Biobank. But the main sort of issue, and frankly, opportunity here is that all of the standard tools, AFNI, FSL, SPM, are built around a linear model, which of course assumes Gaussian additive errors. And a lot of people have done different things with this. Some people just kind of take the data smooth and hope everything's okay, that's Gaussian enough. That's Gaussian enough. Some people just fit the binary data with that into a linear model. And embarrassingly, I am a co-author on that Enzinger 2006 paper. But obviously, we can do better. We're statisticians. We know that you want to respect the statistical properties of the data. So let's talk, let's work on how to build models for this work. So I want to checkpoint two other pieces of work in addition to the work I'll present today. In addition to the work I'll present today, and that is some work that my graduate student and postdoc have been developing Bayesian spatial model GLM for Spiken Slabs. That's BLESS. That's under revision, and hopefully that will be out soon. So that is trying to sort of do a Bayesian approach to this type of data where you explicitly model the spatial dependence. And that was a follow-up to earlier work that I did with Tim Johnson, who I assume is in the room there somewhere, and Tian. Somewhere and Tian Yi and others, where we built a sort of fully Bayesian, fully specified model with sampling to explicitly model the dependence in a spatial probit model. So this work got started by collaboration with Ioannis Cosmidas. He was at UCL and now it's at Warwick. And he has lots of experience with modeling binary data. Experience with modeling binary data, and we had a joint student, Patya Kendalova. And we sort of dug into this and we got started by realizing that there's a very basic problem, and that is how to actually evaluate the methods you're working when it's very actually quite difficult to simulate that data. So that's the first part of the work I'll talk about is how we actually develop a comprehensive and realistic simulation model. So what are we talking about? We're talking about we are not going to do Gaussian models. We're not going to do the general linear model. We're going to do a generalized linear model. You can see this says. Linear model. You can see this as part of a logistic or probit regression. And this, of course, the first sort of hurdle you have to deal with is that this, of course, is not something you can fit explicitly. It requires iterative optimization. And there's particularly a well-known problem that if you get involved in binary data that you will find out about, and that is the problem of separation. So the best way to see separation is a little illustration here. So if we take the top If we take the top model, this is wild. The camera's kind of freaking out. That's okay. If you take the top model as kind of the true data generating process of covariate X and the probability of actually seeing something, then that's the top. And then let's just say, here's a realization. We have a very small data set, only the 10 real observations. And from this true model. Now, if we fit a model to this, you can see that probably we can increase the likelihood by putting a Can increase the likelihood by putting a steeper logic curve because, in fact, there is a separating plane between these two. It turns out that all the events with low x are below some level, and all the zeros with low x are below some level, and all the ones are above some level. And so this says basically that we could drive the likelihood to as large as we want by making basically an infinite slope in the linear scale, which corresponds to the step function in the probability scale. Um, in the probability scale, so this basically leads to a great instability and kind of a huge nuisance. And this is something if you take a discrete data course, you'll learn about this and how to detect it. But what we want to do here is look at mass univariate frameworks for fitting binary-valued neuroimaging data. And in that context, we can't afford to inspect individual voxels. We need to have a sort of process that's just going to work in general. So, of course, David Firth, another Warwick faculty. David Firth, another Warwick faculty, proposed back in 1993 as a that he identified this problem and he proposed it as a finite sample bias problem. And he showed that an appropriate penalty to deal with this would basically correspond to using a Bayesian model with Jeffrey's prior and show that this correct this first order bias. So this means that instead of optimizing a likelihood, say L beta, we're going to optimize L star beta, which has been multiplied by the Has been multiplied by the square root of the determinant of the Fisher's information. And Ionis has gone on and both with David Firth and just on his own, developed a whole series of literature on how to generalize this result to other settings. So, and this is what I said before, we need to, this is really important for us because we cannot be dealing with diagnostics, or frankly, it's just annoying to have a result that may just be an image peppered with NANs saying, sorry. Saying, sorry, model didn't converge there. So, just briefly, this is the work that we did with Tim. And our first goal was to see, okay, how does this Bayesian model compare with this mass univariate approach? The motivation for the work with Tim, in part, was to be able to use a logistic appropriate model with not so much data, just say hundreds of subjects. Of course, our motivation. Of course, our motivation is that we have the UK biobank. So we have thousands of subjects. And the question that we had, and we wanted to answer kind of in a nuanced way, was: do we really need the Bayesian regularization? Is there enough data that we can simply just use a mass univariate model? So just briefly, the spatial Bayesian model that we fit, fit a probate regression and then specified a spatial prior on the regression. Prior on the regression coefficients. So basically saying that within some neighborhood, this conditional auto-regressive prior, that the prior was that you would be an average for any given voxel, you'd be an average of your neighbors. Okay, so the challenge we had is that when we did that work with Tian Yi was that coming up with realistic simulations. So what we did was just some 2D simulations where we plopped down square lesions of varying sizes. Of varying sizes. And that was okay, and it's an easy way to generate large homogeneous sort of rates, but it's not really reasonable. And the reason you might think about this, you might say, oh, well, you have UK bibe and go fit, you know, to tens of thousands of subjects a probability or a logistic regression model and call that truth. Well, I could do that, but if I then drew samples from that, of course, there's no dependence. And so I would get salt and pepper errors and structure from that. Errors and structure from that, and that's not what I want. It's not what real lesion data looks like, it's clumpy. So, we proposed to develop this framework where we would say, okay, we're going to take a large amount of data, as much as we can, and work out some ground truth. And in this setting, even though we have tons of regressors, we're just going to look at the impact of age. And then based on this fitted model, I can now simulate as many subjects as I want for any given age. Subjects as I want for any given age range, of course, within the age range that we start out with. And then here's the key sort of contribution here was that we would then add, we would use a smooth zero mean Gaussian random field and add that to each realization of this, this mean. So this is how we're going to simulate data to give us smooth binary data that has a chance of actually looking like real lesion data. We have this prediction here. Have this prediction here, and we're going to add a GRF to this. And then through the Gaussian CDF, and this is greater than a half, that will be our lesion. So, this GRF is going to assure, instead of just adding sort of salt and pepper noise, that whatever the dispersions from the population mean that we've trained up here, it will be a smooth process. Fine. But of course, the question is, how do you define this? There are many different ways to parametrize and simulate Gaussian random fields. Parameterize and simulate Gaussian random fields. And so that was the sort of heavy lifting that Petya undertook and did a bunch of different evaluations comparing different parameters for the Gaussian random field. And so the key thing was the scale parameter. And so she tried different ones. And what we tried to pay attention to was what was the average lesion size? So how good are we doing at comparing the average lesion size over different sets of age ranges? So she made some bins and this. And this is she's showing both the median and the mean here. But what we're seeing here is: so if we forget the mean, the dash, and just focus on the means, the medians. And these are the median from the real data. And then these are medians when we used our process to sort of add smooth randomness to the prediction. And what you can see is that nothing is perfect. That is, so you can choose a scale. Is perfect. That is, so you can choose a scale to parameter, and that's just going to be way too big. That's going to give us way too big lesions, or you could choose it sort of too small. But of course, it never perfectly lines up. But we found a compromise, 1.5 was pretty close. Now, the one thing that's somewhat disappointing is that, of course, I'm showing you a somewhat simple picture here, which is the median. What was the median size of lesions that were created by our simulation framework comparing to the reference of truth? When you look at the box plots, you can see that. box plots you can see that actually truth is actually way more variable so basically in truth we get some really really large lesions and we did sort of scratch our heads about this and if she spent more time and worked on more perhaps heavy-tailed correlation functions for the Gaussian random field we probably could have gotten this off but let's put our point here was to generate data that was realistic To generate data that was realistic enough. And so we could be more realistic, but the key thing is that we are not generating data that has salt and pepper noise on it. We're getting things that are realistic and display sort of dependencies that match at least the median trend. And so we sort of said, okay, this is good enough, but certainly we could go back and spend more time and probably consider more complex random field error processes to try to match these really heavy-tailed patterns here. But we stuck with just the 1.5. We stuck with just the 1.5 parameter. So, what we're going to do is we're going to compare the Bayesian model to the mass univariate penalized maximum likelihood method in terms of bias MSE. And the reviewers prompted us to sort of look at the asymmetry. And so we call this P-UP, probability of underestimation. And then also, just in general, how well does the spatial map of the Of the parameters agree with truth. So we also compute the Pearson's correlation across space of the estimated and the true values. And we did this for generating 1,000 times. We generated data sets of size 250 or size 1,000. And then again, so it's in the setting where we have truth, we're generating realistic data, and we want to see how we're doing. The basic things that just show is sanity check, we are sort of doing a decent We are sort of doing a decent job of simulating maps. So, here is from all 14,000 UK biobank subjects, just the sample incidence of lesions. If you're not familiar with this data, this is very common. Lesions are not uniformly distributed across white matter. They're more likely to occur in what we call these periventricular areas. So, that's what these red blotches are. They're just in these characteristic areas around the ventricles. And this is showing that our simulated data without some. That are simulated data without subjects matches well. One thing is, and this is kind of an annoying aspect of discrete data, is that in continuous data, you just define the mask, say, based on the M and I. If you're doing sort of, you know, VBM or maybe you have to pay attention to where the signal falls off in fMRI. But with this data, eventually you will get into gray matter and you'll have no events and you really can't model an all-zero. Well, in the Bayesian model, we could, but in this discrete, in this mass univariate setting, we can't deal. And this mass unovariating, we can't deal when there's just no data at all. And so you can note: where are there? You know, this is just something that's going to be the fact of life. With 14,000 subjects, we have 72,000 voxels where there's one or more lesions. But when you only have 1,000 subjects, there's only going to be a smaller number of voxels. So that's just simply because there are 14x more subjects here to be able to estimate these kind of to get events. Estimate these kinds of things to get events in these low, low incidence areas, but that's just a fact of life that we have to have to deal with. So we can compare the z-score between these different effects. So this is the effect of age. And of course, age is a profoundly important effect, explains a lot of variability. And what you see is that the maximum likelihood and the mean biased, the bias reduced, this is basically Frith regression, is looking very, very similar here. Looking very, very similar here. And then, of course, the Bayesian model is actually showing more stuff. And this is, of course, actually not that surprising. The Bayesian regularization gives us more certain estimates. So these C-scores, of course, are not classical. They are posterior estimates divided by posterior standard deviation. But because of the spatial regularization, the standard errors are smaller and we have bigger Z values. And so that's what we're seeing here. But you don't really need power for this particular contrast because, I mean, age is just such a. Because I mean, age is just has such a profound effect. Comparing directly the scatter plot of these beta coefficients for age, you can see this is the difference between the different methods. So this is estimated age versus truth. Again, we know truth because it's a simulation setting. And what you see here is that the maximum likely method just has some crazy, crazy high values. It just kind of goes off the rails. And these are probably cases where there is near perfect separation. Near perfect separation with respect to age, and so it just is having trouble. The first regression has less of that and pulls it in. And of course, the Bayesian method with its regularization is actually doing even better. This is a busy table, and I need to want to get to the new work, so I'll just quickly summarize this, that when you don't have much data and you have very rare incidents, the Bayesian method is the The Bayesian method is the clear winner. Okay, so if you look at the mean squared error, so look at these super low, these are crazy low incidence rates: 005 to 0.01, those are crazy low. And indeed, the mean squared error on the Bayesian method is much smaller than the ML. But crucially, this just this, let's face it, the first regression is almost a Bayesian method, right? So it's a Bayesian univariate method almost, and that has considerably less mean squared error than the maximum likelihood method. But as you get up into higher Method. But as you get up into higher rates, higher base rates, there's less of a difference. And that's the main story here. On the highest rates, there's actually less of a difference. And if we go up to a thousand, and let's not forget we are motivated by the large data setting. Actually, at the highest rates, there's almost no difference between the different methods. And so that's the takeaway. So if you are stuck with very low sample sizes, and in this world, hundreds of subjects is low sample size, you can have a lot. It's a low sample size, you can have a lot to gain from the Bayesian method. But what we're seeing here is that with sufficiently high samples, you're doing okay. And the weird thing about leaving the Gaussian model and stepping into discrete data is that the base rate is super, super important. So if you are super, super interested in these super low incidence voxels, then you, again, you still have something to gain from using the Bayesian method. But as long as you have enough samples and Long as you have enough samples and you have decent incidents, then you're doing just fine with the classical method. So I'm just going to skip through here, just showing you what the lesion map looks like. Again, these periventricular regions popping up. Consistently, we use a square root of probabilities just because it better illustrates what's going on in these sort of non-high intensity areas over here. And so we just wanted to actually use the method. Wanted to actually use the method in anger now with these 14,000 subjects, not just using age, but using sex, an important nuisance, head size, interactions. And our collaborator was very interested. Michelle Veldmans was very interested in developing a cardiovascular risk score. If you know anything about the, there are various risk scores out there, and generally, you should not go and invent your own risk score. I'm forgetting the I'm forgetting the one out in Massachusetts with the nurses. Yeah, they haven't won too. But it turns out you cannot compute that score for the UK Biobank subject because they haven't collected the same variables. So she created a risk score that could be computed just on using the data that's in the UK Biobank, which is cholesterol, hypertension, diabetes, week-to-hip ratio of smoking, and APOE status. And you see what you expect. And you see what you expect: profound effect of age, not that strong effect of sex or age interactions. Head size is important, but what they were very interested in was the CVR score, was also very important. So even in the presence of age and these other effects, the CVR is very important. It's also really fun to then look at the individual impact of the things that go into the CVR risk score. And so here, these are what I might call marginal models. Are what I might call marginal models. So each model is just one of these, sort of like high cholesterol and just the nuisance, age, head size, and all this stuff, but nothing else. And so you can see that actually a lot of these don't really explain much, hypertension, wisdom ratio, diabetes. But then you can ask the question, well, once you put in all the other things, how much do they explain? And that's pretty interesting. Once you add in all these other variables, the importance of diabetes is pretty weak. But even in the presence of all these other variables, hypertension. these other variables, hypertension is still quite important, as is in the sort of the highest incidence areas, waist to hip ratio. So this is something that is a practical matter. I find it's really useful to compare. So a marginal versus a joint model and seeing what actually matters in each of these. Okay, so that was the first work and it was kind of what the first work that Petya did on this data. It was surprising that it was so difficult to actually So, difficult to actually develop a framework. And indeed, we could do better, even and do a better job at matching the exact distribution of sizes. But we are generating smooth lesion maps that follow a known true map according to a covariates, and that is what we needed to develop a simulation framework. And we have confirmed the value of the Bayesian method for low n, and especially when you have a low base rate, but for scission n, Low base rate, but for Cision N and super, not super, not super rare incidents. Uh, this penalized ML Firth method is doing pretty good. But also, if you're doing any of this work, just do not even start with maximum likelihood. It's a nightmare. You'll get tons of divergent, non-convergent voxels. Okay, so then for the last project in Petcha's thesis, Petcha's now works for GlaxoSmithKline here in England. She's one of these totally 100% remote workers. She's still enjoying her life in Oxford. Still enjoying her life in Oxford. We wanted to look at the longitudinal data that's being collected. And so let's just do some epidemiology 101. So I have a really good sense of probability. I think even maybe some non-statisticians do. I've always had a little bit of trouble with odds, but I get it. It's the probability of the thing happening versus the probability of the thing not happening. Okay, okay. And likewise, when you talk about, that's just a baseline measure, when you're talking about measures of association. Measures of association to me, sort of like how much more likely is something going to happen relative risk makes way more sense to me of how much more likely are the odds of that thing happening relative to under you know risk factor happening versus the odds of that risk factor of that thing not happening when there isn't a risk factor. So this always like I always checked on this. And in fact, most people find it's easier to interpret relative. It's easier to interpret relative risks instead of odd ratios, but the canonical link function, as determined by binary data, is that you should be using probit, and that naturally leads itself to an odds ratio interpretation. So some people have argued that actually you should stick with, you know, forget what's optimal. You should stick with what is most interpretable for you. Other people would also say, it's like, yeah, but you know, odds ratios are almost relative, Risk. I mean, come on, you know, look when the baseline rate is really, really low. Rate is really, really low, they're almost the same. It's like, okay, yes, for small odds ratios, for very low incidence rates, yes, these are kind of straight. But once you get up to a strong odds ratio, even for modest incidence rates, there is actually quite a big disparity between. If you just assume this was your odds ratio was three and hence I correspond to a relative risk of three, you'd be underestimating. So this is more evident to say, like, what could we just More of an essay, like what could we just develop this directly? Develop a model directly in terms of relative risk. And what does this mean? So, what how do you get relative risk regression? Well, you don't use a logic link, you use a log link. So, and this is what I've said already before, which is I think really only hardcore epidemiologists and gamblers really have a good intuition about odds and odds ratios. So, let's use a log link and do a regression that completely. Do a regression that completely natively can be interpreted as how much more likely is this thing to happen when it doesn't happen, or how much more likely is this thing to happen with a one-unit increase of this covariate X holding everything else equal. Hey, Tom, it's about five minutes. Thank you, Chris. Okay. So I'll just zip along here just to quickly say that this is still numerically iterative, and you still have the problem of separation. And you still have the problem of separation. So I just illustrated here, whereas the logistic fit might look like this, the log linear fit might look like this. So you still have this problem of separation. Also, we know that we're not using the right model here. So this is a good opportunity to use a generalized estimating equation approach. This has been found to be more stable. And yes, it may happen that individual risks can go above one. There's nothing to stop that from happening, especially when you have low rates. And if you're really just interested. Especially when you have low rates. And if you're really just interested in the coefficients, you're gonna be fine. Okay, so just to summarize, we're gonna use a log link, we're gonna use a marginal model, which estimated with generalized estimating equations, and we're gonna use the link, the variance, mean link implied by a Poisson model, and that gives us some stability. And then we'll use robust standard errors. And we're going to use the Fritz penalty. So I'll just zip through this. So, I'll just zip through this. If you've seen this before, you know this well. Otherwise, go read up a balance generalized equations. Just to keep thinking that we are going to use a correlation, our working correlation will use a compound symmetric. Now, we only have two measurements in our data, so it doesn't matter, but that's the easiest thing to do, just assume equal correlation between the multiple visits. And the way you actually then implement the penalty, so as Ionis is. So, as Ionis is quick to point out, there is no one unique kind of right thing to do in this setting since we're in this sort of estimating equation setting. But it seems logical that probably the same first penalty based on Jefferspar is probably the thing you want to do. So again, you're just adding this penalty for the pth coefficient. Okay, and that's the lovely sandwich estimator variances of the type that we use. I will just, I'll skip this for the sake of time, but we just, we do way. Um, but we just we do way better. So, when you do a penalty, you get many fewer divergent fits. And just as an illustration, we did a little quick simulation with just in a very punishing setting, 50 subjects, and the, you know, the penalize GE converge in like in 10 iterations or less. And if you don't penalize, it just goes off. It just, it, it, it goes off. And so you have to detect that. And the software in R will look to detect that. And we just, it wasn't. That and we just, it was not much of a problem with the penalized method. I'll just quickly say that in simulation settings, we just verify that the regularized and the penalized give quite similar estimates. The main difference is that the penalized method is giving us lower standard errors, which makes sense with the penalty and just gives slight differences in the z-scores. Quick advertisement for longitudinal modeling: it's really important that you appreciate that if you just bump. You appreciate that if you just bung in a time-varying covariate and just as model as a single variable, you are explicitly assuming that the longitudinal and the cross-sectional effects are the same. And you can definitely see that in, for example, this is from another paper, that you often find that the longitudinal and the cross-sectional effects are not the same, and that you really should be modeling them separately. And that just means you have one time invariant version and one time-varying. Variant version and one time varying. And that simplest thing for if it's age, you could do mean age and then the subject residualized age, or you could just do baseline and difference from baseline. There's a sort of standard reference on this is Neuhausenkaisch for that. So this is the model we fit with this data age, time difference, and oh, I'm almost out of time. So let me just zip through to the cool sort of results, which I think are right. What are the things that I think are most exciting? So I think what's really surprising is that the What's really surprising is that the age of it is actually that the age effect is quite homogeneous, and the effect of a longitudinal effect is actually quite homogeneous as well, but weaker. And where's a table here? Sorry, right here. This is the take-home message that there is basically an 8%, here's our relative risk averaged over sort of high, relatively high incidence voxels. There's an 8% increased chance of getting a white matter. Of getting a white matter hypertensity, if you compare two different individuals who differ by one year, but when you look within an individual, there's only a 5% increase in the risk of developing a light matter. So here's this is just demonstrating the difference between the cross-sectional and the longitudinal modeling. And there is just less risk when you look at an individual of developing these as opposed to looking cross-sectionally. Okay, so let me just dip then to the conclusions. I would claim that the binary value of neuromission data is going to grow in. Binary valued neuromaching data is going to grow in importance. MS, probably it's an important area, but what's really different is the white matter hyperintensity data is. We all have these. They are easy to image with a flare sequence. And the number of studies doing longitudinal data is growing. The UK Biobank just got funding to go up to 60,000 subjects for the second visit. And so we need these scalable, flexible models. And I would argue that the relative risk of penal HGEs gives you something that's interpretable. It's scalable and more stable than the non-penal. Scalable and more stable than the non-penalized one. And I think it can be an important complement when you have the subject sizes to the Gayesian methods. I'll stop there. Thanks. Hopefully you can hear the applause, Tom. Thank you, Tom, for a great talk. I think we have time for a couple of questions, so I'm going to bring the microphone right on. Hi, Mark. Hi, Eric. Thanks for. Hi, Mark. Hi, Yerk. Nice to see you guys. Hey, Tom. This is. Hey, Tom, can you hear me? Yeah, here. Okay, great. A very nice talk. It's great to see that binary valued data is starting becoming more important in Virginia. So, questions first of all: this 14,000 subjects from UK BioBank, is this available for people to use? Okay. Yeah. Okay, yes, and crucially, but Bianca, the lesion finding has already been done for you. It's there for you to take. Great. And then, second question is: this longitudinal data, is it also in the UK Biobank? Yeah, it's all there. It's all there. It's growing every year. Great. And then final question is: suppose that there's a lesion at a particular voxel. Now, do you expect this lesion to stay there for the Stay there for the, you know, for future visas. Very good point. So, in general, yes, these white matter lesions do not resolve. So, the main reason why they would go away is just that it's not a perfect segmentation. And so, and people who are in this field would say, yeah, Bianca is only so-so, and you can do better with this or that. And you probably can. But so, that's the reason why you would, it's not strict. You would see some things disappear just due to imperfect segmentation. Hey, Tom, it's John Cornig. Very nice talk. How are you doing? My question is, just on the logistic regression part, choice, logistic regression versus relative risk. Well, the other big advantage of logistic regression is that it's invariant to changes in prevalence. And I just wondered if that's, I mean, okay, bio. I mean, it's okay by completely random representative sample, or could there be issues in your relative estimates? So, is the yeah, I mean, so certainly at a given voxel, uh, are you saying that at the given voxel, I mean, we are modeling differences in prevalence, but I guess you're saying is that you don't have equivariance if you were to just suddenly shift the prevalence. Um, right, yeah, yeah, and so I guess that would be. So, I guess that would be important to consider when interpreting them, but I think what saves us a little bit is that you are just fitting a separate model at every voxel. So, there are definitely some super high incidence voxels and there's some super low incidence ones. But yeah, if it's mixed, that's a good question. That's an interesting question. That's another question I'd love to see someone do is like basically do clustering based on spatial binary data, discover these weird multivariate patterns that could actually be instances where there's a big difference in the baseline rate. A big difference in the baseline rate just hidden among these onseen clusters. But certainly for one binary data set, you can't tell if there's heterogeneity, right? Okay, so I think for the sake of time, let's thank Tom again. Thank you, Mandy. I wish I could be there, guys. Gals. And hopefully, we can, I hope, maybe if you're sticking around for the coffee break, but if so, if you have additional questions, they could ask, maybe just catch you at OHPM, right? Yeah, yeah, okay. Right. Yeah. Yeah. Okay. Thank you so much. Okay.   Maybe I want to be able to eat it. Hopefully, either just thinking bits. Do you need your unscrolled? Sure. See if it works. Oh, this one. Okay, thanks. Okay, thank you. So interesting. All right. Okay, great. So now it is my pleasure to introduce Arvur Kribbin, and he's going to talk to us about classification performance of static and dynamic networks. So if you're wondering if you should be doing dynamic connectivity like I am, Iber is going to help us understand that. Okay. Thank you for the introduction, Mandy. Thank you to the organizers, Carolina, Mark, and Hernando. So, I will talk about this topic, but given that it's Friday, and given that I'm the second last speaker, the late night last night, and given the topics that came up last night, and throughout the week, I'm going to start with what I said I was going to end with, if you don't mind. So, I'm going to start with this reproducibility. So, Yarik brought up some. Yarik brought up some important topics last night. I don't know if I can mention about the discussion that we had last night down at the tables. And so with that in mind about submitting debt, submitting, I thought I'd present this first, see what you think. This is the first time I've presented this work. And I think that this, if I was to choose