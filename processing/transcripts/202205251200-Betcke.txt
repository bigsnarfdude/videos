Thanks a lot. Yeah, thanks a lot. Sorry, Timo. Timo, is it full screen? It looks a little small. Okay. No, it's, I mean, it's the preview full screen. Can't get bigger. Okay. So is it okay for you all? Yeah, yeah. Might be okay. My take okay, yeah. Thank you. I sometimes have this problem because I've got a very high-resolution big monitor here, so maybe that's the issue. Yeah, so yes, so yeah, thanks a lot for inviting me, for giving me the Sean's talk. So I want to talk today about a very interesting application and something that we started just before the pandemic and did most of the work on during the pandemic. And did most of the work on during the pandemic. And so it was in SimCSE 2019 that Ting Yu Wang, who was Lovina's PhD student at the time from Lovina Barber at George Washington University, and they were looking for boundary element applications of their XFMM fast multi-qual code. So Luvina has been working over the years on MIT or FastMIT codes and they now want And they now wanted to use them also in the area of boundary integral equations. And they knew we had our own BMPP code. And it was very nice because at that time we were looking, we were redeveloping our whole BMPP code. We wanted to get away from H matrices for very large problems. We wanted to go towards FMM. We were actually looking for an FMM code. So it was a very nice coincidence. And then, yeah, pandemic came and so we had a lot of time when I had programming in Twitter. A lot of time when I had programming and doing things, and the idea we had was, and I call it a we call it black box coupling approach. It's not about black box fast multiple or so, but it's about separation of concern. We had two very different codes. We had our Python with just-in-time compiling kernel code for dense boundary element computations. We had Lorena C, Lorena Centeno C. Alorina's Centeno C XFMM code, and we wanted to couple them with minimal interference into the codes. So, and we wanted to see with this minimum interference separation of concerns, how far can we actually get for really interesting applications and actually be competitive to the leading codes in the area. And the application area if we decided on some discussions was electrostatics, it was possible. Electrostatics was post-on-Poission simulation of virus structures. That's something Lurina had experienced on, and she was working on it with her former PhD student, Christopher Koopel, who is now a professor in Schiele. And so he came on board with us and then we started looking at these codes and went into a bigger project. And we recently got even some EPSSC funding as part of this. We are now working on scaling this. Can we are now working on scaling this whole thing up to P10 excess scales? So, something about which I'm going to talk about later in the talk. So, what's the setup? So, let me first briefly explain the setup of the whole thing. So, essentially, it is a standard Poisson-Boltzmann equation. So, we have a domain omega one. Inside the domain, we've got lots of atoms and they create potential fields, that's this right-hand side term over here. Term over here. And then you've got so in the interior, you've got Laplace equation. In the exterior, we've got a Yukalo potential. This coming from the solvent in which this whole thing is sitting. And so we want to solve the coupled problem with Laplace in the interior. And these atoms are just sitting in the exterior. We've got the recover potential. And then we've got standard interface couplings condition with interior and exterior permittivities. So, and then so. So we can split up the solution of the interior field into the Coulomb potentials. This is a potential generated by the atoms and a reaction potential, which is the potential of the interaction between the atoms and the exterior solvent. The interesting quantity is for people in computational biology is the so-called servation energy, which tells you something about the energy needed to solve. Needed to solve molecules in solvents. And this solvation energy is essentially, it is a simple sum of the reaction potentials of the reaction potential over the atoms, atom positions. So that's the quantity we want to evaluate at the end. And yeah, it's the coupled interior axiom. And if you assume the interior is homogeneous, then you can just do a penmen modeling out of this. M modeling out of this. So, there are different ways to formulate this. Here we chose an exterior field formulation in the corresponding preprint. I have the papers up later. We give nice justification why this is a good formulation. So, one can actually, you can go, it's the transmission problem. You can compute the interior field or you can compute the exterior field. It turns out that the exterior field is better conditioned. We give a nice explanation about this. We've got some spec. Explanation about this. We've got some spectral pictures and spectral figures explaining this, why this is a very complex formulation. So, and yeah, it's very standard. You've got the interior chiral operator, you've got the exterior chiral operator, you match the two up with the fitting interface conditions, and then you get this coupled systems where you've got here, Y is always your cover potential, L is Laplace. So you've got the double layer matched up here. The double layer matchup, you've got the single layer matchup, and you've got the hypersimilar operators and the conjugate double layer potentials. So, all four operators from the chiton operator. And then the right-hand side is the potential generated by the atoms and the normal potentials. So, and yeah, so this is all fairly well. And it's kind of if you're small and dense, that's something we can implement in 20-30 minutes in our BMPP software without problems. But we actually. Software without problems, but we actually want to go large, and so we needed the FMM code for that. So, and how large do we want to go? Well, actually, we want to go pretty, we wanted to go pretty large, and we want to go much larger than going to show today. So, our target application for this was the Zika virus, which has around 1.6 million atoms. What you see here is the electrostatic surface generated by this. By this, and the generated mesh, it has around 10 million surface elements. So that's what we do in a single node computation, single node 10 million elements. And it's a Galerkin code, so Galerkin interior, Galerkin X serial. And in order to go that large, we are coupling this black box coupling with the fast mitro code. That was our target. And our target was to become: can we actually get into the ballpark of the actual Park of the actual very known standard codes in that application of Possumboats. So, yeah, so in the rest of the talk, I'm going to briefly talk about our own BNP PCI code and about XFMM, talk a little bit about the black box coupling for GalerkenBAM, how we're doing this, and then I'm going to talk about practical issues, ongoing work. So we are close to finishing up a paper on doing the whole thing with finite element boundary element coupling for inhuman. Boundary element coupling for inhomogeneous in order to model also inhomogeneities in the molecule structures and other ongoing work in particular in terms of scaling this up to exascale. Because our actual goal is right now, what we want to do on single node, we can do this kind of seeker virus. We could do it a little bit larger. But our next goal is adenovirus, which is roughly around a bit more than 100 million surface elements. And we also want to model this. Uh, we also want to model this with inhomogeneities, and then we have to model the interior with finite elements, and then we easily add over bayonet elements for volume and elements for the finite element code. So, and that was enough for us to justify applying for infinite exoscape when we managed to get this. And that's the target we are actually working on now. So, yeah, so first of all, a little bit about Encode. MPPCL essentially started this on. Essentially, I started this in 2018 when, after long years with the old C code, I started getting more and more dissatisfied because C, difficult to develop for, didn't support well, modern SIMD structures, GPU offloading, model stuff. So we in 2018 we started developing from scratch a new code, which was all Python together with fast OpenCI kernels. A good discussion with Andes Clark at the time. A good discussion with Andreas Klochner at the time about this and motivation from him. It does Galerkin discretizations of operators for Laplace Hemosmax problems. And thanks to XFMM, everything is hooked up with FMM, a single parameter switch, and you go from dense FMM calculations. We can run compute kernels on CPU or float them to GPU. So the core routines, they are heavily optimized for fast dense assembly of operators, and it's really, really fast for this. Really, really fast for this. We put very explicit SIMI instructions to do very fast dense assembly and easily do matrices with 40-50,000 elements in a very, very short amount of time. So it's really the limiting factor is no computation speed for dense files, it's memory, really only. So, and the library provides coupling interfaces to XFMM, but can be easily adapted to other fast participation libraries because of our black box approach that we are doing. So, yeah, what some characteristics of So, yeah, what some characteristics of PENPPC, I'm not going to go too much into it, but essentially, we have user several technologies for accelerating things. The core library is written on Python, but we use just-in-time computation, NUMBA for quid operations, function space operation, function over grids. Essentially, everything that has order and complexity is accelerated by a NUMBA adjustment time compilation. And for the real heavy stuff, namely operators, send me dense operators, and leave which is order and. We actually use dedicated OpenCL kernels, which are heavily CIMD optimized, and we have them in single and double precision variant. And we actually see the nice difference. We can also offload all our kernels to GPOs. It doesn't make so much sense, to be honest, for integral operators on the surface. It makes more sense for domain potential operators. Why is that? Operators, why is that? We have integral operators on the surface. Just the delay in sending data to the GPU and back. For example, for simple singulator potential, you paid so much delay that it's not very competitive to just do GPU offloading, to just stay on the CPU. And if you go very large for the point when it might become competitive, you want to switch FMM anyway. But it's a bit different when you want to evaluate domain potentials. And here, for example, is comparison of. Is comparison of domain potential valuation. Here is NVIDIA signal precision against POCA. That's CPU OpenCL routine. So NVIDIA signal precision, much, much faster than POCA signal precision. NVIDIA double precision, the reason that this is relatively slow is quite simple. I did this on my workstation laptop and not on a dedicated data center GPU, which would have been fast and double precision too. But everything is nice, the AVX accelerated too. APX accelerated to. So, yeah, I don't have to talk too much about FMM here. I guess in this talk, we've seen lots of stuff and lots of people talking about fast solvers. Just XFMM minus T for those who don't know this. So XFMM is not just a single library, it's a whole evolution of libraries. Lorena recently gave a nice talk at the workshop if you organized about the seven different versions of FMM code she developed over there. FMM code she developed over the time and X FMM minus D is this kind of the newest variant and it's a it's a kernel independent FMM uh so which uses fundamental solutions to approximate the the translation operators and and also uses FFT for MTL operations. So very standard classical kernel independent FMM, but very, very well optimized for single node and Very well optimized for single node and very fast. So, but it's it's not it's not uh multi-node parameters, but it's it's very nice in multi-threaded single node, and it was pure C library. Ours was a pure Python library. We had to hook the whole stuff up together. So, what we did was we vote of better citinio on this side, they vote a very minimal interface of XFMM to Python using Pipeline 11. So, that we could just transfer over the Could just transfer over the particle information and get things started and get the result back. And everything else we did on our side, on the BANPP side. So essentially, we had a black box where we could put in source and target particles, the densities we needed, and get the result back. So that's what we had. And then our question was on our code: how do we couple this easily? And essentially, what we do is we rewrite the evaluation of a boundary operator A. relation of a boundary operator A to a vector x in the following form. So X is, because we use Galerkin, these are coefficients in function spaces. So we have a sparse matrix, which is heavily sparse, very, very sparse, easy to generate on our side, which maps these function space coefficients to actually evaluations of basis functions at quadrature points. So these are then the weights of the particles. G is the Greens function evaluation that's done. The queen's function evaluation that's done by the FMM, or you can actually put in anything that evaluates the Queen's functions interactions there, so it's totally black box. I mean, as a debug motor, it just says 10 simulation, whatever you like. So, and at the end, because we are Galurken, we need this transpose of the other matrix, which actually gives us back, does the integration over the test space. It's also a highly sparse matrix back. Now, interesting thing: the two matrices, C. Interesting thing, the two matrices C and S, what are those? C is a singular correction matrix, because obviously we have to switch over for single software updates. No, I said I want to do tonight. Oh, sorry. So we have to switch over for adjacent elements, single login, we have to switch over to a singular integration. But the FMM, there we But the FMM, there we just put over the information from regular integration. So, what we do there actually we explicitly subtract this out again because we don't want to change anything in the FMM. We don't want to have to tell the FMM what are single and non-single fields, what they know, and so on. We want to treat the FMM as black box. So we subtract that local part about adjacent element out again on our side, and we explicitly add in a sparse matrix which just contains a single integration. And that's something we do. And that's something we generate anyway, which our code has been doing all the time. So that's something we had already. So the only G was given by the FMM code. The sparse matrix is easy to generate from our side. The C matrix, you never actually explicitly generate. If you just iterate, it's just an order and pass through the elements to subtract that out. Okay, so yeah, and then so not much more to say to this. This is very, very straightforward, but a lot. This is very, very straightforward, but allows us to treat this just as a complete black box. So, I'm not going to talk much more about a single integration here, and this audience people know about this. For those who've asked what kind of integration rules we use, we use the fully numerical Erisen-Soto rules, which are very efficient for Shoto rules for Lerken-Polyno-Integral methods for weakly singular kernels. And they were developed at the end of the 90s. I like to go back to these fully numerical. I like to go back to these fully numerical rules because the reason for that is it's just very easy to accelerate in a SMD context because it's just simple array operations. If you go to special function relations and so on, to analytical tricks, you can do this also for Kalerk and stuff. But then it's much harder to accelerate in SME context. So there's a lot to say for fully numerical rules. I know here there were a lot of talks here who were talking about special functions and so on. Special function and so on, always think in the background. Oh, but how can we accelerate this nicely in SIMD and other techniques? And fully numerical is often better. Yeah, so yeah, so then how many FMM passes do we then have? In the Galerkin context, this gives us, well, for the single layer potential, single FMM pass to evaluate a single layer operator. But double layer potential, we need three FMM passes because we have the norm derivative here in the sources. Derivative here in the sources. So we've got three different source directions for the three different component norm derivatives in the contract get double layer. We only have one FMM pass. And then the UCAV hypersingular takes six passes through the FMM tree to evaluate. And the Laplace hypersingular takes three passes through the FMM tree to evaluate. In total, for the total transmission operator, which has all the operators from your carbon or from Laplace, it's over 19 FMM passes. So what we're doing at the So, what we're doing at the moment is in our new FMM course that we're developing, we're developing optimizations, similarly optimized specifically for chitron projectors, to evaluate chiron operators efficiently. So to push many, many right-hand sides at the same time as you go up and down the tree and do some memory optimizations. So, yeah. So, what's the performance of all of this? Here, this is just very simple, showing that we have. Is just very simple showing that we have all of n scaling. That's Poisson, Poisson, for sphere. That's all recognized. We see our order and scaling. Here is some percentages of where the code is spending the time. Blue is Laplace, yellow is Jucaba. Green is the singular correction and red is other. You see that the singular correction, we did some very simple kinds. We didn't bother to heavily optimize this. So that's a little bit more. So, that's a little bit more. As you go to large hog problems, this problem here was to mean unknowns. It's actually just a small percentage of the code. And we know we could optimize this better. It was good enough for us. It didn't harm the overall results very much. So, yeah, so and here is finally something nice to see. That's the Sika virus. This was our target goal to show that we actually can be competitive. So, that was done on a 40-core compute nodes. Done on a 40-core compute nodes total time 140 minutes. Uh, the actual GM rest time was 80 minutes in the iterations. Uh, the other part was for setup time, generating mouse matches and all the other matches needed. So, uh, we noticed there that with our Python code, we had never gone so large with our Python code, and we started noticing on the Python side actually lots of latency issues. And for very large codes, we are actually starting to move away from Python. We are actually starting to move away from Python. That's something we can discuss more on Friday in the discussion on because these latency issues sum up. And then, yeah, we need 18 GMRS situations, which is really good. What do we do for pre-conditioning? We actually use, because it's non-oscillatory problems, we use the simple mass matrix pre-conditioner. Originally, we used the full mass matrix and just did an AU decomposition of this, but then it turned out the code spend most of the time in the sparse AU of the mass matrix. So at the end, what we ended up. So, at the end, what we ended up doing was we did mass lamping of the mass matrix onto the diagonal and used that. This cost us one or two GMS iterations, and that's a price worth paying for not to have to add your decomposition mass matrix. Usually in the past, I was never really concerned. And you have a couple of hundred thousand surface elements doing the sparse value of the mass matrix. You don't care, it's a fraction of a second, a few seconds at most. But for 10 millions, you start caring about this kind of stuff. So, yeah, and this is the whole structure of the virus. We did, for all of this, we did comparison with the standard codes, APPS, and other codes in the community. It turns out for speed comparison is we are actually in the same ballpark of other codes. I don't want to give here exact speed comparisons because that's always a fishy subject in the sense that when you do speed comparisons, Um, when you do speed comparisons, what do you do? You optimize your code to show the uh put in the least possible parameters to get the result you want, and then you tune the other code to show the same result and you try to optimize it in a sense. That's not a very realistic scenario because people don't do it in application. Uh, in applications, you just wouldn't do it. Uh, the question is: are you in the same ballpark and are you sufficiently fast for realistic large applications? And yes, we are for both. And yes, we are for both. So, yeah. So, what we are working on now is moving on from that is full FAM-BAM formulations. We actually want to model inhomogeneities within the structure. So, and here at the moment, we started with simple Johnson-Netelik coupling, which leads to a simple system. Here, I should have said this differently: this A here is. This A here is not the finite element matrix, and M is just mass matrix, and here you've got the double layer and the single layer potential. So, classical Johnston Adelic coupling. We use Phoenix for the interior problem. We work together there with the Phoenix group at the University of Cambridge. So, for those who don't know, Phoenix is a very, very non-very large finite element code, very fast code, and massively parallel. We use BAMPPCL for the X-Ray problem. Right now, we are at the stage as BAMP-PCL is single. Is because MPPCL is a single node, Phoenix is multi-node. We can actually run Phoenix on a cluster and send the information on a single node for the surface elements to have them evolved by MPPCL that works. And that's kind of intermediate solution until we also have a fully cluster-bound code. So yeah, and one issue there, though, that we are working on still here is Johnson Adelaide for pre-conditioning is not very nice. So at the moment, we are experimenting with so-called hybrid formulations, which use. So-called hybrid formulations, which use a weak penalty-based coupling between the interior and exterior surface. So, that you decouple the two problems better, can independently solve the interior and exterior problem, and overall can easier precondition the global problem. So, yeah, we are currently working on that as well. It's not yet fully finished. So, yeah, but just to show it actually does work, this is just for constant permittivity. So, we actually wouldn't need So we actually wouldn't need FenBam to just do a BenBam comparison with BEMBAM coupling, just to see accuracies are the same. It's all working out nicely and converging nicely. And then here, comparison with APBS, which finite difference code, obviously global finite difference code, then chops off towards the infinity just a small approximation there for the phi towards infinity. And we are in good agreement with the results. In good agreement with the results we've got here minus 31.868 for the salvation energy APBS with slightly different physics because they're chopping off is minus 32.34. So for a problem with actual inhomogeneities. And that's pretty good because not only has a slightly different physics, but we also have the problem that the permittivity is written slightly differently. So the geometry and permittivity information is slightly diff differ slightly. Information is slightly diff differs slightly in APPS from us, and there's a small error from that as well. So, so, but it works well, it's close together given these two area sources. So, yeah, so what do we do now to scale the whole thing up nicely? So, we started on Friday in the penalty discussion. We can ask, we can talk more about this. We've noticed, actually, going very large, in the past, I used to be a Python fan, Pythonista, and lesser. So, and less everything with Python. I've moved away a little bit from that for very large problems because we notice issues with Python for very large problems with complex data structures. We try to implement a full, very large-scale fmm just in Python and it just doesn't work very well in terms of performance that we're getting out. We get good performance, but we don't get the top-range performance that we want to see. And so, we started a completely new set of tools. We're actually using Rust. Set of tools. We're actually using Rust for this. For those who don't know Rust, it's a low-level systems language like C C. On Friday, Pena, I can talk about why we choose Rust, right? I don't want to talk about this here. And the goal for us is to develop a set of composable libraries and have push separation of concern to the extremes. So that we have a set of tools that we can put in together in various different ways to easily create different types of fast solvers, different types of integrated. Types of fast solvers, different types of integration methods, and so on. So we have library Rusty Green Carno for just direct green functional evaluations. We have a Rusty tree library, which is highly parallel Octree library. So that's close to release. It's almost done. We saw freaking paper on this. So we've got Rusty compression, it's fast randomized compression algorithms that we need. This Rusty translation, actually, we will call it Rusty Field. That's early development, which has different tools for field translation. Which has different tools for field translation operators, so that you can switch around, test, play with different field translation operators. And then on top, Rusty FMM, Russia inverse, and so on. And also at the end, the full grid and boundary element on top of this. So it's a set of tools. So we are at the process at the moment hiring two postdocs for the whole thing to actually get the whole thing started. And we got two excess scale funding ones in order to do that. Funding ones for in order to do the software for the next few years. Yeah, just a bit of a summary. So, I mean, what you've shown here is very nice. So you've got practically usable and efficient black box coupling of Galerken-Bam and XFMM. What was so nice is that you took two very different libraries, separation of concern, coupled them together, got into the boilpark of the very first source from that area of domain. So everything is steered through interactive Jupyter Notebox, which is very nice. Uh, so which is very nice. Uh, so Python used Python as a glue language between the codes, which worked really, really well. We've got ongoing development now, which I much mentally with these rusty tools of extra scalability software for the EPSS new Archer2 system. It's kind of a mid-way cluster. UK's VATO is extra scale. So it's got 28-bita flush performance to the ROM CPUs. So it's kind of mid-stage for cluster in order for scientists in the UK now to start porting to extract. Scientists in the UK not to start porting to access the radio environments. And yeah, goal for the grants that we develop is large-scale coupled finite and boundary element models where both are totally parallel. Phoenix is already highly parallel. BAMPP is on the verge of becoming totally parallel. So yeah, just for papers, just for those who want to talk later, read up on things of those. And otherwise, thanks a lot for your attention. Oh, and before I forget, For your attention. Oh, and before I forget, I mentioned we are hiring two postdocs. One of the two postdoc positions is still to be filled, and there's going to be an advert soon coming out. And if there's somebody here who's going to be on the job market very soon, just talk to me directly if I can do some shameless advertisement for people to come to London. Thanks a lot for your attention. Thanks, Timo. So, do you have a question from Timo? Question for Timo? If not, I can start. Oh, you are? Sorry. We have one in person. Okay. So far. So far. Hi, Timo. That was very exciting. Nice work. I wondered about the downstream computation that needs to be done of this. That needs to be done of this delta G, the solvation energy. And, like, is that just summing up the normal derivative or something over the surface? I mean, or is it summing the potential times the normal derivative? There's some, it'd be interesting how that's done. You evaluate the domain potential. It's just the domain potential because the atoms at which you evaluate the atom positions, they're inside the domain. So, you compute the reaction potential on the boundary. The reaction potential on the boundary, and then you use a Green's representation theorem to evaluate in the interior the atom positions. Oh, I see. So, okay, reaction lives inside as well. Yeah. Yes, yes, yes. Okay, and what target delta G are you looking for? What's the accuracy? Is there a target that you can sort of check? I mean, it looked like you got within a half a kilocalorie. Yeah, so a couple of percent. So, a couple of percent. Yeah, I mean, we obviously have analytical experiments for validation where we got high accuracy. I mean, before digital convergence, we saw a nice typical Galois convergence fine. In practice, obviously, you've got huge error sources through the modeling of the surface mesh. So, you look something in terms of couple of percentage. So, you want that different types of codes give you results within a couple of percentage errors. Right. And, all right, a final. Right. And all right, a final one. What's the Yukawa decay length compared to the atomic spacing? And, you know, I mean, you're spending all your time doing Yukawa FMMs, but we know Yukawa is a local operator. So is there a way to exploit that somehow? Good point. We haven't looked too much into that. In terms of decay to our spatial length, I can't give you the figure right now. I would have to ask Chris Cooper and look it up. Look it up, but there might be some optimization you can do in terms of your cover right now. If you're treating your cover simply in absolutely the same way as we treat La Plus, then obviously your cover, you might be able to do some less work in the far field than with other operators. Yes. Cool. Thanks a lot. Some of our questions. Questions Andreas has a question? I think it's on. Is this on? Oh, can you hear me? Yeah. Okay. Yep. Yep. Hi, Timo. Nice work. You mentioned overheads and sort of, you know, throwing some dirt in vaguely Python's direction. Can you quantify what are those things? I'm imagining this is something that prevents you from storm scaling, or what was the nature of these costs? Nature of these costs, and you know, from your perspective, you know, they must have been sufficiently devastating that you decided that they couldn't be overcome. Um, I'd be curious. Actually, lots of our overhead problems actually to do with number. So, um, we have lots of small number function calls and lots of routine stumbler for all the order end stuff, grid operation, and so on. So, nice number loops. Uh, the issue is if you do lots of calls. The issue is if you do lots of calls from Python into Numba, we notice issue in terms of latencies, sometimes also threading issues there, slow you down. And we started having problems there. Something we can talk separately about. Actually, my PhD student, Shri, he's currently writing up a paper that's almost finished about the experience of developing a complete fast multiple library with Python and Number. And we are talking very much. Python and Number, and we are talking very much about latency issues and problems there. Essentially, the issue with Number is if you want to make Number fast, you're moving more and more away from Python style developing, but you have to shift more and more to Number and your code starts looking more and more like stuff you would write in C. So as you try to avoid latency problems, shifting data between Python and Number layer. So, and that causes issues. Yeah. Right, okay, that makes sense. So, if like that's that matches my experience. Like that's that that matches my experience that like you know being able to fuse across like sort of reduce the number of of invocations to stuff and like sort of fuse work together so that you're like reducing the O of one part, the raw number of the O of one work that you're doing, the potential to reduce memory traffic and all sorts of other overheads. So I can, yeah, Dr. That makes sense. Yeah, and we notice as we as we compare to some of the application codes, which are purely Some of the application codes, which are purely written in C on some of the standard codes in that area, I mean, they have almost over, they start their main computation almost immediately, but our code spends on the Python layer a lot of time setting up everything and so on, all these structures, which was much faster to set up in C C ‚Åá, which I never really noticed before because our problems weren't that big too before we started this FM engineering. So Thank you. So we are a little bit late, but if there is a last question, we can take it. And if not, we have a question perhaps about your model specifically. Is there any role? Have you thought about non-linear interface conditions? I know if you talk to computational chemists, sometimes they'll say that. You talk to computational chemists, sometimes they'll say that linear Poisson-Boltzmann is not always the best, and that's not your fault, of course. That's the modeler's fault. But have you thought about non-linear interface conditions? And then separately, like, what is your interest in the octary exactly from the application perspective? Yeah, so in terms of non-linearity, it's actually part of our current grant proposal that ultimate goal is to do full non-linear, and that's where we also need the finite event code for. Also, need the finite event code for. Yes, yes, we are strongly interested in that. Yes, second point: what do you mean with interest in the interest in the og tree? So we have to, as we go fully parallel, we have to develop full parallel octre. So, and that obviously, that's, I mean, that's the core. You need the parallel OG tree data structures efficiently. And we actually took the paper by Sundar Schott-Birosch. I'm very embarrassed for three of the paper. I don't have that author at the moment in my head. And modeled very much on top of that. And you're also in communication with Pali Zonder because they for comparison with their C implementation compared to our Rust implementation. But the October, I mean, we've done experiments Octavi with up to We've done experiments actually with up to a billion particles at the moment distributed onto a cluster to check that we can efficiently handle these very large-scale data structures that we do need. And a billion particles sounds huge, but that's corresponding that we have five six elements, five, six for charter points per element. That's a bit over 100 million elements that we're dealing with. Thank you. Thank you. So I think we can all close the discussion for this morning. I thank the speakers again and the panelists again.