Uh joint work, ongoing work with uh Dilan about multigamas in time. Uh so here uh since I chose a since I chose a short presentation, so I have to uh just focus on some key results. So basically, uh two parts. So the first one is reduction. So first, the original formulation in weak formulation. In weak formulation, how it helps to grow existence and equivalence results. And then, a part which is, I think, even more interesting is a part about the master equation, which has never been written before in the context of games of stopping time, mini games of stopping time, and for which I would like to highlight the originality of the structure. Let's start with briefly with the N-player game to gain some intuition about the problem. Okay, so Okay, so uh consider n players, okay, uh indexed by uh one to n, capital N, power players. Uh so each player plays a stopping time uh to k and they control via the stopping tides uh n-dimensional dynamics given by this, so coordinate width. So here two observations. So first we consider symmetry units. So first we consider symmetry games. Mn of the vector x2 minus k here is the empirical measure, okay, of the other players that's the k. Since here we consider the case dynamics. So it's symmetric. And the other observation is that, as you see, we consider the stopped process. So each player, what to say is that, actively, actively stops its process. So player K will stop the process. So, player K, we stop the process K. After that, it's frozen. And it's frozen for the others as well. And here, there is no common noise, just the opponent brand motion. So, PRK will face the following criteria. So, it looks ugly, but it's a bit ugly, it's not that complicated. So, PRK is particularly interest in the kth submit and the kth coordinates, but its criterion also depends on what. But its criterion also depends on what the other player played, actually. So in a symmetric way, so vertical measure. Of course, the value function of the case player depends on the strategies played by the other players. And so here, as we see, it's the frozen dynamics of the other coordinated period. Each player stops the improcessor. It's really important. So here's the. Yes? So ML is. Yes. So M n is a probability measure or it can just operate measure. Just some of the directs. So I mean the the guy who quit the game is still there or not? Yes, it's frozen, but it's still here. Yes. Yes, in a lot of games in the literature it's uh it only depends on the player still in the games. But here once you stop you're frozen but you're still somehow impacting the reward. I don't disappear. I'm frozen but I'm still in the game. Okay and so sorry can you explain the interaction there? Like say toe k times toe minus k what does this mean? So it means that uh it means that uh player k it's a vector, right? It's a vector and you have interactions. So this is just the vector of the two. So that's the other one. And the k coordinates, it's okay here. It's okay here. And it's controlled by the case agent. The player. That the game stops at the first time, at the first stop in time? For the case player, yes. But as I said, the frozen value remains with the others. So if we have two, for example. Okay. Okay, so you have a criterion that depends on the two coordinates. Yeah. Uh if I stop before you, I will freeze my coordinates, but it it will still be in your reward, but just the frozen value. And you, you will still be free to You will still be free to stop your coordinate. 3D lag control. Just replace the alpha by 2. That's 3D lag control. Does this resemble linking or is it like linking the first time? No, no, because you can continue after I exposer. I chose mine, but not yours. So you are still free to continue or not. The reward is going to be collected when Alice stops. Yes, then we're going to decide what. That we're going to decide about what the reward is. The reward will if I stop before you reward, you will have my stop process. Yes, I mean it will be you playing, you stop at some point and then it stops at some point. We wait until everybody stops and then we make the accounting and we get the cost reward of everybody. Yes, exactly. In a simple case, if you have a new interaction, let's say just a brand motion, the reward. Let's say just a brand notion. The reward would be just one. Okay, and for that, we introduced approximity theorem. I would need this definition. Just approximated Nashik real for the LPRS game. So, too nasty, but it's releasable. It's the usual definition. So, I won't spend too much time on it. So, of course, LPRS games are really hard to study in general, so that's why we propose a randomized weak purpose. A randomized weak formulation in the mid-field setting. So, as the number of players is very large, it really goes to infinity. So, we work on the mechanical space. So, here I have two coordinates of my mechanical space. So, of course, Cd is a set of R d value continuous path. I call TO X, my mechanical process. I will often use this notation, IT, which is in persecution with TO, and which is just the survival process. Which is just the survival process corresponding to the pit temp toe. So, yes, of course, the frustration generated by this process. So, somehow this is also the kinetic process, since you have the action between two and I. We will work on the superstar space of P integrable measures. Okay. A mu will be simply the initial condition, uh which is not important in this part because uh in this part in the formulation we don't use any problem, it's just uh it's fixed. Use an algorithmic, just it's fixed, it's given. And my set of control is the following. So I will denote it RM. So it depends on some measure M. M is a is a measure on this space. So somehow a joint loop of stopping strategies and state of the players. Okay. And roughly, yes, it's written as marshmallow problem, but b roughly it means that X will be a diffusion process under each of these controls. Under each of these controls, and I control the joint law of the diffusion X and Tau. That's all I say. So my set P of control is simply the set of joint flow of X and Tau. That's my weak formulation. And it's exactly a randomized topping time in recommendation. So yes, this this one means that there are stopping times. And there's diffusion. And this diffusion. Okay. So, here I want to emphasize: unlike our work on MIFI optimized topping, we don't need to write a stop process here because from the point of view of one agent, it's a standard problem. So, we don't need the interaction appears afterward when you have the fixed point, you have a making less of planets, but not before. Okay, so what's the game? So, we can write it with a very general form of criterion. So, M basically, what I call M is the So, M basically, what I call M is the behaviour of the other players. And given their behaviour, me, the single player, wants to optimize this. So, some reward which depends on my control and of the others. So, of the set of relaxed weak certificates. So, yes, of course, we need to use this set, which is simply a set of optimal evaluations. Distribute the set of optima given the strategy of the other players, so M. So they are marked radically of the above. And I defined very naturally the notion of mean-filled equilibrium, so a weak MFE here, which is when you have the two conditions, so of course P is optimal, even M, and then the matching condition. So that all the players have the same behavior at a Q because it's a very symmetrical game, of course. So it's very simple to produce. So it's very simple to prove the distance in this case. If you assume continuity in J, in P, M, sorry, so in the other players, and concavity P, concavity, then you have MFE. And of course the concavity here in the standard case when you just have an expectation in P, it's free. You can do it a little better. So why actually? So if you just briefly look at the proof, it's a very classical proof. It's a very classical proof, actually. So just some set-valued analysis, okay? You need to prove some regularity for this set-valued mapping. You use Berger CRM to have, again, some regularity on the RMAX. And then you need fixed point. And that's where you need concavity. Because to apply this fixed point theorem, so Caputeni fan fixed bar, you need the above the R max to be convex. The above, the R max to be convex. And since here we are maximizing, we want concrete. Of course, if I were minimizing, I would incorrect it. But just to say that we can do better than just linear case with the computation. For instance, it's very easy to consider various examples of criteria. Okay. So some examples of non-static criteria that can apply to this to this setting. So of course general function of the expectation. So of course derived function of the expectation, okay, with a phi concave. So I think this one is obvious. Optimized topping and dark probability distortion. So here with a concave distortion, you can prove that it fits in our framework. And another one, which is when the equation is a G-expectation with a concave generator. So that's three. So that's three examples of time inconsistent, so-called time consistent problem where we can prove, for which we can prove equilibrium. Of course, since it's time consistent, we will talk about pre-commitment strategy, not dynamics. Okay, sorry. Where where? There is a mistake? So capital G, the G expectation. Yes. Although you are using capital G. Use a capital G. First letter of sentence. I don't say it's just in bank notations. Capital G is not the same as. Capital D is the second order. Okay. Okay, I've got something today. No one will understand this joke outside. So, of course, I really. So of course uh a really n a natural result that we want is what I call the descending convergence. It means that if I have a midfield um equilibrium, it provides an approximated uh Nash equilibrium for the n targets when n is large. So here, yes, okay. Uh for this result I need to come back to this computation. So the case with uh with the expectation, okay, which is already interesting, of course. I need some Of course. I need some aggregate assumption, and then yes, I can construct a sequence of stopping times, so with n the dimension increasing, of error going to zero as n goes to infinity, such that for each n, the vector of to n is an approximate dash equilibrium. So that's what I call the descending coordinates. For the other one, ascending, it's clearly much more difficult, probably not. It's actually much more difficult, and probably not too general in our sense. Because if you start from a Nash equilibrium for the N-Payer games, you converges actually towards a weaker notion of mean field equilibrium. I know this is too strong. We converge to the one of the paper of Carmona de la Ruleka from twenty seventeen, I think, where the we have uh an additional relaxation. And I think they precisely choose uh this flexation because that's how you get the assembly coordinates. How do you get these assembly questions? In our case, no, and Russian is stronger, so it's harder to get these questions. I don't think it's possible in general. Uh okay, uh, ten minutes? I know, I don't actually know. Yeah, yeah. We have to talk about several star questions. But just a quick remark on an obvious method, but it's interesting because it's a connection to our earlier work. So, mid-field potential games. If the criterion of my mid-field games write as a derivative, with Write as a derivative with enough actual derivatives that we defined afterwards, which is very smart. So if it write like this, and if BLC bar do not depend on M, okay, so just standard diffusion, then I can prove that any optimum to this problem is a mid-field equilibrium. In that case, you observe that R does not depend on M because B and C B does not depend on M. And this problem is exactly what we And this problem is exactly what we started with the Jenfenk and Lisa mitty optimized uping problem. And of course, it's interesting to see that this connection, which is well known for control, is true in the case of stopping time. Of course, I would like to write the same when B and C might depend on M. In that case, the problem will not be exactly the same. You will have some correction test. Yes, but let's to see how it works. But this result is interesting because in practice, Is interesting because, in practice, it's easier to solve control problem, just to simulation, rather than games. So it's good to have this connection. Okay, let's talk about the mastery question if I have some time. Okay, just some intuition, because I go back to Markovian case. Purely Markovian dynamics, actually. But even in that case, when it processes Markovian, I will have some power dependency. And he is intuition. Just for two players. So even the two players. So, given uh total played by player two, uh the player one faced this problem. So, what is the state process for this problem? So, B1, okay, so that's the process that will be stopped actively by the player one, but also the process stopped by the other player. So, B2, stopped by player two, will be part of the state process. And B2, because it's stopped by toe two, is actually a part dependence process. But the stopping time, stopping. Process. But the stopping time, stopping rule is in general path dependent. Right. So even with a very simple dynamics, you have some path dependency that arises. Okay, that's for the n equal to because I think it's good to see the intuition. So when you write a v10 tau2, so it's like a micro. So the tortoise is determinacy, right? It's like a random variable, it's law or it's a joint distribution with a b or what's part of it. Is it just a total distribution? Uh to two uh depends on both, because uh the players observe uh each other, let's say have access to all the processes. Poto2 in general will have this structure. Okay, I two is the stopping uh the soil process of to say. But what I'm thinking is that it depends on the joint law of tau2 and b, or it's just a random variable of tau2? No, it's a random variable adapted to the graphic. In this case, in the machine setting, it will be different. But it's just intuition, yes. Because of the behavior of the The behavior of the other players who play stopping strategies and paradox strategy. I will have paradigmatic framework. Okay, so I think you already have seen that a lot of times in this week. So just some notional derivatives on the Vasertian space. So if you recall in this talk, so I defined the linear action derivative this way, basically a fashion derivative, with a new variable from the underlying space that appeared. So here the underlying space is the set of paths, so I print original. Is the set of paths, so I print omega. Okay, so I assume a nice good integral condition. And then I define the path divertives of the linear function divergence in the sense of the paper of James Fenkel. These are Ilo-Amikrain, Christian Keller, and part d√©pon PD. Okay. Just technical key. Okay, so now how do we write the bastard equation? At least formally. At least formally. Formally, please formally. So, first the dynamics of the other player. So, the infinity of player. They play the same strategy because it's a symmetric game, right? Which is a stopping strategy. And so, the dynamics will be given by what we can call a stopped micro-based. In strong formulation. Okay, so here we see it in strong formulation for the sake of simplicity. So, that's the behavior behavior of the other. They play an arbitraristically toe or I. I is related to toe or just the survival process corresponding to the topic type toe. Okay, they play it. So it's given. And me the single player, I just have dynamics, okay, diffusion, where here the law is the law of the others. But if I want to write the master equation, I I cannot just take uh uh any uh flow new. I need to specify a bit the the sh the shape of the flow. Because I anticipate uh because I anticipate uh on how the EQ EQM will look like, okay? And so given the reply of the uh one square of the other, so the toe here, I face this problem, okay, which again depends on the law of the others. So for given toe here, it's uh it's a standard Timastophy problem, even if you have some path dependency. But actually, as you see, the path dependency is not in X, but in the flow of the others. Okay? So Okay? So still, f as far as the individual PR is concerned, it's still x. I don't need omega. The power dependency is in the mu here. The flow of measure and C. Can do without omega for the single clear, which is already a bit less nasty than we could expect. So now we can familiarize the equation, as a master equation, which is this one. And so you have. I'm very brief. So we have three parts. Basically, so the first part is simply the part of the single player. So who keeps diffusing until it stops blocking up special here? So this is the obstacle part, okay, because it remains the standard optimized stopping program for the single player. And here, the part due to the behavior of the other players. And as you see here, the stopping strategy of the other players appears explicitly in the equation. Uh in the equation. Uh and that's why this this interaction polymer with stopping times are always hard to study in the in the dynamic setting because it's because you want uh you want you need to make the stopping hole appear explicitly in the dynamics. Okay? So it's a very singular equation actually. And if I want to formally identify and characterize the unit equilibrium, I have this verification theorem. So let's say let's u hat of u be the You had of you be the survival process associated with the first optimized topping time, which is given by this. So, the first time that I reached the standover is right like this if you use the survival process. Assume that I have a classical solution to the equation corresponding to hi-hat, which of course is possible, but it's just a formal verification. Then I can prove that this is indeed uh the spin time corresponding to hi-hat is indeed a strong Corresponding to high-hat is indeed a strong infidel. It's just for money, because of course it's a very simple eye question, so we will never have small solution. So, what we are thinking now is what's the appropriate notion of weak solution for it. Of course, you can think of viscosity, but for mastery question, you don't have comparison even for classical solutions. So, maybe viscosity is not the perfect way to do it. You could formally, I didn't do it here, you could formally write the HLV Fokker-Planck system, do some other solutions. Maybe it's the best way. Do some other solutions. Maybe it's the best way. I've not decided yet. But yes, of course, we need the. If you want to be realistic, we cannot just do the regulatory requirements. And since I'm the last guy to talk, I would like to thank all the organizers for this really nice workshop. I think it's one of the best I have I've been to. So thank you, Dylan, Jaksha, and Disappointment. Yes. Thanks a lot for this week. Nice picture. All right. Questions? So maybe I missed the same thing.