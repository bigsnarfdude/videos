Okay, I'm very happy to introduce Anna Causamo from Filencia, University of Florence. And so she talks about multimarginal interferogulous optimal transportation for singular cost functions. Okay, please, Anna. Thank you. I will upgrade the timer just to yes. Okay, so thank you for the invitation. So yes, my name is Anna Osmo and the work I'm gonna talk about today. Work I'm going to talk about today was carried out at the University of Ibaskila, Finland, during and after my PhD studies, and is working in collaboration with Japio Regala, my former PhD supervisor, and my colleague Augusto Jerolin. And so we are talking about multi-marginal optimal transportation, which I'll abbreviate MOT. And we do the entropy regularization and consider the so-called singular cost functions. Functions. So let's start with the first word of the title, multimarginal. So the next slide. Did I change the slide? No, not yet. Damn it. Now? No, something weird is going on. Whoops, we just lost them. Yeah, now we're on slide two, I think. Okay, very good. Oh my god. Yes. So the first word on the title, multi-marginal. So this is, I'm sure that everybody is familiar with the standard, two marginal. Familiar with the standard two-marginal OT, and probably most of you have also seen this multi-marginal framework. So, but just to fix notation, let me go briefly through this slide. So, what do we have? So, we are playing these games on a Polish measure space. X is the space, and D is the metric, so the distance, and M is the measure. And we fix the number, capital N, at least 2. This is the number of marginals, so in the standard, OTN is equal to 2. Standard, O T n is equal to 2, but it can be no bigger. Our marginal measures are called mu1, mu2, and so on, mu n. And okay, the minimism problem is the standard one. So we minimize the integral of the cost function integrated with respect to this gamma. And gamma is, gamma belongs to the set of couplings between the margins, so probability measures on the product space x to the power n, so that for each i, the push forward of I the push forward of the measure gamma under the projection map projection on the ith coordinate is the corresponding marginal measure mu i. So you have seen this before, I'm sure. And well, when it comes to going from two to many marginals, this problem, so the existence of the minimizers for this problem, it doesn't get any more difficult. So it's basically the same thing. So we are minimizing a lower semiconductor. Minimizing a lower semi-continuous functional on a compact set of measures, and we have the existence of minimizers under reasonably mild assumptions on the coached functions malc, for example, lower semi-continuity. But okay, of course, then a completely different question is whether or not in this multi-marginal case there exists this so-called mange or deterministic minimizer. So, whether or not the minimizer can be expressed as a product of. Be expressed as a product of maps, but and this when we go from two to many marginals, this becomes a very difficult question, but it's another story, so I'm not gonna talk about it now. So, this is multi-marginal LT and I promised singular costs, so let's see what they are. If and only if I succeed in changing the slide. Let's see, did it work? Yep, very good. So, what are single? So, what are singular costs? So, I consider cost functions that are of the type sum over i and indices i and j different of a function f composed with the distance between points xi and xj. And well, if this function f is decreasing, we say that the cost is repulsive. It's very natural because when points go far from each other, we get cheaper. We get cheaper. And if f is decreasing, and if also when its argument of zero, f close up to plus infinity, we say that the cost function is singular. So it's four-bit and it's infinitely costly, having two points or two particles sitting on the same point in space. And some examples. So, first, there is a repulsive cause that is not singular. So That is not singular. So, if f of t is minus t squared, we sum first the squared distances and then we put a minus sign. This is clearly repulsive, but not singular, because if x is xj, we just get zero. So, nothing too bad happens. Then two examples of singular costs. So, the first one is the most famous ever, I'm sure. It's called Coulomb cost. So, f of t is 1 over t. And this has, this is important. This is an important cost for, I think, the most important application or application of modern MOT. So, this can be used to model coulombic electronic interaction between particles or between electronic densities, and this is why it's famous. And then, the third example, so f of t is minus log t. This is an example of a singular cost that is not bounded from below. Okay. Okay, so here is our course, and then I will tell you a little bit more about the setting in which I'm working. So what do we have? We had some symmetry. So probably one of the most, let's say, one of the most interesting cases in modern MOT for singular cost is the case where all the marginal measures are equal and they are equal to some measure that I called here rho m. That I called here rho m, where m was the reference measure on our police measure space. And okay, and well, now when all the margins are the same, and when the cost has symmetry, I will take one slide back, when the cost is symmetric with respect to the interchange of the coordinates, so actually with respect to any interchange, any permutation of the n labels x1, x2, blah blah blah, xn, when we have these two types of. When we have these two types of symmetry, marginals the same, and closed functions symmetric, we actually then it can be shown that we can restrict ourselves to the so-called set of symmetric couplings of the marginal measure row. So to the set of couplings, gamma, of the marginals, but so that the gamma doesn't change under taking push forwards with respect to the perimetation tau sigma. Sigma of the n labels x1, x2, and xn. So, and why we can do this kind of a reduction in this case where we have these two types of symmetry. Basically, the reason is that, well, any coupling gamma of the marginal measure rho can be symmetrized. So, we can associate to any coupling gamma this kind of a symmetric coupling, symmetric coupling, just by taking the normalized sum of push-forwards under permutations. Of push forwards under permutations and summing, of course, over all permutations sigma of the n labels. And the cost is linear, so the cost function is linear. So we can sort of this has the same costs and so on and so forth. So we can restrict ourselves to that set. And okay, what do we have now? Now we have multi-marginal optimal transport for singular costs, and now we have spaces and measures and sets in which we minimize. So now let me add a step. So now let me add some entropy. And this for you, all of you, is probably very familiar. So the minimization problem I'm considering is that we minimize over the set of symmetric copies of the measured rho this entropy regularized cost functional C epsilon of gamma and it's defined as the non-regularized cost c0 of gamma if I remember my notation right plus epsilon Plus epsilon times the relative entropy of gamma with respect to the reference method of the product space, which I denote by n sub-index n. And so this is a like, and of course the small epsilon is the, or this epsilon is the regularization parameter. So maybe before going. Maybe before going to the gamma convergence result I would like to talk about today. Let me give you some intuition on like in this singular case, singular closed case, what are these minimum, what do these entropy minimizers look like? So the next slide there is a picture done by Augusto Derolin. And okay, before we go to the picture, let me tell you something about Let me tell you something about the non-regularized minimizers. So, in this picture, we have Coulomb Kaust on the real line, and there is a result by Maria Colombo, Simone Di Marino, and Luigi de Pascale that says that actually when our space X is equal to Rd with a Lebesgue measure, sorry, and when D is equal to one, so we are on real line, the reference, the space is really the. Real line direct, the space is really x is equal to r and the Euclidean, and the distance is a Euclidean distance. We know that actually this symmetrized multi problem for the Coulomb cost has a deterministic minimizer. So it's given by a map, or actually by n minus one maps to be more precise, right about the number of marginals. So we really can, by a function, tell where in the optimal situation every point goes. So, and of course, this kind of a minimizer that is concentrated on the graph has infinite entropy with reference measure, it's the left measure. So, now the peak done going from this non-regularized situation to the picture. So, now in the picture for simplicity, n is equal to 2. So, this is two marginal situations. And, okay, on the left. On the left, you see. So, basically, the picture tells how what these entropy regularization minimizers look like when parameter epsilon goes from a ridiculously large number to close to zero. So, on the left, you see a very large epsilon, so ridiculously large. And okay, so here now the role of the entropy is very large. So, what does entropy like? So, what does entropy like? Entropy likes things to be spread out. So, this looks a lot like a product measure and it doesn't really avoid the diagonal. So, remember, if you have a singular coast, we want to avoid diagonal because points do not want to stay close to one to the other one because otherwise the coast is going to explode. So, and now going from left to right, epsilon is going to decrease and there will be more and more ordering, and the minimizer will avoid the diagonal. And the minimizer will avoid the diagonal more and more. And actually, there is a result proven by Buttazzo, Champion, and De Pascale that says that when there is no regularization, actually, minimizers for this MLT problem for singular cost, they uniformly avoid the diagonal. So there is really like an alpha strip around the diagonal so that the support of the minimizer stays away from it. But okay, and you see almost the same phenomenon here. The same phenomenon here on the rightmost picture when the eptilon is really small. So it's really like it looks that it stays out of the diagonal. It's not really true, actually, because one can show that even when the epsilon is very small, but strictly positive, there is actually a little bit of mass arbitrarily close to the diagonal, also for singular costs. But of course, this is not seen in the picture. Seen in the picture. Okay, this was a little bit of intuition about these minimizers. And now let me move on to the gamma convergence. So this is just the definition of gamma convergence. You have seen this before, I'm sure. But okay, so well, where we have these regularized problems is a relevant question to ask whether or not these regularized minimizers gamma converts to the non-regularized ones. To the non-regularized ones, and in the framework of standard optimal transport, so for two marginals, and for famous cost functions like the Brunier cost, so the distance squared, or also the pth power of the distance between the points where p is at least two. This has been proven, this Kama convergence, and on Rd with marginal measures, measures absolutely continuous with respect to the Lebesgue method on R. Continuous with respect to the Lebesgue method on Rd. This gamma convergence has been proven by different mathematicians, so, like, and with different techniques, for example, by Christian Leonard and then Carlier Duval, Pere Mitcher. Ah, but I'm if I forgot to say somebody's name, don't get angry, I just forgot. So, so proofs exist for this, existed for this more standard framework, and during my PhDs. During my PhDs, me and Tapio and Agusto, we studied different things on this multi-martinality and like duality theory Monge problem and so on and so forth. And we also got interested in this entropy regularization and we asked ourselves how could one prove this gamma convergence for a singular cost in the multi-marginal setting? Because I mean it felt natural that okay should hold, it should hold also there. So the question was how to prove it and preferably in To prove it, and preferably in a reasonably general setting, so like a Polish space, Polish measure space, of course. The statement you want is that as epsilon goes to zero, the entropically regularized thing, gamma converges to the non-regularized thing, or what's the statement that you want? Yes, and I yes, actually, just it will be actually in the well, in the next two slides. There will be the functions, and there is also the statement. So, yes, just a second. So, it's here. So, first, let's here so first let's let me define the functional so what is the so what so let's say what did we prove and what what did we find what did we want to prove and what did we prove so now we fix a sequence epsilon n that goes to zero when n goes to infinity and other functionals are called cn and c and their domain is the set of symmetric borel probability measures on the product space x to the probability measures on the product space x to the power capital N and how are they defined? So Cn of gamma is equal to the epsilon n regularized cost of gamma if gamma is a coupling of the marginal rho and plus infinity otherwise. And then epsilon then And then C of gamma is just the non-regularized cost of gamma if c is a coupling and plus infinity if not. So these are the functionals we would like to prove the gamma convergence for. So basically C N goes to C and the theorem is this. So the precise statement is this. Let's see. So we have our coalition. We have our Polish base XDN and our singular coasts, as I defined before. And now the assumptions are the following. So we assume that our density, MERD marginal density, roll of rho, is integrable, so it belongs to the space L1 of Xm. And then we assume also something else, and this can look a bit weird if you have never worked on the singular cost, but I'm going to explain. Cost, but I'm going to explain in a second what this assumption means. So, we assume what is called small concentration of the measure row. So, this is the small concentration assumption of the measure row. So, when we take limit, when r goes to zero of the subrem over all points x in our space of the raw measure of balls B xr, this limit of the subrem has to be bounded from above by this strange number one all. By this strange number, 1 over n times n minus 1 squared. So this basically tells that, okay, the meso row can have atoms, but they cannot be too heavy. And now, as I told, this can be a bit like this can look unfamiliar. And this is why, actually, after a couple of slides, I will show you how this assumption can be explicitly used to prove the existence of something that we want to be able to. That we want to be able to fix. So don't worry about that. And then the last assumption that I call the integral assumption, you will find at the bottom of the slide. So we basically assume that, okay, this function f that we have in the definition of our singular cost C, we assume that outside a small ball close to, let's say, origin, which I call O, this function f integrates. function f integrates to something that is strictly bigger than minus infinity. And this assumption is a similar assumption to when we in the two marginal case what Prenier coached assumed that the marginal measures need to have finite second moments. So basically this gives for example the lower semi-continuity of the non-regularized coached functional capital C0. So this is this is So, this is like the finite second moments assumption. And so, when these three things hold, we have that the functional C and gamma converts to the functional functional C. Just a second, I will check the time. Okay. The proof is technical. It's and there is. There is no way of going into all of the details during this talk, but I will just outline the general structure and tell some interesting, maybe some interesting facts about the proof, some interesting steps. So, okay, let me think. So, what is easy? So, what do we have to prove? So, we have gamma. Okay, now it's only like assuming. Uh, okay, now it's only like a symmetric, um, it's a symmetric measure on the product space, probability boreal probability measure on the product space xn. So we have to put the gamma linear inequality and gamma-lin inequality. Gamma Linux inequality is now pretty straightforward. So we have this first assumption of the theorem. Well, Rollerbro is integrable in our Polish space. And we can actually like this gamma-lined mean inequality. Like this, gamma-lin-dean inequality follows pretty directly from the lower semi-continuity of this entropy, relative entropy, capital E, I introduced before. And this lower semi-continuity can be proved by actually by a kind of a change of reference measure trick. So we know by, okay, this has been proven by many, various mathematicians, for example, by Sturm. We know that, okay, the relative entropy is lower send by. Relative entropy is lower semi-continuous if the reference measure is finite. So basically, we switch from this reference measure Mn to the M-fold product of rho m, which is finite. And then we get basically for free the lower semi-continuity of the relative entropy, and then we get using this, we get the Gamma Lim inequality relatively easily. So the rest of my talk I will spend on discussing. Talk: I will spend on discussing the Kambalinsup inequality, and this is well, maybe this proof is not the most elegant. And if you find an easier proof, I'm very happy to listen. But, okay, and especially if you read the paper, it's a bit messy. So, what do we have? What do we have? Okay, first, what do we have to prove? So, we have this gamma. I didn't write it either, so I try to explain it clearly. So, we have gamma, and now it's only a single. Gamma and now it's only a symmetric Borel probability measure from the product space. And we have to prove the existence of a sequence gamma n that converges to gamma, quickly converges to gamma, and so that the Limpsoup when n goes to infinity of C and gamma n is bounded from above by C of gamma. So we have to find this approximating this kind of approximating sequence, gamma n's. Approximating sequence, gamma ends. Okay, if it happens that gamma is not the coupling of the marginal measures, then we are happy because both sides of the inequality are infinite, so good. So we may assume that gamma is a coupling of the marginal measure rho. If it happens that gamma has infinite non-regularized cost, so C0 of gamma is plus in equal to plus infinity, also. Is equal to plus infinity. Also, in this case, we are happy because the right-hand side of the Gamma-Limsuk inequality is infinite, and we wanted the left-hand side to be less than or equal to the right-hand side. So, we are happy. So, now, okay, we may assume, therefore, that the C0 of gamma, so the non-regularized cost of gamma, like Coulomb or whatever singular cost we have, we may assume that it's finite. So, C0 of gamma is finite. Is finite. And now we try to build our approximating sequence. And we are in the Polish phase. So, Caglier and the others, they used, when they proved a similar result for two marginals and the distance squared cost, they used a very nice block approximation. But here we cannot use it. I mean, at least directly. So, because, okay. Because, okay, if we tried, okay, if we partitioned the space X, it's a nice problem, like some cubes, and then if we took products and approximated the measure gamma rejected to this cube by these product measures, what would happen? We would get immediately an approximating sequence with an infinite C0, so infinite cost, because our cost function is singular. And if we Singular, and if we use these groups, we cannot avoid the diagonal. Okay, do I continue, or is there some? Do you hear me? Yeah, yeah, yeah, yeah. Sure. Okay. Okay. Yes. I I thought there was some problem with the with the um with the connection or something. Okay. Are you changing your slide or still I mean the plan? It's still because I don't have I don't have many slides actually because it's so the slide is is on purpose. It's still the same one. So I'm just like with words by words explaining the idea. Okay. Maybe I should have written more. I don't know how it can be if you are not using. I don't know how it can be, if you are not used to it, it can be difficult to follow me just speaking, but let's try. So, okay. Where was I? Yes, gamma-linsalp inequality and yes, the difficulties when it comes to using the block approximation. So, Cavlier and the others, they used block approximation to prove this similar, the similar result when we had this distance squared cost and the two. This distance squared closed in the two-marginal situation, but we cannot use this. I mean, at least like without doing anything else, because I mean, the block approximation gives master diagonal and if we have a singular cost, then the cost, then we would end up with approximants that have infinite cost C0 and this doesn't work. So we have to do something else. What do we do? Where we can, we do a block approximation, and where we can. Approximation and where we can't, we do something else. So we basically divide our measure gamma. So remember, we want to approximate gamma. You have to find the sequence gamma n so that it converges to gamma and satisfies the gamma-limpsup inequality. So, okay. Now we divide this gamma to two parts. The first part is large, its measure is going converging to going to one. Going converging to going to one when n goes to infinity. And so, and this is the part of the measure for which we can carry the core approach, the block approximation, and then there is a tail for which we in which bad things can happen. And this tail we have to take care of differently. So, what we can do? So, now we have our gamma. We use in regularity of marginal measure or of gamma. Marginalizer or of gamma. So we can so we take a compact set that approximates from the inside the support of gamma. And we take now away from this compact set on the product space, we take away a small stripe around the diagonal. So we and this compact set, set minus the stripe around the diagonal, we approximate with We approximate with the cube approximation, with the block approximation, like Cablier and the others did, because here we can do it. But we have to be a bit careful. So we have to, so we take this strip around the diagonal, we put it away, and it will be part of the remainder measure, we consider later. But the width of the diagonal strip, this will go to zero when m goes to infinity because. zero when n goes to infinity because this is an approximate that should converge to gamma but its width has to be let's say that the diameter of the boreal sets we use for our block approximation the diameter of these sets has to have they have to be very small compared to the width of the dia of the diagonal strip because otherwise because we have to be able to really uniformly avoid the diagonal and so And so, really, there has to be some space around the diagonal, because otherwise, we cannot approximate this thing with the block approximation. So, with basically product measures that come from some partition of the space X. So, okay, so what is our, so now our core part is therefore a block approximation carried out for this compact set that almost feels. Set that almost fills the support of the original measure gamma, this compact set minus the diagonal strip. So this part we can handle with the block approximation and it's okay. The measure of this part will go to one when n goes to infinity. So we will approximate better and better and better this the larger part, the core part of the measure gamma. But we are left with something. We are left with what I call the remainder part of the measure. Remainder part of the measure. So now there is this still, we haven't approximated this stripe around the diagonal, and there is also still something left like the product space X minus this compact set. There is something. So what do we do to handle these parts that can give us difficulty because of the singularity of the measure? Actually, we need, even before going to these compact set and diagonal strips, we have to Diagonal strips, we have to take a little bit, two parts actually away from the original measure gamma. We have to find two points, x and x prime, with coordinates uniformly far from each other, so that the distance between xi and xj is uniformly bounded from below by this Rn. Now, Rn is here one over n. We need to find Over n, we need to find these kind of, let's say, reference points to which we can couple the remainder part of the measure in the way that does not increase the Coulomb cost too much. So we will have to be able to control quantitatively this Coulomb, the Coulomb or the non-regularized cost in this remainder part. So we need these kind of two reference points on the subvert of gamma. Support of gamma to be able to actually handle this bad remainder part of the measure. And now you will understand better the role of these points x and x prime in a second, but first I will tell you how we can find these, because here this small concentration assumption comes into play. So the first one we get for free. So because For free. So, because we have assumed that the non-regularized cost of gamma is finite, it's clear that we have one point x with coordinates x1, x2, plus xn, so that xi is not equal to xj for all i and j different. Okay, but we have to find also this x prime that I choose to index from m plus 1 to 2n because it will be because then at the top of the Because then, at the top of the slide, the condition is nice if written. You can just these points x and x prime together have to satisfy the condition distance between xi and xj is bounded from below by Rn for all i and j going from 1 to 2n. Maybe this was not the clearest way of writing it. Anyway, so how do we, so we have now point x. So we have now point x on the super of gamma that full fields satisfies this condition. How do we find then the x prime? So now here we use explicitly the small concentration assumption that you see on the bottom of the slide. And how do we do the next slide? So it suffices to show that the gamma mester of all points y in the product space so that So that all coordinates of y are different to the coordinates of this x we fixed before. It's sufficient to say that this set has a positive, strictly positive gamma measure, because then of course we can fix a point x prime there in that set. So, and how is it proven? Well, the gamma measure of the set, as you see on the top of the slide, can be bounded using super addictivity from From below by one minus this set for all set of all y's that for different indices satisfy the condition yj is equal to xi and this condition here is actually the one marginal a one mar well a one marginal condition let's say because xi is fixed we have already fixed points x1 x2 blah blah blah2 xn so actually So actually, this second row of the inequality chain is equal to one minus sum for different indices i and j of nothing else than the mass, raw mass of the atom xj. And this then, this second term, we can bout from and there is the minus sign, so we can use this small concentration assumption, and we can bound this second term from below by. from below by one over actually by n minus one times one over n times m minus one squared and we get something that is strictly bigger than zero and so so therefore we have found this uh we have so we can fix this point x prime so that its coordinates are different from every coordinate of this previously chosen point x on the product space and um and And now we have to maybe start from a large enough n if we want to get this condition that you see on the top of this slide to hold. So maybe we have to like, but now it's like we just, there is finite fine, there are like finite distance conditions we have to satisfy. So we can just maybe, maybe start with n is equal to 100 or something, and then we then we have this. And then we have these points. And okay, what do we use these points for? We take small balls around x and x prime, and we use x. So first we use the point x. So, okay, maybe it's not easy to remember, but why did we want? So we try to handle the remainder part of the approximation. So we have approximated compact set minus diagonal strict with the coup back. Set minus diagonal strict with the cube approach cube approximation with the block approximation. And now we want to we still have some marginals to couple, some what is left from the support of gamma that there are some leftovers, for example, this diagonal stripe and maybe also something else outside the contact cell. So there are some leftovers we have to couple. Basically, there is some leftover marginal we have to couple with itself because gamma has all marginals equal to rho, and we have to couple it so that. Have to couple it so that the coulomb cost can be controlled quantitatively, or coulomb or any non-regular ice cost. So, Z0 has to be controlled quantitatively. And okay, this remainder part has a small measure. And if we can have an ice control on the cost C0 on the remainder part, then actually we can do pretty wild things there. So we use the point X to partition our space capital. Other space capital X. So we know that the coordinates X1, X2, plus Xn are far from each other. Let's think about them as particles. So these are far from each other. So actually we make a partisan to our space X. We define for all i, ai is equal to all of the is equal to the set of all y's so that the distance between y and xi And xi is at most rn over 2. So basically, we group our group our space on, we partition our space based on the distances between its points and these predefined points xi. And okay, then what we what do we do? Because, okay, let's maybe maybe one, maybe I will explain. Maybe I will explain the idea of what we do with the remainder part of the measure by an example. So, for example, now we if we have some mass, so if our reminder part of if the marginal of our remainder measure has some mass in the set A2, then okay, we know that this mass is close to the point x2. x2. So, where do we couple it? We couple it to the small balls around x1 and x3 and x4 and blah blah blah up to xn. Why? Because we know that these points x1, x2, x3, x4, and so on and so forth, they are far from each other. There is this lower bound Rn. So then, if we do this coupling, then because we know Then, because we know that in the set A1, sorry, A2, where upper mass was, the points are close to the x2, and this will be far from the other xi's. Then we actually can write the quantitative control of the non-regularized cost for this small, this product measure that we use in place of the original remainder part of the measure that sits on A2. So basically, we do these wild couplings. We do these wild couplings. We just couple widely so that we take care of the fact that distances stay uniformly far from the other one. And it's really rough. But the only important thing is that the Coulomb thing can, the Coulomb cost can be, so the non-regularized cost can be controlled and that this remainder part has a small measure. So we can, so these tails basically go to zero in the controlled enough manner. But then there is still one thing. But then there is still one thing to do. Now we have done this pretty wild coupling, but there is no guarantee that we have used all of our measure that we took initially around this point x. So remember, we had points x and x prime. So there can be some leftovers. We have to couple also them. And we have to be able to control the non-regularized cost of this coupling. So we use this other point x prime and then we simply take We simply take technicalities. There are a lot of technicalities involved, but basically we take products with the points with the small balls around this X prime, and then this what was left over when we did this first coupling trick for the remainder part. So we couple pretty widely also this, but taking care of the fact that we now take all of the marginal that hasn't been yet coupled. And there we are done. I know it's difficult. And then we are done. I know it's difficult, it's very technical, and I don't think, like, just as talking you through this was maybe it wasn't maybe the best choice because it's really a bit heavy. Maybe some picture would have helped you, but I couldn't come up with one. So, okay, so what do we have now? We wanted to have an approximant gamma n that goes to a sequence of gamma n's that go to gamma. So, what do we have now? We have this core part and the remainder part. We sum them together, and this gives us a sequence of... And this gives us a sequence of measures, gamma m prime. And what we can show is that this C0, the non-regularized cost of gamma n prime, goes to the non-regularized cost of gamma because basically the core part approximates gamma increasingly well when n goes to infinity, and the remainder part. Infinity and the remainder particle to zero in the controlled manner. So this is the reason basically. And okay, but what about entropy? Our approximant has only this, this is nothing more than the finite sum of product measures, absolutely continuous with the measure raw. So this has finite entropy. So we know that for all n, epsilon n times the relative entropy of gamma n prime, this is finite. This is finite. We don't still know that it converges to zero. So maybe we have to do an index trick that is here, but the details are maybe not important. So we have to make a convergence a bit slower. Maybe we will have to repeat some terms of this sequence gamma m prime so that we get low enough with the entropy in comparison with this parameter epsilon n. And then we re-index, and from gamma n prime, we get our gamma n. So our Get our gamma n, so our at the end of the day, our approximate. And now, what do we know about this gamma n? So now go to the bottom of the slide, look at the bottom of the slide. So we know that C0 of gamma n goes to the C of gamma. And we also know that now epsilon n times the entropy of gamma n goes to zero. So then so we are done with the gamma convergence. Well, we are done with the proof of the Limb Soup, gamma-limp sup inequality, and then we are done with the proof of the claim. Equality, and then we are done with the proof of the claim. But I know it was heavy to follow. And well, this ends the talk, and I thank you for your attention. Any questions from the audience? Yes, I have one. Yes, I have one. So, if I understood correctly, you were mentioning without the regularization, the minimizers want to avoid the diagonal. Yes. But with the regularization, you can have some allowable amount of mass on the diagonal. Is there a certain portion, like a rate in terms of the epsilon, of how much allowed mass is? I mean, yeah, I mean, not quantitative. Quantitative. So, the only thing I can say. So the only thing I can say now is that there is always arbitrarily close to the diagonal, there is some mass for strictly positive epsilon. Actually, this is, I have been like, I have written a sketch of the proof somewhere. So it's, but quantitatively, because of course it's, it goes down with the epsilon, but quantitatively I cannot say. Okay, thank you. Other questions? Other questions? And actually, I think it's a good question because it could be useful to have this some sort of quantitative knowledge about the measure of measure or antal diagonal. Because, for example, I'm now working on like the dual max, the existence of dual maximizers for this problem, and this some quantitative knowledge will help us a lot. So, it's a very good question, and I think I'm actually gonna work on it in the future. Anna, back in the motivation aspect, so for in particular for the Coulomb cost, do you have any kind of physical interpretation for the epsilon for the entropy regularization? Let's say that not directly physical, but if you think of m or they can be, but I'm not a I'm not aware of it. If you have then if somebody knows them, tell me, I'm very happy. If somebody knows, then tell me, I'm very happy. But actually, indirectly, yes, because I don't know if you have heard of this Hohenberg Kon functional that is used in that is used in density functional theory. And I don't know if you have heard about the connection of multi-marginal optimal transport for singular cost, its connections with this density functional theory. But actually, if we choose the parameter epsilon carefully, then we can. Then the epsilon regularized minimizer for this MOT problem for Coulomb cost can be used to give a lower bound of this Hohenberg-Korn functional density functional theory. Epsilon is a consequence of Log-Sobolev inequality and epsilon here is something like a blank constant times pi over two or something. So yes, there is at least an indirect link. I see. So you mean the Planck constant in quantum mechanics can be related to epsilon? Yes. And if you, so if you, if you epsilon regularize with this epsilon, that is, that is related to Planck constant, if you entropy regularize your Coulomb cost functional, then you can use that regularized functional to give a lower bound. So its regularized cost gives a lower bound for the Hohenberg Cohn functional. Confunctional that this describes basically the energy of this an electronic system. Hohenberg confunctional of the coupling. So basically, we can get some lower bound. For a quantity, people are interested in people who do like quantum chemistry and density functional theory. But I'm not an expert on that. But I can find a reference. I don't know how physical this is because I don't know how physical density functional theory is, but at least it has. Functional theory is, but at least it has some, let's say, quantum mechanical interest. Okay. Another question is, you consider general metric space, right? Yes, Mapoli space. Yeah, if there are any like particular example we can consider in that general setting. Sorry, to the repeat, please. Sorry, could you repeat, please? What is a key example in this general metric space? So, if you want, of course, the standard place is like R D. Okay, the standard situation is that we are on R D and then we have the Ledbeck measure as a reference measure. But one maybe wants to carry out these things on the Riemannian manifold, but not only. For example, if our space X is, for example, a domain. Our space X is, for example, a domain on Rd, some subset. Maybe, maybe it's like a, let's say that it's like a, but I let's say I don't know. I mean, it's maybe it's also a bit like a generalization for the fun of it. But you have some discrete space, I don't know, grid or why not? Yes, also discretization, yes, approximately. Also, discretization, yes, approximation. Also, that, yes, it's a good example, yes. Actually, I haven't really even thought about this, but yeah, or Riemannian manifold, or I don't know, some weird metric measure space with some nice properties, C D K or something strange. I don't know, or also, I don't know, maybe the measure, maybe the space is something like a pretty standard, like R D, but maybe the measure. Standard like R D, but maybe the measure is the reference measure is something well, right as you told, like discrete, for example. So, or has some atoms or has some other properties. So, maybe we want to add some information by modifying the reference measure M that is also possible. So, this is why, maybe one reason why it's good to have a result that doesn't suppose that we have a Lebesgue measure as a reference measure, because we can add information by modifying the measure of the reference measure. Measure. Okay, any other question? Okay, then let's thank Anna again. Hi, both.