Graph in the title, I've put random graphs in the title. So I'm just going to start with an introduction to group testing for anyone who's not come across it before. So let's just imagine, for example, that there's some sort of global pandemic happening and we want to test people to see if they've got a certain disease. So here I have a bunch of people, I think it's 40 people, and I've highlighted some of them. Uh, and I've highlighted some of them in red to say these are the ones who've got the disease but we don't know it. So, the normal thing we would do here is we would test each one of them individually. We'd stick a swab at their nose, put it in some chemicals, maybe send it off to a laboratory, and each of them would individually get their result back, telling them whether they do have the disease or whether they don't have the disease. So, here in my picture, we have 40 people, and that would take 40 tests. However, Tests. However, if you've got your disease well under control, then hopefully not many people have the disease. And so there's another option here, which is what we call group testing. So here's one example of group testing. We could split our 40 people here into eight groups of five. And let's take our group of five here at the top left. We'd shove a swab of each of their noses, but then we'd put the swabs in like the same chemicals. In, like, the same chemicals, and send off this like pooled sample that's got a bit of everyone's nasal whatever in the test tube, and send that off to the laboratory. So, for this top left group, because one of them has the disease, that means that test tube will contain the disease antigen. So, that test in the top left will come up positive. It won't tell us which one of the people has the disease, but the test will come up positive. This one in the top. Positive. This one in the top right, none of these people have the disease, so that pulled sample won't contain any antigens, so that will come back negative. Similarly, second one on the left, negative. Second one on the right. Here, two people have the disease. But all the laboratory will see when they test this test tube is that it has some of the disease antigen in it. So that will just get a positive result again. I won't get any extra information because two of them are positive. It's just positive or negative, depending on zero. Depending on zero or one or more people with the disease. So, here, five of these tests will come back negative when we can rule out those people. But these 15 will require some extra thought. One suggestion is at this point we could just test these people individually like we did before. So, this would now just be 15 individual tests. So, what we've just seen there is something that later on in this talk, I'll call a conservative. In this talk, I'll call a conservative two-stage algorithm, and I'll define what that means later. But we had this first stage of pooled testing where we split people into eight groups and did eight pool tests. And then we had this second stage of individual testing where we did 15 individual tests. And so that was 23 tests overall compared to the 40 tests and we did things individually. So if tests are expensive or rare, this seems good and we've saved ourselves some effort. So the set. So, the setup here is: we have n items, that is the people. Perhaps we have a prevalence, a probability of having the disease, or perhaps we'll use the mathematical convenience where we know the exact number. These are kind of mathematically about the same, so it doesn't really matter. And we can take t-tests of the form: does this group of items contain at least one infected item? That's the query we have. And so, given n and the prevalence. Given N and the prevalence P or the number of infected K, how big does T have to be to work out who has the disease? That's the group testing problem. So how does this relate to random growths? Well, suppose we're doing our testing non-adaptively, which means that we're fixing which swabs will go into which test tubes in advance and then sending off all the tests at once and getting back all the results at once. So in that case, we can. So, in that case, we can illustrate our test design by a bipartite graph. Here, down the left, we've got some nodes corresponding to the items or the people, which I've numbered one to n. And on the right half of the bipartite graph, we've got the tests going from one up to t. And we'll put an edge in this graph to say that the swab that went up person number three's nose has gone in test tube number two. And so we'll put an edge in the graph to say that. The graph to say that. So, in the title of this talk, I said we're going to be looking at three random designs. So, here are the three random designs. The first one is known as the Bernoulli design. And this is just we put in each edge with probability p. So, for each person, we put their swab in each test tube with some probability p. And obviously, we'll pick p so that this works well. But this means nothing is regular on the left, everything has a Poisson degree and the same. A Poisson degree and the same on the right. So, this is the Bernoulli design. Everything is IID independent. Our second design is called constant tests per item. And here, our bipartite graph is regular on the left-hand side. So, every item is in the same number of tests. So, every swab up a nose goes in the same number of test tubes. But each test tube might have a different number of swabs dipped. You might have a different number of swabs dipped in it. So, you know, we'll say that each one of these has out degree four or something, and then for each one of them, we'll pick four tests for it to be in uniformly at random. So the way we'd normally do this, we'd pick the four at random IID with replacement. So that means you can get a double edge in here. That's still just one dip of the swab in the test tube. Right. So people who like to Right. So, people who like to be precise about things might call this near-constant test per item, because of this issue of double edges in this random graph might mean that actually some people are in very slightly fewer tests than this number are. But basically, it's either constant or very nearly constant. And then our third random graph is where we're regular on both sides of the bipartite graph. Both sides of the bipartite graph. So every item is in the same number of tests, and every test contains the same number of items as well. So this one is regular on both sides. Every item node has the same degree R. Every test node has the same degree S. And again, because of double edges, perhaps each test might have S minus one or S minus two tests items in it, but roughly we'll have a doubly regular bipartite graph here. So those are our three designs. Bernoulli, random on both sides. Constant test per item is regular in terms of tests per item, but irregular on the other side. And the doubly regular is regular on both sides, items per test and tests per item. So the basic thinking should be here that adding extra regularity will make our design better, or at least it won't make it any worse. So we expect the doubly So we expect the doubly regular design to be at least as good as the design that's regular on one side, and we expect that to be at least as good as the Bernoulli design, which is totally random. So, aims of this talk. This one has a tick bite because I really am going to try and review some established results. Give some intuition about the thoughts of the three designs. This has an orange question mark beside it because I'm going to do my best here, but I can't promise that I have good intuition about everything. Good intuition about everything I'm going to say. Some of it is still a bit mysterious to me. And then a big red cross by be very precise and prove stuff because I'm not going to try and be very precise and prove stuff. There is one thing I want to talk about before we get on to our random designs, and that's this counting bound, which says the number of tests we need is at least n choose k, where n is the number of items and k is the number of infected items. You've probably proved this. You've probably proved this in your head already, but I'm an information theorist by training, so my information theoretic proof goes like this: there are n choose k possible defective sets, so we need log-based two of n choose k bits of information to describe which are the people who have the infection. Whereas each test is either positive or negative, which is at most one bit of information, and thus, you know. And thus, you know, we need at least t of, we need at least log base two of n choose k of these positive or negative results. The reason that I like to think about it like this is that it allows us to define something called the rate. The rate says, if we use C times this counting bound number of tests, say we use twice the counting bound number of sets, then it's like we've only learnt half a bit per test. Or if we use five times the counting bound, it's like we've learned. Times the counting bound, it's like we've learned a fifth of a bit per test. So this is like relating how close we are to this counting bound to how quickly we've learnt information. And so the rate can be a convenient way of thinking about results we have had. And the counting bound tells us that rate is up by one, again, because we only have a positive or negative result. So first I want to talk about some non-adaptive testing in the sparse regime. Then we'll have a quick break for commercials. And then I want to talk about. Commercials, and then I want to talk about something called conservative two-stage testing in the linear regime. So let's get going with the first one. So, non-adaptive and sparse. Non-adaptive means all the tests designed in advance. We'll assume there's a large number of items tending to infinity. And it's sparse because the number of people with the infection is growing sub-linearly according to this parameter alpha. When alpha is close to zero, this is growing very, very, very slowly. Very, very, very slowly. When alpha is close to one, this is merely pretty slowly. And we want to classify every item correctly with probability tending to one. So 100% right, 99% of the time. Okay, so here's a theorem. And I've kind of got this by smushing together a bunch of theorems from these people listed at the bottom, with the most important at the top and me at the bottom. So there's this. So there's this magic number t star, which has one bit that looks like the counting bound, and then one other complicated bit. And that's how many tests we need to succeed. The doubly regular design achieves this. Excellent news for the doubly regular design. The constant test for item design achieves this. Excellent news for the constant test for item design. But the Bernoulli design only achieves it in the sparsest cases and not in the more dense cases. So I'm going to So, I'm going to illustrate this result by plotting some rates that I talked about before. So, remember, a rate of a half means twice as many tests as the counting bound. So high rate is few tests is good. And on the x-axis here, we have this sparse D parameter. So the most sparse cases are over here. The more denser ones are over here. We have our counting bound of one bit per test. And this blue line is giving us. Is giving us for two of our designs and the best possible how they are performing, the rate that they can achieve. Whereas this Bernoulli design is not quite as good. It is as good when alpha is less than a third. That's where it departs there at the third. But then it's not quite as good as the rest. So I want to try and give some intuition as to why this is. So my intuition works like this. When you're doing non-adaptive testing, you've got two things you've got to do. Things you've got to do. First thing you've got to do is that there are n choose k possible sets of infected items, and that's an enormous number. So the first thing we've got to do is we've just got to chop that down to a manageable number before we can do anything else. And the second thing we want to do is we want to deal with like nearby infected sets, things that are quite similar. And I'll talk about infected hidden items in just a moment. In just a moment. So, to get this enormous number cut down, basically, we want to cut it in half each time. So, we want tests that are 50-50 positive and negative. We'll need to cut it in half about log base two and use k times. And as long as they're more or less independent, that will be fine. And so, the idea there is any of our three designs are like this, because as long as we pick the parameter correctly, it'll be 50-50, positive, and negative. Be 50-50, positive, and negative. And because of all this randomness we've got, they'll probably be roughly independent. So, any of our designs are fine there. Now, this second thing, a problem we can get is if there's a defective item, an infected item, that always appears with other infected items, because then, like, there's no way to find it, because it's all in positive tests, regardless of what its own status is. So that's the other thing that can mess us up. That's a kind of local problem. So roughly speaking, if each of our tests are 50-50 positive or negative, the chance that our item is hidden is a half to the number of tests it's in, or two to the minus number of tests it's in. So if we have an item that's in fewer than average tests, this is bad news because we've suddenly got quite a lot bigger chance that it's going to be hidden. Going to be hidden. So, this is why having constant tests per item helps because it reduces the chance that one of these under-tested items gets hidden in this way. In fact, if you think about it, the number of tests, the optimal number to get this 50-50 happens to be this, log base 2t over k is the optimal number of tests for each item. And there are k of them. So, the chance of having a hidden item. So, the chance of having a hidden item is probably about that. And that's where this number in the theorem came from. I promised I wouldn't prove anything, so I won't say anything more there. But it's just the idea of this local problem is where the term, the second term in the theorem came from, and the counting bound is where the first term came from. And so, if we look at this picture, we've got this straight line for the counting bound, and that's this chopping our big set of possibilities down into. Possibilities down into a reasonable set of possibilities. That's what straight lines look like. Whereas sorting out these hidden items in these local, big overlapping sets, this is what these curved lines look like. And so all our designs are good on the straight line, but our Bernoulli is worse than the other two on the curved line because of this problem of under-tested items being more likely to get hidden. And because this probability of hiddenness is like k log k, we can see that that's going to be more important when things are denser, because that's when k is getting bigger compared to n. So it's over here in the denser cases that the hidden items are going to be the problem. It's over here in the sparser cases where it's only this chopping down that's the problem. So that is my attempt at trying to give some intuition. Give some intuition about why, for non-adaptive testing, these two designs are good because they avoid the problem of under-tested items getting hidden. And why Bernoulli is only good for the sparse case because it's good at chopping things down, but it's bad at dealing with the hidden cases. I do want to mention two open problems about this. First thing is about what about doing this practically, right? If I give you the random If I give you the random design I used, I give you the output, the list of positive and negative test designs I got, then how do you actually find the infected set? I mean, theoretically, you can exhaust through all entries, case choices, but you don't want to do that. So what we do know is that for alpha bigger than half, there's a very simple algorithm that will be optimal. As for alpha less than half, the only known optimal polynomial time algorithm Polynomial time algorithm requires a spatially coupled design. I think Max talked about this in one of the talks earlier in this workshop. So, my open problem is explain what's going on here. Is there some extremely simple algorithm for alpha less than a half we haven't found yet? Or if not, like what's the intuitive reason why this threshold at a half is making things complicated? I'd like to know that. Over in problem two. Open problem two is that in simulations, you can do even better by adding more structure to your design, not just regular on both sides, but more structure than that, e.g., design based on codes. Here's a picture of some simulations I did. As we expect, the constant test per item in gray and the double irregular in green are about the same, as the theorem suggests. The Bernoulli is worse, as the theorem suggests, but this code-based thing is better. What's going on? Thing is better. What's going on there? Commercial break time. Two things I want to advertise. The first thing I want to advertise is this survey paper that I wrote with Ollie, who's in the audience, and our colleague John Scarlett from Singapore. If you want to learn about group testing, I recommend this survey paper. You can even get it as a paperback book if you fancy. And the second thing is, other than sort of joking at the beginning, I'm not actually. Joking at the beginning, I'm not actually going to talk about testing for COVID in this talk, other than to say that with my colleague David Ellis from Bristol, we've written a chapter of a paper about COVID specifically in terms of group testing. And so, if you want to know about how group testing has been used for COVID, you can read it in that book there. In that book, there. And I'm just posting some links to those in the chat as well. Okay, that's the end of the commercials. We can get back to the actual stuff now. So part two, the last six minutes, I want to talk about conservative two-stage testing in the linear regime. So again, we've got a large number of items, n tends to infinity, but this is linear because the prevalence is staying constant as n gets bigger, what you might imagine. Gets bigger, what you might imagine happens in COVID now. So, this means the number of infected items is going to be roughly P times N. And again, we want to classify every item correct. And so, in the first stage, we're going to do a bunch of non-adaptive tests using the designs I talked about earlier. So, we have a first big non-adaptive stage. So, any item that got a negative test there, we know doesn't have the disease. There, we know, doesn't have the disease, so we can rule those out, and then in the second stage, we want to individually test all the remaining items, everything we haven't ruled out. So, this is conservative, a term often used in the literature as trivial, but I don't really like that term, because it could be that you've kind of logically proved someone does have the disease. Like they were in this positive test, but I've ruled out everyone else, so it must have been them that caused it. That's not. That caused it. That's not allowed in conservative testing. Justification is: firstly, this is kind of what's been done in the pandemic. If there's evidence from group testing that someone has the disease, a government normally wants to send them off to get an individual PCR test rather than just say, oh, we've proved using mathematics that you have it, and now you have to stay indoor for two weeks. Didn't turn out to be popular. And the second reason is that it's mathematically easier. So here's the theorem. Again, I smushed together two theorems here. When P is bigger than 38%, just testing everyone individually is best and there's no interesting group testing things to talk about. But when P is under 38%, the doubly regular design is extremely close to optimal. The constant test per item design is a little bit worse, and the Bernoulli design is worse still. Newly designed is worse still. So the ordering is as we'd expect. And then as p tends to zero, as our infection dies out, then both the doubly regular and the constant test for items are optimal, but Bernoulli isn't. And that's not surprising because that's what we saw in the sparse case earlier. As P tends to zero, you're getting back into the sparse case. Again, I'm going to use a picture. Here, prevalence along the bottom, I'm only going to a half because. I'm only going to a half because if it's bigger than a half, you just test everyone individually. On the left-hand side here, I've got expected tests per item. So low is good here. You can see we've got the counting bound in dotted grey. We've got this lower bound that you can barely see. And then we've got our three designs compared to the black line of individual testing, one test per item. If we do this in terms of rate, which I talked about before, and remember with rate, higher is bigger, it kind of explains what's going on. Bigger. It kind of explains what's going on a bit better. Again, I zoomed in on this just to show that this blue line isn't quite at the upper bound, but it is very close. We sort of have this bumpy behavior as we go between the parameters of our design being different integers. It kind of goes from two to three to four and gives these kind of humps. So that's what we've got going on here. So, how can the doubly regular design is better? How come the doubly regular design is better here, but it wasn't in the sparse regime? I have two bits of intuition to share here. First is that if you look at the infected items and the bit of the graph connected to it, if the number of infected items is small, then this bit of the graph can't tell whether it was meant to have regular degrees or not, because you've taken most of the edges away. And so it's not regular anymore, and it can't tell whether it used to be regular or not. Or not. So when k is very small, you kind of lose this knowledge of whether your big graph was regular, because it's only the infected bit of the graph that you care about. Second reason is you want to be maximally informative. And if you have one more item in your test or one less, where K is sparse, no one really minds. But if your test that's meant to have three people in it accidentally has four in it, you vastly increase the probability it comes out as positive. Probability it comes out as positive, and thus you're not going to be as informative. So that's why I think you want to be regular from both sides in the linear regime. I'll just finish with some open problems on this one as well. So can you in fact do better by not being fully regular in terms of the tests per item, but going for like two or three? Maybe kind of morally you want two and a half. Or on the other hand, can you close this tiny gap I have between my bounds? This tiny gap I have between my bound and the doubly regular design to show it's optimal. Obviously, both of those can't be true. Maybe one of them, maybe neither. And also, what about non-conservative? What if you're allowed to use these logical deductions to find people who have the disease? So that's the summary picture of when the three random graph designs that I wanted to talk about, when they are optimal and when they are suboptimal. Suboptimal, and that is the end. Thanks very much.