Okay. Thank you very much. And thanks, the organizer, for inviting me. I guess that most of you don't really know me. I am not belonging to this community. And I guess the reason I'm invited to this, was invited to this conference and some other similar conferences is because of work I've done. Of work I've done many years ago, probably on the average, something like 30 years ago, on embedding of subspaces of LP in low-dimensional little LP spaces. And this is connected with sampling, what some of you call sampling discretization. This is a connection. Now, I decided, so I didn't work on that in the last at least 10 years or so on this subject. There is a closely related subject that I did work on, also not very recently, but let's say until two or three years ago. And I want to interest you in it. It's kind of a subject which is wide open and closely related. open and closely related to the other to the other one. And this is a problem of dimension reduction of finite sets rather than finite subspaces in low-dimensional norm spaces. So, okay, so what do I mean by that? There are several ways to present it. I chose one main one and then I'll do some perturbation on that. So we have a metric space, a finite. So we have a metric space, a finite metric space. It will, well, I didn't write it here, but it will always be a finite metric space, and we have a normal space. And we say that M embeds in X, or Lipschitz embedded in X with distortion C if there is a function from M to X, which almost preserves the distances up to a multiplicative factor of C. And you can always arrange it so that. always arrange it so that you can always arrange it so that the constant is one on the left and three on the right because you are embedding into a normed space so you can scale things if if x is another metric spaces space you cannot always do it but in all this talk x will always be a norm space so i prefer to write it like that okay so this is a this is an embedding with distortion c and the best c that you can And the best C that you can put there, I denote by C sub X of M. Okay. And I'll really be interested in, okay, what will I be interested in? I'll be interested in this number, I call it KnCX. So this is a smallest integer K, positive integer K, such that if you take any subset with n points, With endpoints of the normed space X, you can embed it into a subspace of dimension at most K with distortion C. Is it? Yeah? Is it excuse me? F of X? Yeah, you're yeah, yeah, yeah, yeah, yeah. I'm sorry. I'm sorry, F of X and F of Sorry, f of x and f of y. Thank you very much. Didn't notice this, even though I read it many times. Okay, so you should understand this thing because this is a topic of the course, evaluating, is a topic of this lecture, evaluating this K and C X. Okay, so you have any subset of X with endpoints, you want to embed it into a subspace. Embed it into a subspace of Y of as small a dimension as possible. Sometimes we want more, we don't want just some subspace, but we want subspace of a special kind, especially if X is a special space like little L P or something like that. This will be most of our concern, that you may want Y to be little L P of dimension K. Okay, but let's forget about it. My results will be mostly negative. So if you cannot embed in any subspace of dimension K, you can also not embed in LPK. Okay, so this is the reason why I'm doing that thing. But for practical purposes, you really want to embed into a space you can work with. If you could do it with, say, in LP with small, with embed into LPK with small k, then there is a chance. Small k, then there is a chance for good algorithms for all kinds of things, things like that. Okay. And you can think about c equals two. There is some place in which I speak about small c is close to one. Of course, c is always larger equals than one. Okay, so strangely enough, there are very few results on this problem. Okay, very, very, there are really four, basically four results that I can state, or maybe five. State or maybe five. Okay, so let me tell you what is known. The best known one is a Johnson-Lindstrol's lemma, which works for Hilbert space. And it says that this quantity, so Kn2 is just a constant C, remember? I pretend that it is two. Of little two is of order log n order small. Order smaller or equals than log n. So, what does it mean? That any n points in a Hilbert space embeds with constant two into little, actually into little l2 of dimension constant times log n. Okay? This is very well known and applicable. It is used in algorithms and it's also an easy thing. I mean, it's really a lemma in the paper. This is not the main. Paper. This is not the main result of this paper. Okay, there are by now. So here is, I'm again going to one plus epsilon instead of two. And it's there was for some time there was a question, what is the dependence on epsilon? The proof of Johnson Linear gives log n over epsilon square. And there is a quite a recent result of the Recent result of the Larsen and Nelson, so that this is the best possible. When I write, I'm using the notation of computer science. Theta of that means that there is both an upper bound and a lower. Okay, this is for small epsilon. It's not so important for me, but I want to state the latest result on that. There is another space, not a classical space, in which you have basically the same result. This is some kind of a strange space, two convexified serials and space. This is a result of. Johnson space. This is a result of Johnson and Aurora. It's not an easy space to even write down the normals. These are the only two spaces in which we have kind of positive results. Now there are several negative results. So on the negative side, first of all, there is a theorem of Matushek, which says it is not stated that way in this paper, but the result is that for L infinity, you For L infinity, you don't get log, you get at least n to some power. Again, this is a computer size notation. Omega means that you have a lower bound. Omega means that you have, this is a constant, a positive constant times one over C. Okay, so if you want to embed, so again, if C is two, then let's say then there are endpoints in L infinity. And there are endpoints in L infinity that if you want to embed them in any banner space, any norm space of dimension K, because every norm space of dimension K embeds in L infinity, then you need the dimension of this subspace must be at least n to some positive power. Okay? So this is kind of a negative result. On the positive side of that, there is it's, and this is the reason why I'm to check proof that the Reason why I'm to check proof that there is an earlier result of Johnson-Linus, which says that there is also an upper bound of the same thing. So, for L infinity, it's really like talking about all subspaces and all matrix spaces, every finite matrix space embedded in L infinity, every finite dimensional known space embedded in L infinity. But if you state it in the terms of what I wrote before, then this is what you get. So, for L infinity, the answer is known basically up to some units. Basically, up to some universal constants. And then there is more recently, there is a result of this was open for a while. And then the Brakeman and Sherry Carr proved that for little L1, you also get a similar result. You get that if there are endpoints, again, this means there are endpoints in little L1 that if you want to embed them in the way they write it in L1K. It in L1k, or but I'm saying in any subspace of L1 of dimension k, then the dimension of this space is n to some positive power. Okay, for L1 to embed in some space of dimension k and to embed in L1 of dimension k is basically the same because of the results that are known in what I said before, in embedding subspaces of LP in low-dimensional LLP. I will not enter into that, but it's really. Enter into that, but it's really the same. Okay, now the let me say something about the best known results here as a function of C now when C is large. When C is small, nothing is known basically. So the best results are it's very there is a big gap. What did I do? There is a big gap between the upper bound and the lower bound. Lower bound. The lower bound is like before. It's really like n to a power, a constant times one over c squared. The upper bound, all is that is known, even if you take large c, all that is known is that you can, of course, you can always embed in subspace of dimension n, just take the span of the n points. But all you if you allow large c, then all you can gain is n over c, not not power or anything. Not power or anything. Okay? The right-hand side is not trivial. It's a result of Andonian or Neyman. Okay, now there is one, so this is the latest thing that is done about that thing. One more example due to Staff New York, Gilles Pizier and myself, which says that basically the same result like in L1 happened also in shutter. One happened also in shutten one in the trace plus space. Let me remind you what it is. I hope I remind you. You look at all matrices from all operators from little L2 to little L2. You define the Shatten P norm as a, you look at T star T, this is a positive operator. You can take any power of it. It takes the power of P over 2, takes the Takes the power of p over 2, takes the trace of that, and the power 1 over p of the other side. This is a norm of this matrix. It's the same thing as looking at the single, at the little L P norm of the singular values of T. Okay, this is a norm. It's called the Shatten-T term. I'll be interested mostly in P equals 1. This is a trace class norm. There are other ways to present it. But okay, so really there are just to make. Just to make sense of it, the S infinity norm is just the operator norm. The S2 norm is the Hilbert-Schmidt norm. The S1 norm is what is called the trace class norm, or this is the same thing as the nuclear norm. But never mind, I will not use this. Okay, so it turns out that for this thing, you get a similar result that it's like the L1 result. You get the same thing for S1. Same thing for S1. And moreover, so let me just repeat: what does it mean? It means that there are endpoints in S1, in the Shatten class, 1, in the trace class, such that if Y is any subspace of the Shatten class of dimension K into which this endpoints embed with distortion C, then the dimension of this subspace must be larger than some power of n. Okay? Well, depending only on the distortion. On the distortion. Now, moreover, okay, little L1 embeds with distortion one into S1 on the diagonal. Just look at the diagonal matrices, you get little L1. And of course, in order to prove this theorem, you have to find some example of endpoints. Okay, it turns out that this example are the example that we take, this bed set, is the same as the. This bed set is the same as a Brinkman cherry car. So it's a set which sits on the diagonal. It's actually a subspace of little L1. We know already from Brinkman and Sherry Car that it cannot be embedded in a low-dimensional subspace of L1. The same set also cannot be embedded in a low-dimensional subspace of S1. Why are we interested in S1 and by the way, also in L1? Because these are spaces in which you can do algorithms on. In L1, it translates to algorithms. Money translates to algorithms, well, some kinds of algorithms, like closest neighbors and things like that, translate to linear programming. In S1, you can do something similar because computing eigenvalues or singular values is something which is computable well. So, it maybe if there was a positive result here, if you could really reduce the dimension, then it would be applicable. This says that you can. This says that you cannot do this. So this is the reason we were, this question was asked about the Shatten class after this frequency curve. Now, what I plan to do, so as I said, our theorem is really strengthening the frequent chariot result. And I'm going to, most of the talk, I want to describe the proof of that. I'll not enter into details. There are some computations which I will not do at all, and I'm not sure. Which I'll not do at all, and I'm not sure even if I'll manage to do to give a full picture of the proof. And also, most of the time it will be really the proof for literal ones of the theorem of Rickmann's return, only a much simpler proof than the original one. And the two proofs are the same, only in S1 it's more complicated. Well, let's see how much of that I can do. So, but before I do that, I want to make some remarks, mostly about open problems here. So, these are all the results that are known. Nothing is known more than that. For instance, for little L P, for P larger than one, strictly smaller than infinity and different from two, nothing is known. Basically, nothing is known. But I want to say more. I said it already. For little L P, for instance, you can ask not just what is a smallest case such that you can embed any end. K such that you can embed any endpoint into a substance of dimension k, but you may want to embed it into LPK itself. And for p between one and two, even between zero and two, even though it's p smaller than one, it's not a norm space, any n-dimensional subspace of LP embeds into Lp of dimension n. I wrote times log star of n. It's some power of log n. It depends depending on period. It depends depending on P, I don't want to enter into that. So, up to log factor, embedding into some subspace of dimension K or L P of dimension K is basically the same. So, okay, this is the kind of result that I talked about at the beginning. Okay, but it is also known, this is just a remark, that the number of bars The number above, so this number, it's not exactly the number that I defined before, because now I'm embedding into LPK. It's actually O of n without any logs, okay? This doesn't follow from this thing. Removing with log, it follows from that, and it was known for a long time. Without logs, it's something new, but actually kind of a remark, not very hard. So, this is due to new. Is due to Newman and Robinovich for p equals one, and for the other piece, it's due to me. It's not, it's really kind of a remarkable. What about p larger than two? This is not clear at all. It's known that it's smaller or equal than n square, even with distortion one. This is due to Cliff Ball. Again, a very simple argument, but this is all that is known basically. That is known basically. So, this is an open problem. Another thing is one can ask similar questions for shutting class P. What is the smallest K such that any endpoints embeds in SPK or in a subspace of dimension K? This is not known at all, and I guess this is wide open, and my guess is that K must be exponential. My guess is that K must be exponential in this case. I think that it's very different from little L P. There is an indication for this, on this, in the result of recent result of Regev and Vidic, which are computer scientists. There is a constant, it's an absolute constant, actually, if one asks not for a two-embedding or a constant. embedding or a constant embedding but a one plus one over n to some fixed power embedding so very very close to one yeah this is instead of the two embedding then k must be exponential but so this is and by the way this is not the case in lp in lp even if you ask for such a for such an embedding the k will be some polynomial in n but not exponential so it indicates that there is a big It indicates that there is a big, a real difference between little P and S P, and this really interests me a lot. So this is basically, these are just remarks. It's not, I could skip this thing really, but since everything here is open, I really want to do it. Okay, so what are the example? Remember, I want an example of actually a subset, finite subset of Of little L1, which doesn't embed well into any subs, low-dimensional subspace of little L1 or S1. So the set is what's called the diamond graph. And so it's defined inductively in the following way. So the metric space here are always. I'm talking about graphs in the. I'm talking about graphs in the combinatorial sense of the word. So the metric space are the vertices here. And the distance are given by the graph distance. So in this case, it's just two points, the distance between which is one. Here it's four points. The distance here is one, here is one, here is one, but the distance between the two is one. Is one, but the distance between the top and the bottom, for instance, is two. Yeah, and how do you get these things? So you start with D. This doesn't move very well. Okay, so you start with the left side, which is D0, I call it. And what you do, you erase, to get D1, you erase the edge between them and replace it with these two, with this diamond thing, okay? Diamond thing, okay. And to go from D1 to D2, you do the same thing with each edge. So you take this edge, remove it, and put a diamond instead of, and of course, add two more points. Okay, and the next time the D3 will be, which I didn't draw, is you delete each of the edges and replace it with a diamond. Okay, and you continue like that. So these are. So, these are kind of a sequence of finite sets. It's known that these things are embedded in little L1. I will not go into that. So, you can think about them as being subsets of little L1. These are the examples. I mean, of course, you take large n. I put it also on the blackboard in case we need it later, but I'm not completely sure we need it. And now, the proof that this does not embed in a low-dimensional subspace of. Embed in a low-dimensional subspace of little one or S1. Say, let's start with a okay. What it does, it's imitate a proof, not the original proof of Boekman of Terricard, but a proof of Leonore. And later on, I'll say more about that. It's really not the proof of Leonore, but even a development over that. And it consists of two things, proving two things. First, you prove that the DN. That the DN, so this diamond, this diamond of generation N, does not well embed in, no, does not well embed in SP for P larger. Think about, we'll talk about SP for P larger than one, but very close to one. So first of all, you show it does embed in S1, I just say embedded even in L1, but it does not embed in SP or any. It does not embed in SP or any P larger than one. Doesn't embed, I mean, the distortion grows with N. So, this is the first thing that you have to prove. But you need some exactly the precise quantitative estimates here, which I will not do, by the way. This is one thing that you do. And the other thing that you do that the kind of the opposite thing for subspaces. So, you prove that a k-dimensional subspace of S1 is closed in an. One is closed in a natural way, even to a subspace of sp and, in particular, it's well embedded in sp again with some precise quantitative estimate. So a subspace of low dimension well embed endpoints, the dn does not well embed in spn from this quantitative estimate, you get some relation between the number of points in dn. The number of points in dn and the dimension of the subspace. I will not do the computation, I'm just describing the philosophy. Okay, now I will mostly concentrate on the so these are two completely different things. The second one is kind of in the linear theory. Everything is about linear subspaces, and this is something we know very well for a long time in the geometry. A long time in the geometry of Banov spaces, but not a but this was actually not known what we need um but things like that for instance the same thing for LPE was known for a long long time so what uh Leon Orr the main the main point was the first part the main new thing okay so and I'll also concentrate mainly on the first part I'm not sure if I'll get much I'm not sure if I'll get much of the second part. Okay, so the proof of the first part is very similar to the for sp is a very similar to the proof for little LP and it depends on the estimate for the uniform convexity modules of sp which are by the way the same as for LP. I'll go into it in a minute. The proof that I actually described is not that of Leo, that of Leonard or Is actually not that of Lee and Orr, but one which Bill Johnson and I found, which proves also some other things. I'll say it in a minute. And it's very easy. It's very easy, but kind of, I like it. It's kind of clever. I don't know. It's hard to say about your own word. Clever, but you'll see it's kind of elegant. Elegant, I don't know. Okay, there is actually a strengthening of that in the paper of the strengthening of the theorem of Gilles Pizier and Safner and myself. But I will jump over that. I don't want to one prove actually something stronger. Not only is that things do not embed, but do not in a subspace of low dimension, but they don't embed even as they stop. Even as a stub space of a quotient of no dimension. Okay, but I jump over there. So I want to describe this first part. And this I actually will almost a complete proof, except that I will not write exactly the quantitative estimates. So I remind you first what is the modulus of uniform convexity of the norm space. So this is, you look, it's better to look at the picture. You look, it's better to look at the picture. You look at the unit ball of the space. You take two points. They need to be on the sphere, they can be inside the ball, but the distance between them is larger or equal than epsilon. This is what is written here. You look at the midpoint, x plus y over 2, and you compute this distance, the distance between this midpoint and the place where this... and the place where this red radi radius meets the sphere, compute it in the norm of the space, the length of it, and the norm of this segment that is written, that is drawn here. Okay? This is the... But you take X different for Y. The distance between X and Y is larger equals than epsilon, not smaller or equal. Sorry. Not smaller equal. Sorry. I hope you have the great oracle. Different, I'm sorry. Another mistake. Thank you. Okay. Great oracles. I read this 10 times. Okay. So is it clear this is a model of uniform convexity? And this is a okay. The lemma is the following. I'm looking just I'm looking just at D1. This is D1, this is just the diamond with four points. And assume that it embeds into a space X with, so here I wrote the F, with distortion M, then of course, and I look at the top and the bottom, okay, these two points. So the distance between them is two. So of course. distance between them is two so of course f of the top minus f of the bottom is larger or equal than two this is trivial this is what is written here and the trivial estimate from above is 2m but it can be improved this is the main point if x is uniformly convex it can be improved by multiplying it by one minus delta of two over m turns out okay let me prove this lemma and so i hope so here is a proof And so I hope, so here is the proof. I go fast over it, but so you look at the, so this is a diamond, this D1, and I have a map into a norm space. Look at the, the map is F, but let's look at F over M. And I also assume that the bottom is going to zero. I can always shift everything on the norm space without shifting by vector. It does not change anything. By vector does not change anything, so you can always assume that one of the points goes to zero. So I'm going to assume that this is zero. This left point is going to Y, this right point is going to X, and the top is going to F of top over M. Okay? Now look at the, now what can I say about the distances? The distance between this point and this point in the original thing is one and I applied F over M. And I applied f over m, so now it's at most one. Remember, this is y divided by m. Also, here it's at most one. So x and y are both in the unit ball centered at zero. This is the okay, at this point. So their midpoint is x plus y over two. And what can I say about it? The space is uniformly converted. So you can say, so I'm sorry, you know also that. I'm sorry, you know also that the distance between this point and this point is two, so the distance between x and y is larger or equal than two over n, divided by m. So this means from the uniform convexity that the distance between x plus y over 2 and 0, this is just the norm of x plus y over 2, is smaller than equal to 1 minus delta of that thing. Okay, now the The similar thing goes with the top thing. Okay, and you get what this is not zero anymore, but you get what you get for the top thing is the same thing, except this is not zero, so you have to subtract it. Okay, the distance between x plus y over two and the top point over m is smaller equal also here. Okay, now adding these two things and using just the triangle inequality, you get that f of the. You get that f of the bottom of the m minus f of the top of the bottom is zero. I know, I know, but I want to write it like that. F of the bottom is zero, yeah. But minus f of the top over m is smaller equal than twice the thing, okay? Now you have to notice one, okay, so this is the proof of the lemma. I did it, this is what I said, okay, but there is a simple corollary of that, so you have to think about it for a minute, and this says that now. And you say that now it extends to you see the relation between, okay, so look at the, can you point the camera to the blackboard? So this is D2. D1 is actually embedded in it. If you look just at these four points, this is just D1, but with the distances being doubles original distances. In D1, the distance between this and this is... The distance between this and this is one, and here it's two. All the distances are multiplied by two. So, d1, or you, if you want twice d1, embeds in d2. Okay, if you use this thing, then well, I will not go into that. One has to think about it quietly for a minute. But if you just follow this thing, you get immediately this score. you get immediately this corollary that the relation between the the relation between m m n and m n minus one mn is the best I didn't write it I think but mn is the best constant of embedding the n diamond into a space into the space x okay so this lemma translates to this relation between mn and mn minus one okay and it gives this And it gives this quantity. And this immediately gives, and you can repeat it. This immediately gives, so it gives a lower bound on mn in terms of mn minus one, which is hard to get from exactly what it is in general, but some lower bound. And you can iterate this and you get from this you get a lower bound on mn in in terms of everything is in terms of delta x. As I said, I'm not going to. X. As I said, I'm not going to do the computation. So this is basically what Bill Johnson and I did, and the reason we did it is because this gives this shows that this diamond does not embed nicely in any space, any uniformly convex space, any space in which delta x is not, is bounded away from zero. And moreover, you get you get You get the embedding in terms of delta x, the best embedding in terms of delta x, or a lower bound in the embedding in terms of delta x. And of course, if you have a space which is isomorphic to a uniformly convex space, then the n does not embed into it either nicely. And it tells us that this gives a characterization of uniform convexity in terms, in metric terms, in terms. In metric terms, in terms of embedding diamonds, some metric spaces, these are diamonds, into X. Okay, which was it was not the first one, the first such metric embedding is an old one of Bougain with the different spaces, but this is much simpler and very kind of, I find it elegant. Okay, now, okay, now we go to. Now, okay, now we go to the remember we are talking about sp or lp. So for L P and also S P, there is a very good knowledge of what is a delta of little L P or delta of S P and it it behaves like that. It behaves like epsilon square as a function of epsilon, but the constant in front of it deteriorates when p turn to one. Of course, one is L1 is not uniformly convex. So this is not surprising, but it's important. So, this is not surprising, but it's important for us exactly how deuterioids deteriorate like p minus one. Okay, so for p between one and two, it's like that. P larger than something else, but it doesn't concern us. We are talking about p close to two. Okay, once you have that and you plug it into you plug it into z, this thing and get a lower bound on mn, you get some lower bound on m n. So, for this, from this, what you get is really this thing, it turns out. So, m n is of order. So M n is of order n to the one-half, but there is a constant which deteriorates when p of course it must deteriorate because the diamond does embed in L1. Okay, so for the for L1 this MN is a constant, two or three thing, or something like that. Okay, so it's important for us, very important exactly what is the form of. So this is a quantitative estimate. So, this is a quantitative estimate that you get. And this is what I mean that dn does not well embed in SP. The distortion is like that. By the way, Dn does not have, it doesn't have endpoint, it has exponentially many points, something like four to the end, not exactly, but still you get some estimate on some. So remember, this was the first point to show a metric space which does not well embed into S P. Well, embed into SP. The second step, so this deal with the now we have to deal with the second step, which is the following for S1 or for L1, that the K-dimensional subspace of S1 well embed does well embed into SP. Again, what does it mean well embed? And here is a difference between little LP and SP. For little LP, you For little Lp, you can refer to the results that I said before that a k-dimensional substance of L1 embeds in L1 K bar, where the K bar is really k times constant times K times locally. Okay? It's linear in K, by the way, if it would be K square, it would be also okay for this purpose. So you can use easier results than the best known result here. Okay. No result here. Okay. And of course, it's L1k, you know what is the distance of L1K bar, say, from little L P it's given by the identity map into little L P it's n to the one minus one over P. So here it's easy thing, you don't have to do anything basically. In S1, it's not the case. So I wrote it, the distortion is like embedding this, and this is what I mean. Embedding this, and this is what I mean a good upper bound. Okay, there is a if p is close to one, this is almost a constant, of course, not exactly, but this is what you play with. Turns out that the same thing is true also in S1 and SP, but the proof does not work because we don't know such a theorem, and I don't believe it's true actually. Okay, so this is a problem given K and what is the order of the smallest. What is the order of the smallest m such that every k-dimensional? Now I'm talking about subspaces. This is a linear field. The k-dimensional subspace of S1, 2 embeds into S1 of dimension. S1 of dimension M is just the M by M matrices with a trice cross. No. Okay? This, if we knew that, if we knew that with a good M, good depends on. Good depends on m and k. Even k square or k to the four would be okay for us. It would be, it would suffice, but we don't know that. So we have to follow some other route. So again, I'm emphasizing again, this is maybe the most interesting thing problem for me. No polynomial bound is known. I conjecture that there is no polynomial bound. This result of Regel. Polynomial burn, this result of Regev and Vidix that I studied above shows that at least for not two embedding, but one plus epsilon embedding, the one plus one over n embedding, it's a the relation is exponential. So, but nevertheless, we can prove the following. So, this is what this is the main thing that the main new thing in this thing also. There's a main new thing in this thing of this paper of Asaph, Gilles, and myself that still it's true that for each k and p between one and two, a k-dimensional subsidy of s1 embeds with this same distortion as in Lp into SP. Okay, so this is really the main thing, and I'll not finish my time, but let me just say that the main tool here is. The main tool here is this is maybe I'm talking only for one person here. I'm not sure even that of that. There is a non-commutative version of Lewis Lemmon. Whatever. There is some special, you take a k-dimensional subset of L1, of S1, that it admits a special basis, something like an or it looks like an orthonormal basis. First of all, forget about look. all forget about look at just a ti star tj trace of ti star tj multiplied by some fixed matrix is delta i j it's like an orphanormal basis of a special kind m is m is the sum of all of these things and you have to take also the other the other direction of multiplication it's something like a special orthogonal basis Orthogonal basis. This is what works in LP. This was well known since the 60s, I think, that such a basis exists. I will not say exactly what it is. And the main point was to find something similar in subspaces of S1. Okay, I'm sorry I took two minutes over my time, so I'll stop here. Thank you. Any questions? Any questions? I have a question. May I? Yeah, yeah, of course. Who is talking? Valode Chemilikov was talking. Ah, Valodo. Okay. Yeah, so Gideon, yeah, this last remark about this non-commutative levis lemma. Is it already published or where can we find it? It is published. It is published. I don't remember exactly. Is published. I don't remember exactly. I can send it to you. I don't remember the name. But look for it. It's a paper of Naur, Pizier and myself. It's the only paper of the three of us. Quite recent. Okay, yeah. Okay. Thank you. Any other question? Well, if not, we thank you again. We thank you again. And then we break until