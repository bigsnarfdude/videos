I'm Johns Hopkins and a lot of this work has been figuring out how to analyze some accelerometer data from open source methods and I've been working in this area for probably about two years full on. I came to the one five years ago. That's when I kind of dipped my toes in water in this domain. So I've tried to process a lot of stuff. We've processed the end names data, which Lily will be talking about tomorrow. So Vadim and I were chatting when we were talking about the activity counts from the About the activity counts from the NHANES 2011 and 2014 data. Lily, we'll be talking about that. We have processed that data, that data is publicly available. We've also estimated step counts from that data, also publicly available on our PhysioNet repository. So, this is some of the methods that are employed in kind of processing that data at scale. So, what is, I like to define something by what it is not. This is not a discussion of GGIR. GGIR is a very popular package. Is a very popular package on processing solarometer data. It can process a whole lot of different types. So, gene active, actigraph, activity, things like that. It can do that. It really is built for, I think, the end user. So, you put in like, you point it at a folder, you say go, it processes everything, and it gives you hundreds of different metrics, like sedentary time between every hour of the day, sedentary time overall, give you sleep, it can give you other things like. You have sleep, it can give you other things like that. I don't necessarily love working on that because it is you have to specify like 100 parameters. I don't know what's going on step by step. I also, as a statistician and a programmer, I like seeing what happens step by step and seeing what inputs and outputs are going along the way. And we'll talk about that with respect to visualization. We're not talking about MVPA or thresholds. We're not looking at moderate to vigorous activity, sedentary time, anything like that. We will be talking about. Like that. We will be talking about those in the context of thresholds, of activity counts in a lot of respects, but we're not going to go really deep into that. We're not going to be talking about sleep. And I'm going to show you how to process one person. Not talking about a population. So we're not going to show how to stack this data and do like lasagna plots on the population. Just one person, one time course for about seven days. So what are we going to do? We're going to try to understand the basics of processing accelerometer data. So this was a risk-worn accelerometer. It is an active Accelerometer. It is an ActiGraph device, and there are a lot of different things for different devices. But you can read almost every one of them into R. So, right now we're going to take, so it's weird, so they have GT9X devices, they have GT3X plus devices, they have WGT3X devices and things like that. Those are device names. The output format it gives you is GT3X. So when I say a GT3X, I'm talking usually about a file. So we're going to be able to convert that. File. So, we're going to be able to convert that, break that out of its proprietary format, or it's an open source format, but it's very specific. I can talk to you about the ins and outs, but it's a kind of weird format to me. You don't need to know how to use it. You just need to know how to read it in. Then you can convert it to CSVs or anything like that that we know how to work on, we know how to operate, and then we can convert that to activity counts and steps. All these are relatively open source methods and R. Many of these have a Python analog. So, a good amount of the work that So, a good amount of the work that we have done in the past year and a half is find stuff that was implemented in Python and wrap it up and put it into R because our students, our faculty, and a lot of our field works in R. So, some of these things are a little bit strange in the sense that if you were to use it in Python, you wouldn't use any of this stuff I'm talking about. You'd go to the direct source of Python that I wrapped everything into. But because we are trying to make stuff more user-friendly, bless you, some of the syntactic sugars, some of the things we layer on top is. Tactic sugar, some of the things we layer on top is to try to make things a little bit easier, make the defaults more what you want. So, stop me at any time if you have any questions or anything like that. So, GGIR, we're going to talk about that a little bit, but not. Are there slides on Slack or somewhere we can open up? I think they should be on the Talks 2025 channel. Thank you. So, this is up on our GitHub. I will again send it afterwards, but if you want to follow along today, there is any. There it is. Oh, it's it. Okay, great. Then you can follow along VR. Yeah, if you if you want to, I mean, you know, tutorial, I think, is a little bit, I don't expect people necessarily walking step by step, but everything here should have pretty much the code. There is a, the whole, all of the code here is turned into an R script on the repository as well. And pretty much everything generating this is on GitHub. So you can see where all the sausage is made behind the scenes to create this. You can clone the GitHub, I just follow. Yep, you can clone the GitHub, you can click through the files, do whatever you want. Or if you go on GitHub, you can usually click period, and that will open up a VS Code editor on the web so you can look at the background. So, what is accelerometer data? For the most part of what I'm talking about, almost surely, if you're using stuff nowadays, not the 2003 to 2006 data from Methane's, it's tri-axial. You have X, you have Y, you have Z. You have X, you have Y, you have Z. You're moving in different directions, usually measured in some sort of standard unit, such as G's or milli-Gs. And, you know, G-forces being gravity, meters per second squared, we're talking about forces here. And it's sampled at regular-ish time intervals. So what I mean by that is, even though you set your device to, say, 30 hertz, I want 30 measurements per second. If you get the time stamps out of there, there might be some wiggle room there for a number of things because the device might not sample uniformly. Device might not sample uniformly, and also sometimes timestamps have floating point stuff. So, if you don't see exactly things like this, where it's 0.125, 0.25, 0.375, that might be because some small random error. But if you initialize a device, it will do that. The reason I bring that up is time is just hard to work with. Time and time zones and milliseconds are even harder to work with in some regards, and some software likes to try to estimate. Software likes to try to estimate the sample rate from the data, and if it doesn't do that correctly, things go wrong. Some of them require you to specify the sample rate and things like that. But we'll show you how to extract all that information out of a cheese transfer. So how does it work? So inside that little device running around, there is a micro electromechanical system. So it essentially sees forces and changes in gravity and acceleration and converts that to a digital signal, gets you that back out. So it detects motion or gravitation. So, it detects motion or gravitational forces, but they have to be in acceleration. And it outputs time series data in raw accelerometer states. We are talking raw. We are not talking medium rare. We are talking as close to we can. There isn't really anything else going on behind the scenes. Everything else is derived from these things. All the things they report to you come from this original starting point. So, you know, if you're going to design a study, the question. You know, if you're going to design a study, the question is where to necessarily put the device, and there's a lot of different things to answer about that. It's what do you want to measure? What do you want to look at? The majority of the stuff we've been working on is wrist-worn accelerometers, and that's because compliance is generally higher. It does capture arm movements, so we are trying to do things it's kind of not exactly built for, like estimating the number of steps someone takes in a day. You don't necessarily have to move your arm to take a step. So, you know. So, you know, hip or the ActivePow, which is another device I have here that actually sticks to your leg. If you really want gold standard things, that might be some of the devices you choose. But it might be at the cost of compliance. People forget it. It's on your wrist. It's usually okay. Also, other things like sleep. I would say they're not necessarily the best things in estimating sleep, but a lot of people don't wear a belt to sleep and put something on there, right? But these. But these, I think I still have these. These literally stick to your leg. Like, they're stuck to your leg onto the skin, and it's good for orientation. So, we have done some stuff that we can talk about. I think Lily will maybe chat on that, about comparing some gold standards with the leg monitor and the wristwaring cell monitors. So, I think cloud, do you think where it do have to have certain legal? Yes, it's got a Yes, it's got a picture of a little person, a little stick figure person. I'm asking because someone's actually see data seem to be that even during the merging your wearing it. So sometimes, you know, you take a shower, sometimes you go, it might fall off, you can put it back on there. They do have some algorithms that try to adjust for that, but mostly it's supposed to be in the direction of this little figure. So, common data formats, we're talking. So, common data formats, we're talking about high-resolution data, so multiple measurements per second. Activity counts, these are usually aggregated over some sort of time window. One second, usually a minute. You say how many activity counts do you have in a minute. Wear time detection, were they wearing it? Were they not? I still think the methods for wear time detection are pretty rudimentary. Like, it's just saying how many zeros do you have, looking at essentially a run-length encoding, bouts, and then doing some thresholding on that. But you'll see from some of the data we're That, but you'll see from some of the data we're talking about that might do well for some of these things, but you take it off for 10 minutes, it's not going to pick that up in almost any method I'm talking about. And then, steps. So, Yarak, I think, gave a good comment at the pre-ENR thing. We had a long discussion on like what is a step. So, this is the Ministry of Silly Walks by Monty Python. And, like, which one of those are steps? Are all those steps? If I turn while I'm in the air, or is that a step? This is really important because. This is really important because some of our older adults shuffle, they don't necessarily pick up their feet, they might not be harmonic. Disease populations that have Parkinson's or other kinesthesia problems, they don't necessarily walk in a harmonic way, right? But even basic for every day. So when I was like pushing the stroller or like holding my kids' hands, like I noticed the step count wasn't correct. So a lot of times that still is the case, and the methods in a lot of regards, they kind of Methods in a lot of regards, they kind of do some sort of filter and they say, like, that looks like walking, and then they do peak finding. That's what a lot of it is. So, we'll talk about one deep learning method that we've employed that another group out of Oxford created that seems to do reasonably well, but a lot of it's peak finding, unless you're going to like the Fourier space and doing some sort of cadence, trying to estimate directly from the harmonics of the signal in order to get some sort of steps from it. But still. That has other problems. Yeah, I was going to say, there's other things there that are problematic. So, really, just whenever I say a step, nowadays I say a step, and a step is defined for what I'm doing nowadays as to whatever the algorithm that's estimated steps outputted for this thing. And that's important because if you go to another study, another algorithm, there is not necessarily a large amount of overlap. There's not even necessarily the same orders of magnitude. Even necessarily the same orders of magnitude. This is really important for public health messaging. 10,000 steps, I actually don't think is a bad number for some of the things we've seen, but like some estimates say like the average adult in the U.S. takes 7,000 steps. The other ones, it's like five. Other ones, they've said upwards of like 8,000 to 9,000. Do not believe 8,000 to 9,000 for average adults at all. I think it's way too high. All right, so some example studies. We'll talk about. So, some example studies, we'll talk about NHANES. There's large-scale population studies using your sworn devices. The UK Biobank has over 100,000 people they enrolled. I think it's still a little bit, it's harder now, I think, to get back to the raw data, but they did some processing on that. And again, they use this 100,000 people in a self-supervised learning model to try to develop this step counter. And then they use some external validation data to refine it. But there is some really interesting details. But there is some really interesting deep learning methods you can employ on something like 100,000 people and try to get some sort of estimation. All of us use Fitbit, so that's a little bit different. It's not necessarily starting with the call signal. So really quickly, those who are the R people in the group who've never used Python in any respects, you're used to install that package as it goes into your packages, it's done, right? There's not many conflicts. That is very much due to CRAN. Is very much due to CRAN, the comprehensive R archive network and their army of people making sure that everything doesn't break. Also, every time you submit a package to CRAN, it checks it against this versus everything else. Conda is a, or sorry, Python is a little bit more of the wild west in some regards, but the way they solve that is by environments. So they say, I'm going to work on a project over here, got an environment for the code over here, maybe within the same project, I have a different environment for certain things. That being said, when I wrapped stuff up for Python to use in R. Python to use an R, they don't always play nice together, right? It's this thing I wrapped needs like numpy less than 2.11, and then the other one needs numpy greater than 2.30, so they can actually be not resolvable conflicts. So if you've never been, you never used reticulate or anything like that in R, there has to be a little bit of a shift in thinking about how you're using the software behind it. All right, brief package overview. Lot of words on this slide. So, really briefly. So, really briefly, read.gc3x. It reads GC3x files, right? Pretty explanatory by the name. These are three different activity counts packages. We're going to be using AG counts in 2022. We're going to talk about that tomorrow. They released the method, the algorithm to estimate activity counts, active graph activity counts from raw data. And these are three different implementations of that. GGIR, I said it has a fire hose of outputs, which is a Said it has a fire hose of outputs because it'll give you everything and anything it can generate, and it's your job to deal with it. But again, that is a really useful package and it's well known. GGIR read, they've broken out into just reading functions for different types of accelerometry. We're going to use this package called actigraph.sleepr. That's trying to do things about sleep, but it has implementations for two of the most common non-where time methods. And the one we're going to use is the Troy method. Step count. Step count, count steps. There's a Python implementation. This is the deep learning model out of Oxford. I wrapped that code into an R package. Walking is a bunch of different other walking and step estimation methods that we found, and we implemented those in some different ways in R. Adept, yes. I'm just going to make a comment. You might want to say this is human step or low-line step, as we go. Luckily, we're not converting. Luckily, we're not converting any humans to meat. So, these are all human-derived meat. I don't know. I don't know what a bovine wrist is doing. But I've only used it for homo sapiens. So, that's a good point because some of these methods assuredly will not work for babies, for pediatric populations. And if they're not, I don't have a crawling metric or anything like that, but there are a decent amount of studies that are putting acceleration. A decent amount of studies that are putting accelerometers and wearables on children or babies to see how they're moving, what they're doing, to try to identify potential deficits early on. And this stuff probably isn't going to work for me. So ADEPT, Marta Karas, made this with some of the other people in the room. It does walking segmentation. It's very good at finding walking. We can talk about if it's good at step estimation. Not necessarily, but it wasn't really built for that. So, MIMS unit, this is the package that was released. Package that they release that was released around the time the NHANES data was released because this was the method that they used to estimate MIMS. Real spoiler: MIMS and activity counts have a correlation like 0.98. So it was really an, they weren't trying to reverse engineer, but they created a metric that was very correlated with activity counseling. That being said, the scales and orders of magnitude are way different. How Chang showed us that earlier today. There's accelerometry and There's accelerometry and summarized ectigraphy. These are some good functions for processing accelerometry data. Summarized ectigraphy is a package I created. It's more of a dumping ground for a lot of stuff I was doing over the years. And the naming convention was an attempt to try to get our data similar to the summarized experiment from the genomics world. Because once they standardized their data format, that's where a lot of the methods kind of came through. But that hasn't come to fruition yet. John? Sorry, I had to. Sorry, I had a lot of money. So, for memes unit, can you like how fine you can do it? Can you get it at unit level using this package or at second level? You can absolutely get it at second level. And then, the nice thing about most of the methods that they created are you roll them up in a sum. So, you can create them at a finer grid resolution and then roll them up to a minute if you want. So, if you're like, I want one second, but now I want to analyze in a minute, you can just group by minute and sum them up. Minute and so long. I have a suggestion if you could please add one more package to this list before you disseminate to the rest of the group. So, Marta, myself, and Jen, Schrack, we made this package called ArcTools. It's super simple, but we put a lot of effort into making it robust in large-scale processing. It also has Choi Troyano method, right? And I'd rather use that. I'd rather use that than something else from the internet. I think that's valid because I think there's a little bit of some implementation things on ARC. I'll take that to heart. Alright, so really quickly. I have another question. Say I have data. I just do the DGR back data of the 100,000 subjects. If I just want to try and randomly on your page, Python packages with our packages, I'll see the titles before I have to. I apply those procedures or I have to treat to prepare my data for using those types of things. Good question. Those come in CWA files were Activity 3 files. That is a UK-based company. They did that, right? UK, spending UK money in the UK. And so they give you raw accelerometry data, but you have to request that. That is very, very large. It is probably five to ten terabytes worth of data, even compressed. So, really, there's some things where they want to do gravity calibration. There's some things whether you want to do gravity calibration or not, but right now, with the methods out there, you can read in an activity file using GGIR read, and then you should be able to try to run activity counts to get that from. Yeah. Yeah, I just want to suggest this is slide 10 out of 47. Yeah. And that's what we've been talking a lot about. So maybe like we can give John like 10 minutes to get through some slides and then. Not all the slides are that dense, but I take your point. All right. I take your point. All right. All you need to know: we have a GT3x file. This is from a fig share online. This was a study of people with prostheses, and they were given active graphs on either arm. We were looking at this from a healthy control that had no amputation, no prosthesis, and they had it on their non-dominant race. This is just the code to download. It's in the repo. Don't need to go over that anymore. All right. So, really briefly, how do you read in a GT3x file? Read.gt3x has a function called read.gt3x. Has a function called read.gt3x, get read in gt3x files. And so there's two arguments that are really important. Most of the time, you want to turn that into a data frame. The reason you want to do that is without this, it says like you've, you know what the timestamp is. It does not put a timestamp comp unless you say that equals true. Input zeros equals true. I will talk about that in a second called idle sleep mode. So we will show that in the data as to what that means. But these two are the arguments that you want to enable to read stuff in here. To read stuff in here, and I'll show you why. So, one thing: if you look back here, everything looks like they're exact same time. They are not. They are actually different times to the millisecond level. So, in R, there is an option called digits.sex, and that says you want to say, I want three digits for the seconds of my data. So, the normal default output is an activity DF. Don't have to worry about the class per se. A lot of the stuff we're doing, you're going to turn it into a regular role. We're doing, you're going to turn it into a regular old data frame or tibble, but you have to change this option if you want to see the milliseconds. And that is important because if you have every, if all the data actually has 0, 0, 0, 0, 0 here, and you start doing group buys or something weird, it's not going to do what you want. All right, time zones are hell. They're the worst. They are the worst. They are not great. They are really, really difficult if you have a population that is across a lot of different areas. So you have to keep a lot of. So, you have to keep a lot of track of that. Just really briefly, the data comes out in GMT time zone, but a lovely note here: it says, hey, we put GMT on there, but not really. It's not really, it's the local time. So the time zone already attached to the data by default, because this is what ActiGraph does, and this is what this package is trying to do. It turns into GMT, even though the data is truly at a local level. So you have to do a little bit of time zone and work up. This is a nightmare if you are. A nightmare if you are combining it with the ActivePAL or GPS data or things like that. So you have to be very, very careful on time zones. You can assign different time zones and things like that. But be aware of daylight savings time. Daylight savings times and like UTC doesn't really exist. And sometimes in this, if you force it to like certain time zones that do have daylight savings time, like 2 a.m. and 3 a.m. just like turns into NA because that time doesn't technically exist under a daylight savings time model. Savings nine model. All right, a whole bunch of other stuff that we're not going to necessarily need except for two of them: the sample rate and the acceleration max. So, different devices have different maximum accelerations. This parameter is essential for using the MIMS unit package. They do some extrapolation. You need to know if this maximum, the maximum for that thing is 6Gs or 8Gs. That is the most common ones. For different devices, they might have a max of 2 to 3 Gs. That pretty much. G's, that's pretty much. I think that's very old, that's very limiting. And if you're doing stuff with more than 8 Gs as a human, it's going to be bad. Like, your body doesn't like that. I mean, your arm can move that fast, but a lot of times, even those acceleration maxes, that's like falling or like hitting something. So, just be aware of that. But the sample rate is used very commonly in almost all of the functionality we're talking about here. And so, it attaches a bunch of attributes to the data. A bunch of attributes to the data, and we can look at the header attribute that says, like, this is the device type, signal number, yada, yada, yada. So you got a bunch of information in there that you can extract. So you can do checks, it puts IDs in there, you want to make sure that this is the risk, not the waste. There's information in that kind of thing. All right. And it's important because when you convert from what you get out from GT3X to like a tibble, those attributes might go away. And depending on if you're doing derivatives or summarizations or group buys, those attributes aren't. Those attributes aren't saved in every operation, so you want to just save those things out. Like we did here, you would save this to some X or something like that to use later. All right, zeros, input, impute zeros. So these are not real. You cannot be, you're not, no one is like on a parabolic flight. There's no gravity. It doesn't exist, right? So these having zero in every single direction doesn't exist. We're on Earth, there's gravity. There's gravity, and so it's impossible for this to actually happen. These are essentially missing data points due to this thing called idle sleep mode. Idle sleep mode is a battery-saving device that ActiveGraph employs. So it says, hey, you put this thing on the desk, it's not going to move. So I don't see things moving for a certain amount of time. And it says, I'm not using the battery. I'm just going to essentially turn off. So it turns off, and then you move it again, and it triggers an event to start up again. But the way it's actually stored in the data is. But the way it's actually stored in the data is a missing data point. And the way Actigraph, when you process it through their software, they don't necessarily generate zeros. They use the last observation carry forward. So, for example, here you're moving, here you stop moving. And then they just repeat this over and over and over and over and over and over again. Now, super weird. So, one, take-home messages, don't turn that on. Don't turn that on on your device. All the people wearing the device right now, that is not turned on. The device right now that is not turned on. You will get maybe a little bit of battery life trade-off with that, but this is just weird. It's not good. But you have to make sure that you account for this with respect to what you're processing. And once we've done that, we've done this last observation carry forward, might be the right method, might not be. Really, from a statistician's perspective, those should be NA. But filtering methods, signal processing methods, a lot of the methods out there, you start putting NAs in there, everything starts to break. Start putting NAs in there, everything starts to break. That's not always the case, but a lot of the ones that don't default, they will. So we repeat the last observation carry forward. It has some nice properties. That variance of the signal is still going to be zero, right? And if you actually keep the zeros, it induces a variability from that next point because you went from here and you dropped down to zero. So you do have to account for this in some regards. So we could now just run and make some metrics and stuff like that, but you do need to visualize this data. But you do need to visualize this data or if you do it every single study. So I think this is a, I love this quote from Carl Groman: if you plot less with more data, something is going in the wrong direction. So he was talking about doing large-scale processing of, it wasn't single security, it was microarray data and things like that. And it was like, when we had 10 people, we did all these plots and investigated the data. Then we got like 10,000 people. And everyone's like, let's just run the models. And that's a bad thing, right? If you're getting more data, And that's a bad thing, right? If you get more data and you're like, let's plot less because it's complex. Bad news usually happens as a result of that. So we want to be able to do some plots of this, but plotting at the raw level is very hard for 10,000 people. But we can do some plots, right? So we're going to reshape the data very quickly. None of this is anything different than standard dplyR reshaping stuff. So I'm just reshaping the data so each axis is now one record. So it's a lot of data. What is it? 54 million. What is it? 54 million rows of the data. So here we can just plot like the first five minutes and see what's going on. So it's loading up some packages. I'm filtering between this time and five minutes in the future, just using standard ggplot2 stuff here. But I'm highlighting this area right here because we see after we've inputted it there, inputted, sorry, after we've inferred the last observation carry forward, we see this just flat line. This thing wasn't moving. Now let's look. Wasn't moving. Now let's look at the same thing here. So, the same box here. If we had not done that last observation carry forward, that like drops to zero, this drops to zero. So, again, if you start processing on this, some stuff might work, some stuff might not work. So, you can see kind of what's going on. They're moving around, there's a lot of activity here, there's nothing going on. Let's go back here. There's nothing going on here. You see flat lines here, some more activity, some more activity, and you see some small fluctuations come up. Small fluctuations to that. Good question. So it seems that neither of these is good. If you pass carry forward, it's bad, of course. But if it's all kept at zero, it doesn't work. So again, they should be NA, but truly, you should probably also keep track of that and then put those things to be NA after you've done the processing. But the last observation, Gary, forward, is something specifically you should do if you're going to create activity accounts. Should. Should. Should. So, um. So, okay about that. So, here I like plotted all the data, those 54 million points. And it's hard to see. It also takes a long time to plot. So, this is helpful in some regards, but not really that helpful. One thing I can see very clearly, they took it off at night, right? You didn't wear it at night. No sleeping, no sleep getting, you're not getting any sleep metrics from this day, right? But a lot of times, at least at the raw level, you can average. Times, at least at the raw level, you can average over a second, right? Right now, it's 30 hertz data. You're looking at 30 measurements per second. It's really hard to plot over seven days' worth of data, 54 million observations. You can at least group it by second, look at it times over days, and you see not getting any sleep or not getting any sleep data. They probably had, they took it off here, they took it off somewhere around here, and you see actually where the activity goes on. Okay? So, all right, really briefly. So, all right, really briefly: gravity correction calibration. Take home is there is a gravity calibration method out there that GGIR employs very strongly. It estimates you're not moving the device a lot, and it's saying, hey, Earth has gravity with 1g, so we should pretty much see the axes, the x, the y, and the z, pretty much touching part of this unit sphere. So, it does some sort of projection on that and say, Does some sort of projection on that and say, hey, I found some stuff where there's not a lot of movement. How close are you to the unit sphere? And it does some calibration. It's a scale and shift model and more or less does like a linear regression in the background to give you just some correction factors here. Actigraph doesn't use it. GGR thinks it's important. I think it's important for certain metrics that you're creating. For NMO, including norm minus one truncated that GGR uses, important. If you're using steps or other things like that, probably not. Or other things like that, probably not that important. Also, Jiawe Bai had created a method called the activity index. It's not as relevant in there because it already does some scaling and shifting in the method itself versus this, but just saying out there, this exists. Most of the time, I've seen these numbers should be really close to one, these numbers should be really close to zero. You might be able to use that as a kind of first gut type, gut, sorry, first check. Say if they're not close to that, something's going a little bit. Not close to that, something's not a little bit data. John, I think it accounts, and memes already account for it as well. This is done when they put a feature in 0.2 to 5 Hz. All this bias for low frequency goes away. Yeah, so that's a good point there. So intrinsically, in those methods, they already do some sort of filtering. So it's how much filtering do you do or already d or want to do in your data and how much do you know about filtering to do it? And how much do you know about Flash Variants? John, my question for this gravity calibration because I played with active paw. Who was active paul paper? I was very confused with this gravity calibration, and I just wanted to calculate a Euclidean norm of XYZ and to not deal with a gravity calibration. How bad would it be if I decided not to deal with it, assuming I didn't far away from the model? I mean, I don't think it's that bad. It's not also like the default in some other processing pipeline. In some other processing pipelines, but I think Vincent has been very clear that they think this is the right thing to do if you're using Eucliniate Plum. But I don't think it's good. Again, it's changing stuff, but not that much. But you will see different results, at least slightly. We're not using calibrated data for any of this stuff. It exists. I was just telling you about it. All right, now, got the data. I want to create some counts. So you take the data set, you pass it to calculate counts. We want the Calculate counts. We want the epoch to be 60 to 60 seconds minute level data. I have to specify the time zone, and then it gives you a column called vector magnitude, which I don't want to work with. I just want to call it AC. That's what we got right here. So for every single second, we get an activity count. Or sorry, not second, minute. My apologies. So for this date, for this time, we have 271 activity counts, 1543. That's it. That is the That's it. That is the basics: reading it in, getting to activity counts, similar to what you had in previous, to whatever ActiLife usually gives out. There are options like low frequency extensions and other stuff that ActiLife does, but this at its core is a unit that people analyze. But I have another question. Sorry. So, suppose I have this 15 minutes out. Suppose I did a Euclidean norm and I take an average Euclidean norm. Would those be comparable? Would those be highly correlated? The Euclidean norm of the activity count? No. Sorry, not about the activity account, but I did XYZ. I did the Euclidean norm on that. I got that on an original scale. And then I did the average of that over a minute. And then I have an activity count over a minute. My current naive hope is that those two would be perfectly quarter-white. Absolutely not. Absolutely not. But hold on, they are. Martha wrote a paper on that. They they translate like a dream between millions, counts and mmms. Mimes and this, but not MMO. M-MO is in general pretty bad, but it is highly correlated with counts. My understanding, the question was if you do this type of correlation plots at like say second level data, and then you add. Second level data, and then you aggregate, and then finally, like at the minute level, aggregate and once then what will happen? So I think Marta did it at minute level, right? Yeah, yeah, so definitely Marta established a good correlation between memes and activity counts, but Enmo was pretty fan out on the upper levels. It's correlated, but not perfect. Not above 0.8, I would say. We can have a separate discussion on them. In general, it's a bad idea to use it seriously. So, I should use a Kit Recount. Or means, or means, but not activity index by Java. Maybe a good idea is to share that way from the other. There are a lot of things like activity counts, memes, and ad also. So, which one should I use in my research when I get this detail? So, what are you doing? So, what are you doing? What are you doing? I mean, I'm just trying to get a survival model, suppose I'm modeling or was modeling something. I mean, all of them have different advantages and things like that. We've looked at, so one, really briefly, this is the MIMS versus AC, right? And at least for this one subject, correlation 0.90. Scales are completely different. We're talking 20 versus 5,000. But one of the reasons why I think a lot of people use Activity Council, one of the reasons why we process Use activity counts and one of the reasons why we processed different hands using that is because a lot of thresholds, sedentary time, MVPA, things like that weren't in that. Marta and we did a translation from MIMS to this using some Baltimore Longitudinal Study of Aging with some of that data and saw some threshold, I don't know, is it mapping between the two? But now that AccuGraph kind of released it, you can go back to activity counts. But there are different properties, right? So all these are positive, EndNote's pretty positive, but like the scale. End though is pretty positive, but like the scale is important here. So, some of the things definitely matter with respect to just standard statistics, the standard deviation of this, and things like that. The reason activity counts I think are worthwhile to at least start out with is they have a lot of history and evidence behind them. Not evidence, it's just a lot of people use it. Actigrev was probably the main research tool we used for many years, and I think Fitbit is starting to just take over them with respect to what's being used in the day-to-day for a lot of analysis. Used in the database for a lot of analysis. So the other reason, non-wear data, sorry, non-wear time, use activity counts for the most part. You can use other things, especially if they're mapped one-to-one to MIMS, but I like being able to calculate activity counts just for the fact you say, okay, activity counts are created. Now we can get a non-where estimation. Now I can create whatever metric I want with some sort of mask 0, 1. Is this being worn or not? But these aren't perfect. But these aren't perfect. And so it's just saying: so non-wear periods as consecutive zeros of a certain duration, it's usually about 90 minutes. Take it off for an hour, doesn't count. Take it off for 10 minutes, still says it's where, right? So I've done for like small-scale studies on the order of like 30 people, like I went through and manually segmented time. Like, you got zeros for 10 minutes, like, I'm calling that number. Zeros for 10 minutes, like I'm calling that number 20 minutes, things like that. So looking at a different kind of methodology there, but like, that's a lot. You got to look at plots, you got to, you know, when you start talking about 10,000 people on that, it's not feasible to be demarcating time without like essentially an army of people marking. So again, activity counts, this is what you're seeing for the most part. You can see very much like probably not wearing it, definitely not wearing it. Again, not wearing it in sleep. But this is a pretty common time portion. But this is a pretty common time course for one person. So you can estimate non-wear using ActiveGraph Sleeper, but this will probably be changed to use ARC tools because I trust YASIC with respect to these methods. And I did find some interesting things in this package that were a little bit buggy that I wasn't the biggest fan of. That's why you have to do some of these things in here to change the data type because I was getting some weird stuff here saying, if I didn't change the time. Thing, like, if I didn't change the timestamp to a double, it was like, you have missing times. I'm like, no, I don't. And so I had to do that. Hopefully, they fixed it in the next iteration. But calculate non-wear minutes, but it gives you in this weird, not weird, it gives you it back in this way that's not the way you want. It's like, these are non-wear times. It's like, well, I don't want that. So here's a quick function turning that into a data frame where you can merge it back in. So now you can have an indicator for every single. Right, so now you can have an indicator for every single minute, where or not, where, and then you can just do things like counting that up day by day and things like that with your standard. Johns, the the the new enhance, right, 2013-14, they have their own non-where flags. Do you have an opinion on that? Like, I don't have my own opinion, but they seem to be more fine-brained. Yeah. Right? Have you look at them? Do they look realistic? We even looked at the overlap between this now. We even looked at the overlap between this non-where and that non-where. I think that one will be a lot more. They spent like a number of years looking at it. So I do trust their stuff, and also they flagged it for a number of reasons. Like the ActiveGraph hit like AGs and stuff like that and weird things. So they did a lot of manual things. So I would probably trust their flags, excuse me, a little bit more. And also, it's just like a lot more eyes and an on. So I would probably trust that. But we haven't done a cross-reference outdated this and that. I ended up not using them because I just simply. Not using them because I just simply don't understand the method that was used to estimate. So I still use Choi, even though I agree with all of your criticism. It's just simple. We've looked at manually just some of the end games predictions. And yeah, there's obviously it's bad in some places. Alright, so we're going to wrap this up in probably like two, three minutes, so I don't cut into Arena's time. Minutes, so I don't cut into arena's time. So, here's a plot. Here's a plot today, and y'all not plotting anymore. We're gonna go to mentoring after this. Oh, okay. So, um, got it. So, four minutes. So, you see, so you see some of these things that pop up that, like, oh, you're like, oh, that's totally, that's where. That's where. Probably not. So, it's not perfect. So, it's still helpful, I think, at least at a subject level, even for 10,000. I mean, I work on neuroimaging. I've done studies with like 10,000. Imaging. I've done studies with like 10,000 like MR CTs and stuff like that. You can flip through 10,000 PNGs pretty quickly. Now, what you do with that, how you flag that easily, and then convert and say, like, oh, we got to take that out and that out. That you have to do programmatically, but it is a good gut check just to take a look at some of these plots to see where and how they are. So, MIMS unit package also calculates MIMS. So, really quickly, it interpolates the signal to 100 hertz. So, if you've got big data. Hertz, so if you got big data, it just got bigger. Extrapolate the signal. That reasons I've hit the maximum, so it says you're hitting like 8 G's on either side. It does a little bit of extrapolation there, does a fan test filter. It takes the absolute value of the area under the curves and the trapezoidal rule. And then it says anything less than zero is zero. Or sorry, low values, like within like, I think it's 0.01. It just says like that's zero. So it has some thresholding that they did. So this was really introduced during the Ann Haynes. This was really introduced during the NHANES. They also really, really love this header underscore timestamp column. So, you like every single package if you're working with open source, you have to conform to whatever the function expects. So, it's helpful to rename things like that and then run the MIMS unit. I will say, it takes a bit to run some of this, especially for 10,000 people. If you don't have the computing cluster, you're going to have to take a while. Do you have a thought? I'm still baffled by it. They filter the signal from 0.2 to 5 Hz, but they keep it at 100 Hz, right? You don't have any information about 5 Hertz, about 5 Hz, you need 10 Hz sampling frequency, right? Do they downsample? Like they wouldn't downsample it. I don't know. I mean, I think they were like, we made methods for 100. This is also pretty common in the field, I would say. We made a method that works on 100 hertz, so like we'll just upsample you. Up sample you, and just that's fine. And that's it's a little bit wild to do that. And then it takes a while. It does, it does. And also, like, if you're and Hange is already 80 hertz, I don't need to make it bigger. Like, I just have to request more memory for the computing cluster because, like, you just dumbed, made the data, like, bigger. I don't understand. So, they have some rationale, but I think it was really to try to get towards active traffic. Okay, click. So, again, they're very correlated, and the page. So again, they're very correlated. And the paper Marta put together saw on the order of like 97, 98% across a whole population of older adults. So again, you're going to get high correlation, but it's not necessarily one, but personally. How many people using if like do all the extra work to do MIMS, even though it's like highly? I think so. MIMS, you know, we saw today people are using MIMS. I think because they released that in a very like you download one file, you have all the 10 Download one file, you have all the 5,000 participants of Venhaynes in a format that's 1440 where each column is a menu. So, Lily will be talking tomorrow about how we release a similar thing for activity counts. Now, again, like if they were perfectly correlated or correlated just monotonically well, then if whatever threshold you use for the MIMS unit, if it maps one-to-one in a monotonic way, then you're counting sedentary time below that or MVPA above that, you should get. Above that, you should get the same result. It's just counting things above a thing, and as long as they translate almost together and the thresholds translate together, you'll get that. You might get something a little bit different. But there's just, yeah, you can do the mapping back and forth. It's just a little bit strange. And even though I was on that paper when we wrote the paper with the mapping, it's still like maybe just easier to justify just thresholding like that. So to be fair, memes has the advantage when you are using. The advantage when you are using low-quality accelerometer. John, go back my slide. The last line here truncates low signal values to zero. In the paper that Invest wrote with Marta, we couldn't see that because we use Actigraph. Actigraph has a pretty good quality of data, but if we use Accident UK Biopath, that has a low amplitude noise level, like non-biological high-frequency weakness. Biological high-frequency widget. That last line takes care of that. This is a normal thing in signal acquisition. We call it a dead zone, and that should be tractated kind of. That paper didn't address that because we use active graph, and active graph quality of data is pretty good. And I think that's a huge component because a lot of these things sum up. And although the individual seconds or individual samples, the error. Individual samples, the error might not be that big. Once you start aggregating, it becomes a lot harder. So, that truncation is important. It is a very important aspect of the algorithm. So, really briefly, Scroop made a deep learning model. It does step counts from raw data. We applied that. We also have to use a conda environment for there, but it's got one function mostly. It's called step count from the step count package. You have to put in the sample rate, uses an SSL model, a cell. Uses an SSL model, a self-supervised learning model that they used on the UK Biomank, applied to this other open data set called Oxwalk. And it does reasonably well compared to gold standards. So the IU data, Clemson, we use this other data set called Maria that have gold standards. They have either people walking and watching them, videos, other like click counting. So it's reasonable, and we have released the step counts as well from this method and other methods for end needs. Methods for end needs. And then again, like the nice thing about some of these things, it doesn't matter you calculate it at a second or a minute, you can aggregate up to a minute level just using standard like d platter stuff. And this is kind of what the step count looks like. It's pretty much saying we're not estimating any steps out here, zero here and stuff like that. But this signal, I will say, looks different, just considerably different than the activity counts. So it's not necessarily 100% accurate, but you will be. 100% accurate, but you will be getting different pieces of information, in my opinion, from step counts versus activity counts. Sorry, step counts, activity counts. Just a simple question. So instead of the step counts, can you have an output of the flags when the working happens? Yes, so that actually I didn't really talk about that, but that is, and it's at 10-second intervals that you can roll those up. But that's one of the pieces out, but most of the time, we just grab. One of the pieces out, but most of the time we just grab the thing that says steps. And it's not necessarily a one-to-one steps greater than zero equals walking. That's not always what that is. It says we found walking. We said that would look like walking, but we found no steps because of the way it does like peak finding. Not an expert in this, so maybe this question is a different. But how is it like integrating this data with GPS and maybe you know? And maybe, you know, like maybe thinking about, you know, studies where they start using both GPS and use those standards to understand how long are stacks, you know, to understand like combining basically these two different measurements. Like, are there like other packages up there that Packages out there that you know kind of help integrating these different sources? Like, you know, what's the state? I mean, most, no, I don't think there really is. So, I have a study we were working on that's very similar to that. It's trying to see the number of activity like at home versus abroad. Like within a buffer around your house versus like far away. So, we had to merge most of that data and just process that data like to a minute level and then merge it in. That's where time zone hell comes into play. And so, you want to make sure they merge up. But, you know, it's very, it's. Up, but you know, it is interesting. Like, GPS is important because it can give you speed and things like that, and you can help you filter out things. Like, I don't think, I think the argument that we heard probably five, every, we still hear it every year, is like, you drive a car, you have 40,000 steps. That doesn't really happen anymore in Summer Park. But, like, also, I can be walking up and down the aisle of a bus. Like, I'm walking at high speeds. So, what is that? Like, what should I, what the filtering should, which filtering should be done that? You're walking on a train. Which filtering should be done that? You're walking on a train, things like that. But mostly, there's not as much integration in that regard. And I still think it's very tailored toward the problem. Yeah. So, you're seeing different pieces of information, I think, between steps and activity counts. But, I mean, five years ago, this didn't exist. Five years ago, you couldn't do activity counts. There was something that tried to reverse engineer it, and it was kind of okay, but it was weird to use. Okay, but it was weird to news and practice. They've released some of that data, they've released the algorithm, now it's implemented, and we'll be talking about the NAINS tomorrow. So now you have like 10,000 people, U.S. representative population, that we have activity counsel and steps. And so I think that's really interesting. And we have already seen from some of the talks today how useful some of that data as a venom unit could be. And so maybe adapting some of the pipelines we've already worked on in previous work with Activity Council, you can just integrate that now with that meeting. That now, what that means. So you can do that. Wear time, still kind of not the best, I would say. It's not at the level, I think, what many people would expect it to be at, at least those methods, but they're well established, they're cited a lot, they're used a lot. We need better plots for thousands of people and this kind of stuff, either from a data quality checking approach and also some population-level stuff. So, whoever seen lasagna plots, that was Bruce. Seeing lasagna plots, that was Bruce Sweetheart who came through our department as well. It was really helpful to see activity profiles and things like that. A lot of other metrics exist. I didn't talk about that here, but there is AI. There is MAD. There is what else? Activity counts. There's different type of activity counts. There's other summary metrics, ENMO, ENMO truncated, things like that. And then whatever GGR is giving you for sleep. So a lot of times it's really great in this regard because. It's really great in this regard because each subject, I don't need to borrow information across anybody else to process an individual subject, similar to imaging and other modalities. You can embarrassingly parallel this, throw five, ten thousand jobs on the cluster, and it takes as much time as it takes one throughout the slots. There's some references. So that's like a ballpark, very quick overview of like going from a GT3X file to some downstream derivatives, and then hopefully for a lot of the analysis. Hopefully, for a lot of the analysis, you turn a big crank, all of them are processed, and now you have to do some quality control and then get to analysis. Then we'll call it a later. All right. Um should we take like a ten-minute break maybe? 10-minute break, maybe come back at 210, and then we'll get ourselves organized into some small groups. Alright, sounds good. You know what these small groups are going to be? We will talk about that. Yeah, I don't think both of those are essentially controlled. And they have their stuff on the file source. But only the way I see their back to the repo, I think like maybe they put the steps back to that. It's a little bit annoying, but so do you have the 100,000 CW? Have the 100,000 CWUs. So, you can also both be like 10 stocks. Exactly. So, they used the self-switch crash with the instance, and then they refined it and made scratch. So, which was like another I think they did they did the pressure. So, it wasn't for which one we had done. I think they've done, like, they've seen a supervisor and making a segment for different types of activity, and then they come to the actual definite walk. So it's a lot of fun. It doesn't need some stem counts, but it just won't be a good idea. So, yeah, I mean, that's that's one of the things we like to think. That's the only thing we're like so I the only data we have on is not so I use um sensor live so sensor live is like an app that you can just just click it on and then it'll access so like if you put the pedometer on and you time screen then we just margin it right on our phone and you put your phone back in the clock while you're on acting right and then we can see And then we can see what Apple says is our steps. It is different than an Apple Watch in some respects, but I think the reason we wanted the mapping from that to this is because we wanted to create a Shiny app where someone could say, I did this many steps via this device, and they're not that my relationship. And so we needed the bridge between these two, which is right now my data. But if we have that bridge, then we could say your age. Your like your age, sex, like demographic, you are in the 94th percentile compared to other men compared to the U.S. population. I will be doing that. All right, we're good. 