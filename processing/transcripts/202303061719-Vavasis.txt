Joint work with Levin Tanchanal, my colleague, and Jing A. Shu, who's now a student at Georgia Tech. So, and thanks very much to the organizers for what so far is a really great meeting. So, yeah. So, I'm considering two problems. I'm calling them P1 and P2 throughout the talk, and they look kind of similar, but it turns out there are subtle differences. So, P1, you're given a positive definite matrix, and you want to write it as a sum of a positive definite diagram. sum of a positive definite diagonal matrix, sorry, positive semi-definite diagonal matrix, and a positive semi-definite low-rank matrix. And problem P2, you're given a symmetric matrix, not necessarily positive semi-definite. And again, you want to write it as D plus R, but now D, we allow it to have any signs on the diagonal. We still want R to be a positive semi-definite in low rank. Okay, so so these are the two problems under consideration. They they look kinda similar, but it'll turn out they they have somewhat different problems. They have somewhat different problems. And the notation I'm using is: this SN is just symmetric matrices. Sn with little plus is positive semi-definite. And SN with two pluses, positive definite. That's pretty standard. Question? Existence? Right. So part of the problem is to decide, you know, so the algorithm will fail, presumably, if it can't find such decomposition. Sorry, yeah. So, yes, so then, of course, there's a So, yes, so then of course there's a natural decision problem that I'll mention later, which is does this thing even exist? So, yeah. All right, so where does this problem come from? So, it turns out that these problems have appeared in various forms for nearly a century in different bodies of literature. So, an example application of P1. So, suppose that you have an empirically measured covariance matrix of N stock prices. So, this is in the portfolio optimization. So, this is in the portfolio optimization literature. So, you want to seek an interpretation of that as a sum of our market factors that seem to influence all stock prices, sort of in unison, where R is much smaller than N. And then you also think that each stock price itself has some independent variability, independent from the other stocks. So, the first factors are R, and the independent variability is D. So, that's an application of P1. An application of P2 is An application of P2 is, for example, from a paper by Saunderson et al., they show that if you're trying to fit an ellipsoid in RNs through end-given data points, you get an instance of P2. And so as I said, the literature goes way back on this. The problem of decomposing a covariance matrix was in a paper by Albert from 1944. An algorithm for this problem for solving. Problem for solving both P1 and P2 is proposed by Saunderson, although they show that the algorithm actually occurred earlier in the literature. But they do a sort of a careful analysis of it and guaranteed recovery in 2011. There are other algorithms proposed in the literature for this problem. There's a block coordinate descent algorithm proposed. There's a manifold optimization proposed for it. Stochastic gradient descent has been looked at for problems of this kind. For problems of this kind. So there's a fairly big litter challenge on this problem. But it seems that until we came along, nobody actually asked, well, how hard is the problem? What is the complexity of it? So that's the gap that we tried to fill with our data. So what are our results? So first of all, we can show that P1 and P2 are solvable in polynomial time if R is fixed. The dependence on R though is really bad, so this doesn't work well unless R is quite small. R is the rank. Small. R is the rank, N is the size of the matrix. R is the rank of the factor big R, the lowering factor. For general R, P1 and P2 are both NP-hard. And we define a sense of approximate data and approximate solutions, and they're still NP-hard. And the third result is that P2 is complete for a class called existR. Some of you may already know what that is, but I'll define it later. Okay, so first. Okay, so first I'll just quickly go over what our algorithm is for the case of fixed R. And the algorithm is particularly simple in the case r equals 1. So it illustrates some of the points of the general algorithm. So I'll just go through for the r equals 1 case. It's simple enough that I can just cover it in a couple slides. And this is for P2. So that's the problem where A is not necessarily positive definite and neither is D. So we want to write it in the R equals 1 case. We want to write D, A as the sum of D plus U U you trans. A is the sum of D plus U U transpose for some U. We may as well assume that A is hollow, because if it has anything on the diagonal to begin with, you can just transfer that to D. So then the off-diagonal entries, they have to be of the form UIUJ. And the diagonal entries have to be of the form minus UI squared under the assumption of hollowness. We can start by assuming that in the first row of A, not everything, so the 1, 1 entry is 0. We just assume that. But we can assume that there's something. Just assume that. But we can assume that there's something that's non-zero in that first row. Otherwise, you could just reduce the problem to a smaller size. So that assumption implies, in particular, that u1 can't be zero, and therefore d11 must be negative. So then we get these formulas for the entries of the first row. Each of them is u1 times ui. The remaining ones, the rest of the matrix has to have the form ui uj for all the above diagonal entries. J for all the above diagonal entries. And so if you combine those together, you get this equation for d11 because the numerator has a ui u1 and then a uj u1 and the denominator has a minus u1 squared. So you get this equation, which is a linear equation for 1 over d11. And you get many equations like this. So you get many linear equations for a single scalar variable, x, 1 over d11. So you just pick one and you solve it, and then you can obtain d11, and then you can obtain. D11, and then you can obtain u1 from that, and then u1 is determined up to sine, so you can just take the positive sign, and then finally, you can obtain the rest of the entries of D again by this combination of A1i and D1l. So it's a pretty simple algorithm. There is a case, though, that it can fail if you're paying close attention. It can fail if all the linear equations for x are 0 times x equals 0. Then the algorithm didn't work. And that doesn't necessarily mean that. Didn't work. And that doesn't necessarily mean that there isn't a decomposition. It just means that that algorithm didn't work. But what do you do in that case? Well, that case can only happen if the entire first row of A, except for one entry, is all zeros. And that in turn implies that U has a very special structure so that the whole problem reduces to just n equals 2. Okay, so the r equals 1, n equals 2k. So that's a very trivial problem that you can handle easily. However, even though it's a trivial problem, it involves more than A trivial problem, it involves more than just linear equations, it involves inequalities too. And so that's what happens in the case r greater than one. So our paper proposed an algorithm for the general case of r greater than one. And for sort of a big enough n compared to r and generic data, all the entries of d can be found by just linear equations. So the whole thing is very simple in that case. But for non-generic data, and that includes a bunch of interesting and hard cases, you get n to the You get n to the power of r polynomial systems, and they each have order r squared variables, order r squared constraints. So solving polynomial equations is exponential in number of variables and number of constraints. But if r is fixed, that whole third bullet is polynomial time. Big polynomial, even for, say, r equals 3. So that's the end of the coverage of the algorithm. So any questions so far about that? Okay, so what about if we let our Okay, so what about if we let R now vary? So then we have results showing that it's NP-hard in that case, both P1 and P2. And the reductions all come from three-colorability. So I'm just going to assume that everybody already knows what NP-hard is, because I don't have time to explain it. So we say that a graph is three-colorable if you can assign the colors red, green, and blue to every dot, such that there's no edge that has the same colors at its two endpoints. And it's been known since. And it's been known since 1976 that deciding whether a graph is three colorable is MP complete. So, one way to understand this problem is to think about it as matrix completion. So P2 is a matrix completion problem. P1 is a little different, but so P2, well, what is matrix completion? So you start with a matrix whose entries are either numbers or asterisks. Asterisks, you know, entries that need to be. You know, entries that need to be completed. So, if you start with a partially specified matrix, then a completion of it is where you fill in all the asterisks with numbers. So, if we have a partially specified A, then let's let curly C of A be the set of all its completions. So then we can describe P2 as if you're given a symmetric matrix, partially specified, in which the unknown entries are all the diagonal entries and only the diagonal entries. Entries and only the diagonal entries, find a low-rank semi-definite element of its completions. So, this is a special case of matrix completion. And Peter showed in 96 that matrix completion of a certain form is NP complete. So, what did he prove? So, we need a second symbol now. Star means an unspecified entry. Star hat will be an unspecified entry, but that Unspecified entry, but that you must fill in with a non-zero number. So it's a restricted unspecified entry. So what he showed was that if you're given a graph, you can construct a partly specified symmetric matrix B. The diagonal entries are stars with a little hat. And then elsewhere there are other stars such that the graph is three colorable if and only if there exists a completion of B whose rank is three. And so RNP hardness proofs all rely on Peter's construction. So let me just summarize. So, let me just summarize what he did and what we did with the slide. So, we start with a graph, and you want to know if it's three-colorable. So, then what Peters did was he first extended the graph with a bunch of a polynomial, actually a linear number of additional edges and vertices, with the property that it preserves the three colorability. And then he constructs a partially specified matrix with the hat stars in the diagonal and the stars in zeros in the off-diagonal positions. So that's all what the Peters did, and then this is what we did. So we take this matrix and we encode it in a matrix U here that I'll explain in the next slide. We put unspecified entries in a diagonal matrix here that has all zeros in the off diagonal. And we put node arc adjacency matrices there, K and K transpose. And the consequence of all this is, of all these constructions, the original graph is three colorable. The original graph is three colorable, if and only if this extended graph is three colorable, if and only if this has a rank three completion, and if and only if this has a completion whose rank is the rank of U plus the rank of the sure complement of U plus the rank of all those stars. So how do we construct K and U? So as I said, K is a node-edge adjacency matrix. It has two ones per column. And U we construct from Peter's. U we construct from Peter's matrix in a fairly direct way. For every one of these star hats, we put this matrix. For every one of these zeros, we put a zero matrix. And for every one of these stars, we put a matrix of all twos. And the reason this works is that the entries in the upper left diagonal block can be chosen so that after elimination, the two entries are diminished. So, for instance, they could go to a 0 or to a 1. The rank of the completion is equal to Q. The rank of the completion is equal to Q. So, this is from the Schur complement theorem: that the rank of the completion is Q plus the rank of the Schur complement after the elimination of the diagonal block. And in order for the rank of that Schure complement to be three or less, you have to complete the little blocks of the ones with the three stars in the diagonal, to fill those in with ones. And then the 0-1 pattern of the rest of the matrix has to correspond to the three color classes. So that's the So that's the MV hardness of P2, and that construction transfers directly over to P2. The only difference is that in P2 we have to start by putting big diagonal entries on the diagonal A instead of zeros, and then we subtract diagonal entries from A to get the low-rank matrix R. And we use exactly the same sure complement R. So that construction is actually not that hard. So that construction is actually not that hard. I think I've given you enough description of it so that if you went home and you didn't have anything else to do, then you could probably fill in all the details. The place where it gets hard, though, is that we also wanted to show that approximate P1 and approximate P2 are going to be hard. And the reason is that these applications that I told you about, like the covariance matrix, you don't have an exact matrix A. You have some matrix that you measure, and so it almost surely has errors in it. So, how do we define a proxy? So, how do we define approximate n, P1? So, you're given the A again, and then you're given a promise that there exists a solution to the problem, a low-rank and a positive definite diagonal matrix, such that A minus D naught minus R0, it's not zero, but its norm is less than or equal to epsilon. But you're not given those D naught and R0, you're just given the rank and the A and the epsilon. And then with this promise now, you want to go. With this promise, now you want to go and find another DNNR that satisfies that equation, but you're allowed to, in the problem you solve, you're allowed to a bigger error. And there's a reason why this way of posing it seems to make sense. This seems to be a sensible way to say that, well, maybe the matrix A has some errors in it, plus your computations are going to have some round-off errors. So we're allowing things to sort of blow up. So this is our form of the approximate P1 and P2. And we show that these problems. And we show that these problems are also empty hard. The proof that approximate P2 is mt-hard is quite complicated. So that was the summary of our second kind of result. And our third kind of result is exist R complete problems. So what we show is P2 is exist R complete. So what is exist R? So the canonical problem, so now I'm not assuming that you've heard of this before, so I'm going to explain it. The canonical exist R problem. The canonical existar problem is: if you're given a sequence of multivariate polynomial equations or inequalities and they have integer coefficients, does the system have a real root? And so that's the canonical problem. So what about other problems? So a general decision problem in exist R is exist R complete. So it's in ExistR if it can be transformed to a question about polynomial equations, like the one that I had in the first bullet, like this problem. First bullet, right? Like this problem. So if I can write the problem as polynomial equations, then it's in exist R. And it is known that exist R contains all of MP, and it is also known that existR lies inside a P space. But not much more is known about this class. So Shitov proved in, I think, 2017 that matrix completion exists are complete. And what he showed specifically is that given a pound showed specifically is that given a polynomial system, you can construct from it a partially specified symmetric matrix that has a semi-definite rank 3 completion if and only if the system has a real root. So any polynomial system of equations can be transformed to this kind of matrix completion problem, which is kind of surprising. So our proof builds off of Shitov's reduction. So essentially we run into the same issues that we ran into. We run into the same issues that we ran into with Peter's construction, which is that in Cheetov's construction, he has the unspecified entries all over the place. We are allowed to have them only on the diagonal in our hardness proof. And also, all of our diagonal entries must be unspecified, whereas in Cheetah's construction, some were unspecified and some were specified. And we overcome those two basically using the same techniques. You use sure complements again in a way that you can. You can model all the off-diagonal unspecified entries as diagonal ones that you spread out when you take a short complement of a diagonal block. And we make multiple copies of certain rows and columns to make sure that diagonal almost, all diagonal ones are unspecified, but still some of them we force to take on only certain values so that we can copy Shita's construction where some of his diagonal entries are specified. Diagonal entries are specified. Okay, so this is my last slide, actually. So, am I doing okay for time? Okay, so here's something that we didn't solve. Okay, so it's an open problem. I didn't put it in the open problem session, but it's kind of specific, but we found it kind of interesting that, so we don't know what's happening here. So, which is that we show that P2 is NP-hard, but not P1. Okay, so just to remind you, P1 is the one where you start with a positive definite matrix. The positive definite matrix, and you have to decompose it as a positive definite D and a low rank R. Whereas P2 was the one where you start with a hollow matrix, and then your D, so actually D has to be non-negative that in that case. So now let's review the NP hardness proofs. We first proved that P2 was NP-hard, and then we extended, sorry, first, yeah, we first proved that P2 was not. Sorry, first, yeah, we first proved that P2 was empty hard. We extended it to a P1 empty hardness proof by taking, so we took it that had these asterisks instance, and then we transformed it to a P1 instance by just putting sort of a big number here. So that you could subtract a positive definite diagonal matrix and get the same thing that you would have gotten in the P2 case. But that doesn't work for the example. But that doesn't work for the exist R proof. It works for the NP hardness proof, but not this proof. And why doesn't it work? So, for general integer matrix, the norm of D in the P2 solution can be doubly exponentially larger than the norm of A. Now, how do we know that? Well, we know that because of the exist R completeness proof of P2 that we have. And so that means P2 can express any polynomial system. And in particular, it can express this slightly pathological polynomial system, where the system says x0 has to be 2, x. The system says x0 has to be 2, x1 has to be x0 squared, x2 has to be x1 squared, and so on. So this can be encoded as an order n-size p2 problem. But the solution, it's easy to just read off what the solution is to this equation. x1 has to be 4, x2 has to be 16, and so on. And the solution as xn is a double exponential number in the size of the problem. And so to do this construction to show that p1. Construction to show that P1 is exist R hard, we would have to put these very big diagonal entries here in this place, too big, so double exponentially large. They need exponential space to be written down. So this transformation is not polynomial time anymore. So that's why we're not able to show that P1 exists RP1. And we don't know it's true for one. All right, let's stop it. Questions? Questions. Do you have a sense about whether these problems are typically hard in practice? That's hard to say. Okay, so the papers that have, okay, so the paper by Saunderson et al. proves that under some assumptions, which are sort of hard to check in practice and may not apply to real data, you're guaranteed completion. And then they test their algorithm and it does pretty well. And the other authors also test their algorithms and they seem to get good results, but they don't know what the lowest rank. They don't know what the lowest-ranked solution is, you know, unless they specially constructed the problem, which is what the Sanderson people do. So it's not clear, I guess, is the answer. It is not clear that in practice these problems are easily solvable or not. So are you thinking that there should there should be a hardness for P1, or plausibly we could assume Palinum only found the entries and P1? We don't really know what to think about P1. It's true complexity. So it's at least that P-hard. So we proved that. But we aren't able to show that it lies in P1, but at the same time, we're not able to show that it's exists R complete. So I don't have a conjecture right now, but I think it really is. Oh, sorry, you know hardness. Okay, I think I missed that. You know hardness from your automaton. Yeah, so we know that it's NP heart. So it's it's definitely not polynomial time. But we don't know that it lies in MP. That it lies in empty. And yeah. I guess just somewhat of a comment. So there's a similar flavor, like non-negative matrix structurization, right? That also sort of, okay, since we know it sits in XSR. There is work showing that under some kind of non-degeneracy assumptions, you can, excuse you, a subclass of these problems, you can actually solve them very quickly. Problems you can actually solve very quickly. Sure. I'm wondering if you're aware of some kind of assumptions, not very specific assumptions that you could restrict and we are able to solve it much faster. Because even if XSR is basically practical, it's all about the MSA. Yeah, yeah. Well, okay, so again, I'll point at the work by Saunderson at Ottawa. So where they have a class and they prove that They have a class and they prove that guaranteed recovery with some ideas programming. I mean, our own results, our algorithm, and that first part of it, the linear equations part of that, we didn't discover that. So if you're in the sort of the generic case and the system is big enough, you can just get the whole thing with linear equations. So yeah, there there are definitely cases where we know it can be solved quickly. But I mean, are those cases the ones that occur in practice that I'm not so sure about? Practice that I'm not so sure about.