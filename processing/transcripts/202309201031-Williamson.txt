Here and for the work you're putting into organizing the conference. It's a beautiful location. And after so many years of not seeing people, it's good to see you. Today I'm going to be talking about a four-thirds approximation algorithm for half-integral cycle cuttings. This is a traveling salesman problem. This is joint work with my PhD student, Billy Jin. Billy did the slides. And with former PhD students at the University of Washington, Nathan Klein, who's now doing a postdoc and on his way to a job at Boston University in this. At Boston University in this computer science department. So here's the outline of the talk. I'm going to start out with some preliminaries about the traveling salesman problem. I'm going to tell you what is this class of half-rang rule cycle cut instances and why should you care about them. And then I'll take the remaining time to give you a sketch of the approximation of it. So the traveling salesman problem, I'm sure most of you are familiar with it. I'm going to model it as a complete graph where I have costs on the edges. The graph is undirected. We're considering symmetric cases of the traveling salesman. Considering symmetric cases of the traveling salesman problem, edge costs will obey the triangle inequality. Our goal is to find a minimum cost tour, a minimum cost Hamiltonian cycle. On the right-hand side here, you can see an optimal tour visiting 99 poki stops in San Francisco that was computed by Bill Cook. It's a very fundamental problem. It's one of Karp's original NP-hard problems. What's known about approximation algorithms for the traveling salesman problem? Well, for 45 years, I would begin every talk by saying we know if the best. Begin every talk by saying we know the best we know is a three-halves approximation algorithm due jointly to Christopites and independently by Sergikov. In the last few years, that changed to be 1.5 minus 10, something like 10 to the minus 36, which doesn't sound like much, but in the context of 45 years of lots of people in the audience banging their heads against the problem, it did seem like a pretty big achievement. So that's due to Anna Carlin Nathan Klein, my co-author, and Cheyenne Obais Karan. It's also known that. It's also known that it's going to be hard to approximate when the factor is about 1.008, so there's still the approximability of this problem, is still pretty wide open. A technique that's, or a tool that gets used a lot in approximation algorithms and exact computation for the traveling salesman problem is something known as a sub-throw LP. So, this was introduced in a paper by Danzig, Fulkerson, and Johnson, who looked at exact computation of the traveling salesman problem. It's also sometimes known as a hell-cart bound. It's also sometimes known as a Held-Kart bound because of a paper by Held and Kart that showed how to compute these solutions to this LP. This leads to one of Billy's favorite visual jokes. So we have an LP variable xe for every edge in our graph, and we have two different types of constraints. We have degree constraints that say that the sum of the LP variables of all the edges incident on a vertex should sum up to be 2. And then we have these sub-tour elimination constraints that say for any non-trivial... Constraints that say for any non-trivial set of vertices, the sum of the LP variables of the edges that are in the cut, one endpoint in edge, endpoint inside the cut, S, and one endpoint outside, that sum has to be greater than or equal to 2, and that's because the Tor can come from the outside, has to visit these vertices and go back out again. It's greater than or equal to 2 because the Tor might go back and forth a couple of times. And of course, all these LP variables are greater than or equal to zero. To introduce a little bit of terminology, because I'm going to use it later on. Going to use it later on, we'll say that for a given LP solution, a set is tight if that inequality is met with equality. So this LP relaxation gives us a lower bound on the cost of an optimal tour, and a frequent technique that gets used is to say, okay, we're going to come up with an algorithm that bounds against the cost of the LP, and that implies they're not bound against the optimal tour. And this works well as long as the ratio of the optimal tour to the The ratio of the optimal tor to the LP is always not too large. So that leads to this notion of an integrality gap. So the integrality gap is the worst-case ratio of the value of the optimal tor to the value of the LP relaxation overall. Instances of the problem we're interested in, ones where the cost of A of the triangle inequality. What's known about that? Well, back in 1980, Wolseley showed, leveraging this result by Christophes and Serjikov, that the integrality of the gap is a sub-thural. Integrality gap as a subterral P was no worse than 3 halves. And this result of Carlin et al. shows, in fact, that's no worse than 3 halves minus 10 to the minus 36. And it's long been known that the value of that, of the integrality gap, has to be at least 4 thirds. So it's also been conjectured in the community for quite some time that the integrality gap, in fact, is at this lower bound, that is exactly equal to 4 thirds. And what we end up doing in this paper is we prove that 4 thirds conjecture for a limited class of TSP. Class of TSP instances. So, in particular, the four-thirds conjecture holds for half-integral cycle-cut instances of the traveling salesman problem. By half-integral, I mean I'm thinking about LP solutions where the LP value on the edges is 0, either 0, 1 half, or 1. And cycle cut instances, I'll define a little more precisely in a minute, but it just means that tight cuts have a very specific structure in the graph. The interesting thing is that these half-integral cycle cut instances capture all the worst-case ratios for the integral. All the worst case ratios for the inning, all the worst case instances we know for that four-thirds conductor. All the ones where the ratio of the optimum towards the LP relaxation is, in fact, four-thirds. So let's take a look at this class of instances. Let me define them a little more precisely and motivate our interest in them. For half integral instances in general, there was a conjecture I made with Franz Fellikamp and Ankhen van Zeulen a few years ago, saying that really half integral instances were are the worst case for the integrality gap. Worst case for the integrality gap. The status of that conjecture is still open. I don't know whether it's true or not. But it is, I think, an interesting conjecture. Another reason half integral instances are interesting is that this improvement in the overall TSD ratio was built on top of work that the same authors did for half integral instances just a year or two before that. So there's some sense in which understanding what's happening with the half integral case can help you get to the more general case. Get to the more general case. And we do know better approximation algorithms for the half-integral case. The resulted Anapam and co-authors is that we can do slightly better than one-point calc in that case. But still better than that. So what are cycle cut instances? Well, once again, I'm going to say a set S is a tight cut if the sum of the LP variables of the edges in the cut is exactly equal to 2. And a cycle cut instance is one such that for every tight cut that is non-tight. For every tight cut that is non-trivial has more than one vertex in it, you can always partition that into two other tight cuts. So, given a set S that's a tight cut, there do exist sets A and B inside of it that are also tight cuts. So these half integral cycle cut instances, as I said before, capture all the known cases where the four-thirds conjecture is tight. Here's the canonical example called the envelope graph. Called the envelope graph. It's an instance of the traveling Tailsman problem where I have three, a graph where I have three paths of length k. The cost of traveling between any two vertices, and here is just the number of edges in the shortest path between those two vertices. If you compute the LP solution, the LP puts value one on each of these three long halves, and then a half at these ends down here. So the LP value solution is something like 3K. If you look at the 3K. If you look at the optimal tour, you're going to have to go down one of the paths, come back here, go back to the third path, and you're going to have to double back for a cost of something like 4K. So the ratio of the optimal to the LP is something like 4 thirds, and that approaches 4 thirds as K gets large. There's another set of instances that also have an integrality gap of 4 thirds. These are instances called K Donuts introduced a couple of years ago in a paper by Silvia Boyd and Andro Chevu. By Sylvia Boyd and Andro Chevu, and they show that those instances also have four-thirds integrality gap. These also turn out to be half integral cycle cut instances. So just to convince you or try to convince you that, say, the envelope graph is a half integral cycle cut instance, what do I need to show you? I need to show you for any given type cut, you can partition it into two other type cuts. And here's an example. So this set S is tight because I have four edges of LP value a half that are in the cut. Value of half that are in the cuts, I can partition it into these two cuts A and B, which are also tight cuts. Yeah, so I'm confused about that, but then you presumably also need that each of those. Exactly. But so how does that, I mean, because it doesn't have to, you are, you allow movies out to a set of size two, that then in that case it's okay, then you consider them to be set to size one. Yes. Okay. Yeah. So are there any known finite low-want instances that are known to have an exact report or into the finite crops? I'm trying to remember on the K-donuts whether those are ones that meet the four-thirds bound exactly. I don't remember off the top of my head. Why are these false cycles? Why, sir? Why are these false cycles? Why are default cycle type instructors? What is cyclotron? I'm going to get to another definition of cycle cuts in just a minute, and that might be a little bit better. But maybe another answer that will be more illuminating, if you're familiar with a cactus graph representation of minimum cuts, that's where all the minimum cuts, the cactus graph consists of a bunch of cycles. There aren't any of these forest-like parts to it. So, here's the other definition of cycle cut instances. So, I need to introduce a little bit of terminology. So, we still have our tight cuts. I'm going to say that two sets of vertices cross if their intersection, their set differences, and the complement of their union are all non-empty. I'm going to say a cut is critical if it's a tight cut that doesn't cross any other tight cut. So, let's pick an arbitrary root vertex. So let's pick an arbitrary root vertex and let's look at all the sets that don't contain the root that are these critical cuts. Now since a critical cut by definition doesn't cross any tight cut and doesn't cross any other, therefore it doesn't cross any other critical cuts, that leads to something called a laminar family of sets. So if you have two, a collection of sets, none of, for which no pair cross each other, the only alternatives are the two sets are disjoint or the one contains each other, that leads to The one contains each other, that leads to this nice structure that looks something like this, right? So the uppermost or the largest critical cut in the family will be set of all the vertices minus the root. The bottommost elements will be the singleton vertices, and then you'll have collections like this. So then what's a cycle cut? We'll say that a set in the hierarchy is a cycle cut if it has at least two vertices in it. And if you take all the vertices that are not in that set, so we're picking this set S. In that set, so we're picking this set S. Take all the verses not in that set, take all of its children, all the ones that critical cuts that are immediately contained inside that set, and you contract them. The resulting graph is a cycle. So if I take this one of these greens, take a set in there, contract everything that's not in there, contract its children, you get a cycle. All the data will go be doing consecutively. Sorry? You start from the support graph. You start from the support graph. That's all. I mean, I have a set of nodes, right? Oops. If I have a set of nodes, that all the edges will go between consecutive pairs of those. All the edges, you mean all the edges will be positive x value? Yes, that's correct. Yeah. And we can prove that these two definitions are equivalent. The one I showed you before is entirely equivalent to this definition inside. Did you say V minus R is always in the high round pre-deployed? Yeah, because any singleton vertex is going to have set LP but have a degree cut. So the sum of the LP variables on it is going to be equal to 2. Okay? Anybody willing to live with the definition? Okay. So let's just take the envelope graph and let's look at what the hierarchy is going to look like in this case. Is going to look like in this case, so let's pick a root vertex. And then we look at all the critical cuts, and they're going to be exactly these cuts here. So the outermost green cut, everything except the root, then the orange cut containing these two blue cuts, and then all the singleton vertices by themselves. So the hierarchy is going to look something like this, where we can draw this nice tree structure. The green set contains the orange set plus these singleton vertices here. The orange set contains the two blue sets. And the blue sets contain all the singletons associated with these lower two packs. Associated with these lower two paths. So there's my hierarchy. And you can see this is a cycle cut instance, so I can, again, what I would need to do is show you for every set in the hierarchy, but let's just take one. So let's take this ORN set, right? And if I contract everything not in the Orange set and I contract the two children that are in the Orange set, the result, you can see that Billy didn't do this, I did this, gives me a cycle on the It gives me a cycle on the on the on the edges that are in the support. Okay, so this sums up the section on these instances. We're interested in these half-integral cycle cut instances. So all LP values are 0, 1, 12, or 1. All cuts in the hierarchy are these cycle cuts. And again, all the hard instances that we know of are these half-integral cycle cut instances. So what do we do? So, what do we do? We come up with a randomized algorithm, so that's the expected value of the Tor that we produce is at most 4 thirds times the LP value in this case. Yeah. One question, just sort of framing things. Do you know that there are cycle cut instances which are not half integral? Yes, yeah. In fact, there ex you can find things that are extreme points that are not half integral that are also cyclical. Cycloponia. So, in my remaining time, I'm going to try to sketch the algorithm for you and try to explain why the analysis works. So, first item is a very standard thing in approximation algorithms. I'm not really going to think about finding a Hamiltonian cycle. I'm going to, because I have the triangle inequality, I'm going to try to find Euler and Tours. And as long as I can show that the cost of the Layer and Tours are no more. Show that the cost of the layer and tour is no more than four-thirds times the value of Lp, we can shortcut to us a tour at no greater cost. So, in fact, this is a randomized algorithm. What I'm going to do is construct a distribution of Eulerian tours such that for every edge, I use it at most 4 thirds times this LP value and expectation. And if I can do that, that's enough. That'll give me the result that I want. What we're going to do is we're going to work on this hierarchy top-down. So, I'm going to tell you first what I'm going to do at the uppermost level of the hierarchy. To do it, the uppermost level of the higher key. And then, for every set in the hierarchy, given the set of edges that I've chosen to enter that set, I'm going to choose some edges from the support that I'll enter that set, some number of copies of each of them. I'm going to tell you how do I connect up the children given the set of edges that I chose with a parent. So to simplify the presentation a little bit, I'm going to make some assumptions. Make some assumptions. The first assumption I didn't list here, which is I'm going to assume that all the LP edges that I deal with have LP value half. I'll not consider the edges that have value zero. Any edge that has cost one, I'll just split into two edges such that each has value half. I'm going to consider the case where every set in my hierarchy has exactly two children in it, right? And each of these children has two children in it. I'm also going to assume that the edges joining up the children. Assume that the edges joining up the children in the set are go straight across. The edges edge from here goes up there, and then from here goes down there. I don't have a situation where the edges cross like that. It just makes the presentation a little simple. What does it mean to cross? I'm not sure. Well, it means that the edge, you know, the edge from this child goes, from the upper part of this child goes down to the lower child in this one, or like that. The edges just go straight across like this. So this is just what drawing. Like this. This is just for drawing purposes. And I'm going to, the canonical way of drawing it is that all these black edges here are ones that have LP value a half. The edges that go up like this might have an endpoint in a set that's higher up in the hierarchy. The ones that go down like this will not. They will not have endpoints that go up, go up higher in the hierarchy. Okay, so what do we need to do? Now, since we're constructing an Eulerian tour, we know that the number of edges that we select in any cut has to be even. So, in particular, the number, if I look at these four edges that are entering this set, it's a tight cut, so it's going to have four half edges in it. If I want an O layer in Tour, I need to have the total number of copies of these four edges that I take has to be an even number. And in our, what we do, we're going to take either 0, Our, what we do, we're going to take either 0, 1, or 2 copies of every one of those four edges. And what I want to do is I want to focus on all the ones where I take exactly one copy, right, where the parodies of that edge is odd. So I'm going to think about the different possibilities I could have, and I'm going to group them by type. And I'm going to call each group a state for reasons that'll be clear in a few minutes. Clear in a few minutes. So I'm going to group into four different types of states. State one looks like this, where the edge from one of the children could go higher, the other one does not. Same here. One of the children goes higher, one of them does not. State two, either both edges, I have one edge from each child, it goes higher, or both of them don't go higher. Or both of them don't go higher. For state three, the edges that have, where I'm taking exactly one copy, both come from the same child. And in state four, either I take all four edges exactly once, or I don't take any of them exactly once. I take them either zero or two times. These blue edges are all the ones of parity one. If an edge doesn't exist, it doesn't mean I'm taking it zero. Doesn't exist, it doesn't mean I'm taking it zero times. I could take it either zero or equal. Okay. So I've grouped these cases where I'm taking each edge exactly once by these four different types. So when you say it's just gold set the hierarchy, so the top edges are going to a parent set and the other sets. They might go to a parent. They don't necessarily have to go to a parent. The lower ones are not definitely not going to go to a parent. Not definitely not going to go to a point. Yeah, they might go. I mean, if you think about the edges, think about the edges going across here, then they're going to a sibling. And the ones down are definitely like a siblings. Yes. Okay. So again, what I'm, the point of the algorithm is I say given Of the algorithm is: I say, given the parity and the state that I'm in, what set of edges I chose, I need to tell you how am I going to connect up to children. What edges am I going to use? So let's look at state one. And what I'm going to do is, if I'm in state one, with probability a half, I'll choose this edge to connect up to two children. With probability of half, I'll choose that one. This has a couple of different effects. First of all, it'll be Eulerian, right? Each of these child nodes will be connected. They'll each have even parity in terms of the number of edges. Secondly, I'm also using each one of those edges with probability half. So exactly its contribution to the L P because it has an L P value of a half. Value of a half. The other effect that it has is: look at, so if I do that things in that way, what state do each of the children end up in? So if I think about this child over here, if I choose this edge, this child is now in state three. It has one edge going higher up in the hierarchy, the other edge going. The other edge from the child is from that child node now goes to something that's not going up higher in the hierarchy. And they're both coming out of the same child node. If I choose this lower edge, then that node is in state one. It has one edge from one child going up higher in the hierarchy, one edge from the other child going lower. Going lower, or at least not going higher. Okay, so with probability half, this child ends up in state one. With probability half, it ends up in state three. Same thing for that note there. So now I can say why we were thinking about calling these states, because now I can set up a Markov chain. I haven't told you the rules for what I do for states twos, three, and four. Three and four in the interest of time. But what we can see is that, right, if we're in state one, then given our probabilistic rules for connecting of children with probability of half, we stay in state one, with probability of half, we go to state three. And then given all our other rules, which I haven't told you, we get similar probabilistic transitions between the states. The nice thing about Markov change is that you can That you can have fixed points, right? You can have stationary distributions. So it turns out the stationary distribution associated with this markup chain is at four-ninths of the time you're in state one, two-ninths of the time you're in state two, two-ninths of the time you're in state three, and one-ninths of the time you're in state four. What I showed you, I argued to you in state one, you use any given edge joining children with probability half. And given those other rules that I didn't show you. Given those other rules that I didn't show you, in state two you use an edge with probability half, and in states three and four, you use edges with probability one. All right, so if we're in a stationary distribution, then every edge, it turns out, if you just do the expectation computation, gets used two-thirds of the time, which given that its LP value is a half, is four-thirds times the LP value of the edge, which is exactly what we needed to be true. I only looked at this case where I had exactly two children per node, and we only looked at these edges going straight across. But even in the most general case, this turns out to be a fixed point of the overall distribution. So you still have four states, four layers. We have to think about multiple Markov chains, because we have to think about having even number of children versus having odd number of children. So there's a various number of things to consider, but this ends up being a fixed point for all of those different Markov chains. For all of those those different Markov channels. Okay, so here's a summary of the algorithm then. We're going to work top down in the hierarchy. At the top level, we're going to sample edges according to the fixed point distribution about which state we want the top level to be in. Then once we've sampled those edges, we take a look at the children in the distribution, see what state they're in, join up their children according to the probabilistic rules that we give. And given that, we have And given that we have this fixed point distribution, this stationary point, that means for every set in the hierarchy, the probability that a child or a particular node is in a particular state comes exactly from that fixed point distribution. And under the fixed point distribution, every edge gets used exactly 4 thirds times its LP value of the time and expectation. We get a resulting set of edges that's Eulerian. The total expected cost is 4 thirds times the LP value. Okay, and you can derandimize this L. Okay, and you can derandomize this algorithm if you want. So I see why you get the parity constraints, but I don't see why you have connectivity. Again, I didn't explain all the rules, but whenever I give a rule, I have to ensure that the children are connected inside. Every rule makes sure that the children are always connected. And that'll give me connectivity. Right, so here again, I'm only showing two children, but in general, I'm going to have to show you that all the children are connected. Show you that all the children are connected. Other questions? Because now I'm at the end. So some possible questions that Wog could ask. Can you get 4 thirds for cycle cut instances that are not half integral? That's an interesting question. One thing you might have hoped is that you could just reduce to the half integral. Is that you could just reduce to the half integral case, right? We know in general on the subtor LP you have extreme points that are not half integral, but maybe if you confine to yourself just the cycle cut instances, you got lucky, right? And all the extreme points would be half integral. That would let you reduce the general case to this half integral case. Unfortunately, it turns out not to be true. There are extreme points of the sub-terl P that are cycle cut instances that are not half integral. But I still think this is doable. I think that can. Doable. I think that can be done. This term cycle cut instances came from this work by Carlin Klein and Weiss Garon that they did on a half-integral case. And they had two cases in their hierarchy of cuts. They had either cuts that were cycle cuts or cuts that they called degree cuts. So if you want to improve on this 4/3 per half-integral case, then the next thing to think about is these degree cut instances, which seem easier in some ways and harder in other ways. I can say more about that if people want me to, but I won't. The next thing is that the algorithm that Carlin and all used is called the max entropy algorithm. And you could ask, is max entropy itself going to be the four-thirds approximation algorithm we've all been hoping for? And the answer turns out to be no. So we can show, even for these half-integral cycle cut instances, there is an example such as There is an example such that max entropy is not going to be a four-thirds approximation hour, which can easily be worse. Okay, thanks for your attention. If you want to look at the archive paper, there's a QR code that'll take you to it. Billy is on the market this year, so be looking for his application. And thanks for your attention. Questions? Questions? Yeah. How much worse is max entropy then? It's a little bit worse. Yeah, it's not a lot worse. But if you think four-thirds is the golden answer, which I do, then it's not the right answer. A refinement to my previous question. The the bad examples um include things that are shortest path lengths of undirected graphs, like the bad inputs? Like the bad inputs? So the question is: is it still in the realm of possibility that if we looked at the class of inputs that the metric is, I give you an undirected graph and I want the shortest path. Is it possible that the extreme points are half integral? That they essentially naturally fall into this class? I I don't know the answer. I I have to think the answer is no though. Yeah, I think you're gonna get more general cases. A complimentary question to this last point. How good is max entropy on these systems? Well, that's a great question. I don't know. Right, so I guess Carlin et al. paper showed that it was a little bit better than three halves, right? That's they were running max entropy on these. They were running max entropy on these half integral but both degree cuts and cycle cuts and they could get the lower title. Right, but you know, is it close to four thirds? Yeah, I guess you could ask, you could see what happens in their analysis if you take out the degree cut case and whether that's that would be a a natural thing to try. Are your examples the first ones that are worse than four-thirds for the maximum topic? Okay, the bell rang, so I should probably stop. Thanks, everybody.