Thanks, everybody. The organizers for organizing this. It's really nice to be able to just kind of hooke end everything here and take advantage of having already seen so many great talks. Today, I kind of want to look at machine learning as a tool to help bolster existing methods, not necessarily to go out and place everything out, right? So we've already heard a lot about machine learning's role in gravitational wave physics. I think the Monday Roundtable actually provided a great sample of how a lot of people saw its future. We saw it as something that could accelerate tasks. We saw it as something that could accelerate tasks, help us to make decisions, help us to find out liars. And I think there was generally broad agreement that those are great uses for machine learning. And we've also heard of a lot of big applications of machine learning. We've seen detection pipelines and parameter estimation and data cleaning, and all of these are great. I would classify myself somewhere between machine learning enthusiast and skeptic. So I like to follow this really nice opinion paper by David Hogg. Really nice opinion paper by David Hogg from, I think, April of this year, where he says, you know, machine learning is a great tool, but we've got to be careful about how we use it where. And I think a couple of the really important areas overlap with what we talked about on Monday. You've got to look at the time-critical applications anywhere where we just don't care about understanding underlying physics or outlier detection. So, today I really want to motivate using it to help solve small problems in pursuit of bigger ones and not just tackling something big right away from the outset. Something big right away from the outset. So I'm a detection pipeline expert, and the problems I want to talk about today are within detection pipelines. I'm going to focus on GST Lao, but hopefully some of this is extendable to other pipelines as well. So we actually do a pretty good job detecting things. We've seen 200 plus CBCs. We have a pretty decent purity that's slowly improving over time, both online and archival. And while this seems like we're doing a pretty great job, we have to remember that there's still a lot of things out there that. Remember that there's still a lot of things out there that we might not be seeing. And this is because some of those systems we just don't look for, and then there are also systems that we have to worry about because maybe noise is impacting how we see them, whether that's in low latency or in our archival analyses. And so I think it's really important to generally reduce the impact of these selection biases for two major reasons, at least in my mind. One, we know multi-messenger prospects kind of don't look amazing. We haven't seen as many VMs. Amazing. We haven't seen as many BMS as we maybe hoped. So we really need to maximize every single opportunity that we have. And then, two, over the last couple of years, we've seen a really large interest in taking the detection catalogs and connecting it back to evolutionary channels. So even if we're missing something that's just a BPH, it's still something that could have an outsized impact on our understanding of the universe. So I don't want to go too much into detection pipelines, but I do want to provide. Detection pipelines, but I do want to provide a background of how I think about them since GST RAL tends to operate a little bit differently in some ways. So, when a search is set up, we sample the parameter space in M1, M2, spin1Z, spin2Z, and we cover the space with our desired sampling algorithm until we're sure we're not missing that much SNR. And so, pretty much all of the matched filter pipelines just follow a technique similar to this. Where GSTLAU differs is that we then go ahead. STLAU differs is that we then go ahead and group templates that seem to be similar together. So historically, we've used chert mass and chi-effective. That's changed recently, but I think this is a better way to think about things because we recognize that chert mass and chi-effective have an outsized impact on the inspiral of a gravitational wave signal. So why do we chunk it up like this? Well, for a couple of reasons. Number one, it lets us get away with embarrassingly parallel analysis. Away with an embarrassingly parallel analysis. So each of these little regions has very similar morphology of waveforms, it has a very similar noise response, and we can do almost all of the science that we want to do in these highly local spaces. We calculate the SNR, signal consistency check, we calculate background, we assign a ranking statistic. The only thing in the entire analysis that we don't do in this very local region is calculate the false alarm rate. And that's because we need to take into account the distribution. Because we need to take into account the distribution across the whole space. But I really want to drive home that almost everything we do in low latency is done in a very parallel way in a very local space. The other thing that this lets us do is take advantage of degeneracies in the waveforms. So if we take a look at maybe the binary neutron star space and we plot all of the templates that exist there, they look darn near identical to me. So we've got pretty much the same exact waveform with very minor differences. Waveform with very minor differences, but there's a lot of them. We have a few hundred to a thousand waveforms in this space. So we use dimensionality reduction via singular value decomposition to transform these waveforms into a new space that we just commonly refer to by the abbreviation SVDs. So these new bases, there's only maybe tens of them instead of hundreds, so it's a very compressed space. We do all of our science or all of our initial Of our science or all of our initial filtering in this space, we store reconstruction coefficients that tell us the appropriate weighting for each of these bases. And then when we're done with the filtering, we add these back up according to the prescription that's inside of these coefficients to get our final SNR, our final information corresponding to that physical template. So, what I'm really interested in is asking what type of science we can do just in one of these bins where we've got this. Of these bins, where we've got this set of SVDs, where we've got this set of templates that don't have any immediate physical information described by them, but do encode valid statistics. This is especially interesting to me because there's some old work that showed that these bases seem to be complete, meaning that in this local space under which they're derived, even though they're only constructed from these little tiny dots, they're actually able to reconstruct information across the entirety of that little tiny space. The entirety of that little tiny square. So, in some sense, we are collecting information about this whole patch, and it would just be nice to try to expose that. Okay, so I think a very natural first fit to this is to ask about the signal manifold and to ask if we can interpolate across this local region of the parameter space. We heard from Gabriele's talk the other day that neural networks are a universal approximator, so they. A universal approximator, so they seem like a natural choice to apply to this problem and see if we can figure out what the reconstruction coefficient is at an arbitrary spot in the parameter space. And then we could just interpolate across the entire space. A nice side effect of the way that, I guess, this factorization is done is that interpolating the waveform, at least within the GST-WAL framework, winds up being equivalent to interpolating the SNR. So while this SNR. So while this does provide like an internal waveform surrogate, it will also allow us to rapidly sample the entirety of this local space and maximize the SNR in a local region. And the hope is that this would then allow us to get more accurate sky maps, maybe do some type of better source classification and low latency, or even connect to some type of parameter estimation algorithm that can take advantage of this method. And so this is a very small problem just within the Very small problem just within the context of GST Lau, but I think that machine learning is a very natural fit because I don't really care about the specific weights that I need to get some arbitrary waveform. I'm not really losing out much science there. And two, machine learning is something that we know can, or machine learning algorithms can run very rapidly on GPUs. So if we have an interpolant that can run on a GPU, we can do a lot of SNR interpolations very, very fast. Very, very fast. So, this led to a work called Orthogonal Basis-Informed Waveform Approximation with Neural Nets, or OB-WAN for short. I'm a firm believer that you should choose fun names for all your projects. And the general idea here was that we wanted to see if we could input a pair of masses and a pair of spins, feed it through a neural network, and accurately predict all of the different reconstruction coefficients that we needed for every single. Coefficients that we needed for every single one of the bases within GSTLA. So, for training, we generate sample waveforms and we project those waveforms down onto each of the bases individually. So, this just shows one such example here. So, here we're just projecting onto probably the zeroth basis vector. And you can see that as you move across the space, the amount of contribution that the basis vector has varies. And it varies in a very nice way in this. And it varies in a very nice way in this particular example. Okay, so we found that this actually does a really good job. The neural network does a fantastic job at approximating things, and this is true both for the very short BBH waveforms, but also for BNS waveforms that are maximal. So it's very easy within this framework to get to an arbitrary pair of masses and spins. The mismatches between some fiduciary. Mismatches between some fiducial waveform and the ones predicted by this interpolant are of the order of about 10 to the minus 4, 10 to the minus 5. So it's very, very accurate and on the same level as discrepancies between other waveform families, suggesting it's probably good enough to use to interpolate, not to interpolate, to interpret in the latency. Just as another quick slide, Just as another quick slide, so it does a good job in constructing not just the amplitude, but also the phase. There are some fun features in these plots. One thing I've not talked about is another thing specific to GS2L, where we segment the waveforms in time. That does lead to some funny behavior at edges of time slices, although the residual is still pretty low. And then, secondly, it does a really poor job with phase for BBH mergers, but I think that's okay given the goal. But I think that's okay given the goals of this work. So I did mention that this would hopefully be used in low latency, or at least have some type of low latency application. And so it was really important to me that this would be a very fast model. And so I did some benchmarking on a couple of different GPUs and CPUs. You can use this method to produce a single waveform on either a CPU or a GPU in about the tenth of a millisecond. This is, I think, on par with just a standard Laosuite. Par with just a standard Laosuite CPU call. So it's same word magnitude. But where you really see a benefit is when you start to bash these waveforms on GPUs. On an 800, you can actually get 10,000 waveforms in a single millisecond. So you can really densely sample this space in almost zero time. So you can sample the intrinsic space at arbitrarily high points and use that information in low latency. So the next step that I envision for this project. So, the next step that I envision for this project is to hopefully connect this to some type of SMR maximization. In low latency, we already have a preliminary notice that's sent out with the first thing that we detect. Then a couple of minutes later, detection pipelines wind up reanalyzing the data, trying to maximize the SNR, and we upload the new trigger, which leads to a new sky map. I should point out both of these are from 04, they're both public, but you can see that we go from 217 square degrees down to. You know, 217 squared meters down to just 81. And so I think what's intriguing about this is that there's the potential to try tens of thousands of waveforms almost instantaneously and maybe even be able to provide a sky map like this with the preliminary notice if it's connected to a pipeline in an intelligent way. Okay, so this first project really just looked at how we can explore the signal space using SVEs, but I was also interested in seeing if we can. But I was also interested in seeing if we can learn anything about the noise space. This is a natural place for machine learning as well, because as we've heard, you know, detector noise is non-linear, it's poorly modeled in most cases. And as a detection expert, it's something that winds up haunting me sometimes because we have these signal consistency checks, and every once in a while, some new noise class will come through and trick them. This is especially problematic when we've just got a single detector on because we can't use coincidence to help. Because you can't use coincidence to help suppress the noise background. And the thing that really motivated me for this next project was to the similarity between blip glitches and what I'll call astrophysically interesting BBHs. We know that BBHs that have high mass ratio, very negatively aligned spins, they look very similar to blip glitches. And to demonstrate this, I've got an image of a blip and an image of a BBH here. Image of a VDH here. If you're clever, you'll be able to tell which one is which almost immediately, but at least to the eye, they look nearly identical. If you cheat and you look at the axes and you notice that this is jumping up into the thousands in frequency, then you'll see that this is clearly a blip. This one is a BBH from, I guess, the end of O3. And so, what I wanted to check now is if we can create some type of robust classifier using information that's encoded in the SVDs, which describe the local gravitational waves. Which describe the local gravitational wave signal space. So, the first thing that I wanted to do is see exactly what a blip glitch and a signal would look like inside of this new representation. So, I recast those images in this grayscale form. So, what we're seeing here is again a blip glitch and a C V C signal. On the y-axis, I have all of the different SVD bases. SVD bases. On the x-axis, I have the time in milliseconds, and the coloring on this just denotes the SNR that's being deposited into a basis vector at that instance in time. So one thing that's striking to me is that there's already clearly some type of difference between these two images. One looks maybe a little bit sharper, one has a little bit more jitter around it, but it's still not enough for me to be able to tell which is a BDH and which is a blip. And which is a blip, but certainly this is something that a machine could figure out given enough training samples. So, to check this, oh, actually, first, I want to orient ourselves with what the current signal consistency checks look like in this paradigm, since this is not a normal way at all of visualizing the search response. Within GSC Lab, we used two different signal consistency checks. One is chi-squared, which takes the integrated squared. Which takes the integrated square difference of the signal-to-noise ratio and the SNR scale autocorrelation. This looks at a small window in time and looks at a single template. And the way we can kind of visualize this looking in this space is some box that has a narrow spread and time and encompasses some of the bases, but not necessarily all of them. So it uses only this really local information. The other signal consistency check that we use is the check that we use is the bank chi-squared. Bank chi-squared looks at how the, it checks to see if the signal distributes power across all of the templates in our bank at a single instance in time the way we would expect it to. So this looks at all of the basis vectors, but it really just looks at one instantaneous point in time. So if you're looking at this, you might think, as I did, well, there's a lot more information, certainly all the Information, certainly all the way up here, and there's definitely a lot more information the further you go out from a signal. So, what would happen if we just used all of this information? Can we use a neural network to capture everything that's encoded in there? And so, for this, we used a CNN, and we generated a number of different BBHs, a bunch of different blip glitches using Melissa's Gangli package, and a number of different scattering arches using. Arches using Udahl's model. And after generating a bunch of these, we're left with images that look like this. And we feed each of these through a simple CNN, with the only task being to classify these images as either a signal or a glitch. And what I'll talk about for the rest of the talk, I'm actually only focusing on gravitational waves and glyphs. We did do some work with scattering arches, but I'll We did do some work with scattering arches, but I'll save that for another time. And so the goal is to see how a CNN perform in classifying these compared to a normal detection pipeline, and could we use this to help improve upon existing detection pipelines? So it actually did a really good job. It was kind of annoying how great a CNN did of this. I really just want to focus on this red box up here. You can ignore the rest of the confusion matrix. Rest of the confusion matrix, you can see that the CNN correctly identified over 99% of the signals and over 99% of the glitches, with just a little bit of confusion on the edge here, which is probably almost data limited in this study. So it did a fantastic job. It's easy to say, well, this is amazing. Like, why don't we just go with CNNs to reject glitches? Why don't we just toss detection pipelines out the window? So the first thing that we need to do is actually create a detection pipeline and see how good of a job. Detection pipeline and see how good of a job it already does against these sling glitches. I didn't want to run a full analysis, they're expensive, they take a while to maintain, so instead, I'm using a toy model detection pipeline that uses this weighted SMR statistic. So it takes the SNR and it divides by some function of the signal consistency check. So if there's a poor fit to the data, it gets downweighted, and if it matches the data well, it has a maximum value of the S and R. And so I fed this. And so I fed the same images into this algorithm and set varying decision thresholds based off of RhoBar to see how it performed at rejecting pitches. And the answer is that we already do a pretty great job. If you're looking at a weighted SNR of 8, we don't produce any false positives using this, at least in this training set and for this very simple method. The main negative is that we wind up missing 10% of astrophysical signals if we're just If we're just using the signal consistency check and the SNR, so it'd be nice to see: can the CNN improve upon this as an additional weight? Does it capture some type of complementary information to the information that detection pipelines already have? So, to do this, we use the same detection threshold, but now we use the score that's output by the CNN as an additional weight on our signal consistency check. On our signal consistency check. So the function itself here is kind of arbitrary, but I did select it for a couple of nice reasons. One, if the CNN says that p glitch is 0.5, then it's uninformative and there's no change to the bold ranking statistic. If it says p glitch is 0, this is definitely a signal, then it downranks the impact of that signal consistency check. And if it says that p-glitch is over 0.5, this is definitely a glitch, then it increases that effect. The other thing that I elected to do here. The other thing that I elected to do here is to bound the impact that this can have on the weighting. A lot of people that have played with CNNs or classifiers know that sometimes they can perform really poorly. The nice thing about bounding the impact is that it mitigates the effect it can have on the overall toy model search. Alright, so if we incorporate peak glitch into this pipeline, we do an even better job. So now we have good false positives at an even lower SNR. Positives at an even lower SR down at six, and now we only miss 1% of all transits. So it does seem that the CNN is capturing some additional information that is not being captured by the existing detection pipelines. So I don't have a way of saying that we should use this immediately. This is how we incorporate it into existing pipelines. But I do think that it's at least encouraging me to start thinking about if there's another way that we can use the SVD-basy. Use the SVD bases as some type of signal consistency check. If it winds up being this, great. If you find something analytic, that's also fantastic. So I ran a little bit fast, and I'll just leave you guys with a couple of conclusions. I think the main thing that I want to emphasize is that machine learning is a really powerful tool, and it's not going anywhere, one way or another. We've seen the direction that funding agencies are going. We've seen that it has some really great applications. Really great applications. I would just encourage people to also start thinking about small problems that are close to their heart and not just these huge problems that exist within the field. Because even with these two small things, I think there's ways that it could contribute to both the low latency ecosystem and our general catalogs. So that's all I have. I'm happy to take any questions. Questions? I don't think the main purpose is to classify between the first cases that where you were classified between PBAs and blips, the main purpose is to classify the two. But if you say you have a PBH or another second blitch on top of that or nearby that, then. Okay, so I did not look at overlapping. I did not look at overlapping. I didn't formally look at overlapping cases. It seemed to still perform pretty well, is all I can say there. But that is definitely a concern. So, Miki, so for the I'm interested in the faster part, and I'm wondering how the interpolation errors scale with the, for example, the size of new network. How the interpolation scale? Well, interpolation errors, like uh like you interpolate that coefficient, so your waveform should have some finite errors to represent that collisional waveform, right? Yeah, and I'm wondering how how they how that those errors are determined, right? Um I'm not sure I understand which which errors are you talking about. Um yeah, that's what I'm talking about. However, which errors are lower part of the errors? Errors and I and so that you you change you for example like you could change the size size of the new neural network like a yeah and then so I guess the if you use a more like hyperparameter interuse more hyperparametments then your interpersonalness should be decreased. Yeah, so you could probably do that. That's totally fair. So you could increase the complexity of the network and probably get even better fidelity. But I think the point is at this level does it really matter for a low latency? At this level, it doesn't really matter for a low-latency application. As you increase complexity, you're also going to increase the memory overhead, the time it takes to evaluate the model, which will psychiatric both. Next. Sorry. So, yeah, really nice talk point. I was just curious about these. So, like, you calculate these bases using actual waveforms, right? And then you know what the glitch is. You like try. No, but the glitches. You like transform these glitches from the time domain to the basis functions. No. So the glitches themselves are essentially just projected onto the bases. And that's what winds up giving us these images. The glitches are projected from, it's a time domain glitch. And then that's projected onto the bases that were calculated using the waveforms. And I was just curious, like, it's obviously robust to background noise, but because you calculated the bases using clean waveforms, am I right in that? So it seems to be robust to whatever background is going on here, depending on your noise, glitch plus noise samples and signal plus noise samples. The SGD still remains robust in those. Metropolis. Yeah, so I think the clarifying, that's a good question. It relies on the assumption that the average noise over the last couple of hours or whatever hasn't deviated so much from when the SPDs were computed. But otherwise, it'll be pretty robust noise. Okay. Thank you. Um, so you're talking about the astrophysical signals. They are high masses and negatively large in somewhat parameters for these. Are they down for masses? Down to negative 0.99 for spin, masses. I want to say I followed something from Miriam Cabaro's paper on defining when it looks like a blip, so like probably above 60 solar masses for total mass and then mass ratio above five at least. So my point is that then how do you envision that this algorithm or this network will work? This network will work well for LOMAS? Question. So I did look at that in the context of different BBHs. So, like, if you have a vanilla BBH out there, how does this work? The answer is it still works well. I didn't try it with BNSs, but if you look at lip glitches as your type of noise source, and you look at vanilla BBHs being injected in, and you're still trying to recover it with a noisy template bank, it still does just. With a noisy template bank, it still does just as good and technically a better job. So it's robust, at least locally. I haven't done it across before prankerspace. Yeah. Is that because the network does better with the Inspire only and the monitoring does not capture as well? If so, then how is it doing so well in terms of distribution? So, I think for the BNS, a lot of it comes down to just how much power is deposited during the actual inspiral. And so you're typically not going to have very many bases there. They're going to look pretty sinusoidal. They're going to be easy to add up. Then your second question was: how do we expect it to do good on this if it doesn't do great with the merger? Yeah. Yeah, I think so. With the merger, we Yeah, I think so. With the merger, we were really just seeing it do poorly. With the merger phase, overall, the amplitude was still pretty accurate. So I think, you know, if the phase is off but the amplitude's the same, then maybe this picture changes somewhat, but it would really just be depositing it in like the complex SNR instead of positive. And the spectrum in the phase is more important, right? That's what is the evolution of the phase. Evolution of the phases work would be degenerated. Uh yeah. Yes, I think. I'll have to think about that. More question from me. No one's got their handy. So you started off with this um sentiment of sentiment of scepticism, but feeling there will be potential you've shown there is potential in putting machine learning into existing things. Machine learning into existing things. What do we have to do? I think of a full single machine learning search pipeline, what criteria has to be hit for you to go, okay. I would say like I don't even distrust the existing machine learning search pipelines. I just think that while it, I think you shouldn't immediately drop old methods just because you have something you can shine in. That's pretty much it. Oh, okay. Well, if there's no more comments, we have succeeded in being slightly under time. Thank you to the speakers again. No announcements. 8:45 tomorrow for the conclusion of the drug. And remember, you're not saying that.