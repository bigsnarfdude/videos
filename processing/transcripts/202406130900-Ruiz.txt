Here, so thank you so much. And yeah, today I'm going to talk about this paper that we just presented at iClear this year. So the idea for this paper came from the following. So in my PhD, I worked on convergence, analyzing convergence of GNNs on sequences of graphs converging to graphons. And for these convergence analysis that we do, For these convergence analysis that we do, you know, they hold for graphs that are sampled at random, right? So, where the nodes are sampled at random from these graphons. So, they are the bounds that we get, you know, they're nice bounds, but they're probably pretty loose bounds. And then when I started working with Stephanie, Stephanie has been working on something that is called graph positional encodings, which is just a fancy name for inputting eigenvectors, graph eigenvectors at the input of GNNs, to try to capture global properties of the graph. capture global properties of the graph. And the question that you ask whenever you have to operate with graph eigenvectors on large graphs is: okay, it's very expensive, right, to compute eigen decompositions of large graphs. And so that motivated looking at better ways of sampling graphs, given that we know some information about the signals that are trying to sample. So for instance, if we know that they're meant limited in the sense that they only depend Limited in the sense that they only depend on a few eigenvectors of the graph. Can we use that information to come up with better sampling schemes? So that's kind of like the background. Also, this is the first time I'm presenting these slides other than in poster format. So it might be a little rough around the edges, but I'm happy to hear your feedback. Okay, so you know, doing analysis and problems. Doing analysis and processing of information on graphs is something that scales with the number, the size of the graph, right? So, be it in terms of the number of nodes or number of edges, here we focus on algorithms that scale with the number of vertices. However, in many graph problems, the actual difficulty of the problem or the intrinsic dimensionality of the problem is typically much less than the graph size when you have large graphs. Less than the graph size when you have large graphs, right? So, to give you a few examples, for instance, on well-behaved graphs, if you're trying to do community detection, usually you can do pretty well by just looking at the top k eigenvectors of the adjacency or similarly of the Laplacian matrix. And another example is, for instance, trying to determine the bipartite structure of a graph, and you can do that by looking at a pair of symmetric eigenvalues of the graph. So the idea. So, the idea here is that we can think of the graph size as sort of the data resolution, but you know, the dimensionality of the problem is smaller, right? But as long as this data resolution is larger than the dimensionality of the problem, we should be able to find a good solution. So, like I was mentioning earlier, my work has focused on convergence of signal processing architecture. Of signal processing architectures in general, but specifically GNNs on sequences of graphs that converge to graph limits that are called graph ones that I'm going to discuss in a second. And the practical implication of these convergence results is something that we call transferability, which is essentially just a non-asymptotic statement of these convergence results, right? And essentially, what we, what a transferability theorem for GNNs is, is imagine that you have a GNN that has been pretty. Imagine that you have a GNN that has been pre-trained, the weights are fixed, and that you have two graphs, GN and GM, that you think of them as being part of a sequence that is converging to a graph limit like a graph one, and they have different numbers of nodes. If you take this GNN and you look at the output of the same GNN with the same weights on the graph GN and the graph GM, the difference between the outputs is going to be proportional to the minimum between one over. The minimum between 1 over n and 1 over m. So, what this means is that you could take a GNN, you could train it on the smaller graph, and then transfer it to the larger graph with performance guarantees, especially as you increase the training graph size. Okay. And like I was saying, Erina, you know, if you had, this is a, yes. That is L2 norm. And here I'm looking at this. This is the representation of these graph signals, which are vectors, as functions. So I'm kind of, yes, step functions in unit interval. Okay. Okay. So a similar result like this holds even for graphs with randomly sampled nodes and probability. So it's likely loose. And so, can we do better, right? And the answer is yes, provided. Do better, right? And the answer is yes, provided that we have more information about the signal. So, did you have a question? No? Yes. I was maybe trying to get intuition for the result by considering maybe the simplest possible case. And so I was simply imagining I take the integral 0, 1, I sample n points from. n points from it and I form simple graph or I sample another m and I now have a function and I'm evaluating like but now I the difference of the values of the function so why would it can you provide in that case which is maybe the simplest case intuition as to why the dependency would be one over n plus one. Would be one over n plus one? For me, I think the high-level intuition is think of the Riemannian sum approximation of the integral, right? That's basically the. So as you make the resolution finer in the Riemannian sum, right? You get a convergence. And if you have continuity assumptions on the function, right? So this is an important thing that I wasn't mentioning here, then you get this sort of break. And to that point, And to that point, these continuity assumptions are not very good to have for these rhythm graph models because you need permutation invariance to the labels, right? But even in that case, there would need to be some extra assumptions. Like if I sample the first graph in my example from the interval zero to one half and the second one from one half to one, then the assumptions about the DNA and the fact that they are The DNA and the fact that the sampling from the graphon needs to have some special problem. We're sampling from the same, always from the same interval, so uniformly in 0, 1. We don't make this. Okay. And without continuous assumptions, you still have something like this, but the rates are much worse. It's like one over exponential, but just two. That you know that there's something. Okay. So can we do better? And a lot of people have been. And a lot of people have been looking at this question and the signal processing community, specifically in graph signal processing. So, the first paper in signal sampling theory for graph signals was this paper by this mathematician, Isaac Evanson from Temple University. And he had a very general framework for sampling from these spaces of ven limiteds or Paley Winner signals on graphs. And then his results were largely adapted by these. were largely adapted by these two papers here that were the first to propose like a signal, a sampling theory in the context of signal processing on graphs. This first one kind of had, you know, essentially said that, you know, you take your Laplacian matrix, look at the eigenvectors of this matrix and try to find a sampling set that is orthogonal to the top k eigenvectors if you're sampling for the for the Paley Wiener space corresponding to the Space corresponding to the top K frequencies. And this one here tried to do to make that process more efficient to kind of circumvent having to compute eigenvectors by using them similar to power iteration to approximate the eigenvector. Then there were other approaches. So in these first few papers here, people were actually sampling nodes from a graph. And this one here, which is pretty interesting, what they do is they now Do is they now define a diffusion process, meaning they apply the Laplacian to a graph signal multiple times, and they look at a single node and look at the samples of this diffusion process across time and sample a few points from this, from this diffusion process. And they're able to show that by sampling only a few of them, they can reconstruct the signal on the entire graph. And then later, there are people that, you know, because a lot of these problems. Because a lot of these problems, they rely on greedy heuristics for being solved. Usually you do Gaussian elimination. There was a paper that provided some near-optimal guarantees for alpha superpodular functions in the reconstruction objective, right? So I think in this paper, what they do is they show that the MSC, so when you have a reconstruction objective, which is minimizing the MSC between the sampled signal and the original signal, they show that. Original signal. They show that the MSC is alpha-silver modular and they give near-optimal guarantees in that case. And then, more recently, there was a paper that, under subspace and smoothness priors on the graph signals, kind of generalized a lot of these previous results. Okay, so this is just a non-exhaustive related work in graph signal processing. Okay, but what's the issue with a lot of those? With a lot of those techniques. So I hinted at this already. A lot of them require doing expensive graph operations, right? So if you have a large graph, computing eigenvectors is expensive. Even doing power iteration to get good approximations can be expensive. And so one way that we wanted to circumvent this was to maybe use this idea of convergence or transferability to a limit with low rank to aid in finding. To aid in finding sampling sets for signals on large graphs. So, how did we go about this? So, on the theory side of things, we defined optimal sampling sets on graphons, which are these limits of graphs that I'm going to talk about in a second. And then we proved that optimal sampling sets on graphs converging to graphons converge to these graphon sampling sets. And then, in practice, And then, in practice, we proposed an algorithm inspired by these theoretical results to obtain sampling sets on graphs from these graph on sampling sets. Yeah. So what is being sampled are the nodes of the graph to preserve information on the graph signal, which is right, the features or the data on top of the nodes, vector-value data. For instance, right, like you could sample intervals from the graph on, and then you could reuse those intervals to sample nodes from multiple graphs sampled from that graph on. Yeah, but similarly, what you could also do is, okay, you, when you look at the graph one, you kind of see the lowering structure much better, right? rank structure much better, right? So you can essentially coarsen that graph on and find these sampling sets on this object that is much smaller in terms of number of dimensions and still find sampling sets for these large graphs without having to compute eigenvectors of the large graphs. I'll try to be more clear about this later. Any other questions? Okay. Okay, so just Okay, so just as a preliminary, all of these, all of this theory of graph signal sampling that was developed in the graph signal was in community is actually inspired by sampling in sampling real signals, right? One-dimensional real signals. And the main result in signal sampling theory is the Shannon-Nypus theorem, right? And what it says is that if you have a signal, a continuous signal whose A continuous signal whose Fourier transform has been limited in terms of frequencies between minus B and B, then you can determine, uniquely determine it by sampling it at rate 2B or greater than 2B. So I don't know if you can see, but I tried to exemplify this here. So here, anyway, here we have the signal. The signal. I'll just go with the so here we have this blue signal and the red dots are the samples, right? So as long as you sample with a period less than one half here, you're able to reconstruct the synosome, right? And this figure here, so this I adapted these slides from up. So this, I adapted these slides from a presentation that Teen, who's the first author of this paper, gave at the DeepMath Conference last year. And he had this figure, which I thought was kind of cute. And basically, he said that, okay, what he was saying when he showed that figure was that, you know, if you have a signal and you have a bunch of samples, in fact, like it's enough to kind of sample at the Nyquist rate and pass that through an MLP to learn whatever representation, right? And in the worst case, you could assume that. And in the worst case, it could assume that the MLP is first learning to reconstruct the signal and then learning whatever representation. This is just to illustrate the usefulness of this in machine learning. Okay, so how does this translate to graphs? So, to translate this to graphs, we first have to generalize the notion of a signal bandwidth, and to do that, we need to generalize the signal. Bandwidth, and to do that, we need to generalize the notion of a Fourier transform. So, the graph Fourier transform is given by first looking at the Laplacian. So, here we look at the normalized Laplacian, just because we're going to study convergence later, but it could be the Laplacian or even the adjacency. You look at the eigencomposition of this Laplacian. And then the graph Fourier transform is defined as the projection of your signal onto the eigenvectors of the graph Laplacian. Okay? And then there's another. And then there's a nice interpretation, or there's a nice relationship between the eigenvalues of the Laplacian and the notion of frequency of these eigenvectors. Because you can show that the total variation of these eigenvectors is actually given by the eigenvalues. And because the Laplace matrix is positive semi-definite, you have this nice kind of interpretation as, you know, the larger the eigenvalue, the larger the frequency of this eigenvector. Frequency of this eigenvector oscillation of your graph. And now, once you have this notion of a graph-Bouge transform and this notion of graph frequencies, you can define the notion of a bandwidth and therefore the notion of a Payli Winner space, right? Which is the notion of spaces of signals with bandwidth lambda. Okay, so those are signals that only have spectral components in eigenvectors with eigenvalues at most lambda. Eigenvalues at most lambda. Yeah. The spaces of what? I don't know because I, yeah, I'm not familiar with those. Yeah. You could define it either way, but usually it's just the value lambda. And then you only look at the eigenvalues that are less than lambda. Okay. All right. So what this theorem is saying is that if you have a graph signal that is If you have a graph signal that is bent limited in this sense, right, that belongs to a pale-linear space with parameter lambda, then you could find a so-called uniqueness set, which is given by a subset of the nodes of the graph, that completely determines that signal, meaning that if you know the values of that signal in the subset U, you can reconstruct the signal on the entire graph, right? So if the signal Right? So if the signal match in you, they match everywhere. Okay? Okay, so that was the background on graph signal sampling. Now let's talk about graphons. So graphons are defined as symmetric measurable functions from some space omega squared to 0, 1. Typically, we consider omega to be just a unit interval. To be just the unit interval. And the way that you can think of the graph on is as a graph with an uncountable number of nodes. So you can think of the nodes as the points in the unit interval and the edges as the graphon values at points in the unit interval. Okay. And people who work on graphons, they have come up with two interesting taples interpretations for these objects. For these objects. So you can interpret it as a graph limit in the sense that the densities of certain motifs in these, in graphs converging to graphons, converge to the probability of sampling those motifs from the graphon. And then you can also interpret it as a random graph model in the sense that you could sample nodes uy uniformly from the space omega, and then sample edges Bernoulli with probability w uy uj. Okay. So, this is just to illustrate what I was talking about before, right? So, here we have an unbalanced SBM graphon. We have an unbalanced SBM graphon, and here we have a sequence of graphs with increasing number of nodes that were sampled from it. And you can show that the sequence converges in that sense, that the density of is in that sense that the density of motif counts converge. And so that's the limit interpretation and the random graph model interpretation is using sampling from the unit interval and then sampling edges to generate those graphs. Okay, so you know for our purposes, what I mentioned earlier is that what we're trying to do is to Mentioned earlier is that what we're trying to do is to have this signal sampling theory for graphon signals, right? And then we're trying to show convergence. So basically, what we want is to have a generalization of the signal sampling theory for graphs that I just mentioned a few slides ago, two graphons. And so this is our first result, right? So what is the idea here? So the same way that you have graph Laplacians, you can define Graphon Laplacians. You can define Grafon Laplacians, right? So the Grafon is the symmetric kernel, so it defines this integral operator. If you just do identity minus this integral operator, you get the Graphon Laplacian. And this is a self-adjoint Hilbert-Mid operator, so it has this real spectrum. And so in the same way that we could define Grafon-Fouhere transform and graph, sorry, Graph-Fouhe transform and graph frequencies, we can define Grafon-Fouheat transform and Grafon frequencies in the same way. And graphon frequencies in the same way by looking at the eigenvalues of this operator. And so we can also define these Hayley inner spaces of graphon signals, right? We met with lambda. And so with this setup, we define a uniqueness signal on a graphon, a uniqueness set on a graphon as a set U, which can now be thought of as unions of intervals of 0, 1. Unions of intervals of 0, 1, if we're considering 0, 1 to be our node space, such that if two signals F and G match in the L2 norm on the set U, then they match everywhere on the unit interval in the L2 norm. Okay. Okay, so let me try to give you like the idea behind the proof. So, like I mentioned earlier, the graphon Laplacian is this operator here, right? So, this, you can think of this as kind of like the graphon adjacency matrix, right? This integral operator that normalized by the degrees applied to a graphon signal f of u. And so Laplacian is just identity minus that and. And I included this because, you know, we call it a Poncage inequality, et cetera, et cetera. So I just wanted to kind of show you where it shows up. So a Poncage inequality has this form. This is not really the theorem that we prove. The theorem that we prove is something like, you know, given that you have a signal F that is only supported on a subset S, so it's zero everywhere but in that subset S, then it always satisfies. It always satisfies such an inequality with non-zero lambda. Okay, so it's not a this is not like a vacuous bound. You can always find a lambda such that this is true. I'm not going to go into the details of that because it requires like modifying the graph a little bit, but this is what we use. And so the way in which this result follows from these two propositions is actually pretty. Is actually pretty simple. So let's say that you have two signals F and G that match in the set U, right? So meaning that F minus G in L to U is zero. This is our first assumption. And our second assumption And our second assumption is that both of these signals belong to a Paley-Wiener space with parameter lambda. Okay, we're not going to specify lambda just yet. Okay, so since so basically what we're trying to show here is that these signals match and These signals match in the set U. Do they match everywhere? So if we write that Poncage inequality, now we're writing this over the entire interval, right? Which is equivalent to writing it only on the complement of U, right? Because we know that the signals match in U. Signals match at you. And then we write this, then we use this Bernstein inequality here, which is, this is, it's easy to see that this is true, right? So if you have a signal that only has energy in the, for eigenvectors up to lambda, then Then the application of the Laplacian to the signal is at most lambda norm of the signal, right? You don't create more energy. So using Bernstein's inequality, then we can write this as Okay. Without that, thank you. So, okay, so if So if big lambda times small lambda is greater than one, there's nothing problematic with this inequality. But if this is less than one, the only way that this can hold is if F minus G in this complement of the set U is zero, right? Right? And so what this implies is that for frequencies less than one over big lambda, right, which is the parameter of that Poncage inequality, u is a uniqueness set. Make sense? So this is the high level. Yeah, so to understand intuition, if we think of just an Erdős-Rendney graph, then that's just a constant function. And so it's only got one eigenvalue, which is one or something. Right. So, what does this say? So, what this says is that you can only reconfigure. Can only reconstruct signals that are constant essentially, right? Because you only have that constant eigenvector. Yeah. But so this will become more interesting for more interesting graph ones in a way, because you need a rich enough spectrum. I guess my question is, what I'm confused about the set U. Like for Erdos Renny, it's literally just one point. Literally, just one point, right? Because once you know, okay, I see. Yeah. Okay. Yeah. There's no restrictions on like U being like non-zero measure relative to zero, one? Right, it has to be measurable because we're looking at the L2 norm. It has to be measurable, but it can't have zero measure. zero then it could be zero trivially but then the the l2 norm is not defined if you have zero magic so maybe is it just like any arbitrarily yeah it's like an epsilon around yeah a point okay um yeah i'm not so sure and maybe maybe i had the wrong understanding so what i was going to ask you is can you give examples of Can you give examples of what the set U looks like? And I was having in mind sort of the basic sampling theorem where you can pick Q to be a finite set. And so from a finite number of samples, you can reconstruct the whole signal. And so now the question, but that would be a set of zero measure. And so I can, is it really true that you need to have non-zero measure? I think what What I think what you need is the complement of the L2 of U is not necessarily defined, right? Yeah, but then I guess. But it would be zero for anything. It would be zero for anything. So even if they were different, pointwise, it would be zero still. So because we're looking at things in the L. So because we're looking at things in L2, yeah, we need non-zero mid. Yeah. It's kind of, did I show that in the previous slide? Yeah, like we need, this is what I was trying to show here. Any other questions? Okay. But then there must be a version of the theorem where you require f to be equal to g on the set. Not just the integral. Because the integral being zero would allow for differences. Would allow for differences. What is really needed is that f is equal to g of g. Right, right. I think we tried to look at this theorem for, you know, pointwise, essentially pointwise, but we couldn't figure it out. And yeah, in my work, I kind of look at L2 signal. So, but that's a good point, a limitation. Limitation theory. Let me just now what we want to show is that signal sampling sets on graphs converge to these sampling sets on graphs, right? So the way in which we did this. The way in which we did this was by adapting a paper on the geometry of spectral clustering by the News Group, in which they considered these models, which are these models where you have the data points coming from a mixture model and then the relationships being determined by a kernel on this page. And so, the first thing that we needed to show is that a graph on a Needed to show is that a graph on is in some ways equivalent or can be expressed as this sort of model, right? So essentially, we're just trying to show the equivalence of two probability spaces. So the probability space of with simple space omega, right, which in our case is the unit interval with the uniform measure, with this space omega prime, where we have this mixture model, right? This mixture model, right? So, basically, I mean, you can show this equivalence as long as you have a measure-preserving transformation from space omega to space omega prime. And this measure-preserving transformation is, in fact, the inverse of the CDF of the mixture. Okay, and so as long as And so, as long as the CDF is strictly monotone, then we have such a measure-preserving transformation, right? Because it's biject. This is just to show the equivalence between the two models before we can use this result. Okay, so the first theorem is essentially a corollary from one of the results in that paper. Results in that paper. So, in that paper, what they define this difficulty function, which is given by it's like a function of certain decoupling, indivisibility, and similarity parameters between the elements of the mixture. So, just to give you a high-level idea, so they're trying to see how similar or how much, how similar these mixers. Much how similar these mixture components are to each other, meaning if they overlap in the space omega prime. They're trying to look if they're decoupled or if they're coupled with, in a way, they're trying to look at whether the kernel in that model kind of scrambles the mixture components in a way that they're not orthogonal. And they're also trying to quantify whether each component in the mixture is irreducible. Mixture is irreducible in a way, right? And that defines a difficulty function. So, the higher the difficulty function, the worse the mixture model is. But if that difficulty function is low, then what they show is that if you do spectral clustering on the finite Laplacian, on the Laplacian of the finite graph, then with high probability, you essentially cluster the the the the points the data points right according to uh according to the uh the eigenvectors of the of the limit laplacian okay the of the the the kernel model okay so here so they essentially show that with high probability points from the same cluster which are uh here denoted by the same colors are in this orthogonal Colors are in the orthogonal cones around the eigenvectors of the limit Laplacian. Okay, and so we adapted it by saying, you know, if you do Gaussian elimination on the finite graph Laplacian, then with high probability, you find uniqueness sets on the limits of the plus. Okay? And this is because you're just using And this is because you're just using the eigenvector matrix for both things, right? For spectral clustering, you're doing amians clustering of the eigenvectors. For finding uniqueness sets, you're doing Gaussian elimination of the eigenvector matrix. Is that here? And then from there, we get our result, which is that under those assumptions, right, the same assumptions of indivisibility, decoupling, and similarity. Decoupling and similarity. Then, for a sequence of graphs converging to a graph on, there exists an n such that for all numbers of nodes greater than n, a uniqueness set of this graph g big n is also uniqueness sets of these larger graphs. And that to kind of go from the previous one to this one, you basically only need to show convergence of the eigenvalues and eigenvectors of the graphs to the eigenvalues and eigenvectors of Values and negative vectors of the graph one, because then doing Gaussian elimination is equivalent, or doing Gaussian, whatever you get from Gaussian elimination in the finite graphs converges to whatever you get from Gaussian elimination on the large graphs. Is that clear? Yeah. No. No. So, okay. So, if you have a finite graph, it isn't because what you have to do is you essentially have to check the Poincahé inequality for different subsets of nodes. So, it's combinatorial. So, you know how I was saying earlier that people use a greedy heuristic? That's why, because of the combinatorial nature. What we were trying to do here is because this is a difficult problem to show, to solve, let's solve it on a smaller graph that is actually a coarsening of the graph. So, I'm going to get to. Coarsening of the graph. So I'm going to get to that in a second. Jesus algorithm. Okay, so how do we use this in practice? So we have our large graph, right, that we're trying to subsample. The first thing that we need to do is kind of obtain the graphon model of that graph. I'll get to that in a second. Once we do that, then we have our graphon, then we're going to use these. Are going to use these theory that we have developed to find a uniqueness set for the graph. Okay, and then from there, we're going to essentially sample from this graphon uniqueness set the uniqueness set for our large graph. So, how do we obtain the approximation of a graph on approximation of our large graph? So, we consider the induced. Consider the induced graph on our large graph, which is just essentially taking the graph and plotting the adjacency matrix on the unit square, right? Then what do we do? So, like if we just did that, that doesn't help because this still has dimension n by n, right? What do we do in practice? And this is one thing that is not like it's not illustrated here, but what do we do in practice is we parse. We do in practice is we force in this graph one. So we say, okay, let's now look at intervals of wider intervals here, and then we take the integral of the graph one in those intervals to obtain our cost in graph one. And then that coarse in graph one is just like a smaller graph, right? So we find the uniqueness sets on this smaller graph. And then that gives us a graph. And then that gives us our graph on uniqueness sets. Am I making sense? So, yeah. And as long as I haven't quartz in too much, that I lost all information, right? Okay. So once we find, then once we find, yeah. Okay, sorry. Okay, sorry. Yeah. I don't know what that means. It's not anything was it. What we did is Okay, okay. So precisely what we do is we have we have five nodes, so we uh consider intervals of 1.5. intervals of 1.5, right? 1 over 5, 0.2. Then we assume an ordering of these nodes. That's a limitation. So you have to assume an ordering of these nodes. And then say that this is one, two, three, four, five. And then now there's an edge between one and four. So I have to paint. Yeah, I have to paint this square here. Right? This is one, this is four. Makes sense? This is for. Make sense? Yeah, and this is symmetric because the graph is interacted. Yeah. Yeah. Yeah, yeah. So this works if you like know your graph fun, and so yeah, like the unit interval kind of gives you a natural ordering, right? Interval kind of gives you a natural ordering, right? If you're sampling your graphs from your graph one, you have that natural ordering. If you just have any graph, then it's a problem. What we did in practice in the experiments is we would sort the nodes by degree. So by decreasing degree and hope that that's, you know, that the graphon is kind of continuous for that ordering. But it's a strong assumption. Graphons are only defined at the order. Yeah. Yeah. You don't, but then we're looking at convergent sequences, right? So at least, like, if you're looking at hectic sequences, I am assuming that the labeling is the same. Did that answer your question, Renee? That is probably me. I thought that if I'm given a graph, I guess I don't know the two coordinates. Right. So that's the reason I cannot just put points on the 0, 1 squared. Yeah. So I guess I'm worried about getting it completely wrong. You could. And in practice, when people are, for instance, trying to estimate these graph one models from graphs, they have to go over a set of these permutations. Right? What I'm really curious about is what are the connections of these with kernel methods? Because in kernel methods, when I'm just giving the set of samples, I can just build. Just build the kernel that is n by n, but those come from the kernel in the Hilda state. So there must be work on extrapolating the values of the kernel. Yeah, I don't know about that. But I mean, I don't know about those works. I imagine that there is because for me, like a graph one and this kernel, at least this kernel mixture model. This kernel mixture model, they are equivalent, right? By that result that I showed earlier. So I imagine that that's the case. But I think what is bothering you is like, there's a reason for that to bother you. Like, you know, this makes sense if you have your graph on and you're sampling this graph from your graph on, because then you know, if you're sampling from the graph on, then you know where you sampled in the unit interval. So you have the labeling already, right? So if I sample this from point two, makes sense to put this at point two. But if I haven't, it doesn't. But if I haven't, it doesn't necessarily make sense. But then I, in practice, what I do is I order them by degree and hope that the graph one is continuous for that ordering, or that it looks like a nice function for them. But I don't. I think the difference with the kernel methods, though, is that the kernel, you can't the kernel method doesn't already have invariance to or the kernel method already, there's just one kernel. Already, there's just one kernel in the graph on, there's a set of graphons that all give rise to the same possible network through the permutation of theory. So, you're not necessarily estimating w, you may be estimating w of some permutation of uik. Right? And I think that's that's where the difference lies. But it's a little bit more than that, because you're right. Let's say I have a kernel, and in fact, let's say I know it, then Say, I know it. Then the spectral decomposition of the kernel from that, I can, even a small sample and using the spectral decomposition, extrapolate what the other values. But then there was all of these learning the kernel part of machine learning, the 2000 five approximately, where you would take linear combinations of known kernels. So you have parametric parameters. So, what I'm imagining is that. So, what I'm imagining is that there could be ways of constraining the set of graphons in ways that allow you to get a much structured, I don't know, initialization is the right word here. Yeah, so that you're essentially learning. Yeah. Okay. But with all these limitations, this is how we're constructing our graph on. In our graphon approximation. And then, so what do we do next? So, what I said is that we, so we have that, we have that, this graph on approximation. Let's assume that our graph is large, so it's pretty fine. We don't want to compute an uniqueness set. We don't want to compute the eigenvectors of this n by n induce graph, right? Because then we haven't gained anything. So, what we're going to do in practice. So, what we're going to do in practice is so we have our fine grid. What we're going to do is we're going to integrate the graph on in kind of like coarser, a coarser, at a coarser resolution. Okay, so that we're going to get an M-dimensional object where M is smaller. Where m is smaller than n. And then we're going to compute the eigenvectors of the Laplacian of that smaller object. And that, you know, by doing Gaussian elimination, that then gives us which of these coarser intervals are uniqueness sets. Okay. And then once we have that, we have our graph on uniqueness sets, and we can sample from them to sample from graph uniqueness sets. To sample from graph uniqueness sets because we have shown convergence of graph uniqueness sets to graphon uniqueness sets. So the third step, right, mapping graphon uniqueness sets to graph uniqueness sets follows from convergence. One direction have convergence and the other have. Okay? So then this is the algorithm that we use in our numerical experiments. Our numerical experiments. So, full disclosure: we had two sets of experiments in the paper. One of them is not here. So, one, in one of them, we were simply instead of sampling our graphs at random and training the GNN on the smaller graph and transferring to the large graph to see if it worked well, we were instead sampling using this procedure, right, by assuming that the important, the relevant information was in the top k eigenvectors where k was 10 or something like that. There, we didn't see a very big difference between doing. Very big difference between doing random sampling or optimal sampling. So, results were comparable. But where we saw a bit of an improvement was in the context of those graph positional encodings that I mentioned earlier. So people who use GNNs to do graph classification, right? So your graphs are your samples, and you're trying to, say, classify a molecule according to its chemical properties or something. Or something. They will sometimes use these graph positional encodings, which is just calculating the eigenvector of the Laplace and of the graph and using that as their input signal. And the reason why they do that is because eigenvectors give you global information that makes it easier to classify graphs into different classes because these graph level problems depend on global information. So in this case, it kind of makes sense to use our framework because what are we trying to do? We're trying to sample nodes so as to preserve To preserve eigenvectors in a way, right? Or signals that are in some subspace spanned by some eigenvectors. So here we consider this data set, which is this malnet tiny data set. It's a data set of graphs corresponding to, I think, different function calls, and you have to classify them into like a malicious one or not a malicious one. And we compared four models. So in one of Four models. So, in one of them, we trained the GNNs without these graph positional encodings. In the other one, we used positional encodings calculated on the full graph. So, we calculated the eigenvector on the full graph and fed it at the input of the GNN. And then we contrasted that with computing the eigenvectors on randomly sampled graphs, subgraphs, and these graph on optimally sampled subgraphs. And the way in which we mapped. And the way in which we mapped the eigenvectors from the small graphs to the large graphs was by zero padding. So we zero pad the eigenvectors at all of the nodes that aren't in the sampling set. And there we do see some improvement in performance of doing these optimal sampling versus random sub-sampling of the nodes. This is over 10 realizations, I think. The results are really bad. I think. Bad. I think actually, this is not binary classification, otherwise, this would be very bad. But if I'm not mistaken, this is a difficult problem. Like in the leaderboard, people don't get very good results. I should have included a baseline here to be sure. Okay, so in conclusion, yeah. Can you go back to the previous? I'm trying to understand. I'm trying to understand the concept of graph positional encoding. I think you said that it's an encoding of the features of a node. It's so they are just, you compute the eigenvector de la Pausin, and that is your input signal to the GNN. Now, the concept of positional encoding typically refers to mapping the location, like a pixel in an image. Like a pixel in an image to do some vector encoding, and that is distinct from, say, the intensity or the colors of the patch around. So, what, why positional encoding is the right name? I don't know. This is the name that they gave it. But for me, like the intuition is imagine that you have an SVM graph and you're using the second eigenvector of the Laplacian as the graph position encoding. Then that essentially gives you the community assignments. The community assignments, right, for two community SVM. So, I guess it's like positioning it on the graph. But I tend to think maybe a little bit backwards on this relative to this. Specifically, in the transformers community, if you want to call it that way, people either take a known location and map it to an encoding, or there is work where you also learn the positional encoding. The positional encoding. Right. And so, going back to the comment earlier about the fact that when you are given a graph, you don't know the coordinates of the vertices, it occurred to me that one could use a learned positional encoding during training to actually figure out what the coordinates are. Absolutely. And that the values you have actually are the features on it, but not the positional. Yeah, there's work on this. So there's people who. Work on this. So, there's people who learn, so they learned from the eigenvectors, the positional encodings. And then there's like all sorts of invariants of equivalence, but that's more James and maybe Wilson and Joso's kind of area. But yeah, they consider these position encodings that are invariant to sign flips that you stay along the same direction and also to subspace coordinate changes. Yeah, okay. So, just to conclude, here we're we developed a framework for sampling from large graphs by sampling from graphons, right, provided that we have some additional information about the signals that we're trying to sample. And we adapted that into an algorithm for sampling from larger graphs at a bit of a lower cost, which works well when you know, I mean, as prescribed, right? I mean, as prescribed, right? Like when you know where your signals come from. That's it. Thank you. Yeah. So I really like your talk. I especially enjoy connections with that Scheivinger et al. paper. I really love that paper. And I've been thinking about how to use it for networks for a little bit, but I've been. Bits. But I've been thinking about it from the point of view of trying to see if their parameters are really the right parameters to be using. Because in some sense, I think their results are loose for what they could be. Because they're essentially saying if you have, they're giving sufficient conditions. And not that I would want necessary conditions, somehow less. Less strong, sufficient conditions, maybe. And so I was wondering if you have any thoughts on kind of the optimality of your sampling set results as a function of their three parameters that they introduce in that paper. So would I be right to interpret your question as kind of like a rate of convergence or something? Possibly, but maybe just if there are other parameters that you could be using that kind of generalize their. Using that kind of generalize their parameters? I haven't thought about that, but that's a very good question. And I'll be happy to talk more about this. Yeah, because I also really enjoyed that paper and the connection with graphs, signal stamps. Kind of an indirect question, so I apologize, graphons. I don't know very much about graphons at all, but I think this talk is very interesting. Sometimes when I Is very interesting. Sometimes when I think about the Schiebinger paper or about graph Laplacians and things like that, I like to think of these results of Misha Belkin from long ago about the graph Laplacian associated with samples from a manifold or a nice measure on a nice manifold converging to the loss operator on that manifold in a certain sense. As the number of samples goes to infinity, the bandwidth is scaled correctly. Is there a correspondence between this Graphon theory and the theory you This graphon theory and the theory you've developed? Those types of results for manifolds? There is in the sense that manifolds, you could also look at them as graph limits, right, of limits of geometric graphs. And the Laplace Beltrami operator is also kind of like this well-behaved operator. It's no longer compact, but it still has this countable spectrum, right? So you could, I think compactness is kind of important here, but. Compactness is kind of important here, but I guess you could develop similar results. Those transferability results that I mentioned earlier, there's actually something that I worked with that developed them for graphs converging to manifold limits. Her name is Jiang Wong. Any other questions? Questions online? It's more of a crazy suggestion. Maybe I'm wrong, but in thinking about what could be a healer experiment that I think would be very impactful, is take images. There is a graph, there are many graphs like many use of the fixing. And take any task. I think classification at this point is maybe boring, but take classification as an example. If instead of having to classify very high resolution, I could do it from a very down sample version to get the same accuracy. That would have a lot of implications that training. And so the question would be, you know, the set view would be, well, what subset of pixels do I need to select from the image to? Do I need to select from the image to do a very good presentation? But I'm asking because I don't know much about images, but do people try to just use like the Fourier transform, like NyQuist for that? And how well does that work? No, I think generally speaking, there's been very little work and classification from just random samples. What people do primarily is just down sample, just standard down sample. I see. I see. The other approach, and this was maybe masked out of encoders, is you randomly subsample and you train networks on sub-sample images. But this approach of carefully selecting with generalization guarantees and being able to correlate theorems with practical performance is what I'm trying to do. You don't have generalization guarantees here, though. We're kind of assuming that we. Here, though. We're kind of assuming that we have a good enough model on the small graph, but there's no generalization. Agita works analysis. Okay, thank you. Thank you very much.