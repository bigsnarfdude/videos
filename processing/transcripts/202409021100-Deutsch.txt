And I'm excited to hear your thoughts and comments on it at the end. So, I'm going to start with the roadmap: what are we going to do together in the last 30 minutes? So, I'm going to set a scene and tell you about this model called Hawk's processes and why I think they are actually quite cool. And then we're going to come to the problem that, okay, they're a cool model, but it turns out that they can be too restrictive. And therefore, I'm going to present a solution for a particular. Present a solution for a particular type of data, namely a group chat data, where we need an extended model that we've called the ancestor hogs model. And we'll look at that at a real data example as well. So that's broadly the roadmap of what we're going to do together in the next 30 minutes. So let's start at the start. Hawk's processes, they are a type of point process. So they are used when the data that we have are the recordings of event times. So whenever an event happens, we record. An event happens, we record one entry in our data set. So our data are the event times t1, t2, up to tn. And a point process is usually modeled with an intensity function lambda, lambda t at time t. This is a continuous function across the time t. And when lambda t is larger, lambda of t is larger, we expect more events to happen. That's a classic point process model. That's a classic point process model. Hawk's processes are a type of point processes, point processes that are used when events occur in clusters or bursts. So when one event that appears makes it more likely for other events to follow quite soon after that. So if we just think about what we would like the intensity function to do to capture that, it is the fact that whenever an event happens, like you see T1 on the bottom here, we want to see a spike in the intensity. We want to see a spike in the intensity function because that means that another event soon after becomes more likely. So, that is the general idea of what we'd like a Hawk's process to be and to do. So, now let's put that into the formula of the intensity function, how we can specify for that to happen. And we do this through making the intensity function conditional. Conditional at time t at everything that happened previous to time t. So, this curly h of t is So, this curly h of t is the history of the process up to time t. And the intensity function is a sum of two bits. First of all, we have the background rate. This mu of t is the quote-unquote unexciting part. This is kind of what happens outside of this self-exciting behavior. This is maybe the things that are not part of this bursting behavior. And the second part, this is where it gets excited. And the second part, this is where it gets exciting, and this is where the self-excitement comes from. You can see that at time t, every event that happened previous to time t gets to contribute. And how much this contribution is, is governed by two parts. The first one is the influence magnitude k. We want to restrict this to being less than one, otherwise, we've got non-realistic behavior. We'll get to that in a second, how we can interpret this elsewhere. And this got Elsewhere, and this governs how much is added to the intensity function by one event. And then the influence kernel G here, this governs how this contribution from one event is spread out. Quite often, this is continuously decreasing, such that events that are closer to the point that we're interested in contribute more. But in general, you can really frame this however you want it to be. I usually Be. I usually want this to integrate it to be positive, to be non-negative, and to integrate to one because then we can interpret k much more directly. So, this is all that we ask from this point process. And to give you a little bit more intuition of what these parameters mean and what they do to an intensity function, I think this might be quite helpful. The mu is just the constant thing that works in the background, as you can see here on the bottom. And whenever an event happens, those are on the dots on the bottom panel of this. On the bottom panel of this plot, we see an increase in intensity function. And how much is added to the intensity function over the whole period is governed by k, as is the shaded area indicating here. And how quick is intensity decreasing, that is governed by our kernel. In this case, I've chosen an exponential kernel, quite a popular choice also throughout this talk. And this would then have one parameter, beta, that tells you how quick this is decreasing or not. So this is, I think, a good representations of what. I think a good representation of what the three types of parameters that we have in a classic Hawkes process do. What I've talked about so far is a Hawkes process in one dimension. We can very easily extend this to multiple dimensions. Now having M dimensions where each data point is not just one timestamp, but we also have a dimension D added to it, and the D is in one to M. The d is in one to m. So now at time t, the conditional intensity function in dimension m is again conditional on what happened previous to time t, but contributions from all dimensions now come in. So we've got again background rate, mu m of t, this can be specific to the dimension if desired, and then we sum over all dimensions and all events that happened previous to t, and each of them gets to contribute a And each of them gets to contribute a little bit at time t. And how much is now again covered by specific parameters that tell us from which dimension, into which dimension we influence. So you can see the k has a subscript j to m. It might be a little bit cumbersome for a lot of equations, but here I thought it was really worth highlighting where the event happens in J and the contribution to which dimension is recorded, M. And you can then imagine. At n, and you can then imagine that we can easily arrange all those kj to m parameters in a matrix k. So, the bold case k, that is the parameter that contains all of these entries. Again, we have a kernel, and I've just specified that there are some parameters that belong to the kernel, theta of g, and some parameters that belong to the background rate mu. And there you have it, that is the n-dimensional classic Hawk's process, and that is quite a Classic Hawkes process, and that is quite a flexible model and can capture the self-exciting behavior. There is one interesting thing about the one and n-dimensional Hawk's processes that we're going to need further on, and that is the branching interpretation that we can have of a Hawk's process. When we imagine a Hawks process happening, we can interpret this as events, immigrant events happening from the background rate. So, in this little drawing here, those would be the ones with the. Little drawing here, those would be the ones with the star: so T1, T6, T9. And these then have offsprings. So these are then events that come from the contributions of their parent events. So T2 and T3 come from the process that originated or that come from the additions to the intensity that come from the event T1. Naturally, this is not something that we observe when we get handed a bunch of data. When we get handed a bunch of data. But nevertheless, this is an interpretation that can help us understand the Hawkes process better. And it will be very, very useful for computational reasons later on. So we have immigrant events, those come from the background rate, and we have triggered events. Those come from immigrant events or other triggered events. So as you can see, those are the ones that don't come from the background rate, but from all the additional stuff that we add to the intensity. We add to the intensity. So I think that's a really nice way of thinking about it. As I said, we don't know it, but we do have a way of estimating it, and it will help us greatly when it comes to computational issues. All right, so the Hawks process, hopefully I have convinced you, is quite an interesting and flexible thing to use. But now we run into a problem. This classic structure can be too restrictive. It turns out that the Hulk process can be used in a huge variety. In a huge variety of different applications, and that is what also makes it so exciting to me. So, it has been used for earthquake modeling, it has been used for Twitter or X communication. So, wherever you can see the process where one event makes another event more likely. Two of the examples where HOG's processes, the classical ones, have been already very successful, are email conversations, which is one-to-one conversation. Which is one-to-one conversations, or Twitter or X, where it's one-to-many. So, in the email sense, if I send you an email, it becomes slightly more likely that you're going to send an email as well, because you're going to answer me. And you can already see the branching structure trending through here. Very similar on Twitter. If I post something on Twitter or on X and you follow me, it becomes maybe slightly more likely that you're going to post as well, namely a retweet of my tweet if it was in IQ. So these are already two scenarios in a messaging world. Two scenarios in a messaging world where Hawks processes have been very successful, and there's great papers out there for these examples. But what I wanted to do is to use Hawk's processes for group chats. So those are few to few conversations. They are private and synchronized online conversations with a small number of participants, more than two. And what is special about them is that everyone has equal opportunity to participate. So think about the chat you have with your family, with your colleagues, with your friends. These types of conversations. Your friends, these types of conversations are the ones I want to think about. It turns out that while emails and maybe Twitter do have some examples out there in the literature of how they can be modeled, group chat conversations have hardly anything going at all. I have found one paper that is specifically targeted to group chats. So if you do come across any, please send them through. But I think it's a very interesting type of conversations because we do have them online and it's worth thinking about them. So if we translate them. So, if we translate them into the point process world, that means that each participant is represented by a dimension. And when that person sends a message, so if person A sends a message, we record an event in dimension one. So we've got a multivariate Hawks process where each participant in a group chat is represented by a dimension. And when they send a message, that is recorded an event. And that might make it slightly more likely for other people to send a message as well in the group chat, i.e. reply to that first message. ID replied to that first message. So I think the idea already seems quite natural that something like the Hawks process could be beneficial here. But let's think this through in a little bit more detail and I hope you'll see where the issue comes that we're going to run into if we use a normal Hawks process here. So let's think of a scenario one. Let's say I'm going to send a message to the group chat that we're all in, or maybe it's a group chat for this conference who said, and I say, who would like to come to the pub? To the pub. This is an immigrant event because it's not really a reply to anything else. So, this means that this increases the intensity of a person C, maybe that's U, according to K, A to C. A sent a message, C is the one affected by the message, so this is according to K A to C. Brilliant, we're happy with that. Now, let's look at a second scenario. Now, someone else, maybe a good mate of ours, sends this message. Who would like to come to the pub? Who would like to come to the pub? This is again an immigrant event and it influences person C according to K, B to C, because it came from B and we talk about C. And now I'm also good friends with B. I'm going to reply, I am in. And this is now a triggered event because it is quite directly a reply to B. But this message, I am in, also increases the intensity of C according to KA to C. And this is where we run into a problem because you can't tell me. Into a problem because you can't tell me that you are equally likely to reply to the cop, who would like to come to the pub, and I am in, despite both of them coming from me. And this is where we run into an issue, and this is where we need to extend classic Hawks process slightly to accommodate for this. So, there comes the solution, and this is that we need an extended model called the Ancestor Hawks model. And I want to stress that this is not just for That this is not just for group chats. We actually ran into this issue of different types of messages, triggered and immigrant events wanting to influence slightly differently, and a whole variety of applications. But this is the most prominent one that we chose to display here. All right, so let's get into the solution. I think it's hopefully quite obvious already what we're going to do. We're going to align different influences for immigrants and triggered events. Okay, I think that's. Triggered events. Okay, I think that was hopefully already clear from the little example I gave you. Issue is that when we see data, we don't know which one are immigrants and triggered events, but we're going to work with that nevertheless. So we're going to have, first of all, to introduce a branching variable that just tells us if an event is an immigrant event, then it's going to have a branching variable of value of zero, or if it is an offspring of an event yk, then it's going to have that k as its. As its value for the branching variable. So, this allows us to gauge whether an event is an immigrant or triggered event, but as I said, that is unknown. And what we're now going to do is that in our intensity function, again, it has the first part of the background event, and it has the, we're now going to allow contributions to come from immigrant and triggered events. So, again, it's a conditional intensity function on everything that happened previous to time t. That happened previous to time t, but now suddenly there are three parts. First one, you already know what's the background rate. The second one, now, is also conditional in the fact that the branching variable for that very event is zero. So now we say that K and the kernel G is only reserved for immigrant events. And then all other triggered events, so those whose branching variables are larger than zero, so are one, two, three, and so on. So, our one, two, three, and so on. Those are now governed by a separate set of parameters and a separate kernel. So, these are now L and H. So, we have added now two types of parameters to our parameter roundup here, which are L, which is the equivalent of K, but just for triggered events, and all the parameters that belong to the kernel H as well. These again, these govern the latter govern how the influence of the L parameters is spread. Of the L parameter is spread out, but again, only for triggered events. So we have increased the number of parameters that we had quite substantially. And I think we'll have to justify that later on, but this gives us the flexibility to do what we want. So let's come back to our example. What happens now in the ancestor hawks case? Scenario one, as before, who would like to come to pub tonight? Nothing has changed. If I send that message, if I am A and you are C, it's still going to increase the intensity. It's still going to increase the intensity of view according to KA to C. Okay, nothing has changed here, and we're quite happy about that because that was something we were already quite comfortable with. But what happens now in scenario two? E now sends who would like to come to the pub tonight, and it's again K, E to C what happens to the intensity by in-person C. So this is an immigrant event. So far, so good and identical to what we had seen before. But now comes the change. But now comes the change. If I now write, I am in, this is a triggered event, and it increases the intensity of C according to LA to C. And this now gives us the freedom to have events that are substantially different, because one of them is an immigrant event and one of them is a triggered event, who have different implications for the intensity of the others. And this is a big change, and it gives us the flexibility that we wanted. What about computation? We've added a lot of parameters. Luckily, we do have a few conjugate priors available. And we have one thing that helps us out a lot, which is the branching structure, because it turns out that when we have a branching structure, when we condition on the branching structure, which is unknown, lots of things become independent. And when we have an estimate for all the parameters, we can estimate the branching structure. And I guess you can already. Structure and I guess you can already see where this is going. This is a very good setup for GIB sampling. We can do metropolis hastings if we do a lot of conditioning. And it turns out that lots and lots of these things are independent of each other. And we're quite happy about that because it turns out that sampling from these hawks bros this type of data or posteriors rather, be it ancestor hawks or classic hawks, is quite different, quite difficult when we don't have the branching structure because lots of things can be. Structure because lots of things can be correlated, and that's very, very difficult. But a branching structure helps out here big time. So, that is, I think, quite reassuring. And when I did this for the first time, I was actually quite surprised how well it worked. And then I checked for a lot of bugs and it still worked, which I was quite happy about. So, this is now the time where we come to the real data example. Now, I feel I need to justify to you why we have gone through all this trouble of coming up with a completely new model of introducing a lot of parameters. A lot of parameters, is this really worth it? And spoiler alert, I think it is. Otherwise, I wouldn't be given this talk anyway. So I have collected data from a group chat setting. It turns out, as I said before, that group chats are not that well researched, and there are no publicly available group chat data out there until now, because I collected group chat data from me and my PhD friends, and they all gave me ethical consent to publish this data. And we've gone through the whole ethical consent. This data, and we've gone through the whole ethics approval process. So, this data is now publicly available to anyone who wants to research this type of data. We've got about, I think, a year and a half or up to almost two years worth of data up there, which is really quite exciting. For the little bit of analysis that I want to show you now, I have selected one year of data. These are the messages from 2021. These are about two. One. These are about 2.7,000 messages. And while there were nine people in the group chat, I selected only seven of them because you can see number one and number four here. They sent very, very few messages and their posterior distributions all just fed back to prior because there was not enough to work there for them. I think if we had some hierarchical approaches, it would be a lot easier. But for display reasons, I chose to exclude them. Chose to exclude them, but I excluded less than two percent of all messages through excluding them, and that's perfectly fine. I thought, and you could easily rerun this on much more, on longer data and maybe with all participants. So, what we see here on the left is a plot of when messages were sent throughout the year 2021. What is interesting here is that there are sometimes periods when none of us seem to have messaged a lot. I'm not quite sure what these correspond to do, but if Quite sure what these corresponded to, but it might have been like holidays or something, or it might have actually corresponded to weekends where none of us were in the same city or something like that. So, this is just to show that this is real data that I've collected and has some anomalies like gaps in the data, but these are perfectly fine. All right, now we're gonna look through the model, fit this, and then see if it was really worth going through all the trouble of the ancestor hogs. The intensity function that we're gonna use, you've already function that we're going to use, you've already encountered. Maybe I should specify what the background rate is. This is, I've chosen a step function that is driven by hour of the day. We were PhD students at the time, but still we kept to rather normal working hours. So there were not that many messages sent in the middle of the night and the background rate reflects that. This is something that I've estimated outside of the model. And then each person gets their own multiplier to this background rate. Multiplier to this background rate. We have the contributions from the immigrant messages, so those would be the ones that start a new cascade of messages, and then we have the contributions from the triggered events. As the kernels, the inference kernels, I chose exponentials and I restricted the number of parameters such that all diagonal influences, so what happens to my intensity in response to the message from me, is the same. So this just controls the Is the same. So this just controls the shape. And similarly, that the shape of the response to a message from someone else is the same again. This is mainly done due to computational reasons and to limit the number of parameters somehow, but I think this would really deserve maybe a hierarchical approach, something a little bit more principled. We found this to work okay, but I think this is where we could really improve things as well. So this is the model that we're going to fit. So, this is the model that we kind of fit. Prior choices, I mean, briefly, we put a gamma prior on the multiplier for the background rate that's conjugate, so we're very happy with that. We put uniform priors on K and on all values of K and L. This is mainly to do with the fact that in the traditional Hawk's process, K has to be smaller than one, otherwise we'd have more than one offspring. And we just wanted to err on the side of caution here. And then we've chosen gamma priors for beta and For beta and the gamma, which controls the shape of the influence of the triggered events. Luckily, none of this was sensitive to what none of this was sensitive to the exact choice of numbers. For the latter, we chose them such that the influence would be roughly on the type of scale we'd imagine messages to influence. Maybe that a message doesn't influence weeks and weeks onwards, but it also doesn't stop. Words, but it also doesn't stop influencing 20 seconds after it was sent, and this kind of gives us this nice move around. So, these were the prior choices, but not a lot of sensitivity to these priors, as you will see in a second. So, we have to thank you. We'll now look at the posterior distributions, and I'll walk you through each of those plots separately. So, we'll start on the top left. This is K7 to M. This is what happens to all the other dimensions, to all the other participants. Dimensions to all the other participants when number seven, when the person seven sends a message. It turns out that on average, maybe people send 0.2 messages. There's one person who's just not faced at all and is not interested in replying to seven whatsoever. The dotted line here is when M equals seven. So when seven equals seven, this is the same, the same response. And these are for immigrant messages sent by person seven. Then we see what happens to the other dimension. What happens to the other dimensions when seven sends a triggered message? That is, there would be a response to someone else. This is the top right one. You can see that in general, this is much lower. And this makes sense. This is very much in line with what we had before. This is the fact that people reply much more to the original message rather than to a reply. And if we now look at the bottom, this is now KM to 7. What happens? Who does 7? What happens? Who does seven reply to? And it looks like seven is much less keen on replying to immigrant messages than they are to sending messages. But there seems to be one person who they are maybe more keen to answer, or maybe that is just closer to the prior. We're not quite sure about that. And then again, seven is also much less keen on answering non-immigrant messages, so triggered messages by anyone else. So this is the moral of the story. So, this is the moral of the story of the posterior. And now, as a second to final slide, is this really worth it? I've done a posterior p-value evaluation where I compared the model of the ancestor hawks to the classic hawks. And it turns out that on the log number of events, both of them do very well. There is one summary statistics, median divided by the mean inter-event times, which both of them do horrifically badly. But I think this is very much driven by the large gaps in the data that we. Driven by the large gaps in the data that we had witnessed, that neither of them can accommodate because the background rate doesn't do that. And then, and this is the crucial part for me: there are two summary statistics, namely a Ripley scale, so how many events happen close to an event, and the autocorrelation function of the inter-event times, where the ancestor hogs does much, much, much better than the classic hogs and justifies why we want to do ancestor hogs in the first place, because only ancestor hogs can capture this short dynamic. Capture this short dynamic of what happens immediately after an event. So, to summarize, answer to HALT provides the necessary flexibility to capture the intricate messaging patterns. We've seen in the summary statistics that there are some local patterns that we've seen in the bottom two summary statistics that the classic hogs can't capture, and therefore we need ancestor hogs. And I think this could be really, really useful to prioritize phone notifications because I'm sure we've all come to a group chat where they've All come to a group chat where there have been loads and loads of messages going on, and you kind of want to see only the messages that maybe start the conversations, people who you're closest with. And this can be done without having to process the content of the message using the ancestor hooks. So with that, thank you very much for your attention. And I'm now really looking forward to your comments and questions. Hi, Bella. Thanks for joining us. So I was wondering, you didn't show like simulated data results. So I was wondering if with your group chat example, you can actually check if your results make sense. So is like number seven very friendly with that person that he replies to or she replies to? Like, does it make sense with what you've seen? I guess you know the people there. So So, I'm not able to. So, I actually had to anonymize all the data. So, I don't know who was who. But I know from, I mean, from group chats that we've all been in, that there are some people who maybe suggest things and people who are not that interested in starting a conversation, but very good at contributing to conversations. So, while I can't say if this is the case for exactly the person number seven, it does make a lot of sense. Make a lot of sense in a general group set group chat scenario to me. Okay, and like kind of related to this, so can you model? So you might have some like prior information on how people, like for example, reply to messages. Like I'm the type of person that maybe like replies pretty quickly. My husband replies after a week. So can you like put that into your prior? Is that something that you've tried to do? So I guess like that. To do, so I guess like that makes it complicated. Like, if somebody replies after one week, like it makes it more difficult to understand if it's like, well, so I think this would be about how you choose your kernel. If you choose a car, if you have a more flexible kernel that is maybe not driven by one parameter, like the exponential that I have, but a two-parameter kernel, or maybe something that has a little bit of an offset. I think that could, if you have this information, I'm not quite sure. If you have this information, I'm not quite sure if I would put it in the prior, but probably the model choice might be a good place to do that. And then, so you have to start at the model choice, and then maybe you can emphasize that in the prior. This setup doesn't allow it as I have it, but if you choose a slightly different kernel, I don't see why it shouldn't work. Yeah. Thank you. Thank you for your comment. Oh, I think I have trouble hearing you. I'm so sorry. Can you hear me now? Can you hear me now? Yes, perfect. Much better. I'm not sure it's worth hearing me. Francesca's comment seems to point out introducing subject-specific covariates. Yeah, covariates are something that are used in Hawk's processes and in general Hawk's processes, and also in, and therefore they can absolutely be used in Answer to Hawk's processes as well. There's a variety of ways you can implement them. And I think this would be just to draw upon. And I think this would be just to draw upon the literature that has already been done. It's not a question of reinventing the wheel to use covariates and hox processes, yes. Thanks for the talk, Isabella. I'm just wondering if it's possible to characterize incidents either immigrant events versus about the other one, but is it possible to characterize some events as requiring a response and some Requiring a response and some that don't require response. So, for example, something like sounds good doesn't really require, or K or a thumbs up doesn't require response. But a question or something that requires engagement would potentially be modeled. Yeah, I love this idea. We actually played around with reducing L, so their contributions from triggered events to From triggered events to something to zero totally, which is similar to, I think, what you were maybe suggesting. I guess then it would be more a question of categorizing, like of having different categories of events. And that is, that can be done as well. And there is some parallels to existing Hawk process literature, but none of them fit the current literature directly. So I guess it would be just. Literature directly, so I guess it would be just about introducing then a third category, maybe those ones that don't have any influence at all. I think this could be added, and I can imagine that a lot of the nice independencies would still hold. Yes, this is a great presentation. I've always wondered who is the other person. Always wondered who is the other person in the world who works on groupcasts using Hughes. Turns out it's you. So I'm very excited about this. I have a question for you about, like, is there any potential identifiability issues between the K entries and the L entries if you do not observe any responding structures between the messages, right? So if C replies to something that probably originated by person B, but also replies. Person B, but also replied by person A, then you probably don't really know if C is inspired by A or B, right? But then you have to learn the K entries and L entries depending on you knowing exactly whom C is behind. So I'm wondering if there's any identifiability concerns because of that. Yeah, so we've been grappling with identifiability in general with Hawk's process a little bit. In general, with Hawke's process, a little bit. I'm pretty sure that it should be a right, but that is not enough to answer this question with there's no identifiability issues. What I've observed is in lots of simulation studies, once you have enough data and there is enough of a mix of messages being sent, we easily recovered all parameters. Easily recovered all parameters with much less data than what I have used. Okay, yeah, sounds good. You can potentially do a little bit of language processing using context content itself to further come up with identifiability issues. Yeah, and that ties back to the covariates in the end, right? So I agree that the covariates would be a good next stop. Absolutely. Absolutely. Thank you. Thank you very much. There's not any more questions. Maybe we can