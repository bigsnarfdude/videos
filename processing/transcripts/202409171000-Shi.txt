Nice introduction, and thank you very much for inviting me. It's such a nice workshop, and I really enjoy it so far. So, as you can see, today I'm going to talk about something about the alignment of image alignment. So, it's a very specific topic. So, the problem is very straightforward. So, we just want to, so given two 2D functions, okay, f and g. Okay. F and G, assume. Ah. So it's not uh so I won't use this way. So okay, so assume that you have the two 2D functions, f and g, okay, and and and you want to find the translation vector v, the 2d. vector V, the 2D, two-dimensional vector, and the corresponding rotation angle theta assume that they're all in 2D space. And you want to find such translation and rotation such that these two 2D functions can be aligned together. So their distance is minimized. So any question about this problem? Okay. So at the bottom of this page is just some simple illustration. You can have some location between the two images and translation or even the superposition of the two. Even the superposition of the two operations, and you want to find the correct transformation so that these two images perfectly light, like I said. Okay, so there are many applications of image alignments. For example, for imaging data, medical imaging, you may have data from different Data from different machine devices like MRI, CD scan, and maybe different resources. And you want to somehow align them using, for example, fine transformation or whatever. And similar things can happen in astronomy where you have images from different telescopes that you want to align the images. And our original motivation is from the education in structural biology, especially Quio EM. And in CrawEM, there is a very critical step called 2D class averaging, where you want, you have many, many noisy images, you want to ally them. Of course of all, you want to classify them so that the images in each cluster are from similar viewing directions. And within each cluster, you want to align these images together. But this alignment procedure is typically the computational bottleneck of Prior EM pipeline. And I mean, it highlights. I mean, it highly depends on the performance also high depends on the noise level and heterogeneity of the molecule. Because the molecule is not like a rigid body, it can have some deformation. Okay, so well, this is the motivation of this work. And I just to give you some definition. People behind if they are people beyond the translation of what they uh no no no so so so so we are still doing the rigid alignment but uh we hope this rigid alignment is robust to the deformation so like you have a two images but they they are deformed version of each other but you can still align them you know if you have some deformation then your gradient distance is not very fast so I don't think that's quite something. Yeah, yeah, exactly. Yeah. Yeah, that's a more computationally advanced account. So let's focus on the simpler problem. Okay, so because we consider both rotation and translation, what people really do is like for each translation, they kind of do the boost bus search and for each particular translation, they do this rotation alignment. They try many, many different translations. And each time you do. Translations, and each time you do this rotation alignment. And a lot of ways that you can alternate between the translation and the rotation. So, in any case, this stop problem is super, super important. Okay, so let's focus on it. So, so because something special about this rotation alignment is that where we lack in the 2D space, the 2D rotation is only parameterized by just one parameter. So it's very, so sometimes you don't even need. So, sometimes you don't even need an organization method. You can just do a proof of search. So, if you just use this naive way of computing, like you try all different possible rotations, maybe 360 degrees. And each time you compare, you evaluate the distance of these two functions, maybe in discrete space, like images, you compute some distance between the two matrices. So, every time, assume that here I'm abusing this notation, assuming that. Here I'm abusing this quotation, assuming that they are no longer like continuous Guildy functions, but they are in bio matrices. And then the proof of search will take you, the naive computation will take you about a few iteration operations. But this rotation, if you represent an image in a polar grid and do the, and you can do this really fast using the grid transform. And the confusion complexity will reduce to the capillance now allow because it's just a cross-correlation somehow between the two images, if you consider polar grid. Well, the rotation alignment in Euclidean instance is really fast, but why are we still studying this? So, the thing is that it has some bad property. So, this here is. Property okay, so this here is one example. So, if you have these two images, they're uh just two Gaussian blocks, the only difference is that okay, they're rotated inversion of charges, so the rotation is 180 degree. And additionally, I slightly perturbed one of them towards the center. Okay, I perturbed the right image, this is the one towards the center. So they have different videos towards the center, I mean. And then, if you assume that you want to align these two images in the leading is. Allow these two images in the Euclidean distance. By Euclidean distance, that means that if you consider them as the 2D functions, then it's just the L2 distance. If the matrix is probably slow. So if you do this, you try different rotations and compute distance, then the cooling distance is always a constant. Or it's almost a constant, because they are Gaussian flux, so they still have non-zero pixel values outside, but it's almost a constant. But it's almost a constant, but what about the Wasser distance? Okay, so one nice property of Washington, the Wasserston distance is that it reflects somewhat the geometric information such that the Wasson distance between these two images is exactly the distance between the center of the two Gauss. Center of the two Gaussian. So, which means if you want to align these two images in the wasting distance, the result of alignment will be the rotation such that the center of the two Gaussian blocks are closest. It's the closest. So that's kind of the best we can do. And what's used as gives this nice property. A slightly more complicated illustration is like, okay, so this this illustration is. This illustration is somewhat connected to the CREM. For example, if you consider each molecule, protein molecule has two significant components like the head and the bottom. So we can model them like two Gaussian blocks. And we assume that molecule can have some deformation. So it can extend, it can shrink. So the distance between these two Gaussian golf can change. So if we want to align this two, So, if we want to align these two images and just look at take a look at the Euclidean instance and the Waste one, the only WAS one will give you correct global minimum. By correct, I mean that this global minimum corresponds to the correct rotation angle between the two images. Okay? So let's do some brief review of these previous works on image alignment. Works on image alignment, rotation alignment using different distances. So, actually, the majority of work in this field is all about Euclidean distance. So, one of the earlier work is this FMP-based method for aligning rotations, but they also do translation. So later in 2020, they're faster. In 2020, there are faster methods for doing the location search. But if you don't care about translation, then the complexity for aligning rotations is still outlined. But what's the computational complexity for Washington? Here is just a rough estimation, like it's at least n to power seven. Assume that the images are like have n spell pixels and Pixels and if you use just use the linear programming to solve the this assignment problem by the automo transport map. And the problems every time you rotate the image, you need to recompute the last piece. That's the one so you have one additional out there. So there are some faster approximation methods. For example, you can use Stinghone algorithm to single algorithm to uh to to to uh estimate the uh regularized the entropy regularized at what distance and then the the the outer becomes n to the power five and you can do uh further like you can even uh for the earth mover distance i mean the w1 distance uh you can use the wavelet basic method so so basically what it says is that uh you can do wavelet transform for your images and then the watching distance w1 uh Wasson distance, W1 distance and be approximated by approximated by the weighted L1 distance between the wavelet coefficient of two images. Okay, so then the wavelet transform becomes a total accurate. So it's every time you need to, every time you do the rotation, you need to apply the wavelet transform. So it's totally n cube tau. So can we do better? Because we can obviously see this gap, right? So we have a if the image is 100 by 100, then the If the image is 100 by 100, then the accuracy could be like 100 times slower, even with this approximated Washington. Okay? So the answer is yes. Yes, we can have another version of Washington, but with very fast computation. The idea is not new, right? So most of you, I assume you know the size of Washington. The idea is that even The idea is that even though the waste distance is hard to compute in general, if you consider only the 1D distributions, it has close conclusion. So, for example, if you have this two 1D distribution u and v, and the vast distance between these two 1D distributions is just that you first compute this, from this probability density functions, you compute this CDF, right? CDF, and then you take the inverse function, and then you just take this. And then you just take this model, right? So, the Watson distance after doing this mapping becomes the Euclidian distance somehow in this way. So, I do the following. Okay, so then why not we just project our image into many, many lines with different angles, right? So, for example, picking any statement, you can draw a line that goes across the center of the image, and you can do the projection. You can do the projection, like you do the integral along this orthogonal direction, and you get from this 2D distribution, you got one distribution, especially you are doing a marginalization on this line. And you can do this for any theta. And then the volts and distance between these two images is just that you compute the vast distance along this between these lines, this 1D distribution, which is really fast, those conclusions. Which is really fast, close con solution, and you take the average, that means you take the integral along this on the units of the so this 1D projection, why you can build this, this has a name called the Redon Transform, right? So you're basically doing the REDAL transform image and computing really fast using the non-uniform Fourier transform and just by using the free slice. And just by using the pre-slice theory. So the computational complexity here is n-square login. All right, so let's summarize a little bit. So what we have done here, okay, so what we to compute this last class, what we need to do is that each image, each panel image, you'll do this line projection along certain direction. For example, this direction. Along certain directions. For example, this direction is around theta equal to zero. That means you project onto this horizontal line. But you'll do this many, many times for different thetas. And for each such line projection of these two images, you'll do this native stuff. And I'm not talking about the actual implementation because now we are in the discrete space. So the integral becomes this native. And to compute the inverse CDF, we need to be a little bit careful because we need to do some sort of linear calculation so that we're still in June grid after we do this transformation. Okay, so basically, what we have been talking about is that we have these two images, we do some sort of transformation. Two images, we do some sort of transformation. That's how we got these two rectangular matrices where each line, each vertical line, each column is a inverse CDF along some direction. So this axis is about different angles there. So then what I have said is that the washing distance, washing two distance, I mean, in this case, if I take p equal to two, between these two images, it's just the flow. two images it's just the probinus not between these two images and what what what what what else uh another nice property of what this is that you don't have to every time you rotate you don't have to recompute the the size of what this this projection the reason is that the rotation in the original space corresponds to the stately shape of columns in this So, if you want to minimize over these rotations, you actually minimize over the shift of columns, right? Take the shift of columns and compare. Every time you pick your L, maybe shift to the left as three columns, four columns, five columns, every time you compute this matrices and until you find the minimum. Well, naively computer. Well, it naively can be computed by in capital N cubed operations. But similar, like before, we can, this has a very, you can compute this in free space really fast. So, so idea is that, okay, so now assume that this is your F, okay, this is your G, F, the two matrices, and the Fi is the I toll of the matrix. Okay, so, so, so then. Okay, so so then if we expand uh expand this term, we got uh three terms. Okay, this term is nothing to do with that. This term is nothing to do with that, right? It appears here, but you, if you sum over i and this distinctly shift, it doesn't matter which L you pick. They're all the same, right? So for this two, you don't have to repeatedly compute them. So it's really bad. Compute them so it's really fast for this one. Uh, this is cross-correlation, and for each page j, you can compute this using the fast-free transform. And totally, it can be computed in capital instance login operations. Okay, so here's a summary of the like different methods. So, so. So, so, so, so, so, think that now, okay, we have an algorithm that we hope it works, and it has the computational complexity that matches the Euclidean-based algorithm. And indeed, it's really fast. Okay, so in our implementation, so the order indeed looks similar, it's not in log scale, but you can see it's similar up to a constant, it's almost similar. So, so even for the image of a Tucson by. So, even for the image of a 2,000 by 2,000, we can align them on a single CPU in five seconds. And so if we align 64x64 images 100 times, so this is uncomparison, the different algorithms. So our bias is almost similar to Euclidean system, Euclidean distance, and will transform maybe 100 times slower. Time store and original volume distance is not much store. And this is even for small images, for large images. I mean, these two methods won't stay up. So we talk about the noise property of this Watson distance, and indeed, it is very stable to some sort of performance. For example, we have shown that it's very stable to the shift. That it's very stable to shift. That means if you have two 2D functions, one and the shifted version by a vector B, and then the distance between, a slice watch and P, was distance between these two functions, exactly a constant, and only rely on P times the amount of strength. Okay. And you can compute this constant explicitly. And we don't put this profession in. We don't put this proposition in our name because we assume it's quite trivial. And the second more non-trivial result is about stability to the projection directions. So in this case, we focus more on the tomographic images, where you originally have a 3D function. You want to project that into many, many 2D functions. And assume that you have two projections. Two projections and the approaching directions only people by eye angle after and you hope that your distance will not change a lot between these two images, will not change a lot up to your implant range. Okay, and indeed, we have this quite nice concern. And in many previous works, with general information, this constant is one, but in this specific case, we can improve this too. For example, for two systems is around 0.7. Two system is around 0.7. But there's still some issue. Okay, let me explain this one. So assume that we have these two images, you can really say, okay, they look different, right? The two images. Then you do this transform. Okay, this transform where each column is the inverse CBM along some direction. Along some direction. But you can see that they are not less, much less broadly distinguished. The one reason is that, I mean, for each line, you're kind of doing a projection. You're stumbling over the pixel values along the orthogonal directions that kind of cause some blurry issue. So this averaging along the orthogonal direction is actually inflating the low frequency information. The low frequency information. So, this is a common issue for why you want to reconstruct from the low dimensional projection to recover the high-dimensional structure when you apply the inverse Redon transform. Remember, when you want to apply Readon transform, you have to apply one specific filter such that you can exactly recover the original structure. That filter is called RAM filter. Okay, a ramp filter has the Has this property, which means this free transform is just the value of that at that frequency. And the same thing happens here. Okay, so if we just convolve each projected PDF with this rank filter, we store that high-pass filter to sharpen each PDF. HPDL, then it becomes much, much more distinguishable. And still, it has nice property that it's robust deformation, but it just becomes more distinguishable along this angular direction. And indeed, if you consider that two images, assume that the rotation angle is 180 degree, the not good, not the original velocity distance. The original washing distance will identify the wrong angle, but our method, the filter one, actually identify the correct one. And similarly, we diseased on the MNIST digital for this number two is a tricky example because you know two, somehow it's upside-down version is similar to itself. There's some. There's some like there's some sort of difference between the head and the bottom. Things like this part is more has more curve, but this part is sharper. You know, this counter is sharper. So the question is that can we have the distance that somehow robusts to deformation, but also can identify this type of difference in the deformation. So this, it turns out that this filter specifically does. that this filters as was massive work much better much better even than the two was which is the this this this yellow color so so this problem this is that the the the alignments so for example if I look at this 20 so this means okay about maybe I looked at oh yeah 25 maybe for Euclidean instance only is that only uh 40% are within the uh after do alignment are within uh are uh 25 degree 40 percent are within 25 degree of the correct answer. Yeah, the reason is that we're applying the run filter. If you do the original waste, sorry, I should say the filter is nice wasted. If you do the original waste, it's somewhere here. Okay, it's just another man. So you may ask another question, like if you apply the high-pass filter to the image itself, and then apply the vast, what happens? The answer is it won't help with what the reason, but it helps you. Reason, but it helps here because the reason is that the flash waston is it has it has a projection along each direction. So it kind of over overemphasized the low pass, low frequency information. So you have to apply a high pass filter there, just like you need to apply that filter to to apply the inverse radium transfer. Until the rust of them pick this as my line. Oh, yeah, yeah, right, right. That's a good suggestion. So I think here we are just picking one image. We're not like the yeah, yeah, yeah, yeah, yeah, yeah. We can do this. Yeah. Right, right. Yeah. Yeah, yeah. That if they're the line and well, that's what they yeah, that's what the person discusses. Yeah, if they exactly have oh no, how do you do the rotation? So, it depends on the task you want to do. So, if you just want to align this image, it's not really aligned to a certain image, like a main image or whatever. Yeah, but if, for example, in CrowEM, you're not really aligned to the mean volume or main image, you're aligned to any pair of like you have 100 images and you're you're you're aligned like 10 f uh five thousand images, I mean each. I think I think you have to write it first. Yeah. Oh, yeah, that's another issue. Like, if you don't align them, like, this, the parachute doesn't make sense, actually. I did some tests, actually. Oh. Oh, sure, sure, sure. Let me just quickly show you some examples. So, why do you have some. some uh uh we have some shift in the the the message that the the the the choose uh the our filter slash was can also work much better uh and this here is some examples of the light images here we consider the shifted image where the shift amount is the plus minus six pixel values and the the the image size is 65 by 65 and uh you can see that the result really looks much better than the Verily looks much better than the Euclidean distance. And this is just one brief slide about the experiments on the toy Periam clean data. So the main message here is that you want to use these different metrics to find the nearest neighbor. And for example, if you want to find the key nearest neighbors based on different distances, then there's multiple distances. this vertical this uh vertical axis tells you uh the sum of true angles to the target image to this reference image uh what what what what's the sum of distance to this target image so so this curve basically says what the the the best you can do and uh if you go above this line that means you're starting making mistakes finding the nearest neighbors and uh in general our method is slightly better than the wave the e md but uh our speed is considered But our speed is considered to be much, much faster. I mean, here's the total payful message. So, the slice wasn't a very natural choice for aligning the images because you really don't need to recompute this instance over and over again every time you apply rotation. But the problem is that you really need to apply a high-pass filter. That's what people haven't done before. So, our limit is, as I showed, is really fast and robust to the different. Robust to the different deformations. And we're probably the only algorithms right now that outperforms the original waste, but still have the same computational complexity with the impedance distance. Of course, some future works like implementation in 3D. This is more trickier because the searching space is much larger. You have three over angles, but in this case, probably a close-product search won't work. You'll need some really the automatization based together. Automatization based, but still considered waste distance. Yeah, another thing is to improve the robustness of noise. The really issue here is that the status was not well defined for the images with neck values. It only admits the image has to be defined as a probability density function, so it has to be positive. And after normalization, the sum of pixel values has to be one. It won't. It won't immediately be this requirement while you can add noise about it. So that's something where we need to be carefully considered. Any questions? Yeah, yeah, I missed that point actually. So, why after you apply RAM filter, indeed, you have negative values. But RAM filter is like you take out the zero frequency information. So, the resulting image in the original space will have a positive part and a negative part. And the sum of the mass of these two parts are unstable because the mean of the image is zero. You can use either or use the two parts together. But in this experiment, we only use the positive. On it, good.