The hardcore model was already mentioned before and so maybe you can already guess what this is about from the title and the picture. But let me first go over the definition. So we're going to look at the packings of disks with the centers on the lattice and we're going to have the household resolution. So our configurations are going to be just so functions from the lattice to 0, 1, and we put a 1 wherever we have the center of the disk. And so if we have some domain and the boundary condition, then we have the standard definition for the hardcore measures. So among all the configurations that are being run. The configuration that agree with the boundary conditions, we define a distribution where the probability is proportional to lambda to the number of disks. And then we want to define also the distribution in infinite volume. So I have here two equivalent ways of defining it. I think maybe the one that is more easy for someone who doesn't know this. Easy for someone who doesn't know this thing. It's the second one. So, what we do, we take a sequence of domains, so we can just take these boxes, and for each domain, we pick a boundary condition, and we have a sequence of measures, and such a sequence will always have a weak limit, and the Gibbs measures are just convex components. And the Gibbs measures are just convex combinations of such subsequent elements. The definition is here if you need to review it. So I'm going to talk about the question. So we want to discuss the phase transition in lambda. So there's a theorem that if lambda is sufficiently small, then the boundary conditions cannot significantly. Conditions cannot significantly affect the origin. Another way of saying it is that the Gibbs measure is unique. Really, a Gibbs measure is somehow the way that infinitely four boundary conditions can affect it. And now we want to study the high fugacity, so large lambda. So we just want to have So, we just want to have the theorems of this type. We have for a specific value of t if lambda is sufficient d large, and we want to say that the Gibbs measure is unique or it's non-unique. So, if the boundary conditions affect the R. So, now let's talk about the plan of the talk. So, I've already mentioned the statement, and the story is that The stories that there was quite recent work from 2007 to 2019 that kind of classified the cases and solved all but finitely many. And these finitely many have the special phenomenon called sliding, and there everything is open except for one case that I solved with my advisor. So the plan is first I'm going to So the plan is first I'm going to say some more general things about techniques that can be used for proving non-remembers of the kids user. Then we're going to talk about the justification and then about uh the one solved case and then I'm going to present the the open cases and and and really that's maybe the point of the talk. That's maybe the point of the talk. These finitely many cases present many, many, many different behaviors, and they're all very interesting. Okay. So to talk about these techniques, let's first focus on this case. So you've already seen this picture. And so again, we're talking about large lambda, and there's this. About Large Lambda, there's a statement of what happens. So there is this chessboard pattern, and there are two variations of it. You can shift it by one and get another one. And the theorem says if you put this chessboard pattern as the boundary condition, then you will still see it with high probability in the origin. And so let's talk about how we can prove this. So the heuristic is that. So, the heuristic is that when the configuration is dense, then you see the chessboard pattern. And the chessboard pattern is maximal density. So, we want to be able to measure the deviation from maximal density. And formally, how we can do this is we can look at edges between neighboring sites, and we can say every particle covers the four edges around it. And then we want to look look at the edges that are not covered. We look at the edges that are not covered. And we look at the duals of these edges, and these are the red edges in this picture. They measure how much we deviate from maximal density. And then the idea that we want to apply is that if the origin has a pattern that is different from the boundary, then there will be a cycle of such edges, and then we can apply a pipe. And then we can apply a Tier's argument. So, what this means is we just do a union bound of all possible cycles around the origin, and we want to bound the probability that such a cycle will actually be composed just of defect agents. And for this union bound to be good enough, we need something we call a pirus bound. So, it's a bound on the probability that a cycle appears as. As the defect edges, and it has to be of this form. So that's the general idea, but how do we prove this? So I'm going to present three methods. So one method is the bijection. We're going to define a map an injective map from all configurations that have this cycle as a defect. As a defect into the set of all configurations. So the map is going to be shift what's inside to the left and then add new particles. And what happens is this so what happens to a specific configuration? Its weight is increased by this factor. So the number of edges that are facing to the right. That are facing to the right. And because the map was injective, then this quantity is less than the total probability of the whole space. And so when we move this to the other side, we just get a bound on the probability that this appears as a determined. So maybe achieved a little bit because maybe the number of right-facing edges is not proportional to the size of the. Not proportional to the size of the cycle, but then you can choose the upwards or left or down. So that's one method. A second method is the chessboard estimate. So the chessboard estimate is first of all a very bad name. Chessboard, it has nothing nothing to do with the chessboard that you saw before, and estimate just means inequality. And so and chessboard is just not a good wor word for this. Not a good word for this. So I can't explain it like really in the time I have, but I'm going to go over it quickly with the hope to spark some interest. So suppose you have a symmetric domain. So if your domain was symmetric with some reflection line, and your boundary conditions are also symmetric, then you can condition Then you can condition on the values of the configuration on the reflection line. And what happens is that the configuration on one side becomes IAD to the reflection of the configuration on the other side. And this leads to a property called reflection positivity. And when reflection positivity holds, then you can prove the chessboard estimate. And to say the chessboard estimate, we have to look at periodic boundary conditions. At periodic boundary conditions. So instead of looking at the plane, we look at an L by L draw, so something like this. And we take some rectangle, so let's look at this rectangle, and we look at reflections of this rectangle through its edges. So we can reflect it to any place on the torus. And so then we want. So then we want to define a norm on random variables that depends only on the configuration inside this box. And the norm is, let's take this function and reflect it to every possible place, take the product and the expectation of the product, and then take the power that makes this expression homogeneous. And then what the chessboard estimates. What the the chessboard uh estimate says is that if you if you kind of choose uh a different function in every in every square, then the expectation of this will be less than the product of the the of the norms. So I think that was way too fast to understand, but I'm going to show the application of this. So what we're going to look at is the event that the one by one square, like one of the green. Square, like one of the green squares here, is has only one particle. And then what the chessboard estimate gives us is the probability that if you have a set of such squares, so it's drawn here, all the green squares, the probability that this event happens in all the squares is given by this expression. So we have here the zero. These expressions. So we have here the size of the set, and we have the event that the event happens for every one by one rectangle on this tolls. And this probability we can easily bound. So it just means that the density of particles is going to be way too small. So it's an easy exercise to bound this and get a disbound. And here you have, so if you And here you have, so if you have this red cycle, you can cover it with the green squares and get another way to put the pyres up. So if you're interested, I want to ask, how many know the Chessboard estimates? Okay. Anyway, I have these lecture notes and they're very short, just seven pages. Very short, just seven pages with wide margins and pictures, and they also reference other introductory texts about this thing. Maybe you can check it out. And then there is... But what will be it with boundary conditions? You're right. This chessboard estimate doesn't work, it's boundary or it should be better. Right, so we have. Right, so we have in the work that we did, we found some way to apply this chessboard estimate to periodic, so doubly periodic, so for Gibbs measures that are periodic with respect to a lattice that is full rap. And then we can just use this. How do you balance compare for the two different methods or for the three analysis? How did the balance that you get compare? Balance that you get compare, like the balance on lambda? I don't know. I didn't think about it. Okay, so now there is a third method. And the problem is that I'm completely not qualified to talk about it. It's been four years since the first. The standard idea about this uh periodic, non-periodic, would be that you somehow choose one one particle on the back cycle. One particle on the back side of the torus and fix it. And this will be like boundary condition. Just position so one particle? Position of just one part. I don't know. This is like boundary cognition somehow. So quite often this forms for this reversion. Yeah, I don't know about that. I want to hear about it. Okay, so it's been four years. Okay, so it's been four years since I first wanted to understand it and I tried. There was a chapter in Friday Venomik and there was some lecture notes in Zaratnik and I tried to read them but they were never finished. So I'll still tell you what I think I understand. So let's think instead about the case of the ferromagnetic Easy model. So there you have these interfaces between plus and minus. interfaces between plus and minus and you can and with these interfaces you can do a cluster expansion. So you think of you think of the configuration as a hardcore model where each component of the boundary is a particle and the hardcore restriction is that they just have to not touch. And they have different activities, so which the activity of each Of each boundary is given by its length. And then there is the Potetsky Pius condition that lets you prove that the cluster expansion converges. Now, if you wanted to do this for the model that we just mentioned, that you would have a problem because if you see this red cycle, then it tells you what is on the cycle. It tells you what is on the inside, which type of chessboard pattern is on the inside. And then it limits which other red cycles can be inside. So it's not just being disjoint. And this complicates things. And somehow, Pyrogovsina theory does somehow recursive something recursive with the cluster expansion and deals. The cluster expansion and deal. I don't know how. Okay, so these were the three techniques that I wanted to mention. And now we really dealt with the first case in many different ways. So now let's somewhat systematically talk about the other cases. So, what we want to take from what we had before is that we need to be able to measure the deviation from a full density pack. And there is one method that we can use, is that to replace the disk with another shape, a bigger shape, that still gives you the same hardcore scritching. And if you are lucky, you can choose a shape that will actually tie the plane. Tiler plane. In this case, just the gaps in the packing. So when I say tiling, I mean it covers, and when I say packing, it doesn't have to cover the whole thing. So in the first case, we could inscribe each disk with the square rotated by 45 degrees, and we get that. But if we, for different parameters, then For different parameters, then so here you can think of disks inscribed in these squares, and that will be the d square equals 4k. But this only works for a very few cases. So the second method that is going to work for all cases is to look at the deronatory encryption. So who knows what the deronatory encryption is? Okay, so I'm going to define it in terms of the Voronoi diagram. So, look at the set of black dots. We're going to construct the Deronate triangulation of that. So, first we construct the Voronoi diagram of that. So, for each point, you have a cell, and that is the cell of points that are closer to this point than any other. This point, then they are close to any other point. And if the cells of two points share an edge, then they are connected by an edge in the Telune triangulation. So, in a general configuration, this force, you can have a caveat if you have four cells or more that's joining. That's joining in one point, and you have to somehow resolve this. Normally, you just suppose this was just one point, and you would have these connected by a square, and you would have to choose if to put this edge or this edge. Absolutely. Okay, so this is about the net regulation. And now, how it helps us. So, if you It helps us. So, if you use the Euler formula, then you just see that the number of triangles is going to be roughly twice the number of vertices. So, the number of triangles is given, and you can read the density by looking at the average, so maybe not by looking at the average area, but to maximize. To maximize density, you want to minimize the triangular error. And so, okay, so this whole work of characterizing the cases was done by Alec Mazer, Isabella Stuhl, and Johan Sophot. And so they defined this notion of an M triangle. So it's a triangle with the vertices on the Triangle with the vertices on the lattice, degrees at most 90, and not quite sure why this is needed, so I don't know how to explain it. And of course, you need the side length at least, because the restriction is that the points are in configuration of a distance at least. And now uh an M triangle is uh is such a triangle that is minimal amongst what it's in area. The more in area. And what they proved is that if you have a periodic configuration, that it's maximal density if and only if in the Deronator angulation you have only M triangles. Okay, so what we get is that if we want to construct maximal density configurations, we just have to work with these M triangles and fit them together. And triangles and feed them together into a triangle edge. I don't get what the problem is, like what you're trying to say. What do you mean a maximum density? So you want to pack, you have the restriction of the distance, so you want to find a periodic configuration of points that has the maximum density. With a distance restriction. With a distance restriction, yes. Okay, got it. So and now we can turn this into like constructing a jigsaw path. You have s some rules and you have to build with these triangles. And so, okay, so now I need to explain something. Suppose you already chose three points, you got a triangle, and suppose that was. A triangle, and suppose that was an M triangle. You can complete it into a parallelogram. And because it's a parallelogram, this will also be an M triangle. Because it will be on the lattice, and it will be, of course, of the same area. And then you can, of course, continue this into a maximal density packing, but maybe there is more than one of them. But maybe there is more than one option. So maybe you have this triangle and then you can complete it like that, but maybe also this is also an end frame. And this is where sliding comes in, if you have more than one choice. So they actually somehow proved that this happens in only finitely many cases, and they managed to enumerate all of them, and that some. All of them, and there's some number theory involved in that. So, I'm not sure it's the right point to say, but so this thing they call a sliding base. So, here you have this sliding base, but you also have this. This edge is also a sliding base, because if you put the third point here, then it's also. Here, then it's also the same frame, so it's also an air angle. So we have sliding in different directions. Okay, so now in the non-sliding cases, pretty much what happens is similar to what we saw in the first case. If you have an area that is fully packed, and another area that is fully packed in a different Area that is fully packed in a different way. So maybe it's just shifted by one unit, or maybe it's rotated by 90 degrees. And if you want to continue both of them, eventually you will have some boundary between them that has non-M triangles. And there you see that, and this boundary you can use as a defect controller, similar to what we saw before. But two of the techniques fail. So the first one fails because Fails. So the first one fails because we just don't have enough room to shift the configuration. The second one fails because we don't have reflection positivity. Reflection positivity only holds for these first two cases. But Pyologovsinai works. And so Mazarsku and Sukhov actually write this formally and approve it. And okay, so this is the table of all the sliding cases. Sliding cases. So it's not the whole table. Even these two are not the whole table. There are 63 rows. Okay, I think I'll come back to it. But I want to summarize kind of the last few minutes was all the work of Mazesh Blue and Sofo. Mazashlu and so forth. So so to summarize what they did, they uh they uh proved that the non-siding cases uh have the multiple GIFs measure using curls in IDI. And the more impressive is that they actually managed to find this table of all the all the cases. This was also independently verified by Dimitri Kostrunker. And they also made a conjecture that all the styling cases have just unique metrics. Okay, this matchup. So, this concludes the third part of the talk. And now we're going to talk about the case of d squared equal to 4. So, as before, in this case, we don't have to look at triangulations because we have types. And so, you see these white gaps, they measure the deviation from the The deviation from the maximal density. So, one thing you can kind of guess is: okay, so you can see areas of full packing that look one way and other areas of full packing that look another way. And maybe, okay, so they they they definitely don't have to be separated by a contour of the of the loss in uh in In density. So maybe there is no phase transition. Maybe it's just the unique gift measure. And so there's a paper from the 80s about reflection positivity by John Freulich, Robert Israel, Paris Moon, and Borkman. So they So they said that the conventional wisdom is that this will have a unique Gibbs measure. But the colours mean? Oh, yeah. So the colors are the parity of the coordinates. So for example, say this is 0, 0, then this is 0, 1, modulo 2, and this is And this is 10, and this will be 1 1. The coordinates of the squares. Yeah, so, but back then, I guess they didn't have this nice simulation. In this simulation, it's quite easy, I think, to see that something is going on when you have two different phases. Uh so so it was the model repetition, for instance? Oh, uh so it's not very scientific, it's just uh there is a bigger simulation and it has periodic boundary conditions and I think we just we just took this snapshot before it even converged. So, if you let it stay for longer times, and so one of these sites. Times and so one of these sides will overtake the other, and probably it will just stay like that for your life. So there's four. So let's try to explain what we see here. So that's the theorem we put. So there are four Gibbs measures that are periodic and extremal. And extremal, and so let's talk about one of them. So, we call it new vertical zero. So, we have vertical columns in this measure, and the zero denotes the shift. So, you can take the same picture and shift it by one to the side, and you get a different picture. So, what we proved about this measure is that, first of all, it's extreme, and it's so it's And so it's invariant to translations up and down, and also invariant to translations by two units right and left. And this second item of column order just says that in between the columns, the density is very low. So most of the tiles are going to be inside the columns. And I'm not going to talk about the third decay of correlation. And every periodic Hips measure is going to be. measure is going to be a convex combination of one of the four. So the three others can be created from this one by shifting and rotating. Maybe you mentioned this before, but could you explain what you mean by periodic deep symbol? Okay, so doubly periodic means so you have a measure that is defined on the whole plane and you can achieve Find on the whole plane, and you can shift it. And this just means that there is a full-rank lattice, so that shifting the measure by any element of the lattice keeps the measure the same. Yeah, I didn't say what extreme extreme means the tail trivial. So with every event that is um the tail sigma trip has a probability zero or one. As probability 0 or 1. This is stronger than being ergodic. So I'm not going to say much about the proof, I'm going to say something about intuition. So we have this phase of columns, and I want to look at somehow a simplified version of it, where all the tiles are restricted to stay in their columns. I want to calculate the partition function of To calculate the partition function of just that. So it separates into one-dimensional system, so I can actually calculate this. And I get this formula. So you have some term that is some volume term, and so you have the power hw here, and you have this boundary term that comes from the edges of the column. So if you, suppose you had a cyclic boundary condition. Cyclic boundary conditions on each column, then you wouldn't have this. But since you do, then the restriction on the parity of the squares gives you this term. And now the intuition for why the phase transition happens is suppose you don't have this rotated rectangle in the middle, then you can calculate the partition function, and then you can calculate with this rotation. Calculate with this rotated area, and because of these ends of the columns and the rows, you get an extra factor. Oh, there is a mistake here. You have a half. Okay, yeah. So that's the intuition, and I'll say. And I'll say very little about the pool. So the pool it's pretty different from that. So we look at look at the configuration and we draw these green lines that are kind of fault lines between neighboring columns that are not the same. And we call them sticks and then we look at uh these uh pairs of uh concentric squares and Concentric squares, and we say it's properly divided if there's a stick that goes through both of them. And then here you have it's properly horizontally divided, and here it's not properly divided, and this is kind of the bad event. So we want to look at some grid of such squares and apply a piles argument on this kind of bad effect. So we use the chessboard estimate to prove that this. Use the chessboard estimate to prove that this event has extremely low probability. So that's what I'm going to say about the proof. If you want more about the proof, then there is a recorded lecture from the UBC Probability Seminar. You can find it online. And so in the so that was the fourth part of the lecture and now we get to the fun part and these are the open cases. So there is 3 by 3. 3 by 3 is just, so it's equivalent to d squared equals 9. And the only thing that really changes is that we don't have reflection positivity. So this is So, this is a simulation, and here I didn't want to have nine colors, so I color according to just the X column. And you again, you see, it means you have a column or again complete simulation. And then heuristic calls, but no reflection positivity. But if you chose the other, instead of vertical, you chose horizontal, it would. Vertical, each as horizontal, it would look like an even distribution with the three colors. Yeah, yes, but you will have kind of long stretches. In each column, you have kind of a long stretch of one column and then another. How are you simulating this? So it's I guess it's called double dynamics. So so at each in principle at each uh step you Step your starting with just blue and then doing library metrics? Is that why it's quite easy to I guess I tried different different things? I don't remember what that is. But I think anyway, eventually you will get it. So just you look at a single site and see kind of results. Um how about find if you had periodic boundary conditions and didn't have a multiple of three, you wouldn't see it all blue, right? You would see a mixture of colors. Yeah, that's right. So that's right. So you would see in the two by two, you definitely would see two different faces. So, what can I say about this? I kind of have some vague ideas on maybe you can kind of do a find some graphical representation of this water where these nice areas are just actually a ground state. And if you have that, then you can maybe apply to all those numbers. But then I wouldn't have to. But then I would have to learn it. So, yeah, that's basically what I have about this case. And then let's look back at the silence because we exhausted all the cases where we have tiles, so we have to look back at the triangulations. And so, in the small cases, there is this sliding in multiple directions, and it seems complicated. Directions and it seems complicated. I didn't try, it could be interesting, but I didn't try to see what happens. But beyond t square equals 20, you only have these two types of behavior. So you can either have sliding by one side up and down or right and left, of course, because if these are angles and they're 90 degree ranges. And triangles and the 90-degree rotations, also. And you have this kind of diagonal shape. So it will look something like that. If you do a simulation, so these are the sliding bases. And it's kind of like the columns that we had before, but there is a complication because There is a complication because this is kind of interlacing between the two columns, so they are dependent. So if you want to think of kind of a caricature like we had for the 2 by 2, then let's take the case of vertical columns. Then you would have some gaps, but then the rule. But then the rule is that between every two gaps you have to have a gap, exactly one gap in a neighboring column. And the same for every column. So that's the kind of picture that I expect you see. I don't know if you can actually give a nice expression for the partition function here. I didn't think about it a lot. That's one thing you can say, and maybe if you can do that, then it seems quite hopeful. But even if you cannot, then I think there's a pretty convincing argument that you should have the multiple git measures. And the argument is simply that you expect the gaps to be sparse. Sparse. So you expect to have a unique infinite component of M triangles. Because you have sliding only in one direction, only on one base, then the sliding on this infinite component will be always in the same direction, which is not the same as the d-square plus 4 case, because there, actually, if you look at the triangles in the new. If you look at the triangles, then then you can you have sliding in different directions. So in this sense, this this should be easier, but it it's not clear what technique can be used to prove it. Yeah, so that's all I had to say about the square lattice, but Mazar Stro and Sukov also tried the Tried the hexagonal lattice and the triangular lattice. And the triangular lattice is boring for us because it turns out that you can never have sliding because just all the M triangles happen to be equilateral and then there is no slide. But for the exaggerated lattice, they also checked and it's a bit more complicated there. Complicated there in some sense, so they don't work with M triangles exactly, but they managed to find the sliding cases, and there are exactly four. So, I'll start with the two bigger ones that they don't have something very intelligent to say about. So, this is just a picture from their paper. And you can see here the sliding between have the equilateral triangles and non-equilateral triangles. So, it's a bit more complicated than what we had before, because you can Than what we had before, because you could have sliding in different directions. But then there are the two smaller cases, and there we have again tiles available. So in this picture, you see just that these points are the hexagonal lattice. And at each particle, if you have a particle centered here, then you put a tile that is a triangle with edge length. It is a triangle with edge edging streets. And so you can try to do a simulation. And so we had 2 by 2 squares, now we have 2 by 2 triangles, and it seems very similar. We have columns. So again, I want to do this thing of calculating the partition function of the curriculum of the face. The phase. So, how do I do it? This is just the density, what you get from the maximum density. And then you have gaps in the columns. So, suppose you have the gap, the density of the gaps is 1 over m. Then now a gap, if you think about it, you see that here the size of a gap is just the size of a tiling triangle. So, you lose lambda. Lambda to the minus one weight, but then you have this additional entropy of choosing where you put the gap. And then, how many times do you do that? So it's the area divided by the density of the gaps. And you can just find the air that maximizes and get this approximation for the partition functions. But then you run another simulation and you get something else. Something else. And I don't know what like what changed. It happened. I think maybe I saw this one first, actually. And then I was surprised to see this gather. But first I hypothesized the first and was surprised to see this. So again I can try to say, okay, so now it's a bit more difficult, but I have an interpretation of this. An interpretation of this. So, the interpretation is that you have a hexagonal lattice, but it's perturbed or deformed. So, you keep all the gradients of the edges, the the angles, but you're you're allowed to change the the length. So, suppose you drew this, you can pick uh your favourite length for each vertical segment. Favorite length for each vertical segment, and then you then it forces the drawing of the non-vertical segments. And then you can keep going. This means you have one degree of freedom per hexagon. And now how, how, so this is this uh L and L is going to be the typical uh side length of the hexagon. And then you have uh so how much density do you want? So, how much density do you lose? So, let's look back at this whole picture. You have three hexagon over four areas of a triangle. So, every hexagon has six corners, but they are shared by three hexagons, so you get two hexagon uh two two corners per hexagon. two quarters per hexagon and which means which means lambda to the minus half and then you ask how many hexagons do we have so the so area of a hexagon is about uh its side x squared and you again maximize and you get exactly this not exactly you get the same expression, which means you have to do something more accurate to To do something more accurate to tell which one is the true one. So I don't know, but I think if this is the true one, then I should expect a unique measure, but it's not obvious. If this is the true one, then of course more than one. Okay, so that was this case. And one last thing, I don't have something very small to say about it. Something very small to say about it, but you have this, you can again find time. So here you have the, in black you see the exaggeratice, and if this is the center of a particle, then you can use this as the time. And you have six, it makes sense to color it by six colors somehow. And to me, it seems very likely that here there is. It seems very likely that here the gifts measure is unique. So just okay, I did a simulation, I didn't find any pattern. And then even if you look just at timings, then still there is some freedom. So if you add this extra freedom of having gaps, then it seems that it must be in an EPS management. I don't know. But to prove it, But to prove it, it's still a problem. And also, there is this kind of analogy with the monomer-dimer model. So there too, you have the dimer model, which is exactly solvable, and here this model of tiling is also exactly solvable. And there, also, you have some freedom. So, just it's even though it's a tiling, you have many. Even though it's a tiling, you have many just choosing the boundary doesn't force the tiling. And it is known that the monomer dimer has a unique measure. So Tyler talked to me about it. It's not like you can just move the argument to this setting and it works. Um okay, I think that's all I have to say. Does this study give you any intuition for what happens in the continuum when you place disks in a I don't think so. I mean it was uh suggested that this is a good way to study the continuum. I don't think so because uh I don't think so because if you want to take the limit, the continuum limit, then you take lambda to be like one over t squared. So it's completely different. You said that in the hexagon, like if it looks like this picture, then probably it's unique. And if it looks like this picture, then probably there's non-uniqueness. Why can't you have non-uniqueness with a disordered phase element or or like a, you know? Or, like, the disordered-looking one and the structured one. Like, why can't the disordered one be a Gibbs measure and then also an ordered one be a Gibbs measure? That would be a really unlikely coincidence that these two things actually have the same, how to say, like the same partition function. Um the same partition function uh approximately. You know, uh for instance, when you said you weren't sure if it looked like adding these two pictures, I guess when I asked about the simulations, something that I'm wondering is if you were to start with the first picture, do you know if the simulation will stay looking like the first picture for a very long time? Like, but this sort of, if you're looking at large lambda and you expect a phase transition, you'd expect, like, I guess maybe if you. Expect, but I guess maybe if you start from empty, you hopefully lock into one of your possible extremal ones. Or equivalently, if you start an extreme one, you should stay there kind of for a long time. So, do you know what happens if you start with like different initial conditions for this hexagon where you end up? I don't know. I mean, I didn't very scientific about doing the simulation, so I didn't write down what I did. Have people tried to classify the extremal states in like Z3, the different P? So Mazaston and Sukho did something on Z3. I don't really remember much about it. And I tried to think about the two by two by two, but it seems quite bad. Another is about some of the challenges that you have to understand the optimal constraint. Understand the optimal display packet on V3, which seems to be somewhat harder than the candidate. So, my honor. I'm not going to try to understand that if they thought about it, then realize they've reached a seemingly very hard problem to continue. I guess just the idea is this Delaunay idea here is how you can prove optimality of a 2D packet very simply, but there's no such proof for 3D. So questions?