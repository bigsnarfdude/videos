Recognizing rotations of Zn. Thanks. Over to you. Thanks. Yeah. Thanks to the organizers. I'm happy to be speaking here. And this is joint work with Steve Miller, who's here also at Rutgers. So yeah, I guess let's get started. Let's see, there we go. Okay. Yeah, so I'm going to start with some background, just kind of like an overview of what the talk is about and the problems we're trying to solve. And then I'll get into some more details piece by piece. More details piece by piece. So, the context for this kind of problem has to do with post-quantum cryptography. So, that's like building crypto systems that would be secure against a quantum computer, right? So, if you have some sort of encryption that would rely on the difficulty of, let's say, factoring products of large primes, that won't be safe if you can attack it with a quantum computer. So, you need these sort of hard math problems that are resistant against these quantum computer attacks. And a popular choice for this would be lattice problems, different kinds of problems. Lattice problems, different kinds of problems that have to do with lattices. And for a problem to be useful for cryptography, it needs to be average case part, right? Not just in certain special cases. And the problem that we'll take up, which I'll talk about in a little more detail, is efficiently recognizing rotations of the Zn lattice. So given some basis for a lattice, which is really the rotation, a rotation of Zn, can we efficiently recognize that it is this kind of rotation? So we actually are going to do. So, what we actually are going to do is we're going to compare different kinds of algorithms that you can use to generate lattice bases and then compare how secure they are against this problem of recognizing rotations of CN. This is really going to be the same as looking at different methods to sample GLNZ, which I will also talk about in a little bit more detail. And just as a preview of our results, we found that there are weaknesses in certain methods for getting for sampling GLNZ, including Including a method of multiplying elementary matrices, these unipotents, and also a method for generating matrices that was used in the DRS submission to the NIST post-quantum cryptography competition. And we found that other methods, which we'll also get into, were stronger and more secure in this kind of context. And just to make sure, I mean, I'm sure this is the 10th time you're hearing this, but just to make sure we're all on the same page, a lattice is integral. A lattice is an integral set of integral linear combinations of some basis of Rn. So it's the Z linear combinations. It's an abelian group. And if you think about it in terms of a picture, right, it's some sort of grid in Euclidean space. So when you have a lattice, there are many choices you could choose for this basis. So you could have, for example, this red basis U1 and U2, or you could have V1 and V2. These are different bases and they have some different They have some different, they'll play out a little bit differently in practice. But if you want to transform from one basis to another, you can do so with integer coefficients, right? Because it's a lattice. So these are integer linear combinations of the other one. So what that gives you is a matrix that transforms from one to the other. It has integer entries, and its inverse, which is switching back from one basis to the next, is also an integer matrix. So, the point of all this is to say that picking a basis for your lattice ends up being equivalent to picking an element of GLNZ. So, we talk about sampling bases or we talk about sampling GLNZ, we're really talking about the same thing. So, but why do we care about the bases? So what happens is that some bases are easier to work with than others. Some of them are just nicer. So, for solving different kinds of lattice problems, sometimes you can solve them more efficiently if you have a nice basis. If you have a nice basis. So, just as for like flavor or background, as an example of a hard lattice problem, one thing you can kind of do is if I give you a lattice and a basis for it, I can ask you, what's the shortest vector in this lattice? And this is not something you can always solve very efficiently as far as people know. So, this is considered a hard problem, but it's easier if you have a nicer basis. So, I mean, the intuition, let's say, that I would kind of be thinking about this is that you want to. Be thinking about this is that u1 and u2, they're sort of like they're short, they're like nearly orthogonal, sort of gives you a nice could sort of figure out nicely what this grid is, but with harder basis vectors, it's sort of more complicated to try and find the shortest point that's actually in the lattice. Just another example of a hard lattice problem would be the closest vector problem. So I don't know if you guys can see, right? There's this green point in Rn here. Great. So this is, so just if I give. So, this is so just if I give you this point and I ask you what's the closest lattice point to this vector, then that's also considered a hard lattice problem. So the application for cryptography is, so if I have an easy basis, a nice basis for my lattice, and if no one else knows it, then I can solve certain lattice problems efficiently, but nobody else can. So that's how I can build my crypto system. And to make a crypto system out of the lattice, I need to know, I need a way to take, if I know. I need a way to take if I know my lattice and I know my easy basis to make some sort of hard basis that people can't just reduce down to a nice basis very quickly. So, but it's not obvious or known whether every lattice has even a hard basis. And in particular, rotations of the Zn lattice, it's not known whether or not these have hard bases. So these are pretty nice lattices, right? Because you just take your standard ZN, maybe you rotate it. So does it have a hard basis or not? And more specifically, the problem, which was made known to us by The problem, which was made known to us by Lens Jordan Silverberg, and this is the first time, like we know that it's in print. So, given some basis vectors for your lattice, so you have some lattice and you have some basis, can you efficiently detect that this is a rotation of Zn? And just putting this down into some more math, right? So you have a matrix whose rows are the basis vectors of your lattice. And you can think about that this spans Zn if and only if this matrix isn't. This matrix is in GLNZ. So, the way that we can formulate this is if you take the matrix B times B transpose, you can ask, can we efficiently determine whether or not this grand matrix is M times M transpose for some M in G L N Z? And if so, can we efficiently find this M, right? So, can we efficiently recover M from this grand matrix where M is in GLNZ? And the reason this is equivalent to this problem of The reason this is equivalent to this problem of recognizing Zn is that, so B is M times R, so that's Zn rotated, right? If and only if B times B transpose is equal to M times M transpose, where M is in GLNZ. And the practical way we're going to work with this is this average case. So we're going to find some way of sampling GLNZ. We're going to talk about these different algorithms that we're going to compare and see how well we can efficiently. See how well we can efficiently recover m from m times n transpose from these different distributions. So I guess, yeah. So that brings us, I guess, to these different algorithms for sampling GLNZ, which we're going to compare. So the first one is sort of the most obvious, which is just, so you want a random basics in GLNZ. So just take every entry and just generate it randomly, right? And then see if it's in GLNZ. Right, and then see if it's in GLNZ. So, if it is, great, and if not, then try again. Right, so you're just picking each entry one by one randomly in some interval. And the benefit of this is that it does sample uniformly, right? You're just picking each entry randomly. But the problem is that if you just pick random entries, it's very unlikely that you're actually going to land in GLNZ. So this becomes very inefficient and it's really impractical for any sort of large dimension. It's really only useful for small n, and we'll talk about later an algorithm that uses this as a subroutine where you just look at some sort of small matrices that you're You just look at some sort of small matrices that you're generating this way. So, the next algorithm, though, is like a little bit more, it's a little bit more sophisticated, it's a little bit more fun, which is so you take these products of unipotent matrices. So what does this mean? You've got this identity matrix, and then you have some sort of value on an off-diagonal somewhere, which is going to be minus one, zero, or one. And then you take this long product of matrices of this form. So you generate a bunch of these, and then you multiply them. A bunch of these and then you multiply them. So the nice thing about this is: okay, it's a nice algorithm. It sounds cool, I think. It exhausts SLNZ, which ends up being working the same as GLNZ for us. It's nice to implement. It's implemented in Magma as this random SLNZ function. But the problem is that it actually does produce biased output. And therefore, it's easy to break. And we were able to break it for this recognition. For this, recognizing the rotations of Zn problem. Another drawback is that to get good randomness, you need large L. So that's long products of matrices. So it can be slow to generate a good matrix. Our third algorithm is kind of similar, but it's a little bit different. So remember, we said we were going to be brute forcing some stuff. So what happens is you have this big n by n matrix. You generate some small d by n. Some small D by D matrices using this brute force method. And then you sort of embed them in different, like as blocks at different rows and columns. So that's what this highlighted stuff is. There's these little matrices that are embedded in these big matrices. And then you multiply that. So this is good because you've got this randomness in here from these little matrices, and it samples more and more uniformly as the small matrices grow that you're sampling uniformly. Uniformly. It is similar to the previous algorithm, algorithm when d equals 2. And so it's easier to break. And this brute force step means that we can't make d too large, right? Because then we're just going back to our root force, which is too slow. But you can kind of think of this as a way of interpolating between the first and second algorithm. Yeah, I mean, maybe there's an analogy if you're familiar with lattice basis reduction that algorithm two has some things in common with sort of like a reverse L. With sort of like a reverse LLL process, and this is more like a PKZ. All right, so that brings us to algorithm four, which is very different, which is not similar to the last two, which is, okay, so you fill out the bottom, everything but the top row, just fill in entry by entry. Then it's likely that there exists a top row that makes the whole matrix in GLNZ. So there could, there's going to be some, or it's likely that there's going to be some top row that. Or it's likely that there's going to be some top row that works. You can find a row like that and reduce it to make it smaller. This is based on a suggestion of Joseph Silverman, and this ends up being basically our best method, which I'll talk about in a little bit more when we get to the experiments. And finally, we have this algorithm five, which is this Hermit normal form. So this is a known method that people use. You take this m by n matrix, you're just generating an entry by entry, and you do some Hermit normal form, which gives you something. Reduce some herminormal form, which gives you something in GLNZ. What's surprising is that even though this looks very different from the algorithm I just mentioned, it actually often produces the same output. There's something actually similar about them. So this has good randomness properties like algorithm four does, but it seems like algorithm four is a little bit better. Okay, so that was the list of the different ways that we sample GLNZ, but let's talk about how we actually compare these. So here's our process. So, given a matrix B generated by algorithm N, you know, attempt to solve problem 2B. So, that's recovering the matrix from the gram matrix using the following steps. So, first we do the LLL, which is a lattice basis reduction algorithm. We did it just taking out of the box on the Graham matrix. So, trying to reduce it down. So, trying to reduce it down using this known algorithm, followed by BKZ, which is another lattice basis reduction. And then we say we have success if all output basis vectors have norm one. So that means we were successfully able to recognize a rotation of Zn. So, yeah, I mean, so basically, we were taking matrices, generating them by whatever algorithm, and then applying this procedure. So, algorithm one was too slow to make matrices, so we're not testing that. But this is algorithm two, the products of unipotence method. And so, this stuff, these dimensions, this is things we were able to break. We were able to recognize the ends from these with these parameters, which shows that there's weaknesses, even in these high dimensions, of the matrices that were generated this way. And I think, I mean, we think that the reason. We think that the reason for this is because these are matrices that had a special structure, right? They're coming from this product of these matrices. So that is leading to some sort of non-randomness going on here. Algorithm three is a little bit more complicated looking because it has more parameters, but the idea here is, so just looking at this table, this green, the green highlights are the things that we are able to break, like recover M. Recover M. And so we found was that for the thing that really seemed to prevent us from breaking it was when the product was very wrong. So we were doing a product of a lot of matrices. That seemed to be the thing that was causing us to fail. And there are other parameters that it's a little bit, so it's a little bit, there's a lot to compare. But what you see is that even so in these other higher dimensions, we were able to break it even in dimensions 500 and everything like that. Everything like that. And oh, this is not highlighted, but in very high dimensions, we were still able to break it. So what's going on here? I mean, right? So first of all, when the dimension is big, we had to use smaller D. So this is more similar to this random SLNZ, which has this possibly the structure that's causing some non-randomness. Our attack. Our attack works well in high dimensions. And what we see is that the really the thing that helps is the long products, right? Like if you look at this last row or in the other examples. So we can compare this though to algorithm four, which is better. So what we notice is that for larger dimensions, Larger dimensions, so n is the dimension, right? The size of the matrix. So for n greater than or equal to 110, we didn't find m at any time. So basically, we recommend using algorithm four for applications where you need randomness. And it seems like really the thing that's that algorithm four, the weakness that algorithm four or five don't have, which these other algorithms have, is the product of the smaller matrices. This seems to be some contributing to some. Matrices. This seems to be some contributing to some sort of weakness when you generate things in that way. And that's sort of an inspiration for this application to the DRS submission to the NIST post-quantum cryptography competition, because this crypto system has a matrix generation method in it. And that matrix generation method has this sort of product structure. It's a different one, which we'll see in detail. I think it's interesting. So they have a random. So, they have a random lattice basis construction as part of their whole crypto system. They make a matrix in GLNZ as a product. And we're looking at how their matrix generation works with respect to the problem of recognizing rotations of ZN. So, we're not attacking DRS, not the whole cryptosynthesis, but we are going to analyze how well we can break their GLNZ sampling. So, here's the details of how it works. So, you've got these parameters, you've got these dimensions, and bit security. Security. And the matrix in GLNZ is constructed as a product of factors. So, okay, so you have these 49 factors. Let's see. Yeah. So you have these small matrices, A plus or minus. They have this form. And what you can do is you can sample these, you can make these diagonal matrices. So you pick an So, you pick n over two sines, you get this block diagonal matrix where it's got these A's along the diagonal. And then you can also sample some permutation matrices, right? Just pick some random permutation matrices and make this really long product, right? Or this long product of 24 matrices, I mean 24, 20 in 24 steps. So, permutation matrix gamma, permutation matrix gamma. And when you've got this whole product, so Um, so now we want to see. We want to see if this is going to work. Um, how are we, how well are we going to be able to attack this with respect to our same process that we've been doing before? Um, and what we found is that we were able to break this kind of thing, um, which I'll say more about in the next slide, but these matrices are really not, they're not so random, right? Because if you look at this, uh, oh, so this was made with um Mathematica's like matrix plot. So this is um the colors correspond to the entries, um, and what you can see is there's these like lines. So I don't know if it's Can see is there's these like lines, right? I don't know if it's showing up on your screen, right? But there's these sort of like bands going through, um, which shows that there's some sort of structure to this matrix, right? It's not just a random matrix like you would get if you just randomly generated the entries. There's some structure to it. And this really pans out when we tried to attack it because what we found was that in these high dimensions, like 1160 and 1518, we were completely able to recover M. Able to recover m so not secure with respect to recognizing rotations of gl and z of the z and lattice. And in dimension 912, which is interesting because it's a smaller dimension, we were able to do it only for 47 or fewer factors. So just going back for a second, there's, so this is one through 24, so that's 48, 49. And we got screwed to 47 in the product. So this is a graph. So, this is a route of how long things took. And so, basically, what we found was that there was really a weakness in this matrix generation method that was used in DRS. And just to mention, so just a reminder that we didn't break DRS, but we broke their matrix generation with respect to the recognizing ZN problem. All right, so that's basically the gist of this. That was our experiments. Experiments. Yeah, and so interesting questions. So first, let's thank Tamara for a nice talk. Yes. Thanks. And so do people have questions or comments? I do have some questions then. Some questions and sure, yeah. Yeah, well, it's well, simple and almost naive questions. So, the topic of generating random matrices is something that was quite studied for even for numerical analysis. And so, did you compare with some methods which were already used? Wait, so I missed the beginning of the question, I think. I'm just gonna put on my headphones. I'm just gonna put on my headphones. Sorry, I can't hear it. Sorry. Do you hear me? Try to speak again. Yes. Do you hear me? Yes, thank you. Yeah. Okay, great. Yeah. Okay, great. So as I said, so generating random matrices, it's topics that have been studied for, for instance, for some experimentation for numerical analysis. And there were several papers on this subject. So, did you look at it? So, this is the first question. And the second question is more twisted. There are some strong Are some strong ways to generate random elements in groups, in finite groups? We have several papers on it, in particular from Diaconis and other collaborators. And I know that Stefan likely knows it. And so instead, I mean, you could generate elements instead of GLNZ. You can apply this algorithm in generating in a GLN FP with a large P, for instance. Instance. And would it be interesting or not? So again, just to connect your work with Zoo's work. Yeah, I mean, that sounds interesting. I'm not really familiar, but I'll ask, I mean, I'll pass it over to Steve to see if he has anything. I'm pretty sure Stéphane knows this work. Well, so the answer is yes, we've thought about both. And Dioconus comes up in both contexts. He had a method for generating. In both contexts, he had a method for generating random rotations, and that method was to make a random, basically make a random matrix and compute its polar decomposition. It's Graham-Schmidt or USSA, depending what you want to call it. And you get a rotation matrix that way. And the Hermit normal form algorithm, which is algorithm five, essentially the same as four, is kind of analogous to that. As far as the issue with finite groups, so So, what we do, I mean, if you project mod n, gives you a, gives you, I think in all the cases we looked at, gives something equity distributed in the finite group. That's certainly true for multiplying the random uniponents. The problem is I don't see any good way to go to lift it back. So even if you could hand me a randomly distributed element in a finite group, when you do Chinese remainder theorem or things like that to get the numbers bigger, the size gets out of hand. The size gets out of hand, and that's where the bias comes in that makes it attackable through lattice reduction. Okay, I see your points. It is intriguing, the Belsox. Okay, thanks. Do we have other questions, comments? Sorry, a stupid question again. I think that it will be easy to adapt the method to the Hermitian case. So instead of working with Z and working for, for instance, for Gaussian integers or Eisenstein integers or even cyclotomic units. That's an excellent question. That's a it's an excellent question. I I haven't thought about it. This is this is subtle, so there are all sorts of things that could enter, like class numbers could enter. Thanks. Then Then, if there are no further questions, let's thank Tamar for a very nice presentation. And can people give me some guidance, Philippe? Should we go straight on or should we wait till quarter past you?