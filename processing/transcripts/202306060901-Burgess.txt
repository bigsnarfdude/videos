Welcome everyone for the first session of the second day. Our first speaker of the day is Professor Chief Pachas and he's going to talk about primordial decodance and reliable late time predictions. Thank you. And thank you also for organizing this and invitation to conference. It's always really a pleasure. Come. It's always really a pleasure to be here for obvious reasons. Inside or outside, it's always a great place to be. Can you hear me at home, Robert? Yes. Thank you. Good. Okay, so I'd like to talk about, this is kind of going to be like a movie within a movie. So there's two talks that I'm going to give. And one of them is about that calculational technique. Calculational technique, which is basically a mashup of open quantum systems and effective field theories. So open effective field theories. And I want to, the main message is to sell you those tools as to why they have some utility and they are something that could be worth learning in the sense that they can do things that other tools might not be able to. And I'd like to make that pitch in the context of a concrete That pitch in the context of a concrete calculation. So I'd like to describe a calculation, and the calculation is a contribution to the decoherence of primordial fluctuations during vanilla single field inflationary cosmology. So the one is meant to be the vehicle for the other, and you'll see that when I get to the table of contents, the actual calculation is a fairly shortish thing at the end because once you have the framework, it's the step. Once you have the framework, the steps are kind of fairly straightforward, although they're tedious. And when we get there, we'll get there. So it's work done with these lovely people: Benson Benin, Rich Holman, Jerome Martin, and Greg Kappanek. And many of us have been around for a long time. The one you might not know is Greg, who's a postdoc now at Imperial. And he is a heavy lifter here. The paper I'm describing is the one 2211, and there's another one that should be coming out shortly. That should be coming out shortly. And the XXX is there. I mean, I'd say 06, but it might be 07. All right, so before I tell you what I'm going to tell you, let me tell you what I think I'm going to tell you, or what I'd like to tell you. And so I'm going to be talking about open systems in a quantum open system in a gravitational context. And there's a level at which you're all going to be ready for that because there's a sense in which the use of those, the utility of those. The use of those, the utility of those tools is clear. If you have horizons, you know, many of the puzzles we have in applying quantum mechanics and gravity involve horizons. Horizons are clearly places where stuff crosses, it doesn't come back, and so you're doing measurements in a limited part of space-time. The thing that separates you from the stuff you're not measuring is not a conservation law. It's not like a low-energy effective theory where you're looking at low-energy states and you're not looking at high-energy states, and the criterion was a conserved thing, energy. It's just that somebody went through this wall. It's just that somebody went through this wall and it didn't come back. And so that's a language for which open systems is more appropriate. And it's because of that, the fact that the things you're not looking at are not separated from you by a conservation law, that you get phenomenon happening that you don't get in a normal coherence effective field theories, like things like decoherence or thermalization and stuff. So the openness is key. So that's what I said. So the other aspect of it that is probably less well appreciated, at least it was to me, was that many of our problems in quantum mechanics, in applying quantum mechanics to gravity, are specific to late times. And so examples are information loss and long-time inflation. But there's a generic problem there. And the generic problem is that. And the generic problem is that perturbative methods essentially always fail at late times for reasons that I'm going to get to in a second, but it's kind of clear that if you, it doesn't matter how small an interaction Hamiltonian is compared to an unperturbed Hamiltonian, when I look at e to the i h t, there's a t for which the perturbation theory is never well described by 1 plus h t. It's that the exponential has to be there. And that's something which is kind of a fact of life in almost all areas of physics. It often surprises particle physicists because we tend to do wave packets. Is because we tend to do wave packet scattering and then the late time to turn off. But if you have an environment that just sits there, if you interact with it even perturbatively weakly, small things can accumulate and give you problems at late times. And that's a generic issue if you're looking at making predictions at late times. Particularly if you're using, making inferences based on weakly interacting systems, it's very common, say in describing Hawking radiation, to think in terms of I have a free quantum field that interacts with. I have a free quantum field that interacts with a gravitational background, but I ignore all the interactions amongst the fields, the quantum fields. And a rationale for that would be that I get to make those interactions up, I'll make them small, I'll treat them perturbatively, and then what I'm thinking about when I do the free field case is the leading order perturbation theory. But that's always a dangerous line of inference if you're interested in late times, because perturbation theory will be suspect. And part of the story here is to try and back out what can you say reliably at late times without having to solve your. Reliably at late times without having to solve your theory exactly. And there are tools, the good news is there are tools for making late time predictions, even if you're limited to perturbative understanding. And so the sales pitch is it behooves us to learn those tools because these are tools that have been developed in other areas of physics, have been tested against experiments in many ways, and so they're very well and robustly developed, and we should be using them if we can. So, the application of those tools I'm going to use is to decoherence of polymeric fluctuations. So, let me, in my motivation here, set you up for that. So if you're thinking about where there's a fact of life that we see, there's things like the microwave background. There are correlations in the microwave background across different directions in the sky. And we'd like to understand that, the origins of those correlations in some physical way. And the organizing thing in an expanding The organizing thing in an expanding universe for describing things like that is the Hubble scale. If you look at the evolution of various quantities, they solve some wave equation, but it's typically true that the expansion of the universe acts as a friction. And so there's this feature that fluctuations are freezing. Fluctuations, if they become longer than the Hubble scales, will like to freeze there. So k over A is subdominant to K being the wave number here I've written in a momentum space, A being the scale factor of the A being the scale factor of the expansion, and H being the usual Hubble expansion rate. When this thing wins, it's acting like a friction term and it's making things freeze. And so you have this picture that during inflation, what will happen is that modes will be expanding. They will leave the Hubble scale. And when they do that, they tend to freeze. And then you're led to a late-time prediction for fields that are basically constants in a Hubble patch. They're constant because the expansion is ironing out all the wrinkles, but the constant could be at different values in different Hubble patches. Could be at different values in different Hubble patches because the actual constant you arrive at depends on the initial conditions that you came through, and different patches will differ on that. So, as we look back at the universe, so where this is the orange thing is the light cone, we're up here, we're looking back, we see slices at given times, and so we see bigger and bigger pieces of the universe as we look back. The blue circles are supposed to be the Hubble patches, and those are expanding during the expansion of the universe. So, you see that as we look back, these Hubble patches are the things for which things are Are the things for which things are during inflation, for example, are frozen at different places? And so it's like looking at a bank of TV sets. And if somebody had played you a bank, if you're looking at a bank of TV sets in a store window, and if they all play the same thing, you're thinking there's an explanation for that. And that's often what we're looking at in cosmology. So we look at the TV sets, the size of the TV sets is small compared to the size of the correlations, and that's the thing you're trying to explain. Then inflation and bouncing and all the other. And bouncing and all the other proposals. They're trying to deal with that given that the basic fact is that the Hubble scale, once you're outside the Hubble scale, not much happens. So this is the picture that Robert showed you, but turned sideways because I want to have time on the bottom length going up. The Hubble scale is expanding. Other scales during a radiation-dominated universe are expanding, but not as quickly. So they're actually, if they start off bigger than the Hubble scale, they end up being smaller if you wait long enough. And we see a range of scales, a range of lengths in the sky. Of scales, a range of lengths in the sky, and we traced them back. It wasn't long ago that they were actually super Hubble, and so then it's hard to make a dynamical description of what they were doing because they were basically all frozen out there. And the way you fix that is you change the extrapolation. It wasn't always radiation dominated. The inflationary extrapolation is to an epoch where the Hubble scale wasn't changing, and during that time, scales are growing faster than the Hubble scale. Scales are growing faster than the Hubble scale. The balanced version of this would have just had something tracking. Everybody's agreeing there was acceleration happening at some point. And then people, what Slavon Mukhanov always says is that, so people argue very heatedly whether or not we're talking about inflation or bounces, but everybody's agreeing that the fluctuations you're looking at in the end of the day were quantum fluctuations. And so that's the big observation. And it might surprise you at first because although the scales we're looking at now, now we're Scales we're looking at now, now we have a way in which we can think of them dynamically because now they're inside in the remote past the Hubble scale. It makes it sound like you're just moving an initial condition here to an initial condition here, and you haven't really made much progress, but then the remarkable thing about the quantum fluctuations is that, first, the classical fluctuations are being ruthlessly erased by the expansion, so that in an inflationary universe, the classical fluctuations are being ironed out very efficiently. So the naive participation. So, the naive prediction is that there are no initial conditions here. The amplitude of them is zero, and so that's what you should be seeing over here. But what makes that not the prediction is because the classical predictions are basically zero, the quantum ones can win even though they're small. And so the quantum fluctuations perk along here at a size set by the Hubble scale, and the Hubble scale is not doing much because it's basically a constant, and so you get a very scale-invariant set of fluctuations because the fluctuations are being controlled essentially by the modes as they cross the Hubble scale. By the modes as they cross the Hubble scale, and at different modes of different size are seeing at different times, but the Hubble scale is not changing, so they're seeing the same kind of scale-in-variant fluctuations on those grounds. So the story I want to tell is about why did those quantum fluctuations, when we say that they're a good description of what we see, what we mean is somebody, somebody, oops, I do it again. Somebody here examines the later universe and they assume that the classical fields that are describing the density Classical fields that are describing the density fluctuations are given in some classical distribution, and the statistical properties of that distribution are what would have been predicted by the quantum state here. That's what the agreement really means. But there's a conceptual problem of why did these initial things that are quantum, why should they be described by a classical story later in the universe? So the question I want to ask is, and many others have asked, and I'll give some of the partial references, is if you imagine thinking of the density matrix If you imagine thinking of the density matrix of the fields whose quantum fluctuations you're interested in, call them phi. If the density matrix, if I write it in a field basis, it's got some off-diagonal matrix elements, and if it starts off in a pure state, then it would have had this form in terms of the wave functional of the vacuum. That's what it means to have some sort of an initial state giving you quantum fluctuations. What it would mean to say that I have a classical distribution in field space would have been that this density matrix would have been, I would instead be talking about. I would instead be talking about some probability of the fields, which only refer to one field rather than two, and it would be the norm of this thing. So there's a side note here that I'm telling you stories about late time evolution, and one of those stories that's been developed a lot in literature recently that is similar to the story I'm telling you is talking about the late time evolution of this, the diagonal element of the density matrix, the probabilities that people are actually measuring when they look. Are actually measuring when they look at late times and they compare it to observations. There's a story there that at late times that can be described by stochastic inflation, where that probability is evolving according to some Fokker-Planck thing as if it was a thermal random walk. And the tools I'm talking about can be used to explain that as well, but I'm not going to talk about that here. Instead, I'm going to focus instead of on the diagonal elements, I want to focus on the off-diagonal elements. I want to ask why is it that the off-diagonal elements are evolving That the off-diagonal elements are evolving to something which is diagonal, and why are they doing it in field space? Which is what the assumption is that people make when they compare the observations. Now, there's two, people have been thinking about this since the 80s, and there's two really good reasons why you don't have to worry about what I'm saying. And these two reasons are true. So I'm not going to argue that the reasoning is wrong, but I'm telling you them to tell you why I'm going to tell you anyway what the evolution is. So the first reason that you should So, the first reason that you needn't think about what the off-diagonal elements of the density matrix are doing is called decoherence without decoherence. And the observation people make is that if you're measuring at late times density fluctuations, you're typically only sampling correlation functions of the field of interest at a given time. So you're looking at some expectation value of a bunch of fields, maybe at different positions, but at the same time. If you actually calculate that, it's a trace of your observable divided by times the density. Your observable divided times the density matrix, it only involves, because you're working in a field basis, or if you work in a field basis, all you see in this kind of a calculation is the density, is the diagonal elements of the density matrix, because this thing is diagonal in a field basis. If you did different times, that would not have been true, because fields at different times don't commute, but fields at a specific time do commute. And so the observation is you could calculate what the off-diagonal elements are doing, but no one cares because they're measuring what they're measuring are things that. What we're measuring are things that see the diagonal lens. That's a true statement. A second true statement is that things are behaving in a classical way. There's many notions of classical. And what I just described as being the diagonalization of the density matrix is kind of a decoherence classical. But a different notion of classical is WKB classical. So if you had a quantum system where you had a wave function, which had a, it had a lot of, most of its dependence on the coordinate is in the exponent. On the coordinate is in the exponent. So there's a big number lambda, and you imagine that you're in this WD limit where all the derivatives that are important are associated with this big phase. Then if I calculate the momentum, the canonical momentum for phi for a state like that, and I only differentiate the top part because I think that's the biggest piece because lambda is large, then I get this. But this is more or less saying that I've got eigenvalues for momentum, which are functions of position. That's a very classical thing. I'm kind of saying that if you tell me what the field eigenvalue is, Saying that if you tell me what the field eigenvalue is, I've also told you what the momentum eigenvalue is. That's the sense in which these states, a state that's like that, you could say, is squeezed in the sense that it's very classical. And what's going on is that the commutator of the momentum in the field is order one, and that's subdominant compared to this expectation value. So it's kind of a one over lambda effect. So there's a sense in which if you're ever in the WKB limit, that you're going to behave as if you were classical in this very precise sense. And that's also a true Steve. But I'm going to still tell you my story. I'm going to still tell you my story because two reasons. One is because I can, and the theory is telling you what this tension matrix does, so why not ask it what it does to the diagonal parts of the tensing matrix? The other reason is, supposing it's true that I find that the diagonal elements are going to zero very quickly, which is going to be true, people are interested in testing whether or not these primordial fluctuations have a quantum origin. Have a quantum origin. And so they're looking for ways to get around those first two things I told you: that decoherence without decoherence and the issue of being in a WKB limit. Those things say that it's going to be hard to see that there's a quantum nature to those fluctuations, but it doesn't say they're impossible. And so people are trying to design ways to see fluctuations, the quantum nature of the fluctuations, trying to get around those obstacles. But this is potentially another obstacle. If I'm telling you the density matrix is really diagonalizing on a time scale very fast compared to the, you know, Scale very fast compared to the, you know, when it's out in the wilderness outside the Hubble scale, that's something else. Even if you solve those first two problems, you've got to solve this problem. So part of the exercise will be you want to know how the quantumness of your system or the decoherence of your system depends on your parameters. So if you, by all means, people should look for the quantum, to test the quantum nature of prime order fluctuations. But supposing you find something, you want to know what did you learn? And so the goal is, the story I'm going to The goal is, and the story I'm going to tell you is to calculate the decoherence as functions of your inflationary model so that you can say, Well, I did not see that model, but I might have seen this model. Now, many people have thought about decoherence, and the decoherence story is the kind of system, what happens there is if you're looking at a small subsystem of a quantum thing, and you trace out the rest, then the system you're looking at can start evolving in this diagonal way. And there's a discussion as to what the rest is. As to what the rest is. So you said that people are looking ways to measure this coherence, decoherence without decoherence. So what are we looking at the whole spectrum at different redshifts to see if they're all... Yeah, that's one thing you could do is you'd look for fields at different times. You could look at the same time, not just look at fields, look at derivatives of fields and fields at the same time. And I think the spirit of it is that people are looking for some sort of a Bell's inequality that you could make from those kind of observables. That would be the smoking gun. Those are the kind of observables that would be the smoking gun of not being classified. So, and people, including Robert, have been thinking about this for a long time, and there's various proposals for what the environment could be. So, the main thing I'm going to talk about that's different is I'm going to have a very specific picture, which people often also talk about, but I'm going to calculate for you what the ecoherence is from a very specific environment. And here's what the environment will be. So, in this story that I was telling you, we're here, we see these scales, and this red band is supposed to be the range of scales that we see. Red band is supposed to be the range of scales that we see. We track them back, and they were outside the Hubble scale. At some point, inflation ended, and so if we go back further, then it turns out those guys go back inside the Hubble scale, and those are the ones we're going to call. That's the system. That's what we're actually measuring in this system. What I'm going to call the environment is everybody else, all the other modes in your fields, and that includes the long ones and the short ones, but the short ones are going to be the ones that are important. So here's an example of a mode that we're not measuring now, but is a short wavelength compared to the ones we are measuring. Compared to the ones we are measuring, and I'll be interested in calculating its behavior. I'm going to be only able to do it, it turns out, at a time, and here's a representative example of it, during inflation. So I'm going to be expanding in three small things, and it's all about control parameters in this business. And the three small things will be, I will expand in the size of the slow roll parameters, so that will only be a good approximation during inflation, so that'll limit me to the left of this line. I'm going to work in the semi-classical approximation as everybody does in this building. As everybody does in this business. And that's controlled. If you ask what makes that a good approximation, it's controlled by the ratio of the Hubble scale divided by Planck's scale squared. So that's going to be small. That's something which is typically small during inflation anyway. That's kind of one of the virtues of inflation. And I will be focused on super Hubble modes. And for those, K over A was much smaller than H. So this ratio K over AH is going to be small. And I'm going to work the leading order in it. Going to work to leading order in it. So I'm going to be mostly focused in this little diamond here. And the answer is going to be this: I have some measure for decoherence, delta, where delta is zero if and only if it's decoherence. It's a pure state, but delta can be arbitrarily large. And I'm going to track its evolution, and I'm going to tell you that if I expand in all of these three things, the leading piece is this. And so it's small because it's got a bunch of small factors, but it grows like crazy, grows like the volume of the universe. And so you might have. The universe. And so you might have thought that I could only trust this as long as this thing itself is order one because I'm going to be working perturbatively. And then this is where my story about resumming late times is going to come in. I'm going to tell you that you can probably trust this, although proving that is still a work in progress, but I'm going to argue that this can be trusted even when it's one or even when it's 10. There's a control parameter in addition to these ones which is allowing you to re-sum late-time behavior. Quick question, Cliff. Yeah. You said you were focusing on. You said you were focusing on superhumble K. Yeah. Although this bucket, HA over K is actually large. Right, so the K that's appearing here is the K of the system. And there's a separate K, which is the K of the environment. And so the environment does not have to be super hubble, except for one point in my calculation. I'm going to use it to be super hubble. And then when I do that, I'll be restricting myself to the range of environment that's in this little space here. That's in this little space here, and I'll be ignoring everybody down here. But I'm always, the guy that's appearing here is the guy for these red lines, the ones that we're looking at. Okay, thanks. Okay. That's where I'm going. I will have succeeded if I have convinced you that this is true and why those tools that give it to you are useful. But here's the open. So I'm going to first tell you the tools, that's the open effective field theories, and I'm going to sell them from the point of view of late-time. To sell them from the point of view of late time control, because that was a surprise to me, that these tools are useful just if you didn't care about horizons, they're still useful. And then I'll tell you the calculation of the primordial decoherence, and so I'm just going to basically do a straightforward application of these tools to a specific theory, and it's going to be the vanilla theory. I'm going to choose just the simplest single field inflationary model, and the interactions I'm going to use are the ones that have to be there in the model for it to succeed. The gravitational self-reflection. For it to succeed, the gravitational self-interactions of the inflaton and the metric. Super weak, probably the weakest interactions in the problem, but they're already going to give you a lot of decoherence. And so, other interactions that could be there, I'm not going to calculate. I'm going to implicitly assume that they're just going to make it worse. I can't prove that, but this is why I'm hoping that you guys can tell me there's a tool you could be using to show, like an entropy-like tool, that what you've calculated is a floor for how much decovariance you have. But I don't know how to do that right now. And that's where we're going. So, let's start with the tools. So let's start with tools. And I forgot to put my timer on. How much time do I have left? One minute? All right. So this picture was taken last night. That's the elk that we saw. The sun was setting. It's really beautiful. I felt like a bit of a boot camp going. A bunch of us went up and I felt like I was trying to. A bunch of us went up, and I felt like I was trying to go, go, go. We all got up there, we did calisthenics, and we did a bunch of push-ups. Okay, so the late time resummation story is about control of approximations at late times. So why do you need to worry about it? So what's the problem with late times for perturbation theory? The basic thing was what I told you, that time evolution in quantum mechanics exists. Doesn't matter how small that is compared to that. There's a T for which this is not this. That's the basic observation. That's the basic observation. But the fact that that's true everywhere in everything you know about physics is maybe less familiar to you, unless you're a condensed matter or an optics person. But here's a picture of a mundane thing that you look at as an undergraduate. This is a classic example of this. Photons, any one photon has a very, very small likelihood of interacting with any one atom. Electromagnetism is not a strongly coupled theory. But if you're in a regime where every single photon is either refracted or reflected, and nobody goes straight through, Or reflected, and nobody goes straight through. Perturbation theory is a really shitty description of that. And that's because perturbation theory says nothing happens, and then you change it a little bit. And so what happened was, you know, you hit 50 billion atoms, and then something happens. And so then you're no longer in a perturbative regime. You're in this regime where that is not a good description. You really have to do that. But the fact that we can say a lot about geometrical optics means you didn't have to diagonalize QED in an interacting system to say things. You can be much more efficient, and you don't have to work. Efficient, and you don't have to work at all orders in alpha, you have to identify a way of controlling the late time contribution of the low orders in alpha, and that's what these tools do. So, I want to take the sentence example of that, which you're all familiar with, and run the, oh, I should first before I do that, I want to make the point that these things happen in cosmology. And that the reason that I'm applying this to cosmology is that there's calculations that have been done because there's enough symmetry in the problem that they're doable. So, the one I'm going to focus on is this one by So, the one I'm going to focus on is this one by Thomas and Woodard from 2005. They took basically a scalar field in the desitter space and they gave it a little small self-interaction, lambda phi to the fourth. Without that, you could think of this as being basically the influx time, or a spectator scalar during inflation. And if you calculate things like the correlation functions of phi with itself, and here they're at the same point, you get some basic thing. If n is 1 is the one that you normally think about, you get this thing here which involves the log of a. Log of a, remember, during inflation is that's just t. Of A member during inflation is that's just t. That's the famous statement when n is equal to one that the fluctuations in phi squared are like a random walk, they're linear in time. And that kind of is the physical intuition behind this stochastic story, is that it's like a random walk. But then they calculated the corrections to it in powers of lambda, and they found something like this. And the point is that there's a coefficient here, which is also time-dependent. And the fact that this is a power of time is the way that the theory tells you. The way that the theory tells you that perturbation theory is failing. You know, you can make lambda as small as you like, you calculate the directions, and the theory is saying, but there's a problem at late times, and I'm telling you that by having this secular growth phenomenon. Now, Samus and Woodard did another nice thing. So they did this calculation just in the field theory. It's the long wavelength modes are the ones that are important. So they look at the infrared part of the field theory and they calculated the rate of change of phi to the 2n. Of phi to the 2n. They didn't actually calculate the explicit time dependence, but they calculated d dt of 5. So these violation of perturbation theory is in addition to the asymptotic growth of the asymptotic growth of the perturbation theory, these are in addition to the asymptotic theory. This is a diagnostic of that. So it's a sign that perturbation theory is failing. It's a sign that perturbation theory is failing at a late enough time. Just adjust uh the the pictures. It's the sitter space, it's a spectral field and it's just you're watching what happens if I spectrum. Related question, since you have logarithms, and A is a A is dimensionless. Dimensional, but whenever we have logarithms, there is always a question what is inside the logarithm, a cut on it. Yeah, so uh I suppose this calculation is infrared divergent. Calculation is infrared diversion, diversion is some infrared cutoffs, which should be sticked together with that. Yeah, and you're right that there's a very closely related set of phenomena that the sector growth is related to, but not the same as infrared divergences. And the infrared divergences you kind of see here, that this first statement that if I threw away this thing, just took the one term, phi squared being linear in time, you could say, why did that happen? Phi squared should have been a constant, because it's a decider-invariant system, it should be homogeneous. Invariant system should be homogeneous. And what's happening there is that if you just did the mode sum for phi squared, that sum over modes is diverging for small k. And it's when you do that regularization that you acquire the, if you cut it off some physical scale, that introduces n-dependence, and that's where the difference came from. So they're very closely linked phenomena. And it's true that the ways in which people think about this are often driven by that. So they kind of think about what's the um you know, are there tools for resuming long distance things which can be used to get the late times? Be used to get the late times. And there's a sense in which what I'm telling you contains those approaches. Question? Yeah. Just wondering whether metric fluctuations also exhibit such secular growth? I believe they do, yeah. Although I'm not sure how explicitly that's been shown. And if this secular growth happens, can it be resumed? I can it be resummed in a certain sense? I guess that's what you're about to tell me. That's my pitch. Yeah, I believe they can be resumed by the same kind of tools that I'm about to describe. But I think in the gravity case, there's a lot of issues that I'm not going to give you the solution to, but which I think will have solutions, like gauge dependence issues. And there's a lot of things you have to think about for gravity that are more complicated. But I think the logic of what I'm telling you should work. Okay. And so I'm setting up the resummation story here now. And what Samus and what it did is they calculated the rate. Now, and what Samus and Water did is that they calculated the rate of change of this. They didn't actually give you the formula I'm showing you here, they're giving you the rate of change of phi, and they're calculating that. They are calculating this, but they're calculating it for time intervals that are small enough that perturbation theory is working. But then they said, well, you know, stochastic inflation has been around for a long time, and it also predicts how these things evolve. So they said, well, there's a stochastic picture, which is basically based on this random walk kind of story of a leaving term, says that I should be thinking of this as being some sort of a Fokker-Planck evolution for the probability of five. A Fokker-Planck evolution for the probability of phi, where probability of phi is the probability of finding a value for phi for some Hubble patch, which is a statistical thing for different Hubble patches. And if you just ask for the mean and the variance of that thing, that distribution, to evolve the way we believe it does in cosmology, that gives you this kind of an evolution for p, where this is basically the derivative of the potential that we're assuming. And in this case, we're working at lambda pi to the fourth, that's why it's cubic. But it would have been more generally b prime. This is like a noise term, and this is like a drift term. So they said. So they said, well, this is giving a prediction for the evolution of phi to the 2n. I just take, you know, I take the expectation value, I differentiate time, and I differentiate that, and I use this, and I can predict what happens. And they found that they agreed. So they said that the direct field theory calculation and the stochastic calculation are giving you the same answer for this infrared part of the field. But what's nice about it is that the field theory calculation, you don't know how to say what happens at late times, but in the stochastic one, you do. To late times, but in the stochastic one, you do, if you really believe that the stochastic story is working to late times, then it's describing the relaxation towards a time-independent state. And the time-independent state you get by setting that to zero and solving this differential equation, and that's the solution. And it's basically for any v giving you something which is an exponential of v. And so it tells you why the secular growth happened. It's saying that supposing I had had a free theory, these fluctuations are coming into the horizon in a very Gaussian way. And if it's been a free theory, V would have been quadratic, and so they would have remained Gaussian. Been quadratic, and so they would have remained Gaussian. But when I put in the perturbation, if I have a 5 to the 4 theory, then this is something that's very non-Gaussian. And so the big thing that's happening is that the statistics of the fluctuations is changing very slowly, but over long times it's significantly non-Gaussian. And this kind of this sector growth as a diagnostic of something large happening is often interpreted in inflationary stories as a sign that the citizen space is unstable. And you kind of And you can kind of test that here because if this is what's going on, you can say, well, I can take that pop-up distribution and I can ask what's the energy density in it and see how big is the back reaction to do with this. And that's actually not very large. This, when you calculate the expectation value of T minu, just gives you something of order h to the fourth, which is just a generic loop size thing. So you knew that quantum corrections were going to come in at that level anyway. And so I would say there's no evidence here. This doesn't mean that the status space is stable. It just says that there's no evidence in these secular growth things necessarily. Evidence in these secular growth things necessarily that it's unstable because the issue of back reaction isn't large. Question? A is just a constant in your previous one? No, A is a function of time. With that the asymptotic growth with respect to lambda. Lambda being what the copy. The growth is happening, it depends what you mean here. So the phi to the 2n is growing in this explicitly. n is growing in this explicit way, that it depends on time in that way, so it's growing as a variable. So my question is, if I go to lambda to the power of n, will the coefficient grow as n factorials? Let me get three slides on and then ask that question again if I don't, if you're not having. I think there's a very precise answer to that. But I'd like to give it to you in an example, which is simpler to think about. So the answer will be that those terms will be there, but there's a subset of them which you can capture and which are dominating late times that are not all of them. Are dominating late times that are not all of them. So, and the example that's like that, that people are most familiar with, is exponential decay laws, right? So, you have some sort of a radioactive atoms and they're decaying. You know that the number that survives is exponentially falling. The coefficient is the mean decay width, their decay rate. And typically, that's a perturbative thing. So, you know, it's a maybe it's the weak interaction Hamiltonian that gives you the decay. So, it's something which is, let's say we had a coupling constant, G, which is some measure. Which is some measure of the weak interaction coupling constant, then gamma would have been some explicitly perturbative thing often at second order. And so, strictly speaking, if you're doing perturbation theory in the same logic that I'm using, you wouldn't really trust this, you'd trust this. All you're really calculating in perturbation theory is that there's a linear time growth in your survival probability. And that's, again, telling you perturbation theory is going to fail eventually. And so, what do you do? In this case, we think we know what we do. And so, I want to kind of pick at why, because I think the reason that works here is actually the reason that works in. Works here is actually the reason that it works in all of the examples that people have made for resuming late times. So I think it's kind of the universal argument that people are using in practice, although I don't think I can prove that as a theorem. So how does it work? So we know that things are breaking down at late times. So we take some time window, T0, and we know it breaks down eventually perturbation theory. But for shorter times, we know what's going on, and this is what's going on. But the important thing is that if we did that, we could have done that calculation for any, this was not a magical time, we could have chosen any time and done the same calculation. Time, we could have chosen any time and done the same calculation. So there's a bunch of overlapping windows for which the answer is always the same, the same gamma. So, what we really do for exponential decays is we say, well, we have a reason to believe that this differential evolution is actually a better approximation than the linear time growth, because this evolution we know, it just relies on the decays being statistically independent. If I can assume that that's true, I can derive this differential decay rate, and for small times it'll agree with what perturbation theory gave me. So, gamma, I can. Me. So, gamma I can identify by taking the small time limit of a solution and then comparing the linear and time thing and that tells me gamma. But then, because I believe this has got a broader domain of validity, the solutions to this have a broader domain of validity than the nominally small time solution that I used to derive gamma. And I'm using the fact that I got a broader domain of validity by differentiating n rather than just working with n directly. And so, what you're doing is you're basically using the fact that the evolution, there's nothing unique about where you started. You could have done it. There's nothing unique about where you started. You could have done it for a whole bunch of overlapping windows. And then the differential evolution was capturing the union of all those windows. And so we had, because of that, a much broader domain of validity. And so now this is where the answer to your question comes in. What's happening is that you're keeping, you're working essentially to all orders in g squared t, but you're dropping things that are order g to the fourth t. So there's lots of things that are higher order in g that you're not trying to calculate. And if you, so that means that this resummation you can trust for times that are order 1 over g squared compared to the characteristic time. But if you want to know. Time, but if you want to know things at order one over g to the fourth times that time, you have to work harder. And so, but you can imagine working harder, you can imagine calculating gamma to higher order in g, and then that kind of resums you farther into the future. But the thing that's making it work is you have a differential thing whose domain of validity you have to quantify. But once you've done that, then you have a systematic way to capture the time dependence in a way which is parametrically dependent on the couple of muscle. The kind of thing that would have screwed it up would be this. If the evolution had been something like this, which cares about the whole history. If the evolution had been something like this, which cares about the whole history, and it wasn't Markovian in the sense that the rate of change depended on what's happening at this time, and we could do that at any time, this kind of thing would not give you any more, the domain of validity of this would not be any broader than the domain of validity of the evolution of n itself. This whole thing smells like the renormalization group, right? When you first learn about renormalization, you calculate what's the change of scale of alpha, the fine structure constant, and you say, well, alpha is alpha naught plus alpha naught squared log of something. And that derivation relied on. And that derivation relied on alpha times log being small. But then you differentiate that, you say, well, now d alpha dÎ¼ is alpha squared, no logs anymore. And you say, oh, I like that equation. I'll integrate it. And I get an equation which should have had no more information in it because I just differentiated and integrated again. But now I get an expression when I integrate, which gives me alpha as a function of the correction to alpha, which are all orders in alpha log, and they only rely on alpha itself being small rather than alpha. Alpha itself being small rather than alpha log being small. And so, and that same thing happened: you differentiated it. In principle, you didn't learn anything, but you got an equation whose domain of validity was that alpha had to be small, and it didn't require the log to be also small. And so that logic of differentiating and integrating is a great one as long as you've got a reason to believe the differential one is better than negative. And typically, that requires it to be equally true anywhere. So you can kind of set it up at any time, and then it's a union of the domains of living. Am I correct in understanding that here you're we're getting like an entry into some non-perturbative function because you're taking a logarithm of n? You're basically populating b log n d. I don't think the logarithm was crucial, although I might be wrong about that. In this case, it turned out to be the logarithm, but I think it's true that the, what's important is that there's the rate of change, you can set up an evolution equation for the rate of change of whatever you're interested in in a way which is not making a reference to where you started. Is not making a reference to where you started. So it's telling you that at any instant in time, I can predict what's going to happen next. And it's equally good at any time. Yeah, in that example, it would have been. Yeah, but I think that there's examples. A subset of this is a dynamical renormalization group. And it's often true that that's an exponentiation, but it's not always true. So I don't think it has to be a log. But I might be wrong. All right. So this is all my way of setting up. So, this is all my way of setting up the open systems thing. And so, let's talk about open systems. So, open systems, as you all know, you've got some bunch of degrees of freedom, blue ones and red ones. You want to only, you just decide by fiat, I'm only going to measure things involving the red guy. I don't measure the blue guys, but I want to know how the blue guys affect the red guys. And so, the way you set that up is you have a Hamiltonian, one for the red guys, one for the blue guys, and one that makes them talk to each other. And you know that if I could calculate the trace. And you know that if I could calculate the trace of the full density matrix of the whole thing over the environment, the blue stuff, then if I could figure out that as a function of time, I would be able to predict the evolution of all the things that only involve the red guy for all time. So that would be a solution to my problem of how things, how measurements will evolve. So the problem is, how do I calculate the time evolution of that? And the thing is, you know that the full system is evolving by some Liu-Voe equation using the full Hamiltonian. So it sounds like the answer is stupidly easy. All you just do is take a trace of that. Stupidly easy, all you just do is take the trace of that. And the problem is that when you take the trace of the Liu equation over the environment, is that this right-hand side doesn't just involve the thing you're trying to solve for. And so you don't have a differential equation that you've set up that you can use to get the evolution of rho in general. But the solution to that problem has been known, it was solved shortly after I was born, so that was a long time ago, really, by Nakazima and Swansea. By Nakajima and Swanzig. So that was in 1958, and so I was one year old, and Swanzig in 1960. And basically, the reason that they were able to do it in generality is that it's a linear problem, that the evolution is linear, this is a Liouville equation, and the projection onto this system A, the red stuff, it's also linear. So, what you do is you take your evolution equation, and we just projected it onto the system we wanted to follow. You can also project it on the system we don't want to follow, and that's some linear equation. But it's linear, so you solve it. Equation, but it's linear, so you solve it. So you solve the environment as a function of the system that you're trying to figure out what it is. You plug that back into the evolution you want it to have. Now you've got something that just refers to the system that you're interested in. And you can do that very generally in perturbation theory. And this is what the answer turns out to be. If I take the interaction Hamiltonian to be a product of two operators, one in each sector, you can do something more general. Then in perturbation theory, that's a linear process that you can just integrate. And this is what the answer turns out to be. And this is what the answer turns out to be. The first leading term is the mean field approximation, that it's a Hamiltonian evolution where the reduced density matrix is evolving where it's weighted by the average behavior of the environment. So that's kind of something you were intuitively thinking about as being what the mean field description should be. But then the second order thing does not have the form of the Hamiltonian commuting with Rho. So that's where you first see things that are not like pure to mix transitions or thermalization happening. Or thermalization happening that you would not have seen in a unitary evolution. And it involves an integral over what's happened, weighted by a correlation function of the fluctuations of B around its mean value. And if you're asking kind of what's the domain of validity of the mean field approximation, these other things have to be small, essentially. And that's kind of what controls that in advanced matter physics, say. And now you can kind of see how this could be relevant to coherence. Coherence. At first order in the interactions, nothing is ever going to decohere, but this is something which can decohere you. This is the kind of thing that looks like it can take a pure state to a mixed state. But it's going to be something you're not going to be able to trust at late times. The late time problem comes in because of the integral here. If this thing has got a fairly, unless it has a very sharply peaked behavior, this integral for late times, it becomes an integral to infinity, and then the question is, does this thing converge or not? And the smallness of the cup. Or not. And the smallness of the coupling constant won't help you if this integral is diverging. And that's how you'd see sector growth happen in this kind of a thing. So we're interested in this term. But the problem is, and I'd like to be able to make some story about late times, but I can't because it's exactly the kind of thing that I can't use my earlier argument on because it cares about the whole history of what happened. And it kind of makes sense that it does because so far this equation is capturing the whole system. We've basically made no approximations except for the statement that we're only. Made no approximations except for the statement that we're only going to measure part of it. So it's as complicated as the original system was. So here's where the effective field theory part comes in: that things don't have to be simple until you have a hierarchy of scales. So let's look for a hierarchy of scales. And the way that that comes to you in this framework is if this correlation function, if that happens to have a characteristic size, so that when t gets peaked around t equals s, and it falls off with some characteristic scale, so think temperature or something, if it's a thermal bath, then if the rest of the integrand is very slowly varying compared to that, Integrand is very slowly varying compared to that scale, then you can tailor expand the rest of the integrand around s equals t, and then now it's going to depend only on rho at the time that you want the derivative of rho. That'll become a Markovian limit, if that's a good approximation. But whether it's a good approximation or not, you have to check. You have to ask, is this sharply peaked compared to the time scales over which the thing is evolving? And so when you do that, if it is a good approximation, then the integral becomes some factor. And if this thing did not depend on t0, I could use this equation to make my argument, that if Equation to make my argument that if I can make this, if this does not depend on t0, I can start it anywhere, and I can make the evolution for the next little window, I can prove that it's described by this, and then I can expect the solutions to be valid over the union of those little windows that I calculated perturbatively. And so that would be a way in which I could resume later times. And it would be the same kind of thing where I'd be working to all orders in g squared t, for example, where g is the government constant, but dropping g to the fourth t. Dropping G to the fourth. What is delta B? Delta B is delta B is a correlation function of B minus the X, B minus the average. Sorry, and the higher the extra terms, there will just be sort of more nested commentators. Yeah, right. Right. So so so so if if you have a system, the things you'd have to do to make this work is you'd have to check that it's true. You have to check that it's true, that this approximation is true, that it's peaked compared to the variation of the thing you're looking at. And then you have to check that this thing doesn't depend on T0. And then you have a chance. There may be other tools, but these tools, you have a chance of using them. So for decoherence, in particular, is where I want to go if I want to apply this kind of tools. You can kind of see that that linear order, you're never going to get decorence from that. You're never going to get the ignorance from that. So, you have to at least go to second order to see anything. And that's the main thing I want you to remember: is that when I get to power counting in gravity, that's going to be the guideline as to how big the effect should be. It has to be second order. Now, this is the kind of thing that's going to do what I told you I wanted to do, have the density matrix become diagonal. There's two A's there. If I take that, then the natural basis to try and evaluate this contribution in is the one that diagonalizes A. If I do that, what happens is that these various terms group into something which is like alpha 1 minus alpha 2 squared. Something which is like alpha 1 minus alpha 2 squared, where those are the two things I took the matrix element in. And so the evolution that you're led to is something like this, where the rate of change of the matrix element, the off-diagonal matrix element, is proportional to itself with some factor that's going to give you an exponential-like solution. And it's going to drive different, when alpha 1 minus alpha 2 is different, if f is positive, it's going to want you to be greater than 0. But when they're equal, nothing happens, so it doesn't say anything about the diagonal at all. So this is the kind of dynamics that will make things diagonal, and it'll do it in a basis that diagonalizes A, which is the interaction of the Hamiltonian between the Which is the interaction of the Hamiltonian between the environment and the system. Now, in inflation, this is what I'm going to use. And one question becomes: why is A the field basis? Which is what people are in practice using. And in inflation, what's going to happen is that basically every interaction, Hamiltonian right down, is diagonal on the field basis. Because of the squeezing of the states, the states, as they leave the Hubble scale, they become very squeezed. And I remember I showed you in that WKB limit that basically momentum and the position have the same eigenvalue if you're deep in that regime. If you're deep in that regime. And that's making it as if the field basis is diagonalizing everything, because the momentum and the fields are all diagonal there. So there's a very general argument that's been known actually since the 90s that if this story is to go through, it will be natural that the field basis will be the one that is the classical one. There's a story here, which I'm going to skip, which just basically says that if your interactions are local, there's a good reason why you might expect the answer in cosmology when you do this, when you solve this equation. You do this, when you solve this equation, we've got the solutions are exponentials. This thing here is controlling how fast the off-diagonal pieces go to zero, it'll have some local form, and there's a good reason why you might expect to find the volume there. And I'm going to be giving you a volume there, and so I'm just saying this because this general argument for local scalar interactions, it looks like the gravitational ones do the same thing. Although I wouldn't say that that has been proven, that it has to be true. But it's a general. Sir, can I ask the question, just to clarify? Yeah. If you're If your delta B's were like heavy modes that you were trying to integrate out and you were thinking of them as the environment and the system was some long wavelength fluctuations of some field, could be a scalar field. What would be the nature of the kernel, this thing that you said C of T S, which was supposed to be sharply peaked, would the delta B correlated be sharply peaked for a massive peak? For a massive field? It depends on exactly the situation, specifically the system you're talking about. It can be sharply picked. But if I'm just thinking about integrating out some heavy modes for some field theory and trying to write down effective theory in the usual sense for the long-wavelength modes. In the way the Wilsonian sense. What would make this? Make this equation that you wrote down for the off-diagonal elements, what would be different for the standard case versus the case in inflation where you say that the off-diagonal modes actually do? That's a great question. I know the answer to that question. So it's a great question. And so you're basically asking: so, when we do effective field theories, we're doing something very much in the spirit. We're looking at some subsystem which is the low energy states. We're ignoring the high energy states. So, why don't all these things happen? States, so why don't all these things happen there? Essentially, why doesn't the vacuum decohere around us all the time? And the reason is that what's crucial about effective field theory is the Wilsonian one, is the criterion you're using to separate your system from your environment is based on a conserved thing, energy. So you might also use baryon number or something else, but you're always using something like energy. So you're saying that if I start at low energies, that's my system. Then conservation of energy says I have to. Then, conservation of energy says I have to stay at low energies. And so I'm never going to be evolving into high energies. And so, because of that, you know, the natural basis to use is energy. So that means that I would need, in order to have these kind of effects happening, I'd need an off-diagonal piece in the energy, interaction energy, which links those things. And that doesn't happen because of the conservation of energy. And so it's a very special feature that if you take your system and your environment and you distinguish them. Take your system and your environment, and you distinguish them using the eigenvalues of the conserved thing, that you don't get, all of these things that I'm getting here, these kernels are all zero. You can also have light modes. It depends on how you set it up, yeah. But you could do that. So if I integrate off the write mode, I know that I get non-local interaction with the theory. And that's an important point because when people are doing things like quantum fields in curved space, they either often In curved space, they either are often making inferences based on perturbative arguments where they get intuition on a free theory, or they use general arguments that are based on Wilsonian reasoning. And things like locality are not true for open systems in the way they are for Wilsonian systems. And so you have to check all those things. And some people do, some don't, but it's true that you have to check. Okay, so the bottom line here is that there's a story, and if you actually did this for, you know, people actually do do this thing for thermalization, for example. Thing for thermalization, for example. If you actually calculate something interacting with a thermal bath, then the story I told you goes through, things decohere, and it depends on how you couple it to the bath. And the characteristic time is being set, in that case, by the temperature, and all the kind of stories that you're used to hearing about remain true. But the reason you can talk about thermalization times, even though you understand things perturbably, is because of an argument like this. Okay, so I'm going to close out now. So that's the framework I want to use, and I want to apply it. And I want to apply it to this cosmological primordial fluctuation story. And that's how I'll close the talk out. That equation, I forgot to say it, this Lindblad equation, this second order equation that I got to here, this piece basically in this piece, in the case that I can write this approximation is true that I've sharply peaked, this is a Lindblad equation. And Lindblad equations have been studied because they have the property that they preserve. Have been studied because they have the property that they preserve the trace of rho and the positivity of rho. And so you can kind of see that if you got anything, you had to get something that's satisfied that they, any good approximation has to have those properties. And so you typically are led to something like that in these kind of arguments. So that's why I'm going to later on talking about the Lindblad equation for a metric. Okay, so here's the system. So it's just me, vanilla, single-field inflation, Einstein term, no fancy gravity, and there's no higher gravity. No fancy gravity, and there's no higher derivative terms because I own it. This is going to be effective field theories up the wazoo. And we're going to expand around some background classical thing. And our metric is some FRW thing, and it's going to be some near decision limit. And we've got a Friedman equation. And it's at this point that you see the need for this. This is where I have three expansion parameters I'm going to get to. This is the first one. So the fact that I'm going to be in a near-decider limit is going to be important because I'm going to be. Desider limit is going to be important because I'm going to be systematically neglecting these kinds of terms, and that will be what restricts me to be making predictions only during inflation. And that's being driven by the background evolution. Then I'm going to do the standard fluctuation story. So we're going to look at the change to the background, and we're going to have the metric we'll have a change to the background. And there's various scalar contributions, psi and zeta, and there's a tensor contribution. And there's a usual story that says that all these various scalar things are those gauge things that are. Are those gauge things that are related by gauge transformations? And so there's going to emerge one scalar variable, which I'll either call zeta, where I'm thinking of it as being the curvature perturbation, and I'm going to choose the one, in that case, the principal, that is the one that's frozen at least times, when it's a super helpful moment. But I'll sometimes use a different set of variables. But here, I, like everybody else, I'm going to be doing a classical calculation, and I'm going to have to justify why this quantum system is well approximated by the classical. This quantum system is well approximated by the classical approximation. And what's often said here is they say, well, I just have to make this small because that's what I'm expanding in. And that's not false. But the thing is, the equations that are telling you how big delta phi is in terms of the parameters of your model. And in this case, the parameter that's making that stay small is that h squared over 4 pi and blank squared small. And that's the control parameter for semi-classical approximations. And if someone doesn't give you that, then it's very reasonable to ask why a classical description is describing the quantum system. Is describing the quantum system, because it needn't be true that that's small. All right, now the variables I'm going to track are delta phi. Sometimes I'm going to use zeta, which is that guy there, and it has other terms in a ticker gauge, but sometimes I'll use v, which is largely delta phi, but it also has other terms. And I'm going to jump back and forth, and they're the same variable and different changes of variables of one another. And this is going to be useful if I'm interested in looking at late-time time-dependence for super level modes. Dependence for super level modes. And this is the one that's useful if I want to count where the slow roll parameters go. Because there's factors of epsilon in the definition of zeta, which kind of hides some of the slow rolling of the field itself. But this variable will be the one for which it would be easy to count where the epsilon is going. So I'll jump back and forth, but they're the same variable when I get to know them. So now here's our story. This is our system. It's the range of modes that we see in the late time sky. And I'm going to be able to do something because something that's going to work for me later on in the calculation. In the calculation, is that these modes are going to be able to tell you the story of decoherence mode by mode. So I'm not going to have to mix modes. And so, because of that, I'm going to be able to follow these guys as a function of their co-moving wave number. And that's an accident to the approximation, the leading approximation that we're going to do here. And then, so now I'm going to divide the fields into the system, which is these guys, and then the rest. And in principle, that involves long wavelengths and short wavelengths, but the long ones aren't going to do anything interesting, so really in practice. Anything interesting, so really in practice it's going to be the short ones, it'll matter. So that's everywhere in my Lagrangian, I'll do that split for the modes, system, and environment. And now I look at the interactions that are there, and these have been written down since the dawn of time, and I'm using the version that Maldesein wrote down when he was doing the bispectral recalculation. And so it's the self-interactions of the metric and the inflaton, and they're basically driven by the expansion of the Einstein action, which has all these two derivative interactions. And so the organizing principle is. And so the organizing principle is: there's a cubic thing in the fluctuations, which is proportional to 1 over m Planck, and it has two derivatives. There's a quadratic piece, which is order 1 over Planck squared, or two derivatives. And all the interactions, they have more and more powers of the field, but they all have exactly two derivatives, because that's what's true of the Einstein overaction. But each field costs an M blank. And so remember, for decoherence, we're looking for something which is second order in the interaction. So it's going to have to be at least one of our m blank square. But because it has to be second order, you might have thought that over one of our implying square. Have thought at one or one or plane squared, I could have worked to linear order in something like that, but that's linear order, and decoherence will never come from the linear order piece, because that's always something which is Hamiltonian evolution. So it's going to be sufficient to think about the cubic interactions at order one over and Planck, and I'm going to choose the things that are least suppressed by slow roll, and those will be the interactions that we'll use to couple the environment to the, we'll see how they couple the environment to the system. So here's an example of what these interactions are. Again, we're using Zeta. This has involved three scalar fluctuations. Theta. This has involved three scalar fluctuations, so that's why I write SSS. Here's something involves one, two tensors and a scalar. That's SS STT, and here's one of the two scalars and a tensor. And there's also three tensors and a whole bunch of things. And all the dot, dot, dot things here, they either have morsel role suppression or they trade spatial derivatives. These partials are spatial derivatives. They trade them for time derivatives. And in the calculation I'm going to describe, I'm only going to track the spatial derivatives here. And there's a question, which if someone asked me down the road, I can talk about, but why is it good to drop the time derivatives? But why is it good to drop the time derivative term plus one term in the dot dot dots, which I am in practice going to do in the answer? Now we take this cubic thing, we write system plus environment, and we expand it all out. So you get system cubed, you get environment cubed, and you get all the signs of cross terms. And now you can go through and see which ones are going to be useful. So these ones are not useful because they don't couple the system to the environment. All I really care about for decoherence is what couples the system to the environment. So I get rid of those ones. These ones are not so useful because they These ones are not so useful because they involve two derivatives of the system and one of the environment. Remember, the system is the longest wavelength in the problem. So k'th grid is small, and the environment is a shorter wavelength, so it's got a greater k. And momentum conservation says that the three k's involved there have to add up into the triangle, and I can't make a triangle with one long side and two short ones. So that says that I have to have two derivatives on the environment and not have one on the system. So that's my idea for that one. And then the last one is a derivative on the One is a derivative on the environment is more costly than a derivative on the system because the derivative of the environment is, by assumption, belong to rate-like both. So, the dominant piece is this one that's left, which is involving derivatives on the environment, and the system is linear in the system, and the system is undifferentiated. And because this is linear, when I look at the evolution of the second order in this, I'm going to get something which is Gaussian again in the system. And that's why I can follow things mode by mode, because the modes are not going to miss, because in this approximation, the leading approximation, we're just talking about linear interactions. We're just talking about linear interactions. So, what about this is only some. No, and then this term I'm looking at is explicitly not Clausonian, because I'm looking at something which is going to deep over here in my space. All right. So now I take this interaction. I'm going to take this particular one. I'll get three scalars. Okay, that's showtime. Right, three fifty. Very close to the end. So uh I'll turn off the alarm just a bit. I'll turn off the alarm just as a little good. So here's the interaction. I'm going to now do that. Look at that thing that's quadratic in the interaction Hamiltonian. And I have to do a correlation function. I have to argue something's sharply peaked and the other things are slowly varying. Here is the interaction thing in terms of this variable v. And I'm using v because the slow row parameters are explicit in that. And you see that the interaction is explicitly proportional to the square root of epsilon. I'm going to need two of these things. So you're not surprised that the answer I'm going to quote you is going to be linear and epsilon at the end of the day. Be linear and epsilon at the end of the day. And this is the guy in the environment I have to look at the correlation functions of and ask: are they sharply peaked? And I'll just tell you they are. And that took a long time to show, but it's true. And it's not obvious, but you can work it out that it is true. And so what happens is that I can now do that Taylor expansion argument in this case, provided I'm looking at super Hubble modes during inflation and I'm looking at those two things. Those two things. And so when I do that, I get this Lindblad equation, and this is what its form is. It has some coefficient f, which is the integral over that correlation function. And it's got nested correlators of, or commutators of the field, the system field, and the density matrix. And I'm just separating this term. This is the term that looks like a commutator or something. This looks like Hamiltonian evolution. So whatever that is, that won't equate you. And I've left the first order term here just to make that point that this is a Hamiltonian committee of something. And this is really the second-order contribution that has the. Really, the second-order contribution that has the commutator form. And this is the non-commutator piece, and they're distinguished by the real and imaginary parts of that correlator F. So you calculate what they are. Let me first tell you about the imaginary part of F. You're expecting this is gravity. I'm doing a loop essentially in gravity. There should be ultraviolet divergences there. Why didn't I worry with that? Here they are. The imaginary part of F diverges. And in dimensional organization, there's an n minus 4 here. And there's a bunch of dependence on K and eta. And this can be renormalized in the same way that you can always do it in an effective field theory. That you can always do it in an effective field theory. There's a curvature squared term, and the eta dependence and the k dependence is eta is the conformal. They're consistent with what would be obtained by a curvature squared term. And the important thing is that they're all in the thing that's not decohering you. So if I'm only interested in decoherence, I only care about that. And so the issue is, does the real part of F have an emergence? And it doesn't. It's just given by this. It's ultraviolet finite, so it doesn't matter how I recognize things in the end, as long as I can make my normalization argument. And this is the answer. Normalization argument. And this is the answer. It's got the epsilon I told you had to be there, that's got the h squared over m plus squared, that has to be there, which is the loop. Then the rest of the stuff is what you calculate. I'm working in this limit where I'm deep superhubbles, so k over ah is small. And this variable's minus k eta is the same thing as k over ah. So minus k eta is a small variable at late times. It goes to minus infinity, it goes to infinity at early times. And so this is what you get. The first term here is the leading term. And these are all subdominant by powers of k eta. Subdominant by powers of k eta. And all the details, like what's the k for which I split the system from the environment, that's what k star is, or what's the time at which I assume things were uncorrelated, that's what eta in is. Those are all in the subdominant thing. The first thing doesn't care. It's a universal answer that you're getting deep in the consider space. The one plus two is, I'm looking at scalar modes here. One is the contribution of the scalar environment, and two is the contribution of the tensor environment. That's the only thing. And so the rest of it is just algebra. The rest of it is just the algebra. So that's the punchline. And now all I have to do is tell you what the decoherence is due to that. And this is what it does. So the variable, this is the purity, the trace of rho squared for mode k. It's one if and only if the state is pure. Otherwise, it's between 0 and 1 because the probabilities are all between 0 and 1. And I'm just parametrizing it in this way as a function of delta as a function of time. And I'm doing this because I'm basically doing a functional determinant, so you're not supposed to be. I'm basically doing a functional determinant, so you're not surprised to find you get one over squared or something. And so when delta is zero, this is one, and then delta is not zero, it's not one. And so delta is zero is the criterion for being pure. And I'm going to calculate what delta is from that previous expression. And this is the formula I quoted to you at the very beginning of the talk. And so it's got the epsilon that had to be there, it's got the a squared around plane squared, it's got this leading power of 1 over k cubed, and that's the thing, which if I write that as k over ah, is giving you the a cube that I told you is probably going to be there. And that's the thing that's growing like gangbusters turning. And that's the thing that's growing like gangbusters during inflation, because that's when I eat the 3HT for 60 E volt and 3H is 60. The lesson is, I've taken the weakest interaction in the problem, and here's where you pay that price. Epsilon is small, H over M Planck is small, and because this is an inflationary model we've applied to the data, H over M Planck, this combination is 10 to the minus 5, because that's the size of the fluctuations we see in the CMB. Epsilon, there's an upper bound on it because the bigger epsilon is, the more likely you shows intensor modes, which we haven't seen. The more likely you shows intensor modes, which we haven't seen. So it has to be less than, this whole thing is less than 10 to the minus 14. That's the price you pay by looking at the weakest interaction that had to be there. It's the weakest interaction, but it had to be there. So you had no option of it being there. And then this thing can easily win over this thing, because e to the 60 cubed is really, really big. So it just, it's very efficient, this diagonalization process. If I did the same thing for tensors, it's a similar story. I haven't, where we I haven't, where we haven't calculated the tensor-tensor-tensor contribution, but we do know how the scalars declare the tensors, and the only main difference is that there's no slow-row suppression because the interaction involved is not slow-row suppressed in the same way. So it'll be faster than it was for the scalars. So let me summarize. This is the bottom line. So the takeaway messages are: the calculation asks: what's the cohering quantum fluctuations during inflation? And the real answer is we don't know because it depends what the environment is, we don't know what the full theory is. The environment is, we don't know what the full theory is. But these are the tools to answer that question with because you can normally answer them perturbatively, but you can also, in principle, answer them for a longer time, provided you're in this regime where you can reason many times. The evidence that I'm showing you here is that short wavelength interactions of the fields that had to be there with the interactions that had to be there are already super efficient at decoherent things. It doesn't mean that you can't have quantum systems that are not decoherent. It means that you have to choose your model. Decoherence, it means that you have to choose your model in such a way that you, this decoherence that's generically there isn't there. And that means you have to make epsilon super small. But those models can exist. So there are observable consequences. I'm kind of the bad cop in a lot of these things. If you're going to be trying to find the evidence for there being quantum fluctuations that are driving progressive fluctuations, it's going to be hard, is the main message here. Because the generic vanilla models that people are using that are agreeing with the data are not the ones that are giving you the things that you like. Giving you the things that you like. But that doesn't mean they don't exist. But because you now know parametrically how the decoherence depends on the parameters, you can seek them if you're trying to find a system that would give you a coherence surviving until now. The other takeaway message, which is a bit of a downer, is that projector methods are always failing at late times. So if anyone is giving you conclusions at late times, these techniques in themselves won't solve problems necessarily of the things like information loss. But anyone who solves those problems, Information loss, but anyone who solved those problems has to tell you how they dealt with this because those page time is that is exactly the time where you'd expect there to be secular growth problems for gravitational strength interactions. And so that means that the bar for giving you a solution has to include this kind of a discussion of how you get back to late times in a controlled way. But the good news is those tools exist. So it's not all bad cop, it's not all Agent Smith. There's also a little bit of Neo in there. And the main message is that these resummation tools exist. That these resummation tools exist, and there's entire chapters in these books in open systems which are non-Markovian. So I gave you a Markovian story. It behooves us to, we should be exploiting these tools, because these are very well understood tools that are asking the same kinds of questions that it turns out we're asking. And so we're crazy if we don't use them. Thank you for your time. So one more question. Yeah. Uh so you basically neglect some blank suppress operators in the click. Yeah. Can you prove that those decohere system faster than Planck? No, and uh and I also can't prove that the electromagnetic interactions or other interactions that are bigger than Planck aren't faster. And another thing I didn't emphasize that, what I can't prove is all I really did is I showed you that, you know, I'm saying I, all we really did, but I'm All we really did, what I'm telling you about, is that in the story of length versus time, you know, you have Hubble scale is doing this, modes are coming out doing this, and inflation ended here. I'm telling you how big the answer is here. And what we care about is what it is here. And so I have no argument as to why it stayed small. So I'd love there to be some sort of an open system diagnostic for that, that I could say, well, if you have these interactions, then I can tell you that there's this thing that's growing and never shrinking. And I don't know what those tools are. And I don't know what those tools are. If those exist, I'd love to know. Krishna? In the division of higher mode and heavy and light mode, it looked when you go to B system, B environment, that analysis that you did, very similar to what we do in a constant. Excepting the same question that was asked before, then I gave this talk in Europe, and I was looking for a sad to see it then thing, and I did this in the UK, which is probably the wrong place to use. Messi was the example. Sorry, catch up, I'm not listening to you. Let me just set up that slide. Okay. Previous from with V S V systems in the 