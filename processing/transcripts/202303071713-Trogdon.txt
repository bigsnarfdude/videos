So after in based on what you see, do you think that random matrices can actually be useful in telling us something about algorithms? But I'm going to just focus on a couple cases and show phenomenon that I think are interesting and hopefully you agree. So this is joint work with Percy Daft, Su Sai Ding, Tyler Chen, that you know, and Elliot De Gang. Okay, so this is the So, this is the title of our workshop. I want to add maybe a little bit extra. So, my training comes more from mathematical physics than it does numerical linear algebra. And if you think these things are very separate, you should go read this paper, which is fantastic, which connects eigenvalue algorithms to completely integrable systems. So these fields, in my view, are intertwined. These fields, in my view, are intertwined. And a lot of what I'm doing here has techniques from mathematical physics under the hood, arguably, and also random matrix theory really was an output of mathematical composites. Okay, so what's my plan? So I'm going to talk about Lanchos and CG and the power method on getting off Michael's talk on sample covariance matrices. And I'm not going to try to compare them. And I'm not going to try to compare them. I'm just going to kind of compare the performance qualitatively. Of course, they're not solving the same problem. But I want to kind of show differences in behavior between the two methods. I'll touch briefly on Lanchos and finite precision effects. And then I'm not going to talk about this at all, but I want you to ask me about it. Okay, so what are our building blocks? So for me, the The real Geneva ensemble is I'll let it be rectangular, and I'm going to always scale it by square root of M. So it's an N by M matrix scaled by square root of M. And then there's a complex version where you just basically add two independent copies with one of them multiplied by I. And then we have, you know, I won't use this at all, but you can form the copy. I won't use this at all, but you can form the classical matrix ensembles by building up from these Gaussian matrices. The Gaussian unitary ensemble is just x and star, where x is a complex square gene of matrix. Okay, but what I care about for this talk are sample covariance matrices. And so the two classical ensembles are the real Wishart ensemble and the complex Wishart ensemble. And these are formed by XX transposer XX star for a rectangular. xx star for a rectangular Geneva matrix. Okay, and I have this square root of m that's inside there that sometimes people multiply xx transpose and then divide by m. Okay? So these are Gaussian matrices and x is independent columns. In principle, there's no reason why you should make that restriction. But first you do Gaussian and then you see what you can do from there. From there. So, a classical example that's kind of nice to think about is having plus or minus ones with equal probability. Okay? So I'll primarily focus on the Gaussian case, but try to give indications of what goes beyond that. Okay? And then, as we saw in the last talk, the Marchenko-Pasteur distribution describes the limiting eigenvalue distribution for these sample covariance matrices, and an important parameter for me, I guess. Parameter for me, I guess for most problems in this world, is this d parameter here, which is really the aspectation of the parameter. Okay, and so this determines the limiting support of the Marchenko-Pasteur distribution. And really, this convergence is that you have this empirical measure that converges weakly to your Marchenko-Pasteur rule. So let's talk about conjugate gradient in Wishart. What can we say? Art. What can we say? And I would argue basically everything. We'll have full formulas for every detail of the residuals and the errors. So I just want to also highlight that there's always, with CG or Lanchos, there's orthogonal polynomials under the hood. And right, we all know that you can recast CG as a polynomial minimization problem. Polynomial minimization problem. And really, you can write it in this form where you're integrating against this measure, which involves eigenvector and eigenvalue information. And this is something that we have dealt with a lot in the numerical linear algebra world, but this is maybe one of the first places it appears in the random matrix literature. I think there's some this is really a summary of the book that's really great. It's a book that's really great. It has references to other work, but this is kind of a comprehensive use of this eigenvector empirical spectral distribution. So now instead of weights of 1 over n on each eigenvalue, you have eigenvector information sitting on each eigenvalue. And this really is related to a resolvent, as Michael was saying. So, this is really an important new way to study random matrices. To study random matrices or looking at quadratic forms with the resolvent, which is what a Stilge transformer says. Okay, so right, there's more connections to mathematical physics here. So anytime there's orthogonal polynomials, you have some Riemann-Hilbert characterizations of orthogonal polynomials that are extremely useful for analysis. Again, ask me about that. So, wishing. So, wish art. What can we say? So, there's this beautiful work by Silverstein, expanded upon by Ioanna and Alan, where you apply Householder to a Gaussian matrix. You really do Golip-Cahan on the Gaussian matrix, and you get a bi-diagonal matrix, and you can completely characterize the distribution of the entries. You get independent chi's on the diagonal and the sub-diagonal. Diagonal and the subdiagonal with degrees of freedom denoted by the subscript. Okay? But, well, householder and Lanchos share something in common. If you do Lanchos with the first standard basis vector as your starting vector, it's equivalent to householder in exact arithmetic. So invariance is actually telling us that this is what CG does on a general right-hand side. Okay? So this is Now, the Cholesky factorization in a distributional sense of the Lanchos matrix that comes out of Wishart. Okay? From there, you can say a residual at K step is this product of independent chi's, or product or ratio of independent chi's, and the error at step K is this. So this is no approximation to distributional formula for the residual of the errors. For the residual of the errors. So, in a sense, you can say everything you want to say about Wishart as a conjugate gradient applied to Wishart and how it performs. Okay, and so Elliot and I did this there, but more is true. You, okay, that was for Gaussian entries. Now you play an argument of, okay, what if we replace Gaussian entries one by one with non Gaussian entries, satisfying some moment restrictions. Satisfying some moment restrictions, and you use the previous formula, and you can show it's universal, and it extends. So, this is true now in an asymptotic sense, as the matrix size tends to infinity, where you have maybe plus or, you can't do plus or minus ones because they don't satisfy that moment condition. So the formula changes slightly if you do plus or minus ones. But if you match the first four moments of Gaussian for your entry distribution, this will be your asymptotic limit for your observance. So okay. And this so then this is yeah, normal fluctuations. Okay. So we were particularly interested in understanding this halting time. So this is the number of iterations you need to hit a tolerance of epsilon. And we kind of summarized everything that we could think of in this type of a plot. So this gray curve is a whole bunch of trials. Is a whole bunch of trials, sample paths, if you will, of running CG on different samples of the Wichard distribution. The red is the mean, the true mean, which we can compute because we have the distributional formula. The black is the limiting form that we know in the large matrix limit everything's going to concentrate on. And what are these histograms in the background? So, this blue is So, this blue histogram label, this is the halting histogram. This is really the distribution of crossings of this green epsilon curve. So, really a histogram of all your halting times, your run times. And then this k equals 10, this is really the distribution of crossings of this vertical line, looking at the residuals you experience at step 10. Okay, and so this is with 25, matrix size 25. Now we go up to matrix size 1000, and it just concentrates on the limiting behavior. And you see that, okay, my halting histogram is basically deterministic. So I don't have a lot of time, but here's a different regime, just I need one more, where One more, where 20,000 samples, a 2,000 by 2,000 matrix, every single time it took, what, 22 iterations. I mean, there is a small probability, but it's exponentially small, that it deviates from that. Okay? You can do this with, let me just, in the interest of time, move along. So you can do this with non-trivial covariance with spikes. And so this is. Spikes. And so this is a situation where you have two bulk components for your sample covariance matrix and you have a couple outliers. So Lantros feels the outliers initially and then settles into a quasi-periodic oscillation. And once you understand what the orthogonal polynomials look like to the limiting measure, this kind of all makes perfect sense and that's how we calculated the limiting curves. So it's really orthogonal polynomials, limiting orthogonal polynomials that are telling me. Orthogonal polynomials that are telling me what's happening. And then this is actually true provided where you can allow k to grow slowly with the matrix size. So you can at least go into farther out in the iteration regime. We don't know what the optimal exponent is there. 1 sixth is correct. Okay, so moving right along. So now that was kind of the full story for conjugate gradient, at least full story for Wishart. Now what happens? For wish art. Now, what happens for power method? Okay, we're solving a different problem, yes, but let's just kind of see what happens. Okay, so again, we have concentration, again, because these are really quadratic forms. Power method, so we're going to converge. So, your moments of your this weighted, this VESD, are going to converge to moments of Marchenko-Pasteur. And so, you're still going to. And so you're still going to get concentration of your iterates of your power method as the matrix size grows. But things start to look quite a bit different, at least on this exponential log scale. So this black curve is what everything should limit to in the large matrix limit. This blue curve is still my halting distribution. And you can kind of see what we have to go to, now it's a 4,000 by 4,000. To now it's a 4,000 by 4,000 matrix, and we're still not even seeing nearly the same kind of concentration. It's very, very far from being a deterministic halting histogram for even larger matrices. So what's your condition for the former equation? Let's see, what did I do? So something that's not, let's keep going. When this quantity, so looking at true eigenvalue minus my iterate. True eigenvalue minus my iterate is less than epsilon. My largest eigenvalue is converging to a constant, so this is as good as relative, roughly speaking. This is power method now as household? Just vanilla power method. Right. Okay. So, right, and I kind of want you just to get this shape in mind. And so this, this is. And so, this is in the really the gap-independent regime that Joel talked about. Our largest and second-largest eigenvalue are effectively the same for such a large matrix. And so we're really seeing a 1 over k that you see from the gap-independent bound. Okay, but now if we said what happens for power method if we go into the asymptotic regime, where we're actually feeling the top gap, keep this shape in mind. Kind of keep this shape in mind. Well, we have a result in the asymptotic regime where you see a similar kind of histogram. This really says that this halting time after rescaling and appropriately converges in distribution to something random. So this is very much not concentration of the runtime. But I can't explain at this point why this distribution looks roughly like This distribution. It kind of has the same rough shape. So I think there's a really interesting transition that happens from number of iterations fixed, or really a fixed epsilon. As epsilon starts becoming smaller with the matrix size, you'll shift from in principal deterministic halting that you don't see into a regime where you actually get this convergence in distribution or something that depends just on the top count, limiting distribution of the top gap. Okay, one minute to, okay, so I guess my take-home message so far is that the concentration of algorithms, even if they depend just roughly on quadratic forms, applied to random matrices can vary wildly. And I think it does tell us something about the algorithm. Exactly what, I still can't really say. But let me say. But let me say two slides on Lanchos and finite precision. So we all know that Lanchos is unstable. And I guess one heuristic is that once a Ritz value effectively converges, you're going to lose stability. But we're converging to Marchenko-Pasteur. Lancio should be basically giving us the orthogonal polynomials. Orthogonal polynomial zeros interlace. So we should really not be converging to any eigenvalue. Should really not be converging to any eigenvalues. And so that was the intuition that Tyler and I used to start thinking about: well, maybe line chose is actually stable on random matrices in a sense. And so it is. It is forward and backward stable on random matrices using this kind of intuition. There's really the fact that the measure converges to something that has nice orthogonal polynomials is really what you need. And this plot here is showing in single percentage. Here is showing in single precision the difference between the floating-point entries in the Lanchose matrix and the effectively true entries in the Lanchose matrix. And as n increases, you have accuracy for a longer and longer number of iterations. But it grows sublinearly in the matrix size. The matrix size. And so our analysis at least captures that. We certainly don't have the optimal exponent. But basically, if you run Lantros on a random matrix and that's the only experience you have with the algorithm, you might think that it's going to be stable. But it's certainly not. And I guess maybe I have it on a slide. Yeah, I guess I have it on this slide. This is CG, but if you apply land. If you apply Lanchos to a matrix that has a spike model, it'll go unstable immediately, right? Because it's finding that eigenvalue, the Ritz value is converging. The other ones aren't, but that one's causing the instability. Okay, so running algorithms on random matrices may highlight important aspects. It feels like CG is an efficient algorithm while the power method is not. And somehow that is captured exactly precisely, I won't say. Exactly precisely, I won't say. But it also hides other aspects potentially, like this stability of lanchos. But I'm using this, and I encourage others, to use it to understand more and more the wider and wider classes of random matrices, to just probe algorithms, to just, I think it's an interesting area, interesting convergence of numerical linear algebra, computer science, and mathematical physics. Science and mathematical physics. So I'll stop there. Questions? So the classic Daniel bound for conjugate gradients says that you get epsilon convergence after log of one over epsilon times the square root of the conditional variables. So is that how these matrices behave or not? They, so if you. A, so if you, the bound, the limiting behavior you get here is a factor of two smaller than that bound. Or just a concept factor of two. Yeah, just a concept factor of two. That's right. You think you might be seeing something that has to do with the algorithm that you're solving as opposed to the algorithm that you're using? Um this is algorithmic. I mean I feel like it's just you know an algorithmic feature of what we're doing. If you ran two different algorithms for the same problem, do you still expect to see? That's a good question. So, what we have done, it's not going to answer the question, but we spent a lot of time thinking about what we call algorithm signatures. So, you run an algorithm, and on a wide class of distributions, universality kicks in and it gives you kind of a limiting distribution for that algorithm. And it kind of gives you a curve that's associated with the algorithm. If you run a different algorithm, you'll get a potentially different curve. Algorithm, you'll get a potentially different curve. So, concentration or not concentration, I don't know. But algorithms on the same problem can produce, different algorithms on the same class of problems can produce different limiting distributions for their runtime. But just to add to that, eigenvalue and solving linear systems are definitely very different. Basically, any algorithm that we pick, like, you are going to see log 1 over epsilon convergence for solving linear systems. Over epsilon convergence for solving linear systems. And on the other hand, eigenvalue, the leading eigenvalue, most of the algorithms we are going to try are going to have a 1 over epsilon, 1 over root epsilon convergence, and hence you will not see sharp convergence in distribution. You might get different limiting distributions, but this kind of phenomenon of sharp threshold is actually a problem in saying that. Right. Yeah, I mean, and in these problems, you can prove that, yes, both out. Can prove that yes, both algorithms have deterministic halting times. It's just the size of problem you need to go for the eigenvalue algorithm is probably the square of the size that you need for the linear systems. So, what happens if you run your algorithms on sparse random matrices? That's a good question. We haven't looked into that. I mean, I I think the natural place to go is to banded matrices. Matrices. Because there's at least, I mean, there's some at least conjectures, some theory that are telling you about what you expect for when you have, say, semicircle, when you don't, when your bulk statistics change. So that would be a good place to start. Often I think of analysis of an algorithm on a random internet. Of an algorithm on a random instance as like an average case analysis, which might lead towards a smooth analysis perspective. Is there any hope for a stability claim coming through that perspective? So I think in the sample covariance case, as you take more and more samples, as you change this aspect ratio, you're going to converge, you know, it's going to basically give you your covariance matrix in the large data limit without increasing the matrix size. Increasing the matrix size. So there's a chance where you could think of that as a perturbation of a deterministic matrix that is your covariance and potentially get towards a sweet analysis in that context. But we haven't pushed it. All right. Maybe it is time for dinner. So thank you very much, Tom.