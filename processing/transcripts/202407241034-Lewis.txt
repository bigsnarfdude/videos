Okay, well hello everybody. I'm um thanks Staphney for an excellent talk. I'm also a representative of the ethical part of the statistical, computational, translational and ethical. But bridging over to the translational as well. So I'm part of the Genomes People Research Programme at Brigham and Women's Hospital and the mission of that programme is to accelerate the implementation of genomic medicine. So I'm going to be sharing Medicine. So, I'm going to be sharing more sort of the ethical issues emerging at the coalface. So, ethical challenges. What are we talking about? In general, we're thinking carefully about the benefits of risks and benefits to the research that we do, and in particular, who do they accrue to? But I think a helpful thing, and sort of one image that I'm hoping that might stick with you, is that we've got. Is that we've got an underlying two by two matrix here portraying a former life of city management consultant here. So, usually in research ethics we're focused on thinking about risks and benefits, two main categories. We're thinking about the benefits to society that are going to accrue from the research that we might do, be that more basic research or more translational research. And then we're focused on the risks that accrue to the research participants. To the research participants. So, in this case, the people who are actually going to sign up to be a participant in a biobank. And the whole sort of human research ethics apparatus, which in the US is the sort of institutional review board system in every country, basically has something equivalent, is really motivated to protect participants in research from the harm. But it is two by two, and so thinking about these other two quadrants, which has been where a lot of the focus has been. Which has been where a lot of the focus has been from my world over the last decade or two. First of all, there's the sort of increasing pressure on the likes of yourselves or the people who are setting up the biobanks to think about returning value to the participants in the findings. So in Ordovas, for example, they're returning results, they're returning ancestry results, and they're also returning clinical action of the findings. Returning clinically actionable findings to all of their participants, and there's sort of an emerging ethical and legal consensus which is really pushing all people who are setting up new studies to do something up here in this quadrant. And I'm happy to talk about that in the discussion. Not going to be the focus of what I'm talking about today. And then what Daphne was talking about is my About is much more in this quadrant, which is the risks that we might and the potential harms that might accrue not to the participants of our research but to society at large, usually specific groups of people that we're worried about. And I'm going to be talking about some experiences that I've had in the eMERGE network, which I looked at up. It's not technically a biobank. Enough, it's not technically a biobank, it is a network of biorepositories. But we've heard eMERGE referred to many times throughout this workshop. And we're currently in phase four, we just finished recruitment last week, that's 25,000 individuals to whom we are returning genomic risk assessments, and they're each getting about 10 polygenic risk scores integrated to varying degrees with other information, including clinical information, into these big reports and a sort of partway through returning those reports. Partway through returning those reports for these individuals. There's 10 sites: the coordinating centre and various other partners. It was a $75 billion NHGRI-sponsored project that utilises what gets called embedded LCI. So Daphne already introduced LC. LC is ethical, legal, and social implications of genomics. And for those of you who don't know, the NHGRI has a federal mandate to spend 5% of all its funds on this type of research. And that in turn stems from the history of the human genome. Stems from the history of the Human Genome Project, where basically Watson realised that he couldn't get the effort off the ground unless he said, Hey, we are actually going to address some of these concerns that the public at large have with the prospect of doing this. So, let me tell you a bit more about MBTLC. So, there's usually two main parts of it, or here's one way of thinking about it, there's being two main parts of it. One is people like myself will run research studies, usually going out in Studies, usually going out and engaging with stakeholders, things like patients, providers, scientists, etc., to try and answer specific research questions. So in the case of eMERGE4, it really wasn't clear what clinical report designs would be preferred by patients and providers or how they might react to that information. So like how on earth should we divine the design these reports and sort of processes that go along with returning them With returning them in the sort of optimal way. And then the other sort of stream of work beyond these concrete research projects is to provide guidance on issues as they come up. And it's very key that this requires first identifying that there are ethical issues, which is probably the hardest thing in model. And what does that process involve? Well, first kind of working out what the whole picture is, and then collaboratively search. Collaboratively searching for solutions. So, a lot of herding paths. And an example of this that we faced was how to handle the differential performance of polygenic scores by group, which you've also heard a bunch about this workshop. So, just to give two very concrete examples here, I led a study where we interviewed patients and providers on the design of these reports, and we got from that study much more information about how those providers. Much more information about how those providers would actually use those reports with their patients. These three different use cases emerged from that. That came up yesterday. It's like, wow, what is going to be the clinical impact of these polygenic risk scores? So that was an example of a concrete research project. And the hope is that some of these research projects then inform network decisions, and that's the way they're immerged set up. And then here's an example of how we thought about this differential performance issue. Because there's underlying equity concerns, but also a load of practical challenges that we face. So a series of decision points. Again, not the focus of what I wanted to talk about today. Instead, I wanted to talk about something that we're struggling with right now that I suspect that various people Various people that either you or that you work with are also faced with, which is how we think about sharing the data more broadly that we collect from our participants. And my colleague Maya Savatello from Polombia and I have been corralling cats around this in particular. So, okay, so a network like eMERGE, we have all of these different sites who are collecting the bunch of data. Collecting a bunch of data. And then we have to smush it all together and into the data that the network analyses. And that is going to be non-human subjects data. So there's a process of going from human subjects data, aggregating it, and making it non-human subjects data. And then we have to make some decisions about what of that data we then share with the rest of you. So let's. So let's talk through some of the issues that came up. And something that I like about being part of these big projects is the sort of ethical thinking we're doing is not high in the sky. It's like we need to make decisions right now. And it can help indicate where we need to go. But in our case, as is the case for most such data these days, the model is controlled access. So what you're doing is you're going to So, what you're doing is you're going to upload your data somewhere where people who have a certain set of credentials can go and access it. And that goes through something called a data access committee. Most NHGRI-funded research is kind of almost forced to use AMVIL, but not quite forced to use AMVIL. And so that has its own data access committee that then becomes our data access committee. And as you probably all know, if you've ever gone for NIH funding, Going for NIH funding, before they send you any funds, you have to commit to how you're going to manage the sharing of these right up front before you've hit any of the problems that are going to come up downstream. So, I want to talk a little bit about how we handle some of these risks and then a little bit about how we're hoping to handle some of these risks as well. Okay, so. Okay, so minimizing risks from re-identification, because really the risks to individual participants can only accrue if people know who they are somehow, right? So our informed consent form, like most, will say something like, your privacy is very important to us. We've all probably read a sentence like that every time that we click, yes, I accept the terms and conditions. Thank you very much. Conditions. Thank you very much. So, what we do not say, and what basically nobody says these days, is we will de-identify your data because we fundamentally believe that that's not possible. So, huge massive technical efforts go into things that protect privacy. Actually, a lot of talks have included things about federal learning, etc. Learning, et cetera, and massive, massive, massive efforts. But this is the kind of bar that we're trying to reach. So, one question to you is, what is privacy? And this is actually a pretty contested concept. Is it something to do with secrecy and keeping your data secret? Is it something to do with control or access? And then Um and then uh what you know what what kind of norms govern that? Um and one way to think about it is like what could you reasonably expect as somebody who is donating your data to a bioxay about how that data is going to be used, who has access to the cypher. So we want it we want it we want to minimize these risks. We want to minimize these risks. So obviously, we're going to remove direct identifiers, but we also have to handle something called quasi-identifiers. You've heard of quasi-identifiers. Yeah, a little bit, right? So quasi-identifiers are those things which, in combination with other data points, could lead you to be able to triangulate somebody's identity. So even without their name and address, you can be like, oh, well, this. Oh, well, you know, they're this old, they've got this job, they live in this sit code. There's only a few people who have them. We can kind of work out who that is. And there's some standard approaches to do that. If you've got the resources and you've got the technical know-how, then one approach that lots of people are keen on is to basically sprinkle a noise and so differential privacy in case related that were in that category. And a more easier to And a more easier to implement a more standard approach is to quarter the data somehow. So, for example, you define age buckets, like if this person is between age 65 and 70. And you can do that before you release your data. An alternative is you can say to people who use your data, thou shalt not report on a cell size of smaller than X number of people. But it's very standard to roll up. It's very standard to roll up data. So when you're looking at data, you're not seeing the underlying values that the researchers collected, you're seeing these roll-up gaps. Here's an example. So I mentioned this the other day, but the way we think about race and ethnicity data, the way we collect that, is changing, is in the middle of changing, which poses harmonisation concerns. Concerns, but we, like all of us, are using the new system where you can select multiple of these top-level categories. And as you can see, you get a really big long tail of even in 25,000 people, you know, just two people identify with these three top-level categories, and then there are many. Categories, and then there are many other such where you've got this like big, long, potentially identifying tail. So, what should our strategy be? A fairly standard thing to do is to bucket everybody who takes more than one category into multiple, just say, you know, multiple race and/or ethnicities. But there are issues with that. So, for example, So, for example, you might be able to see from this upset plot that people who picked American Indian or Alaska Native, there's actually 68 people who identified just that way. And there's many more who identified in combination with some of these other categories. So, if we were to take that, bringing all the multiples together, we would kind of obscure many of the individuals who identify with that group. So, in the end, what we're going to do is we're going to roll up this part of the distribution into other combination of categories. So, point being that there are sort of concerns that come up with this rolling out process. So, okay, so that's you're reducing the possibility of re-identification. You can also might want to reduce risks of re-identification if that happens. If that happens. And by doing that, the main thing that one does there is to adapt sensitive information. So, my question to you is: what counts as sensitive information? And how do we decide what counts as sensitive information? So, in our case, what we did is we sent surveys to all of the sites, asked them for their thoughts on this, and how they would like to actually put that into practice. And then we had a discussion and we aligned on. Had a discussion and we aligned on what we did want to adapt. And during the process of this phase of the eMERGE Network, there were some pretty large political developments, including the overturn of Roe v. Wade and a lot of legislation targeted at gender-affirming care, particularly farmers. And what we ended up deciding to do was to redact anything in our data that could link anybody with. With something that might now be considered a legal bus. And then we had discussions about adapting various other things, but this is what we decided to adapt. And then finally, you manage access to trusted researchers. So in our case, that's handled by the Anvil DEGAP Data Access Committee. And then you make people tick the box to say, I have read and understood this safe use agreement. The safe peace agreement, which usually promises that you will not try to be identified by individuals. So, this is a sort of package of things that come with thinking about these risks to individual participants. And I'm going to return later to sort of the how this can be done better. But now I want to switch to this other type of risk, risk to public, and most bio-ec systems. And most bioethicists, Daphne is one of them, are more concerned really about this category than they are about this category. And the challenge is it's much harder to know how to deal with it. So what are we worried about? What types of harms? We might worry that there are research questions that are fundamentally problematic and just shouldn't be asked. So I invite you to reflect on whether you think that there are any research questions. Reflect on whether you think that there are any research questions that could be asked of the data that you regularly use that fall into that category. Or another slightly different category would be research questions that cannot be adequately answered, so it's actually irresponsible to ask them of this type of data. Like you're just going to get misleading results, so don't go there. Another category that we might worry about, well, so here's an example, like links between Like links between genetics, race, and IQ probably falls into the scaffold. That's probably the most obvious one that people are facing. But we're also worried about, just in the process of doing research, that methodological choices that we take, analytical choices, dissemination choices, that promote inaccurate ideas that make problem. Computer ideas that make it confident. And a lot of this isn't like researchers having any kind of malintent. It's just that these series of choices lead to health research. And unfortunately for the genetics field, a lot of the stuff that gets published ends up on nasty corners of the web like 4chan, where people are like, oh, you see, there are biological races, and you know, it's no wonder that blah blah blah would like. Know it's no wonder that blah blah blah whatever. So, a tragic case of that was the buffalo shooting back in 2022. And the gunman, the murderer in this case, had previously posted a creed online where he cited a bunch of genetics research, maybe even some, authored by some of you, in justification of his racist beliefs. So, the authors of that research didn't have any kind of maltext. They were just stumbling into these same kind of issues that we've been struggling with again and again. And the challenge for thinking about this type of harm is it's really hard to know. It's really, really hard to know how to analyze it. So, there's various stages of the research process, as Staphanie was highlighting, like Staphyne was highlighting, like, you know, some of us are actually recruiting research participants and going through a research ethics committee, like an IRB in the US. But in many cases, that's not happening. Instead, like people who will access the e-match data, you're going through data access requests to get at the data. And so maybe there are all these various different leverage points that we could think about to sort of try and tighten the net of some of these. And tighten the net of some of these potential for group arms. One of them might be these data custodians and data access committees. But unfortunately, they don't want to have any role here. These bodies see themselves as checking: are you a bona fighting researcher? Is it within the terms of the consent that the participants sign? So there are some things if you're a better resource stack that you could think about doing, including. Including flagging access requests that need closer scrutiny, either by users, research initiators, that is, the DAC themselves, or the public, maybe. You could require some sort of training. Anyone who's had to access all of us has probably been through that training. You can require adherence to publication policies. We're thinking about ways that we could do a lightweight require training, some sort of guidelines for use for our data, but it's not going to have much enforcement. But it's not going to have much enforcement. So this ends up being the picture. While we've got various things in place for us as a network, we're not very empowered to do anything once the data is posted for sharing that might help with these commands beyond some kind of guidelines for use. So let me close with saying with what an ideal would look like and here your profinities of Daphne's talk. So when I pose to you, like, what can I pose to you like what counts as sensitive data. Like, should we be the ones deciding? This is an example. Or, like, who should be the ones deciding what counts as sensitive data? And I think the answer is that we need that sort of deliberative democracy, community participation to help guide us in that process. So, I think this is a really powerful thought that Barbara Koenig and others have kind of articulated, which is Articulated, which is that the likes of myself, a lot of what we end up doing is putting more emphasis, more words into the informed consent document, and that's not the way to go. That's just not the way to go for all the obvious reasons. And instead of doing that, we should consent to be about giving up control to accept a set of procedures for your data to then be governed. It's consent to be governed. It's consent to be governed. So, the biggest need here is for robust data governance strategies guided by the right stakeholders, resourcing the staff with the right issues lagged for their attention. So, this is the more aspirational piece that I at least have been convinced by through these sort of on-the-ground experiences. So, I'll stop there and I'll thank some of these collaborators at eMERGE, including Meyer, Malia Fullerton, Jody Jackson, and various other co-conspirators. Various other co-conspirators, and my primary mentor, Robert Green, and my funding, which is from the K-95. Thanks very much.