Reinforcement learning. Okay, floor is yours. Thank you. Thank you for the invitation to come speak here. I'm going to talk about something that's very closely related to, I think, many of the talks I've seen, including the immediate previous one, but from a slightly different approach. So I am a computational neuroscientist, so I think a lot about animals, but also specifically how animals interact with the physical environment. The physical environment. And so the topic for today is thinking about turbulent plumes, not from the perspective of very precise, beautiful, numerical simulations of these kinds of structures, but how these plumes are also relevant for biological animals that have to interact with them. So in fact, if you really think about it, odor plumes, especially these turbulent odor plumes that you can see with all of these multi-scale structures in physics, are beautiful and we want to understand them from the physical perspective, but they're also really Them from the physical perspective, but they're also really important for animals to be able to interact with in a really real way. Specifically, following these odor plumes to find their source is a behavior that is incredibly ancient and ubiquitous among all different kinds of animals, from single-celled organisms to the size of whales. So this spans multiple size scales of bodies of animals that actually care about finding the sources of these odor plumes. And this is important because it's a And this is important because it's important for everything that's important to an animal about survival and reproduction. The source of the odor plume could be a piece of food that they would like to find, it could be a mate so that they can reproduce, and it could be a cue for how they can get back to a nesting site so that they can find sleeping bike shelter and be safe. And so this capability of following turbulent odor plumes is something that we've seen across all different kinds of life, not just animals. Of life, not just animals. Like I said, this applies to single-celled organisms as well. And so it's something that's been of intense interest for biologists at this intersection of biology and engineering for a really long time. So what we do know from looking at animals performing this behavior is that there tends to be a strategy that they use. So here's a picture that's taken by a friend and collaborator, Floris Van Bruegel, where this is literally a Where this is literally a time-lapse picture of a single fruit fly as it's flying towards the strawberry at the very bottom of the screen here. So, the strawberry is emitting rotten strawberry odors, which we cannot take a picture of, unfortunately, not straightforwardly. But we can take a picture of how the fly is trying to localize this fruit because it wants to go there to eat it and perhaps lay its eggs on it. So, what we see here is something that's again really universal among sperms. So among from sperms to sperm whales, we see animals doing this behavior where when they detect odors, they will do a surging behavior where they go towards, they go upwind. And when they lose the odor detection, they will do a casting behavior where they go back and forth until they happen upon the odor again. So this is called the cast and surge algorithm. And at least, you know, there's lots of variations depending on the exact organism you're looking at, the size scales, the relative size scales of their body to the turbulence. Body to the turbulence. But this is kind of a general algorithm that's been observed many, many times. And so we're going to dig into this a little bit deeper. I'm going to be primarily talking about one paper today. This is the pre-print version of it. She just submitted a revision to this thing like yesterday. It was led by a very talented graduate student, Saaprud Singh, who has since graduated and now works at Meta. And our collaborators in this are Flores Vernbrigal, who's Canonical Engineering, University of Nevada Arena. Engineering, University of Nevada Arino, and my colleague, long-term collaborator, Regina Shrau, who is a computer science at the University of Washington. And so, the way we were approaching this behavior is that it's been studied a lot from both the algorithmic level, by computational scientists, as well as from the biological level, by all manners of behavioral biologists, neuroscientists of all kinds. And so, we were actually trying to approach it from a normative perspective, from reinforcement learning, by posing it as a task that an agent would have. As a task that an agent would have to solve, proposing some kind of architecture and trying to think about: okay, not only how do animals do it, how ought they do it? What is the correct solution? What's the optimal solution if you were to try to optimize it, given only locally available information, right? Because the animal can't have a highly resolved mesh simulation of the turbulent plume. It can only manage to do this behavior based on what it can locally measure from itself at one point in space. So to be clear, So, to be clear, this kind of behavior, this cast and search behavior, only applies at relatively long size scales. Once the animal succeeds in getting close enough to the origin of the odor, then vision comes into play to actually see the thing that it's trying to get to. And so vision takes over and that becomes a dominant source of sensory information. But when it's far away enough from the object, it can't see it. And so that's when it's really relying on odor sensing as well as mechanosensing in the form of wind direction. Form of wind direction. And so I'm talking about it in terms of turbulent plumes in the air, but it's actually a lot more general than that. You can also, again, depending on the scaling, you can think of it as a plume of chemical in the water if you happen to be a whale or a fish or something like that. If you are a terrestrial animal and you're more constrained to a surface, then the similar strategies actually have also been observed for terrestrial animals following odor trails that are on the ground. And so you see here diagrams. And so you see here diagrams from an ant and a rat who have the same behavior where they're sniffing on the ground and kind of going back and forth, finding the trail, following it. And when they lose it temporarily, they'll sniff around left and right. It's casting behavior that we will see. But going back to the problem of doing this for airborne odors in the air, the problem is actually a really difficult one because if you had a concentration odor gradient, then you would just follow the gradient. Um, then you would just follow the gradient, right? You go up gradient, and that seems to be a very robust strategy. Um, but in in these in these turbulent plumes that are carried by air, the odor packets tend to be shredded into this really complex multi-scale structure so that the odor encounters by an individual agent or animal in this environment is intermittent. In other words, computing a gradient is basically impossible. There's no trail to follow because the entire thing is at vecting in the air, so you can't follow it. That vecting in the air, so you can't follow anything. And you are not actually encountering reliable odors, you're only encountering them sporadically. And even if you're in the middle of the cone of the plume, it's entirely possible for you to not encounter any odor sensory cues for quite a while, right? And so that's what makes this a challenging problem. In addition, especially in the real world, wind direction can change unpredictably. It's not guaranteed that if you simply head up wind, that's where the piece of food, the little That's where the piece of food, the little strawberry, had been. And also, the plume statistics may vary as well. And so, solving this task, what we do know is it requires multi-sensory integration. You have to be able to detect odors, as well as some minimal sense of where you are in space and something about where the wind is blowing. And we also, because of the intermittency, the sensory information, know that it requires some kind of memory. So, people have known this for a long time. So, if you were to try to think about this, To think about this, especially on the long scales that we care about this behavior in the natural world, you're really talking about experiments that biologists find difficult to do, not because they're actually necessarily difficult, because they're logistically difficult. So we're talking about even for the order of a fruit fly. So picture a fruit fly, the kind that might be buzzing around your fruit right now in the anywhere you might have fruit in a warm environment. They're tiny, they're like a couple of millimeters, right? But in fact, in real life, Right. But in fact, in real life, these millimeter scale insects can actually manage to navigate and find localized odor plumes on the order of meters to kilometers. And so here's just a couple of examples of what people have tried to deal with, the size scale of the past that's being solved here. One approach has been to use wind tunnels. You can actually construct a wind tunnel again on the size of meters, on the order of meters, where you can control the odor plume, actually visualize it, and then use. Actually, visualize it and then use high-speed video tracking to track these insects and their trajectories as they try to localize the plume. But really, they can do this for way longer distances. And so there's been these long distance dispersal experiments. Here's the most recent one that our friends have done where they went to a dried lake bed in the desert in California and then released, I don't know, like millions of flies in the middle and then trapped them with a circular ring of traps about a kilometer. Circular ring of traps about a kilometer away. And you actually were able to track, collect a bunch of them in this way. And so we know that this behavior is incredibly powerful, even for flies that are a few millimeters themselves, they are able to travel at least kilometers in search of odors and track them that way. And so the way that we're going to approach this problem in this particular project is to think of the animal interacting with the world in terms of actions and then. The world in terms of actions and measurables. The animal gets to do stuff, make decisions about where it goes, and it's going to make those decisions based on the history of measurable information from the world. So the first step in doing this is that we're going to make a turbulent plume simulation, which is embarrassingly simple compared to some of the other talks at this workshop. We're going to make a really simplistic set of assumptions. And we're doing this primarily for computational reasons. Doing deep reinforcement learning is itself Doing deep reinforcement learning is itself pretty computationally intensive. It takes a lot of computing power and also a lot more hypergrammer tuning. And so we really needed our physical simulation to be computationally tractable. And so, you know, I'll just say that just up front, especially to this audience, the simulation itself is currently very simplistic because it takes zero time to run. So of course, we would like to explore the possibility of challenging our agents with more realistic simulations in the future. It's just going to take longer and or take more. It's just going to take longer and/or take more computing power. Okay. So, our turbulent plume simulation makes the following set of assumptions. What you're going to see is a series of pictures where the origin, the cross, is always the origin of the odors. That's the right answer, where the agent is trying to get to. And this is going to be a two-dimensional arena. Odors are emitted as packets of odors, and they're emitted as Poisson events. So, they're unpredictable in time in terms of their intermittency. They are then advected downwind. They are then advected downwind. The wind direction is usually. I'm going to draw this little diagram here where there's a little circle with an arrow in it. That is the current wind direction. In some simulations, it's constant. In other simulations, we change the wind direction as well in the middle, unpredictably. And as these odor packets are affected downwind, there's a little bit of dispersion. And then there's also diffusion, where they become more diffuse in time in a circle. Okay, that's the entire simulation. These are the odors that our agent will be challenged with. Our agent will be challenged with. And so the agents themselves have the following architecture. So they receive sensory inputs, which is local wind direction, and that's a vector, xy direction, and local odor concentration at wherever it is in space. And so that's three numbers. This is a state with three numbers. This is going to get put into a recurrent neural network, an RNN, which computes. We train this thing using a re. We train this thing using reinforcement learning, and then the entire thing has an acture-critic infrastructure where the outputs of the recurrent, sorry, actually the full state, the full state of the RNN gets fed into two arms, the actor arm and the critic arm. Both the actor arm and the critic arm are actually just feed forward multi-layer perceptrons. So this is a two-layer neural network, but only feed forward. There's no recurrence in the actor arm or the critic arm. The critic arm, its job, is to estimate the value. And we train this. The value, and we train this using reinforcement learning, and then during execution, it's only the afterarm that it outputs the actions. And so, the agent has agency over two numbers, which is does it move forward? Does it try to flap and go forward? And does it turn? And we restrict the range of possible values for moves and turns scaled roughly to the order of a food fly. So, these are pseudo-realistic in terms of how fast they can move forward and how much they can turn and how quickly. Okay. They can turn and how quickly, okay. And the arena again is scaled to let's say the size of your kitchen, right? So it's on the order of meters. So just kind of imagine this is kind of thing where you might be a fly across the kitchen. There's a piece of fruit on the other side of the kitchen that you might want to eat at, and you're trying to find it before you can see it. That's the task. Okay, that's the order of the task we're talking about. Okay, so how do the agents do? We here have here, I'm going to show a series of movies. On the left-hand side is the movie. hand side is the is the is one one example of a trajectory of an agent after it's been trained and what you can see is that it's uh it's kind of bumbling about um and we've illustrated the history of this trajectory you see that this plume actually does change has already changed direction several times during during the simulation and the green green is where the agent actually encounters some kind of odor and blue is when it has not encountered any odor so you can see it very much recapitulates this behavior It very much recapitulates this behavior that I talked about in hand-wavy terms earlier, where it knew it knows nothing about that theory. But just by training it in its environment, it has recapitulated this cast and surge strategy. So you can see there, that was the turn, right? It's like, oh, I'm lost. There's no odor. I'm going to turn around in this big circle until I find it. And then once it succeeds in getting close enough to the plume where the plume is pretty tight, it tries to stay inside the plume and follows it to the origin of the odor. Close it to the origin of the odor. Okay. All right. So, so far, so good. We have largely recapitulated the qualitative behavior that we think we were looking for in, that has been observed throughout all of biology. Now, what's really cool about this way of approaching the problem is not only have we recapitulated the behavior, the macroscopic behavior of what we think an agent of this kind ought to do, we now have full state access to exactly what the computations. Access to exactly what the computations are going on on the inside. And so, simultaneously to looking at the emergent behaviors, we can also look at all the details of the neurodynamics by interrogating the history of activity of this recurrent neural network that we've trained. And so I'm going to be talking about that a little more as well. So I'm going to be unpacking both the behavior as well as the neurodynamics for the interaction of this agent with different plumes. So here again are a couple of things that we can observe about the behavior. Things that we can observe about the behavior. We see that there's two, there's three distinct behavioral modules. Here again is another agent where you can see that it's doing this thing where it was lost, so it does this big loopy thing, right? It comes back and tries to stay in. As long as it stays in, it just kind of goes upwind. So this movie here is kind of the easiest possible scenario where the wind does not change directions. You just really have to go upwind. And the density of odor packets is. The density of odor packets is also relatively dense. So, we have a variety of different examples where we can basically crank down the birth rate of the odor packets, also turn down the dispersion rate. And basically, there's a variety of parameters that we can change in the physical simulation to make the odors more and more intermittent and to make the change, to make the task there correspondingly more and more challenging. So, here's a successful case. Here is a case where it was not particularly more difficult, but the agent just totally got lost, right? Just totally got lost, right? So it's, you know, it's like it's nowhere close. It's doing this weird thing where it's turning around a tight little corkscrew. It's totally hopeless, right? Like it's not going to make it. Sometimes when you're doing a research project, that's what it feels like to me. It's turning around the circle, but you're nowhere near where you're supposed to be. Anyway, we all get lost sometimes. So I'll make one note here. So it's possible that some of you may be familiar with a larger class of algorithms that have been developed to solve this larger class. developed to solve this larger class of problems. So there is an algorithm that's very, very has been very influential in the literature called the Infotaxis algorithm. And Infotaxis is the name of it is in parallel with chemotaxis, which is an even more ancient algorithm for following concentration gradients. So infotaxis was developed as a solution to the same partially observable market process, POMDB process that we're solving for the case where the packets information are intermittent and also unreliable. Are intermittent and also unreliable. So, I will make a couple of quick comparisons between what we have done to the input taxes algorithm. I'm happy to discuss this in more detail, especially if anyone's interested in the nuts and bolts and the guts of exactly what the differences are. The key differences are that our deep reinforced learning policy that we train is able to recapitulate the CAS and search behaviors in a way that only relies on. In a way that only relies on locally available information, whereas the infotaxis algorithm kind of implicitly requires that the agent has some representation of the entire arena, right? So here we are only using local information. In InfoTaxis, the agent needs to have some notion of the extent of the arena and have some memory of the parcelation that's involved there. And the second biggest difference is that the info taxes assumes that the agent is able to perform a particular task. Agent is able to perform a particular type of Bayesian inference for updating pieces of new information coming in. There's been tons of work. This paper came out in 2007. So there's been tons of work since to relax and soften that constraint. So there are other types of inference that are still compatible with this behavior. But we've sort of taken a different approach altogether by saying, okay, I don't really care what kind of Asian inference update you're using. We're just going to have this be inside the neural network architecture. And so, in that way, will we construct it as an agent that Will be constructed as an agent that is just intrinsically a little more interpretable. So there's nothing in what we're doing that cannot plausibly be implemented by a local agent. And we make no assumptions about what kinds of inference it's able to perform. Okay, so interesting fact. So by analyzing these agents and their behavior, we were able to come up with a really interesting observation that we believe to be a testable hypothesis. So agents. So, agents are affected by the wind. So, they're at the whim of the environment. And it seems that they, when they're surging forward, we observed that our agents track the plume center line rather than the wind direction. In other words, if the wind direction, the current instantaneous wind direction differs from the plume center line, from where the plume is actually going, then the agent's actually tracking the plume rather than the wind direction. So, this was something that was a small discussion in the literature because people had. Small discussion in the literature because people had observed different flying insects doing one versus the other. So we have some normative evidence for one versus the other by observing our agents. Okay, so the other thing we can do is again dig into the neurodynamics. So by looking inside the guts of what's happening in this RNN, we can actually reduce the dimensionality of this activity. In this case, it's 64-dimensional. We observed it's relatively actually low-dimensional. We observed it's relatively actually low-dimensional here. I'm projecting it onto the first three principal components. You can actually do more, you can do less, it doesn't really matter. But this is just to make the point that the neurodynamics are actually relatively low-dimensional and that there is representation in here of things that we know to be important to animals that are performing this task. For example, there is the head direction. So there is representation in the neurodynamics of which way the artificial agent is actually heading. So again, this is not. Is actually heading. So, again, this is not something that's sensory information, something that they have to compute and integrate in time from their own self-motion in order to represent it. And we know that this is something that fruit flies and presumably, you know, it's not just fruit flying. I mean, rats do it, we do it. All animals that navigate in the physical world seem to have a representation of where we are headed. Okay, so that's something that pops out of our neurosimulations as well. We find a couple of other things like the exponentially weighted moving. The exponentially weighted moving average of the odor concentration, right? So it's given instantaneous odor concentration, but it seems to be keeping a memory of a short time history of the odor history, which is really important if you're trying to follow a plume and it's intermittent. We find evidence that it might be counting the steps since the last plume encounter, right? Because that's really important, not only what was the odor history, but also how long has it been since I last smelled something? That's an important piece of information. Thing. That's an important piece of information you imagine that you might want to represent. And then we also find some representation of other related features and variables, like a weighted moving average of odor encounters, right? Just counting how many odors that we've encountered in the past. So those are kind of like correlative things that are in there. And the other thing we get to do is we get to actually look into this idea. Look into this idea of memory. So the RNN architecture is wonderful. It's harder to train, but it actually just implicitly has memory. So we didn't have to force it to have memory. Here we were actually using, for those of you who care, these are vanilla RNNs. They're not LSTMs. They're not GRUs. They're actually just vanilla RNNs, which made them harder to train, but it actually made it easier to analyze afterwards. But we also compared the performance of these. The performance of these RNN agents with if you had just replaced that RNN unit in the deep reinforcement learning agent by multi-layer perceptrons that have different time histories artificially fed up to it. So MLP2 just means a multi-layer perceptron of time history two. So the current instantaneous step and the previous one. And then we also tried four time steps, history, six, et cetera, all the way up to 12. And what we see is that there is a correlation. See, is that there is a correlation among these feedforward agents where the more memory we give it, the better it does. And this performance gain is more exaggerated in more challenging plume scenarios. So for example, on the sparse scenario where the odor packets are more sparse, as well as in the switch many scenario where the winds are kind of just changing directions all the time. And so the more challenging the task is, it seems like the more important the role of memory is. More important the role of memory is in this performance. And so, not only is there a correlation with the history of the multilayer perceptrons, we also see that it's kind of overall the recurrent neural network, the RNN agents, kind of like almost a discrete step in performance in terms of they just do better than any MLP that we were able to train. So, digging a little bit more into the RNNs, we can analyze many aspects of its dynamics, and we can also analyze And we can also analyze its recurrence matrix W. This is the internal recurrence matrix of the RNN itself. Before training, you can see that its eigenvalue spectra are, you know, this is the initialization. So we kind of initialize it so that it's randomly distributed inside the unit circle in the imaginary plane. After training, a couple of crazy things happen. And this is something we didn't dig into that much in the paper. I dearly would love to, but it's something that we put in because we observed it, but I think it deserves a lot more study. But I think it deserves a lot more study. What we see is that for all of the agents who try, this is just one particular agent. We have five of these that are in supplemental methods. What you see is that before, basically all the eigenvalues are inside the unit circle. After training, we reliably see several that pop outside the unit circle, at least one that is strictly real and outside the unit circle. And there's also, there tends to be a couple of these out here. These out here, a couple of complex conjugate pairs that are on the left side of the unit circle. I won't dwell into my personal opinions about exactly what they mean, but these are here to analyze. The other thing we also did was look at the eigenvalues of the recurrence Jacobian. This is a way to estimate, again, the time scales of integration that are relevant for how the RNA is solving this task. We're comparing here before training in Here, before training in orange, and then after training in blue. And what you can see is that before training, again, this is the random initialization of the weights in the RNN. There's a spectrum of different time scales that are being integrated against. And what we see then is that after training, after the agents learn to perform this task, it actually has a maximum of tens, let's say order tens of time steps that it's integrating over, suggesting that at least for the arenas and the statistics of the plume that we've presented to it, that's the time step. That we've presented to it, that's the time scale that's important for solving this task. It's not really particularly relevant for them to remember anything for a lot longer than that. Okay, so I've talked about deep reinforcement learning and one way of thinking about how animals can and algorithms can plausibly interact with a simulated plume environment for the specific task of localizing odors. So, I'm just going to philosophize for one slide, restrict it to one slide. I showed you earlier. To one slide, I showed you earlier this picture of animals and the life of different scales interacting with plumes. Like I said, this is an incredibly evolutionarily ancient behavior that has been really important for all kinds of life, probably for a very, very long time. And so lots of life can do this. They can localize sources of odors. This is not a problem, right? But you can see on the top here, I'm showing you a bacterium. Bacteria can do this. Bacteria and bacteria can do this, you can single-cell organism can always do this too. And so, it sort of begs the question: okay, so like these two examples of life up here don't have nervous systems, they're single-celled organisms. They don't actually have a brain to speak of, right? A worm has a nervous system. I don't care to talk about the semantics of whether or not they have a brain or not. By the time you write a fly, you definitely have a brain. And so, some of the larger animals here do have brains. Okay, but back to the point, if all of these, although all of these forms of life can focus. Although all of these forms of life can perform odor localization, why do we even have a nervous system in the first place? And so the idea that makes the most sense to me is this idea that's sort of taken a little bit from a few other folks, as well as a piece of terminology from soft robotics, this idea of embodied intelligence. That it's not just about this abstract computation, since it's not what we have, a nervous system, right? To think and philosophize and abstract, but really our nervous system is. But really, our nervous systems evolved in concert, in close concert, with the evolution of our bodies. And so, as the sizes of organisms got larger, so did the size scales of the fluid and environments that they interacted with, which necessitated that they have memory, right? That they remember more about their past, in part because they had more agency over their future. So, if you are a plankton floating around in the ocean, you're literally at the winds of whatever currents come by. Like, you can do whatever. Whatever currents come by, like you can do whatever you want, but it won't really make a difference because you're being you're being carried by that current anyway. But if you're a whale, you're significantly larger, and that difference means that you can actually make a decision of where you want to swim and be able to implement it. And so, as the bodies of organisms got larger, its interaction with the environment and the fluid environment in particular, I would argue, also changed. And so, that necessitated a nervous system so that we can do things like have memory and implement. Have memory and implement complex algorithmic plans so that we can actually still accomplish the same thing we want to accomplish, like localized odors, which is important again for finding food, locating mates, as well as doing things like returning to your nest after a long day of hunting. Okay, so I'm going to thank my lab and especially Saprit here. So there's his picture right here. And thank you for listening. For listening. Well, let's thank Professor Brenton. Really, really thought-provoking and exciting talk. So if there are questions on Zoom, we'll take that. I didn't see one, but let's take some from audience over here. Any questions from audience over here? There you go. Hold on. All right, thank you for the talk. Uh can you hear me? Uh yes, I think the network is now very stable. Um, from the neural network to actual the biology side. So, uh, in the middle of the talk, when you start to refer to neurons and activation of neurons and And activation of neurons. And are you referring to the actual biological neurons, or that's a neurons from the neural network that you train? Great question. It bothers the heck out of me too when people use these two terms interchangeably, but I was using them interchangeably. In the context of my talk, I was strictly talking about deep reinforcement learning. And so the neurons were units in the RNN. Units in the RNA. And so there's tons of people, my friends and colleagues, who have done recordings of actual neurons in actual animals and looked at their activity traces. Everything in the project and the paper I'm talking about are artificial neurons and artificial neural networks. Like just a kind of a question about that lost agent that captured. Agent that kept on going on a cross-crew strategy. Did you try like you actually come up with some strategy of adding memory and by changing some of those strategies? Are you able to avoid those kinds of scenarios where it will just be permanently lost or something like that? Yeah, I don't know about why. That's a bit philosophical. I don't know why they get lost, but the same agent with the same exact neural network. With the same exact neural network, the same exact implementation of the strategy. It's deterministic for every plume simulation. But if you gave it a very similar but slightly different plume simulation, it may not get lost. So the behavior at a macroscopic level is stochastic. So over many, many trials, on average, the agent will successfully localize the source. But from trial to trial, it does get lost. I don't really know why. And we haven't messed. Know why, um, and we haven't messed with the strategy. The strategy is just what pops out of the training process. Interesting, very cool. Um, all right, since we are over time, let's thank uh Professor Brenton. We have thank you, uh, thank you. Um, next speaker is Professor Kritika Manohar.