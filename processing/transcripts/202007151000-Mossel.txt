So, we're welcome to the third lecture in the sequence of lectures by Alken and Mosel on simplicity and complexity in belief propagation. The lecture will be recorded, so if you do not want to appear on the screen, please keep your video and your audio off. There will be an opportunity for questions both through chat. Questions both through chat, which will be posed intermittently to the speaker, as well as at the end when we stop recording the lecture. All participants can unmute themselves and ask further questions. Today, after today's lecture of Echennes, we will also have a lecture half hour later starting at 1730 UTC. At 17:30 UTC, we will have a lecture by Shirishandu Ganguly on large deviations of sparse random graphs. His lecture series will continue on Thursday at this time, so at 16 UTC, and on Friday as well at this time. There will be another exercise session for El Henan's lectures tomorrow after the first lecture, so at 7. The first lecture, so at 17:30 UTC. So to start with the lecture, I see Alchemen has already shared the screen. So thank you very much. Thank you for the survivors for showing up for the last lecture. You made it to the last lecture. You should congratulate yourself. I don't know, maybe if the number is small, maybe you should make it that everybody can unmute themselves, or you think that's a bad strategy. I don't know. I mean, I feel like it is a small enough group, but maybe. It is a small enough group, but maybe we can do that. But of course, then we will hear somebody washing a machine, and that person won't be there. So, I don't know. It's up to you. Something if you want to do it, I do not object. So, in the worst case, if we hear the washing machine, we can mute everybody. Okay, so where were we? Last time, we were trying to understand the nature of the more fragile phase transition that we have for large Q, where the theorem. That we have for a large q, where the theory is nonlinear. And one of the theorems that we mentioned last time is that for every q, no matter how large q is, if we look at this process on the DRE tree where we want to understand what can we say about the root from the leaves, right? So we have this x naught here at the root, and then see the information about on xh, and someone wants to understand what can we say about x naught given xh. So this is the threshold for sense sense. Census reconstruction or count reconstruction. So, this means that instead of x of h, you're just given how many are there of each type. Or, you know, there's if you get information that it's permutation invariant. And for robust reconstruction, there's additional form of noise. So, okay, we won't talk about the proofs of this theorem more. What I want to talk about is today to start with is sort of the opposite direction, and just to give you some intuition for how can we prove this theorem. intuition for how can we prove a theorem like that. So if so I give you actually the easiest theorem in this direction. It's easy because we allow Q to be arbitrarily large. So here is the theorem. If d theta, you fix dn theta such that d theta is greater than one, then there exists a q theta, some finite q theta, such that if the number of color q is bigger than q theta, then if we look at this random variable, which is the expected value of the rule Which is the expected value of the root. So this should be xh given xh. So this random variable is a non-trivial random variable. It's not always 1 over q1 over q1 over q. It has variance, which means that you can say something non-trivial about the others. Okay, so again, everything is in the limit as h goes to infinity. So that's what we want to prove now. So the order of quantifiers is a little maybe cheating, but I think that's the easiest proof and maybe the first proof in this direction. So I mean, we'll. This direction. So, I mean, we'll do that. So, first of all, I fix D and theta such that theta is greater than one. And then I want to show that for large enough Q, if the number of colors is large enough, I can infer something about the wood from the leaves, no matter how deep the tree is. Okay, so let's do the proof. So, what's the easiest cue for which it would be to do this pool? Q for which it would be to do this proof. So the easiest Q would be the largest Q. What's the largest number you can think of? Anyone? Five. Anyone? Second? Five? Five. Okay, very good. I can think about a larger number. So I will choose Q equal to infinity. Okay, so Q equal to infinity, we didn't define this model for Q equal to infinity. So let's just think what would be. So let's just think what would this model be for q equal to infinity, right? So let's try to think what would it be. So I still have infinitely many colors, so I start with one of them. Okay, it's a little maybe hard to imagine what does it mean to start with one of them uniformly at random, so let's not worry about it. So I start with this color, and then here I copy it, and here I decided not to copy it. What do I do when I do not copy a color? I choose another color uniformly at once. So we don't really know what this means either, but it definitely means that it's going to be a color. It definitely means that it's going to be a color chosen differently than any of the colors that we've seen so far. So, here we're going to call, maybe we're going to call this color one, even though it's not necessarily the first color. And here we're going to give color two. And then maybe here I'm going to copy. So this would also be one, this would also be two. Then I'm going to choose a different color, and this is going to be three. Here I'm not going to copy, not going to copy, and I'm going to have four and five. But you know, these labels are arbitrary, right? They don't really mean anything. So in this case, whenever I change, I change to something that I haven't seen. I change, I change to something that they haven't seen before. Okay, so that's that's the basic model. And I claim that clearly, okay, clearly in this case, the threshold is d theta is equal to one. So if, or in other words, what I claim here is that if d theta greater than one, then xh tells me something about the leaf or gives information. And give information on X naught. Okay, so let's try to understand why. Okay, and it's true asymptotically, right? So obviously, you know, if you have one level, you look at any of the colors, but you have in one level, you say this is pretty likely to be true. So, okay, so what is d theta equal to one? D theta equal to one is obviously just the branching process threshold here. And let's see how can we argue that based on branching process argument. So here's what here's my claim. So, here's what here's my claim. I'll just draw it in the picture. So, what would be a good situation for us? A good situation for us, and I'm just going to do it for d equal to so the truth binary, is that I copied here and I copied here. This obviously happened with some probability theta squared. And then what I want in the rest, I don't necessarily want that I copied everywhere. I just want there's one path from the root on this tree to the leaves that where I always copy. Okay, so what is Okay, so what is the construction again? So these edges that I mark, edges that I copied, so let's call this a color star. So I have a star here, I have a star here, I have a star here. And all I want is that from this point on, there's exactly at least one path that gets to the leaves of this subtree. Okay, and I don't care about anything else. So, what is the claim here? So, there are two claims. The claim, first claim, claim one, in a configuration like that, I know that. In a configuration like that, I know that the root has to be starved. When I mean in such configuration, maybe I should say in such leaf configuration. Okay, if the leaves of the tree are such that in this sub-tree there's a star here, and in this sub-tree there's a star here, I know that the same color cannot appear. I know that the same color cannot appear two times in different ways. So it must be the case that they emanated from the same place, and this place has to be the root. So if I have a star here and a star here, then the root has to be a star. And claim two, the probability of this configuration is at least theta square. That's the probability that I copied from here to here and from here to here. Times, let me call. To here, times, let me call it p branching process squared. So that's the probability that the branching process from here survives. If the branching process from here survives, then I will have at least one guy with a star, and I'll have at least a guy from staria. And of course, the key factor is if detail is greater than one, then we know that the branching process will survive with the positive probability. Okay, so this is sort of cheating. This is for Q is equal to infinity. But still, there is a lesson here. The lesson here is that it's actually not something about the numbers. The number of stars that I'm using here is just two. If you look at any colors that appears on the tree, you know, it appears many, many times. So it's really not about the numbers. It's about where I see the colors, right? It's something about the location. And, you know, the key factor is that I see the same color on the left and on the right. And it's very different. On the left and on the right, and it's very different from the count reconstruction that we've seen before. So, that's maybe it's not about how much, it's about where the stuff is. Questions about this proof? Okay, good. So, now how do you do the general, the other, the finite Q case? Okay, so let's just do, I'll just indicate and again give us an exercise for a finite Q. us an exercise for a finite q again let's look at d equal to and fix a d such that d theta is greater than one and again for the q we are going to fix later we are not going to commit yet on what's the value of q we are going to try to do something similar so i'll tell you what the something similar is it's a little more complicated so here is my big tree here is my tree t i'm going to infer the root color to be c if there is something that's called l d there is something that's called L diluted binary subtree T prime up to T with the root at zero and where all the leaves have color C. What is L diluted tree? So this is a binary tree. Two diluted tree means that I'm allowed to skip a level. So you know maybe I should okay let's see if I can use a different color. So I'll draw a big bound binary I'll draw a big a big binary tree here. A big binary tree here. Where is my binary tree? Okay, so what is a two-diluted tree? Let me try to paint it with a different color. So a two-diluted tree, maybe it's this node and this node. So it means that I'm allowed to skip a level. And then maybe from this node, I go to this node. And from this node, I go from this node. And from this node, I go to this node. And from this node, I go to this node. To this node, for this node, I go to this node. So it's not a binary tree. I'm allowed to skip every two levels. And what I'm saying is, my procedure is that if this color and this color and this color and this color in my list are all going to be red, I'm going to declare the color of the route to be red. Okay? And I can do it for more than two. So this was two diluted here. L was equal to two. I can do it for three levels, four levels, and so on and so forth. Okay? So I'm going to fix the parameters of that later. But the basic procedure is going to be, it's again. But the basic procedure is going to be, it's again going to be something based on location, but instead of just having two guys, it's going to be more fractal. I'm going to look for a fractal-like structure where all the colors in this fractal are the same color. And if all the colors in this fractal are the same color, I'm going to recover the root to be that color. Okay, so that's going to be my inference procedure. Okay, so what are the exercises here? I mean, so now, you know, these are the drilling. Now, you know, these are really just exercises in branching processes, so I won't do them. So, the two exercises are the following. So, there exists, if again, so let's just remember the order of parameter. What I started is what I studied, it is I fixed d and theta such that d theta are greater than one. I didn't say anything about q yet. So, the first exercise is. So, the first exercise is that once I did it, when I fixed a detector such that detector greater than one, there exists an L and an epsilon greater than zero, such that if the color of the root is C, the probability that such a tree exists is at least an epsilon. Okay, so you will see structures like that. And maybe the fact is that L and epsilon do not depend on Q. You know, it's a uniform one, you know, something that's also for L, you know, you will see this structure. So, you know, you can do this inference once in a while. You know, you can do this inference once in a while. Now, this inference, unlike the previous case, it's not necessarily the case that if I see such a fractal, it means that the root is that color. So, I somehow have to make sure that when I infer, I don't make mistakes. And for this, we need the second claim. And the second claim, we'll use the fact that Q is large. So, the claim is that for all epsilon greater than zero and L, if Q is sufficiently large, so Q is a function of epsilon and L, and if On an L, and if the root is colored by a color different than the C, then the probability that there is an L diluted 2 to the L minus 1 tree with all the leaves of color not equal to C is at least 1 minus epsilon over there. So this is a much stronger claim. What do I want here? I want here that when I draw my picture, so I draw it in blocks. This is the first A levels. This was not color C. I'm allowing just one guy maybe to have the color C. Everything else has to be. Have the color C. Everything else has to be color not C, and then recursively I have another L levels. So below this guys that is not colored in C, I allow maybe either none of them are colored C or at most one guy that is colored C. And if the number of colors is large enough, again, it's a branching process argument. You can show that, you know, such a big tree exists where all the colors are not. Now, why are these two claims useful? I mean, I won't show you in detail, but it's an easy combinatorial exercise to show. But it's an easy combinatorial exercise to show that if this tree exists, this tree doesn't exist, and vice versa, right? So if you have this fractal of color C, you cannot have this fractal where everything is color different and C. And so one exclude the other. So this would say that you would not make mistakes too often. If the color is not C, you will most likely not say C. Okay, so again, the interpretation of exercise one and two. Exercise one says if the color is C, you are going to say C at least with probability epsilon and the Probability epsilon, and the second exercise says if the color is not C, the probability that you will say C is at most epsilon over T. So for exorcis 2, are you looking for a tree where all the leaves have the same colour that is not equal to C? No. The only requirement is that the gallery is not C. Now I'm just thinking in a very simple-minded way. It's either C or not C. That rights either C or not C, and I want essentially everything to be not C in this fractal sense. Right, and Ratul was asking to verify that the leaves of the diluted tree are all on the same level. They're all on the same level. They're all at level H, which I assume is a multiple of L. And in exercise one, you're not specifying the level? It's again, it's an exercise one, it's it's L H is again a multiple of L if you want, but it's true for every H. You want, but it's true for every h. So again, the epsilon is allowed to, right? So it's a property of branching process. So I don't know if you've seen this property before, but for example, if P is large enough, and I just look at the, say, four regular tree, if P is large enough, then if P is large enough, I can get a three regular tree where all the branching happens, where there's always branching. Okay, so this is a similar phenomenon. Okay, so this is a similar phenomenon. So the critical value just for percolation or for branching processes is one quarter. But if my requirement is much stronger, it's to find the three regular tree inside the four regular tree where everything is open, then for a large enough p, this will happen. And you said you write similar recursions to the recursion that you usually write in branching process for this condition, and you check, you get some fixed point equation, and you check there's a root that's less than one. That's, I mean, it's something classical in the theory of logic. Classical in the theory of budgeting process, maybe not all of you have seen it, but it's classical and it follows the same proof that people usually do in classical batching processes. Okay, any other questions about this? Okay, good. So there's another exercise. I mean, this is the easiest exercise in this calculation picture. Is that if d theta, I don't know why did I write lambda, if d theta is less or equal than one, then there will Less or equal than one, then the root and the leaves are asymptotically independent for every q, and that's sort of clear, right? Because again, from this branching process picture, if d theta is two theta in our case is less than one or less or equal than one, then we know that this branching process will die. So we will not reach the leaves, and therefore what we see at the leaves and the root is going to be independent. Okay, so that's all I wanted to say about this simple reasoning, of course. Of course, if you're smarter, you do better. So, Alan Sly found a different way to think about the correlation between different parts of the tree by doing a more careful expansion of the magnetization. Okay, we have to say what is magnetization, but the magnetization is sort of the expected value of the root given the leaves, right? So, Mn is in some sense a vector version of the expected value of X. Version of the expected value of x naught given xh and what he showed is they showed that you know the if you do the expansion as a function of both d and q you get the main term which is the curst and stigma term d theta square then you get a second order term where you see you get this mn squared and the key thing about it is this this comes with a positive or a negative sign depending if q is bigger than 4 or less than 4 so if q is bigger than So, if q is bigger than 4, this term is positive, and if q is less than 4, then this term is negative. And there's a bunch of things once you do that, and you know, it's not completely trivial, but Alan, so again, this is positive if q is bigger than 4. But once you do that, I mean, what Alan succeeded to prove is the following. First of all, you can prove that if q is greater or equal than 5, then the Keston-Stiggle bound is not tight, and the intuition is not too hard. And the intuition is not too hard. The magnetization will go down and down and down and down, but then at some point it will start going up when you're very close to the cast and stigma bound. So that's not too hard to derive once you have this recursion. And also, interestingly, show that if Q is equal to 3 and D is greater than some thousand or a million, then the Kestman-Steben bound is tight. Okay, so this requires more work because you have to show that the magnetic boundaries More work because you have to show that the magnetization quantity is low enough. And I think Alan also has results for Q equals 4, where he used the next order term, but I'm not sure these are published. So let me not tell you what they are. Okay, so then it's depending if it's ferromagnetic or anti-ferromagnetic. I mean, it's even more delicate. Okay, and for the few of you who went to the exercise section yesterday, I mean, one of the things that I don't know if Federic had a chance to get to or not. So, for general Malkov chains, if you look at not just the scoping process, you can actually have the second eigenvalue be zero, yet the root and the leaves are not independent, right? So, you know, you can get, you know, there's sort of no relationship between theta, which is the second eigenvalue of the matrix, and the question if the root and the latest is the And the question if the root and the leaves are independent, right? So for country construction, lambda 2 equals zero means that no matter how wide is the tree, if you just look at the census, then it's independent of the root. But if you are allowed to do whatever you want, you're allowed to apply belief propagation. Then there are even examples where the second eigenvalue is zero. So you really forget information very fast, and the root and the leaves are not incorrect. Okay, there's an exercise here in the notes that does that, but there are. That does that, you know, but there are other examples even more sophisticated. Okay, so let's see how much time we have. So maybe, yeah, maybe I'll give the conjecture and then we'll take it two minutes break. So for those of you who stayed, maybe you're interested enough to actually think about research conjectures in this area. So these are two conjectures that I think I'm formally making for the first time ever. Time ever. And you know, so you may be in particular, it may be pretty easy to refute them. I think that proving them is going to be difficult. I'm going to tell you two conjectures about sort of the fragility, another interpretation of the fragility of the Kesten-Stingen bone in the setup. And you are welcome to think about it. It's definitely something that I'm thinking about in some way or another for many years. Okay, so we are going to consider a model where not all the thetas are the same, right? So there's going to be a tree, maybe the tree is going to be binary, but each edge is going to have its own theta. Okay, so that's the model that we're going to consider. And we are going to consider a large Q, it's going to be large enough in particular. There's an interval between what I call theta r, it can be any number that you want, and the crest and stigma bound, where the root and the leaves are not in. Well, the wood and the leaves are not independent. So the variance of the expected conditional expectation of x not given xh does not go to zero. We know it actually has to go to a limit in this case. So we're looking at the interval where there's some action. So this is in the case where all, so this is in this in this case, theta E is equal to theta for only. Okay, so you know, we are looking at the case where you can do recover the root better than random, but you cannot do it in these robust ways. Robust ways. So here are two conjectures. The first conjecture is that there is no estimator f such that f of x of h and x0 have non-zero or non-eglogible correlation for all the models where different edges, all of them are in this interval theta r to theta ks, but they are not the same for different edges. So you assume that theta e is no? That theta E is known, so the estimator can depend on the no, I exactly. I can assume that the estimator does not allow to know what theta is, right? So, again, if right, so there are actually two, yeah, excellent question. So, there's actually two questions here. I actually don't even know the answer for sure if you know the theta is. This has to do with some monotonicities of this model, but I think this model should be monotone, and that might not be too hard. So, maybe a question. Maybe a question, a preliminary question that you want to show is that if theta E is bigger than, I don't know, theta R plus epsilon for all edges, then X naught, X H and X naught are not correlated, are significantly correlated. Or asymptotically correlated. Okay, so that's that's definitely you know one thing that you would want to check as a prerequisite for conjecture one. But conjecture one talks about the situation where I don't know what r the theta is. I'm just promised that they are all in an interval. You can choose theta r to be any number that you want below theta ks. I'm just telling you that they're in interval between theta r, this theta r and theta ks. And you don't know what they are. And I want a universal. And you don't know what they are, and I want a universal estimator that does not depend on the values of theta that will give you correlation. Does that make sense, Omar? Yes, and is there any reason to require that the thetas are a lot larger than theta ks? Right, so there is a reason, very good. So, Omar is already thinking about the problem. So, what we know, and I mean, so unfortunately, I think I only wrote the proof for q equal to. We know that at least for q equal to. That at least for q equal to, if theta e is bigger than theta ks plus a little bit plus any epsilon that you want, such f exists. No, I meant for the conjecture if some of them are smaller than that than theta ks and some are larger. So yeah, but I mean it doesn't really, yeah, you can you can. Yeah, you can you can you can you can you can allow yourself to do that too. I mean it it's the it would it would still make the conjecture interesting. I agree. Yeah, yeah, but I sorry I didn't answer your question, but I do I do want to note that for the case that q is equal to 2, if all the theta is are bigger than theta ks plus epsilon, then there is a such an f. And it's not completely a trivial f, but such an f exists, right? So there is a function that doesn't need to know the theta is and recovers the root in a way that's correlated. Covers the root in a way that's correlated with the leaf. Can you not see what I write, Elia? No, I can see it, but I'm wondering when you've changed the, when theta is allowed to be variable, do you always know what the theta r is? No, so this is the question that let me repeat. So, I mean, let me just try to maybe motivate it a little bit, right? So, in like in application like phylogenetic reconstruction, it's not reasonable to assume that you know the data. You know the thetas. But it is reasonable to assume that the theta lie relatively in an interval where say they're close to one or much bigger than the Keston sigma bond because you know how much time evolved between species. So making an assumption like theta Es are all much bigger than theta ks is a reasonable assumption. But making the assumption what that you know what they are, that's the most tricky assumption, right? So in some situation, and you know this would lead to the second conjecture, in some situation you want to make this conjecture, the theta e To make this conjecture that theta e is, and then you want to a reconstruction that does not depend on knowing that the theta e and in the case that q is equal to 2, such a procedure exists. What I'm claiming is that for larger q, it should probably also work. I never checked it, if all the theta z's are bigger than theta ks plus epsilon. I mean, in the even q case, it follows from this general philosophy that whenever you know for q equals 2, you know for even q. So, you know, maybe there's a little bit of working or checking it. There's a little bit of working or checking it for odd Q, but I think the fragility comes where you're below the Kestenstigmobound. So your thetas are such that if you are given the thetas, you can apply belief propagation that takes into account in a very crucial way what the actual values in theta E is in trying to recover the posterior, and you'll do well. But if you don't know the theta E, there isn't like an algorithm that ignores the detail of the process and recovers the But do we know for sure that theta r is not zero? We know if q is you you mean you mean that the interval is non-empty? Yeah. Right, so that's exactly the point of the Keston-Stingum bound or the result that we've seen is we've seen that for q equal to q greater or equal than five, this interval is not empty, right? Because the kest and stingum bound is lower, is Is lower is higher than you know the threshold for actual reconstruction or for actual recovery of the wood. So this interval non-empty when q is greater or equal than five. Okay, so this conjecture only makes sense for large q, definitely not for q equal to, but definitely for q greater or equal than five. Okay, this is a non-empty interval in this sense, index in this case. Okay, so that's going to. Okay, that's conjecture number one. Maybe I'm happy to talk about this later. And maybe at a high level, conjecture number two, it says it is impossible to recover phylogenetic trees using order of eight samples under these conditions above. And the conditions above means that you don't know what the data are. So again, conjecture, you know, the converse of conjecture one in the case q equals two, you know, does not hold. And this helps us in recovering phylogenetic trees. So I conjecture not only conjecture one. So I conjecture not only conjecture one, but also that the corollary of conjecture one, you know, the reconstruction of phylogenetic phylogenetic tree breaks down because of that. Okay, I'll just say because I want to take the break. So strong version of impossible would mean there's just no information theoretical way. And weak version would mean that it's computationally much harder. And then the fact that it's computationally much harder, we can never prove. Computationally, much harder, we can never prove. You know, you just say, you know, the only way I know how to do it is via some exponential time algorithm, and it feels like I cannot do it. But maybe even the strong version is correct. I didn't really make progress on this problem. I thought about it on and off for the last few months. I didn't really make progress on this problem. Right now, I have no intuition for if the stronger version or the weaker version, or maybe we can just disprove my conjecture, whatever you do, you'll make me happy. Okay, so let's take a two-minute break now, in which you can ask questions, and then I finally talk about some elements of simplicity and complexity of belief propagation. And I'm happy to take questions. Yes, and I think participants should also be able to unmute if they want to ask questions or ask on the chat. And I guess it's conceivably there could be some perturbative argument if the thetas are all in some very small interval around some theta. Yeah, so my conjecture is pretty strong. I didn't necessarily specify that theta is the first value where you can do something. I think that my conjecture is that actually for every epsilon that you can think about, if you look at the interval between theta ks and theta ks minus epsilon, you are doomed. So this is a pretty strong conjecture, so you might be able to refute it. Okay, and Subharta is asking if there is a robust analogue for the block model recovery problem. The block model recovery problem? That actually sounds like a very good open question, too. So, right, but let's just recall that. So, let's actually think about it, Silverbata is a good question. So, as usual, so let's just think about it aloud for a second. So, for the block model, it's already conjectured when Q is large, there's a computational statistical gap. Statistical gap. But now, Subavata is asking maybe a different question. Suppose we are looking at a block model where the parameters, thetas, are different for different edges, and you do not know them. You just know that they are in some interval, in this interval, if you want. Do we know that this is information theoretically impossible? So, this is a great conjecture. So, let me call it S conjecture. Similar. Similar information theoretical phenomena in block models. So I think this is a great question. I don't know that anybody looked at this question. I think this is a very pretty natural question to look at. It's a very nice question. Okay, good. Okay, good. So let's move on to the last part of the question, part of the talk. So obviously I have 20 minutes, so I won't prove anything. Maybe I'll state a couple of things. Maybe not. We'll see how we do. Maybe I'll just tell you stupid jokes. I didn't tell enough stupid jokes in the lecture series. So I'm going to talk a little bit about the complexity of BP. So, you know, the question is, what is the complexity of BP? We've seen a lot of BP in. Seen a lot of BP in this lecture. What is the way to formally measure what is the complexity? So, in some sense, it's slow, right? We've seen this recursion, it's running the volume of the tree in linear time. You start from the leaves, you know, you compute the real number, then you compute another real number, then you compute another real number, so maybe it's very low. But I mean, there are some notions of complexity. One is that it uses real numbers, right? The process that we talked about. Right, the process that we talked about, say with q equals 2, is discrete, right? It just bits. Why is it that when I do the inference, I have to use real numbers? Is it really necessary? And the other question is that it uses depth. It's this recursive procedure. So it's different than just summing the guy, right? You do something iterative. Is it in some sense necessary? And the fractal picture that we've seen just in the proofs for the behavior above the castenstic compound. The behavior above the castenistic compound suggests that maybe depth is needed, right? So, okay, so here's my stupid joke. Okay, I'll do one stupid joke. Okay, what is everywhere and understands everything? Okay, so what is everywhere and understand everything? You know, if you Google something like that up, you'll get omnipresence. So, this is not going to turn into a course in theology. So, my answer for what is everywhere and understand everything is the deep net on your smartphone that. The deep net on your smartphone that understands what you say, right? So, each of us has a smartphone or Alexa or some other software, and you know, you talk to this thing, and miraculously it understands what you said. And I don't really want to claim that anything we're doing is too relevant to deep nets, but you know, these are some hierarchical processes that run very fast because they understand what you sell, but they're based on multi-layer, right? So, I think for me, one of the questions that came from the the questions that came from the from you know the lack of understanding of deepness is the following question so you know one of the reasons I ask these questions about B is the following question mathematically it is natural to ask given you know what we see in BMX if we have a process that satisfies three natural properties one is it's a realistic model of data so you have a model that generates realistic model of data to reconstruction Two, reconstruction. You have algorithms that reverse engineer the generative process. So you have algorithms that actually can, these are maybe very heavy data and very, very heavy computational processes that can put this deep net on your phone. So this is what Google does. It takes them a lot of time and effort. And three depths, what you actually have in your phone requires depth. Okay, so if we take a very, very abstract point of all of these deep depth things, you know. Of all of this deep net thing is, you know, as people are doing probability, what do we want? We want a probability model, a probability model that generates data that looks somewhat reasonable. It's a model for which you can come up with the deep nets that will do the inference. So you can recover the net that you're looking at. And finally, it actually requires depth in some form, I'll say. Okay, so what I claim is that we already looked at this model. So this broadcast process model or Model. So, this broadcast process model or this three Markov chain is realistic, right? People are using it in phylogenetic, in information theory, in a bunch of areas. We saw that you can reconstruct it. This is phylogenetic reconstruction. And I'm given a lot of examples. A lot of examples I can build the tree that generated them. And then maybe the missing piece in this idea of trying to find this trinity is, you know, do you really need the depth? Okay, so that's somehow an abstract point of why you are really interested in this question of depth. Interested in this question of depth. And the related question: you know, why do we really use real numbers? Deep net, MBP use real numbers. Why do you need to use real numbers when everything that you're talking about is discrete? Okay, so these are sort of high-level motivations for the question, for the result that I'm briefly going to talk about now. Okay, so I'll actually start from this problem. Why do you have to use real numbers and why? And one way to think about this is that what are the memory requirements for belief propagation? So, believe propagation, you propagate these real numbers going up. And already in this paper of Evans, Kenyans, Perseus, and Schulman from 20 years ago, they stated that even for Q equal to n recursive algorithm on the tree, you know, that send messages back on the tree that uses at most some constant number of B bits of memory per node. Number of b bits of memory per node can only distinguish the root value better than random for theta that is less than theta b, but this theta b is not the constant stigma bound, it's a bigger number. So, if you want to do what belief propagation does, this simple non-linear recursion with real numbers that we've seen, where you just multiply and add and divide, and you want to do it with bounded amount of memory, you know, you can truncate, but you can think about other ways of doing it with bounded amount of memory, then you will not get all the way to this session. You will miss something. Not get all the way to the session, you will miss something. So, let me show you the picture. So, we proved that in joint work with Vishesh Jain, Federic Koller, who is the TA for Discarse, Jingbo Liu was a postdoc here. We proved that this is correct. And I'll give you the picture from slides that Jingbo made. These are very nice pictures. So, let me try to show them. So, what happens when we do believe propagation? So, this is Belief propagation. So, this is not completely consistent with our notation, but there's the root x1. This is a bit, you know, there's a noisy channel that gets to x2, x2, x3, x4, x5, x6, and 7. Then there are the y's. This is what belief propagation calculates. So, these are some surreal numbers. Then we combine these real numbers via some non-linear recursion. And then we get an estimate which is an estimate which gives us some two probabilities, right? So that it goes down the tree, and then from the tree, we go up the tree. So, the model that we're looking at, like, no, the broadcast model is exactly the broadcast model that we looked at so far. But in the reconstruction, we are only allowing functions where each of y's contains a log L bits of memory, so it can take at most L values. Okay, we are not allowing it to take an infinite precision real number, it can take only at most L possible values, or you can describe it with log L bit string. This log L bit string, and what the theorem that we proved said, or what the original conjecture was: of if KPS says that you really need the infinite precisions to get all the way to the casting stigma. Okay, so any other procedure that you have, you said, oh, I'm going to look five level downs and do majority iteratively, this is going to give you something, but it's not going to go to the customistic one. Oh, I'm going to do belief propagation, but I'm going to discretize it and keep just 25. It and keep just 25 digits, you know, this is not going to get you to the Kistan Sigma. Whatever you are going to do, this is, you know, that doesn't use all the infinite accuracy, it's not going to get you to the Kistan Sigma. So it shows to some extent that you really need to use real numbers. Okay, I think in the last 15 minutes, so Jingbo is, you know, this proof is pretty beautiful. And Jingbo is... Yeah, what was the estimate on? Yeah, it was shifted. So theta b is bigger than theta, and it has the right scaling, which is a polynomial in b so we have both an upper and a lower bound with different constants here, actually. So this should be theta. And one direction is obtained by discretizing belief propagation, but not in the most trivial way, right? So the constructive way is discretizing belief propagation, but the main work is a lower bound. But but the main walk here is is a lower bound. Okay, so I won't talk about the proof because I I really want to tell you about a different model where you can which is more related to this question of depth and here the game is a little different now. We want we still want to recover x naught from x h but before we had to do it with But before we had to do it with following this very specific tree strategy, now we are allowed to do any strategies that we want, right? So we have some deep net in our cell phone, it takes XH, it does some non-non-linear operation, then it goes another layer, another layer, another layer. We want actually, we look at different architectures, but what happens if we allow also non-trial architectures? And the question is, can we do it with slow depths? And of course, we cannot expect to prove. We cannot expect to prove that we need a huge depth because BP has a depth of order log n, which is order h. Okay, so you know, we cannot expect to get a lower bound that's better than order h. And, you know, here's some parameter from deep nets. Let's not worry about it. The question is, can we prove something like this? So, everything I'm going to talk to tell you about is some results from work with Anku Moita and Colin Sandon on this problem. This problem. The problem is that I don't know how in 15 minutes I show you three complexity classes in computer science. I'm just going to skip AC naught. And I'm going to tell you what T C naught. Okay, so T C naught is like A C naught, which I skipped. But with majority guides, so that we tell you what it means. These are logical silk kits where you're allowed to do logical operations. And the logical operations that you're allowed to do are and you're You're allowed to do all. You're allowed to do not, obviously. And you're allowed to do majority with any threshold that you want. And any number of inputs, right? So it's a circuit that, you know, you have some architecture. Here, all here is XH. Maybe some inputs are duplicated. Then a bunch of them are going to an end. There's an overlap. Some other ones are going to an O, some other. ones are going to an O, some other subset is going to a majority, then these guys are going to see, you know, and it goes on for multiple layers like that. But the number of layers that I'm allowed to have is constant. It does not allow to grow with age. So this is T C naught. So what is the result that we get for this set of architecture T C naught? So here is a result that already shows something, you know, yeah, it's hard to see why it's Know, yeah, it's hard to see why it's connected to the cast and stinger bond, but I'll maybe I'll try to explain. When the number of colors q is two and theta is large enough, it's close enough to one, then in fact, in this class T C naught, you can estimate the root as well as B P. Okay, so even though BP had H level, this class had a constant number of level. Okay, so maybe I should write this equal to thus minus epsilon. Thus minus epsilon, minus epsilon, and this epsilon depends on the depth. And the conjecture is that this is true for all theta when q is equal to 2. So the conjecture is that for when q is equal to 2, in the linear case, actually circuits of bounded depths with this majority gates can do as well as BP. We know it only when theta is very close to 1. Of course, when theta is very small, we also know it because BP doesn't do anything, it just returns some. BP doesn't do anything, it just returns something independent of the root, so it's very easy to return something independent from the root, right? So that's one result. And I'll just, okay, so that's one result. So maybe I'll draw just the picture in terms of what I said. So here is theta Ks. So here we know that all algorithms fail just because information theoretically you can do nothing. All fail. So there's no complexity question. So, there's no complexity question here, right? You cannot recover the wood because there's not enough information in the leaves. So, it's really a question about what happened between theta ks and one. And the conjecture is that between theta k and s and one, the kind of algorithm that we use in TC0 actually works in all of this regime. But I mean, right now, we don't know how to prove that. Okay, and I'll mention one more class, and this class is called NC1. NC1. So this is the class of order log n or order h in our notation depth circuits with and or and not gates. You can also include majority gates. It doesn't really change much for complexity reasons. And as always in complexity theory, there's something that's not known. So this class TC0 that we've seen before is known to be contained in NC1, but we don't know if they're the same or not. So it could be that TC0 is actually equal to NC1. That Tc0 is actually equal to NC1. That's a big open problem in computational complexity. Maybe not as big as NP versus P, but still a big open problem. Okay, so one thing that's easy is that BP is NC is in NC1. In the circuit where you use all the log and depth, it's pretty easy to do things. But more interestingly, there is a broadcast process for which classifying better than random is NC1 complete. Okay, so I won't describe this broadcast process to you, but it means that if you want to estimate the width better than random for the broadcast process, you know, it's a three by three Markov chain or 16 by 16 Markov chain, but I'm not going to describe for you. If you want to do better this random for this 3x3 or 16 by 16 Markov chain, then you need the full power of NC1. You need the power that, you know, assuming. You need a power that, you know, assuming that NC1 is not equal to TC0, it means that you need the full power of all the fraction or the same number of layers that you have up to a constant as the broadcast process. So, in other words, unless TC0 is equal to NC1, order H depths or the log n depths is needed. Okay, maybe I'll just end with this conjecture. And with this conjecture, so we are very far from proving this conjecture, but the conjecture would be that for any broadcast process below the cast and stigma bound, and where BP classifies better than random, classification is NC1 complete. So you really need the full power of NC1 in order to classify better than random when you are below the cast and stigma bound. So this conjecture would say that we see the cast and stigma bound again in terms of the computational. In terms of the computational complexity of learning, the woods, not just with three architectures, but with any architectures you actually need it to be. Okay, so maybe I'll conclude here. I have more details about various books, but if you'll ask me, I'll happily apply. But what's the conclusion of this maybe lecture, of this series of lectures, is that BP is simple, it's running linear time, and above the case bound, it behaves like a linear algorithm. We've seen it both in the analysis of BP for the tree process, and we also mentioned, even though we didn't see the details, that in some sense, that's what we behave for the primary. Sense that's what we behave for problems like the block model. On the other hand, BP is complex. Below the KS bound, our understanding of what BP does for trees is that it does something very fractal-ish that's harder to understand. We believe that there are statistical computational gaps both for phylogenetics and for block models, and maybe even just for the broadcast process. And there's some indication that it requires depth and precision. Depth and precision. So, I think that's all I wanted to say in a high level. And again, I have more details about various proofs. So, if you ask me questions, I'm happy to answer. So, I also wanted to thank you guys, the organizers, for organizing and for everybody attending. Whenever you agree to do something like that the week before, you're like, oh, why did I agree to do that? This is so terrible. I'm so stupid. But then, you know, when you actually prepare the lectures and talk about them and think about And talk about them and think about them, you know, you realize new connections and new conjectures, and you know, having intelligent people listening to your rambling and asking intelligent questions, you know, is always good. So thank you guys. Well, thank you, Elchanan. Let's unmute the participants. So we can thank Elchanen for his three lectures. I can mute people, but they can unmute themselves for questions. Let's see. There's some thanks in the chat. Still waiting for questions. Okay, so maybe to start things off. Things of so when you say that you can do things with this bounded depth networks, do you have some explicit construction for networks? So let me tell you very good. So you predicted very well. You predicted very well some of what I do. So here's the TC node construction. I'll tell you how it goes. Essentially, you do the following. I'm going to do the bad thing that I shouldn't be doing, which is I'm going to Be doing, which is, I'm going to take maybe some number of subtrees, and for each of them, I'm going to take a majority. So, I'm going to take a majority of this sub-type, of this, subtree, of this, subtree, of this, subtitle, of this subtree, and I'm going to take some estimate of the majority would give me an estimate of the roots. These estimates are not optimal. These estimates are correlated with the roots, but they are not optimal. And then I'm going to apply BP on the list for a constant number of levels. So, even though this is big, this is actually small. So, even though this is big, this is actually small. So, here maybe I have five levels, and I apply BP on these values. Okay, so this is very, very explicit. And for a constant number of levels, I can do whatever I want. And why does it work? It works because in the real regime that we know that it works, it works because we have a result which we developed in the context of block orders with John Neiman and Alan Sly that BP with noise class. BP with noise classify as BP without noise if theta is close enough to one for Q equal to two. So it's not just, right? So we have this notion that, you know, noise doesn't change the threshold, but you can think about the situation. I have the broadcast process, right? So I remind you this picture: I have this broadcast process. And at the end, I add a little bit of noise or a lot of noise for every individual guy. Now, suppose I apply BP to this. So I already know by. So, I already know by some robustness result that this will be non-trivial. But in fact, it's not will only be non-trivial, it will asymptotically perform exactly as well as BP. So, the probability that you will be correct will be exactly the same. Okay? So, there's some BP adds some error correction in this regime. Second? And the noise is not so large that it overwhelms this? So, this noise can be anything. This noise here can be any. This noise here can be any number that's less than one, any number that's less than enough. As long as you let me maybe write the noise goes to one. So let me write the limit. Okay, so let's does the limit h goes to infinity? The limit of the noise, which I'm going to call to eta. Okay, we have to decide if we call it zero or one, so let me call it zero. Okay, of the probability that BP X. BP XH is equal to X naught is equal to the limit of H goes to infinity. Oh, sorry, YH is equal to not is equal to the limit of the probability. So X is going to be this vector and Y is going to be the noisy version. YH is going to be going to be equal to the limit as f goes to infinity, probability that BP of X. is equal to x0. Okay, so on the right hand side we have the quantity that we like. That's the quantity, the chance that I'm going to be correct. On the left hand side I'm telling you that for every value of eta no matter how large the noise is as long as it's a constant amount of man I'm going to get the same level of accuracy as h goes to infinity because the limit eta goes to zero can also be for every eta greater than zero. Every eta greater than zero. Instead of this, I can just write for every it's corrected me, but I really wanted to write for every so this is true for every eta greater than zero. So it doesn't matter how much noise you have the when you take the limit of the depth going to infinity, if you say just do once, you will go as well as BP. Yes. Yeah and you don't and for this application you don't need to to have eta going to zero inside the no in for this application we don't we not need we don't need the eta to go inside again but if you want the in the application the statement is the following if I want to do as well as b if I want to do epsilon if I want to do 0.0 If I want to do 0.01 as well as dp, then there's going to be a bounded depth circuit, but the depth will depend on 0.01. Right. Okay, and yeah, so this, it seems, it's hopeless that this would work for large q, I guess. Yeah, yeah, no, this, yeah, something like that does not. We know that it doesn't work for large q, right? So this is just for q equals q, right? For large q, we know that. Q right for large q, we know that you know, once you add noise, the y bp of y and x are independent, right? So, this when you're above the cast and stable or below the away from the cast. Yes. Okay, so Linden was asking if you could go over the AC0 sure. So, I see, let me start with the definition quickly. So, I see zero is the weakest class. AC0 is the class of Is the class of bounded depth circuits with just n and all, no majorities. It's exactly what we had before, but no majority. And the theorem here is that AC naught, if you are given any algorithm of AC naught, AC naught, and you apply it to this vector xh, you are going to classify x naught just as well as a random bit. So if I just apply a constant number of levels of end and all, from this I'm trying to estimate what the root is, then. Estimate what the root is, then it's going, I cannot do better than random. I mean, this is not trivial to prove. I mean, I think, yeah, to act.