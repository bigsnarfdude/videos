And get together everyone and hear about so many great topics. But today I'm going to talk about something related to entry-wise expector analysis for network and the text models. We often see this kind of low-rank models in various applications, like the block model for network data. Another example I will show today is the top model for test data. So for these kinds of problems, the data matrix is Is the sum of a signal matrix plus a mean zero noise matrix. And the signal matrix has a low rank. And if we let psi to contain the eigenvectors of the signal matrix, then it contains all the information needed for estimating the model parameters. But we don't observe x0. So in spectral methods, what we do is that we just compute the leading eigenvectors of the whole. Leading eigenvectors of the whole data matrix and then use cassi hat. So I think spectral methods are very popular in practice. One reason is that it is very fast. So eigen decomposition is actually a non-convex optimization problem, and I think it is probably the only non-convex optimization that we have the fastest over in all the packages. And the question is that how much, how close are the empirical Are the empirical end vectors to the population eigenvectors? So we need to know this to analyze the arrows of the spectral methods. And I believe a lot of people have used the famous sine-theta theorem. So here, the left-hand side is a sine-zeta distance between cassette hat and cassette. So we can, it's not about how close Kai hat is to Cai, it's actually about how close the column space of these two matrices are. And this rotation. And this rotation matrix is actually stochastic. And if you want to make it non-stochastic, you have to make stronger assumptions, such as eigengap assumptions. Unfortunately, for many spectral algorithms, including spectral class 3 and the mixed score algorithm we have already seen, their output are invariant to any rotation. So the existence of this rotation here is not a problem. Now, how to bound this? Yeah, on the right-hand side, Yeah, on the right-hand side, you can see that this is the spectral norm of the noise matrix. So the convenience of the sine-sega theorem is that you never need to touch the eigenvectors themselves. All you need to do is just to study the spectrum of the noise matrix. But there is no free launch. So as the price we pay, we actually cannot get strong conclusions. So for example, if you consider spectral clustering, this only tells you something about the sub-squared. Tells you something about the sub-square error, but not individual errors. So, in order to gather the strong results we need, and especially needed for very downstream tasks, we do need entry-wise eigenvector analysis. And we want to clarify that many papers did entry-wise eigenvector analysis, but it is actually not. So, it is what I call the 2 to infinity bond. So, this is the left-hand of the sine theta. It's actually, this is an n-by-k matrix. So, this is about k matrix. So this is about the sum of the squares of these n rows in this matrix. And then 2 to infinity bound says that you just look at the L Trumum of each individual row and then take the maximum. And obviously we expect this to be much smaller than this. However, when we do 2, 2 infinity mount, the implication is that we think the bounds for different rows are roughly at the same order. Otherwise, taking the maximum will be Otherwise, taking the maximum will be too conservative. Unfortunately, for many real problems, we have very severe heterogeneity. So, for example, in this very famous paper by Brabask and Albert, they mentioned this power loading in many real networks. They basically say that some degrees of some words can be thousands of times higher than some others. And also, in natural languages, we have the zip score. Because of this severe heterogeneity in real data, the In real data, the bound for different rows of these eigenvectors can be at very different orders. And my goal today is to figure out these bounds so that they can be customized for different rows. And then this is a brief literature for the study of 2 to infinity bounds. And please excuse me if I miss any important reference here. So the earlier studies come from the random matrix theory people. They mainly care about some simple models. Care about some simple models like predicting graphs. And again, statistical literature. In the third literature, we have people studying this signal, general signal plus noise model. And I categorize them into two brief points. So in this line of work, we put a no distributional assumption on the noise. Then the technique is mainly about doing the perturbation analysis for eigen decomposition. And then in this work, they assume some mild, like independent. Some model like independence of the entries assumption on the noise matrix, and they use the leap one-out technique. But basically, for all these works, their limitations are: first, they can only deal with the data with no severe heterogeneity. So that the 2,2-infinity bond is enough. But for data with severe heterogeneity, 2,2-infinity bond is too conservative. Second thing is that they focus on the original data matrix, but not the normalized data matrix. Normalize the data matrix. So, for example, for network analysis, we often use the normalized graph or plusion. So, even this very, very simple normalization can cause a big trouble. So, in this paper, they just studied how to do this for the simple stochastic flock model, and they found that this is already very difficult compared with studying the adjacency matrix. Okay, for today, I have a my focus is that I have a relatively ambitious goal. So, I will provide the So I will provide the entry-wise eigenvector analysis mainly for the normalized adjacency matrix of the networks. And I will also mention normalized La Partial matrix. I will mention adjacency as a remark, but this might be easier to analyze. And then in the last two slides, I will briefly mention that I can also do the same thing for the word count matrix. Why is my problem challenging? First, the noise matrix is not independent, does not have Matrix does not have independent entries. So, some sort of the dependence comes from the data generating itself. For example, in topic models, because of multinomial counts, we naturally have dependence there. Sometimes the dependence comes from doing the normalization. For example, we normalize adjacency to graph Laplace, to normalize to Laplace, then we have dependence across all the entries. Second challenge is that I want to accommodate the CV or degree heterogeneity. GT. So that these bounds will be at different orders. The last thing that I want to mention is count data. So counts data are very different from the Gaussian data. The reason is that, well, we, so for example, when you apply the concentration inequalities, many of them depend on the sub-Gaussian norm. But for example, for Berlini counts, the sub-Gaussian norm is just one. It's definitely too conservative to use. So because of these challenges, I needed to develop a new theory and a new technology. Of a new theory and a new technique. And I will focus on the network model. And this is the symmetric adjacency matrix. And then the normalized La portion is defined in this way. So I take the degrees of the nodes, and then I multiply the adjacency, anomalize the adjacency matrix using the degrees in this way. Since here, I have minus sign here. So if for some nodes the degrees are too small, this H will blow up. So in order to prevent that issue, So in order to prevent that issue, I use the regularized version. Here I put a reach regularization here. And this tau can be an arbitrary constant. I usually set a tau to be equal to one. Next, I'm going to show you how do I provide, how I will provide an entry-wise eigenvector analysis result for this normalized applied matrix. And I need a sample model. So this CCMM, unfortunately, we already have talks talking about. We already have talks talking about these models, I don't need to talk about details. I want to mention that for M2I's eigenvectors, we don't really need this specific model. So, this is more like a running model. The nice thing of this DCMM model is that we have these degree parameters, C to I, that directly tells us the heterogeneity in the data. So, here, the Bernoulli probabilities factorize to this part related to degree and this part related to degree. Degree and this part related to the community structure and the mixed membership. So these two are not identifiable. So for a simplistic identifiability, I always assume that these diagonal entries of T are equals to 1. And then this theta i vector basically carries both the overall sparsity. So these theta i's, they carry both the overall sparsity of the network and also the degree hyperchange in the network. Will change the network. And then you can see that the adjacency matrix has this signal part and the noise part. These are the centered benefit entries. It's a generalized weak matrix. This diagonal matrix has negligible effect. So when you have C V or degree heterogeneity, the C5I is a very different order, and then both the signal part and the noise part will be affected by the degree heterogeneity. Okay, now let me present my videos. Now, let me present my results. I will need to show you some regularity conditions. So, first, I assume all the theta i's are bounded by constants. This is very weak, as I mentioned, for most sparse networks, but real networks, they are very sparse. So the theta i's are all very, very small, much smaller than one. And this one is for by the identifiability. And then I introduce this matrix G. So from the definition, you can see that the chaos entries G is basically about how This would be about how the fraction of nodes who have non-zero memberships in both community K and community L. So this G matrix actually captures the balance of communities. And I just put some mild regularity coefficients here. Interesting thing is that if you take the P and the G, so the eigenvalues of this matrix actually controls the population eigenvalues. So if you work on Eigen values. So if you've worked on stochastic block models, we know that the minimum eigenvalue of P captures the signal strength. But here we have mixed membership, we have rural heterogeneity, so it will be replaced by PG, and we use beta n to denote its order. And the last condition is about the sparsity. So n theta bar square is the order of the average node degree. So if beta n is at the constant order and k is bounded, then I can allow the average. Then I can allow the average degree to be as small as five. So, under these assumptions, now I control the difference between the empirical eigenvectors of the normalized La Partian and its population counterpart. So, here I replace A by this Berlini probability matrix omega and H by its expectation. So, in this bond, as you can see, let's look at these two terms. Let's look at these two terms. So, this is the order of the degree of node i. So, if this degree of node i is at the least of the log n, then this term disappears, dominated by this term. And then, this term is actually increasing with c ta i as a square root. But if this bound further falls, if this degree of node i is much smaller than node, then it means that these are. Then it means that these are really really low degree notes. And then this term is much larger than this term. Then you can see that the theta i is actually cancelled. So for those really low degree notes, the bound becomes flat. And here, I'm using a figure to show you what, just to add a visualization of the results, I basically just treat this bound as a function of theta i. As you can see, it's a monotone increasing function. If the function if the it's uh for this range it just increases with c high and then for really really low degree nodes this becomes the flat okay so this result doesn't require any condition about the degree heterogeneity so your theta i's can be very very different okay and of course if you assume the theta i's are the same you can still apply it that you just get a simpler plot okay Okay, now let's see how we can. I want to make a remark before I say the application. I want to make a remark about the adjacency. So for adjacency matrix analysis is actually simpler, but the result, the button we get, will be less satisfactory. So in these papers, by the way, these are methodology papers. I mentioned nothing about entry, but actually in the proof, we did do all these entry-wise eigenvector analysis. We found that for adjacency matrix, so this will factor. So this will factorize out into two terms. So one term depends on theta i and the other term does not depend on theta i. But it turns out that the term does not depend on theta i is governed by theta max. So therefore eventually we actually just get the same bound for all the theta i. So as you can see for adjacency matrix, if we have some really large degree nodes, and then when you do the PCA, because all the high degree and low degree nodes are put together in the PCA. Together in the TCA. So eventually, we will get a lot of noise and it becomes quite bad if the theta max is large. So, this is the reason that we actually recommend using this normalized La Partial matrix. Okay, now given that, we have these kinds of entry-wise exact results, and I want to apply it to study the spectral methods. The spectral methods. So we apply it to this mixed membership estimation problem. So here, this is the data set of the web blog, a data set collected during the 2004 presidential election. That's the first election. Well, the social media started to play a big role. And then we think there are two communities, and then this membership help us get this political oriented. This political orientation of each flower. So this can be extremely liberal or moderately liberal. I mean, 0.5, 0.5 is actually a new true. And I gave a talk just two days ago in a workshop to the e-com people. They are very excited about seeing this, but they are very interested in seeing the political orientations of different parties and see how it can affect the other things. Things. So estimating these mixed memberships is indeed very useful, many real data. And Professor Xi also mentioned another application where we use this to analyze the co-authorship and co-citation networks and statisticians. And this is a mixed school algorithm. And Professor Ben and Professor Jing both mentioned it in our talks. So I don't think I need any other, I don't want to repeat it. Want to repeat it. But I do want to make a connection to the entry-wise eigenvector analysis. Here we construct this score matrix. The idea is to normalize each from the case segment to the case eigenvector by the first eigenvector. And this is an n by k minus 1 matrix. So if we look at row by row, each row of this r hat matrix is only related to i's row of casi hat. row of cosi hat. So therefore, if we have entry-wise eigenvector analysis, we'll be able to give precise large deviation bound for each row of this r hat. And then we plug it into the remaining steps. We are able to get the error bounds for estimating each individual pi i. This is the result. Okay, first, previously, I have just put a condition on the case eigenvalue of p. eigenvalue of p times ga. Now I will add one more condition. It's about the first gap between the first eigenvalue and the remaining ones. The reason is that here in the score I needed to use the first eigenvector and it's special. So I need another eigengap between first and second. And since this PG is a non-negative matrix, so by parent theorem for non-negative matrices, the multiplicity of the first eigenvector, eigenvector eigenvector, eigenvector is guaranteed to have a multiplicity of only one. So it's okay to assume a gap between these two. And then beta n is replaced by the delta n. And then I have this node-wise error rate. So in this node I wise error rate, first, it's the minimum between this one and the one. The reason is that the L1 war of each of them is one. So therefore this sum is at most true. So that's why we have the minimum of this guy. So that's why we have the minimum of this guy and the one. And then if you look at the bound, you can see that now it is a monotonous decreasing function of the CDIs. So previously we have seen that in eigenvectors, the higher degree, the more noise you see in that entry, but in that row. But the signals are also stronger. So if we look at the signal-to-noise ratio, we actually have higher signal-to-noise ratio for higher degree nodes. So that's why this part becomes a monotone sequencing. Becomes a mono-half decrease function. How does this bound depend on? Because obviously there should be alpha n and beta n somewhere here. How are they and yeah, yeah, that's a good point. So the delta n is the minimum of alpha n divided by square of k. And oh sorry, this is a typo here. It should be k plus, should be square root of k alpha n. For most, you will see examples. For most examples, delta n is just the beta n. Course, delta n is just the beta n itself. Okay, and then you can see that this is a minimum of theta bar and the theta i. So if the theta i is significantly slower than theta bar, this decreases, this actually increases with zeta i at a 1 over square root rate. However, if the zeta i just exceeds theta bar, it becomes a flat again. The reason is that uh in this whole procedure, we actually needed to first uh do some vertex hunking. First, do some vertex hunting. And then that's kind of related to some global quantities of this whole model. So for those global quantities, the accuracy is actually controlled by zeta bar. That's why then for theta i larger, that's true together even faster. Okay, then I want to show you a few examples, four examples, just simple correlator. So we consider this special key, where the diagnoses are one and off-diagnoses are pn. And then our And then I will assume that the theta i is alpha n is drawn from this fixed distribution f times a n. So a n captures the sparsity, the whole sparsity level, and this distribution f just captures the degree heterogeneity. The first example is when the f is bounded above and below by constants. So that means the maximum and minimum CDI's are at the same order. Then in this case, we can get Then in this case, we can gather the error rate in this way. So, in this error rate, this quantity here is closely related to a quantity in the stochastic block model study. People know that if this quantity is large enough, and then you can get an exact recovery. But for mixed membership estimation, no, there is never exact recovery. This quantity actually appears in the bound. This is for the moderate degree heterogeneity. So, what if we have severe degree heterogeneity? So, what if we have severe degree heterogeneity? Let me assume that my F, this distribution, is a gamma distribution. Okay, so first, gamma is actually unbounded, but it's not a problem here. The reason is that I'm not assuming sita i is to be bound. I'm assuming this one times a n, and the a n can be really, really small, like it's into a minus order. And then when I draw gamma, I just truncate that at an into plus order, something plus, so it won't affect the result. Flash, so it won't affect the results. Okay, and then in this gamma distribution, when x is large, it looks like exponential, or when x is small, and especially where alpha is smaller than 1, you actually get an unbounded density here. And then applying our result, you can get the rig looks like this. And if this alpha is smaller than one half, okay, so if this alpha is larger than 1 half. This alpha is larger than one half. This rate is actually the same as what I get here. The same as the moderate degree heterogeneity case. But if alpha is smaller than one half, so that means you have too many low degree nodes. And we know that the lower degree actually is the more difficult to estimate the alkai highs. So if you have too many low degree, extremely low degree nodes, the rate will be significantly smaller. So this shows that the So this shows that the severe degree heterogeneity does slow down the rate. It does cause problems. That does make the problem more difficult. Okay, then this is the Pareto distribution. So a Pareto distribution is often used to mimic the power law degree in the real networks. So in this Pareto distribution, it has a support bounded below from zero. Okay? And then if you look at, so there is no You look at so there is no extremely low degree nodes, but we can have some really high degree nodes. This is because this CDF, if you take it, it decays just polynomially fast. So it's not very fast. Then in this case, surprisingly, the rate is still the same. So the reason is that if you have some really high degree notes here, they actually just make the task even simpler because the signal-to-noise ratio for Simpler because the signal-to-noise ratio for higher liquid modes is actually higher. So it's not a big problem. Then I have another example where I just assume that the theta i's after the scaling a n, the theta i's only take finitely many values, and these are the fraction probabilities. And you can see that if this xj is larger than 1, this part never causes any trouble. But if this xj is significantly smaller than 1. Is significantly smaller than class, means it has some really cluster of really low degrees, and then it may make the rate slower. Okay, so with this special example, now you will ask me, do you have any general conclusion? Yes, so in this paper, we actually provide a very general conclusion. We don't even need a CTA i to be IID from any distribution. This is conditioning on CTA1 to C. When we condition all these CTAIs, how do we describe the degree? How do we describe the degree of heterogeneity? We just define this CDF after remove scaling here. And then this CDF fully captures the degree heterogeneity for arbitrary C-high, arbitrary zone. And then you define this as the baseline rate, the rate for the moderate degree heterogeneity in case. And then the actual error rate is actually a functional of the A functional of this baseline rate and with respect to this CDF here. And you don't have to worry about if this integral exists or not, because this one only takes finitely many possible values. So this integral always exists. So this is true for arbitrary degree heterogeneity. This is the upper bound. This is the bound I obtained using the mixed score algorithm. Then we also have a matching lower bound. Have a matching lower bound. And in this matching lower bound, the supremum is conditioned on the, given the theta, I just take the supremum over all possible, over other parameters. So therefore, the rate is what I call the degree heterogeneity aware rate. So the rate of ware depends on the theta. And you can see that we can exactly match. Okay, so these are some simulations. So here, Simulations. So here, this figure shows us how the node-wise arrow will depend on the CI. It's actually a monotonous decreening function of the CTI. Becomes flat for extremely small, low-degree nodes and extremely high-degree nodes. The most interesting part is actually this regime. So we want to see if the theory matches with our simulations. So what we do is that this is our theta i from the IID. Theta i from the IID from the Pareto distribution, so that we have enough heterogeneity there. And then for each node, we actually know the theta line. We can also evaluate the error at that node. Then I do this log log plot. So if I get a straight line with a slope of minus one half, then the theory and the theory is verified. So it turns out that this is. So it turns out that this is very close to a line with slow finance on paths. Then I want to spend a slide showing you how we prove it. The proof is actually pretty complicated. So I have to make some preparation. So to do the preparation, I have to first tell you how people do this problem if they assume the noise matrix has independent attributes. So in my problem, the noise is not independent. My problem, the noise is not independent. But if we have the independent entry, then we can do this leave one out technique. This is the definition of the eigenvector. And then I put this lamp hat to the left-hand side. And then cassi-hat just equals to A times Cai hat. So the ith entry of cosine hat is just an inner product between cosine hat itself and the i-th column of a. But this if these two are independent of each other, I can apply standard large deviation inequalities. Unfortunately, cassette hat also depends on the ice colour program. How can we do that? Leave one out means I'm trying to create a proxy of cassette hat so that I completely remove the effect of the ice roll of A. The idea is that I will create a tutor where I just set the I. A tilde, where I just set the ice row and the column of A to B0. So this is symmetric features. I'm going to do this. And then I take the first eigenvector of this A tilde. Now this casay tilde is completely independent of the i column of A. Okay? Now if I can replace this casay hat by casay tilta, now I get independence, so I can do standard large deviation analysis. Okay? And then furthermore, And then, furthermore, if you can further show that the cassette tilde is also close to the true casi, then you can replace this by the true casai. So if you replace by the true cassette, then this part, this vector no longer depends on which i you leave out. So this becomes the same vector for all the i's. But the cassette tilde depends on which, oh, we're all called unique out. So this vector is what they call the first order approximation. So you can go to this paper. So you can go to this paper for details of this technique. Basically, the idea is very simple to understand, but the analysis you can imagine is very difficult. It's because all these approximations needed to bundle them very carefully. For my problem, it's even more challenging. Why is it even more challenging? I actually spend a few years thinking about this one, just with many, many unsuccessful attempts. Okay, first, there is no longer any. First, there is no longer any dependence, independence in my noise matrix, because of this Laplacian normalization. Therefore, how to define the leave one out becomes very tricky. Second, we have this H. So one idea to deal with is that how about if I just replace this diagonal matrix by its population counterpart? Okay? Then I get independence again. But turns out even though this is a diagonal matrix, But turns out, even though this is a diagonal metric, if you just replace this, the arrow is too huge to control. No way of doing that. Okay, and then we found some way to actually conduct the leaf out. And then we try to bound the leaf1out arrow. That is the error of replacing Z hat by the leaf1out version. It turns out that this is very complicated because of the severe degree heterogeneity. So a lot of crude bonding does not work at all. Does not work at all. And the last thing is that we want to mention is that this first-order approximation introduced by this paper is not a good approximation for the cassette our problem because of the very severe degree heterogeneity. And I can briefly show you how we do that. So first we do the leap one out of the adjacency matrix. You can just completely remove it, or you can just remove the noise part, but keep the expectation. And then, as you may have thought, And then, as you may have thought about it, may have expected, we are also to leave out for the diagonal matrix we use to normalize. But this diagonal matrix, if you want to remove the effects of the ice row and the column of A, you actually have to do, it will affect every entry of this diagram, which is H. So we create this version, and then this is our leave one-out version. Okay, now from this leave one-out version to this actual popsicle. Version to this actual postulation version, you can see that we change both a to omega and hi to h0. We cannot analyze simultaneously, we need to introduce some intermediate quantities. And you can replace this one first, get this one, and try to make a link here. Does not work. So the only way is that you can replace this one by omega and then go through this link. The reason is that from here to here, as I mentioned, replacing this diagonal matrix, H. This diagonal matrix, H, by its population counterpart introduces a lot of errors. So, in order to bypass that difficulty, I want to make sure that these two matrices are both exactly low rank. Then I can bypass the difficulty. So, this is why I can only take this route. So, if I do this route, and then I can control these two to be very close to each other, and then the problem is to control cosai hat and cassai tu hat. And then, in my next slide, I will no longer. And then in my next slide, I will no longer have a cassette, I will just have cassette kilta. So you can treat this cassette kilta as actually my population quantity, because these two are already very close. Okay. Now when you bound these two, it turns out that you cannot just study the leave-one out effect and then study the leave-one out impact. So this is then how the leave-one-out version gets close to the population quantity. It's impossible to do it separately because Impossible to do it separately because everything is intertwined. So when you study this, this one will come up. And when you study this, this one will come up. So this is our T lemma. As you can see, I try to bound this guy between Cafe hat and the Cafe Qta. This is now my population quantity. And then I decompose it into three terms. And then this is the, to study how the leave-one out version behaves, this is the effect of leave-one out. But when you study this term, But when you study this term, the effect one out appears again. And when you, okay, then I can further bound it. But when I further bound it, this term appears again. So basically everyone, everything is intertwined. Okay, so the idea is that here is that we basically get, eventually we get some inequality, while this one is on the both left-hand side and the right-hand side. And then we can, since the coefficients on the right-hand side in front of these points are much smaller, so we can move it to the left-hand side. The left hand side. That's the idea. Okay, so I think I still have a few five minutes. Okay, we start a little early. Five minutes. Okay, and then this is how we prove it. I want to show that I think these techniques are quite general and can be extended. So one extension is to consider the directed network. We can propose the directed DCMM model. And then when we do the normalization, And then when we do the normalization, we use the left and right degrees to do that. So this is a relatively straightforward extension. Another extension is to the general rank K-mount. So this is even more straightforward because in our entry-wise eigenvector analysis, we actually don't need the particular structure. We just need some bound on the individual entries. Now everything goes. Another thing that is very interesting is that now more and more people are interested in the multi- More people are interested in the multi-view network models, like Mariana actually mentioned something like that. I think Shuko will also talk about something like that. So, when you have multiple networks, you actually get a tensor. So, you can do a tensor decomposition, or another way is that, because these are counts data, we are not negative data, you can actually pull the adjacency matrix together, you've got to pull it, then you can do the eigen decalation. I would say that I'm currently working on this problem, I would say that this actually. Problem, I would say that this is actually easier than the original problem. Why? It's because when you pull the ATs, you get more Gaussian nets for each individual entry. So it actually makes the problem slightly easier. Okay, and then just last few couple minutes, I want to say that the going beyond the network model, so the text data is another top sort of data. It's very has very similar phenomena. has very similar phenomenon. So in this text data, we get an unbiased C count matrix. So you can view it as a bipartite weighted network. So we count the count for each word in each document. So it's just a bipartite network. And then the topping model is a low-rank model. So here, these toppings are like the factor. It's one sort of factor. Factor is one sort of factor, and then these are the topic widths of documents. These are another sort of factor. And therefore, this problem, the noise, is not additive noise, it actually naturally arises from this multinomial distribution. And then we have another, I have two papers, one already published and one will post the archive very soon. So we basically studied the singular background. The singular vectors of the data matrix subject to some proper normalization. And then again, we can get the very precise entrywise analysis. And so, here I want to highlight is that the bound will depends on the sum of the J row of the topic major. What are they the quality back? So, this shows the overall frequency of this word in the whole compass. So, for some common words, So for some common words, this HJ can be very large, but for some infrequent words, this HJ can be very small. But no matter how HJ looks like, we can get the precise entry-wise bound. And then given this entry-wise bound, we can then further get the optimality of the spectral methods for doing topic modeling. I will wrap up my talk with some take-home messages. So I think the spectral methods are very powerful in many. Methods are very powerful in many applications with the low-ranking structure. And today I spend most of my time talking about the eigen analysis of the normalized La Partial matrix. And the entry-wise eigenvector analysis is kind of result much stronger than what we can get from sine-zeta theorem, also much stronger than what we can get from the 2,2-infinity bounds. And with this kind of result, we can derive a lot of results we want for downstream tasks. And to get this kind of result, we can get. And to get this kind of, we do need some new techniques. Today I actually use quite a sophisticated leave one-out technique. And then these kind of results have a lot of extensions. So most of the results I presented today is in this archive technique. Thank you. Any questions? I just had a small reference on this multi-layer networks. On this multi-layer networks with this degree heterogeneity, I think there is this paper where they are studying this GDPG setting. I think it was a recent paper by, I don't know, Achtelberg, Arroya, and people that... Yeah, yeah, it's thanks. Yeah, I think I agree. There are many, many working in this. I don't think there are many. I'm just telling you because you have to kind of compare, and maybe you're doing much better. But you're doing much better, it will be even better for you. Yeah, yeah, I agree. So if I usually write a paper about myself, I would definitely cite those without either. Thanks for pointing out the reference. Maybe one small remark. So when you say, right, we put a network trigger, maybe the analysis is easier. I quite doubt it, okay? Because in NatEU network you In in not EU network, you will allow the network to be even sparser. Yeah, exactly. Exactly. I I do have a well yeah, exactly. So if you if you look at the absolute sum, you look at the sum of these Bernoulli entries, right? If that entry is actually much smaller than one, the problem is kind of the same difficulty as the original one. But if each individual network's 50 level, for example, is lower, then when you pull Holy flower, and when you're poor, you actually get more Gaussian names. So, yeah, I completely agree with this. Yeah. Any questions? Nothing's the speaker again. Thank you. I guess we'll have another quick five minutes for a hand over. 