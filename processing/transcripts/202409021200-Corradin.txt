Good. So, short version, I'm talking about nesting component random metrics. And I have to say sorry twice. The first is I'm sorry because maybe the first part of the talk is a bit too much an introduction. I was not expecting too much by using optimometric people in the room. And the second, I'm sorry because I don't have the same drawing skill of Jeff. So I have some draws that are way worse than his draws. Are way worse than these rows? That would be part of the power and update. Sure summary: I'm reviewing quickly completely random measure and compound random measure, and then I'm presenting the two main contributions that we are providing, and there's a posterior characterization of this compound random measure and extending the compound random measure to the nested case. And everything is motivated by an ecological application where we are trying to define class. To define clustering structural and distributional conjecture on the distribution of particulate matter data in Lombardy, which is one of the worst areas in Europe about particulate matter. And just final conclusion. I'm using column in the slide. They are consistent just within a single slide, not across slides. Well, short introduction. Completely random measures are a class of random measures that can be Random measure that can be represented at random jumps at random location, and which have this fantastic property. If you take at this joint composition of subset of their support and you evaluate the measure on this subset, you get something that is mutually independent. Not just a fancy property, you inherit a lot of tractability from this property. And it's something we are trying to preserve even later. They can be Serve even later. They can be seen as a functional of a Poisson process, is a quite useful representation for the next slides. And in particular, the whole measure is characterized by the intensity of this Poisson process. And we are dealing with a Poisson process where the intensity factorizes in two terms: one that is modeling the jumps and one that is modeling the location where the measure is jumping. And basically, you can see that as Mark Poisson process. See that as Mach Poisson processes. And these measures can be then normalized and used as a probability measure. They have been done a lot in the literature. Early contribution was regarded at all. And they're quite flexible and powerful. And by choosing different intensity functions, you can have different structures of the measure that you're obtaining, something that concentrates mass in fewer weights or distributes the mass across weights. Basically, what we're doing is that. Basically, what we're doing is that you take up a process and then you mark it with some atoms that, in this case, are the thetas. And this is the teleboard drawing far away from Jeff. Now, we have to jump in somehow independence things. Instead of considering a single measure, we want to consider a vector of measure. There's plenty of strategy to do that in literature. I see many people in the room that propose different strategies in the years. Good. And among the possible things. And among the possible things that we can do here, I'm exploring compound random measure. And compound random measure were introduced a few years ago by Griffin and Leisen. And the idea is that you start from a common intensity and you apply a transformation to this common intensity, obtaining a multivariate intensity that somehow is borrowing information over the jumps, while the locations are fully shared across the dimensions. Tricky construction, what you get is preserving some properties. What you get is preserving some properties. So, marginally, you have completely random measure as well. How you choose the function that is transforming the intensity that is called score distribution? Well, you can choose in a different way. It should be like possible to be integrated, etc. But in particular, if you choose it by factorizing it, everything becomes more tractable. Makes also sense because it means that how you transform one dimension is not affecting how you transform. Is not affecting how you transform another dimension, and from a modeling perspective, makes sense. So, all the borrowing of information is captured by the latent measure. What you obtain as results is a vector of completely random measure, whereas basically each dimension is a completely random measure itself, where you have a common part JI times a direct function of the diatons multiplied by a group dimension-specific terms, mji. Terms mji here within this notation. We get this vector, we normalize it, and we get a vector of dependent random probability measure. With the same television skill, you get up on some process, you mark it, and then you apply transformation on the jumps. And somehow you're changing how you allocate the mass on specific atoms by this multiplicative term specific for each dimension. Among the cases, Among the cases, something that I'm considering later in the slide, is that if you take a stable distribution as the common latent intensity and a gamma distribution as core distribution, factorization that each term is a gamma distribution, everything is more tractable. And you can also somehow study the dependencies. And basically, when sigma is increasing, the distance. The distance from the common measure and the dimension-specific measure is decreasing, so everything became more similar. As far as when pi is increasing, everything became more similar. So, you can somehow play with how much closer the dimension to the latent measure. Now, our main contribution, one of the main contributions that we proposed was to derive a positive. Proposed was to derive a posterior characterization of this object to perform somehow a conditional inference. And just some notation, I'm assuming here that theta1, bold, theta d bold are a d-dimensional sample, partially exchangeable from the model that you see in the top part of the slide. Sample can have ties within dimension and across dimension. You have some bordering of information across dimension. Basically, you can observe things. Can observe things, the same thing more times. And just as notation, theta one star, theta k star are the unique values observed within all the dimensions together. And I'm denoting by nj, how many times I'm observing a specific atom across all the dimensions together. Now, the posterior distribution of this guy drops down in Drop down in is can be break down in two terms. The first term is the red one, is a restriction of the complete compound random measure vector to all the support without the atoms that we observe. And then we have the green parts that is what happens on the atoms that we observe a posteriorly. The first part is a compound random measure itself with updated intensity function that depends somehow. Depends somehow on the score distribution that we're assuming and the latent intensity that we're assuming. So, same family, just updated parameters. Green parts is a measure with random jumps at certain location, and the distribution of the jumps depends on the intensity that we have plus the score distribution that we're assuming. So, basically, we have these two terms, and it seems terrible from terrible from equation perspective but is quite practical as far as you make suitable choice of f and new star. Now, cool things is that one of my favorite properties of completely random measure is a quasi-self similarity. So for a Dirichlet process you have a complete self-similarity is that when you restrict the measure to a subset of the support you are in the same family. Here happens the same things in the sense that the red part in But the red part, oops, oops, too many oops. When you restrict the measure, you get something that is in the same family, just updating the parameters. What is the cool part of this is that I can use a standard computational strategy that I can use for random measure, because I'm somehow in the same family with marginal distributions that are completely random measure. So, from a computational perspective, I have something that is easily tractable. Basically, what we are doing is that I'm capting the support somehow, and what is left on the support is still a compound random measure, but with updated intensity and score distribution. Second thing is that with suitable parametric choice here with gamma distributed scores, everything is more tractable in the sense that if you look back here, f prime is e to the power something time f. e to the power something time f. With a gamma, you are still in a gamma family, and similarly for the psi function. So that's nice. And of course I can somehow embed this measure in a mixed model framework. So I get this measure, I have my fantastic posterior, and normalize this measure, and I have a vector of random probability measure that I can convolute with a kernel. I obtain a vector of random densities and the densities And the density support depends on the choice that I'm making on a kernel function k here. And the mixing distribution are almost fully discrete. So at the same time, I can perform density estimate and clustering also across dimensions. And nice things is that the mixer model inherits the same posterior representation from the compound random measure. And so we can perform conditional inference even in this case. For example, we can. In this case, and for example, we exploited Ferguson and class algorithm just because we were playing with the stable latent intensity. So, what you're doing is that you combine your measure with a kernel and you obtain random densities somehow that vary across dimensions. Now, one step further is that recently in literature there has been some tension about clustering also distributions. Somehow you want to Somehow you want to not just estimate this density on different dimensions, not just clustering observation, but also cluster together distributions that are similar. And the idea is tricky, it's just that you add one layer in the hierarchy of your model, and so the measure that you are using, the dimension-specific measure that you are using, are sampling from something that is Sampling from something that is discrete, here Q tilde. And this Q tilde have random jobs, a location, and these locations are given by a vector of compound random measure. Peculiar things of this specification of the model is that you range from a full exchangeability of the observation to a full partially exchangeable case. You range somehow between these two scenarios, and the position where you are depends on how many. Position where you are depends on how many dimensions you're clustering together, and this strategy can be used to do clustering across distributions that are almost fully discrete or also convolute with a kernel later. They inherit the same posterior characterization of compound random measure. The only added step here is that you have to care about the clustering together different dimensions, and Q tilde can be also. Q tilde can be also in finite dimensional if you want to have a more flexible model. Of course, we can convolute this with a kernel, and what we obtain now is a vector of random density with ties. So, some of the dimensions are the same, some of the dimensions are different. And within that, we can do distributional clustering on a general level. On a general level. The posterior representation also, even if you are considering the nested mixture compound case. And posterior inference is quite trivial here. You just need to care about reallocating each dimension to a specific element in Q tilde. That is something that can done quite smoothly. Basically, with respect to what we were doing before, now some Of what we were doing before, now some of the dimensions coincides and you have the same distribution across some of the dimensions that we are observing. How they work in practice, we made a comparison with Francesco's early contribution, and we consider different scenarios by simulating data from a nested mixture of Gaussian distribution with partially overlapping components. Overlapping components and low or high number of group of data, low is five, high is 20, and low or number or high separation of the components. So the data generated process is more with the components more close to each other or far departing from each other. And comparison with the combonatome model of Francesco, basically we have some similarities in terms of performances. On the left side, you have the accurate. Side, you have the accuracy of the clustering that we are doing across distribution. On the right side is the accuracy of the density estimation. And we are performing quite similar. We are more accurate in the results that we are doing, that we are getting. Now, back to the data. Again, we start from this problem where basically we have this ecological problem. We have this ecological problem in Lombardy. Lombardy is also peculiar because you have some aga that are close to the mountains, some that are super flat, and this within Piano Capadana is an aga in Italy where all the particular metals stay within this valley and it's not going away. And the idea here is to cluster together different provinces with the maybe that they can coordinate actions to react from this problem. This problem. And we want to perform density estimation and clustering also because there's something related to density estimation that I'm showing a bit. So we have these quantities that are positive values. We consider a mixture of log normal distribution with a nested compound random measure mixing distribution so that we can also perform clustering across the distribution. And as parametric choice, we set the weights of the nesting measure distributed as a symmetry. Measure distributed as a symmetric Dirichlet distribution. And we use the stable gamma representation of before, where everything is more tractable, and just a generic base measure for sampling the location that depends on the kernel that we are choosing. As posterior summaries, we are interested in the clustering of the distribution. We resort to a weight procedure based on validation of information. Based on validation of information. And also on density estimation, somehow we want to make some compute some functional of the density. The European Union has some strong regulation about the days that particular matter should not exceed a certain threshold. If you're exceeding more than that, you're getting fined by the European Commission. So that is to quantify the risk of exceeding that for specific Lombard diagram. And you can do that taking a functional And you can do that taking a functional of this density specific for each provinces. So we obtain a clustering quite satisfying in terms of posterior similarity metrics. And basically, we identify three main clusters that also have some kind of geographical interpretation. The green area is close to the mountain, so where the area is fresher, and the particular matter is going down to the people living. To the people living in worse area. The blue one is a bit in the middle, so the area with some green parts or not far from the mountains. The red one is actually where I'm living and where the area is terrible. And so you see Milan, that is one of the most industrialized area in Europe and the area for Radni. So you have these three main clusters. In terms of distribution, the first row is an entire cluster. Is an entire cluster, and then the last three are a separate cluster, and then you have the five ones that are the bad ones. And you see that somehow they have a similar behavior, they are some certain skewness. And basically, for the last one that are the lucky people living close to the mountains, the risk of exceeding the European regulation is quite low, quantifying the Q fifty and with a credible interval. Credible interval down there. Whereas in the area super-industrialized, it is quite terrible in the sense that they are always breaking the European regulation. Now, time to say goodbye. Not on Monday, but some conclusion and take-home message. Well, this model can be quite, this modeling strategy can be nice when you have like, when you want to cluster distribution and you want Cluster distribution, and you want a general flexible strategy where you can play with the intensity that you are choosing that can give you different reinforcement in the measure that you're obtaining. It's coming at a cost in the sense that if the intensity is a bit complex, you need to resort to Fergusonic class representation. This is completely killing the computational side. Things that we want to do in the future is like using the posterior characterization somehow. Using the posterior characterization somehow to both study more theoretical property of this model and to scale up the inference that we can do. And also, we are exploring some modeling strategies that extend the dependence structure presented in the last slides. And thank you for your attention and some references. If you have some other comments or questions, please text me even after the presentation. Me even after the presentation. Thank you. Buy a weekend house in Borneo. Thank you. Thank you for the suggestion. I don't know if you consider the only consider the stable for the directing measure. Uh actually I play with uh other distribution in gamma, etc. Tractability is the same, you get something different maybe in terms of model fitting, but I saw that depends on the distribution that you're having on the latent parameters. So it's change is affecting somehow how the weights are scaling. But we try to play with different intensity and with different score distribution. Thank you. Thank you. There's not any online question. I cannot see. Okay, perfect. Yeah. Thanks to all the speakers and thanks.