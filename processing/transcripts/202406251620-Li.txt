Thank you, Tivo, for the nice introduction and invitation. So, today I'm going to present our work on macro alpha potential game. That is a joint work with my advisor, Xing, and our co-host Chen Mei Shanka and Man Si. So, let me start by a very simple motivation from the reinforcement learning perspective. So, when we talk about reinforcement learning, usually there is an agent trying to learn to make some decisions by interacting with the environment. The environment. And the multi-agent reinforcement learning simply extends this framework, this setting to a setting where there are employers interacting with each other in a shared environment. So each of the agent will have their own target, while also their strategy may be influenced by other people's strategies. So some examples can be seen in the autonomous driven or auctions. So, to formulate it, in other work, we focus on a setting with discrete time update in a macro game. So basically, we have a finite set of the agents. So, here n denotes we have n players and a finite state space. And each agent i will have a finite set of actions. And UI here denotes the reward at each time step. At each time stamp for agent I. And P here denotes the probability transition of the system. So here we use this pi to denote the policy of each agent, which is a function from the state to a probability simplex on the action set of agent I. And here we use pi to denote the joint policy of everyone's policy and pi-negative i denotes the joint policy. And pine active I denotes the joint policy of everyone except player I. So, generally speaking, the agent I want to maximize her value function, which is defined as a discounted sum of the reward at each step. So such macro games are in general difficult to solve due to the dynamic interactions or some decentralized decision-making requirement. Decision-making requirements, and so on. So, people have studied some tractable instances of macro games. Some of them have worked on the like two-pair zero, two-player zero-sum games, so common interest game. And recently, macro-potential game has also become a popular topic, which is also one of the main inspiration of our work. So, let me quickly talk about the idea of macro-potential game. So, this Potential game. So basically, in a macro potential game, the case vector is that if one agent changed her policy, so here if this agent changed her policy, then the difference can be captured by a common function phi, such that this phi also changed this policy pi from pi i to pi prime. So this phi is a common function for all the players, but this structure holds for all the agent i. Hold for all the agent i's. This is the case structure of the macro potential game. And with such a nice structure, it can be easily shown that the global maximizer of the potential function phi is a Nash equilibrium. So basically, it converts the problem of finding Nash of the game into optimize over one single function. So with such nice structure, some convergent multi-agent reinforcement learning algorithm have been developed. Algorithm have been developed, such as some policy gradient-based works, or some other branch will focus on the best response-based algorithms. So, however, there are still some limitations on the macro-prudential game. One of the key limitations is that it's hard to verify the existence or the construct such a potential function phi. And, like Shin mentioned yesterday, Mentioned yesterday, even games where each stage is a static potential game in general is not a macro potential game. So here the reward function at each state, even it satisfies this potential game structure at each time step in general, even under some simple transitions, is still not an MPG. So therefore, a natural question is that is there a more general framework beyond the macro potential game? Beyond the macro potential game. So in our work, we use this min-max framework to study macro games. So basically, we say that a macro game G is called a macro-alpha potential game with some parameter alpha, and alpha is defined in this form. So basically, if we first focus on the inner maximization problems, this part is motivated from the definition of macrocorps. From the definition of macro potential game. If for any state, any player, and any policy, if this part is zero, then we can conclude that this side is a potential function, this one is just a potential game. But we know that in general, games are not macro-potential game. So we have another minimization problem here by finding a proper function phi to be the common function phi here. So here we found So here we found phi in a class of uniformly epicontinuous functions. So our first results saying that there actually exists a function phi in f such that this alpha as a minimization as a minimization value can be achieved with a function phi here. And we call this function phi as alpha potential function. As alpha potential function. And like I just mentioned, if this phi just yields zero, this part, then bad net definition is just an MPG. So we propose a framework that includes MPG as a special case. So we proposed such a min-max framework to study the general macro games by utilizing this parameter alpha. And there are three major questions we're trying to solve in this. Questions we're trying to solve in this work. So, first, how to find alpha, or at least how to find a proper upper bound for alpha given any macro game. And can we have any algorithm with a natural grade convergence guarantee, either it's policy gradient-based or best response-based? And third is that we want to give some examples that how to use this alpha prudential game framework to characterize some macro games that are known to be B. Macro games that are known to be beyond MPG. So, lastly, I will give this macro congestion game as an example. So, the first question: fun alpha. So, we can reformulate that min-max framework as the following optimization problem. But in general, this optimization problem is still very hard to solve because our decision variable here is a function in a class of functions f. In a class of functions f. So we can first provide a relaxation by considering a subset f tilde in f. So if we consider, so here we consider relaxation instead of optimized over a large function, a large class of function f, we focus on a smaller subset f tilde. And we assume that this f tilde has very specific structure. physics structure. So we assume that for any function in this f tilde, there is a function small phi such that this psi is just a discounted sum of the small phi here. So what is the advantage to doing such relaxation? It's mainly because we can utilize the occupation measure to further simplify our relaxation problem here. So here we So here we denote the state action. We define the state action occupation measure. Basically, it tells us that under policy pi, what is your probability that the state will enter S prime and player will choose A prime on this state. And with this probability measure, we can rewrite our phi in F tilde just as the weighted sum of the phi here, which is weighted. sum of the phi here which is weighted by the occupation matter and similarly for the value function we can write it just as the sum of the reward reward function and our relaxation problems can be written equivalently using the occupation measure here so now our decision variable here is a function on the finite set of state and finite Set of state and finite set of actions. So basically, we can interpret this phi just as some vector. But now we will have a semi-infinite linear programming because this semi-infinite mainly come from the constraint here. We have, due to the policies, we still have uncountable constraint. But nevertheless, we can still solve this programming using some algorithms proposed in the previous work. Previous work. So, just to quickly sum up, given any microphone games, we can use occupation measure to write such a relaxation problem and use algorithm to find the optimal value of these problems. That value is served as an upper bound of the given parameter alpha here. So, this answers our first part. So, the next part is about algorithm and natural graph analysis. So, here we utilize So, here we utilize a common performance measure here, which is called Nash regret. So, Nash regret is the average of the deviation from the Nash equilibrium. So, basically, for each i, Ri measures how much player I can improve given everyone else's policy. And if for any i, this r i is zero, then the pi t is simply in the Nash equilibrium. So, our first algorithm is a policy gradient algorithm, which is originally proposed by Deans and his co-author. They propose this algorithm aiming to solve the macro potential game. And basically, for each player I, they do policy gradient ascent, and the gradient is evaluated through the expected q function here. function here. Our first result is to show that to give a bound for the natural grad using the algorithm one. And this alpha is the gain parameter we just defined. And in the special case that this alpha equal to zero, we recover the result from their paper when they study the macro potential gain. So to introduce the next To introduce the next algorithm, which is the best response-based algorithm, just let me briefly introduce some notations here. So here we introduce an entropy regularized value function. So this part is just the original value function, but here at each time step, we also add the entropy term to the reward. And correspondingly, we define the entropy regularized Q function here. Q function here, and this Qt is just the average of the Q function here by assuming that Ai is sampled from pi i. With those notations, we'll be able to define this maximum improvement of the smooth q function. So here is a current policy, and this part is a maximum the player I can achieve by just devoting her. Can achieve by just debating her own policy. So, this part measures how player I can garner by debating her own policy here. So, our algorithm follows these three steps. So, first, for each I and S, we can get this maximum improvement for each player I and each state S. And we choose the tuple that gives the maximum improvement. And we update. And we update this player I bar's policy in the state S bar with the one stage best response. So we just update these players at this stage. At the other stage, the policy just keeps the same. And those steps are repeated until no more improvements can be made and returns the policy profile. So we show that under So we show that under proper regularity conditions we can provide the natural grad for such a sequential best response algorithm. And here is like a comparison between the policy gradient algorithm and sequential best response algorithm. In terms of the training epoch, the dependence is the same. The sequential best response algorithm has a better dependence on the number of Better dependence on the number of players. And in terms of the game parameters, the positive gradient method has a better dependence on the game parameter alpha. So finally, let me quickly give an example about how to use this marker of alpha potential game framework to characterize some games. Yeah. So yesterday Shin briefly talked about So yesterday Shin briefly talked about the idea of static congestion game, where each agent just choose a resource from a set of resources. And the cost of each resource is depending on how many people using it. So it's just a congestion game. And the macro of congestion, the macro congestion game is just like for each state, we're following this rule, but now we have another state transitions, and the players will need to choose the resources at each. Need to choose the resources at each time step. So, to quickly formulate it, we have m players, and each time the agent we assume it choose one resources, and e denote the total resources. And for the action of the player I, it's just the vectors of 0, 1 to denoting which resources the agent is using. And here, this one denotes the aggregated demand, which is just this part. Which is just this part just measure that under the action A, how many people are using E here. And the cost of each resource is just depending on the current state and how many people are using it. And the reward for the agent I is just the sum of all the resources and the cost for each resource. So to come up with a proprietary candidate potential function. It candidate potential function for Markov congestion game. We utilize the structure that since for each state, at each time step, it is a static congestion game, which means that there is a static potential function phi satisfy this equation. Therefore, we propose that we consider a function phi defined as a discounted sum of this static potential. So under such construction of the function psi, we are able to show that under proper regularity conditions, we show that alpha, the alpha is upper bounded by this constant. So we can first show that this side here is bounded and it's not depending on n. And overall, our alpha is bounded by the one over n. By the one over n here. So finally, just some numerical result of using POSI gradient and sequential best response. This gives that under different state, what is the distribution of players using different resources, A, B, C, D. And here is just the convergence of the two algorithm, which shows that both algorithms can find the Nash equilibrium in this macro congestion game case. Macro congestion game case. So, to sum up, we propose this min-max framework to study general macro games by quantifying the parameter alpha, and we can formulate relaxation problems using the state action occupation measure. Finally, we propose two algorithms, positive, gradient-based, and best response-based, and provide the cross-bounding NASH. And we utilize this framework to characterize micro-congestion games that are beyond micro. Macro contesting games that are beyond macro games. Yeah, so that's all for my presentation. Thank you, and any questions?