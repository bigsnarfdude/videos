Yes. Sorry to be separating you from dinner and also we're running late so I'll try to be as quick as possible if I don't quicker. Okay, so just because it's the first time meeting many of you all in person or at all, I wanted to tell you a little bit about the things I do. So of course this sits in numerical linear algebra, but I also work a lot on spectral graph theory, questions and theoretical computer science, machine learning, especially from a matrix framework, things like determinantal point processes. External point processes. So, if you're interested in these two other things, feel free to talk to me about it. The outline for the talk is: well, let's see what Gaussian elimination is. That should go very quickly. Talk about growth factor and a little bit about why we should care, although maybe I'm lying to you a little bit there. A quick history of the question and talk about some recent results, some things that Alan and I have been working on. All right, Daush elimination. Alright, Gaussian elimination. You eliminate unknowns by subtracting equations from each other. This dates back all the way to the second century. 1600 years before Gaussian. Yes. Happens to play about some cost. The idea of Gaussian elimination is we're performing a sequence of... So he was the last guy to describe it. The most important thing. So what is Gaussian elimination doing? It's a sequence of rank one updates. Doing, it's a sequence of rank one updates where you get you start with some n by n matrix, you perform a rank one update to the lower right block, and eventually you get some one by one at the end. And you do this whole procedure, and what you get out is an LU factorization, where the L is sort of the ratio of pivot to some entry, and the U is just the entries or matrix. Top down each time. So far, so good. So far, so good. And the idea is: given A equals LU, we can solve AS equals B quickly, and we can do other things. Okay. Growth factor. Growth factor is the largest magnitude entry we encounter during Gaussian elimination, scaled by the largest magnitude entry of our matrix. You can say over all i, j, and k, or you can restrict to pivots. We'll be talking about complete pivoting today. So for Pivoting today. So, for this, all these things are equivalent. And if you're maximizing over, let's say, all n by n matrices, really, you only need to concern yourself with the last problem. And the idea is that three factors control our ability to solve a linear system, the growth factor, the condition number, and how many bits of precision we have. And here is one sort of like typical bound you'll find in a normal textbook where here you're Where here you see the dependence on the computed entries of L and U, and these can be large even if our matrix is well conditioned, as shown by this example. So I hope I'm giving some mild sort of argument as to why this is important. So we do need some pivoting, more or less, I'll say. You can do partial pivoting, you swap rows so that the pivot is largest, sort of a Sort of among your first column, and you can get as large as 2 to the n minus 1, and this is tight for this nice little matrix here. But what Alan and I have been focusing on is this question of complete pivoting. So you swap rows and columns so that your pivot is the largest magnitude entry in whatever matrix you're left with. And this has really been of great theoretical interest in the numerical linear algebra community, in part because For a community, in part because you can prove a quasi-polynomial bound on the growth, and also in part because some important people really carry on. This is how things work in computers. So there are some reasonable questions as this polynomial and a conjecture, which was sort of one of the big conjectures in numerical linear algebra for a while, was, is this at most n for real n by n matrices? For real n by n matrices. So, this was conjectured by Wilkinson. It's often called Pryor's conjecture. If you look at Cryer's papers from the 60s, this abstract will say it's a conjecture of Wilkinson. If you look at papers of Tornheim from 65, he'll say a conjecture of Wilkinson. And so, even though in some books we'll see they attribute it to Pryor, I'll say Wilkinson. But for those of you familiar with the problem, you'll know that it doesn't actually really matter. Matter because it's not true. Okay, so a little bit of history. Well, I would say the modern sort of analysis of this sort of thing starts with Goldstein and von Neumann. They were together, they had an actual computer, and they were concerned about errors of solving problems on this computer. And they were one of the first people to really think about Gauss elimination in terms of matrices. Elimination in terms of matrices. I mean, I think Turing is the one who actually coined the idea of like LU, but they were sort of one of the earliest people to really talk about Gaussian elimination in this matrix language. And they refer to complete pending as customary procedure. It is the thing to do. These slides are on the Burrs webpage if you want to take a look. I have a little excerpt from Little excerpt from Meyer's History of Gaussian Elimination, giving some fascinating context about the start of this. Don't read this, we don't have time, but you can look at it on the website if you'd like. In the 60s, Wilkinson really started a rigorous analysis of the error for Gaussian elimination. He proved this bound in 1961, where he's really only using Hanamar's epiphany over and over again. And everyone believes this bound is quite pessimistic. Quite pessimistic, and there was this 1965 conjecture that this growth is at most n for all n by n real matrices. And the sort of, you might ask, well, why n? Well, if you take a Hadamard matrix, it's not too hard to show that the growth is going to be at least n. So the idea is that the growth is n and it's tight for Hadamard matrices. Hadamard matrices are special, which is a nice story. And I think Hadamard matrices are special in their own right anyway. Are special in their own right, anyway. I say for all real matrices because it was known even back in the 60s that if you take a complex matrix, you can get growth larger than n even when n equals 3. Good? Okay, now post-Wilkinson. So I should say, during Wilkinson's time and sort of shortly after, there were lots of papers, sort of growth chasers, as Alan likes to call them. As Alan likes to call them. Lots of people proving this conjecture for small values of n in the 60s, for n equals 3, n equals 4, n equals 5, and proving some really interesting types of things. Things went a little quiet in the 70s and early 80s, and then Nick Trefethen wrote this paper in 1985, Three Mysteries of Gaussian Illumination. And obviously, this is one of the mysteries. Three years later, Day and Peterson were numerically searching. Peterson were numerically searching for large growth using this nonlinear programming Stanford optimization lab library. Trefethen and Schreiber wrote their sort of well-known, now very well-known, average case analysis of growth two years later. And just one year later, Nick Gould, using Lancelot, found an example for n equals 13 in a floating point that is ever so slightly greater than n. And a year later, Greater than n. And a year later, Allen took this example, found that this exact matrix isn't actually completely pivoted, an exact arithmetic, did some jiggling, and then found a counterexample of an exact arithmetic. And also, Alan sort of asked some questions around if you see something that satisfies some bound in floating point, does it mean that you're going to have something in exact arithmetic? Because you take something that's floating point. Because you take something that's floating point, you try to do it in exact arithmetic, and it may not actually be component. And there's been lots of work since then, you know, some amazing results like smooth analysis. There's even a nice paper that came out last year of Wong and Tikamarov looking at average case analysis for Gaussian elimination with partial pivoting. And so there has been a lot of activity since 92 on growth factors, but it's really Growth factors, but it's really been very probabilistic in some ways. And so the true worst-case question: really, there hasn't been so much that's happened. And well, you know, sometimes the problems you work on, the things that you think are important, these things are passed down to you. These things are sort of like instilled in you. And so, you know, Nick advised Alan. Michelle Gomans was actually my advisor at MIT, but Alan was very much. At MIT, but Alan was very much like my secondary advisor. We sort of met every week, and so Alan instilled sort of love and interest in this problem in me. And what Alan and I have been able to do is we've made progress on a couple of fronts. The first being sort of the one we're most excited to show is that this conjecture is actually false for all n greater than or equal to 11, not just this one example. In fact, it's off by a multiplicative constant. By a multiplicative constant. And I'll have some very nice pictures to show that we should really think that this growth factor is super linear. The second thing is I'm going to show some equivalence of this question with a combinatorial version of this question. So the type of question a combinatorialist might care about, they might say, give me a binary matrix, and then show that with complete pivoting. Show that with complete pivoting with these determinantal conditions, that the growth factor is at most something. The growth factor is at least something. And we are able to make sort of sweeping statements about the equivalence of these growth factor questions over these restricted sets or the entire problem. To solve one is to solve all of them up to some small blow-up. And then talking about growth factor and floating point and exact arithmetic, a little bit about how these relate. Bit about how these relate. So, you know, you pick up your favorite numerical analysis textbook, these bounds for sort of Gaussian elimination, they have the growth factor in floating-point arithmetic. But yet, everything I've been showing you, most of these conjectures and most of these questions are looking at exact arithmetic. And you should expect these should be the same, but we have some things where we've made this precise point. So, to start, the main sort of workhorse that we have that allows us Course that we have that allows us to prove some of these theorems is the following type of technical lemma. Where first I want to define some notation. The main thing that helps us is our ability to make statements about sort of classes of matrices that aren't just completely pivoted, but either overly or under-completely pivoted with some sort of factor epsilon. So maybe you have a matrix that's not quite completely pivoted, but Quite completely pivoted, but the pivot is, you know, at most like one plus epsilon times the largest magnitude entry. Or you can make epsilon negative to saying, I don't just want to be completely pivoted. I want my pivot to be actually strictly bigger than everything else by some multiplicative effect. And by proving bounds relating the growth between these sets, you can actually do some things. So, first, a technical lemma. First, a technical lemma. This looks ugly. Don't try to read this fully, but the main idea is: you give me some matrix that isn't completely pivoted, but it's close up to some epsilon factor, and you give me some delta to say, I'd like you to take this and give me a new matrix that's not just completely pivoted, but maybe overly completely pivoted. Like the pivot is actually strictly larger than everything else by some factor. And the sort of level. And the sort of level, which is algorithmic in nature, says, I can actually do this, and my growth factor is not going to be too bad. Like, I can actually produce a matrix that's not going to change too much. So, this is a workhorse that sort of underli all the things we're doing. Okay, so let's talk about this Wilkinson's conjecture being false. So, the idea for finding large growth, and here is a plot showing observed growth factors all the way from n equals. Growth factors all the way from n equals 1 to 75 plus n equals 100. And the theorem is that the growth factor is at least like 1.0045n for all n greater than or equal to 11. You might wonder, oh, that's a very small thing. The smallness of that comes from n equals 11. That's how sort of far off it is from n. So you can't really do much better than this unless you find some matrix for n equals 11 that has very large growth. But it's very large growth, and we show that the limb soup of this ratio is at least 3.3, which is kind of very different from you know, if you look at certain like numerical analysis textbooks, they'll say, yes, you know, growth factor is larger than n, but no one's actually found an example that's larger than, say, like 1.01. You'll find this in some books. And so I want to say, no, we have found them, and if you look at how growth behaves, you really get the sense this behavior. You really get the sense, this being your observed growth factor divided by n, you really get the strong sense that this is super linear in n. And that what we thought before is really off by some factor. And the idea of proving this is using this technical lemma, which I sort of showed you, using some nonlinear optimization software, Jump and IPOpt, using, you guessed it, Julia, and you know, Ellen. So we're doing lots of Julia here. So we're doing lots of Julia here, Halen's cluster, and then sort of a lemma that allows us to take numerically computed matrices with large growth factors that are small to make sort of infinitely many of them. Yes? It's not like concave down, though. So it kind of looks like it factors should be quadratic growth from these numerical relationships. Your words, not mine. No, it looks like. No, it doesn't look like a straight line. It doesn't look like it should be a straight line. And I do want to say that as far as trussing these numbers, all the way up to 25, you should feel pretty good about trussing. This becomes computationally very difficult to do for larger n. And so I should say that from 25 to 75, especially this equals 100, you should feel very speculatively about like this is these are lower bounds. And I only feel pretty good until we get up to 25. Go until we get up to 25. After 25, you should treat it as a very pessimistic lower bound, in my opinion. But is this should this we interpret this as saying is at least quadratic? If you said it, it's your conjecture. I conjectured super linear. Super quadratic is your conjecture. Okay. Perfect. So here's a table of computed values for all these different things. It is in All these different things. It is in my talk on the Birds website. I'm not going to leave this up here for you to stare at. Unless you want to stare at. For your plot, right? So that's for your example, right? And then you said the limb soup goes to 3.3. So what happens? It goes up and then it flattens out now? Oh, I mean, the point is, my sort of extrapolation dilemmas, unfortunately for complete pivoting, don't allow me to gain an export. So, in the same paper, which I should say, So in the same paper, which I should say, this conference has motivated Ellen and I, it's being posted on Archive on Wednesday, is what Ellen and I have said. You'll also see a result for rook pivoting. And there, because of a nice property of rook pivoting, you can actually take large growth for small values, and it can now convert into something that sort of changes in the export. So for rook pivoting, we have super linear. Have super linear. And you can get these sorts of things. So this is a statement. This is like what you can prove. This is what you can prove. My example keeps going up, and I really think the behavior, and in fact, the real question is trying to find mathematical constructions that give you this super linear bound is something that we're actively working on. I'm very interested in. So I wanted to share with you all that this is something we do. When you say achieves this, you mean achieves the thing. Yeah, achieves this lower bound. Achieves these lower bounds? Yes. So these lower bounds are the result of this software spitting out a matrix and me doing some algorithmic post-processing that turns it into a mathematical proof. Because it spits something out, it's inflowing. Spit something out. It's in floating-point arithmetic. It doesn't necessarily mean that there's something with that growth in exact arithmetic. And so you need to do, you know, you need to use like rational big ints, you need to do some things, and you need a mathematically proof procedure that guarantees that you're going to get, well, if you want mathematical proof, that's what you need. Maybe just it's spinning it outside. Can I just ask, is there any, has anyone studied how different the pivot growth can be in finite position versus exactly? In finite position versus exactly? So if you have log, let's say log n bits, it should be the same. Yeah, it should be like, it should be exactly the same. Yeah, more or less. Almost exactly the same. But yeah, this was a question of Alan's actually in a paper from 1970. I don't know if you look at all your matrices, do you see a pattern? Do you have a way to generate larger matrices because you understand what they look like or what they do? No, so this has been quite elusive. This is something we've been trying. The thing that you do notice is that the behavior really does, and here, let me get to the slide so you can see. So it wasn't so important. The behavior. So this is a plot where here we're looking at on the left side the pivots during Gaussian elimination. This is on the log scale. This is n equals 100. These would be the pivots. Would be the pivots if Wilkinson's bound was tight for n equals objects. So Wilkinson has some bound. If you assume that bound is tight, then you get a specific value for each pivot. The blue is an actual matrix that we have. And the yellow is take a Hadamard matrix for n equals 100, and it's one of the completely pivoted versions. What you'll notice is that Wilkinson's bound is really this idea of starting very, very small, very early on. Very small, very early on because really you do have this constraint. You know, by Hannemar's inequality, the sort of sum of the log pivots is constrained to start with very, very small pivots and only grow at the end. And the thing you see over and over again is the sort of slow and steady, very small growth factors early on, and it really spikes towards the end, much more so than you would see from a Hadamard matrix. So matrices that are trying to move towards Matrices that are trying to move towards the sort of Wilkinson-type shape and away from sort of Hadamard. And that is my time, but I will just sort of state that for restricted entries, it's the same up to a quadratic factor. And then also for floating point, if your number of bits is, let's say, log squared n under Wilkinson's bound, but if you believe that growth factor is polynomial, then log n is all you need for this to be the same up to like a one plus. Need for this to be the same up to like a one plus one graph. And I'll move back to this because I'm sure people want to look at it a little bit more. And this is how the determinant behaves. Here, really, a large determinant is persisting kind of more than like Hadamard is very time and Say that again? Sorry, people were for the observed. What matrix are you computing it for? On the right plot? So for the observed, this is a matrix that I found. So you find a matrix, this is, yeah. It's the best matrix I could find for n equals. It's an optimal matrix that we found, right? It's not a random matrix. Yeah, no, it's a good matrix. It's the best one we could find. So yeah, it's the best one we could find. Though there are probably better. Yeah, the best one. So when you run the optimization, presumably it depends on where you start. So if you take different starting point, do you find lots of different ones that are tangled to the same bound? Or are they similar in some way? Yeah, so you start with some random starting point and you need to iterate. You need to do this many times with many different starting points. And so for instance, like I can tell you. So for instance, I can tell you for things computed, let's say between 0 and 25, what we did is we're running this nonlinear software, and we did, I think, 20,000 sort of, you know, starting iterate, 20,000 sort of like initial conditions, and then about 1,000 iterations of some nonlinear software. Up to 1,000 if it converges quickly. I'm having trouble here at all. Does growth factor say anything about the complexity of solving the linear system impact, right? Like, if you want, if you're solving system over queue, like what does the growth factor say about, like, does it say anything about like, I never encounter large numbers like this? Well, yeah, I mean, so the growth factor, yeah, so if you're doing it over Q, it would exactly say that I never encounter does growth factor imply that if I Growth backer imply that if I just start with saving features with like log n bits and I just keep track of like fractions, that I just never, like, I mean, the current bound is something like enter the log n, right? It's the log n plus like things for log squared infrastructure. Does it say that I never encounter rationals with more than log squared infrastructure? Oh. Yeah, sure. It's not that's not obvious. Yeah, that's a different code like it tries to Yeah, there's a different goal that tries to uh keep the uh the entries of LNU in check. But let's let's talk about it. Give me a chance to think about it and let's talk about it tomorrow. After dinner. So I'll follow up on Gunnar's question. So of those like 20,000 trials, how many are like close to like this best one? I would say not many. So it's like a handful. It's really hard to find, and it's a handful. Yeah. I would say for. I would say for very small values, you will see this over and over again. Like if you take n equals 5, you'll see the same thing over and over again. But if you go to larger values, it actually is really hard to find, which sort of gives some sense that even the picture I showed you isn't the actual picture. Even if it's only an anaphobe, you'd trust me to know how it works if they're similar in any way. They're all similar in the way that they have this type of structure. Yeah, sure. The argument, like the Argument, like the thing that happens is it starts very slow, very steady, and the idea is it builds a matrix sort of right around here. I think maybe with like log n steps left. And at first it might look like it's building some sort of Hadamard matrix, but it's not. It's building some other type of structure that's very hard to tease out. And it's trying to build this with some, you know, all entries fairly large. And yeah, that's sort of the game, and you will always see that behavior. See that mean. Oh, so then, like, your optimization, if it's really the end that matters, can you like start with a matrix that, already, like, you know, you found a good matrix that seems to be going and then just solve like the bottom part of it, like optimize that. Say that again? Yeah, so if you found like a reasonably good matrix, it has like this curve, like the blue one. So now, like, you want to really just optimize these last several steps where it's really blowing up. So, can you just reduce the optimization? So, can you just like reduce the optimization to figuring out? Oh, so if you look just at the last couple of steps, it's always optimal, conditional on where you can then. So, maybe what do you conjecture? So, it looks like you're starting almost to prove that Wilkinson bound is tight, whereas we were thinking it was not tight. I mean, it looks like you're getting to Wilkinson was right with his bound. Yeah, yeah. So, okay, this is something that Alan. Okay, I don't want. This is something that Alan, okay. I don't want to say anything too strong, but I really do. When I look at this, I think Wilkinson was more right than we actually thought. That's my opinion. I wasn't going to say that, but I think Wilkinson was more right than we thought. Is that the super polymer? Yeah, that's a quasi-pollen movement. That's folks. That's the actual polymer. I was a watch that just